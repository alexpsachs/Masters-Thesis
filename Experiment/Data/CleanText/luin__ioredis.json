{
    "patrickdevivo": "+1\n. ",
    "phlip9": "+1\n. If we're trying to improve the performance of the JavaScript parser, it would make sense to start with performance benchmarks comparing the parsers from ioredis, node_redis, and hiredis so we don't end up making inadvertent regressions.\nThis talk by Felix Geisend\u00f6rfer seems relevant: Faster than C (pdf)\nHe talks about getting the node-mysql parser up to speed parity (in some cases even exceeding) with the native C bindings.\n. Awesome, thanks!\n. Sure, I'll probably get to this later tonight.\n. ",
    "luin": "Closing this issue for the pull request #154\n. @adoyle-h Yes, refer to https://github.com/luin/ioredis/blob/master/API.md for lazyConnect\n. \u54c8\u54c8\u4e0d\u9519\uff0c\u4f60\u4eec\u4e3a\u5565\u4e5f\u81ea\u5df1\u5b9e\u73b0\u5462\uff0c\u662f\u56e0\u4e3a\u60f3\u7528 thunk\uff1f\n. \u8fd9\u6837\u554a\uff0c\u5df2 star\uff0c\u591a\u591a\u4ea4\u6d41\uff01\n. :)\nI have just published v1.0.6. It seems to be a bug of npm.\n. makes sense. I prefer to create an external package to include these scripts since it's easy to use them without increasing complexity to ioredis itself:\njavascript\nvar Redis = require('ioredis');\nvar redis = new Redis();\nrequire('ioredis-extensions')(redis);\n. Closing this issue since it seems to be a little hacky to implement pipelining in cluster.\n. Support for pipelining is also done!\n. Hi gagle! I chose the debug module because there are already many libraries(3814 currently) depending on it. It's hard to avoid using all of these modules(e.g. Koa, Connect, Stylus, etc.) when writing a Node.js app. As far as I am concerned, debug is a fait accompli. However I'm open to suggestions and keep an open mind if you'd like to persuade me either way :)\n. @dirkbonhomme lol\n. Thank your for the correction\n. You shouldn't make a master to be the slave of one of it's slaves. Say 6380 is the slave of 6379, you shouldn't call SLAVEOF localhost 6380 on 6379 because it will make these two nodes both be the slave of each other, which doesn't make any sense and no failover happens.\nActually you shouldn't call SLAVEOF manually to trigger a failover in a sentinel cluster, instead you can kill 6379 and the failover will happen automatically.\n. Please also let me know if there's no issue :-)\nSent from Outlook\nOn Mon, Apr 27, 2015 at 10:50 AM -0700, \"Saltuk Alakus\" notifications@github.com wrote:\nActually initially I had tried that, but in the same machine killed redis instance quickly gets up so master does not change. I will check in a more real setup, may be more than 1 machine. If I observe an issue I will let you know. Thanks.\n\u2014\nReply to this email directly or view it on GitHub.\n. ioredis is developed from scratch and I haven't read all the code of node_redis. So to be frank, I don't know why ioredis has a better performance than node_redis :-)\n. Could you please provide an example to reproduce the max call stack errors?\n. I ran the tests successfully and get the same error as you. I create a new branch to fix the error: https://github.com/luin/ioredis/commit/b81b625a4ab93d71e997f32a291b2c956710ff6a. It's faster than yours when all arguments are strings. However it seems that this change still doesn't pass all your tests. Could you please check it out and see whether it works correctly?\n. Released in 1.1.3. Thank you for the pull request!\n. This behavior is currently by designed since auto re-brpop may have side effects. For instance when you give brpop a timeout other than 0, it would be ambiguity to decide the value of the timeout for the new brpop when reconnected.\n. You may send the brpop command on the ready event which will be emitted when connected/reconnected:\njavascript\nvar client = new Redis();\nclient.on('ready', ListenToRedis);\nHowever I'm considering to add an option to enable auto re-brpop when reconnected.\n. Released in 1.2.0. Doc: https://github.com/luin/ioredis#auto-reconnect\n. In most situations you don't, since connections to redis is lightweight and will be closed when your program exits.\n. It seems to work for me. Could you please post your logs here?\nhttps://github.com/luin/ioredis#debug\n. it seems that it takes a long time to connect to the cluster before sending multi command. Which ioredis version are you using? If you are using 1.2.0, could you post a more comprehensive logs including connection status like: https://gist.github.com/luin/5f8be349903d2cb41894\n. well, that's strange. Are you using co? I tried the following code and it's working fine:\n``` javascript\nvar cluster = new Redis.Cluster([{\n  port: 6380,\n  host: '127.0.0.1'\n}, {\n  port: 6381,\n  host: '127.0.0.1'\n}]);\nvar co = require('co');\nco(function* () {\n  var redis_key = 'processing_batch:devices';\n  console.log('processing_batch:devices');\n  return yield cluster.multi().lrange(redis_key, 0, 99).ltrim(redis_key, 100, -1).exec();\n}).then(function () {\n  console.log('got reply:', arguments);\n}).catch(function () {\n  console.log('got error:', arguments);\n});\n```\nTypically promise should be fulfilled if callback is invoked since ioredis uses nodeify method of bluebird: https://github.com/petkaantonov/bluebird/blob/master/API.md#nodeifyfunction-callback--object-options---promise\n. Is bb.coroutine a npm package? Could you also post the code that using promise?\n. Here's my code:\n``` javascript\nvar redis = new Redis.Cluster([{\n  port: 6380,\n  host: '127.0.0.1'\n}, {\n  port: 6381,\n  host: '127.0.0.1'\n}]);\nvar bluebird = require('bluebird');\nvar running_inserts = false;\nvar keys = ['a', 'b', 'c'];\nvar run_inserts = bluebird.coroutine(function* () {\n  if (!running_inserts) {\n    var processing_fn = function(key) {\n      return function(replies) {\n        console.log(arguments);\n        if (replies[0][1] && replies[0][1].length > 0) {\n          var batch = replies[0][1];\n          //do something with the batch\n        }\n      };\n    };\n    running_inserts = true;\n    for (var i = 0; i < keys.length; i++) {\n      var redis_key = \"processing_batch:\" + keys[i];\n      console.log(redis_key);\n      var result = yield redis.multi().lrange(redis_key, 0, 99).ltrim(redis_key, 100, -1).exec();\n      processing_fn(keys[i])(result);\n    }\n    running_inserts = false;\n  }\n});\nrun_inserts();\n```\nAnd the logs I received:\nprocessing_batch:a\n{ '0': [ [ null, [] ], [ null, 'OK' ] ] }\nprocessing_batch:b\n{ '0': [ [ null, [] ], [ null, 'OK' ] ] }\nprocessing_batch:c\n{ '0': [ [ null, [] ], [ null, 'OK' ] ] }\nYou may use the above code the connect to your cluster to see whether it's a connection-related error or not.\n. It makes no difference to execute run_inserts in a setInterval. Are you still able to reproduce the problem?\n. I've run the benchmark in my computers and our servers, and the results are similar. However there are many factors that can impact the results, so it's hard to say why yours are different. I just submit a commit to print system info as well when running benchmark, and here's the result on my laptop(MacBook Pro, Retina, 15-inch, Late 2013):\n```\nioredis git:master \u276f npm run bench                \u2739\n\nioredis@1.2.0 bench /Users/luin/opensource/ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nioredis: 1.2.0\nnode_redis: 0.12.1\nCPU: 8\nOS: darwin x64\n==========================\n                  simple set\n      75,846 op/s \u00bb ioredis\n      43,185 op/s \u00bb node_redis\n\n                  simple get\n      75,463 op/s \u00bb ioredis\n      41,240 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      12,329 op/s \u00bb ioredis\n       4,146 op/s \u00bb node_redis\n\n                  lrange 100\n      58,273 op/s \u00bb ioredis\n      45,782 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 61,455.94 ms\n```\nCould you pull the latest commit and run the benchmark again?\n. Which redis-server version are you using? Is Redis running on the same machine as the client?\n. I added a link to this issue to the benchmark section in README, and will leave this issue open util we know more about the reason of the difference of results.\nAt a guess, ioredis has better performance when the server is able to handle higher concurrency.\n. npm run bench. \u6587\u6863\u91cc\u6709\u5199 https://github.com/luin/ioredis#benchmark\n. npm run \u65f6\u4f1a\u8bbe\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\u4fdd\u8bc1 devdeps \u91cc\u9762\u7684\u4f9d\u8d56\u4f1a\u4f5c\u4e3a\u5168\u5c40\u4f9d\u8d56\u3002Windows \u4f1a\u6709\u95ee\u9898\u3002\u53ef\u4ee5\u76f4\u63a5\u7528 npm i -g matcha\n. \u76f4\u63a5\u6267\u884c matcha single_node.js \u5427\n. ioredis works with node-style callback and promise. However in you case, you mix them up. The following code should work as you expect:\n``` javascript\nreturn redis.multi()\n    .sismember('test-set', someID)\n    .sismember('another-test-set', someID)\n    .exec().then(function (results) {\n      var exists = results.reduce(function (prev, curr, i) {\n          return (curr[1] === 1 || prev)\n      }, false)\n  if (exists === false) {\n      return redis.multi()\n          .sadd('test-set', someID)\n          .sadd('another-test-set', someID)\n          .exec()\n  }\n});\n\n``\n. This error stack doesn't help much. Could you please enableshowFriendlyErrorStack(described in https://github.com/luin/ioredis#error-handling) and give me a full error stack.\n. https://github.com/luin/ioredis#error-handling\n. What do you mean by \"avoid\"? If you don't want see these errors in the log, just give every command a callback or useonPossiblyUnhandledRejectionto catch errors globally.\n. Not possible with ioredis. For instanceCLUSTERDOWN The cluster is down` is because not all slots are served by the cluster, which can be avoided by config Redis properly. \n. It seems that we can just resend the command when a CLUSTERDOWN error is received. Thank you for the information.\nIf you want to handle CLUSTERDOWN on the application side, you have to catch all errors returned from Redis and resend the command if the error is CLUSTERDOWN.\n. You can get previous errors by error.previousErrors to see what they are. For instance:\njavascript\nredis.multi().set('foo').get('foo').exec().catch(function (err) {\n  console.log(err.previousErrors);\n});\n. autoResendUnfulfilledCommands is used to send unfulfilled commands after a reconnection. Since CLUSTERDOWN is caused by a master's being offline, we may use clusterRetryStrategy option to retry the node.\n. Is there a resharding or failover happens after cluster has been initialized?\n. Yes, you are right. ioredis should be able to handle these MOVED errors. I'll try to fix these errors tonight.\n. @thelinuxlich As stated in the README, ioredis will use the first key in the pipeline queue to calculate the slot. So the problem here isn't ioredis use the wrong key, instead is ioredis doesn't handle MOVED errors properly in the transaction.\n. @thelinuxlich No, it doesn't help since it's a bug of ioredis. I'll fix it soon :-)\n. @thelinuxlich That's interesting. However I'm implementing a more stable transaction strategy in cluster mode.\n. @thelinuxlich Yes, this commit just fixed CLUSTERDOWN errors, and I'm still working on handling MOVED errors :-)\n. There is lot of work to do to implement a stable transaction in cluster mode. However the job is getting done :-)\nPull request #33 should handle MOVED, ASK and CLUSTERDOWN error properly. I'm writing more tests for it and will ship it in the next version. Welcome to do some tests if you have time.\n. Released in 1.3.0\n. @ScheerMT I second that idea. If someone writes a package that works with ioredis, just post it to this issue. I'd like to add these packages to the readme (or wiki) sometime.\n. Released in 1.2.7\n. Hi @WaldoHatesYou, It's a bug introduced recently.\nFixed in 1.2.3. Thank you for pointing this out.\n. In most use cases, the pure javascript parser and hiredis are similar in terms of performance.\n. Could you give me an example where detect_buffers is needed?\n. ioredis handles binary data in a different way described here and here. messageBuffer should be listened instead of message if you want to get binary message using ioredis(https://github.com/Automattic/socket.io-redis/blob/master/index.js#L77).\n. Released in 1.3.1. These changes improves the performance a lot. Thank you for pointing this out :-).\n. Yes, it's possible. However you still need migrate your code since the results of exec are different between ioredis and node_redis. For more information: https://github.com/luin/ioredis/wiki/Migrating-from-node_redis\n. Hi, I just implement this API and ship it in 1.3.2\n. It sounds useful. How would the syntax look? Is it something like new Redis('@hostname')?\n. Sounds great! I would like to have this feature :-)\n. I prefer new Redis({ srv: 'redis.consul' }).\n. pr is still welcomed\n. Makes sense. Besides a list of preferred slaves, I think we may also accept a function to let user pick up the slave:\njavascript\nvar redis = new Redis({\n  sentinels: [{ host: 'localhost', port: 26379 }, { host: 'localhost', port: 26380 }],\n  preferredSlaves: function (availableSlaves) {\n    return _.sample(availableSlaves);\n  },\n  name: 'mymaster'\n});\n. Aha, my fault. It seems that slave priority have to be set on the Redis side. See http://redis.io/topics/sentinel#slaves-priority for details.\n. I got your point. The code looks great. Feel free to send a pull request and I'll definitely merge it.\n. The easiest way is to limit total amount of reconnects in development env:\n``` javascript\nvar retryStrategy;\nif (process.env.NODE_ENV === 'production') {\n  retryStrategy = function (times) {\n    var delay = Math.min(times * 2, 2000);\n    return delay;\n  };\n} else {\n  retryStrategy = function (times) {\n    if (times < 5) {\n      return 200;\n    }\n  };\n}\nvar Redis = require('ioredis');\nvar redis = new Redis({\n  retryStrategy: retryStrategy\n});\nredis.on('end', function () {\n  throw new Error('Redis is down');\n});\n```\nIf you don't need reconnecting, you can just throw errors in the close event:\n``` javascript\nvar Redis = require('ioredis');\nvar redis = new Redis();\nredis.on('close', function () {\n  throw new Error('Redis is down');\n});\n``\n. Done. I just ran the tests and it turns out all tests are passed in node 0.10.x. I forget the reason why I state ioredis only works with Node.js whose version >= 0.11.16.\n. Looks like to be the same problem as https://github.com/luin/ioredis/commit/f663fe236d4b83d3f836e61748906eb9952a6211. I'm looking into it.\n. Released in 1.3.6\n. There's already a mock server used in the unit tests of ioredis. https://github.com/luin/ioredis/blob/master/test/helpers/mock_server.js\n. In terms of unit testing, which you need is not a fake client that doing exactly the same thing(hopefully) as ioredis except leaving Redis server untouched, instead, you need a convenient way to stub calls to Redis server, which ismockServer` doing. I've checked out mock libraries you suggested long ago and IMHO they are on the wrong tack.\n. Closing the issue. Feel free to reopen it with more detailed informations.\n. The mock server isn't intent on being a redis emulator, instead, it's designed to make testing for ioredis easier. Refer to https://github.com/luin/ioredis/blob/master/test/functional/cluster.js for more use cases.\nA simple example to test whether ioredis sends GET command to the redis server (mock server here) when calling .get() method:\njavascript\nvar port = 17379;\nit('should send GET command', function (done) {\n  var redis = new Redis({ port: port });\n  var server = new MockServer(port, function (argv) {\n    if (argv[0] === 'get') {\n      redis.disconnect();\n      server.disconnect();\n      done();\n    }\n  });\n  redis.get('foo');\n});\n. Not that I know of. As for me, mock_server is a good choice if you are writing unit tests since it's handy to test the interaction between your function and the redis server. For integration testing, running a real redis server is a wiser choice because emulators like fakeredis cannot 100% give you the same behaviour as real redis.\n. @stipsan looks promising!\n. @stipsan Sounds good. Would you like to make a pull request for this?\n. Yes, it would be useful to have a method like client.unref() to do the same thing since client.connector.stream isn't a public variable. I'm going to add this feature in next release.\n. After some tests, unref seems to be a little unpredictable when used to exit the process when there's no more work to be done. For example:\njavascript\nvar Redis = require('ioredis');\nvar redis = new Redis();\nredis.on('ready', function () {\n  redis.get('foo', console.log);\n  redis.stream.unref();\n});\nThe above code will exit without waiting the result  of get command being printed. However, the following code will have the result printed before the process exits.\njavascript\nvar redis = new Redis();\nsetTimeout(function () {\n  redis.get('foo', console.log);\n  redis.stream.unref();\n}, 100);\nWhat's more, since unref doesn't have any awareness of the Redis protocol, when invoking some blocked commands(e.g. brpop), process will exit without waiting the block command being returned.\nI'm considering add an option unrefWhenDrain or something similar. When the option is enabled, ioredis would call unref when there's no more pending command and call ref when a new command is invoked.\n. Could you show me your code causing the error?\n. I don't think there's any problem when requiring ioredis without hiredis installed since require('hiredis') is wrapped in a try/catch.\nI'm closing this issue. However if you can reproduce the problem, feel free to open this issue again.\n. Yes, ioredis uses bluebird instead of native promise for better performance and the support for node 0.10.x\n. Besides API, ioredis also uses Promise internally, so it wouldn't be much harder to support promise API natively.\n. Yes, ioredis does support these options. Actually ioredis just passes all arguments to the Redis server directly.\n. it should be redis.set('mykey', 'blah', 'EX', '10');\n. No, it's not correct. Originally ioredis only works with 0.11.16, but now ioredis is compatible with 0.10.16.\n. Thank you for reporting this. It's a bug that monitors don't work after reconnection and will be fixed in the next release. However I just did some tests but haven't found any way to reproduce the Command queue state error. Could you please show me the code causing the error? Here's mine:\n``` javascript\nvar redis = new Redis(6999);\nvar listener = function (time, args) {\n  console.log(time, args);\n};\nredis.monitor(function (err, monitor) {\n  monitor.on('monitor', listener);\n  redis.on('ready', function () {\n    monitor.removeListener('monitor', listener);\n    monitor.on('monitor', listener);\n  });\n});\n``\n. It's a glob pattern.?means any char.\n. Codes in the master branch should fix this issue. I'm implementing auto-resubscribe and doing more tests before releasing a new version.\n. Seems like you are invoking commands before cluster is connected. I saw your pr and the changes you making would cause this error.\n. In most cases, you don't need to make sure the cluster is connected sinceRedis.Cluster` buffers all commands invoked before the cluster is connected and will execute them when cluster is ready. So the following codes works:\njavascript\nvar instance = new redis.Cluster(hosts, { lazyConnect: true });\ninstance.set(Date.now().toString(), 'somerandomvalue');\nIf lazyConnect is enabled, Redis.Cluster wouldn't connect to the every node in the Cluster but a few(typical only one) of them just to get the cluster info. When a command is invoked and the target node hasn't been connected, ioredis will connent to it automatically.\nready event in Redis.Cluster simply means ioredis has gotten the info of the Cluster from one of the nodes.\n. Let me know if there's anything I can do :-)\n. @AVVS That makes sense. At first glance ioredis should retry refreshing the cluster info(related option: clusterRetryStrategy) when none of the nodes are ready. I'm going on a trip today and will look into this issue on the plane. Thank you for the information\n. Should be fixed in 1.4.0\n. Yes, When the cluster slots command returns an empty array(see the below snippet), ioredis would raise the TypeError, which is obvious a bug.\n127.0.0.1:8888> cluster slots\n(empty list or set)\n. I submitted a pr to fix this issue. I'll check it again tomorrow and welcome to give it a try if you have time.\n. Kuai gei wo juan qian, da wo zhifubao li.\n. When invoking a command without a key associated(like script load, info etc.), the command will be sent to a random node. In your case, script load and evalsha may be sent to the different nodes.\nYou can use custom commands instead:\n``` javascript\nredis.defineCommand('echo', {\n  numberOfKeys: 1,\n  lua: 'return {KEYS[1],ARGV[1],ARGV[2]}  '\n});\nrequire('co')(function *() {\n  var res = yield redis.echo('hello', 'world', '1234');\n  console.log(res);\n});\n```\nYou can also use pipelining to send multiple commands to the same node.\nREF:\nhttps://github.com/luin/ioredis#lua-scripting\nhttps://github.com/luin/ioredis#pipelining\nhttps://github.com/luin/ioredis#transaction-and-pipeline-in-cluster-mode\n. Dealing with a cluster is not as same as dealing with a single redis node. It's inevitable to make some changes on the application side in order to migrate to a Redis cluster.\nI'm considering add a broadcast method to send commands to all of the nodes though:\njavascript\ncluster.broadcast().script('load', 'return {KEYS[1],ARGV[1],ARGV[2]}');\n. Btw, you can access nodes in the cluster via cluster.nodes and send commands to all of them yourself:\njavascript\nyield Promise.all(Object.keys(cluster.nodes).map(function (nodeName) {\n  var node = cluster.nodes[nodeName];\n  return node.script('load');\n}));\n. \u53ef\u4ee5\u53c2\u8003\u8fd9\u91cc\uff1a\nhttps://github.com/luin/ioredis#auto-reconnect\nhttps://github.com/luin/ioredis#connection-events\n\u5f53\u8fde\u63a5\u4e0d\u4e0a\u65f6\uff0cioredis \u4f1a\u53d1\u51fa close \u4e8b\u4ef6\uff0c\u5e76\u8c03\u7528 retryStrategy\uff0c\u5982\u679c\u8fd4\u56de\u6570\u5b57\u5219\u91cd\u8bd5\uff0c\u5426\u5219\u53d1\u51fa end \u4e8b\u4ef6\u3002\n. retryStrategy \u662f\u6709\u9ed8\u8ba4\u503c\u7684\uff0c\u6587\u6863\u91cc\u6709\u5199\uff0c\u6240\u4ee5\u4e0d\u8bbe\u7f6e retryStrategy \u7684\u8bdd\u4f1a\u4e0d\u65ad\u8fdb\u884c\u91cd\u8fde\u3002\u5982\u679c\u65e0\u6cd5\u8fde\u63a5\u4f1a\u62a5\u9519\u7136\u540e\u65ad\u5f00\uff0c\u5373\u5148\u53d1\u51fa error \u4e8b\u4ef6\uff0c\u518d\u53d1\u51fa close \u4e8b\u4ef6\u3002\n. \u8fd9\u4e2a\u8981\u53d6\u51b3\u4e8e\u4f60\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u679c\u4e0d quit \u7684\u8bdd\u8fde\u63a5\u4f1a\u4e00\u76f4\u4fdd\u6301\u3002\u4e00\u4e2a\u5e94\u7528\u4fdd\u6301\u5341\u51e0\u4e2a\u8fde\u63a5\u662f\u6ca1\u95ee\u9898\u7684\uff0c\u5f53\u8fdb\u7a0b\u9000\u51fa\u540e\uff0c\u6240\u6709\u8fde\u63a5\u90fd\u4f1a\u81ea\u52a8\u65ad\u5f00\u3002\u4f46\u5982\u679c\u6bcf\u6b21\u8bf7\u6c42\u90fd\u521b\u5efa\u4e00\u4e2a\u65b0\u8fde\u63a5\uff0c\u8fde\u63a5\u6570\u91cf\u65e0\u9650\u4e0a\u6da8\u80af\u5b9a\u5c31\u4e0d\u884c\u4e86\uff0c\u8981\u624b\u52a8 quit\u3002\n. \u8fd9\u4e2a\u4e0e\u4f60\u7684\u5e94\u7528\u903b\u8f91\u76f8\u5173\uff0cRedis \u662f\u5355\u8fdb\u7a0b\u7684\uff0c\u6240\u4ee5\u5efa\u7acb\u591a\u4e2a\u8fde\u63a5\u6ca1\u5fc5\u8981\n. \u80af\u5b9a\u662f\u7b2c\u4e00\u4e2a\n. \u6ca1\u6709\u8fde\u63a5\u6c60\u7684\u8bbe\u8ba1\n. Either\njavascript\nredis.echo(3, 'k1', 'k2', 'k3', 'a1', 'a2', 'a3', 'a4', function() {});\nor\njavascript\nredis.echo(3, KEYS, ARGS, function() {});\nworks.\n. https://github.com/luin/ioredis#auto-reconnect\nWhat's you need is retryStrategy.\n. max_attempts in node_redis is used for reconnection, not for per request, just like retryStrategy in ioredis. You can set enableOfflineQueue to false to disable offline queue.\nYou may handle retrying and timeout of a request on the application side.\n. Currently timeout method works because when the command timed out, bluebird rejects the promise. However since not knowing the fact, ioredis will still try to resend the command until get a reply from redis server, which would be ignored though because the promise has already been rejected.\nIt's not hard to add the feature of per-request retries limiting with the API something like redis. maxAttempts(3).set('somekey', 'somevalue') and redis.timeout(5000).set('somekey', 'somevalue'). Aware of the limiting, ioredis is able to pulling the command out of offline queue as long as the limiting is reached.\nHowever I'd like to see a better API since redis.timeout(5000).set(k, v) may mislead people into thinking timeout is a redis command.\n. Does adding an option of maxAttemptsPerRequest to the Redis constructor works for you? @Jabher \n. It haven't been implemented. I'm just wondering whether it would solve your problem to add a maxAttemptsPerRequest option. :-)\n. I would prefer adding maxAttemptsPerRequest and timeoutPerRequest options. Any other ideas?\n. @AVVS That sounds good.\n. You may disable the retryStrategy option so that once the connection to redis is lost, all commands will be rejected immediately.\nIf you're using promise, you can use timeout method:\njavascript\nlet data;\ntry {\n  data = await redis.get('key').timeout(500);\n} catch (err) {\n  data = await mysql.query(/* ... */);\n}\ntimeoutPerRequest option is the top priority one in my todo list though.\n. @nicholasf It's not easy to support timeoutPerRequest option efficiently. The alternative way is to support maxRetriesPerRequest, which can be implemented on the client side by flushing the offlineQueue in the retryStrategy, so I'm not sure whether this feature need to be implemented.\n. When a node is disconnected(https://github.com/luin/ioredis/blob/master/lib/cluster.js#L407), ioredis will re-select a new subscriber automatically(https://github.com/luin/ioredis/blob/master/lib/cluster.js#L164). So you don't need to call selectSubscriber manually.\n. Awesome! Thank you for the patch!\n. It's because sentinel nodes didn't return the master address.\nCould you please use redis-cli to connect to one of the sentinel and see what does it return?\nshell\n$ redis-cli -h redis-1 -p 26379\nredis-1:26379> sentinel get-master-addr-by-name redismastert\nI guess you misspelled the master name which should be redismaster instead of redismastert\n. I'm closing the issue. Feel free to reopen it if the problem still here :-)\n. The first thought looks interesting. Have you found any articles proving the difference?\n. Commands that are not associated with specific keys(e.g. keys, info) are sent to a random node of the cluster.\nCurrently you may use:\njavascript\nPromise.all(Object.keys(cluster.nodes).map(function (nodeName) {\n  return cluster.nodes[nodeName].keys('*');\n}));\nI'm considering add a broadcast method though.\n. retryStrategy should satisfy your needs. For instance you want to limit retry times to 3:\njavascript\nvar redis = new Redis({\n  retryStrategy: function (times) {\n    if (times <= 3) {\n      return 200;\n    }\n  }\n});\n. It makes people confused to have multiple options for the same purpose IMHO(most glaring example is the priority of these options). retryStrategy is more relevant to the demands of the production environment than maxRetryTimes and maxRetryDelay.\n. Events could be caught by listening to the specified events of Redis instances. Say you can catch connect errors by listening to error and close. it's just a matter of convenience to return a promise of the connect method for developers to catch connect errors easier.\n. connect always throws when the connection is failed regardless of the value of enableOfflineQueue:\njavascript\nvar redis = new Redis({\n  host: 'non-exists host',\n  lazyConnect: true,\n  enableOfflineQueue: false\n});\nredis.connect().catch(function () {\n  console.log('should throw');\n});\n. Awesome! I'll check it out ASAP.\n. Just added some comments\n. Looks good to me. Does it work?\n. Just a note, the following code also works:\n``` javascript\nvar redis = new Redis(6880);\nvar func = Redis.Promise.coroutine(function *() {\n  try {\n    // set timeout\n    var bar = yield redis.get('foo').timeout(1000);\n  } catch (e) {\n    console.log(e);\n    // e: [TimeoutError: operation timed out]\n  }\n  console.log('bar', bar);\n});\nfunc();\n``\n. Looks good. Could you please change the indent to 2 spaces?\n. Merged. Thank you for the patch!\n. When connected to the one of the nodes in cluster, ioredis emitsconnectevent and sendsCLUSTER SLOTSto get all nodes that is responsible for at least one slot. If the result is empty(which is your situation), ioredis would disconnect from the cluster(emittingclose`) and try to reconnect.\nIf you listen to the close event, the result would be:\nioredis git:e323df0 \u276f node test\nconnected\nclosed\nconnected\nclosed\nconnected\nclosed\nTo limit the retry times, setting the clusterRetryStrategy option.\nBTW, you could use DEBUG=ioredis:* node yourcode.js to see the debug messages.\n. quit is a redis command just like get/set etc.\nhttp://redis.io/commands/quit\n. It's a bug that getting a NaN db when the pathname is empty. Fixed in 1.5.1. Thanks for pointing it out.\n. Thank you for reviewing the code. IMHO exports = module.exports is a convention that used in many node modules(e.g. Express).\nLet me know if there's anything that I can help you. Thanks again for using ioredis.\n. Sorry for my late response, I was on the plane. Exporting multiple object via exports is absolutely not a bad design. In fact the most natural way to exports values in Node.js is assigning them to the exports instead of re-assigning module.exports. For instance ioredis may exports values by:\njavascript\nexports.Redis = // xxx\nexports.Cluster = // xxx\nexports.ReplyError = // xxx\nThis way when people want to init a new Redis instance to connect to the Redis server, they have to:\njavascript\nvar ioredis = require('ioredis')\nvar redis = new ioredis.Redis()\nHowever almost 90% developer may care only the Redis class, it's a little redundant to write ioredis.Redis every time. So this library exports the Redis class as a default value and on the other hand, still exports the other values(Cluster, ReplyError) via exports.\nexports = module.exports is used to make sure exports point to the module.exports.\nYour way exporting values by using ioredisExports has the same result as the library current does. But semantically ioredisExports makes people confused. Think about the following two code snippets:\njavascript\nimport Redis, { Cluster, ReplyError } from 'ioredis';\njavascript\nimport Redis from 'ioredis';\nlet Cluster = Redis.Cluster;\nlet ReplyError = Redis.ReplyError;\n. Exporting multiple objects and having one of them as the default value of the module are different from exporting an object(IORedis) that has multiple objects as its properties, semantically.\n. How does LRU work when used as the eviction policy of the offline queue?\n. Seems a fixed-size offline queue can solve the problem.\n. Accurately a fixed-size FIFO queue. The oldest command would be flushed with an error when the queue gets above a certain size.\n. @dirkbonhomme I really like this approach, which let developer deal with offline queue in a more flexible way.\nCurrently offline queue could be accessed by:\njavascript\nretryStrategy: function (times) {\n    if(times > 10 && redis.offlineQueue.length > 500){\n        redis.offlineQueue.pop(...)\n   }\n}\nHowever offlineQueue property is not documented and it's a double ended queue instead of a typical array.\n. Yes, I like the suggestion from @dirkbonhomme, and since offlineQueue property is not a native array (as my previous comment said), we should document it.\nHowever, although this way is very flexible, it may not that straightforward to use.\nI don't think flushing commands that mutate state do harm since if your application relies on a command's executing successfully, you'd better handle it's error response properly. For a command that relying on the previous command, they would be better to be sent within a transaction.\n. The commands that are shifted from the command queue will be rejected by a specified error (\"offline queue is full\"). Say we have two commands that the latter command relies on the former one.:\njavascript\nredis.set('foo', 'bar').then(function (res) {\n  return redis.get('foo');\n});\nThe second command will only be invoked when the first command is successful, so that shift the first command from the offline queue when the offline queue is full should not break the application.\nI agree with you that maxOfflineQueueMemorySize may be better than maxOfflineQueueSize. However it's difficult to get the size of commands because we only converted them to redis protocol strings at the time of sending them to the redis server instead of pushing them to the offline queue.\nI think we may support maxOfflineQueueSize and at the same time document the solution from @dirkbonhomme.\n. Let user handle offline queue in retryStrategy is a very flexible way, however I'm afraid that's not enough since retryStrategy is only invoked when reconnecting, user can't do anything with offline queue between retries. maxOfflineQueueSize is useful for this case.\nFlushing the overflow commands is a simple way and shouldn't cause any problem about the state if developers handle the errors of each command properly.\nWhat's more, we may emit an 'offlineQueueOverflow' event when the offline line queue size reaches the maxOfflineQueueSize option to let users handle the offline queue themselves before we flushing the overflow commands.\nRelated issue: https://github.com/NodeRedis/node_redis/issues/932.\n. I reverted the maxOfflineQueueSize from the master branch and open a pull request (https://github.com/luin/ioredis/pull/241) instead so we can discuss it.\n. Code in the master branch should fix this bug. Thank you for letting me know.\n. Nice library!\nYes, the length of cluster.masterNodes is possible to change when adding or removing(e.g. failover) master nodes in the cluster, which is likely to happen in real world scenarios.\n. According to redis cluster spec, keys in a command should belong to the same slot. That means you can't brpop two lists belong to the different slots.\nThe following code works:\njavascript\nclusterClient.brpop('{list}1', '{list}2', function(err, data){\n});\nHowever ioredis definitely need to show a more friendly error info in this case.\n. hi, I just did some tests and what I get is a ReplyError instead of {}. Can you reproduce it?\n. I'm closing the issue. If there's any problem related to it, feel free to reopen it.\n. Seems to be a connection error. When a connection error happens, ioredis would emit error when there's at lease one listener. If there's none of listener listen to the error event, ioredis would just ignore the error and close the connection(and reconnect if retryStrategy is set).\n. hi, are you using ioredis 1.5.4? I found a bug in 1.5.3 that may lead to this problem. \n. makes sense\n. Added in 1.5.6\n. Your solution makes sense. However since SentinelConnector#connect invokes callback asynchronously, we'd better keep the interface same in Connector#connect, so process.nextTick is used here.\nThe problem is we should keep net.createConnection and callback running in the same event loop to ensure all connection errors can be caught. I submitted a pull request #84 which hopefully fixes the issue. Could you please help me review it? :-)\n. Closing the issue since #84 should fix it.\n. This issue happens when reconnecting failed more than one times. Fixed in the master branch. Thanks for reporting.\n. Awesome! Thanks for the patch.\n. \ud83d\udc4f\n. send_command is a private method in node_redis.\n. Well, I checked the documentation of node_redis again and it turns out send_command is a public method since it's documented here. I'm going to implement it in ioredis.\nBTW, Where does metroplex use send_command method? I just checked it out and didn't find it.\n. Added in 1.5.10\n. Typically if you want to make sure the execution order of commands, you should either using pipelining or invoking another command after the previous command returning. For instance:\njavascript\nredis.pipeline().multi().del('foo').del('bar').exec().get('foo').exec();\nOr\njavascript\nredis.multi().del('foo').del('bar').exec(function () {\n  redis.get('foo');\n});\nThis is because although almost all simple commands are sent to the redis server follow the exactly same order they invoked, there are still some exceptions. For example when invoking a custom lua command:\njavascript\nredis.echo();\nredis.get('foo');\nioredis first sends evalsha xxx to the redis server and then send get. However if redis returns a NOSCRIPT error for the evalsha command, ioredis will re-send eval xxx instead. So that outwardly echo is sent after get command.\nWhen it comes to multi/exec, ioredis internally queues commands in the other event loop, so it's sent after the latter commands. As for me, I don't like this behaviour and I'm going to make some changes to queue commands in the same event loop. However although not endearing, the current behaviour isn't wrong.\n. I'm going to try to change this behaviour at this weekend to make multi works like other simple commands.\nBTW just some random workarounds:\n``` javascript\nemitter.on('foo', function () {\n  redis.multi().del('bar').del('baz').exec();\n});\nemitter.on('foo', function () {\n  setImmediate(function () {\n    redis.get.('bar', function (err, res) {\n      if (res) console.log('(\u5c6e\u0ca0\u76ca\u0ca0)\u5c6e Y U STILL HERE');\n    });\n  });\n});\n```\n``` javascript\nemitter.on('foo', function () {\n  redis.multi().del('bar').del('baz').exec(function () {\n    emitter.emit('after foo');\n  });\n});\nemitter.on('after foo', function () {\n  redis.get.('bar', function (err, res) {\n    if (res) console.log('(\u5c6e\u0ca0\u76ca\u0ca0)\u5c6e Y U STILL HERE');\n  });\n});\n```\n``` javascript\nemitter.on('foo', function () {\n  redis.multi({ pipeline: false });\n  redis.del('bar');\n  redis.del('baz');\n  redis.exec();\n});\nemitter.on('foo', function () {\n  redis.get.('bar', function (err, res) {\n    if (res) console.log('(\u5c6e\u0ca0\u76ca\u0ca0)\u5c6e Y U STILL HERE');\n  });\n});\n``\n. Fixed in 1.5.11. \ud83d\ude04\n. \u8fd8\u6ca1\u6709\u8ba1\u5212\u3002Redis \u662f\u5355\u7ebf\u7a0b\u6a21\u578b\uff0c\u6240\u4ee5\u591a\u4e2a\u8fde\u63a5\u4e0d\u4f1a\u6709\u6027\u80fd\u63d0\u5347\u3002\u4f60\u7684\u5e94\u7528\u573a\u666f\u662f\uff1f\n. \u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u6d4b\u8bd5\u4ee3\u7801\u5417\uff1f\u6211\u5468\u672b\u8bd5\u4e00\u4e0b\n. \u653e\u5230 #123 \u8ba8\u8bba\u4e86\uff0c\u8fd9\u4e2a\u5173\u6389\u54af~\n. I incline to implement encryption in a wrapper because Redis is designed to be accessed by trusted clients inside trusted environments, it shouldn't be a general requirement to encrypt data in Redis. Besides, it's pretty straightforward to implement it in a wrapper.\n. There's no special way of handling thescancommand in ioredis. I just found a library https://github.com/brycebaril/redis-scanstreams doing the trick, which however only works with node_redis. I'm thinking of adding some convenient methods to ioredis.\n. I'm going to work on it this weekend.\n. Added 1.6.0. You can check out the [unit tests](https://github.com/luin/ioredis/blob/2ea9c9646d3ed69b95b8e24c74a4cb173d0ee2a8/test/functional/scan_stream.js) for the usage. (It takes longer time to write documentation for a non-native speaker \ud83d\ude22)\n. Just updated documentation: https://github.com/luin/ioredis/blob/a170ac7e95e01ab20e3ccdba0abcd437fb17c90e/README.md#streamify-scaning\n. @damianobarbati It's not the recommended way to usescan` since there may be millions of keys in a redis server, thus this will blow up the memory.. That will be great. We can give user an option to specify a key prefix:\njavascript\nvar redis = new Redis({ keyPrefix: 'app1:' });\nand then the keys of every command sent by this instance should get a prefix of 'app1:'. For instance:\n1. redis.set('foo', 'bar') will actually send SET app1:foo bar;\n2. redis.blpop('key1', 'key2', 'key3', 0) will actually send BLPOP app1:key1 app1:key2 app1:key3 0;\n. We already have a method Command#getKeys for getting keys in a command(https://github.com/luin/ioredis/blob/master/lib/command.js#L85).\n. I prefer to keeping this option simple. My intention to add keyPrefix option is to allow multiple apps using one Redis server.keyPrefix is used as a namespace, so a single string is enough.\n. @Delagen This has already been discussed. Refer to https://github.com/luin/ioredis/issues/239 for more information.\n. Hi Sebastian, ioredis only sends each command to one specified node. If the command has a key, ioredis would calculate the slot of it and send the command to the node serving the slot. However if the command doesn't has a key(e.g. info, flushall etc.), it'll be sent to a random node of the cluster. So when you invoke cluster.flushall, only one node would be flushed.\nIf you want to flush all data of the cluster, you should send flushall to all master nodes, which can be accessed via cluster.masterNodes(https://github.com/luin/ioredis/blob/master/test/functional/cluster.js#L839):\n``` javascript\nfor (var key in cluster.masterNodes) {\n  if (!cluster.hasOwnProperty(key)) continue;\nvar node = cluster.masterNodes[key];\n  node.flushall();\n}\n```\nHowever it make me confused that you said waiting 2 seconds would fix this, if you can confirm this, I'll definitely look into it.\n. Implemented in 2.0. Closing this issue.\nExample:\njavascript\n// Send `FLUSHDB` command to all slaves:\nvar slaves = cluster.nodes('slave');\nPromise.all(slaves.map(function (node) {\n  return node.flushdb();\n}));\n. ioredis supports all commands of Redis. So you can definitely call: redis.expire('foo', 10) or redis.set('foo', 'bar', 'ex', 10)\n. Oops, it seems Command hasn't been exported correctly for a long time. Just fixed it. Thank you for pointing it out.\nutils.convertMapToArray is an internal module of ioredis. The code snippet is just a sample telling people how to use setArgumentTransformer. Added a comment in the Readme. \ud83d\ude04\n. Is it possible to enable showFriendlyErrorStack to figure out which line in your code causing the error? Or you may just check the command property of the error object(every ReplyError has a command property showing the command of this error).\nEither way I'm going to check the code again today to see whether lpush and hset are possible to cause this error.\n. Cool! Enjoy ioredis :-)\n. Looks like you tried to JSON.parse a string in the file /home/vagrant/congredi-api/nodeJs/io.js(line 20).\n. Just implemented the new parser: https://github.com/luin/ioredis/blob/new-parser/lib/parsers/ioredis.js. However, the result doesn't show much difference comparing with the current parser. So I'm closing this issue before we have time to revisit it.\nThe new parser will only be faster when the replies from Redis are broken into many small frames. For example, running the benchmark using the following code:\njavascript\n    javascript.execute(new Buffer('$3'));\n    javascript.execute(new Buffer('\\r'));\n    javascript.execute(new Buffer('\\n'));\n    javascript.execute(new Buffer('f'));\n    javascript.execute(new Buffer('o'));\n    javascript.execute(new Buffer('o'));\n    javascript.execute(new Buffer('\\r\\n'));\nAnd the result will be:\n\nWhich is not going to happen in the real-world environment.\n. Hi, NOSCRIPT occurs because exec is invoking before eval. You can refer to my comment to another issue: https://github.com/luin/ioredis/issues/91#issuecomment-118865612. The first time you run the following code:\njavascript\nredis.multi({ pipeline: false });\nredis.set('foo', 'bar');\nredis.get('foo');\nredis.echoDynamicKeyNumber(2, 'k1', 'k2', 'a1', 'a2');\nredis.exec();\nWould actually sending commands to Redis server as the following order:\nMULTI\nSET foo bar\nGET foo\nEVALSHA\nEXEC\nEVAL\nSince pipelining in ioredis is very flexible, you can use pipelining this way:\njavascript\nvar pipeline = redis.multi().echoDynamicKeyNumber(2, 'k1', 'k2', 'a1', 'a2');\nif (condition) {\n  pipeline.get('foo');\n}\npipeline.exec();\n. Awesome! Just checked out the changes and they're really nice! I'm going to do more tests and we'll soon have this feature. \ud83c\udf7a\n. Released in 1.7.0 \ud83c\udf7a\n. autoResendUnfulfilledCommands option is passed to the Redis instances of the cluster(https://github.com/luin/ioredis/blob/master/lib/cluster.js#L167), so it should work. I tried the following code:\n``` javascript\nvar cluster = new Redis.Cluster([\n  { port: 6380 },\n  { port: 6381 },\n  { port: 6382 }\n]);\nfunction loop(err, res) {\n  (err || res) && console.log(err, res);\n  cluster.blpop('qqq', 0, loop);\n}\nloop();\n```\nWhen I lpush a item to qqq, it prints it, and when I stop the cluster and restart it, lpush a item to qqq again, it still prints it correctly.\n. Just submitted a pr to fix it.https://github.com/luin/ioredis/pull/112. I'm going to do more testing and write unit tests for it. Welcome to test it if you get time.\n. Fixed in 1.7.1\n. ioredis would keep trying to reconnect until the redis server is available again by default. This behaviour is useful in the production that your commands won't fail for a temporary disconnection.\nYou can refer to the documentation to disable/customize auto-reconnecting: https://github.com/luin/ioredis#auto-reconnect\n. status property representing the connection status. It would be ready when the connection is active.\n. Yes, it's correct. When the key doesn't exist, redis would return an \"empty\" value instead of throwing an error, that is for a string key, it would return null, and for a set key, it would return [].\n. Merged! Thank you for the patch \ud83d\ude04\n. \ud83c\udf7a\n. Thanks for the correction! I just read all the changes carefully, and they're really helpful to a non-native speaker \ud83d\ude04\ud83c\udf88\ud83c\udf88\ud83c\udf88\n. \u54c7\uff0c\u4f60\u7adf\u7136\u4f1a\u4e2d\u6587\uff01\n. Awesome! \ud83c\udf7a\n. \u8868\u4e2d ioredis \u7684 subscribe \u7684\u5cf0\u8c37\u503c\u5dee\u8ddd\u6bd4\u8f83\u5927\uff0c\u548c node_redis \u5bf9\u6bd4\u5982\u4f55\u5462\n. \u66f4\u65b0\u4e86\u9996\u9875\u7684 README\uff0c\u591a\u8c22\uff01\n. Cool! Released in 1.7.3 \ud83c\udf7a\n. Actually they should have similar performance since ioredis passes arguments to the flatten method of Lodash without checking whether they are arrays: https://github.com/luin/ioredis/blob/master/lib/command.js#L48\n. Sounds interesting. I wrote a tests for it: https://gist.github.com/luin/d06770c736866e0571d0. The result shows the former form is little faster. It looks like _.flatten would be faster if the arguments are all plain strings/numbers: https://github.com/lodash/lodash/blob/3.10.0/lodash.src.js#L2122\n. I ran your test case and got the similar result as yours. I though there may be too few arguments in my previous test case, so I updated it but the results between the two forms are still slight. Don't know why.\n. Thanks! merged.\n. The most useful feature I want to implement is Read Write Splitting. For instance, if I have a master and N slaves, I would like to splitting reads and writes so that reads go to the slave servers and writes go to the master server:\n``` javascript\n// here we initialized a typical Redis instance connected to the master server\nvar master = new Redis();\n// and then use it to initialize a Pool instannce.\n// the pool will connect to all slaves of the master automatically.\nvar pool = new Redis.Pool(master);\n// writes will be sent to the master\npool.set('foo', 'bar');\n// and reads will be sent to one of the slaves.\n// however, the result may not be 'bar' because of the replication latency.\npool.get('foo');\n``\n. \u8fd9\u4e2a\u548c\u8fde\u63a5\u6c60\u6ca1\u5173\u7cfb\u3002\u91cd\u8fde\u8fc7\u7a0b\u5f88\u5feb\uff0c\u671f\u95f4\u7684\u547d\u4ee4\u90fd\u4f1a\u88ab\u8bb0\u5f55\u4e0b\u6765\uff0c\u5e76\u5728\u8fde\u63a5\u6210\u529f\u540e\u91cd\u65b0\u53d1\u9001\u3002\u53e6\u5916\u65b0\u7248 ioredis \u9ed8\u8ba4\u652f\u6301 tcp keepalive\n. @majintao \u6ca1\u6709\u5fc5\u8981\u7528 pool\uff0c\u5355 client \u8fde\u63a5\u8db3\u591f\u4e86\u3002\n. @doublesharpscaleReadsfor Sentinel groups should not be hard to be implemented on the client side (Unlike the one for cluster, which have to deal with slots and redirections), so I suggest to not include this feature in ioredis, and instead, write a library for it.\n. Hi,Redis#duplicate` only duplicates the options of the Redis. For example the following code should work:\njavascript\nlet redis = new Redis({ db: 5 })\nlet redis2 = redis.duplicate()\nRelated code: https://github.com/luin/ioredis/blob/master/lib/redis.js#L318\n. Hi, access a property(user_id here) of a null value would lead to an exception and break the application(try the following code). It seems that you wrapped the code with a try/catch so you didn't get the exception\n``` javascript\nvar Redis = require('ioredis');\nvar redis = new Redis();\nvar user = null;\nredis.set('tok:' + user.user_id, function (err, success) {\n  if(err)\n    console.log(err);\n  if(success)\n    console.log(success);\n});\nconsole.log(\"redis key set\");\n```\ngot:\nioredis git:master \u276f node test.js                                                                                                                           \n/Users/luin/opensource/ioredis/test.js:6\nredis.set('tok:' + user.user_id, function (err, success) {\n                       ^\nTypeError: Cannot read property 'user_id' of null\n. Makes sense. Thanks for the pr! \ud83d\ude04\n. I'm doing a little more testing and will release a new version later today.\n. I think we should just return an empty array. Would you like to submit a pull request for this?\n. Thanks for the pr \ud83d\ude04\n. If there's no slave available and the role option is set to the 'slave', then ioredis would retry to reconnect to the sentinel and fetch the slaves info again or just reject with an error according to the value of the sentinelRetryStrategy option(Check out README.md for more information). ioredis would never connect to the master when role is slave.\n. I prefer to implementing this logic on the client side since it's not hard.\n. Nice catch! Thanks for the pull request!\n. queued \u662f\u4ec0\u4e48\u610f\u601d\uff1f\n. \u54e6\u554a\uff0ctransaction \u662f\u57fa\u4e8e pipelining \u7684\uff0c\u6bcf\u4e2a\u5b50\u547d\u4ee4\u7684\u56de\u8c03\u51fd\u6570\u90fd\u662f\u8fd4\u56de\u547d\u4ee4\u672c\u8eab\u7684\u7ed3\u679c\u3002multi \u4e2d\u7684\u5b50\u547d\u4ee4\u7ed3\u679c\u662f\u547d\u4ee4\u52a0\u5165\u7684\u72b6\u6001\uff0c\u5373 QUEUED \u6216\u8005 error\u3002\u8fd9\u6837\u53ef\u4ee5\u4f7f\u63a5\u53e3\u4fdd\u6301\u7edf\u4e00\n. \u55ef\u55ef\u6211\u660e\u767d\u4f60\u7684\u610f\u601d\u3002\u73b0\u5728\u7684\u8868\u73b0\u4e3b\u8981\u662f\u8003\u8651\u5230\u4e0b\u9762\u51e0\u4e2a\u56e0\u7d20\uff1a\n1. \u4fdd\u6301\u63a5\u53e3\u548c node_redis \u517c\u5bb9\uff1b\n2. transaction \u662f\u57fa\u4e8e pipelining \u7684\uff0c\u6240\u4ee5\u8fd4\u56de QUEUED \u662f\u9ed8\u8ba4\u7684\u8868\u73b0\uff0c\u5982\u679c\u4fee\u6539\u7684\u8bdd\u4f1a\u9020\u6210\u8868\u73b0\u4e0d\u4e00\u81f4\uff1b\n3. QUEUED \u672c\u8eab\u662f\u6709\u610f\u4e49\u7684\uff0c\u8fd9\u6837\u53ef\u4ee5\u77e5\u9053\u4e00\u4e2a\u547d\u4ee4\u662f\u5426\u6210\u529f\u5730\u88ab\u52a0\u5165\u5230\u4e86 transaction \u4e2d\uff08\u5982\u679c\u8fd4\u56de\u4e00\u4e2a\u9519\u8bef\u7684\u8bdd\u5219\u8868\u793a\u52a0\u5165\u5931\u8d25\uff09\uff1b\n4. \u5728 exec \u4e2d\u5904\u7406\u7ed3\u679c\u5927\u90e8\u5206\u60c5\u51b5\u4e0b\u5e76\u4e0d\u7279\u522b\u9ebb\u70e6\uff1b\n. \u662f\u7684\uff0c\u53ea\u652f\u6301 2.8.12\u3002Redis Sentinel 2 \u786e\u5b9e\u662f Redis 2.8 \u53d1\u5e03\u7684\uff0c\u4f46\u662f 2.8.12 \u4fee\u590d\u4e86\u975e\u5e38\u591a\u7684 Sentinel \u95ee\u9898\uff0c\u53ef\u4ee5\u8bf4 Sentinel \u5728 2.8.12 \u524d\u662f\u6709\u95ee\u9898\u7684\u3002\u6240\u4ee5 ioredis \u53ea\u652f\u6301 2.8.12\u3002\n\u5927\u90e8\u5206 Sentinel \u529f\u80fd\u90fd\u80fd\u6b63\u5e38\u7684\u5728 2.8.12 \u524d\u7684\u7248\u672c\u8fd0\u884c\uff0c\u4f46\u662f failover \u4f1a\u6709\u95ee\u9898\u3002\n. 31s \u7684\u8bdd\u53ef\u80fd\u662f\u56e0\u4e3a sentinel down-after-milliseconds mymaster 30000 \u7684\u914d\u7f6e\u539f\u56e0\uff0c\u9ed8\u8ba4 30s \u540e redis \u624d\u4f1a\u8ba4\u4e3a master \u574f\u6389\u4e86\u3002\n. What's your purpose to run this module on the browser side?\n. What's the use case for this? What do you expect when connecting to serval non-clustered instances?\n. You can easily write a wrapper on the client side to detect whether the given hosts are standalone servers or clusters. There are many differences between communicating with a standalone server and a cluster(multiple-keys command, pipelining & scripting). Most of the time, the code that works with a standalone server doesn't work with a cluster, so that it makes sense to have both new Redis.Cluster and new Redis.\n. Haven't come across this issue before. I'm going to look into it the weekend. Currently you can set the timeout of a request by:\njavascript\nredis.get('foo').timeout(100).then(function() {}).catch(function() {});\nPossibly related issue #61.\n. redis.get('foo') returns a bluebird Promise. The timeout method is provided by bluebird, so I don't think we need to document it in ioredis's README. However I'd like to find a way to make timeout works with both promise and node-style callback.\n. There are three parameters to control how the OS would detect a dead connection: tcp_keepalive_time, tcp_keepalive_intvl and tcp_keepalive_probes(http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html). Connections are considered as dead after there are tcp_keepalive_intvl * tcp_keepalive_probes seconds(defaults to 75s * 9 = 11min) not receiving the ACK response from the peer.\nObviously the default configuration are not reasonable for a Redis tcp connection, however, node.js only supports set the tcp_keepalive_time parameter(https://github.com/nodejs/node-v0.x-archive/issues/4109).\nI will add an option to ioredis to let users enable keepalive and specify tcp_keepalive_time. For the other two parameters, you can either set them at the system level, or just implement heartbeat on the client side(e.g. send PING command to the Redis every 2s, and close the client by redis.disconnect() when there's no response for a specified interval).\n. @LeDominik It may be because redis server didn't close the TCP connection before closing so the only way client can notice the dead connection would be the keepalive mechanism. Do the server and client run in the same machine? And how do you quit the redis?\n. @LeDominik It should because the client is keeping reconnecting to the Redis. Check out https://github.com/luin/ioredis#auto-reconnect\n. @LeDominik Yup\n. Well, that makes sense. I'd like to add an option for this.\n. Closing this issue since the keepAlive option is added in 1.8.0. Refer to #61 for any further updates about per-operation timeout.\n. +1 for this, except .DS_Store should be ignored globally instead of being added to the project's .gitignore file since this kind of files are not ioredis specified.\n. You'd better amend this commit or just submit an other commit to revert the changes of .gitignore so that I can merge this pr directly. :rocket: \n. That's really strange. Are you able to reproduce it?\n. 1. Which version of ioredis are you using?\n2. Could you please add the following codes to log self and self.connect before the 78 line of event_handler.js:\njavascript\n   console.log('print self:', self);\n   console.log('print self.connect:', self.connect.toString());\n. :laughing: \n. What errors did you get?\n. That's normal because ECONNREFUSED is emitted when connection has failed. By default ioredis will try to reconnect immediately(see https://github.com/luin/ioredis#auto-reconnect) so you will see many errors like this.\n. Considering the performance and usage of Redis, 2ms, 4ms are reasonable reconnecting intervals, especially in the production environment.\n. redis.zrange('foo', 0, 100, 'WITHSCORES');\n. redis.zadd('key', score1, member1, score2, member2)\nor\nredis.zadd('key', [score1, member1, score2, member2])\n. This syntax isn't support by redis natively. However, you can just use redis.zadd('key', _.flatten([member1, member2, member3].map(member => [score, member])))\n. Yes, the db option is supported, and I just updated the doc for it. Thanks for pointing it out.\n. Hi vestly,\nHow did you use ioredis to connect to the AWS redis cluster? AWS redis cluster is not as same as the cluster bundled in Redis 3.x. The redis version that AWS provided is 2.6.x & 2.8.x, and failover is handled  on the AWS side. So basically you can only use new Redis to connect to the cluster node.\n. I found a related issue: https://github.com/redis/redis-rb/issues/543. IMO, the correct behaviour is to disconnect all client connections after a failover happens, which is as same as Redis Sentinel. However given the current implementation of AWS, I'd like to add a reconnectOnError option inspired by the issue above to make it possible to handle the failover.\n. Just submitted a pull request for this feature request: https://github.com/luin/ioredis/pull/148. Welcome to test it if you have time.\n. Released in 1.9.0 :rocket: \n. Hi amit777,\nioredis doesn't emit Redis connection error error. Counld you please turn on the debug logs (by DEBUG=ioredis:* node yourapp.js) to see what is going on?\n. I'm closing this since it seems not a ioredis specified issue. Feel free to reopen it with more information.\n. Never mind. Hope you can enjoy ioredis :laughing: \n. Is 2 the same as iojs-v2?\n. That's good to know. :laughing: \n. Just pushed\n. Yes, I'm going to implement it in the next version.\n. Just submitted a pull request for TLS support.\n. Released in 1.9.0 :rocket: \n. Namespace is already supported, check out https://github.com/luin/ioredis#transparent-key-prefixing\n. :laughing: \n. Awesome! I'll check it out this weekend.\n. It might be a little confused that setArgumentTransformer and setReplyTransformer affect all Redis instances, whereas custom commands are defined within a single instance.\nAs for me, it would be better to support custom commands transforming by adding a property in defineCommand method, for example:\n``` javascript\nredis.defineCommand('test', {\n  lua: 'return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}',\n  transform: {\n    argument: function (obj) {\n      return [obj.keys.length].concat(obj.keys).concat(obj.values);\n    }\n  }\n});\nredis.test({ keys: ['a'], values: ['b'] });\n```\nHowever I doubt whether it's necessary to support this feature, since there's a more straightforward way to do this:\n``` javascript\nredis.defineCommand('test', {\n  lua: 'return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}'\n});\nvar oriTest = redis.test;\nredis.test = function (obj) {\n  oriTest.apply(redis, [obj.keys.length].concat(obj.keys).concat(obj.values));\n};\nredis.test({ keys: ['a'], values: ['b'] });\n``\n. LGTM. Thanks for the pr! :laughing: \n. Hi @Adrien-P \nThanks for using ioredis. The redis servers running on the other machines returns 127.0.0.1 is because you bound the redis to the wrong network interface (in this case,bind 127.0.0.1`).\nYou can refer to this issue for more details: https://github.com/antirez/redis/pull/1590/files.\n. :laughing: \n. I agree with you that the debug output is a little misleading when using with sentinel. However, changing the options would cause ioredis to reconnect to the wrong node (connect to the master node directly without asking the sentinels first).\nI just did some checks and I think it might solve the problem to get the node addresses from the socket object instead of the options:\njavascript\nvar address = this.stream.address();\ndebug('status[%s]: %s -> %s', address.address + ':' + address.port, this.status || '[empty]', status);\nBut we should check if the socket object (aka. this.stream) existing before fetching its address.\n. It seems if conneting via unix socket/pipe (new Redis({ path: '/tmp/redis.sock' })), socket.remoteAddress would be undefined, leading to the wrong debug log.\nWe may check the path option first:\n``` javascript\nRedis.prototype.setStatus = function (status, arg) {\n  var address = this.options.path;\n  if (!address) {\n    var remoteHost = this.stream ? this.stream.remoteAddress : this.options.host;\n    var remotePort = this.stream ? this.stream.remotePort : this.options.port;\n    address = remoteHost + ':' + remotePort;\n  }\ndebug('status[%s]: %s -> %s', (address, this.status || '[empty]', status);\n  this.status = status;\n  process.nextTick(this.emit.bind(this, status, arg));\n};\n``\n. Command replies transforming is not supported in transaction (multi) currently. Since this feature breaks backwards compatibility, it will be implemented in 2.0.0\n. Yep!redis.set('key', 100, 'ex', 10). That would be great :-). ioredis just passes all arguments directly to the redis server\n. @wesleymilan You should run a redis server > 2.6.12 in order to supportSETcommand withEXoption. Refer to http://redis.io/commands/set for more information.\n. @aalexgabi do you mean adding a note aboutsetcommand withex` option is only available in redis > 2.8?\n. @aalexgabi I agree with you that the api documentation should be more informative. As noted in README, all arguments are passed directly to the redis server, so that ioredis supports every command / option that your redis server supports. Given people may run different versions of redis servers, the commands / options may differ, so it's a little hard to document them clearly.\nHowever, I think some examples do help, and it's also better to note how to pass options like EX so that people won't confused.\n. Cool! Could you please change ex to EX since the latter is used in the official documentation.\n. :laughing: \n. Great! Thank you for the patch!\n. The callback of expire will be invoked when the command is queued instead of executed  (so that the result of the callback would always be QUEUED). Or you can just set the timeout using the SET command:\n``` javascript\nredis.set(key, content, 'EX', timeout, function (err, res) {\n});\n```\n. \u54a6\uff1f\u5565\u95ee\u9898\uff1f\u4e3a\u5565\u9700\u8981 callback \u5462\n. \u662f\u60f3\u901a\u8fc7 callback \u83b7\u53d6\u5230\u952e\u8fc7\u671f\u7684\u4e8b\u4ef6\u5417\n. \u6240\u6709\u547d\u4ee4\u7684 callback \u503c\u90fd\u662f\u547d\u4ee4\u672c\u8eab\u7684\u7ed3\u679c\uff0c\u6ca1\u6709\u5bf9\u4efb\u4f55\u547d\u4ee4\u505a\u7279\u6b8a\u5316\u5904\u7406\uff0c\u8fd9\u6837\u6709\u5229\u4e8e\u4fdd\u8bc1\u8bed\u4e49\u7684\u7edf\u4e00\uff0c\u4e5f\u548c\u5176\u4ed6\u7684 redis client \u5bf9\u9f50\u3002\u53e6\u5916 Redis \u6ca1\u6cd5\u5b9e\u73b0 expire \u5230\u671f\u540e\u7684\u901a\u77e5\u63d0\u9192\uff0c\u6240\u4ee5\u8fd9\u4e2a\u7406\u8bba\u4e0a\u4e5f\u6ca1\u6cd5\u5b9e\u73b0\u3002\n. \u8fd9\u4e2a\u6ca1\u6cd5\u5b9e\u73b0\u5230\u671f\u63d0\u9192\uff0c\u53ea\u80fd\u5b9e\u73b0\u5220\u9664\u63d0\u9192\u3002\u6bd4\u5982 expire \u4e00\u4e2a\u952e 100 \u79d2\u8fc7\u671f\uff0cRedis \u4e0d\u4f1a\u6070\u597d\u5728\u7b2c\u4e00\u767e\u79d2\u5220\u9664\u8fd9\u4e2a\u952e\uff0c\u800c\u662f\u5728\u4e0b\u4e00\u6b21\u8bbf\u95ee\u8fd9\u4e2a\u952e\u6216\u8005\u540e\u53f0\u5783\u573e\u56de\u6536\u65f6\u624d\u4f1a\u771f\u6b63\u5220\u9664\u5b83\n. No. It will be executed immediately right after the redis handled the command.\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: damiano.barbati notifications@github.com\nSent: Tuesday, June 26, 2018 9:37:58 PM\nTo: luin/ioredis\nCc: \u5b50\u9a85; State change\nSubject: Re: [luin/ioredis] how expire key with callback (#163)\nHey guys, can you explain what's the final behaviour?\nredis.set(key, content, 'EX', timeout, function (err, res) {\n is this execute right when the key is EXPIRED?\n});\n\u2015\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/163#issuecomment-400311660, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_tzv3mUmDt7cAT69g0uYOSj5yRtUks5uAjk2gaJpZM4GFXO7.\n. Yes, we may emit an event when the password is wrong. May be auth error.\n. error event is a special event that would crash the app if there's no listener for it. I would go for another event name like auth error. :laughing: \n. The test fails because it expects ioredis would print a warning message if the server doesn't require a password: https://github.com/luin/ioredis/blob/master/test/functional/auth.js#L44. I'm thinking about whether we can remove this test.\n. Released with 1.9.1 :rocket: \n. It seems a little strange since ioredis doesn't emit error event if there's no listener for it. Which version of Kue (or which git branch) are you using? I'll check it out and see what I can do.\n. ECONNREFUSED error is caused by connecting to an unreachable address.\nIn your case, when the master is offline, the sentinel won't promote one of the slaves to the master immediately. Instead, sentinels will wait for several seconds before promoting (see the following config).\nSo that when the master is down, ioredis will ask the sentinels for the new master address, and still get the address of the offline master, which will cause the ECONNREFUSED error when ioredis connect to it.\nNormally when connecting failed, ioredis will emit an error event if there's at lease one listener to it and reconnect again. When used with kue however, kue would listen to the error event of ioredis and re-emit it, which will crash your application. The workaround is add a listener to the error event.\n```\nNumber of milliseconds the master (or any attached slave or sentinel) should\nbe unreachable (as in, not acceptable reply to PING, continuously, for the\nspecified period) in order to consider it in S_DOWN state (Subjectively\nDown).\n\nDefault is 30 seconds.\nsentinel down-after-milliseconds mymaster 30000\n``\n. Cool! :laughing: \n. You may refer to https://github.com/luin/ioredis/wiki/Migrating-from-node_redis\n. Hi @nopjmp, the result is correct since the keys in the redis isprefix:test1andprefix:test2instead oftest1andtest2. I don't think ioredis should strip away the prefix before returning since there are many other commands return key names (e.g.KEYS), strip away the prefix in only one command will make people confused.\n. Hi @Adrien-P,\nYou should only send readonly commands via the clients that havereadOnlyoption enabled. Maybe we should improve our documentation to precisely delineate this behaviour, or improve the code to check whether the command that being sent is a readonly command, and if not, we should send it to the master node directly.\n. Yes, I agree with you. I'd like to deprecate thereadOnlyoption in v2.0, and add a new optionscaleReads(since I'm not a native speaker, I'm open to suggestions for the name :laughing: ), which will automatically send reads to all nodes and writes only to the master.\n. @shaharmor v2 will be out within two months.\n. It's the top priority in my list, however I still haven't got time to implement it. A project of my company will switch to redis cluster soon (we have 60GB redis data) and we rely on ioredis heavily, so I'll definitely have this feature implemented before our migration.\n. Yes, I've considered this case and there will be an option for this.\n. Closing in favour of #247.\n. Doesn't tls stream also emitconnectevent?\n. Good to know:-). According to the [issue](https://github.com/nodejs/node-v0.x-archive/issues/6903), it seems thatsecureConnectis more preferable thanconnect. I just wrote a comment\n. Thank you for the patch! Merged :rocket: \n. Shipped in 1.10.0\n. Redis instance is extended from EventEmitter: https://nodejs.org/api/events.html#events_emitter_removealllisteners_event\n. Aha, good to know! I'll definitely try it out.\n. Commands that doesn't have a key name as the arguments(e.g.scan,keys&info) will be sent to a random node of the cluster. So that it doesn't make sense to providescanStreamin the cluster mode since it will only scan a random node instead of the whole cluster.\n. Yes, you're right. Other scan commands work with cluster. I'll consider whether to implement it.\nAny pr is welcome :-)\n. Released in v1.11.0 :rocket: \n. Awesome! :rocket: Thanks for the pr!\n. Shipped in 1.10.0\n. \u4f7f\u7528 buffer \u5462\uff0c\u628amget\u6362\u6210mgetBuffer`\n. \u662f\u6709\u8fd9\u4e2a\u56e0\u7d20\uff0c\u53e6\u4e00\u4e2a\u95ee\u9898\u5728\u4e8e\u73b0\u5728\u7684 parser \u7684\u8bbe\u8ba1\u5728\u5904\u7406\u5927\u6570\u636e\u7684\u65f6\u5019\uff0c\u5982\u679c\u4fe1\u606f\u4e0d\u5b8c\u6574\uff0c\u4f1a\u8fdb\u884c\u591a\u6b21\u56de\u6eaf\uff0c\u6240\u4ee5\u6027\u80fd\u4f1a\u6709\u95ee\u9898\n. 10~15ms \u662f\u4e0d\u6b63\u5e38\u7684\uff0c\u56e0\u4e3a\u6bcf\u6b21\u56de\u6eaf\u90fd\u4f1a\u963b\u585e\u4e8b\u4ef6\u5faa\u73af\uff0c\u6240\u4ee5\u624d\u5bfc\u81f4\u8fd9\u4e48\u6162\u3002\u6211\u4e4b\u524d\u505a\u4e86\u4e0b\u4fee\u6539\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8fd9\u4e2a\u95ee\u9898\uff0c\u4e0d\u8fc7\u5f53\u65f6\u89c9\u5f97\u8fd9\u4e2a\u4fee\u6539\u6027\u4ef7\u6bd4\u6bd4\u8f83\u4f4e\u5c31\u653e\u5f03\u4e86\uff1ahttps://github.com/luin/ioredis/issues/103\u3002\n\u8fd9\u6837\u770b\u6765\u786e\u5b9e\u9700\u8981\u6539\u8fdb\u4e00\u4e0b\uff0c\u6211\u627e\u65f6\u95f4\u6d4b\u8bd5\u4e00\u4e0b\n. @BridgeAR google translator is awesome!\n@ekousp \u5728\u9879\u76ee\u4e2d\u5b89\u88c5 hiredis \u5c31\u80fd\u81ea\u52a8\u4f7f\u7528\uff0c\u5728 README \u4e2d\u6709\u4ecb\u7ecd\uff1anpm install hiredis\n. @ekousp \u662f\u7684\uff0c\u56e0\u4e3a\u6570\u636e\u592a\u591a\uff0c\u6240\u4ee5\u5206\u6210\u4e86\u51e0\u6b21\u4f20\u7ed9 js parser\u3002\u56e0\u4e3a\u6bcf\u6b21\u90fd\u4e0d\u662f\u5b8c\u6574\u7684\u6570\u636e\u5305\uff0c\u6240\u4ee5 js parser \u6bcf\u6b21\u90fd\u4f1a\u91cd\u65b0\u7ec4\u88c5 buffer \u5e76\u6355\u83b7\u5f02\u5e38\uff0c\u4ece\u800c\u963b\u585e\u4e86 event loop\u3002\n. node \u7684\u5b57\u7b26\u4e32\u90fd\u662f utf8 \u7684\uff0c\u800c redis \u672c\u8eab\u6ca1\u6709\u5b57\u7b26\u4e32\u7684\u6982\u5ff5\u3002\u6240\u4ee5\u53ef\u4ee5\u68c0\u67e5\u4e0b\u539f\u59cb\u503c\u662f\u4e0d\u662f utf8 \u5b57\u7b26\u4e32\u3002\n``` javascript\nvar redis = new Redis();\nredis.set('\u952e', '\u503c');\nredis.get('\u952e', console.log); // \u503c\n```\n. \u6709\u95ee\u9898\u53ef\u4ee5\u76f4\u63a5\u53d1\u6211\u90ae\u4ef6\uff0cQQ \u7528\u7684\u6ca1\u6709\u90ae\u4ef6\u9891\u7e41\uff0c\u6240\u4ee5\u90ae\u4ef6\u56de\u590d\u4f1a\u5feb\u5f88\u591a\n. \u6587\u6863\uff1ahttp://redis.io/commands/set\n2.6.12 \u624d\u652f\u6301 ex \u9009\u9879\u3002\n. \u76f4\u63a5\u7528\njavascript\nredis.get(`${city}:${busname}:${id}:DriverId`)\n\u5c31\u597d\u4e86\uff0c\u6ca1\u5fc5\u8981\u9020\u8f6e\u5b50\u3002\n. https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/template_strings Node.js \u65b0\u7248\u53ef\u4ee5\u7528\n. Currently there's no support for connecting to a password-protected cluster. It's not straightforward to implement this feature since the nodes may have different passwords. Although passwords can be specified when you creating a Redis.Cluster instance (like your example), ioredis may auto-detect more nodes later that have different passwords.\nAccording to this topic: https://groups.google.com/d/msg/redis-db/Z8lMxTfDct8/Rny9BIK9xGYJ, it seems setting passwords to the cluster may not be a best practice, and redis-trib.rb also doesn't support it.\nWhat do you suggest given the above thoughts?\n. Hi, this feature is implemented and shipped in 1.10.0. Refer to https://github.com/luin/ioredis#password for details. Let me know whether this works for you :laughing: \n. Per-command timeout is not supported but under discussion. Refer to #61 for more information.\n. No client supports this feature as far as I know :-(. Dose timeout method provided by bluebird (redis.set('foo', 'bar').timeout(5000)) works for you?\n. Closing this in favour of #61.\n. :rocket: \n. People already have posted results with the newest node_redis version here: https://github.com/luin/ioredis/issues/25. In my computer (iMac 5k), there's not much difference comparing with the result shows in the readme when I use the newer node_redis (ioredis is still 50% faster). \nI noticed people got different results in their computers/servers. However, IMO the difference is trivial since both two libraries are very fast, and neither node_redis nor ioredis will become the bottleneck in most applications.\nAnyway I'll just remove the benchmark section from the README as most people shouldn't care about that, and if someone really does, they should run the benchmarks in their servers themselves.\n. ioredis use bluebird's Promise implemention instead of the native one for better performance and more features. Since the former is a superset of the latter, it's no problem to use ioredis with ES6 promises.\n. Yes, ioredis considers all undefineds as empty strings. What's your use case to explicitly pass an undefined?\n. That makes sense. However, some people rely on the fact that undefined and null will consider as empty string. For example, redis.set('user.name', user.name);, where user.name is undefined, sends SET user.name \"\" to redis. If we just pop all undefineds, the command will fail.\n. The arity defined in commands.js is not reliable since it may differ in different Redis versions (commands in newer version may receive more arguments, e.g. SET), and some commands' arity are even dynamic.\n. \u51fa\u73b0\u8fd9\u79cd\u95ee\u9898\u65f6\u5ba2\u6237\u7aef\u5230 Redis \u4e4b\u95f4\u6ca1\u6709\u8bbe\u7f6e\u5176\u4ed6\u4ee3\u7406\u4e86\u5417\uff1fRedis \u534f\u8bae\u5fc5\u987b\u4f9d\u8d56\u4e8e\u670d\u52a1\u5668\u80fd\u591f\u4e25\u683c\u6309\u7167\u8bf7\u6c42\u987a\u5e8f\u4f9d\u6b21\u8fd4\u56de\u5904\u7406\u7ed3\u679c\uff0c\u5426\u5219\u5c31\u4f1a\u51fa\u73b0\u4f60\u8bf4\u7684\u95ee\u9898\u3002\n\u5c31\u50cf\u4f60\u8bf4\u7684\uff0cioredis \u4f1a\u628a\u53d1\u51fa\u7684\u547d\u4ee4\u5b58\u50a8\u5728\u961f\u5217\u4e2d\uff0c\u6bcf\u5f53\u6536\u5230\u65b0\u56de\u590d\u65f6\u5c31\u4f1a\u5f39\u51fa\u547d\u4ee4\u8fdb\u884c\u5904\u7406\u3002\u8fd9\u4e2a\u961f\u5217\u672c\u8eab\u662f\u53ef\u9760\u7684\uff0c\u4e0d\u4f1a\u201c\u4e2d\u95f4\u4e22\u5931\u4e86\u4e00\u6bb5\u201d\u3002\u76ee\u524d ioredis \u8fd8\u6ca1\u6709\u6536\u5230\u8fc7\u76f4\u8fde Redis \u51fa\u73b0\u7c7b\u4f3c\u95ee\u9898\u7684\u53cd\u9988\uff0c\u4f60\u4eec\u7684 qps \u5927\u6982\u662f\u591a\u5c11\u5462\uff1f\u6362\u6210 hiredis \u89e3\u6790\u6a21\u5757\uff08npm install hiredis \u540e\uff0cioredis \u4f1a\u81ea\u52a8\u4f7f\u7528 hiredis\uff09\u4f1a\u6709\u7c7b\u4f3c\u7684\u95ee\u9898\u5417\uff1f\n. \u5bf9\u7684\uff0c\u6bcf\u6b21 write command \u90fd\u5bf9\u5e94\u7740\u4e00\u6b21\u5bf9 socket \u7684\u5199\u5165\uff08\u5373\u53d1\u9001\u4e00\u6761 Redis \u547d\u4ee4\uff09\u3002\u5982\u679c\u80fd\u5f97\u5230\u4e32\u6570\u636e\u524d\u7684\u65e5\u5fd7\u5c31\u6700\u597d\u4e86\uff0c\u53ef\u4ee5\u770b\u5230\u90a3\u65f6\u6709\u6ca1\u6709\u7279\u6b8a\u4e8b\u4ef6\u7684\u53d1\u751f\uff0c\u6bd4\u5982\u670d\u52a1\u5668\u6389\u7ebf\u4e4b\u7c7b\u7684\u3002\u53e6\u5916\u53ef\u4ee5\u8003\u8651\u518d\u5c1d\u8bd5\u4e00\u4e0b\u4f7f\u7528 hiredis\uff0c\u6211\u8fd9\u8fb9\u4f1a\u5c1d\u8bd5\u6784\u5efa\u4e00\u4e2a\u8d1f\u8f7d\u6d4b\u8bd5\u3002\n. \u55ef\uff0c\u5982\u679c\u51fa\u73b0\u8fd9\u79cd\u95ee\u9898\u5c31\u5e94\u8be5\u662f ioredis \u5185\u90e8\u5b58\u50a8\u7684\u547d\u4ee4\u987a\u5e8f\u548c\u6536\u5230\u7684\u7ed3\u679c\u987a\u5e8f\u4e0d\u5339\u914d\u9020\u6210\u7684\uff0c\u6240\u4ee5\u4e0d\u4f1a\u5f71\u54cd\u5199\u64cd\u4f5c\uff08\u4e5f\u5c31\u662f\u8bf4 Redis \u91cc\u9762\u5b58\u7684\u503c\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff09\u3002\u987a\u4fbf\u95ee\u4e00\u4e0b\u51fa\u95ee\u9898\u7684 ioredis \u7248\u672c\u662f\uff1f\u53e6\u5916\u6709\u6ca1\u6709\u4f7f\u7528 pub/sub \u6216\u8005 monitor \u547d\u4ee4\uff1f\n. \u770b redis.getAsync \u597d\u50cf\u7528\u7684\u662f node_redis \u800c\u4e0d\u662f ioredis\uff1f\u5efa\u8bae\u53ef\u4ee5\u7528 hiredis \u89c2\u5bdf\u4e00\u4e0b\u662f\u4e0d\u662f\u8fd8\u4f1a\u6709\u95ee\u9898\n. write command \u53ea\u6709\u5f53\u5199\u5165 socket \u65f6\u624d\u4f1a\u51fa\u73b0\uff0c\u6240\u4ee5\u4e00\u6761\u547d\u4ee4\u53ea\u4f1a\u8f93\u51fa\u4e00\u4e2a\u3002\u770b\u5230\u7684\u662f\u4e0d\u662f queue command\uff1f\n. \u770b\u8d77\u6765\u8fd9\u4e2a\u9891\u7387\u4e0d\u662f\u5f88\u9ad8\uff0c\u6700\u597d\u8fd8\u662f\u7528 hiredis \u89c2\u5bdf\u4e00\u4e0b\u8fd8\u4f1a\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u4e2a\u95ee\u9898\n. \u4e00\u4e2a command \u786e\u5b9e\u53ea\u4f1a\u6253\u5370\u4e00\u6b21\uff0c\u4f60\u53ef\u4ee5\u5728\u672c\u5730\u8bd5\u4e00\u4e0b\u3002\n. \u56e0\u4e3a\u6ca1\u6709\u518d\u6536\u5230\u5176\u4ed6\u540c\u6837\u7684\u95ee\u9898\uff0c\u6240\u4ee5\u5148\u5173\u95ed\u8fd9\u4e2a issue\uff0c\u5982\u679c\u4ecd\u7136\u9047\u5230\u95ee\u9898\uff0c\u6b22\u8fce reopen \u8fd9\u4e2a issue \u7ee7\u7eed\u8ba8\u8bba\u3002\n. \"connect\" event should be fired when connected. Doesn't the following code work for you?\njavascript\nvar redis = new Redis();\nredis.on('connect', function () {\n  // the callback will be invoked once connected\n  console.log('connect', arguments);\n});\n. I'm closing this issue. Feel free to reopen it with more information.\n. Closing this since it's a duplicate of #192.\n. Could you enable the debug info DEBUG=ioredis:* and post the logs here?\n. What about connect to one of the nodes (whether using redis-cli or new Redis()) and invoke CLUSTER slots to see what's returned.\n. Yes, that's correct. It requires binding every node to the public network interface to setup a cluster correctly.\n. It will be better to share a single redis connection among all users. I'm closing this since it's not a ioredis-specified question.\n. I tried on my computer, and only got a true. Could you please also log the value of err?\n. What ioredis version are you using? I remember it has already been fixed.\n. Could you please post the result of command cluster slots?\n. That's strange. It seems that redis guarantees cluster slots returns a nested array so that items should be an array. Could you print the result of cluster slots that ioredis gets when this error happens?\n. Just checked the source code of cluster slots command: https://github.com/antirez/redis/blob/unstable/src/cluster.c#L3737. Haven't find out why items is not an array. Anyway, let me know if the error happens again.\n@AVVS Are you able to reproduce the error (using the 1.7.6 version)?\n. Could you please post your code here that can reproduce the problem?\n. The readOnly option should be defined in the second parameter:\njavascript\nvar cluster = new Redis.Cluster(nodes, { readOnly: true });\nThis option means every command sent from the cluster instance is readonly so that ioredis will send them to both masters and slaves instead of just masters in order to scale reads and writes.\nI understand you intention that sending readonly commands to slaves and others to masters. This feature has been discussed in #170 and will be implemented in ioredis v2.0.\n. > The first issue I notice is even if I explicity disable command queues (enableOfflineQueue: false), this command will always be added to an internal queue so it is the first thing executed once a connection has been made.\nYes, as you said, this is an intentional behaviour.\n\n..., or even maintain a separate command queue for internal commands vs commands issued by the consumer.\n\nActually the internal command queue is independent of the normal command queue. I'd like to see a test case if possible.\n. I think the problem is ioredis doesn't handle the error. res will be undefined when error occurs. I'll handle it in next version. It is possible to log the error in your code so I can know why the command is failed? Thank you!\njavascript\nthis.opt.redis[this.opt.command](args, function (_, res) {\n  console.log('ScanStream receives an error', _);\n. This error will be emitted when the connection is down and enableOfflineQueue is false. I think the way to handle this is to use another Redis instance to do the scanning and enable the offline queue.\n. Released in 1.12.1 :laughing: \njavascript\nvar stream = redis.scanstream({ count: 1 });\nstream.on('data', function (data) {\n});\nstream.on('error', function (err) {\n});\n. Yes, it's a bug. Released in 1.11.1, now it will emit error event when all sentinels are unreachable. Thank you for pointing it out! :rocket: \n. @Gorokhov Yes, by default ioredis will keep reconnecting to Redis until the connection has been made. This behaviour is important when it's used in production, that even redis servers are down for several seconds, ioredis will still connect to them once they are online again.\n. @kunth sentinelRetryStrategy is only invoked when all the sentinel servers are not reachable.\n. @kunth and sentinelRetryStrategy is still not invoked? Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?\n. That helps. It seems sentinelRetryStrategy is invoked since in your case sentinelRetryStrategy always returns 1000 and the logs says Retrying from scratch after 1000. Have you tried adding a console.log to the sentinelRetryStrategy function?\n. ioredis only emits error event when sentinelRetryStrategy returns a non-number value. end and error event means the connection is failed and you should  manually restart a new connection.\n. Yes, that's correct.\n. > How does the library gets notified when a master goes down?\nioredis considers a node is down when receiving a TCP end event. What's more, if ioredis receives an specified error (e.g. CLUSTERDOWN) when sending a command, it will know the cluster is down.\n\nWhat about requests that were already sent to the dead master?\n\nioredis will retry them when cluster is online again. There are several options to adjust this behaviour.\n\nGiven that the library knows a master has failed, what will happened until the slave takes over?\nWill the requests be queued? failed? retried?\n\nAgain, this requests are queued and will be retried when the new master is online.\n\nIn a situation that the slave fails to take over and there are slots not allocated, what happens to requests targeted for those slots?\nWhat happens if the master never comes back? Will the library just get filled with callbacks waiting for it to come back?\n\nSee retryDelayOnClusterDown option.\n. Thanks for the pull request! I'll take a look at it these days. Does it solves the 100% CPU issue?\n. Released in 1.12.1 :laughing: \n. I'm writing some tests for it, and I'll release a version once done\n. Of course will do! Actually I regenerate the contributors every time I release a version. :laughing: \n. Just a reference here: https://github.com/antirez/redis/blob/cbcffed90792ef83bcba3bfcbbd5e64380d1b0b9/src/cluster.c#L5165. Seems we should send asking command before retrying each failed command.\n. Closing in favour of #264.\n. Fixed in 1.12.2\n. Could you enable the debug mode and post the logs here (DEBUG=ioredis:* node yourapp.js)? If you're using version 1.12.1, upgrade to 1.12.2 may fix the issue.\n. I'll setup a test environment on my computer ASAP.\n. Thank you for the details.\nFirst of all, the option enableOfflineQueue specified in the second parameter is used in the cluster internally instead of passing to the Redis constructor directly.\nSecond, retryStrategy is disabled in the cluster mode since ioredis will reconnect to the offline node automatically.\nThe problem you run into is because ioredis is keeping trying to fetch the result from the offline node until getting a response. The callback will be fulfilled when the node is online again. Unfortunately there's not an option to control the retry times, which is been working on in the 2.x branch and will be released within two months.\n. That's correct. In the next major version, the option enableOfflineQueue will be renamed to clusterEnableOfflineQueue for the sake of unambiguous. The current option maxRedirections will be used not only for MOVED and ASK errors, but also for other errors that may cause redirections, so that it's possible to reject the command after several failures.\n. Options updated in 2.0. Closing this issue.\n. Why don't you just use the index to detect which command the response belongs to?\n. Hmm...I see. I don't think there's a general way to handle this easily. The syntax coms.push( [ 'zadd', ukey, data.stamp, insrKey, 'THIS-IS-UPDATE-COM' ] ).\nexec() breaks backward compatibility.\n. Why do you disable pipelining of multi?\n. Pipelining makes sure that all commands are sent together to the redis server. In your case:\n``` javascript\nfunction removeIndex(key, index) {\n  redis.multi().lset(key, index, 'deleted').lrem(key, 0,  'deleted').exec().then().catch();\n  // Or\n  // var multi = redis.multi();\n  // multi.lset(key, index, 'deleted');\n  // multi.lrem(key, 0, 'deleted');\n  // multi.exec().then().catch();\n}\n```\nAn alternative (and better) way to do this is using lua scripts instead:\n``` javascript\nredis.defineCommand('lremindex', {\n  numberOfKeys: 1,\n  lua: 'local FLAG = \"deleted\" redis.call(\"lset\", KEYS[1], ARGV[1], FLAG) redis.call(\"lrem\", KEYS[1], 1, FLAG)'\n  });\nfunction removeIndex(key, index) {\n  redis.lremindex(key, index).then().catch();\n}\n``\n. You may refer to the documentation: https://github.com/luin/ioredis#transaction-and-pipeline-in-cluster-mode. It's required to enablepipelinein Cluster mode so that ioredis will send all commands in a multi group to the same node.\n. Could you upgrade to 1.12.2 and test again? This issue has been fixed already.\n. That's strange. Does enabling debug infoDEBUG=ioredis:* node yourapphelp?\n. I'm closing this issue since ioredis v2 is released and there're many code changed. Reopen it if the error still exists.\n. +1 for theclosemethod. However, I doubt whether theisClosedis necessary here since it seems not to be a standard method ofStream.Readable`. You may store the status of the stream on the client side:\n``` javascript\nvar streamIsClosed;\nreq.once(\"close\", function() {\n    stream.close(); // begin connection draining\n    streamIsClosed = true;\n});\nstream.on(\"data\", function(keys) {\n    if (streamIsClosed) {\n        return; // connection draining\n    }\n    // do work here, or pipe to another stream\n});\n``\n. Doesn'tredis.spop(\"myset\", 3)work?\n. You're running a redis server that doesn't supportcount` argument. Here's a notice from the redis doc:\n\nThe count argument will be available in a later version and is not available in 2.6, 2.8, 3.0\n\nhttp://redis.io/commands/spop\n. The easiest way to find out the problem is to enable the debug info (DEBUG=ioredis node yourapp) if the problem is reproducible.\n. Are you able to reproduce this problem? What do you get if you use redis-cli (or telnet) to connect to the redis directly?\n. Are you spoping a hash key? It looks like json is a hash instead of set. However, it doesn't seem to be a ioredis-specified problem since you got  the same result using redis-cli.\n. Aha! What about enable the debug mode (DEBUG=ioredis:* node yourapp) and see the logs? What's more, running MONITOR command in the redis-cli so you can know what commands are sent to the redis server.\n. Fixed in 1.13.1. Thank you for pointing out!\n. There's no argument transformer for msetnx yet. The built-in ones can be found here: https://github.com/luin/ioredis/blob/master/lib/command.js#L302-L335, and you can define one yourself for msetnx.\nHowever, I think we should have a built-in transformer for msetnx :laughing: .\n. Yes, you're right. Didn't notice this command when I implemented transformers.\n. ioredis emits Failed to refresh slots cache when none of the nodes can be connected to. You may listen to the node error event for the detail information or just check the lastNodeError property of the err:\njavascript\ndb.on('error',function(err){\n    console.log(\"REDIS CONNECT error \"+ err);\n    console.log('node error', err.lastNodeError);\n});\n. ReplyError: ERR unknown command 'cluster is because the redis server connected to doesn't support cluster (Redis supports cluster after 3.0).\nConnection is closed is because the server is not reachable.\nWhat about using redis-cli to connect to one of the nodes (e.g. xxx.us-west-2.compute.amazonaws.com:30002), invoke cluster slots and see what you get.\n. Can you run the following code with debug mode enabled (DEBUG=ioredis:* node yourapp.js):\n``` javascript\nvar Redis = require('ioredis');\nvar redis = new Redis.Cluster([{\n  port: 30001\n}]);\nredis.on('error',function (err) {\n    console.log(\"REDIS CONNECT error \"+ err);\n    console.log('node error', err.lastNodeError);\n});\n```\nand post the logs here?\n. It's because the result is returned by redis, and you'll get the same result when using redis-cli.\n. Are originRedisClient and targetRedisClient connected to the redis servers with different versions? I tried with two server with the same version and it worked:\n``` javascript\nvar redis = new Redis();\nvar redis2 = new Redis(6909);\nvar key = 'foo';\nredis.dumpBuffer(key, function () {\n  redis2.restore(key, 0, value, function (err, res) {\n    console.log(err, res); // err is null and res is 'OK'\n  });\n});\n``\n. Technically the result of yourgetcommand within a transaction isQUEUEDinstead of0x00ff44(which is the result ofexeccommand). So if you pass a callback to thegetBuffercommand, you will get a buffer, which value isQUEUED`:\njavascript\nredis.multi().getBuffer('foo', function (_, res) {\n  console.log(res.toString()); // res is a buffer, value is 'QUEUED'\n}).exec();\nI just released a new version to supports execBuffer for transactions:\njavascript\nredis.multi().get('foo').execBuffer();\nIt's a little misleading of course since the interface between transaction and pipelining are similar. I'm planning to rewrite this part to make client.multi().getBuffer('test').exec() returns a buffer and deprecate execBuffer. However I'll do it in the next major version since it breaks BC.\nLet me know if anyone has any idea of it.\n. redis is just an instance of the built-in EventEmitter module. You can refer to the official documentation here: https://nodejs.org/dist/latest-v4.x/docs/api/events.html#events_emitter_removelistener_event_listener\n. ready fires right after fetching the slot information of the cluster by sending CLUSTER SLOTS to one of the nodes in the cluster, so that ready means there's at least one node is accessible and ioredis has already known where the other nodes are.\nConnection is closed is because all nodes are down and clusterRetryStrategy is disabled.\nHowever, it's strange to get ready event when you put in a couple random hosts that don't exist. Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?\n. That's correct. CLUSTER SLOTS command returns successfully even if the cluster in the CLUSTER DOWN state. In that case, when emits ready, ioredis has already connected to a node and fetch the information of the cluster successfully.\n. yeah, you're right. ready event is a little misleading here. I'm going to make some changes on this in 2.0.\n. Closing this issue in favour of #261\n. duplicate of #158. Will be fixed in the next major version :rocket: \n. I'd like to, but I don't use TypeScript. Pull request is welcome!\n. Related issue: https://github.com/DefinitelyTyped/DefinitelyTyped/pull/8910\n. Close this pr in favour of #245.\n. Happy new year!\n. The callback of subscribe command is invoked when all channels are subscribed successfully. ioredis won't emit subscribe event, so your code seems to be a solution.\n. Yes, that makes sense.\n. A little busy these days, will get time to implement it this weekend. Pull request is welcome before that.\n. Sorry for my late response. It's really strange since this error only emits when the enableOfflineQueue is false: https://github.com/luin/ioredis/blob/master/lib/cluster.js#L479-L486.\nWhat do you mean by the library didn't finish loading? Are you sending commands before setting cluster.options.enableOfflineQueue = true?\n. I don't think it's possible since the options is defined in the constructor of Cluster (https://github.com/luin/ioredis/blob/master/lib/cluster.js#L64).\n. Well that's because ioredis flattens the parameters of the command, so that the following two are equal:\njavascript\nclient.set('redis-array', []);\nclient.set('redis-array');\nAnd so do the following two:\njavascript\nclient.sadd('set', ['a', 'b', 'c']);\nclient.sadd('set', 'a', 'b', 'c');\nIt's a documented feature: https://github.com/luin/ioredis#basic-usage and I've pointed this out on https://github.com/luin/ioredis/wiki/Migrating-from-node_redis.\nThank you for letting me know. \n. Great work!\n. Awesome! Fix the typeof part that @BridgeAR pointed out and the pr will be good to merge :laughing: . Serval test cases for this pr will be cool.\n. I'll write the test cases for it. Merged :rocket: \n. What did you get in the callback?\n. The sentinel command is only available on the sentinel server instead of the normal Redis server.\nBy supplying sentinels options, you're asking ioredis to connect to a normal Redis server with the info that got from the sentinel servers:\njavascript\nvar redis = new Redis({\n    sentinels: [{ host: 'x.x.x.x', port: 26379 }, { host: 'x.x.x.x', port: 26379 }],\n    name: 'r1'\n});\nIf you want to talk with sentinel servers, just connect to the sentinel servers directly:\njavascript\nvar redis = new Redis({\n  host: 'x.x.x.x',\n  port: 26379\n});\n. Nice catch :rocket: \n. Actually it's not a bug. ioredis relies on the COMMAND command of Redis to get the position of the keys in a command's parameters. For keys (as well as scan) command, Redis tells ioredis that it doesn't contain a key, so that ioredis won't prefix it.\n. @Volox I agree with you that it is of course a little inconvenience for people who want to use the keyPrefix option as namespaces in order to have multiple applications share a single redis instance.\nHowever, keyPrefix literally means prefix keys with a string, but the parameter of KEY command is  a pattern instead of a key name, so that it won't be prefixed.\nIt's not hard to make the pattern also be prefixed, but I don't want to let ioredis do too much magic. There are other commands that should be prefixed if we handle the KEYS command, for instance, SCAN.\nI don't want to mislead people into thinking they can reply on the keyPrefix as namespaces. For example, FLUSHDB actually deletes all the keys in the redis instead of the keys whose name started with keyPrefix.\nThe workaround is to create another instance for KEYS and SCAN command or just create a wrapper yourself.\n. ioredis enables tcp keep alive by default, so that dead connections should be detected after a while.\n. You may config the TCP_KEEPINTVL and TCP_KEEPCNT options at sysctl level to make system detect dead connection more quick. Or you can send ping command periodically and reconnect the connection when the command timed out.\nThe following code sends ping every 500ms and will force ioredis to reconnect if not receiving a response after one second. I haven't tested the code but it should work.\njavascript\nvar redis = new Redis();\nsetInterval(function () {\n  redis.ping().timeout(1000).catch(function () {\n    redis.disconnect(true); // true indicates we need reconnect\n  });\n}, 500);\n. @BridgeAR What do you think about adding a built-in heartbeat support similar to the above code? It seems to be a common requirement given Node.js doesn't support specifying TCP_KEEPINTVL and TCP_KEEPCNT per socket (https://github.com/nodejs/node/issues/4048).\n. Just released v1.15.0 :rocket:\n. What's node.js  version are you using?\n. It seems to be a bug of libuv with windows, so I don't think we can do anything about that.\n. Hi @lejoix, thank you for using ioredis.\nIt's not possible to catch a error in an async callback:\njavascript\nvar redis = new Redis();\ntry {\n  redis.get('foo', function () {\n    throw new Error('sync error here');\n  });\n} catch (e) {\n  // The error won't be caught, instead it will crash the process.\n}\nI don't think there's any difference between node_redis and ioredis here, or are you using the domain module or something similar?\n. Could you use the following code to reproduce the problem?\n``` javascript\nvar connect = require('connect');\nvar connectDomain = require('connect-domain');\nvar Redis = require('ioredis');\nvar redis = new Redis();\nvar app = connect()\n    .use(connectDomain())\n    .use(function(req, res){\n      redis.get('foo', function () {\n        throw new Error('hi');\n      });\n    })\n    .use(function(err, req, res, next) {\n        res.end(err.message);\n    });\napp.listen(3001);\n```\nI got \"hi\" when running with the latest version of ioredis and node v0.10.14\nioredis doesn't bind the callback to the current active domain explicitly since bluebird (a dependency of ioredis) should handle the binding.\n. Hmm...That's strange. Domain is hard to debug, and IMO that's why it's deprecated for a while. \ud83d\ude04\nLet me know if I can help anyway. \n. As for me, Promise is good enough, especially when working with async/await (or generator and co module). Refer to the generator and ES7 async/await part: https://strongloop.com/strongblog/async-error-handling-expressjs-es7-promises-generators/.\nHere's an example:\njavascript\nconst Redis = require('ioredis');\nconst redis = new Redis();\nfunction *() {\n  try {\n    const bar = redis.get('foo');\n    if (bar !== 'bar') {\n      throw new Error('bar is not bar');\n    }\n  } catch (err) {\n    // handle errors here\n  }\n}\n. @shaharmor ioredis uses key ${ip}:${port} to differentiate between the different connections. So using a  host string instead of an IP will create two connections for each node. That will definitely use more memory (twice maybe) but should not cause memory leak.\nSeems we can resolve the host to the IP manually to solve the problem.\n. All connections not included in the result of CLUSTER SLOTS will be disconnected. So it's normal that the connections using hostnames are lost since CLUSTER SLOTS only returns ips.\nCould you give me a reproducible example for the memory leak?\n. I'll do some tests these days.\n. How does Map benefit us despite of semantic?\nIs there any chance V8 gc not getting involved. What about calling gc explicitly before printing memory usage (https://simonmcmanus.wordpress.com/2013/01/03/forcing-garbage-collection-with-node-js-and-v8/)?\n. The heapdump seems not telling much information at first sight. Will look into it this weekend.\n. I ran @shaharmor's gist on my computer and the memory usage report is very similar to v1.x. I didn't notice any sign of memory leak.\nHere's my result: https://gist.github.com/luin/674a876791da9d35def3\n. My node version is 4.2.1.\nioredis git:2.x \u276f redis-cli -p 6380                                                      \u23ce \u272d\n127.0.0.1:6380> cluster nodes\n0a9570f08b951255ea1d753eb13552eaef34b1a5 127.0.0.1:6383 slave 1b781975c7bb4a5cdee0879587fe28ed04ca3a15 0 1455956006170 4 connected\n311d5c617cd24d48719238071cca4a0aba62df69 127.0.0.1:6381 master - 0 1455956005125 2 connected 5461-10922\ne25c7a0336d08750f13390929c986f55cefe3fc2 127.0.0.1:6384 slave 311d5c617cd24d48719238071cca4a0aba62df69 0 1455956008237 5 connected\n8a82bad90c4c0de3e93bf6a5cbce31b635cd2d57 127.0.0.1:6385 slave 34d65132841ed3fda45d6fd79c7ba4f43292b741 0 1455956007206 6 connected\n34d65132841ed3fda45d6fd79c7ba4f43292b741 127.0.0.1:6382 master - 0 1455956002014 3 connected 10923-16383\n1b781975c7bb4a5cdee0879587fe28ed04ca3a15 127.0.0.1:6380 myself,master - 0 0 1 connected 0-5460\n. @shaharmor Could you enable the debug mode DEBUG=ioredis:* yourapp.js to see whether the commands are added into the offlineQueue or clusterDownQueue?\n. I saw the leaking when running the docker in an Ubuntu (kernel 4.4.0). Digging into the problem...\n. @shaharmor Still not have any substantive progress. If I call global.gc() manually every 500ms, the leaking would not be obvious (which is similar to the master branch). However, if not calling global.gc(), 2.x branch costs a lot more memory than master branch. Very strange.\n. @ramonsnir @shaharmor @AVVS Finally found the \"leaking\": https://github.com/luin/ioredis/commit/410af511b7796ee3f2c656c28a8a1842c6b0f23d. :beers: \n. @shaharmor Just released v2.0.0-alpha2. I'll finish the other todos asap.\n. @ScheerMT \n\nCurious on how that was causing a leak...Was it because it was in an enclosing scope and the function inside of it was referencing it?\n\nNot sure about that. It seems to be a problem that _.partial creates many closures that never used.\n\nThere was talk about this project merging with another redis package at one point i believe... Did that fall through and you are proceeding with a v2 release seperate of that?\n\nIt's not easy to merge node_redis and ioredis into a single project. However we are trying to create some common modules which can be used by both them. For example, redis-parser and redis-commands are the two modules that integrated into ioredis in this pull request.\n. All todos are finished. I merged 2.x branch into master and the old master branch are renamed to 1.x. v2.0.0-alpha3 is released.\n. Never see this problem before. Is the problem reproducible? If it is, enable the debug mode (DEBUG=ioredis:* node yourapp.js) should give us more detailed information.\n. It seems there are two connections, one is \":3001\" and the other is \"127.0.0.1:3001\". It's strange that the first connection didn't have the host part. Did you provide a host with an empty string or null?\nsismember is sent via the \":3001\" connection, and the following logs are the part related with \":3001\":\nioredis:redis status[:3001]: [empty] -> connecting +2s\nioredis:redis status[:3001]: [empty] -> connecting +2ms\nioredis:redis queue command[0] -> sismember(all_playlist_slugs:123456789,asdasd) +0ms\nioredis:redis status[127.0.0.1:3001]: connecting -> connect +1ms\nioredis:redis write command[0] -> info() +0ms\nsismember command will only be sent after the status going to \"ready\" ([empty] ----> connection ----> connect --info--> ready). So what the following several logs after ioredis:redis write command[0] -> info() +0ms you got? They should be similar to:\nioredis:redis write command[0] -> info() +0ms\nioredis:redis status[:3001]: connect -> ready +1ms\nioredis:connection send 1 commands in offline queue +0ms\nioredis:redis write command[0] -> sismember(all_playlist_slugs:123456789,asdasd) +0ms\n. Hmm... Seems that the log doesn't tell anything. A minimal reproducible example would be more helpful. Does the following code is able to reproduce the problem?\njavascript\nvar Redis = require('ioredis');\nvar redis = new Redis(50440);\nredis.sismember('all_playlist_ids', '59855178', console.log.bind(console, 'got result'));\n. Closed due to inactive status. Feel free to reopen it if the problem still exists.\n. @dophlin What problem did you get? Could you give me a minimal reproducible example?\n. Sounds interesting. Could you give me an example about the custom function case?\nP.S. It seems that redis always consider EVAL and EVALSHA as writes, so the slaves will not handle them (https://github.com/antirez/redis/blob/unstable/src/cluster.c#L5178-L5184).\n. That makes sense. What about a function with two parameters slaves and command? slaves is an array of the slaves serving the slot. Return an index of the array to send the command to the specified slave, or return a false value to send to master.\njavascript\nnew Redis.Cluster([/* nodes */], {\n  scaleReads: function (slaves, command) {\n    if (command.name === 'get') {\n      return 0;\n    }\n  }\n});\n. Closing this issue in favour of #252.\n. Awesome! Thank you for the pull request!\nIt seems better to change function(slaves, command) {} to function(nodes, command) {} where the first  element of nodes is the master node and the reset are slaves so that it's possible to send the queries to masters.\n. LGTM. Any thoughts about changing function(slaves, command) {} to function(nodes, command) {} so that we don't need to call slice(1) everytime.\n. Cool! Merged. :+1: \n. Check out redis.stream after connect event:\njavascript\nvar redis = new Redis();\nredis.on('connect', function () {\n  console.log(redis.stream.remoteAddress);\n  console.log(redis.stream.remoteFamily);\n  console.log(redis.stream.remotePort);\n});\n. Duplicate of #239.\nThe first argument of KEYS command is a pattern rather than key name, so we don't prefix it with keyPrefix.\n. Thank you for the feedback! This issue is related with db option instead of sentinel. The following code can reproduce the error:\njavascript\nvar redis = new Redis({ db: 10 });\nredis.on('connect', function () {\n  redis.subscribe('anychannel');\n});\nThe error is because ioredis sends the SELECT command to switch to db 10 on the ready event. In most cases that would not cause any problem because commands sent after the connect event and before the ready event will be added to the offline queue and will be resent after the ready event.\nHowever, pub/sub commands (SUBSCRIBE, PSUBSCRIBE and UNSUBSCRIBE etc) are special that ioredis allows them to be sent to Redis before the ready event. So when we sends SELECT command on the ready event, the connection has been in the subscribe mode and Redis will reject the SELECT command.\n. Release in 1.15.1 :rocket: \n. @seunlanlege This issue has been resolved, so there may be another issue. Could you enable debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?. @seunlanlege What's the constructor argument of Redis? And what's the version of Redis server? Please enable the showFriendlyErrorStack option to see which command cause the exception (new Redis({showFriendlyErrorStack: true})).. (\u310f\uffe3\u25bd\uffe3)\u310f   \u311f(\uffe3\u25bd\uffe3\u311f)\n. Did you unsubscribe the previous channels?\n. authError event is only emitted when the password provided is wrong. If the password is empty, ioredis won't send the AUTH command to the Redis, so that there's no chance can ioredis know the password is wrong.\nHowever, if you send any command manually, you'll get the NOAUTH error returned by Redis server.\n. @mkozjak That makes sense. I added it to the 2.x list. Thank you for pointing this out!\n. Closing this issue in favour of #246.\n. I fixed the description in the 2.x branch several days ago. Forgot to merge it back to master branch. Thank you for the pull request!\n. I'm not familiar with newrelic. But I think the problem may on the newrelic side.\n. Cool. Let me know if there's any update or anything I can help.\n. Great work!\n. \ud83c\udf89\n. It's merged to the 2.x branch (#246). I'll release a new 2.0.0-alpha version soon.\n. WATCH command itself is supported. However users should handle retries themselves when transaction failed.\n. It seems you didn't provided a password. The option name for password is password in ioredis instead of auth_pass. However for Medis, since the option name is correct, it's strange that your connection is rejected. Could you show me the error message you got from Medis?\n. The password option should be passed at the same level as host and port:\n``` javascript\n'use strict';\nvar Redis = require('ioredis');\nvar redis = new Redis({\n  host: 'scout-staging.redis.cache.windows.net',\n  port: 6379,\n  password: 'CUV8YV3X6Pl9qGd6ICbywk0rKVMrxPjQTO8wdhieIh8='\n});\nredis.info(console.log);\n``\n. :laughing: \n. Yes, that's correct. Redis servers return an empty list instead ofnullwhenhgetall` a non-existsed key, so in ioredis, an empty object is returned. The related code is here: https://github.com/luin/ioredis/blob/master/lib/command.js#L286-L295.\nI added this into the wiki. Thank you for pointing this out!\n. Every command (including del) accepts a callback. Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?\n. Oh, my bad. I thought most people will use the javascript parser so I used it as default. After looking the changes on redis-parser, seems setting the default parser option to null is same to \"auto\" in the previous versions of redis-parser.\n. @BridgeAR Aha, never mind. I didn't read the changed carefully (\"auto\" is indeed a tricky default value :laughing: ).\nI opened a pull request #272 for this.\n@shaharmor I also added you as a collaborator.\n. @shaharmor Not much, just submit a pull request for every bugfix and feature so that we don't need to revert a commit on the master branch like me. :joy: \n. :clap: \n. It looks correct. You can enable the debug mode to see what happened (DEBUG=ioredis:* yourapp.js).\n. 1. client side heartheat is useful since TCP keepalive options supported by Node.js is not flexible enough.\n2. redis.disconnect(true) will reconnect to the Redis (https://github.com/luin/ioredis/blob/master/lib/redis.js#L302).\n. I looked into your modification, but I don't see where you reuse the original stream and reconnect again since Connector#reconnect doesn't call Connector#connect. Should process.nextTick be this.connect in Connector#reconnect?\n. I'm wondering whether just calling redis.stream.destroy() instead of adding two more functions does the trick, since if the socket is destroyed, a reconnection will be triggered automatically.\n. Sorry I didn't notice this issue. Could you enable the debug mode (DEBUG=ioredis:* node yourapp.js) to see the logs? What ioredis version are you using?\n. Thank you for the details. It seems that hosts uco-dev*** are resolved to internal ips 10.149.*.*. Did you create the cluster with the hosts rather than the ips? What about using IP address instead?\n. Yes, it might be a bug if the code works with v1.15.1. I'll look into it this weekend.\n. @devnetf This issue should be fixed in v2.0.0-rc2. Feel free to reopen it if the issue still exists. Thank you for the feedback!\n. It's indeed a interesting command...To support it, maybe we can add a flag to the Redis#condition variable to indicate the status of client reply. If the status is not ON, then we just don't add the command to the command queue.\n. Every command is asynchronous. However, since redis uses a single thread model and all commands are sent one by one, when get command is invoked, set command should already be executed.\nSo that in most cases, the following code will always prints \"bar\":\njavascript\nredis.set('foo', 'bar');\nredis.get('foo', console.log);\n. I haven't tried redis-sentinel v3.2. However, the Redis release notes says \"Redis 3.0 is mostly a strict subset of 3.\", so it should not have any trouble connecting to sentinels.\nCould you run your app in the debug mode (DEBUG=ioredis:* node yourapp.js) to see the detail logs?\n. Aha, you're right. Protected mode is a new feature in Redis 3.2. I noticed it on https://www.reddit.com/r/redis/comments/3zv85m/new_security_feature_redis_protected_mode/ three months ago. We should definitely handle this error (by printing a warning message maybe) on the library.\n. @zeroone001 The sentinel command is only supported by Redis Sentinel.\n. @Deathle55 protected-mode is very useful to protect your sentinel servers. You may avoid the errors by using bind directive to bind the redis/sentinel to only several specified interfaces.\n. You should listen to the error event instead:\njavascript\nredis.on('error', function () {});\n. onPossiblyUnhandledRejection is only used to handle uncaught command error (e.g. redis.set('foo')). Redis class itself is an instance of EventEmitter, the connection errors will be emitted in error event.\n. I haven't used Twemproxy, but if it doesn't support client command, that's the problem.\n. I'm closing this issue. Feel free to reopen it if you think the issue is still valid.\n. This issue has been discussed before (https://github.com/luin/ioredis/issues/61). I tried to implement max retries option several months ago but it was a little difficult. I'll revisit this issue again since I agree that command timeout or retry limiting is pretty useful.\nI'm closing this issue in favour of #61.\n. It shouldn't be a problem to include the whole lodash module because the package size isn't sensitive for the backend side IMO.\nioredis uses about 10 methods of lodash. We would have too many dependencies if we have each lodash method as a single dependency.\n. Currently you can use connectTimeout option to specify a timeout for connection, and ioredis will reconnect to the server when reaching timeout.\nHowever no error will be emitted in case of timeout. I'll fix this soon.\n. redis.disconnect() should do the trick. Related issue here: https://github.com/NodeRedis/node_redis/issues/959\n. Could you please enable debug mode (DEBUG=ioredis:* node yourapp.js) and put the logs here?\n. Hi, sorry for the late response.\n1. auth error will be emitted as \"authError\" event instead of \"error\" event.\n2. It's hard to tell if a sentinel works correctly on the client side.\n3. You may change the password or just disconnect from the Redis server in the \"authError\" event.\n. 1. I just checked the code, and it would be strange if the wrong password error is emitted with error event. Related code: https://github.com/luin/ioredis/blob/4e4f5e2628c63c68556c12b8f7d4e91008407de4/lib/redis/event_handler.js#L17\n2. It's possible by connecting to the sentinel servers directly and subscribing related events: http://redis.io/topics/sentinel#pubsub-messages.\n3. You may disconnect in the handler of the authError event. Here's an example:\n``` javascript\nvar redis = new Redis({ password: 'wrongpassword' });\nredis.on('authError', function () {\n  // Set to the correct password\n  redis.options.password = 'test';\n// and reconnect\n  redis.disconnect(true);\n});\nredis.get('foo', console.log); // Will be rejected with auth error\nsetTimeout(function () {\n  redis.get('foo', console.log);\n}, 1000);\n```\n\nin this case the error is an authError however connection is OK hence ioredis just queues the commands and never tell you that you will never be able to get/set since the authentication failed?\n\nI agree with you that it's a little strange, may be we should just reject all commands if the password is wrong.\n. The latest pre-release version v2.0.0-rc4 will emit error of wrong password with \"error\" event. All commands will be rejected with the same error and a reconnection will be made.\n. I'm closing this issue since ioredis v2 is released. Feel free to reopen it if you think problem still exists.\n. ENETUNREACH means the network was unreachable. In your case, the client is able to connect to 192.168.185.195: 26379 but not to 192.168.180.0:6379. It should have nothing to do with whether the debug mode is enabled.\n. Yes, ioredis won't emit error if there's no listener attached (https://github.com/luin/ioredis/blob/master/lib/redis/event_handler.js#L104). It's strange the error crashes your app. Could you post the code that can reproduce the problem?\n. Closing this issue. Feel free to reopen it with more details if the issue still valid.\n. I considered this feature when I wrote the first version of ioredis. I have to say it's not easy to implement a streaming parser for Redis protocol. Given that Redis is a in-memory database, typically it won't hold large data per key, so I won't consider it as a high priority feature. However, any pull requests for this feature are welcomed \ud83d\ude06 .\n. Nice catch. However, maybe we could just set dropBufferSupport to true since we don't need any binary command.\n. Just change\njavascript\nconnectTimeout: this.options.connectTimeout,\ndropBufferSupport: this.options.dropBufferSupport\nto\njavascript\nconnectTimeout: this.options.connectTimeout,\ndropBufferSupport: true\nwill be OK since the client created in sentinel_connector.js is used temporarily and will be destroyed when sentinels are connected.\n. LGTM. Could you please change the double quotes to single quotes?\n. Syntax errors (wrong command name or argument count) will just abort the transaction and cause the whole transaction being rejected.\nPartially applied transactions are only caused by runtime errors, as you said, applying commands on wrong type keys. In these cases, we should provide an easier way to let people handle the results and errors. The current format [[err1, val1], [err2, val2]] will make things clearer on which commands are failed and which commands are success. What's more, this format will force developers to handle the errors explicitly. However I'm not sure if there's any better way to do this.\nThe way node_redis handle the transaction is also feasible to me (returning replies following the format [res1, err2, res3]). It would be easier for developers to handle the results, but people have to call if (results[1] instanceof Error) {} to check whether the result is an error. I afraid this will lead many developers forgetting handle the errors.\n. I agree with you that [[err1, res1], [err2, res2]] is non-intuitive. @BridgeAR What do you think about this issue. Currently node_redis and ioredis have different result format for transactions, and I think it would be better to let them use an unified format.\nThe solution @an-sh suggested (reject the whole transaction when one of the commands in the transaction fails) makes sense to me. We may use two different error classes for EXECABORT and partial errors.\n. I decided to release ioredis v2 without changing the transaction result format and do that in ioredis v3 (which I think will be released in one or two months).\n. Could you please provide more information about the question? If you mean get keys by pattern, you may use keys command.\n. value is not an integer or out of range shouldn't be caused by hmset command. Instead, incr or decr may cause the error.\nYou can enable the showFriendlyErrorStack option to see where the error is from.\n. Closing this issue in favour of #309. \ud83d\ude06 \n. ioredis should be able to handle the failover so you don't have to. When a failover happens, Redis will disconnect all the clients that connected to the old master and the new master, so ioredis will ask the sentinel servers again for the new master node. Didn't it work for you?\n. No matter what causing the failover, as I said, Redis always disconnects all clients so that clients are able to know whether a failover happens. Refer to http://redis.io/topics/sentinel-clients#sentinel-failover-disconnection for details.\n. This is similar to the warning message in https://github.com/luin/ioredis#transparent-key-prefixing. Since the option is keyPrefix, the prefix will be only applied to keys, instead of patterns and channels.\nYour fix seems good to me, however I'm not sure if we should use another option (maybe channelPrefix) to set the prefix for channels.\n. redis-commands/commands.json is generated by the script https://github.com/NodeRedis/redis-commands/blob/master/tools/build.js. It's just the result of COMMAND command, which I think we shouldn't change.\nTo support channelPrefix, we can write a argument transformer for publish command (we also have to pass the options to the transformer so it can know what to prefix https://github.com/luin/ioredis/blob/master/lib/command.js#L67).\nHowever I think channelPrefix option is not necessary since it's not hard to be implemented by the users.\n. Nice work! Could you please revert the changes of API.md since it's generated by scripts npm run generate-docs.\n. \ud83d\ude80 \n. Just skipped through the code and it LGTM. I'll think about it carefully when I have time before merging this though.\n. Just check out the example: https://github.com/luin/ioredis/blob/master/examples/basic_operations.js\n. @enst There're hundreds of Redis commands available, we just can't mention all of them. However, the usages of them are similar.\n. It's a bug of ioredis v2.0.0. Should be fixed in v2.0.1. Thank you for pointing this out!\n. Released in v2.0.1. \ud83d\ude06 \n. Hmm...Currently Cluster#disconnect() will disconnect all nodes, but it won't wait for all pending commands.\nI've not tried but setting this.manuallyClosing to true and calling this.setStatus('disconnecting') before invoking redis.quit() for each cluster.nodes() should be about to prevent nodes from reconnecting.\nSeems we can add a Cluster#quit() method to do this for users.\n. A little busy these two weeks. I'll revisit this issue this weekend. \ud83d\udeb4 \n. Why don't just use the pipeline given it pretty easy to use:\njavascript\nredis.pipeline().set('foo', 'bar').get('foo').exec();\nAuto pipelining is a little tricky IMO.\n. As far as I know, sentinel doesn't support setting a password (more precisely, allow setting a password with requirepass directive but not support AUTH command).\nFor the sentinel bundled with Redis 3.2, which has protected-mode enabled, you should use the bind directive to restrict Redis to certain interfaces.\nBTW, when denied by the protect mode, ioredis should return a more informative error message, here're what I got:\n[ioredis] Unhandled error event: Error: read ECONNRESET\n    at exports._errnoException (util.js:870:11)\n    at TCP.onread (net.js:552:26)\n[ioredis] Unhandled error event: Error: write EPIPE\n    at exports._errnoException (util.js:870:11)\n    at WriteWrap.afterWrite (net.js:769:14)\n[ioredis] Unhandled error event: Error: Connection is closed.\n    at close (/Users/luin/opensource/ioredis/lib/redis/event_handler.js:99:21)\n    at Socket.<anonymous> (/Users/luin/opensource/ioredis/lib/redis/event_handler.js:79:14)\n    at Socket.g (events.js:260:16)\n    at emitOne (events.js:77:13)\n    at Socket.emit (events.js:169:7)\n    at TCP._onclose (net.js:477:12)\n[ioredis] Unhandled error event: ReplyError: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.\n. > Hello,\n\nI would like to re-consider authentication upon redis-sentinel, because it's now available thanks to this commit, available since redis 5.0.\nThanks a lot\n\nAuthentication for sentinel has already been supported in ioredis v4.9.0: https://github.com/luin/ioredis/blob/master/Changelog.md#490-2019-03-18. IMO it would be better to use a tunnel that supports compression/decompression.\n. stunnel for example. It doesn't make sense to add compression support to ioredis since Redis doesn't support decompression.\n. Aha, I got your point and yes it does make sense. I thought you meant compressing the TCP stream to save the bandwidth.\nYou can monkey-patch the redis commands you need to implement pack & unpack currently. I think adding the support for async hooks can make things easier and more flexible than just offering compression support.\nFor example:\njavascript\nconst redis = new Redis({\n  hooks: {\n    beforeSend(command, args, done) {\n      if (command.name === 'set') {\n        compress(args[1], function (err, res) {\n          args[1] = res;\n          done();\n        });\n      } else {\n        done();\n      }\n    },\n    afterFetch() { /* ... */ }\n  }\n});\n. Closing this in favour of #347.\n. It also possible that people disable the cluster mode temporarily. In this case, ioredis will keep retrying and the connection will be alive again when the cluster mode is enabled. If we just throw an error when connecting to a standalone Redis server, the connection will not be recovered in this case.\n. Have you listened to the error event to the redis instance? redis.on('error', handleError)\n. ioredis won't emit error message unless you listen it explicitly (like your case). Did you use other library listening to the error message and re-emit it?\n. That's strange. What ioredis version are you using?\n. I used the following code trying to reproduce the issue:\n``` javascript\n'use strict';\nvar Redis = require('ioredis');\nvar redis = new Redis({\n  port: 2722,\n  showFriendlyErrorStack: (process.env.NODE_ENV !== \"production\"),\n});\nRedis.Promise.onPossiblyUnhandledRejection((error) => {\n  console.log(\"Redis unhandled error \", error);\n});\nredis.on(\"error\", (error) => {\n  console.log(\"Redis connection error\", error);\n});\n```\nI didn't have any Redis server listen to port 2722, so I got the following outputs:\nioredis git:master \u276f node test.js                                                                  \u23ce\nRedis connection error { [Error: connect ECONNREFUSED 127.0.0.1:2722]\n  code: 'ECONNREFUSED',\n  errno: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 2722 }\nRedis connection error { [Error: connect ECONNREFUSED 127.0.0.1:2722]\n  code: 'ECONNREFUSED',\n  errno: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 2722 }\nRedis connection error { [Error: connect ECONNREFUSED 127.0.0.1:2722]\n  code: 'ECONNREFUSED',\n  errno: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 2722 }\nRedis connection error { [Error: connect ECONNREFUSED 127.0.0.1:2722]\n  code: 'ECONNREFUSED',\n  errno: 'ECONNREFUSED',\n  syscall: 'connect',\n  address: '127.0.0.1',\n  port: 2722 }\nMy application kept reconnecting and didn't crash.\nSince you exported the redis instance, did you re-emit the error in the other place of your code like this:\nconst redis = require('./lib/yourredis');\nredis.on('error', function (err) {\n  this.emit('error', err);\n  // or process.exit(1);\n});\n. Could you run the following code and see the output:\n``` javascript\n'use strict';\nvar Redis = require('./index');\nvar redis = new Redis({\n  port: 2722\n});\nRedis.Promise.onPossiblyUnhandledRejection((error) => {\n  console.log(\"Redis unhandled error \", error);\n});\nredis.on(\"error\", (error) => {\n  console.log(\"Redis connection error\", error);\n  process.exit(1);\n});\nprocess.on('exit', function (){\n  console.log('Exiting...listener count', redis.listenerCount('error'));\n});\n```\n. @hellish Is 127.0.0.1:6666 your slave? Could you give me a minimal reproducible example so I can debug locally?. @hellish Hi, I ran the same setup on my machine and use the following code to see whether the error can be reproduced:\n```javascript\nvar Redis = require('ioredis');\nvar redis = new Redis({\n  sentinels: [{ host: '127.0.0.1', port: 7000 }],\n  name: 'mymaster'\n});\nsetInterval(function () {\n  redis.ping(console.log);\n}, 1000);\n```\nNothing happened when I killed 6666. Here's my sentinel configuration:\n```\nsentinel myid 57ed4573534842da2a805a88e32d98d548d05c21\nsentinel monitor mymaster 127.0.0.1 5555 2\nsentinel down-after-milliseconds mymaster 60000\nsentinel config-epoch mymaster 0\nport 7000\nGenerated by CONFIG REWRITE\ndir \"/Users/luin\"\nsentinel leader-epoch mymaster 0\nsentinel known-slave mymaster 127.0.0.1 6666\nsentinel current-epoch 0\n```\nCould you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here if the error still exists?. @hellish Seems there is more than one connection are made in your code. Could you use the following code to see if killing the slave crashes the application?\n```javascript\nvar Redis = require('ioredis');\nvar redis = new Redis({\n  sentinels: [{ host: '127.0.0.1', port: 7000 }],\n  name: 'mymaster'\n});\nsetInterval(function () {\n  redis.ping(console.log);\n}, 1000);\n```. @hellish The two things are normal though the debug logs were somewhat misleading which I will fix in the next version.\nI think the setup may be misconfigured. What's the configuration of the sentinel? Here are what ioredis do when connecting to a redis instance via sentinel:\n\nConnect to the sentinel (:7000).\nAsk for the address of the master (Will get :5555).\nConnect to the master (:5000).\nDisconnect from the sentinel (:7000).\n\nAnd when the master is down, ioredis will reconnect from scratch from 1. So obviously the connection process has nothing to do with the slave, and the slave's down shouldn't affect the application.. That's strange that the app connected to :6666 instead of :5555. Can you post the full logs here, including how the connection is made? Here's mine:\nshell\ntestsentinel \u276f DEBUG=ioredis:* node index.js                       \u23ce\n  ioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[127.0.0.1:7000]: [empty] -> connecting +3ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster) +3ms\n  ioredis:redis status[127.0.0.1:7000]: connecting -> connect +5ms\n  ioredis:redis status[127.0.0.1:7000]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster) +1ms\n  ioredis:redis write command[0] -> sentinel(sentinels,mymaster) +4ms\n  ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ] +0ms\n  ioredis:redis status[127.0.0.1:5555]: connecting -> connect +11ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[127.0.0.1:7000]: ready -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[127.0.0.1:7000]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:5555]: connect -> ready +1ms\n  ioredis:redis write command[0] -> ping() +979ms\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'. @Salakar Just fixed it. If one of the test failed, the connection to the server will not be disconnected, so other tests will also fail.\n. keyPrefix is only applied to the keys when sending commands to Redis. It has no intention to be a total solution as a namespaces to allow multiple applications sharing a single Redis server.\nIt's pretty similar to another issue https://github.com/luin/ioredis/issues/239.\n. @Tomino2112 Yeah, I know there are some use cases where the support for namespace become very useful. However it's a little difficult to implement that.\nThe support for keyPrefix option is achieved with the benefit of the Redis command API so we could be able to know which arguments of a command are keys. However, there's no such API for replies, which means we have to hardcode a list recording which replies should be transformed. This way is far from reliable IMO and requires a lot of work.\n. I just added a link to this issue to the README. Thank you for pointing this out!\n. Have you tried with ioredis v2.0.0 \n. Done. \ud83d\udc4d \n. Just reviewed the code, and it turns out keyPrefix, dropBufferSupport and showFriendlyErrorStack are the only three options that have to passed to the root options instead of redisOptions. It has nothing to do with pipeline that even you simply run cluster.get('foo'), the key will also not be prefixed (could you please confirm this?).\nThis is not by designed and should be considered as a bug. However fixing this bug will break BC, so I prefer to update the documentation about this and fix it in v3.0. What do you think?\n. Another workaround is to use a separate Redis instance with dropBufferSupport to false and use the JavaScript parser to deal with the buffer.\nSince node-redis-parser 2.x (which will be included in ioredis v3.0) has deprecated the hiredis parser, we can just remove the warning of dropBufferSupport option in ioredis v3.0.\n. The warning will only be displayed when creating parsers: https://github.com/luin/ioredis/blob/master/lib/redis/parser.js#L38-L40. Did your connection often reconnect? \n. Did you bind your cluster to the right interface?\n. A generic pool like https://github.com/coopernurse/node-pool should be enough.\n. The first result is the error of the pipeline, which indicates whether the pipeline itself is failed. The results for each individual command are passed as the second parameter. \n. \ud83c\udf7b \n. \u5982\u679c\u662f\u8fd9\u6837\u90a3\u5c31\u6ca1\u6709\u53d8\u901a\u65b9\u6cd5\u4e86\u3002\u4e0d\u8fc7 Redis Cluster \u81ea\u8eab\u662f\u652f\u6301 keys \u547d\u4ee4\u7684\uff0c\u5e94\u8be5\u662f\u4ee3\u7406\u6216\u8005\u4fee\u6539\u7248\u7684\u95ee\u9898\u3002\nGet Outlook for iOS\nOn Thu, Jun 16, 2016 at 8:48 PM +0800, \"huangyingjie\" notifications@github.com wrote:\n\u767b\u5f55redis \u5ba2\u6237\u7aef\u624b\u52a8\u6267\u884c\u4e5f\u4e0d\u652f\u6301\uff0c\u5e94\u8be5\u662fredis\u672c\u8eab\u5c31\u4e0d\u652f\u6301\uff0c\u6709\u53d8\u901a\u7684\u529e\u6cd5\u5417\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @stranger520 \u53d1\u9519\u5730\u65b9\u4e86\uff0c\u8fd9\u4e2a\u662f ioredis \u7684 repo. Yes, that's because the connection is still active. Calling redis.disconnect() or process.exit() should exit the program.\n. Could you enable the debug mode and post the logs here? DEBUG=ioredis:* node yourapp.js\n. I got the correct results with my setup:\nMy code:\n``` javascript\n'use strict';\nvar Cluster = require('ioredis').Cluster;\nvar cluster = new Cluster([\n  { host: '127.0.0.1', port: '7000' },\n  { host: '127.0.0.1', port: '7001' },\n  { host: '127.0.0.1', port: '7002' }\n]);\ncluster.on('ready', function () {\n  console.log(cluster.nodes().length);\n  console.log(cluster.nodes('slave').length);\n});\n```\nThe logs:\nshell\nioredis git:master \u276f DEBUG=ioredis:* node test\n  ioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:redis status[127.0.0.1:7000]: [empty] -> wait +3ms\n  ioredis:redis status[127.0.0.1:7001]: [empty] -> wait +2ms\n  ioredis:redis status[127.0.0.1:7002]: [empty] -> wait +0ms\n  ioredis:cluster getting slot cache from 127.0.0.1:7000 +1ms\n  ioredis:redis status[127.0.0.1:7000]: wait -> connecting +2ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[127.0.0.1:7002]: wait -> connecting +1ms\n  ioredis:redis status[127.0.0.1:7000]: connecting -> connect +5ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[127.0.0.1:7002]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[127.0.0.1:7000]: connect -> ready +3ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[127.0.0.1:7002]: connect -> ready +0ms\n  ioredis:redis status[127.0.0.1:7003]: [empty] -> wait +3ms\n  ioredis:redis status[127.0.0.1:7005]: [empty] -> wait +0ms\n  ioredis:redis status[127.0.0.1:7004]: [empty] -> wait +0ms\n  ioredis:cluster status: connecting -> connect +0ms\n  ioredis:redis write command[0] -> cluster(info) +1ms\n  ioredis:cluster status: connect -> ready +1ms\n6\n3\nAnd the redis cluster setup: https://github.com/Grokzen/docker-redis-cluster/blob/master/Dockerfile\n. Closing this issue. Feel free to reopen it with more information if the problem still exists.\n. Seems to be fixed in https://github.com/luin/ioredis/pull/322. Not released though. \n. Should be fixed in v2.1.0. Closing...\n. Good catch! Actually ioredis sends undefined to the Redis server directly without converting it to a 0. However Redis considers empty values as zero: https://github.com/antirez/redis/blob/226f679651ca62394021738af0ce467a290c7b90/src/object.c#L639-L641.\n. Yes, it seems not good as a default behaviour to handle with undefined value. I think this issue should be resolved by Redis itself (e.g. returning an error when the parameter of expire is not a integer) instead of ioredis.\n. As far as I understand, even we just send messages to a specified ndoe, Redis will eventually send them to all other nodes of the cluster. I don't think there's anything we can do on the client side to solve the problem.\n. Actually I also think the current way of handling cluster pubsub is not effective, and your proposal looks good to me. However, given that sending messages to specified nodes based on the slots map is not a standard that Redis has implemented, I'm hesitating to introduce this to ioredis.\nSince it's not hard to implement on the client side, what do you think about creating a wrapper for that on top of ioredis?\n. Released in v2.2.0. Thank you for the patch! \ud83c\udf7b \n. Great work! Writable stream offers the cork() method to buffer the data in the memory. In our case, I think it should do the trick by calling cork() on the initial write and uncork() when the queue size reaches the limitation or in the next event loop.\n. Could you reproduce the error?\n. Is the error returned by the quit() method? The behaviours of Cluster#quit() between 2.1 and 2.2 are different that 2.1 sends QUIT to a random node while 2.2 sends QUIT to every node.\nIs it possible to provide a minimal reproducible example for this issue?\n. Calling quit() when some nodes are not connected may cause this error, haven't dug into the problem though.\n. Network latency will not make this example fails since the commands are processed one by one.\n. @migg24 yes, that may happen since the two commands are sent with two tcp connection so you're not sure which one is handled by Redis first.\nClosing this pr.\n. Could explain the question more specifically?\n. My apologies for not reviewing the license before using it. I've sent an email to the contributors of the flexbuffer module.\nI'll replace it if there's no response.\n. Seems lazyConnect option should do the trick.\n. hmm...It doesn't work in cluster mode. What's your use case?\n. Yeah, should be pretty similar to the standalone version: https://github.com/luin/ioredis/blob/master/lib/redis.js#L126-L131\n. Did you bind the redis nodes to the wrong interfaces?\n. I believe that's not a ioredis specified issue. Refer to http://stackoverflow.com/questions/37341646/redis-cli-redirected-to-127-0-0-1 for details.\n. I'm closing this issue. Feel free to reopen it with more information.\n. Nice work! Cluster#disconnect() and Cluster#quit() require special handling since when not connected to the cluster, connectionPool will not emit drain event when calling reset([]).\n. all options can be passed as query strings. e.g. redis://127.0.0.1:6379/0?dropBufferSupport=true\n. Nice catch! Could you change the error message to utils. utils.CONNECTION_CLOSED_ERROR_MSG?\n. Just redis.client('list', function (err, result) {}); should work. ioredis supports all redis commands so that you don't need to create new Redis.Command instances yourself.\nBTW, as for command client list, actually the command part is client, and list is just the first argument.\n. You can define your own reply transformer to parse it (https://github.com/luin/ioredis#transforming-arguments--replies) or simply parse it on the client side.\n. what do you mean by \"persistent connection\"?\n. ioredis already supports detecting loading in standalong version: https://github.com/luin/ioredis/blob/master/lib/redis.js#L420-L428. Seems we just need to wait for the \"ready\" event of the new redis node here: status.https://github.com/luin/ioredis/blob/master/lib/cluster/connection_pool.js#L58-L63\n. Hmm...I just checked the code, and it seems that when a node has not finished loading data from the disk, the commands sent to it will be added to its offline queue instead of sending to the redis immediately.\n. That's strange. Either the node is a slave or a master doesn't affect the support of offline queue. Are you able to reproduce the issue? Or enable the debug log maybe? \n. Hi @aleemb. keyPrefix is first supported by ioredis, and at that time, node_redis didn't have this feature, so the migration guides didn't include it. I just revisited the guides and found out it mentioned \"The options passed to the constructor are different\".\nAs for me, keyPrefix is better since the prefix only applies on the keys rather than patterns and channels.\n. what error did you get when using promise?\n. That's a bug. To fix it, I prefer to throwing an error when the pipeline is reused instead of clearing the command queue and start a new pipeline internally (which is what you expect in your example) to avoid misleading users into thinking reuse the pipeline is different from creating a new pipeline.\n. It seems there's a bug in router handler for \"/sync\" that the first argument passed to updateUsers is already a pipeline, so that Pipeline#pipeline() would cause an error at the first line of updateUsers.\n. As for me, introducing Pipeline#pipeline() makes things more complex. Your example can be rewritten without the support of reusing pipeline:\n``` javascript\nfunction updateUser(client, user) {\n  client.hmset(user.key, user);\n}\nfunction updateUsers(pipeline, users) {\n   users.forEach(function (user) {\n     updateUser(pipeline, user);\n   });\n   return pipeline;\n}\n// more functions like updateUser and updateUsers\nfunction deleteUser(client, user) { ... }\nfunction deleteUsers(client, users) { ... }\nfunction addUser(client, user) { ... }\nfunction addUsers(client, users) { ... }\n// ...\nvar db = require(\"./myDB\"); // instance of ioredis\n// express app batch processing\napp.post(\"/sync\", function (req, res) {\n  var pipeline = db.pipeline();\n  // first we fetch data from DB, by ids\n  getBatch(pipeline, req);\n  pipeline.exec().then(function (batch) {\n    var innerPipeline = db.pipeline();\n    // at that point pipeline has an empty queue\n    deleteUsers(innerPipeline, batch.usersToDelete);\n    addUsers(innerPipeline, batch.usersToAdd);\n    updateUsers(innerPipeline, batch.usersToUpdate);\n    return innerPipeline.exec();\n  }).then(function () {\n    // ... error handling, etc\n    res.sendStatus(200);\n  }).catch(function () {\n    res.sendStatus(500);\n  });\n});\n// single processing\napp.put(\"/update\", function (req, res) {\n  var update = updateUsers(db.pipeline(), req.usersToUpdate).exec();\n  update.then(function () {\n    // ... error handling, etc\n    res.sendStatus(200);\n  }).catch(function () {\n    res.sendStatus(500);\n  });\n});\n```\nIt's simpler and easier to understand.\n. Which ttl did you mean? Could you provide an example?\n. Redis session? Do you mean the arguments of the EXPIRE command?\n. Aha. What value of the default ttl do you want? You can define a new command with a default ttl set:\njavascript\nredis.exipreWithDefaultTTL = function (key, seconds) {\n  seconds  = seconds || 0;\n  return redis.expire(key, seconds);\n};\n. I've not used thread-pool. Did you get any errors when using ioredis with thread-pool?\n. Do you mean generic-pool or thread-pool? For the former one, you should be able to use it with ioredis.\n. This shouldn't be handled inside the ioredis since getting an inexistent key will receive an empty value instead of throwing an error. You may, as @AVVS said, just throw an error when amazonSTR is null.\n. You should bind the redis to the correct interface instead of 0.0.0.0. 10.2.1.1 for example.\n. Global level transformations will be deprecated in the 3.0 version and hooks will be introduced. Currently connection-level transformers are not supported. Two workarounds may be used before 3.0:\n1. Your approach, clear the require cache to force node.js requires a isolate ioredis instance for each connection.\n2. Simply override the command like:\njavascript\nconst Redis = require('ioredis');\nconst redis = new Redis();\nconst zrangebyscore = redis.zrangebyscore.bind(redis);\nredis.zrangebyscore = function (args, /* other arguments...*/) {\n  return zrangebyscore(args,  /* other arguments...*/).then((reply) => {\n    // transform reply here\n    return reply;\n  });\n};\nSee https://github.com/luin/ioredis/issues/347 for details.\n. 6379 and localhost are the default options when port/host are not specified. They aren't used when sentinels option is provided.\nCould you please enable the debug mode to see the logs by running DEBUG=ioredis:* node yourapp.js. And connect to one of your sentinel with redis-cli ($ redis-cli -p 16380) and run the command SENTINEL sentinels and see the replies.\n. What about the replies of the command sentinel get-master-addr-by-name?\n\n. Then what about sentinel get-master-addr-by-name?\n. Is 127.0.0.1:6380 your master node?\n. Hi @dmason30!\nWhat do you mean by \"initial data\"? If you mean the message that emitted before the connection is established, than the answer is you can't because redis's pub/sub is fire and forget, the message will not be stored in the memory.\n. Yes, this is the expected behaviour. Because redis doesn't store the published messages in the memory/disk. Refer to http://redis.io/topics/pubsub for details.\n. \u786e\u4fdd pipeline \u548c multi \u7684 key \u90fd\u5c5e\u4e8e\u540c\u4e00\u4e2a slot\uff0c\u5426\u5219\u5c31\u62c6\u5f00\n. Released with 2.4.0. Thank you for the pull request (and the documentation for the changes) :-)\n. Could you confirm whether the data stored in the database is correct? Did you try logging the result and i when they were not same (console.log('error value', result, i))?\n. I'm closing this for now. Feel free to reopen this issue with more details.\n. Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here? Also remember to print the unexpected values (by console.log('error value', result, i)).\nBtw, what's your Redis setup? Standalone or Cluster? Are you using the official Redis version or a SaaS Redis service? Which Redis server and ioredis versions are you using?\n. > I think ioredis can make sure we can get the corresponding return value exactly in async get.\n\nright\uff1f\n\nThis is the most basic requirement for a redis client. So the answer is definitely yes.\n. \u7528\u7684\u4e0d\u662f\u5b98\u65b9\u7684 Redis \u5427\u3002\u9519\u8bef\u4fe1\u606f\u4e0d\u662f\u5199\u7740\u4e86\u4e48...ERR AUTH Authentication password incomplete.The password is instanceid:password, example: 05719bf0c3ae11e4:123456\n. \u76f8\u4fe1\u6211\uff0c\u8fd9\u4e2a\u7edd\u5bf9\u4e0d\u662f\u5b98\u65b9\u7248\u7684 Redis\uff0c\u662f\u4fee\u6539\u8fc7\u7684\u3002\u9519\u8bef\u4fe1\u606f\u8bf4\u7684\u662f\u5bc6\u7801\u4e0d\u5b8c\u6574\uff0c\u683c\u5f0f\u662f instanceid:password\uff0c\u4f60\u53ea\u4f20\u4e86\u540e\u4e00\u90e8\u5206\uff08\u6216\u524d\u4e00\u90e8\u5206\uff09\n. \u6211\u7684\u610f\u601d\u662f\u4f60\u7ebf\u4e0a\uff08\u670d\u52a1\u5668\u4e0a\uff09\u8fde\u7684 Redis \u670d\u52a1\u5668\u662f\u975e\u5b98\u65b9\u4fee\u6539\u7248\uff0c\u4ee3\u7801\u88ab\u6539\u8fc7\u4e86\u3002Redis \u670d\u52a1\u5668\u62a5\u9519\u8bf4\u4f60\u7684\u5bc6\u7801\u6709\u9519\u8bef\uff0c\u548c ioredis \u6ca1\u5173\u7cfb\u3002\n. \u4e0d\u53ef\u80fd\u7684...\u548c\u6a21\u5757\u6ca1\u5173\u7cfb\uff0c\u4f60\u518d\u4ed4\u7ec6\u68c0\u67e5\u4e00\u4e0b\u5427\n. 1. \u963f\u91cc\u4e91 Redis \u662f\u975e\u5b98\u65b9\u7684\uff0c\u88ab\u4fee\u6539\u8fc7\u4e86\uff1b\n2. \u4f60\u73b0\u5728\u9047\u5230\u7684\u95ee\u9898\u662f\u5bc6\u7801\u9519\u8bef\uff1b\n3. \u4e0d\u8bba\u662f ioredis \u8fd8\u662f redis \u6a21\u5757\u90fd\u4f1a\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\n. \u5f53\u7136\u53ef\u4ee5\u7528 ioredis \u4e86\uff0c\u5bc6\u7801\u6211\u4e0a\u9762\u4e0d\u662f\u8bf4\u4e86\u4e48\u3002\u3002\u4f60\u73b0\u5728\u7684\u5bc6\u7801\u9519\u4e86\uff0c\u963f\u91cc\u4e91\u8981\u6c42\u5bc6\u7801\u662f instance_id:password\uff0c\u4f60\u53ea\u63d0\u4f9b\u4e86 password\u3002\njavascript\nnew Redis({ password: 'instance_id:password' });\n\u628a\u5bf9\u5e94\u7684 instance_id \u548c password \u6362\u6210\u4f60\u7684\u5c31\u884c\u4e86\u3002\n. redis.getset(key, value)\n. \u770b\u770b err \u7684 lastNodeError \u5c5e\u6027\u3002\u9ebb\u70e6\u95ee\u9898\u90fd\u63d0\u5230 StackOverflow \u5427\uff0c\u8fd9\u91cc\u53ea\u4f9b bug \u548c feature \u4ea4\u6d41\u3002\u591a\u8c22\u4e86\n. It seems to be a network problem. Are you able to connect to your redis instance via redis-cli?\n. The ECONNRESET error means other side of the TCP conversation (Redis server or any proxy) abruptly closed its end of the connection, so it shouldn't be a problem on the ioredis side. Could you check if ioredis ran in the same network environment as RDM or Jedis? Please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here.\n. ioredis simply, as documented, passed all arguments to Redis directly. Refer to the official documentation for the usage of each command: http://redis.io/commands.\n. javascript\nredis.sort('newuserlist', 'get', 'user:userid:*:username');\n. It seems to be a network problem.\n\u662f\u7f51\u7edc\u539f\u56e0\u5427\uff0c\u770b\u4f60\u5177\u4f53\u7684\u7f51\u7edc\u73af\u5883\u4e86\n. \u548c ioredis \u65e0\u5173\u7684\uff0c\u8d85\u65f6\u539f\u56e0\u5f88\u591a\uff0c\u6bd4\u5982\u7f51\u7edc RTT \u6bd4\u8f83\u9ad8\u6216\u8005 Redis \u7ebf\u7a0b\u963b\u585e\u3002\n. \u55ef\uff0c\u6392\u9664\u8d85\u65f6\u7684\u8bdd\u5c31\u662f\u7f51\u7edc\u95ee\u9898\u6216\u8005 Node.js \u8fd9\u8fb9 CPU \u963b\u585e\u4e86\u3002\u628a ioredis \u548c java \u7684\u4ee3\u7801\u653e\u5230\u540c\u4e00\u4e2a\u7f51\u7edc\u73af\u5883\u770b\u770b\uff0c\u5982\u679c\u8fd8\u6709\u95ee\u9898\uff0c\u786e\u8ba4\u4e0b\u662f\u4e0d\u662f Redis Cluster \u8fd4\u56de\u7684\u6570\u636e\u6bd4\u8f83\u5927\uff0cJavaScript \u7684 parser \u5904\u7406\u4e0d\u8fc7\u6765\n. \u5982\u679c CPU \u8fbe\u5230 100% \u5fc5\u7136\u4f1a\u5bfc\u81f4\u963b\u585e\u7684\n. I've cherry-picked the commits to another pull request #425. Closing this.. A workaround is sending ROLE command every second (or shorter depending on your need), and disconnect manually when the result changed to slave. However, messages will still be lost when published before heartbeat detecting the failover.\n. Aha, Nice solution! Thank you for sharing this.\n. It seems the auth info part should be encoded (encodeURIComponent) before put into the url.\nDetails:\nhttps://github.com/nodejs/node/blob/master/lib/url.js#L561\nhttps://www.ietf.org/rfc/rfc3986.txt\n. There're already several related issues posted: #254 #359 #325. A warning is added to the \"Transparent Key Prefixing\" section in README for this matter.\nCurrently I'm not planning to add support for prefixing patterns and replies given it's not a good practice to use key prefix as namespace to allow multiple applications share a same redis database.\nPS - thank you for using ioredis \ud83d\ude06 \n. Name spacing is recommended only when it's used in the same application. For example, it's useful to store users and posts details in the same database by prefixing them with users: & posts::\nSET users:178.name \"Bob\"\nSET posts:201.name \"Welcome to the WordPress\"\nkeyPrefix is designed for this use case: you may create two Redis instances with different prefixes, one for users and the other for posts:\njavascript\nuserRedis.set(`${id}.name`, 'bob')\npostRedis.set(`${id}.name`, 'Welcome to the WordPress')\nIn the above case, keys (and scan) is rarely needed to list all keys that have the same prefixes since, in the production, you may have a SET for indexing them.\nHowever, it's not recommended to have multiple application sharing the same database:\nSET APP1:data.1 good\nSET App2:data.1 bad\nYou should setup a single Redis database for each application.. \u53ea\u652f\u6301 Redis 3.0 \u4e4b\u540e\u7684\u5b98\u65b9 Cluster\u3002Codis \u6211\u8bb0\u5f97\u662f\u505a\u4e86\u900f\u660e\u4ee3\u7406\uff0c\u76f4\u63a5\u7528 new Redis() \u8fde\u5c31\u884c\n. That's strange. Did you connect to an official Redis instance or a proxy server? Can you reproduce the issue?\n. Seems there's an issue in ioredis of handling wrong bound IP.\n. The cluster nodes may have been bound to the wrong interface (so they returned local address like 127.0.0.1 which ioredis can't connect to), you can check this by running CLUSTER NODES in the redis-cli.\nDebug mode can be enabled by DEBUG=ioredis:* yourapp.js.\n. Related issue: https://github.com/luin/ioredis/issues/330\n. Good point! Thank you for the patch.\n. What you want is read-write splitting, which is already supported by Redis Cluster (new Redis.Cluster()). However, given your setup, I think you even don't need it since the dataset is relatively small (that the whole dataset can be held by one node) and the performance of Redis is very good when all your commands are simply GET & SET.\n. Read-write splitting is only implemented in Cluster mode currently. The support of it for standalone Redis or Sentinel is not here yet but pr is welcome.\nIt shouldn't be hard to do the same thing for Sentinel. You can refer to https://github.com/luin/ioredis/blob/master/lib/cluster/index.js#L427-L433 for how to detecting whether a command is readonly (that can be sent to slaves) or not. Then override Redis#sendCommand or just write a wrapper for it to send every command to different Redis instances.\n. @nodo Refresh available nodes periodically with INFO command and pick a slave randomly or via a config function.\n. @nodo The information about slave addresses from INFO is related with your setup, the IP will not be private if your bind the slaves to the current interface.\nYes, INFO may be called every second or so.\n. @toredash It depends on what you think is read-write splitting. The discussions above consider \"read-write splitting\" as an automatic way to send different commands to different nodes via ONE Redis instance. Your example works, while developers have to do the splitting themselves.. @dev-drprasad That's supported in the cluster mode. Otherwise, you may refer to @toredash 's approach.. mset \u4e0d\u652f\u6301\u8fd9\u6837\u7528\uff0c\u5177\u4f53\u53ef\u4ee5\u67e5\u4e00\u4e0b\u76f8\u5173\u6587\u6863\u3002\u4f60\u53ef\u80fd\u9700\u8981 hmset \u6216\u8005 mset(45,JSON.stringify({aa:30,bb:'40'}))\n. Did your redis server run behind any proxies (SaaS services, codis or something)? Which ioredis and Redis version are you using?\n. Could you please try to reproduce the problem so I can debug locally?\n. How can I reproduce it locally?\n. I'm not sure but it may be related with your codebase. I'm willing to help if you have further information for this issue.\n. Yes, the result of [ [ null, 'O' ], [ null, 'K' ] ] should not be returned, but I still can't find out where the result comes from. Let me know if you are able to provide a self-contained code to reproduce the problem. I'll keep looking into this issue.\n. Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs during this problem happens here?\n. When a command is rejected with error MOVED, ioredis will update the local slot cache and send the command to the new node. It's strange that the command is sent multiple times to the same node.\n. @thelinuxlich Is this issue related with your environment? If the issue cannot be reproduced with general setups, it would be very helpful to provide the docker files so I can debug the problem locally in exact the same environment as yours. Otherwise is it possible to help me narrow down the possibility where the problem happens.\n. That's strange since there's not much difference between the two versions: https://github.com/luin/ioredis/compare/v2.0.0...v2.4.0. Hmm...\n. @thelinuxlich Could you please also test with ioredis v2.1.0 & v2.2.0 so we can know which version was the issue introduced. Thanks a lot \ud83d\ude06 \n. That means 2.1.0 doesn't have the problem? Looks like there's no much difference in cluster mode between 2.1.0 & 2.2.0: https://github.com/luin/ioredis/compare/v2.1.0...v2.2.0. Hmm...\n. @Aditya-Chowdhry No, the issue hasn't been reproduced in my environment.\n@thelinuxlich How did you solve the problem in the end?\n@AVVS The link you posted seems to be broken. Which part of code did you mention?. @AVVS Thanks for the update. How is this line of code related with the issue?. @Aditya-Chowdhry Was 6389 down while getting the error? Is the issue reproducible?. @Aditya-Chowdhry Just fixed this issue in v3.2.1. Thank you for the example, that helps!. @tuananh null and undefined are converted to empty strings in ioredis. It makes sense to calculate slot for an empty string.. Related issue: https://github.com/luin/ioredis/issues/387.\nscaleReads can be only passed via the second argument.\n. Currently nope. Please refer to #387 for details.\n. Pipeline is designed to reduce the network latency of sending/receiving many small commands. All commands in a pipeline have to be stored in the memory before sending them off, so it would require huge memory if your commands contain large data.\nPipeline \u662f\u4e3a\u4e86\u51cf\u5c11\u53d1\u9001\u5f88\u591a\u5c0f\u547d\u4ee4\u5bfc\u81f4\u7684\u7f51\u7edc\u5ef6\u8fdf\u800c\u8bbe\u8ba1\u7684\u3002\u901a\u8fc7 Pipeline \u53d1\u9001\u7684\u6240\u6709\u547d\u4ee4\u90fd\u5fc5\u987b\u5148\u5b58\u50a8\u5230\u5185\u5b58\u4e2d\uff0c\u6240\u4ee5\u5982\u679c\u4f60\u7684\u547d\u4ee4\u4e2d\u5305\u542b\u5f88\u591a\u5927\u6570\u636e\u65f6\uff0c\u5c31\u4f1a\u6d88\u8017\u5f88\u591a\u5185\u5b58\u3002 \n. It's not a ioredis specified question. I suggest you refer to Redis community for help: http://redis.io/community.\n. Synchronous function option makes sure no additional IO operation is required when selecting slaves. Just keep pinging the slaves in your app and maintain a table to store the best one. Then return it in preferredSlaves synchronously.\n. @Nepoxx Yes, that's correct. Just chose a slave randomly at the first time.\n. Create a separate ioredis client and ping periodically may be not able to keep other connections alive.\nKue provides the client member to access the underlying redis instance.\n. https://github.com/luin/ioredis#running-commands-to-multiple-nodes\n. Check out sentinelRetryStrategy option.\n. Yes, an error event will be emitted when sentinelRetryStrategy returns a non-number. Besides, Redis#connect() will be rejected with the same error.\n. That's strange. How many sentinels do you have? Given ioredis only emits the last error (https://github.com/luin/ioredis/blob/5f740758593bb0e2342d25c1f84e43418d6cb868/lib/connectors/sentinel_connector.js#L78-L79), it seems ioredis tried to connect to another sentinel and got Connection is closed after ERR No such master with that name.\n. Could you post full logs here including retries for the two sentinels?\n. It shows 127.0.0.1:26380 refuse the connection ECONNREFUSED instead of returning ERR No such master with that name.\n. \u53ef\u4ee5\u76f4\u63a5\u7981\u7528\u9ed8\u8ba4\u7684\u81ea\u52a8\u91cd\u8fde\uff0c\u6539\u6210\u76d1\u542c end \u4e8b\u4ef6\u5e76\u5728\u5176\u4e2d\u91cd\u65b0 connect\nGet Outlook for iOS\n_____________________________\nFrom: \u6750\u4e3b notifications@github.com\nSent: Saturday, November 19, 2016 4:48 PM\nSubject: [luin/ioredis] redis\u65ad\u5f00\u91cd\u8fde\u903b\u8f91 (#398)\nTo: luin/ioredis ioredis@noreply.github.com\n\u6ca1\u6709\u8bbe\u7f6e retryStrategyconst redisClient = new Redis(6379, '127.0.0.1');const value = yield redisClient.get('key');\n\u8fd9\u91cc\u5982\u679credis\u670d\u52a1\u5668\u65ad\u5f00\u4e86\uff0credisClient.get \u8fd9\u53e5\u4f1a\u963b\u585e\u4f4f\uff0c\u7136\u540e\u5f00\u59cb\u4e0d\u65ad\u91cd\u8fde\u3002\n\u80fd\u4e0d\u80fd\u8ba9 redisClient \u7684\u91cd\u8fde\u8ddf\u4e1a\u52a1\u903b\u8f91\u533a\u5206\u5f00\u6765\uff1f\u6bd4\u5982 redisClient \u81ea\u5df1\u5355\u72ec\u53e6\u5f00\u4e2a\u7ebf\u7a0b\u53bb\u91cd\u8fde\uff0c\u7136\u540e redisClient.get \u8fd9\u4e9b\u65b9\u6cd5\u5728\u8c03\u7528\u7684\u65f6\u5019\uff0c\u68c0\u6d4b\u5982\u679c\u6ca1\u8fde\u63a5\u6210\u529f\u5c31\u76f4\u63a5\u8fd4\u56de\u201credis\u672a\u8fde\u63a5\u201d\u9519\u8bef\u7801\uff0c\u8fd9\u6837\u624d\u4e0d\u4f1a\u56e0\u4e3a\u91cd\u8fde\u7684\u903b\u8f91\u800c\u963b\u585e\u5230\u4e1a\u52a1\u903b\u8f91\u3002\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.  \n. Which ioredis version are you using?. I'm closing this since the warning has been reverted in the latest Node.js version.. It's recommended to listen to the error event and handle the errors explicitly, otherwise ioredis will logs the error the stderr.. ioredis supports all Redis commands. Try redis.hmget().. Yes, status shows the connection status of a connection, and it's documented on the README.md.. When you create a Cluster instance, ioredis will fetch the node information asynchronously. You need to listen to the connect event and get the master nodes after that.\nIf you really want to run the command synchronously, you can provide readOnly property to each node, true means the node is a slave.. Thank you for addressing that! I've added you as the contributor, please review & merge the pull request above.. v2.4.1 released. \ud83d\ude06 . I've sent a invite link to you so that you should be able to write to this repo. You can check out https://github.com/luin/ioredis/invitations if you don't see it.\nIt seems tls.TLSSocket is the subclass of net.Socket, so I think it should have the setNoDelay method (ref: https://github.com/nodejs/node-v0.x-archive/issues/6194).. ioredis follows exactly what Redis server returns. If you try to hgetall a non-existent key, Redis will return an empty list instead of nil. Given that, ioredis returns an empty object instead of null.. I definitely agree that we can drop the support for node < 4 so we don't need to worry about the BC for the older version, though I'm a little worried about the performance of es2015 with V8 (http://v8project.blogspot.kr/2016/12/v8-release-56.html?m=1).. Just released ioredis v4.0.0-0. It's a major beta release that drop the support for Node.js < 6. We uses the native Promise by default instead of bluebird (and users can switch back via Redis.Promise = require('bluebird')).\nThe total package size dropping to 758KB from 1.4MB \ud83d\ude06.. Question 1\nioredis follows exactly what the Redis server does. When the syntax and arguments are all valid, the transaction will never fail (thus the err object will always be null). Instead, if some of the commands fail, the error will be returned via results. Details: https://redis.io/topics/transactions#errors-inside-a-transaction\nQuestion 2\nSlot cache is a must-have feature for a redis client to be used in production. So given ioredis is widely used by many companies, we of course implement it. Refer to getInfoFromNode function and the handler for MOVED, ASK error for details.\nThank you for purchasing Medis \ud83d\ude06! Pardon me but I'm wondering what does \"add multiple instance connection at the same time\" mean.. Sorry for the late response. There's a problem that the QUIT command returns an OK so the command.resolve() should reflect it.. I think command.resolve('OK') should be command.resolve(new Buffer('OK')) to handle the case that users call redis.quitBuffer().\nWe'd better also include a test for it:\nredis.quitBuffer().then(function (result) {\n    expect(result).to.be.instanceof(Buffer);\n    expect(result.toString()).to.eql('OK');\n}). Yes I think it should be a separate test since quit & quitBuffer are two different commands.. LGTM! \ud83d\udc4d . Sure! Just released v2.5.0. Thank you for the pull request.. It should return with the correct result:\n\nCould you please provide a minimal reproducible example?. Thank for the report! I just confirmed this is a bug due to a race condition. It happens when sending pipeline/transaction that includes info or config command on the \"connect\" event.\nI'm going to fix this today. BTW, It's recommended to handle the \"ready\" event instead of the \"connect\" event, since the latter one is a low level event emitted by the \"net\" module directly.. Fixed in v2.4.3. \ud83c\udf7b . \ud83c\udf7b . It's a redis command so you can check out the redis official document. The unit of expire is second \u00a0while pexpire is ms.\nGet Outlook for iOS\nOn Wed, Dec 21, 2016 at 9:44 PM +0800, \"vipreshjha\" notifications@github.com wrote:\nredis.expire command takes the time in Seconds or Milliseconds?\nWould be nice if this was also documented somewhere.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Sorry for the late response. I do not recommend to use pipeline in this case since the slot-node table changes and accessing slot-node on the application side breaks the slot redirection (ask, moved or node failovers).\nFor the question 2: ioredis checks the keys of all the commands to ensure that all these commands are able to sent to the same node.\nFor your case, just send the 5000 commands without pipeline should not cause any performance problems because Node.js by default follows pipeline mode (that sends the next command without waiting for the receiving of the previous command) when sending commands.. @AVVS Why not just call Cluster#pipeline() directly instead of Redis#pipeline() given all your keys belong to the same slot since your prefix them with the common {} flag.. @AVVS Aha! Then your code looks good to me. Why don't you just update your lua script to support multiple keys in one command?. Hi, sorry for the late response. You must make sure all keys belong to the same slot when using pipeline in the cluster mode.\nFor example:\njavascript\ncluster.pipeline().set('{foo}1', 'bar').set('{foo}2', 'bar').exec(). COUNT 10 should be specified as two arguments. Try 'COUNT', 10. Makes sense to me. However this change will break BC so this feature may be shipped with v3.0.. This repo is the redis client for Node.js. Please refer to the lettuce repo.. The v2 of the js parser had performance issues when parsing big chunk of data (https://github.com/NodeRedis/node_redis/pull/900#issuecomment-259638072) but it seems this problem has already been fixed. I'll do some benchmarks on it and will switch to it if the performance go better. At that time the recommendation will be definitely removed from the docs.\nThank you for bringing this up!. Closing this since #425 has been merged.. Could you enable the debug mode DEBUG=ioredis:* node yourapp.js and post the logs here?. I'm closing this. Feel free to reopen it with details.. Try to listen to the \"error\" event to see what the error is:\njavascript\nclient = new Redis.Cluster(redisClusters, {\n            retryDelayOnFailover : '100'\n        });\nclient.on('error', function (err) {\n  console.log(err.lastNodeError);\n});\nWhat error do you get?. Thank you for the pull request! The \"sharded\" here refer to https://en.wikipedia.org/wiki/Shard_(database_architecture).. By default offline queue is enabled to keep the commands sent during disconnected. Disabling it should solve your problem.. Just pass the arguments correspondingly:\n```\n\nredis-cli\nzrangebylex myzset [aaa (g\n```\n\njavascript\nredis.zrangebylex('myzset', '[aaa', '(g'). What's the lastNodeError property of the Failed to refresh slots cache error?. Hmm...This error indicates cluster is disabled for the Redis server you connected to. Could you check again whether the cluster-enabled is enabled?. The Connection is closed error means the node you connected to closed the connection for some reason (e.g. misconfigure of the cluster). Try to enable the debug mode DEBUG=ioredis:* node yourapp.js may show you the direction where the problem is.. You missed the keyword new: new IORedis.Cluster({...}). \"error\" event is emitted when the connection is failed. The best practice is to handle the error explicitly, for instance:\njavascript\nredis.on('error', error => {\n  logServer.send('redis connection error', error)\n  // reconnect or simply exit the application\n})\nBy this way, ioredis won't log \"Unhandled error event\".. It's not an ioredis related issue. Promise#delay only receives one parameter. Refer to http://bluebirdjs.com/docs/api/promise.delay.html for details.. Nope. Reconnecting when invoking commands is not supported, though you can write a wrapper for that. I recommend using the retryStrategy option for reconnecting. . Or only allow limited reconnecting attempts, and reconnect on disconnected:\n```javascript\nconst redis = new Redis({\n  retryStrategy(attempts) {\n    if (attempts < 3) {\n      return 100\n    }\n    // otherwise just disconnect from the server to let all the error handlers\n    // of commands get involved. \n  }\n})\nredis.on('end', () => {\n  redis.connect()\n})\n``. @marvel308 Could you provide your code to reproduce the issue?. I'm closing this. Feel free to reopen it with details.. Using a single instance of Redis for an app is the recommend way (instead of creating a new instance for each request/operation). Multiple operations can be invoked at the same time without any problem.. The last element is the result of the pipeline itself.. Thank you for the pull request! Could you give us more details of which problem this pr is intended to solve? I revisited #387 but read-write splitting is already supported in the Cluster mode.. Sorry for the late response. I've been thinking about whether this feature should be in a separate module since ioredis is focused on the official Redis features instead of the platform-specified ones. . Please try to add a statement (debug('resolved: %s:%s', resolved.host, resolved.port);`) to log the resolved address:\n\nin lib/connectors/sentinel_connector.js to see whether the sentinels return the correct master address.. I'm not sure but the issue may due to the configuration since it seems the correct address has been resolved to the client.. A1:\nWhere's this.serverInformation? I searched the whole repo but didn't find the property.\nA2:\nYou can't currently. ioredis will disconnected with the active sentinel when the master/slave address has been resolved.. Thank you for the pull request! All look good to me except the db: 0 part. I'm afraid setting it explicitly will lead people into thinking db is required even if they just want to connect to the first database.. Aha! Merged! \ud83c\udf7b . No limits on the ioredis side.. Try to attach the error event and log the error:\njavascript\nredis.on('error', function (error) {\n  console.dir(error)\n}). What ioredis version are you using?. @tuananh @WClouds Could you confirm that there's only one Redis instance created so that the \"Unhandled error\" can't be emitted from other instances that not bound the \"error\" event?\nThe unhandled warning is only printed when \"error\" event has no listener. Here's the source code of it:\njavascript\n  if (this.listeners(eventName).length > 0) {\n    return this.emit.apply(this, arguments);\n  }\n  if (error && error instanceof Error) {\n    console.error('[ioredis] Unhandled error event:', error.stack);\n  }. Sorry for the delay. Merged and released with v3.0.0-1! \ud83c\udf7b . Sentinel is a high availability solution for Redis. ioredis supports read-scales by leveraging the features that provided by sentinel. Refer to https://redis.io/topics/sentinel for details.\nIt seems Amazon Elasticache Redis doesn't use sentinel in their replication implementation, so in this case ioredis can't scale reads since there's no official interface offered by AWS for fetching the list of slaves.. No, there's no built-in way for this. However, it's possible for you to create multiple connections to your replication (each for a slave), and route the queries to random connections. For example:\n```javascript\nconst slaves = [\n  new Redis(//),\n  new Redis(//),\n  new Redis(/**/)\n]\nimport {sample} from 'lodash'\nsample(slaves).get('foo')\n``. Hi @zbmowrey, the usage above ofsinteris correct. Could you confirm you connected to the same Redis instance as redis-cli? Try to runmonitor` command in the redis-cli and run the code again to see whether the command and arguments sent to Redis are correct.. Hi @ty10r, thank you for bringing this up.\nTypically an \"error\" event in Node.js crashes the current process if there's no listener attaching on it. However, when comes to the production environment, crashing the whole application in the case of network issues definitely to be a bad default behavior.\nInstead, by default we console.error errors to let developers know they should handle the \"error\" events explicitly (logging errors to databases or their log centers).. I'm closing this. Feel free to reopen it for further questions.. @cmhsu-cs 11 million rows may consume large memory (GC will get involved) and require CPU resource for parsing them. It pretty depends on what data does each row has, so just try with your own data.. All IO operations are async (won't block the main event loop), while the parsing process happens on the main event loop.. @cmhsu-cs  Parsing replies to JavaScript data structures. . The callback here may be called twice.. Didn't the quoted documentation answer the question?. This kind of warnings are valid since there are promises that being rejected but not caught explicitly.\nFor example:\njavascript\nredis.get('foo').then(console.log)\nIn the above case, the get command is not being caught explicitly, so once it fails, the warning will be logged. To solve the issue, simply chain a catch() to the command or maybe add a global listener to bluebrid to suppress all the warnings.. @benzaita The error stack shows there are pending commands when shutting down WiFi. Could you please enable showFriendlyErrorStack option to see where did the error come from?. Hmm...ioredis defaults the connection timeout to 3 seconds, which seems not enough for your case (+5s). You can custom this option:\njavascript\nnew Redis({\n  connectTimeout: 10000\n}). 1. Redis \u662f\u5355\u8fdb\u7a0b\u670d\u52a1\u5668\uff0c\u6240\u4ee5 ioredis \u6ca1\u6709\u4f7f\u7528\u8fde\u63a5\u6c60\u3002\u6267\u884c\u5b8c\u547d\u4ee4\u540e\u65e0\u9700 QUIT\uff0c\u8fdb\u7a0b\u9000\u51fa\u65f6\u4f1a\u81ea\u52a8\u91ca\u653e\u3002\n2. \u6ca1\u6709\u4f7f\u7528. \u662f\u7684\uff0c\u90fd\u4e00\u6837. There are two mechanisms that ioredis used to track connection status: Promise and event. Promise that returned by Redis#connect() will reject as soon as the connection is failed (even a reconnection will be made). Meanwhile, \"error\" event will be emitted when the connection is failed (no matter whether you caught the Promise error).. No, but you can simply listen to the error event explictly to avoid the \"Unhandled\" warning:\njavascript\nredis.on('error', err => {\n  // handle the err here or just ignore them\n});. In which case do you need to know the length of a pipeline queue?. javascript\nvar redis = new Redis()\nredis.select(12). You may want to disable retryStrategy if it's required to make every command be aware of the server's offline.. Calling redis.connect() when the server is ready.. I would recommend you create two separate clients each connect to a different database instead of using select to avoid potential conflictions.. Thank you for pointing this out! pos += 2 should be put into the hasOwnProperty block. Using [pos] = key instead of push was originally for performance reasons. However, I agree that we should use push instead since that's the case why push is designed, and the performance difference between them can be neglected.\nA PR is welcome!. It's a little confused whether score should be the key or the value of an object (at least more confused than mset for a hash key).\nSince ioredis flattens arguments, the following form is supported:\njavascript\nredis.zadd('key', [17, 'a'], [18, 'b'], [19, 'c'])\nGiven that I don't think the transformer is necessary.. I'm aware of this fact. It's just not straightforward compared with the transformer of hmset for the hash key, which itself is an object.\nAnyway, I'd like to accept a PR for that, as long as the transformer keeps the same format with hmset that only takes effect when the argument length is 2 and supports both Map and plain object.. \ud83c\udf7b . There should not be any difference. Please refer to the documentation on connect-redis: https://github.com/tj/connect-redis#custom-redis-clients. javascript\nvar session = require('express-session');\nvar RedisStore = require('connect-redis')(session);\nvar Redis = require('ioredis');\nvar redis = new Redis.Cluster([{\n  host:'192.168.0.12',\n  port:123\n}]);\napp.use(session({\n    store: new RedisStore({ client: redis }),\n    secret: 'keyboard cat'\n}));\nI didn't try but the code above should do the trick.. @tnguyen14 No, you don't. What did you get when executing cluster slots with redis-cli?. Try getting masters after the ready event:\njavascript\nvar cluster = new Redis.Cluster(/*...*/)\ncluster.on('ready', function () {\n  console.log(cluster.nodes(\"master\").length)\n}). When using Promise#spread(), you should make sure the fulfillment value is an array or an iterable object. Please refer to http://bluebirdjs.com/docs/api/spread.html for details.\nWhy do you need spread here instead of then?. \u76ee\u524d\u6ca1\u6709\uff0c\u4e0d\u8fc7\u6b22\u8fce PR. @shaharmor Sounds good to me.. Released with v3.1.0 \ud83c\udf7b . Yes, custom commands are supported to be sent within pipelines. See https://github.com/luin/ioredis/blob/master/test/functional/pipeline.js#L103 for details.. Hi @christianpv! Thank you for the pull request.\nI checked out your comments in redis-parser, however, I'm wondering what error did you encounter. Although /lib/hiredis.js requires hiredis, lib/parser.js wraps require('./hiredis') in a try-catch block, so no error would occur if 'hiredis' isn't specified as the parser name, and even it is, an error message will be printed to ask you to remove the name option.. I created a repo trying to reproduce the error: https://github.com/luin/test-ioredis-parser.\nWhen I ran node index.js, the output are:\nHiredis parser is abandoned in ioredis v3.0, and JavaScript parser will be used\nAnd no error occured. Is it your case?. @christianpv Just as @BridgeAR said, you bundled all js files into one file. It's pretty uncommon in the backend world. Ignore the node_modules folder should solve the problem.. @christianpv It makes sense to bundle your code into a single file, but it's not common that the code includes third-party modules as far as I know. For example, it's impossible to include native modules written in C/C++ into the output file (I'm a little curious about how you did since hiredis itself is a native module).\nhttps://github.com/DxCx/webpack-graphql-server uses webpack-node-externals to ignore node_modules.\nDropping supports for older Node.js versions is a breaking change, so redis-parser v3 will be included in the next major version of ioredis. I will merge this pull request when we're prepared for that.\nAnyway, I would recommend you to ignore files under node_modules folder currently.. Sounds good! A PR is welcome.. Thank you for the pull request! There are two missing trailing commas. Could you add them?. redis.pubsub('NUMSUB', channelName)\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: Evgeny Bondarenko notifications@github.com<mailto:notifications@github.com>\nSent: Tuesday, June 13, 2017 8:09 PM\nSubject: [luin/ioredis] getting number of active subscribers (#483)\nTo: luin/ioredis ioredis@noreply.github.com<mailto:ioredis@noreply.github.com>\nCc: Subscribed subscribed@noreply.github.com<mailto:subscribed@noreply.github.com>\nHi! I am a bit stuck on getting a number of active subscribers to channel. It should be possible withPUBSUB NUMSUB [channel] https://redis.io/commands/pubsub but I can't find interface for this in ioredis :(\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/483, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_suOJjGAyPo2DD6deRrYadX5McHDks5sDnv8gaJpZM4N4ZV7.\n. ioredis fetches the role of nodes asynchronously and will emit \"ready\" when successful. So Cluster#nodes() may not return the correct roles if called before the \"ready\" event.. Did you invoke SELECT command or set the \"db\" option explicitly?\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: Varun Dixit notifications@github.com\nSent: Sunday, June 18, 2017 9:43:37 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] ReplyError: ERR SELECT is not allowed in cluster mode (#485)\nSeeing SELECT not allowed in cluster mode every time we connect to Redis.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/485, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_vYD8hAZskbFknLRvxnSIRu3zMyuks5sFSmJgaJpZM4N9f87.\n. Thank you for the report! Could you please enable the showFriendlyErrorStack option so we can see where the SELECT command is sent via error stack?\nHere's the example code:\njavascript\nnew Redis.Cluster(yournodes, {\n  redisOptions: {\n    showFriendlyErrorStack: true\n  }\n}). That's strange. Are you running other Redis instances at the same time causing the error? Anyway, I'll use the code you provided and try to reproduce the problem.. @vdixit14 I did some trivial modifies on your code to use multiple redis instances in the same host: https://gist.github.com/luin/69d40a36c7466793c934172382e04ea9. I used https://github.com/Grokzen/docker-redis-cluster for running six cluster nodes and here's the output I got:\nRedis host: 127.0.0.1 port: 7000\nRedis host: 127.0.0.1 port: 7001\nRedis host: 127.0.0.1 port: 7002\nRedis host: 127.0.0.1 port: 7003\nRedis host: 127.0.0.1 port: 7004\nRedis host: 127.0.0.1 port: 7005\nUsing redis cluster connection with 6 nodes\nRedis is ready to work hard!\nI didn't see any error in my setup. Do I miss something?. Read/Write scaling is only supported in the Cluster mode. However, you may use sentinels options for the failover support of a Sentinel setup.. No, ioredis doesn't support that as it's not supported by Redis. . Thank you for the patch! \ud83c\udf7b . You may try running the MONITOR command on a redis-cli to see whether the commands sent to the Redis server are correct.. It seems you used async.map in a wrong way. The code should be:\njavascript\nasync.map(records, function(record, next) {\n  cluster.set(record.key, record.value, 'EX', expiry, next);\n},. Nice catch! As for me, it makes sense to hide all passwords in the logs. What do you think @shaharmor @AVVS ?. Sorry for the late response. The issue is caused by the select command failed because the connection is manually closed. It should be related to line 32 in event_handler.js. Add a catch for the errors that have the message of CONNECTION_CLOSED_ERROR_MSG and ignore them should solve the problem. However, there are other three selects in this file, not sure whether we need to catch the same errors for them.. Released in v3.1.3. What version of Redis server are you using? Is it the official version? It seems that select command is not supported in that instance, so selecting database will raise the error.. createClient is an alias for new Redis to stay compatible with the redis module. It's deprecated and may be removed in the future major updates.\nIt's recommended to use new Redis instead for the new code.. Thank you for the pull request!\nIt's a little confusing that the stunnel option has nothing to do with stunnel itself (Although it solves the problem that the hosts resolved by the Redis Cluster are not the correct addresses that the clients should connect to, which is caused by stunnel).\nAs for me, a general option name would make more sense (e.g. nodeAddressMap). Anyway, I'm curious whether there's a better way to setup a redis cluster behind an SSL proxy which is transparent to the clients.. redis.subscribe doesn't support wildcard. psubscribe should do the trick. Refer to https://redis.io/commands/psubscribe for details.. scanStream returns a typical Node.js stream. So, of course 'data' event will be emitted multiple times. When all data are received, \"end\" event will be emitted. Refer to https://github.com/luin/ioredis#streamify-scanning for details.. Redis Modules provide commands with the similar interface with the native commands. For example, redis-graph offers GRAPH.CREATENODE command, so in ioredis, you should be able to use it via call method:\njavascript\nredis.call('GRAPH.CREATENODE', 'graph', 'label')\n. Just like other commands: redis.geoadd('l', 'a'). Did you get any error?. javascript\nredis.set('key', 100, 'EX', 10);. Could you provide a test case for this? The following code:\n```javascript\nconst Redis = require('ioredis')\nconst redis = new Redis()\nconst redisCmds = [\n  [\n    \"set\",\n    \"key1\",\n    \"data1\"\n  ],\n  [\n    \"zadd\",\n    \"key2\",\n    1,\n    \"data2\"\n  ],\n  [\n    \"zadd\",\n    \"key3\",\n    1,\n    \"data3\"\n  ],\n  [\n    \"zadd\",\n    \"key4\",\n    1,\n    \"data4\"\n  ]\n]\nconst pipeline = redis.multi(redisCmds)\npipeline.exec(function () {})\nconsole.log(redisCmds)\n```\nLogs:\njavascript\n[ [ 'set', 'key1', 'data1' ],\n  [ 'zadd', 'key2', 1, 'data2' ],\n  [ 'zadd', 'key3', 1, 'data3' ],\n  [ 'zadd', 'key4', 1, 'data4' ] ]. @ccs018 Thank you for pointing this out! Just released v3.1.2. Sorry for not releasing this version when the fixed had been made.. Does upgrading to redis-parser 3.0 reduce the CPU usage?. Could you please post the .cpuprofile files here for analyzing?. I recommend you to use https://github.com/node-inspector/v8-profiler for profiling JavaScript code:\njavascript\nconst profiler = require('v8-profiler');\nprofiler.startProfiling('1');\n// Your code here\nconst profile1 = profiler.stopProfiling();\nprofile1.export(function(error, result) {\n  fs.writeFileSync('profile.cpuprofile', result);\n  profile1.delete();\n});\nPost the profile.cpuprofile here and I'd like to help you find which part of your code slow down the program.. Well...since what it generated not only shows the CPU usage of JavaScript code but also C++ code.\nFrom the summary:\n```\nticks  total  nonlib   name\n  55221   22.4%   24.0%  JavaScript\n  161754   65.7%   70.2%  C++\n   9564    3.9%    4.1%  GC\n  15573    6.3%          Shared libraries\n  13483    5.5%          Unaccounted\n```\nThe major CPU usage (65.7%) occurred in C++ code. And for the C++ part:\n```\nticks  total  nonlib   name\n  72399   29.4%   31.4%  write\n  31416   12.8%   13.6%  syscall\n   3456    1.4%    1.5%  v8::internal::IncrementalMarking::Step(unsigned long, v8::internal::IncrementalMarking::CompletionAction, v8::internal::IncrementalMarking::ForceCompletionAction, v8::internal::StepOrigin)\n```\nwrite cost many ticks, and \"Bottom up\" section showed where did these write came from. So it doesn't mean LazyCompile: *Command.transformReply /app/node_modules/ioredis/lib/command.js:207:45 takes 29443 ticks here.. @shaylevi2 Yes, that's correct. For the log, 40.7% + 8.9% writes came from ioredis. It happens when converting buffers to strings (which is necessary since the data from the socket is buffer).\nHope that answers your question.. Closing this. Let me know if there are further questions.. \u6570\u636e\u5e93\u64cd\u4f5c\u53ea\u80fd\u5728\u540e\u7aef\u5b9e\u73b0\uff0c\u6ca1\u6709\u529e\u6cd5\u5728\u524d\u7aef\u64cd\u4f5c\u7684\u3002. Nice catch! What about using Object.keys to get the keys? See \"Object.keys for loop\" in https://jsperf.com/object-keys-vs-for-in-with-closure/3. We are using ioredis connecting to tens of Redis instances (each instance has 256GB memory allocated) and the load is pretty heavy. However, we haven't met the memory issue caused by args & keys properties.\nAnyway, I'll try to remove these properties from the commandQueue this weekend.. Finding out several features depends on the args. For example, the autoResendUnfulfilledCommands option, the reconnectOnError option and the fillSubCommand function in the reply handler. It's a little tricky to delete this property before pushing commands into the commandQueue.. \u53ef\u4ee5\u53c2\u8003 Redis \u5b98\u65b9\u6587\u6863\uff0chttps://redis.io/commands. Thank you for reporting this. This issue has been discussed and noted on the README: https://github.com/luin/ioredis#transparent-key-prefixing.. \u6ca1\u592a\u770b\u61c2...\u8981\u4e0d\u8bf4\u4e2d\u6587... bind 0.0.0.0 \u5e94\u8be5\u5c31\u53ef\u4ee5\u4e86\uff0c\u9047\u5230\u4ec0\u4e48\u95ee\u9898\u4e86\u5417. \u54e6\u554a\u660e\u767d\u4e86\uff0cRedis \u7684\u96c6\u7fa4\u8981\u6c42\u7ed1\u5b9a\u5230\u5916\u90e8\u8bbf\u95ee\u7684\u7f51\u7edc interface\u3002\u6bd4\u5982\u5982\u679c\u662f\u5185\u7f51\u8bbf\u95ee\uff0c\u5c31\u7ed1\u5b9a\u5230\u5185\u7f51\u5730\u5740\uff0c\u5982\u679c\u662f\u5916\u7f51\u8bbf\u95ee\uff0c\u5c31\u7ed1\u5b9a\u5230\u5916\u7f51\u5730\u5740\u3002\u56e0\u4e3a\u8fd9\u4e2a\u7ed1\u5b9a\u7684\u5730\u5740\u4e0d\u4ec5\u662f\u7528\u6765\u5185\u90e8\u901a\u8baf\u7684\uff0c\u800c\u4e14\u8fd8\u8981\u8fd4\u56de\u7ed9\u5ba2\u6237\u7aef\uff0c\u7531\u5ba2\u6237\u7aef\u76f4\u8fde\u3002\u4e0d\u5199 bind \u7684\u8bdd\u9700\u8981\u67e5\u4e00\u4e0b\u5bf9\u5e94 Redis \u7248\u672c\u7684\u5b98\u65b9\u6587\u6863\u4e86\uff0c\u6211\u8fd9\u8fb9\u8bb0\u4e0d\u592a\u6e05\u4e86. Refer to the retryStrategy option for details. By the way, the event loop was probably not blocked (if it was, you should notice the CPU usage of the process reached nearly 100%). Check the middleware or HTTP callbacks in your App to see whether the request handler will keep waiting for redis commands.\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: Michael Thai notifications@github.com\nSent: Tuesday, August 29, 2017 2:11:07 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] Server Blocked Upon Redis Failure (#514)\nHi,\nI noticed when my Redis server fails or shuts down unexpectedly, my server is unable to accept any requests. It seems like the event loop is blocked with retry attempts to reconnect to Redis. Is there a way the server can continue to process requests/responses while periodically attempting to reconnect to the Redis server? Thanks!\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object._errnoException (util.js:1041:11)\nat _exceptionWithHostPort (util.js:1064:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1153:14)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/514, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_htw-qi6OyyikWVux8nct90wCbXLks5scwK6gaJpZM4PE3ei.\n. It's pretty straight to wrap the result of smembers in a Set:\njavascript\nconst set = new Set(await redis.smembers())\nDoes this work for you?. I'm afraid there's no way to cancel BRPOP since Redis doesn't provide an interface for it. The timeout argument of BRPOP is designed for this use case as for me \ud83d\ude06 . A client only has a single connection to Redis. If a BRPOP timeouts, the connection is able to process subsequent commands (e.g. another BRPOP).. You need to turn on the enableOfflineQueue option in the redisOptions property.. I'm closing this. Feel free to reopen it if the problem still exists.. Sounds interesting! I haven't used join-monster before, but I'd like to merge a pull request for the feature.. Thanks for the details. It's indeed a little complex to map a graph request to a Redis structure. The most important part is, there's not a standard on how to store relations in Redis. For instance, m-m can be implemented in Redis using set/string or zset, so it's impossible to decide which command should be used to resolve the graph query.\nUsing the graph module may be a way to solve the problem. However, I noticed there's a module/standard about implementing the relations with Redis: https://github.com/soveran/ohm. I'm not sure but it may be easier if we build the query resolver on the top of it.\nA question is why MULTI is needed here. Transaction in Redis is a simple layer to make sure that the commands are executed sequentially. However, if any commands fail, other commands will be executed normally.. A pipeline would be enough in this case:\njavascript\nconst redis = new Redis()\nconst ret = await redis.pipeline().get('foo').hget('hash', 'key').exec(). It should be a network issue that returned by the system when ioredis is trying to establish a TCP connection. The error means the address is not available. When receiving network errors, ioredis will try to reconnect to the server, that's why you get many error alerts. You may want to disable the retry strategy and enable the debug mode to see why the error happens (wrong IP or port maybe).\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: patrikx3 notifications@github.com\nSent: Tuesday, September 12, 2017 5:53:10 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] EADDRNOTAVAIL (#519)\nCIao!\nHow are you?\nDo you think what this error is?\nIt is making me crazy. I get like (my console.error is connected to my e-mail and I get like 10000 email's.)\n const setting = {\n           url: '\"url\": \"redis://:password@redis.patrikx3.com:6379/0\"'\n }\n const client = new Redis(settings.url);\n client.on('ready', async() => {\n                console.info(`${consolePrefix} ready`)\n                this.client = client;\n                resolve();\n            })\n  client.on('error', async(e) => {\n                console.error(`${consolePrefix} error`)\n                reject(e);\n            })\n\nSo I get like 1000 emails:\n2017-9-11 23:41:31\nPID: 8981\nJSON:\n{\n  \"0\": \"[ioredis] Unhandled error event:\",\n  \"1\": \"Error: connect EADDRNOTAVAIL 127.0.0.1:6379 - Local (127.0.0.1:0)\\n    at Object._errnoException (util.js:1041:11)\\n    at _exceptionWithHostPort (util.js:1064:20)\\n    at internalConnect (net.js:941:16)\\n    at net.js:1036:9\\n    at _combinedTickCallback (internal/process/next_tick.js:131:7)\\n    at process._tickCallback (internal/process/next_tick.js:180:9)\"\n}\nShould, I just drop this error? It looks like working.\nOr I am doing a loop somewhere?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/519, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_okhMWvrbMGHLIKJc_nZ0KLqt-Zeks5shavGgaJpZM4PT0jm.\n. A Cluster will create multiple Redis instances, each instance connecting to a Redis server of the cluster. Every instance has retryStrategy set to null, which means ioredis won't auto reconnect when the connection is closed. Instead, ioredis will refresh the nodes in the cluster and create the new instance connecting to the new server.\nYou may want to listen to the -node and +node event to know when a node is destroyed or created.. PR is welcome! To your questions:\n\nwhat the redaction string should be? (in my example I used )\n\nI'd recommend printing the heading several chars of the large argument to provide more details for developers to debug. For example: foobarfoobarfoobar<...8292 more chars>. (I'm not a native speaker so I'm open to any suggestions.\n\nwhat should be the length threshold for string threshold?\n\n50~200 should be enough as for me.\n\nshould the threshold be configurable?\n\nI don't think an option is needed for that.\n\nshould there be somekind of config option and/or ENV var that can be used to turn off the redaction completely?\n\nThe same answer as above. Let's make it default.. Nice patch! However, it seems that these changes may have a impact on the performance even when the debug mode is off (e.g. truncating strings).\nI just found out that the debug module supports custom formatters. What about define a custom formatter for the arguments?\nFor example:\n```\nconst createDebug = require('debug');\ncreateDebug.formatters.a = (v) => {\n  return genRedactedString(v, len);\n};\nmodule.exports = createDebug;. Awesome! Makes sense to me. I'll do some tests this weekend, and merge then.. Released as v3.2.0. \ud83c\udf7a . Maybe this.client = new Redis.Cluster(redisConfig.cluster.cluster)?. Never used REJSON module before. However, redis.call should work well with all redis modules. For example:\njavascript\nvar redis = new Redis()\nredis.call('JSON.GET', 'foo'). Could you confirm whether this issue is transaction related? What about a single redis.get(key) before transaction?. Haven't used the tunnel-ssh module before, but the code looks good to me. What error did you get?. Could you enable the debug mode and post the logs here?\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: pranavsreedhar92 notifications@github.com\nSent: Thursday, October 5, 2017 3:43:37 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] Cluster mode not fetching any data. (#528)\nAble to get data when connected to single instance. But when connecting to cluster not able to fetch any data.\nvar Redis = require('ioredis');\nvar cluster = new Redis.Cluster([\n{\nport: \"\",\nhost: \"\",\npassword: \"\"\n}\n]);\ncluster.zrangebyscore(redisKey, sTimetoUtcMilli, eTimetoUtcMilli, function (err, reply) {\n    if (reply) {\n        console.log('Success from zrangebyscore');\n        console.log(reply);\n    } else {\n        console.log(err);\n        callback(err);\n    }\n});\n\nThanks in advance.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/528, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_j-gLfg6axWnEd5Drq0-2UpH00aSks5spIipgaJpZM4PurO2.\n. The logs didn't show anything helpful. It seems there are some issues in the cluster setup. Could you try to run cluster info in one of the nodes in the cluster and show me the result?. Did you call cluster.disconnect() or cluster.quit() manually? It helps if you could provide a minimal reproducible example.. Please enable the debug mode to see whether the logs help.. Your app had some troubles connecting to some of the nodes (see the timeout errors in the logs), and that's the cause of the issue.. I'm closing this since it seems not related with ioredis. Feel free to reopen this issue if the problem still exists.. You can just use call:\njavascript\nredis.call('ft.search', 'my-index', '@text: foo', 'LIMIT', 1). Ordering is guaranteed by Redis. You can rely on the order of the results.. Hi @mjh1 ! This is the expected result and documented in https://github.com/luin/ioredis#running-commands-to-multiple-nodes. Refer to the examples of the above link for how to aggregate results on the client side.. '\"\"' should simply be ''. The former one is a non-empty string whose content is \"\".. Thank you for pointing this out! Just published.. Sorry for the late response.\nThat's not a limitation of Redis. It's just not easy to support chaining custom commands in the cluster setup: ioredis always try evalsha first when sending custom commands, and when the command fails, eval will be sent instead. However, when it comes to pipelines, to make sure all commands in a pipeline will be received by Redis in order, ioredis sends script exists command to check if all lua scripts have loaded in the target redis node and will load the scripts which haven't loaded before sending the pipeline.\nIn the cluster setup, the nodes may change (e.g. failover happens) during the process of loading scripts, so it's hard to make sure that the scripts are loaded to the correct redis node. That's why this feature haven't been implemented.. @manast Script cache may be cleared by the SCRIPT FLUSH command.  Despite that, calling SCRIPT LOAD when defining commands indeed solves the problem (we also need to resend all SCRIPT LOAD again when a new connection has been made). Good idea! \ud83d\udc4d . I'm not sure but should this be io.adapter(socketredis({pubClient: redis, subClient: redis})) instead of io.adapter(socketredis(redis))?. \u53ef\u4ee5\u8d34\u4e00\u4e0b sentinel down-after-milliseconds mymaster \u7684\u914d\u7f6e\uff0c\u5982\u679c\u662f 60000\uff0c\u90a3\u4e48\u9700\u8981\u7b49\u5f85 1 \u5206\u949f\u7684\u65f6\u95f4\u6240\u6709 Sentinel \u624d\u80fd\u8fdb\u884c failover\u3002\u53ef\u4ee5\u8fde\u63a5\u4e0d\u540c\u7684\u54e8\u5175\u786e\u8ba4\u6bcf\u4e2a\u54e8\u5175\u662f\u5426\u90fd\u8fd4\u56de\u4e86\u65b0\u7684\u5730\u5740\u3002\u53e6\u5916\u53ef\u4ee5\u6253\u5f00 DEBUG=ioredis:* \u6765\u67e5\u770b\u65e5\u5fd7\u4fe1\u606f. ioredis should work well with async/await.. Did you disable the keepAlive option? Seems that enabling keepAlive (which is default) should solve the problem.. Refer to https://redis.io/topics/notifications for details.. Hi @slavaGanzin, it seems the benchmark is a little not fair that the \"pipeline\" group sends 1000 commands per test while the \"set\" group sends only 1.\n```javascript\nvar pipelineQuery = []\nfor (let i = 0; i < 100; i++) {\n  pipelineQuery.push(['set', 'foo', 'bar'])\n}\nsuite('multi: SET foo bar', function () {\n  set('mintime', 5000);\n  set('concurrency', 300);\nbefore(function (start) {\n    waitReady(start);\n  });\nbench('javascript parser + dropBufferSupport: true', function (next) {\n    redisJD.multi(pipelineQuery).exec(next);\n  });\nafter(quit);\n})\nsuite('SET foo bar', function () {\n  set('mintime', 5000);\n  set('concurrency', 300);\n  before(function (start) {\n    waitReady(start);\n  });\nbench('javascript parser + dropBufferSupport: true', function (next) {\n    let pending = pipelineQuery.length\n    pipelineQuery.forEach(([command, arg1, arg2]) => {\n      redisJDcommand;\n    })\n  });\nafter(quit);\n});\n```\nThe above code shows the following result:\n```\n                      multi: SET foo bar\n           1,067 op/s \u00bb javascript parser + dropBufferSupport: true\n                  SET foo bar\n         632 op/s \u00bb javascript parser + dropBufferSupport: true\n\n```\nWhat's more, pipeline guarantees commands in a pipeline will be executed in order.. What's your test code? Here's mine which works:\n```javascript\nconst redis = new Redis()\nredis.call('multi', console.log)\nredis.call('set', 'foo', 'bar', console.log)\nredis.call('get', 'foo', console.log)\nredis.call('exec', console.log)\n```\nLogs:\nnull 'OK'\nnull 'QUEUED'\nnull 'QUEUED'\nnull [ 'OK', 'bar' ]. \u8fd9\u6837\u554a...\u540e\u6765\u6362\u4e86\u4ec0\u4e48\u7248\u672c\u5462\uff1f\u81ea\u52a8\u5316\u6d4b\u8bd5\u662f\u5728 0.10.16 \u73af\u5883\u4e0b\u8dd1\u7684\uff0c\u6211\u624b\u5de5\u518d\u8bd5\u8bd5\u770b\u3002. Done. Thank you for pointing this out!. You can still passes password option as how you connect to a single redis instance:\njavascript\nvar redis = new Redis({\n  sentinels: [{ host: 'redis-sentinel', port: 26379 }],\n  password: \"My-highly-secured-password\",\n  name: 'mymaster'\n});\nHowever, it's required that all your redis instances' password are same.. zrangebyscore('test2', 0, 100,'WITHSCORES','Limit', 0, 10);. Is there a standard for this URL schema? For example, this is what ioredis follows for the redis:// schema: https://www.iana.org/assignments/uri-schemes/prov/redis. \u51fa\u73b0\u8fd9\u79cd\u95ee\u9898\u662f\u7531\u4e8e client <--> redis server \u7684\u8fde\u63a5\u65ad\u5f00\uff0c\u8fd9\u65f6 ioredis \u4f1a\u91cd\u65b0\u4ece sentinels \u4e2d\u53d6 master \u5730\u5740\uff0c\u5982\u679c 4 \u4e2a sentinel \u90fd\u4e0d\u80fd\u8fde\u63a5\u5c31\u4f1a\u62a5\u8fd9\u4e2a\u9519\u8bef\u3002\u5e94\u8be5\u662f\u7f51\u7edc\u95ee\u9898\uff0c\u5982\u679c\u60f3\u6df1\u5165\u4e86\u89e3\u53ef\u4ee5\u901a\u8fc7 DEBUG=ioredis:* node yourapp.js \u6765\u6253\u5f00\u65e5\u5fd7\u3002. Could you provide a minimal reproducible example so I can test on my side?. \u55ef\u662f\u7684\uff0c\u4e0d\u652f\u6301\u8fd9\u4e2a\u529f\u80fd\u3002\u662f\u4ec0\u4e48\u573a\u666f\u9700\u8981\u8fd9\u4e2a\u5462\uff1f. Hi @gigi, Could you post the debug logs here? The issues may be different since the cluster state is failed in the original poster's case.. enableOfflineQueue in redisOptions is set to true by default and can't be changed in v4. It has no connection with enableOfflineQueue in the cluster options.. Glad the issue is resolved!. @tony-gutierrez Just added. Thank you for pointing this out!. @heri16 v4.0.0-2 adds a fix for this that will refresh the slots periodically.. ioredis doesn't fetch all the master list periodically from the servers, only when the MOVED or ASK error is received. That makes the result of #nodes() not up to date though.\nWe can refresh the master list in the background periodically to solve the problem. I'll look into that.. @ccs018 Hi, sorry for the really late response.\nAccording to the official documentation https://redis.io/topics/cluster-spec#clients-first-connection-and-handling-of-redirections, ioredis will refresh the slots only when a MOVED error is received. That means even when a node is down temporarily and it's not removed from the cluster, ioredis will disconnect it and won't reconnect.\nI'm wondering whether this should be a problem since once sending a command that belong to this node, ioredis will receive a MOVED error, at which time ioredis refresh the slots information and connect to the lost node.. @ccs018 The use cases you described won't be affected by setting enableOfflineQueue to true since offline queue only works when the connection is live or a retry attempt is happening. When a node is lost, ioredis will close the connection immediately because retryStrategy is null, so no more command will be added into the offline queue.. \u8fd9\u4e2a\u548c ioredis \u65e0\u5173\uff0c\u662f JavaScript \u8bed\u8a00\u57fa\u7840\u95ee\u9898\u3002\u89e3\u51b3\u65b9\u6848\u975e\u5e38\u591a\uff0c\u6bd4\u5982\uff1a\njavascript\nasync function () {\n  const keys = await redis.keys('*')\n  return await keys.map(key => redis.hgetall(key))\n}\n\u6216\u8005\u76f4\u63a5\njavascript\nredis.keys('*').map(redis.hgetall.bind(redis)). stream.once(CONNECT_EVENT, eventHandler.connectHandler(_this)) will trigger _this.setState('connect'), which will emit connect event so that _this.once(CONNECT_EVENT, connectionConnectHandler) can catch the event and resolve the promise.\nWe can just update Redis#setStatus(). When a 'connect' status is received, checking whether we are connecting via tls or not, and emit secureConnect or connect correspondingly.. Oops, sorry for the late response. Didn't see this pull request on my notification list. Yes, this is indeed a bug that this pull request should be merged earlier.\nThis issue has been solved in the commit https://github.com/luin/ioredis/commit/f0c600be8ee5edaa1f24d64ff512f507dab04741.. Sorry for the late response. I omitted this issue. My apologies.\nRunning conn.nodes immediately after conn = new Redis.Cluster() doesn't work since ioredis haven't fetched the slot info from the Redis cluster. The recommended way to do this is to call Cluster#nodes() after the ready event:\njavascript\nconn.on('ready', () => {\n  conn.nodes(\"master\").map((node) => {\n      return node.keys(\"*:*\");\n  })\n})\nDoes this solve your issue @JoeNyland ?. @kierxn Could you enable showFriendlyErrorStack option to see which line of the code cause the Connection is closed. error?:\njavascript\nnew Redis.Cluster([\"cluster.id.clustercfg.euw1.cache.amazonaws.com\"], {redisOptions: {showFriendlyErrorStack: true}});. Connection is closed. happens when a command has been sent to a node whose status is \"end\". \nThere are several reasons that a node may come to the \"end\" status. The most common one is when a failover happens in the cluster (a master is down and a slave become the master). For your case, a KEYS command may block the Redis server for seconds, which may be considered to be down, and a failover process will be triggered.. Thank you for the pull request! I'm not sure whether it is valid to send arguments with the type of number. According to the RESP documentation: https://redis.io/topics/protocol:\nA client sends to the Redis server a RESP Array consisting of just Bulk Strings.. Pull requests for this change are welcome! However, it seems that `Buffer.from()` is not supported until Node.js 6.x, which will be a breaking change if we replace `new Buffer()` with it since we provides support for Node.js 0.10.16. Can we check if the `Buffer.from()` exists, and if not, fallback to `new Buffer()`?. ```javascript\n\nfunction scan (count) {\n  return redis.hscan('MY_HM_EVT', count || 0, 'MATCH', '_i', 'COUNT', 9999999).then((nextCount, results) => {\n    console.log(results)\n    return scan(nextCount)\n  })\n}\n``. Yes.. Hi @manast.\nFor the case 4, I tested locally and the behavior is working as expected (logs the result when reconnected). That's strange that ioredis just times out when the source list has elements. Could you try to callLLEN source` on a redis-cli when reconnected to see whether there are elements in the source list?\nioredis reconnecting to the server forever, so all commands will be blocking when disconnected. This behavior makes sense when it comes to an application that the connection will recover shortly (<10s~1min).\nSetting retryStrategy to null and handling reconnection manually in the close event may solve the problem:\ncase 1: prints errors immediately.\ncase 2: prints errors immediately.\ncase 3: prints errors immediately when disconnected.\ncase 4: prints errors immediately.. The two methods should be same. I tested locally and the first method works for me. Which ioredis version did you use?. Could you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?. @stevehorn No. Deleting a nonexistent key is a noop.. The promise of quit() resolves when the QUIT command has processed by the Redis server, just like the other commands. There isn't special handling for quit() that only resolves when the connection has been closed.\nI'd recommend listen to the end event when you need trigger only when the connection has end:\njavascript\nclient.on('end', () => console.log('end'))\nclient.quit(). Actually, you can't. The value of each field in a hash key should be a string. Redis doesn't support other data types other than that.\nYou may want to use JSON.stringify({/* your object */}) and store the result in Redis.. \u55ef\uff0c\u662f\u53ef\u4ee5\u4fdd\u8bc1\u987a\u5e8f\u7684\u3002\u56e0\u4e3a\u662f\u5355\u94fe\u63a5\uff0c\u6240\u4ee5\u5148\u53d1\u7684\u4f1a\u5148\u5230 Redis\uff0c\u4e5f\u4f1a\u88ab\u5148\u5904\u7406\u5e76\u8fd4\u56de\u3002 . Sorry for the late response! Yes, adding commands to pipeline directly should do the trick. XADD command will be added as soon as it's added to the redis-commands package.. Hi, the behavior has been fixed in the beta release v4.0.0-0. Thank you for pointing this out!. This error is not ioredis related. It happens when calling a multiple-keys command (such as mget) with keys that not belong to the same slot.. Just looked into the code of connect-redis. It seems connect-redis doesn't support Redis cluster, so you need use new redis instead of new redis.Cluster.. Fixed in v4.0.0-2. Thank you for pointing this out!. What ioredis version & Redis version did you use?. It turns out the first argument for the call method is special that it should be a string. The following two are valid:\njavascript\nredis.call('command', 'info', 'set')\nredis.call('command', ['info', 'set']) . This code is correct. What problem did you encounter?. The definition may be out of date. Every command returns a Promise no matter whether the last argument is a function.. It seems to be a issue on Jedis, not ioredis. Closing. The default retryStrategy option waits for the recovering of the connection forever. You may want to pass a customized one in the case that the connection may be lost forever. Refer to https://github.com/luin/ioredis#auto-reconnect for details.. maxRetriesPerRequest has added in the beta relase v4.0.0-0. \ud83c\udf7a . @soulrebel What about implementing a heartbeat strategy that sending PING every several seconds and the connection will be considered lost once timed out?. Added a pull request trying to address this issue: https://github.com/luin/ioredis/pull/658. What's your thoughts? . Sorry for the late response. Could you enable the debug log (DEBUG=ioredis:* node yourapp.js) and post the logs here?. @luckyscript This is a bug in ioredis and will be fixed in the next release (v4.0.0-2). Currently you can ignore this issue safely.\nThis issue happens when resolved a master address successfully use the last sentinel in the provided sentinel list. When a reconnection happens, ioredis will connect to the next sentinel, which is the end of the list, thus a \"Unreachable\" error is emitted.\nThank you for pointing this out!. Sorry for the (really) late response. This issue has been resolved in #758. Let me know if there's any problem.. I'm not sure if it's possible to do that since Redis Cluster isn't a black box to the developers (unlike twemproxy & codis). There're many things developers should care about when using cluster (e.g. multiple-key operations, so developers should redesign there's key schema when switching to the cluster). Thus there are two separated classes Redis & Redis.Cluster for the two situations.\nI'm afraid providing a unified way may mislead developers into thinking they are transparent to the developers.. Released in v4.0.0-0. We using native Promise by default in this version.. \u4e00\u822c\u4e0d\u9700\u8981\u8fde\u63a5\u6c60\u3002\u4e0d\u9700\u8981\u624b\u52a8 quit\uff0c\u53ef\u4ee5\u76f4\u63a5\u4fdd\u6301\u957f\u8fde\u63a5. > Refresh slots upon a disconnection from one of the cluster nodes\nA disconnection won't lead the update of slots immediatially. A cluster will consider a master to be failing only after NODE_TIMEOUT has elapsed.\nSo most of time we only need refetch the slots in two different situations (https://redis.io/topics/cluster-spec#clients-first-connection-and-handling-of-redirections):\n At startup in order to populate the initial slots configuration.\n When a MOVED redirection is received.\nHowever, given a failing of a slave won't trigger a MOVED error, so refresh slots intervally help us solve the problem.\n\nRemove callback support? Not sure why we have to support callbacks if we support promises. If anyone really needs callback support, they can \"callbackify\" the promises themselves. Why do we need the entire library to support it?\n\nGood idea. I've seriously considered removing callback support. However, what makes me tend to keep callback support in this major version is that many built-in functions (fs.readFile) still follow callback-style, and util.promisify was absent until v8.0.0. The major frameworks (e.g. express, mocha) continue to be friendly to the callback style, so there should be many legacy codes prevent developers from migrating to the ioredis v4 if we drop callback support at this moment.\nWe may do this on the next major version, which only supports node >= v8 that support async functions.. Just release a beta version v4.0.0-0. The remaining issues will be resolved in the next beta version.. Did ioredis try to reconnect to the cluster when the error happened? Could you enable the debug logs (DEBUG=ioredis:* node yourapp.js) and post the logs here?. \u770b\u72b6\u6001\u662f\u6b63\u5e38\u7684\u3002sentinel \u6a21\u5f0f\u4e0b\uff0c\u6bcf\u6b21\u8fde\u63a5\u54e8\u5175\u83b7\u53d6\u5230 master \u5730\u5740\u540e\uff0c\u5c31\u4f1a\u65ad\u5f00\u548c\u54e8\u5175\u7684\u94fe\u63a5\u3002\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: GoBrianGo notifications@github.com\nSent: Tuesday, June 26, 2018 7:16:48 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] redis reconnect (#644)\n\u6211\u60f3\u95ee\u95ee\u6211\u94fe\u63a5redis\u603b\u662f\u65ad\u5f00\u91cd\u8fde\uff0c\u662f\u4ec0\u4e48\u539f\u56e0\u5462\u3002\u4e0b\u9762\u662f\u8f93\u51fa\u65e5\u5fd7\nioredis:redis status[10.12.34.21:6379]: reconnecting -> connecting +55ms\nioredis:redis status[10.12.34.22:26379]: [empty] -> connecting +0ms\nioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster) +0ms\nioredis:redis status[10.12.34.22:26379]: connecting -> connect +18ms\nioredis:redis status[10.12.34.22:26379]: connect -> ready +0ms\nioredis:connection send 1 commands in offline queue +0ms\nioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster) +1ms\nioredis:redis write command[0] -> sentinel(sentinels,mymaster) +17ms\nioredis:SentinelConnector sentinels [ { host: '10.12.34.21', port: '26379' },\n{ host: '10.12.34.22', port: 26379 },\n{ host: '10.12.34.21', port: 26379 } ] +20ms\nioredis:SentinelConnector resolved: 10.12.34.21:6379 +0ms\nioredis:redis status[10.12.34.22:26379]: ready -> close +17ms\nioredis:connection skip reconnecting since the connection is manually closed. +0ms\nioredis:redis status[10.12.34.22:26379]: close -> end +0ms\nioredis:redis status[10.12.34.21:6379]: connecting -> connect +3ms\nioredis:redis write command[0] -> info() +0ms\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/644, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_ji-C3GwpL825CYkTLhI5Xx_PIA-ks5uAhgggaJpZM4U3u05.\n. ioredis \u6709 pipeline \u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u4f18\u5316\u6279\u91cf\u7684\u547d\u4ee4\u3002\u56e0\u4e3a Redis \u662f\u5355\u7ebf\u7a0b\u6a21\u578b\uff0c\u8fde\u63a5\u6c60\u53ea\u80fd\u7f13\u89e3\u53d1\u9001\u547d\u4ee4\u7684\u5f00\u9500\uff0c\u5728 Node.js \u573a\u666f\u4e0b\u4f18\u5316\u4e0d\u5927. I\u2019m not sure but that should work. What about running DEBUG=ioredis:* node server directly?\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: GoBrianGo notifications@github.com\nSent: Wednesday, June 27, 2018 11:38:59 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] debug command (#645)\nhow can i set debug command in package.json\npackage.json\n\"scripts\": {\n\"debug\": \"DEBUG=ioredis:* node server\"\n},\nand then i use \"npm run debug\", it's not working.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/645, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_ustDTuwQISDvjsCV07KZL_lojx4ks5uAv5TgaJpZM4U5AeW.\n. Can setting the three keys via redis-cli reproduce the problem?. Well, that's strange. I don't think this problem is related with ioredis. Maybe asking in the redis repo? Run monitor in a redis-cli may help you debug the issue.. LGTM! We should also make Redis#connect() resolved only when the status is ready to keep the behavior same. I'll post a pull request for this.. Committed to https://github.com/luin/ioredis/pull/642 with cherry-pick. See also for the changes on Redis: https://github.com/luin/ioredis/pull/648. Just fixed in 4.0.0-1. Sorry for the inconvenient.. Hi, it seems that the default backlog can be changed at https://github.com/antirez/redis/blob/94658303e9ec5050189728fb8bc514ee682dd5fe/redis.conf#L101.. Not sure but I thought sentinels and typical redis servers share the same code according to the backlog part.\nKeeping a long live connection to sentinels is possible but is not trivial to implement. We'll think about that.. This is an issue on the TypeScript project. The response type should be Promise<'OK'>.. Which version of ioredis did you use? There should have GEOADD command.\n```javascript\nconst Redis = require('ioredis')\nconst redis = new Redis()\nconsole.log(redis.geoadd)\n```. \u53ef\u4ee5\u53c2\u8003 Redis \u5b98\u65b9\u6587\u6863\u3002https://redis.io/commands/set\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: Zhao Andy notifications@github.com\nSent: Thursday, July 5, 2018 1:15:29 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] redis.set('key', 100, 'EX', 10); (#655)\n'key'\u3001100\u3001EX\u300110\u5206\u522b\u4ee3\u8868\u4ec0\u4e48\u610f\u601d\uff1f\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/655, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_oYlpn7T_IFytF2gMOcL5Q54Dfcbks5uDaDxgaJpZM4VDQqf.\n. \u8fd9\u4e2a\u95ee\u9898\u53d1\u751f\u7684\u539f\u56e0\u662f redis.subscribe() \u540e\u8fd8\u6267\u884c\u4e86\u5176\u4ed6\u547d\u4ee4\u3002\u5982\u679c\u9700\u8981\u6267\u884c\u5176\u4ed6\u547d\u4ee4\u7684\u8bdd\u9700\u8981\u65b0\u5efa\u4e2a\u8fde\u63a5\uff0c\u5728\u65b0\u8fde\u63a5\u5904\u7406\uff0c\u4e0d\u8981\u590d\u7528\u7ed9 socket.io \u7684\u8fde\u63a5\u3002\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: zxtyjx notifications@github.com\nSent: Thursday, July 5, 2018 4:43:02 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] Connection in subscriber mode, only subscriber commands may be used (#656)\n\u6211\u5728\u4f7f\u7528socket.io\u8fde\u63a5redis\u96c6\u7fa4\u65f6\uff0c\u4f1a\u51fa\u73b0\u4ee5\u4e0b\u9519\u8bef\uff0c\u9ebb\u70e6\u5e2e\u5fd9\u770b\u4e0b\u662f\u600e\u4e48\u5bfc\u81f4\u7684\uff0c\u8c22\u8c22\uff5e\uff5e\uff5e\uff5e\uff5e\nUnhandled rejection Error: Connection in subscriber mode, only subscriber commands may be used\nat Redis.sendCommand (/home/socket-server/node_modules/ioredis/lib/redis.js:562:20)\nat tryConnection (/home/socket-server/node_modules/ioredis/lib/cluster/index.js:537:13)\nat Cluster.sendCommand (/home/socket-server/node_modules/ioredis/lib/cluster/index.js:479:3)\nat Cluster.publish (/home/socket-server/node_modules/ioredis/lib/commander.js:131:17)\nat /home/socket-server/node_modules/socket.io-redis/index.js:470:11\nat tryCatcher (/home/socket-server/node_modules/bluebird/js/release/util.js:16:23)\nat Promise.successAdapter [as _fulfillmentHandler0] (/home/socket-server/node_modules/bluebird/js/release/nodeify.js:23:30)\nat Promise._settlePromise (/home/socket-server/node_modules/bluebird/js/release/promise.js:566:21)\nat Promise._settlePromise0 (/home/socket-server/node_modules/bluebird/js/release/promise.js:614:10)\nat Promise._settlePromises (/home/socket-server/node_modules/bluebird/js/release/promise.js:693:18)\nat Async._drainQueue (/home/socket-server/node_modules/bluebird/js/release/async.js:133:16)\nat Async._drainQueues (/home/socket-server/node_modules/bluebird/js/release/async.js:143:10)\nat Immediate.Async.drainQueues (/home/socket-server/node_modules/bluebird/js/release/async.js:17:14)\nat runCallback (timers.js:789:20)\nat tryOnImmediate (timers.js:751:5)\nat processImmediate [as _immediateCallback] (timers.js:722:5)\n\u2015\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/656, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_i68b9W_naEoPtFI9t8_Xs8q8Mwbks5uDdGWgaJpZM4VDehC.\n. @vweevers This option will be disabled by default and it won't add any side effects when disabled.\n@ccs018 If you issue a command, The timeoutPerCommand interval will check if the response has been received every 500ms, and if not after the specified time (timeoutPerCommand option), the callback will be invoked with a timeout error.. @vweevers Keep-alive is enabled by default. Refer to https://github.com/luin/ioredis/blob/master/API.md for details on the options for that.. It's async. But the offline queue is enabled by default, which means commands sent before the connection has been established were queued and will be sent again.\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: hezjing notifications@github.com\nSent: Monday, July 9, 2018 10:33:26 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] new Redis() is synchronize? (#661)\nHi\nA Redis connection will be created when we create the instance like this,\nnew Redis(6380, '192.168.100.1', { password: 'password' });\nMay I know if this is a synchronize or asynchronize function? Is it correct to say that a connection is created right after this statement is executed?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/661, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_uLIK-QDxVSZQTK27O-ojyoLdP01ks5uEsD2gaJpZM4VG_uP.\n. \u5347\u7ea7\u5230 v4 \u5c31\u53ef\u4ee5\u4e86\u3002v3 \u4e2d redis.connect() \u4f1a\u5728 connect \u4e8b\u4ef6\u5c31 resolve\uff0cv4 \u4e2d\u662f\u5728 ready \u4e8b\u4ef6\u3002. \u5728 next tag \u4e0b\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: zengming00 notifications@github.com\nSent: Thursday, July 19, 2018 9:24:25 AM\nTo: luin/ioredis\nCc: \u5b50\u9a85; Comment\nSubject: Re: [luin/ioredis] \u8fde\u63a5\u540e\u7b2c\u4e00\u6b21\u67e5\u8be2\u62a5\u9519 (#665)\n\u73b0\u5728\u6700\u65b0\u7684\u5c31\u662f3.2.2\u5440\uff0c\u54ea\u6765\u7684v4?\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/665#issuecomment-406124578, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_nNVyN__EK86PWjxDdTbnvLyvDDwks5uH9_JgaJpZM4VUQvk.\n. \u9700\u8981\u542f\u52a8\u591a\u4e2a multi\u3002\njavascript\nredis.multi().rpush('a', '1').exec()\nredis.multi().rpush('a', '1').exec(). Thanks for the pull request!. Hi, could you enable the logs (DEBUG=ioredis:* node yourapp.js) and post the logs here?. Fixed. Will be releaseed in the next version. Thanks for pointing this out!. Yes, ioredis will end up including typings in the module when all the code have been migrated to TypeScript.\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: twawszczak notifications@github.com\nSent: Wednesday, July 25, 2018 10:14:24 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] v4 typings (#669)\nIs there a plan to include typings for lib inside build, or it still has to be in @typeshttps://github.com/types repo?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/669, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_n9yDNcqVV8eAeK-TDcbEJDwqD0pks5uKH1AgaJpZM4VgKef.\n. @egoarka The migration has already started (e.g. https://github.com/luin/ioredis/blob/master/lib/ScanStream.ts). Pull requests that migrating JS to TS are welcome!. Sounds interesting. BTW, Currently you can use the preferredSlaves option to specify priority of each slave.. Still have one or two bugs to be fixed. Will be released around 15th August.. Just released.. ioredis doesn't remove duplicated keys. Would you like make a pull request to add a note on the README?. Awesome! And thanks @vweevers for the advice!. Hi, thank you for pointing this out! This issue exists on v3.x and has been fixed on v4.x.. Just released v4.0.0. Closing this issue.. Hi @yuricodes! Are you using @types/ioredis? If you are, you may submit an issue on https://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/types/ioredis.. It seems the problem has been solved on https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/ioredis/index.d.ts#L47 @scf4 . What's the use case in which you want to get the master IP?. Thank you for the pull request! API.md is generated from the code. Could you please change the code first and generate API.md by running npm run generate-docs?. Thank you for pointing this out!\nHow often did you see this issue? Could you try v4.0.0-1 to see whether the issue exists?. What else commands besides LPUSH & BRPOP are used on the list? Did you notice any reconnections between apps and Redis server during that time?. I'm running a sample that does lpush and brpop, hoping we can reproduce the issue.\nThe major difference between v4 & v3 is that when a Redis server goes down, v3 will wait forever until the server goes back, while v4 only wait for ~20s and will emit an error after that. However, when the server goes back, both v3 and v4 will continue to work automatically. I'm wondering whether it is possible that the error handler in your app prevents self-healing happens (e.g. restarting the app by calling process.exit()).\nWhat's the typical qps of lpush? And are you using the official cluster (that need client-side sharding) or Sentinel?. Thank you for the information. Still not sure about that, but it may be related with the cluster setup. bgsave shouldn't be a problem unless it blocks IO for 10+ seconds, which is not possible with a 9gb instance.. Could you enable the debug mode (DEBUG=ioredis:* node yourapp) and post logs here?\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: James Dixon notifications@github.com\nSent: Tuesday, August 28, 2018 9:13:35 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] connecting to AWS ElasticCache (#689)\nHi, I'm moving resources from Azure to AWS and am having trouble connecting to Elasticache through ioredis.\nHere's my basic config:\nconst redisClient = new Redis({\n  port: process.env.REDIS_PORT,\n  host: process.env.REDIS_HOST\n})\nHere's my Elasticache instance:\n[image]https://user-images.githubusercontent.com/110114/44694844-2d983480-aa2d-11e8-8a81-2bc6a8ba410d.png\nWhen trying to connect, it just times out. The strange thing is that if I use node-redis with essentially the same config, it connects immediately.\nAnything special that needs to be done?\nThx!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/689, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_gDbRTCxorZVcaUkYpSN2VlQqs4eks5uVJk_gaJpZM4WOvqX.\n. Hi, thank you for pointing this out. Done.. Thanks for the pull request. I'm not sure how to deal with passwords that contain ':' or other symbols (e.g. '@') according to the standard. Node.js has an official module to exact the password from a URL. However, that module only introduces from Node.js v7.\nWe may have to refer to https://url.spec.whatwg.org/#concept-url-password for parsing the URL correctly.. \ud83c\udf7b . @lakano What the version of redis-commands installed in your project (you can see it via npm ls | grep redis-commands)?. Hi, haven't dug into this issue, but I remember there's an issue on the Redis repo about this issue: https://github.com/antirez/redis/issues/2527. The option announce-ip seems to have been added since Redis 4.x.. @jeremytm Sorry for the late response. I've already migrated the code to TypeScript and will submit a pull request for the NAT support (by adding a nat map option) in the next week. I'll let you know then.. @jeremytm Would you like to share the modification of the code that works for your case or just make a pull request?. @jeremytm Neat solution! A question for it though that since Redis Cluster returns IPs instead of hostnames, do you have to specify both IPs & hostnames? E.g. let's say rugby-cache-0001-001.xxxxxx.0001.use1.cache.amazonaws.com points to 172.189.1.2, the nat mappings should be:\n{\n  'rugby-cache-0001-001.xxxxxx.0001.use1.cache.amazonaws.com': {\n    '6379': 'ec2-xx-x-xx-xx.compute-1.amazonaws.com:6379'\n  },\n  '172.189.1.2': {\n    '6379': 'ec2-xx-x-xx-xx.compute-1.amazonaws.com:6379' // same as the above\n  },\n}. Could you enable the debug mode DEBUG=ioredis:* node yourapp.js and post the logs here?. Do you set the timeout option on the redis server? Run CONFIG GET timeout to check it out.. Could you post the code here that can reproduce the issue?. @callsea1 Could you run config get timeout to see whether an idle client timeout has been set on the redis server?. @simondutertre I've tested locally using the following code:\n```javascript\nconst Redis = require('ioredis')\nconst redis = new Redis.Cluster([{\n  port: 30001\n}, {\n  port: 30002\n}, {\n  port: 30003\n}])\nredis.on('connect', console.log.bind(console, 'abc'))\n```\nOnly one connect is emitted. Could you enable the debug mode DEBUG=ioredis:* node yourapp.js and post the logs here?. This issue has been solved in v4.2.0. Feel free to reopen this issue if the issue still exists.. @callsea1 That may be a different issue. Could you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?. A fix (WIP) is here https://github.com/luin/ioredis/pull/697. Waiting for tests to be added.. The previous solution solves the problem partially, though, a better way is to create a specialized connection for the subscription. I've updated this pull request with the new solution. Please take a look at it.. Have fully tested with socket.io-redis. Merged and released in v4.0.1. Feel free to report any issues in this. version. This is the expected result format that has been documented here: https://github.com/luin/ioredis#transaction.. Hi, the first error can be caused by the supplied password is incorrect, and the second warning is seen because a password is supplied but the redis server doesn't require a password.. This issue has been solved in v4.0.1. Feel free to reopen this issue if the problem still exists.. Refer to https://get-reddie.com/blog/redis4-cluster-docker-compose/ for details on how to connect to a redis server inside a docker. Let me know if it helps.. It really depends on your use case. If you don't want to lose any command and connection is pretty stable, keeping maxOfflineQueueSize to Infinity should not be a problem.. Hi, this is an issue on the redis side:\n127.0.0.1:6379> multi\nOK\n127.0.0.1:6379> hmset abc a b c\nQUEUED\n127.0.0.1:6379> set foo\n(error) ERR wrong number of arguments for 'set' command. That's not how punsubscribe work in Redis. punsubscribe is used to unsubscribe from patterns that are subscribed via psubscribe. Refer to https://redis.io/commands/punsubscribe for details.. That's not possible. What the use case that you want to pass a readonly connection to apps that want to write data to?. How do bee-queue work with a Redis connection which is read-only?. This change makes sense to me for fixing the issue. However, I'm wondering there may be a case that there is only one node in the cluster (or only one node that we can reach from the given node), this._readyCheck will be triggered endless.. The test above failed because the new code skips the reconnection. However, the test will make node2 alive again when reconnect (see the stub function of refreshSlotsCache). Since the node2 doesn't become alive, ioredis will pick a node random. So node1 will be picked, which will return 'OK' for both get foo & set foo bar since we don't specify the reply handler for these commands.. Do you think resolving the hostnames to IPs before connecting could solve the problem?. @AVVS Got it. Since Redis cluster doesn't support hostname at all (https://github.com/antirez/redis/issues/2410), it's reasonable to resolve the hostnames to IPs before connecting (and probably print a warning, similar to https://github.com/antirez/redis/pull/2323).\nAs for the rework of ready check, in which case this pull request doesn't eliminate the problem?. Please enable the debug mode DEBUG=ioredis:* node yourapp.js and post the logs here.. Sorry for the late response. Redis.Cluster is required for the cluster mode in ElastiCache. What about disable the enableReadyCheck? new Redis.Cluster([/*..*/], {enableReadyCheck: false}). This may be related with issue https://github.com/luin/ioredis/issues/701 & https://github.com/luin/ioredis/issues/709. @liam-murray-xealth Thank you for pointing this out. What's the result of the command CLUSTER SLOTS? \"Disconnect xxx.usw2.cache.amazonaws.com:6379 because the node does not hold any slot\" happens when the result of CLUSTER SLOTS doesn't contain that host.. @tvb What about using the configuration endpoint with new Redis() (instead of new Redis.Cluster())? I don't have a running ElastiCache cluster instance so I can't test with it. I'll set up one soon.. This issue should be solved in v4.2.0. Using configuration endpoint as startup nodes for Cluster. Feel free to reopen this issue if the problem still exists.. Would you try to upgrade @types/ioredis package to see whether the problem still exists? The package is located in another repo: https://github.com/DefinitelyTyped/DefinitelyTyped/blob/8fb0189dd4432b854d72d9e78fcdea53829b24fe/types/ioredis/index.d.ts.. Not sure where the problem is. Here's my setup:\n@types/ioredis: 4.0.3\nioredis: 4.0.0\n\n. Tested with 3.0.3 & 3.1.1. Sounds interesting. I'm just wondering how to release a beta version (e.g. v4.0.0-0) if we are using semantic-release? It seems this lib will publish every change to the npm.. That two branch workflow makes sense. PR is welcome.. Closing \ud83d\udc4d . Could you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here?. ioredis 4 \u53ea\u652f\u6301 node.js 6 \u4ee5\u4e0a\u7248\u672c\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: Peng Liu notifications@github.com\nSent: Monday, October 8, 2018 6:23:35 PM\nTo: luin/ioredis\nCc: \u5b50\u9a85; Comment\nSubject: Re: [luin/ioredis] AWS Elasticache Redis & TLS (#714)\n\u5b50\u9a85\u4f60\u597d\n\u6ca1\u60f3\u5230\u4f60\u4f1a\u56de\u590d\u6211\u7684\u95ee\u9898\u3002\u975e\u5e38\u611f\u8c22\u3002\n\u70e6\u8bf7\u67e5\u770b\u4e0b\u9762debug\u4fe1\u606f\uff1a\n[image: image.png]\nDebug\u8f93\u51fa\u6587\u672c\u5982\u4e0b\uff1a\nvar { StandaloneConnector, SentinelConnector } = require('./connectors');\n^\nSyntaxError: Unexpected token {\nat exports.runInThisContext (vm.js:53:16)\nat Module._compile (module.js:374:25)\nat Object.Module._extensions..js (module.js:417:10)\nat Module.load (module.js:344:32)\nat Function.Module._load (module.js:301:12)\nat Module.require (module.js:354:17)\nat require (internal/module.js:12:17)\nat Object.\n(/var/www/html/wp-dashborad/wp-content/themes/istorageDash/socketio/node_modules/ioredis/built/index.js:2:28)\nat Module._compile (module.js:410:26)\nat Object.Module._extensions..js (module.js:417:10)\nMany thanks for your reply.\nKind Regards,\nPENG LIU\nEmail: frank.kingdom@gmail.com\nOn Sat, 6 Oct 2018 at 18:51, \u5b50\u9a85 notifications@github.com wrote:\n\nCould you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and\npost the logs here?\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/issues/714#issuecomment-427593654, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AC3r096wkt_Wbj2Q91yok98sePjn_FlEks5uiO2ygaJpZM4XJ1fg\n.\n\n\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/714#issuecomment-427785001, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_gMjpvoUeydtJrMLjUPJxe5oq9Ndks5uiyengaJpZM4XJ1fg.\n. Sorry for not noticing that. Just published a new version for redis-commands. Now a fresh install of ioredis will support Redis 5.0 commands.. Fixed in v4.0.2. \u611f\u8c22\u6307\u51fa\u95ee\u9898\uff01. You can set environment variable DEBUG to ioredis:*\nGet Outlookhttps://aka.ms/qtex0l for iOS\n\nFrom: jnst notifications@github.com\nSent: Friday, October 12, 2018 1:17:47 PM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] Query logging (#719)\nI think it would be convenient if I could logging the Redis query as below.\nRun\n$ npx ts-node sample.ts\nDEBUG -- :   Redis (1.8ms) echo hello\nhello\nsample.ts\nimport * as Redis from \"ioredis\";\n(async () => {\n  const redis = new Redis();\nconst result = await redis.echo('hello');\n  console.log(result);\nredis.disconnect();\n})();\nFor example, is there a mechanism to prepare a logging flag in RedisOptions or output query log with an event?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/719, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_uO21jX28Zl89dR135DA0s38b5k1ks5ukCX7gaJpZM4XYzlC.\n. > I'd advise to make these env variables only available on the master/release branches so that people wont be able to steal them with malicious PR\nIt seems Travis-CI doesn't support setting the scope of env variables.. Merged!. 4.2.0 is released with the \"latest\" tag instead of \"next\". Besides that, everything works like a charm. @AVVS . @boltzjf It has nothing to do with Redis 5.0. It has been included in ioredis v4.2.0.. The latest version of ioredis should support zpopmin. Note that ioredis depends on redis-commands v1.4.0 for the support for this command (https://github.com/luin/ioredis/blob/master/package.json#L37).. Just released v4.1.0 that supports this command.. Could you run npm ls to see what the version of ioredis & redis-commands?. Could you turn debug mode on (DEBUG=ioredis: node yourapp) and post the logs here?. The current NPM_TOKEN & GH_TOKEN are invalid, so we can test if the process works without pushing commits to GitHub & publishing new version.. According to the documentation of Travis CI, pull requests that sent from a forked repo won't have access to the encrypted env vars. I submitted a test pull request and the NPM_TOKEN & GH_TOKEN are not logged: https://travis-ci.org/luin/ioredis/jobs/442282959#L591.. That makes sense. We could just write the encrypted tokens as env vars to the deploy stage in .travis-ci.yml instead of configuring them in the dashboard of Travis CI that is visible by all the stages.. Aha, good to know! btw, I think the semantic-release have been set up successfully. I'll merge https://github.com/luin/ioredis/pull/723 tomorrow to see whether all things work.. It was an issue with tag v4.1.0 (it was attached to the wrong commit so that semantic-release can't find it). I re-published the tag again, so semantic-release works well again. Just updated the draft release to the new tag.. @acuervof Thank you for the details. I tested with 4.2.0 and \"Redis connection for cluster established\" is printed to the console. Could you enable the debug mode with 4.2.0 (DEBUG=ioredis: node yourapp.js) and post the logs here?. The requirement of all commits that will be merged into the master branch should come from PRs is disabled because semantic-release wouldn't be able to push new commits to the master then.. What will happen when a failover happens in the Cluster while scanning?. I'm not sure actually. As for me, it may be impossible to implement a robust SCAN method since duplicated keys will be returned if failovers happened while scanning. By providing a simple SCAN method, we hide details about how scan work with Redis cluster, which may mislead people into thinking they will always get perfect correct results, which is not true.. Closing this issue then.. Does it work that connecting to the ip that returned by ioredis:SentinelConnector resolved: ip directly?. > @luin what do you think of the approach, should we just redo this into event-based handling instead of chaining promises?\nSorry for the late response! Been pretty busy lately \ud83d\ude22 . This approach works while add a little complexity. I would consider Cluster#connect() failed when the current connection attempt is not successful (although the subsequent ones may success). This keeps the same behavios as Redis#connect(). People need to rely on the events like \"ready\"/\"connect\"/\"close\"/\"end\" if they need to know the connection status in a higher level (most cases will be this).\nCurrently, the problem seems to be that we remove the close handler too early so the \"reject()\" won't be called if a disconnection happens between the \"refresh\" event and \"ready\" event. To solve that, we may move the removeListener('close') to the ready event callback, so the Cluster#connect() will be rejected in your test case (although the cluster wil be ready eventually). What do you think?. @kiko35 Either make sure that there's only one key in the pipeline or make sure that all keys in the pipeline have the same part that surrounds by brackets (like the example above).. Could you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and post the logs here? I think it's an issue related to NAT.. You can disable the clusterRetryStrategy option. See readme for details.. No, ioredis doesn't support rollbacking data.. Redis always returns IPs instead of hostnames as node info for commands like CLUSTER SLOTS (in other words, Redis cluster doesn't support hostnames at all). If ioredis doesn't resolve hostnames to IPs, when people provide hostnames as startup nodes info, we can't tell whether the node address returned (which are IPs) from Redis is the same ones we've got (which are hostnames).\nHowever, encrypted Elasticache does some changes on the official Redis that will return hostnames. To solve the problem, we can simply disable the dns lookup (by returning hostnames directly):\n```javascript\nconst Redis = require('ioredis');\nconst nodes = [{\n    host: 'clustercfg.xxx.use1.cache.amazonaws.com',\n    port: '6379',\n}];\nconst options = {\n    dnsLookup: (address, callback) => callback(null, address),\n    redisOptions: {\n        tls: {}\n    }\n}\nconst cluster = new Redis.Cluster(nodes, options);\n``. That strange. I tried on my machine connecting to an encrypted Elasticache instance withdnsLookupoption successfully. The related code is here: https://github.com/luin/ioredis/blob/master/lib/cluster/index.ts#L710. The process of resolving hostnames to IPs only happens during the initial connection, which should be able to be overridden by thednsLookupoption.. The logresolved hostname clustercfg.xxx.use1.cache.amazonaws.com to IP clustercfg.xxx.use1.cache.amazonaws.comindicates ioredis still resolves the hostname to IP, which is not expected since thednsLookupoption is provided. Could you log the currentthis.options.dnsLookup` at  https://github.com/luin/ioredis/blob/master/lib/cluster/index.ts#L690 to see whether the option has been set correctly?. > That is what you expected to see, correct?\nYes, that is. It seems the dnsLookup option solves your issue.\n\nSometimes I will see that error, but eventually it connects to the cluster.\n\nThat should be another issue I think. Does it accurs without dnsLookup?. Hi @vanrysss , what problem did you experience when using ioredis with Elastiche Redis?. @emadum Thanks for suggesting! Redis.Cluster is designed to communicate with the official Redis cluster (https://redis.io/topics/cluster-spec, cluster mode enabled with Elasticache), so we may have another class (like Redis.ReplicaGroup) for that use case. Currently there's no plan to implement that, but I'll revisit this about two or three months later. Pull requests for this feature will be always welcome.. The feature of hooks hasn't been implemented yet. Where did you see that?. ioredis provides nat option that should solve your problem (by mapping private addresses to the public addresses): https://github.com/luin/ioredis#nat-mapping.. Thank you for asking! There's not much progress on this issue currently. Maintainers of both projects are been busy, and the merge process will introduce breaking changes, which will be pretty bothersome to the developers.\nHaving said that, the two projects have already shared a lot of important code (say redis-parser & redis-commands) and may find more common modules to share in future. . Have you tried connecting to Redis via host \"redis\" (see https://docs.docker.com/compose/networking/)?\n```javascript\nconst redis = require('ioredis')\nconst appdb = new redis({host: 'redis'})\nmodule.exports = { db }\n``. Will, that is a network problem and it should not be related to ioredis.. What do you mean by \"support\"? The replication process itself should be transparent to clients.. @natesilva This issue should have been fixed in v4.0.1 (https://github.com/luin/ioredis/pull/697). Doesn't it work for you?. Thanks for the pull request! A question here, does connecting to sentinel servers need tls support?. We can provide two options in this version:\n1.enableTLSForSentinelMode(default value isfalse). If true, thetlsoption will be used to connect to the redis server that resolved from sentinels.\n2.sentinelTLS(default value isundefined). Similar to thetls` option but used for connecting to sentinels.\nIn v5, enableTLSForSentinelMode will be removed and sentinelTLS will remain intact. I've changed the comments I left below the code accordingly. What do you think?. The new version has been released automatically: v4.5.0. The version is released under the next tag (can be installed via npm i ioredis@next or npm i ioredis@4.5.0) and will be released to the latest tag in a month.. That because ioredis tried to reconnect to the server. Disable auto-reconnecting will resolve the issue: https://github.com/luin/ioredis#auto-reconnect. Please checkout the retryStrategy option in the README.\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: FlyAway notifications@github.com\nSent: Thursday, January 10, 2019 9:10 PM\nTo: luin/ioredis\nCc: \u5b50\u9a85; Comment\nSubject: Re: [luin/ioredis] client.hgetall don't throw error,when redis server is stop (#776)\nget commands pending ,wait forever\u3002 not catch any error info\nconst Redis = require('ioredis');\nconst redis = new Redis({\naxRetriesPerRequest: 10\n\n});\n// ,\nredis.get('1')\n.then(data => console.log(data))\n\n.catch(err => console.log(111111111111111111, err))\n\nredis.on('error', err => console.log())\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/776#issuecomment-453090556, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_len34buFjqhyEQx3wtgd-NEemx0ks5vBzu-gaJpZM4Z20FI.\n. maxRetriesPerRequest was introduced in v4.0.0, thus an upgrade is required to use this option.. \u6709\u9047\u5230\u4ec0\u4e48\u95ee\u9898\u5417\uff1fLastError \u4e3a\u7a7a\u4f46\u662f\u8fd8\u662f\u904d\u5386\u4e0b\u4e00\u4e2a sentinel \u4e00\u822c\u662f\u56e0\u4e3a\u914d\u7f6e\u51fa\u9519\u5bfc\u81f4 sentinel \u8fd4\u56de\u4e86\u7a7a\u7ed3\u679c\u3002\u8fd9\u4e2a\u65f6\u5019\u611f\u89c9\u8c03\u7528 sentinelRetryStrategy \u8ba9\u5f00\u53d1\u8005\u4ecb\u5165\u4f1a\u6bd4\u8f83\u597d\uff1f. Thank you for pointing this out! We could improve https://github.com/luin/ioredis/blob/master/lib/pipeline.js#L269-L282 by getting distinct scripts before sending them to redis.. Released with v4.5.1. You can just disable the enableReadyCheck option.. Hi @privateOmega. I wonder if the command was sent to the correct db (that same one redis-cli connected to). Could you please run monitor on the redis-cli and execute the program again to see whether the command is sent to the correct db?. @privateOmega I don't think it's a problem on the ioredis side. You may want to create a minimal demo that can reproduce the issue to see where the problem is. For example:\njavascript\nconst Redis = require('ioredis')\nnew Redis().zrevrange('exactstore:rse:frank', 0, -1, console.log);. Thank you for the pull request! I would like to have an option like maxLoadingRetryTime instead since allowing users to limit to max waiting time may be more useful in practice.. What about giving a default value for this option (like 10s)? So people won't be blocked to long if they don't notice this option.. There isn't a direct way to do that, but currently you can overwrite Redis#sendCommand() to achieve that:\njavascript\nconst Redis = require('ioredis');\nconst { sendCommand } = Redis.prototype\nRedis.prototype.sendCommand = function (command) {\n  console.log(command.name);\n  sendCommand.apply(this, arguments);\n}. Could you please enable the debug mode (DEBUG=ioredis:* node yourapp.js) and run the program with 4.5.1 and post the logs here?\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: Guillaume Lakano notifications@github.com\nSent: Friday, January 25, 2019 12:49 AM\nTo: luin/ioredis\nCc: Subscribed\nSubject: [luin/ioredis] Error: Too many Cluster redirections (#787)\nHello!\nI've upgraded ioredis from 4.2.3 to 4.5.1 and now I have this kind of errors.\nI've also tried to move to 4.6.0\nIf I revert back to 4.2.3, I doesn't have this problem.\nIf you have any idea to help me to fix that, thanks ! :)\n{\"log\":\"  Error: Too many Cluster redirections. Last error: Error: Connection is closed.\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457716816Z\"}\n{\"log\":\"      at Cluster.handleError (/home/node/node_modules/ioredis/built/cluster/index.js:531:38)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.45772379Z\"}\n{\"log\":\"      at Command.command.reject (/home/node/node_modules/ioredis/built/cluster/index.js:410:23)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.45772891Z\"}\n{\"log\":\"      at Redis.flushQueue (/home/node/node_modules/ioredis/built/redis.js:384:26)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457733731Z\"}\n{\"log\":\"      at close (/home/node/node_modules/ioredis/built/redis/event_handler.js:102:14)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457738482Z\"}\n{\"log\":\"      at Socket.\\u003canonymous\\u003e (/home/node/node_modules/ioredis/built/redis/event_handler.js:69:20)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457743188Z\"}\n{\"log\":\"      at Object.onceWrapper (events.js:273:13)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457748452Z\"}\n{\"log\":\"      at Socket.emit (events.js:182:13)\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457753123Z\"}\n{\"log\":\"      at TCP._handle.close (net.js:606:12),\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457757466Z\"}\n{\"log\":\"  [ null, null ] ]\\n\",\"stream\":\"stdout\",\"time\":\"2019-01-24T16:41:14.457761851Z\"}\n{\"log\":\"(node:8) UnhandledPromiseRejectionWarning: ReplyError: ERR value is not a valid float\\n\",\"stream\":\"stderr\",\"time\":\"2019-01-24T16:41:24.596806401Z\"}\n{\"log\":\"    at parseError (/home/node/node_modules/redis-parser/lib/parser.js:179:12)\\n\",\"stream\":\"stderr\",\"time\":\"2019-01-24T16:41:24.59685172Z\"}\n{\"log\":\"    at parseType (/home/node/node_modules/redis-parser/lib/parser.js:302:14)\\n\",\"stream\":\"stderr\",\"time\":\"2019-01-24T16:41:24.596856275Z\"}\n{\"log\":\"(node:8) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)\\n\",\"stream\":\"stderr\",\"time\":\"2019-01-24T16:41:24.596859821Z\"}\n{\"log\":\"(node:8) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\\n\",\"stream\":\"stderr\",\"time\":\"2019-01-24T16:41:24.59686564Z\"}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/luin/ioredis/issues/787, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AAmz_o_397SomcH_WQk5GzIh0eymz1sgks5vGeQKgaJpZM4aRSCH.\n. Thank you for the further information. According to the logs, the cluster gives different results for the command cluster slots, especially for the slot range \"10923~16383 \". Sometimes the node 10.42.10.43:6379 was returned, but sometimes 10.42.10.43:6379 was given, leading ioredis sends the command to different node every time.\nI noticed there are some timeout errors in the log. Is there any network issue in the cluster?. Thanks for the pull request! Created another pull request for this issue. The new approach works when the debug mode is disabled.\nhttps://github.com/luin/ioredis/pull/793. The reason is same, that different nodes gave different results for command cluster slots. Seems to be a network partition happened in the cluster.. What do you get when invoking sentinel sentinels mymaster on redis-cli? It seems the result contains a server whose host is 127.0.0.1.. Yes, almost all redis commands including scan & llen are supported with cluster mode. It seems to be a nat problem. Refer to https://github.com/luin/ioredis#nat-mapping for details.. There's no built-in way for that, but you can run command on each node and aggregate them manually. Refer to https://github.com/luin/ioredis#running-commands-to-multiple-nodes for details.. It's a limitation on the redis server side. When subscribing a channel, ioredis will only subscribe on a single node of the cluster. Redis server won't sync the subscription to other nodes, so when running .pubsub, the command may be sent to a different node, in which case an empty array will be returned.\nAs a workaround, there's a private variable that recording the subscribed channels on the client side: \njavascript\nawait cluster.subscribe('ch');\nconsole.log(cluster.subscriber.subscriber.condition.subscriber.set)`\nBut this may change in the later version.. Closing this issue. Feel free to reopen it with more details.. Refer to maxclients option in redis.conf for details.. It seems to be a network problem which is not related with ioredis. Try if telent works.. I'm wondering why do you use multi in the cluster setup when the commands inside it belong to different slots? ioredis will distribute commands to the correct node as long as pipeline & multi are not used.. @Eywek Aha, I got the point. Indeed the commands with keys belonging to different slots will not work (e.g. mget) with Redis.Cluster. We may have another class for that feature to avoid breaking backward compatibility.. Did you connect to the correct node? You can enable the debug mode (DEBUG=ioredis:* node yourapp.js) for detail logs. I tested on my side with Redis v5 and things worked correctly:\n```javascript\nconst Redis = require('ioredis')\nconst redis = new Redis({\n  port: 5555,\n  password: 'f9e0fwpoefpowef0e0fefwleek9ewl'\n})\nredis.get('foo', console.log)\n``. A pull request will be welcome.. Nice work! Sorry for missing the original issue.. SincereadOnlyonly works in cluster mode, we'd better only have it in cluster.js and listen to thereadyevent in thecreateNodemethod of cluster.js\n. I think we don't need to connect to the slaves whenreadOnlyis disabled.\n. Yes, and I prefer the latter way to create a function fortransformand_keys.push`:\njavascript\nfunction (arg, keys)\n  keys.push(transform ? transform(arg) : arg);\n}\nHowever I'm not sure whether that would impact the performance. \n. There's no place we can cache the keyPrefix option. options is possible to be undefined since it's optional. We may put\njavascript\nif (typeof options === 'undefined') {\n  options = {};\n}\nat the top of the constructor so that we won't need to test options everytime.\n. Aha, you're right. The way works for me.\n. authError looks good to me\n. As for me, syntax like cluster.to('masters').flushall(); seems to be a little better that we don't need to give every Redis command a cluster version method.\n. Or maybe just give users an array of master nodes (the same as _.values(this.masterNodes)), so they can send commands to every master node easily:\njavascript\ncluster.masters.map(node => node.flushall()).then().catch();\n. Not yet. Personally I'd like to go for the latter one since although it requires a bit more code, users are easier to figure out how do things work. I'll think about it however.\n. It would be better to use var CONNECT_EVENT = _this.options.tls ? 'secureConnect' : 'connect'; instead\n. if (Array.isArray(redis)) may be better IMO, and we should check whether redis is null/undefined and default to the master node. \n. Nice catch. It seems to fix the test by just changing\njavascript\ncluster.get('foo', function () {\n  cluster.get('foo');\n});\nto\njavascript\ncluster.get('foo');\n. Enum may not be able to avoid typos IMO. For example, I may wrongly write Redis.READY as Redis.REEDY, and the latter will be an undefined, no exception would be thrown.\n. This test makes sure non-binary methods (get, set) works when dropBufferSupport is enabled.\n. Aha...you're right.\n\n. return (this.options.dropBufferSupport ? exec : execBuffer).apply(this, arguments); should fix the tests in \"test/functional/transaction.js\"\n. Should we return the number of the node been disconnected or just \"OK\" to keep in line with the result of native quit?\n. db defaults to 0 so why we add it here?. Updating properties of the option introducing side effects. For example:\n```javascript\nconst sentinels: [{host: '192.168.1.3'}]\nconst redis = new Redis({sentinels})\nconsole.log(sentinels) // values changed unexpectedly\n```\nWhat about give the default port when creating a new instance by port: endpoint.port || 26379: https://github.com/luin/ioredis/blob/master/lib/connectors/sentinel_connector.js#L213.\nThere's an isSentinelEql function should be updated accordingly.. db option is not sentinel-specified and it's documented here: https://github.com/luin/ioredis/blob/master/API.md#new-redisport-host-options. What problem did you encounter?. We shouldn't callback() with an error here when we still keep retrying since a callback must not be called more than once.\nInstead, what about just emit an event (like \"sentinelError\") when all sentinels are unreachable just like when a connection to a standalone Redis server is down.. If there's no lastError, shouldn't we just connect to the first sentinel again instead of just returning?. We may set lastError to null when resolved successfully.. Yes, the callbacks will be called besides an error event. self.flushQueue() does the trick.. Yes, that's correct.. Good catch! I'll fix this in a separate commit.. \"disconnecting\" returns a different error.. Shouldn't branch limit to \"master\"?. Why we need a dry run?. Will the dry run works when GITHUB_TOKEN & NPM_TOKEN is absent?. if (_this.options.enableTLSForSentinel && _this.options.tls) {. tls: this.options.sentinelTLS. Shouldn't Object.assign(resolved, _this.options.tls) be Object.assign(resolved, {tls: _this.options.tls})?. javascript\nscripts.map(function (item) {\n    return item.sha;\n  }).filter(function (sha, index, shaList) {\n    return shaList.indexOf(sha) === index;\n  })\nWould be better to change to\njavascript\nArray.from(new Set(scripts.map(({sha}) => sha))). Don't add ; here. Code style changes should belong to a separate pull request.. I'm wondering what's the point of this change. Could you please provide an example to demonstrate in which case this change helps?. Sorry for the late response. @types/ioredis module has been updated to support returning a null to stop reconnection, thus I think this pull request can be closed.. We should allow empty natMap ({}) to keep the same behavior as one in cluster: https://github.com/luin/ioredis/blob/master/lib/cluster/index.ts#L428.. Typo here. sentinelPasseword should be sentinelPassword I guess?. Missing assertion here. Should define authed as false in the outer block, and add an assertion of expect(authed).to.eql(true) when argv[0] === 'sentinel' && argv[1] === 'get-master-addr-by-name' to make sure ioredis sends AUTH before asking for master address.\nClose the connections here instead of on sentinel.once('connect').. Simply make sure the test will fail when there's a typo in option name \ud83d\ude04 . ",
    "adoyle-h": "The feature has been implemented?\n. I get it. thanks\n. ",
    "zensh": "\u6211\u90fd\u641e\u4e86\u5927\u534a\u5e74\u4e86\uff0c\u5c31\u662f\u4e3a\u4e86 thunk\n. ",
    "KingScooty": "Installing fine now. Must have been some server weirdness.\n. :)\n. ",
    "jonathanong": "Yup. That would be awesome!\n. Ah ok. Feel free to close then :)\n. ",
    "perrin4869": "I actually developed recently a set of tools for easy usage of lua scripts within node.js. Basically I take a lua script, and output a javascript module that can be easily used with ioredis or any other redis client: redis-lua2js, gulp-redis-lua2js, node-hook-redis-lua, and some standalone lua commands using those tools: redis-pdel. Maybe these can help solve this issue?\n. ",
    "stale[bot]": "This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. This issue has been automatically marked as stale because it has not had recent activity. It will be closed after 7 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n. ",
    "lpinca": "That was fast.\n. Node.js v0.12.2, early 2009 MacBook Pro, redis-server 3.0.0 running on the same machine:\n``` txt\nmacbook:ioredis luigi$ npm run bench\n\nioredis@1.2.0 bench /Users/luigi/Desktop/repos/ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nioredis: 1.2.0\nnode_redis: 0.12.1\nCPU: 2\nOS: darwin x64\n==========================\n                  simple set\n      23,289 op/s \u00bb ioredis\n      27,762 op/s \u00bb node_redis\n\n                  simple get\n      22,474 op/s \u00bb ioredis\n      26,978 op/s \u00bb node_redis\n\n                  simple get with pipeline\n       4,312 op/s \u00bb ioredis\n       2,758 op/s \u00bb node_redis\n\n                  lrange 100\n      18,529 op/s \u00bb ioredis\n      22,116 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 67,757.50 ms\n```\n. I understand, thanks for the explanation.\nUnfortunately I can't use a callback or a pipeline because I have something like this:\n``` js\nemitter.on('foo', function () {\n  redis.multi().del('bar').del('baz').exec();\n});\nemitter.on('foo', function () {\n  redis.get.('bar', function (err, res) {\n    if (res) console.log('(\u5c6e\u0ca0\u76ca\u0ca0)\u5c6e Y U STILL HERE');\n  });\n});\n```\nand the two listerners can't be merged because the second one is attached later. A workaround for this particular case is to call redis.get in the next tick, but I'm a bit worried about possible race conditions in a general use case.\n. The last example with { pipeline: false }, works well for my use case, thanks.\nJust a minor thing: it would be nice if the callback for exec was optional, right now an error is thrown if one is not provided.\n``` js\n'use strict';\nvar Redis = require('ioredis')\n  , redis = new Redis();\nredis.multi({ pipeline: false });\nredis.del('foo');\nredis.del('bar');\nredis.exec();\n```\nmacbook:order luigi$ node index.js \n/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/async.js:43\n        fn = function () { throw arg; };\n                                 ^\nTypeError: undefined is not a function\n    at wrapper (/Users/luigi/Desktop/order/node_modules/ioredis/lib/transaction.js:58:7)\n    at tryCatcher (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/util.js:24:31)\n    at Promise.successAdapter (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/nodeify.js:22:30)\n    at Promise._settlePromiseAt (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/promise.js:563:21)\n    at Promise._settlePromises (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/promise.js:681:14)\n    at Async._drainQueue (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/async.js:123:16)\n    at Async._drainQueues (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/async.js:133:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (/Users/luigi/Desktop/order/node_modules/ioredis/node_modules/bluebird/js/main/async.js:15:14)\n    at processImmediate [as _immediateCallback] (timers.js:367:17)\n. :+1:\n. I don't know if there are better ways but I guess this will work:\njs\ncache.doThat.apply(cache, keys.unshift(keys.length));\n. It's actually written in the basic-usage section :smile:.\n. Yes, but using an array of arguments will \"remove\" this loop.\n. That's indeed interesting, I would have never expected that.\nFeel free to close this.\n. Here is my own test case which confirms my original assumption.\nThe difference is quite noticeable:\nmacbook:bench luigi$ node index.js \nwithout copy loop x 50,886 ops/sec \u00b10.64% (96 runs sampled)\nwith copy loop x 33,855 ops/sec \u00b10.38% (95 runs sampled)\nFastest is without copy loop\n. I don't know either, but it doesn't matter, I doubt that someone will ever use a number of arguments big enough to make the difference.\n. Yes.\n``` shell\n$ nvm install 2\n################################################################## 100,0%\nWARNING: checksums are currently disabled for io.js\nNow using io.js v2.5.0 (npm v2.13.2)\n```\n. ",
    "gagle": "I don't use Koa or anything related to Express, in fact I hate it. I'm using Hapi.\nPersonally I don't have any interest in knowing what is doing a module internally. Debugging messages should be controlled by the client because we tipically have a wrapper around these kind of modules to decouple as maximum as possible the cache usage and its actual implementation. One day I could change to node_redis or to memcached or whatever. This message debug('received connection from sentinel'); doesn't give to me any valuable information, it doesn't help me to debug my server if something goes wrong. When using a third-party module I'm assuming that it is well-tested and is working, so I don't need to know that I'm connected to a \"sentinel\", in fact I don't know what a sentinel is and I don't care so much, I just want to \"connect to redis to cache stuff\".\n. The problem comes when I want to use the DEBUG environment variable for my own purposes.\n. ",
    "dirkbonhomme": "Correct me if I'm wrong, but this debug library is a noop unless activated right? And it's one of many packages created by TJ so they ain't come any better than this.\n. Hmm makes sense. Maybe it's easy to disable the debug module in ioredis with a config flag?\n. Love the avatar @Horx \n. I'm also seeing great results:\nnodejs: 0.10.25\nioredis: 1.2.0\nos: ubuntu x86_64 (Vagrant)\n```\n\nioredis@1.2.0 bench /vagrant/\nmatcha benchmarks/*.js\n\n                  simple set\n     104,896 op/s \u00bb ioredis\n      55,791 op/s \u00bb node_redis\n\n                  simple get\n      95,655 op/s \u00bb ioredis\n      54,011 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      19,481 op/s \u00bb ioredis\n       7,032 op/s \u00bb node_redis\n\n                  lrange 100\n      80,833 op/s \u00bb ioredis\n      46,986 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 57,291.53 ms\n```\n. I currently detect this by using\nredis.on('end', function(){\n   ...\n});\n. Is the queue size maybe something that can be included in the retry strategy? Or is it best to keep this separated?\nretryStrategy: function (times, commandsQueue) {\n    if(times > 10 && commandsQueue.length > 500){\n        commandsQueue.splice(...)\n   }\n   ...\n}\n. I think there's just no way a library like this can decide on its own what to do with a commands queue in case of a disconnect. Some projects won't care (e.g. when storing non-critical logs into Redis), some projects will want 100% guarantees (e.g. when pushing critical data) and some projects will probably dump all queued commands into a file and do a controlled crash.\nMy suggestion is to document some examples on how to use the retryStrategy/offlineQueue. Users can then either copy-paste these examples or adjust them to their liking.\n. It's using the exact same arguments as Redis does: https://redis.io/commands/expire\nSo for expire it appears to be seconds. You can also use this dedicated command http://redis.io/commands/SETEX ;)\n. @arankhem no, you'll have to do it with 2 commands:\n- hmset(key, data)\n- expire(key, ttl). Looks ok to me. Simple option and easy to understand.\n. Nice find! As a general rule I think we should aim to have all input variables be immutable. (as in: ioredis should not modify them in any way). ",
    "MarkHerhold": "I would like to take the opposite side here and thank the author for using the debug module. It makes debugging applications a lot easier and provides very adjustable logging. :+1: \nExcluding the debug content is trivial. Simply add -ioredis:* to your DEBUG environment variable.\ne.g. DEBUG=some:stuff,-ioredis:*\n. ",
    "dstroot": "I agree with @MarkHerhold !!   This is a standard module used in many places and I'm glad it is used here.\n. Great!  Thanks @dirkbonhomme and @luin!  \nI used luin's approach and stopped execution if we lose redis connectivity.  \njs\nredis.on('end', function () {\n  debug('Cannot connect to Redis!');\n  process.exit(1);\n});\n. ",
    "tipiirai": ":+1: for using debug\n. ",
    "horx": "@dirkbonhomme  :smile: \n. ",
    "devoto13": "No problems. \nThank you for a great library!\n. ",
    "saltukalakus": "Actually initially I had tried that, but in the same machine killed redis instance quickly gets up so master does not change. I will check in a more real setup, may be more than 1 machine. If I observe an issue I will let you know. Thanks.\n. Sure :)\n. ",
    "AVVS": "I'm currently writing redis backend for https://github.com/pgte/abstract-skiff-persistence, which is basically a copy of https://github.com/pgte/skiff-level/blob/master/lib/index.js with a different backend, and without the proposed change pipeline.exec(callback); would crash when not using hiredis with max call stack error, and when using hiredis it would crash with smth saying that it can't read .name of the command or smth. I'm pretty sure it tries to parse the buffer, that was encoded as a msgpack and fails somehow during serialization/deserialization \nTo simulate this you can run the tests as proposed in https://github.com/pgte/abstract-skiff-persistence - basic payload is a bit incorrect - : instead of , - but otherwise it's fine to test.\n``` js\nimport msgpack5 from 'msgpack5';\nimport SkiffPersistence from 'abstract-skiff-persistence';\nimport _ from 'lodash';\nimport async from 'neo-async';\nlet msgpack = msgpack5();\nfunction encode(val) {\n  return msgpack.encode(val).slice();\n}\nfunction decode(buf) {\n  return msgpack.decode(buf);\n}\nclass SkiffRedis extends SkiffPersistence {\nconstructor(options={}) {\n\n    super(options);\n\n    let { redis, namespace } = options;\n\n    // in case we use redis cluster, make sure that namespace is hashed\n    this._namespace = namespace || '{skiff-redis}';\n\n    this._redis = redis;\n\n    this._redis.on('error', this._onError);\n\n    this.nodes = {};\n}\n\n_onError(err) {\n    this.emit('error', err);\n}\n\n_key(...parts) {\n    if (parts.length === 0) {\n        throw new Error('parts must include at least one value');\n    }\n\n    return this._namespace + parts.join('~');\n}\n\n_saveMeta(nodeId, state, callback) {\n    let redis = this._redis;\n    let pipeline = redis.pipeline();\n    let log = state.log;\n    let logEntries = log.entries;\n\n    // do not enter hashtable mode\n    state.log = _.without(log, ['entries']);\n\n    pipeline.set(this._key(nodeId, 'meta'), encode(state));\n\n    let minLogIndex = logEntries.length && logEntries[0].index || 0;\n    let maxLogIndex = logEntries.length && logEntries[logEntries.length - 1].index || Infinity;\n    let logNamespace = this._key(nodeId, 'logs');\n    let cursor = 0;\n    let maxReadIndex = 0;\n\n    async.doUntil(\n        function zscanRedis(next) {\n            redis.zscanBuffer(logNamespace, cursor, 'count', 10, function zscanResponse(err, response) {\n                if (err) {\n                    return next(err);\n                }\n\n                // update cursor\n                cursor = parseInt(response[0], 10);\n\n                // process log entries\n                let entries = response[1];\n                for (let i = 0, l = entries.length; i < l; i += 2) {\n                    let index = parseInt(entries[i + 1], 10);\n                    let entry = entries[i];\n\n                    if (index > maxReadIndex) {\n                        maxReadIndex = index;\n                    }\n\n                    if (index < minLogIndex || index > maxLogIndex) {\n                        pipeline.zrem(logNamespace, entry);\n                    } else {\n                        let correspondingNewEntry = logEntries[index - minLogIndex];\n                        let decodedEntry = decode(entries[i]);\n                        if (correspondingNewEntry.uuid !== decodedEntry.uuid) {\n                            pipeline.zrem(entry);\n                            pipeline.zadd(logNamespace, correspondingNewEntry.index, encode(correspondingNewEntry));\n                        }\n                    }\n\n                }\n\n                next();\n\n            });\n        },\n\n        function isIterationComplete() {\n            return cursor === 0;\n        },\n\n        function saveMeta(err) {\n            if (err) {\n                return callback(err);\n            }\n\n            if (maxReadIndex < maxLogIndex) {\n                logEntries.slice(maxReadIndex - minLogIndex + 1).forEach(function (entry) {\n                    pipeline.zadd(logNamespace, entry.index, encode(entry));\n                });\n            }\n\n            pipeline.exec(callback);\n        }\n\n    );\n}\n\n_loadMeta(nodeId, callback) {\n\n    this._redis\n        .pipeline()\n        .getBuffer(this._key(nodeId, 'meta'))\n        .zrangebyscoreBuffer(this._key(nodeId, 'logs'), '-inf', '+inf')\n        .exec(function (___, results) {\n            // err is always null\n\n            let [err, meta] = results[0];\n            let [logerr, logEntries] = results[1];\n\n            if (err || logerr) {\n                return callback(err || logerr);\n            }\n\n            if (meta) {\n                meta = decode(meta);\n                meta.log.entries = logEntries.map(decode);\n            }\n\n            callback(null, meta);\n\n        });\n\n}\n\n_applyCommand(nodeId, commitIndex, command, callback) {\n\n}\n\n_lastAppliedCommitIndex(nodeId, callback) {\n\n}\n\n_saveCommitIndex(nodeId, commitIndex, callback) {\n\n}\n\n_createReadStream(nodeId) {\n\n}\n\n_createWriteStream(nodeId) {\n\n}\n\n_removeAllState(nodeId, callback) {\n\n}\n\n_close(callback) {\n    this._redis.disconnect();\n    this._redis.removeListener('error', this._onError);\n    callback();\n}\n\n}\nexport default SkiffRedis;\n```\nTest file included: \n``` js\n'use strict';\nvar test = require('abstract-skiff-persistence/test/all');\nvar Redis = require('ioredis');\nvar redis = new Redis();\nvar SkiffRedisPersistence = require('../lib');\nvar p = new SkiffRedisPersistence({ redis: redis });\nvar options = {\n  commands: [\n    {type: 'put', key: 'a', value: 1},\n    {type: 'put', key: 'b', value: 2},\n    {type: 'del', key: 'a'},\n    {type: 'put', key: 'c', value: {some: 'object'}}\n  ],\n  expectedReads: [\n    {key: 'b', value: 2},\n    {key: 'c', value: {some: 'object'}}\n  ],\n  newWrites: [\n    {type: 'put', key: 'd', value: 3},\n    {type: 'put', key: 'e', value: {some: 'other object'}},\n    {type: 'put', key: 'f', value: 'some other string'}\n  ],\n  newReads: [\n    {key: 'd', value: 3},\n    {key: 'e', value: {some: 'other object'}},\n    {key: 'f', value: 'some other string'}\n  ]\n};\ntest(p, options);\n```\nMakefile to transpile es6 if needed:\n``` shell\nSRC = $(wildcard src/*.js)\nLIB = $(SRC:src/%.js=lib/%.js)\nlib: $(LIB)\nlib/%.js: src/%.js\n    mkdir -p $(@D)\n    ./node_modules/.bin/babel $< -o $@\n. The code I've posted is by no means complete, but this is where the bug appeared\n. I'm still working on the code, so its fine that it doesn't pass my tests. As long as you got rid of the error this should be great\n.\nVitalys-MacBook-Pro:mservice-smtp vitaly$ redis-cli -v\nredis-cli 2.8.19\nVitalys-MacBook-Pro:ioredis vitaly$ npm run bench\n\nioredis@1.2.0 bench /Users/vitaly/projects/ioredis\nmatcha benchmarks/*.js\n\n==========================\nioredis: 1.2.0\nnode_redis: 0.12.1\nCPU: 8\nOS: darwin x64\n==========================\n                  simple set\n      86,129 op/s \u00bb ioredis\n      75,235 op/s \u00bb node_redis\n\n                  simple get\n      82,437 op/s \u00bb ioredis\n      74,842 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      15,923 op/s \u00bb ioredis\n       8,383 op/s \u00bb node_redis\n\n                  lrange 100\n      61,766 op/s \u00bb ioredis\n      54,674 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 56,919.29 ms\nVitalys-MacBook-Pro:ioredis vitaly$ node -v\nv0.10.38\n```\n. Updated for mac / iojs 2.2.1. Redis 3.0.1\n```\nVitalys-MacBook-Pro:ioredis vitaly$ npm run bench\n\nioredis@1.5.4 bench /Users/vitaly/projects/ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nioredis: 1.5.4\nnode_redis: 0.12.1\nCPU: 8\nOS: darwin x64\n==========================\n                  simple set\n     113,225 op/s \u00bb ioredis\n      88,521 op/s \u00bb node_redis\n\n                  simple get\n      98,778 op/s \u00bb ioredis\n      80,953 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      14,946 op/s \u00bb ioredis\n       7,907 op/s \u00bb node_redis\n\n                  lrange 100\n      86,316 op/s \u00bb ioredis\n      81,688 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 58,897.17 ms\nVitalys-MacBook-Pro:ioredis vitaly$ node -v\nv2.2.1\n```\n. @thelinuxlich I believe you are trying to perform multi-key operations on the cluster, therefore you get the following errors\nIf you need to that, you need to make sure they have the same hash, ie {somehash}your-key-name1, {somehash}your-key-name2 etc\n. In that case multi() is fine (same key -> new_key), but the error says that the hash, that was resolved to new_key is now on another machine in the cluster. These errors should really be handled by the library. What the error says is that, hey, your hash slot caching is wrong, it needs to be updated and the operation needs to be retried.\n@luin, please take a look at this, as I believe it needs to be improved. IE, MOVED reply must be handled.\n\nOn May 6, 2015, at 10:03 PM, Alisson Cavalcante Agiani notifications@github.com wrote:\nMy code has one transaction:\nredis.multi().setnx(new_key, possible_new_session_id).expire(new_key, 1800).exec()\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/issues/28#issuecomment-99719567.\n. @thelinuxlich thunk-redis takes hash from the first key, and then applies it to all the operations in the multi, but they don't make use of pipeline. Here the first key from pipeline is taken and then the hash is applied to the whole pipeline.\n\nBeside that its pretty much the same, except that ioredis seems cleaner (and with a bug not handling MOVED responses :dash:)\n. It uses node-redis-scripty for lua management, which is an extra module and imo is not needed, since ioredis incorporates these features already\n. https://github.com/luin/ioredis#connection-events\n. any plans to add this?\n. Ok, I'll try to get something done\n\nOn 10 Sep 2015, at 08:46, Zihua Li notifications@github.com wrote:\npr is still welcomed\n\u2014\nReply to this email directly or view it on GitHub.\n. @luin can you update package version and release this on npm? Kind of using that in production :)\n. Redis configuration at the point of running:\n\njs\n{\n            hosts: [\n                { host: 'localhost', port: 30001 },\n                { host: 'localhost', port: 30002 },\n                { host: 'localhost', port: 30003 },\n                { host: 'localhost', port: 30004 },\n                { host: 'localhost', port: 30005 },\n                { host: 'localhost', port: 30006 }\n            ],\n            options: {\n                friendlyStackTraces: true,\n                lazyConnect: true\n            }\n        }\nConnection sequence:\n``` js\n    before(function (done) {\n        exec(createRedisCluster, [ 'start' ], { cwd: redisClusterCwd }, done);\n    });\nbefore(function (done) {\n    this.timeout(10000);\n    exec(createRedisCluster, [ 'create' ], { cwd: redisClusterCwd }, done);\n});\n\nbefore(function () {\n    this.service = require('../lib/service.js')(overwrite);\n    return this.service;\n});\n\n// tests...\nafter(function () {\n    return this.service.call('close');\n});\n\nafter(function (done) {\n    exec(createRedisCluster, [ 'stop' ], { cwd: redisClusterCwd }, done);\n});\n\nafter(function (done) {\n    exec(createRedisCluster, [ 'clean' ], { cwd: redisClusterCwd }, done);\n});\n\n});\n```\n. I assume that it would be this line that fails, though stack traces don't really give me anything no matter how I try\nhttps://github.com/luin/ioredis/blob/master/lib/cluster.js#L296\n. Just to clarify - operations that I perform is basically setex, set, get, hmset, hsetnx, but doubt this is of any help\n. Well, how do I make sure that cluster is connected?\nI've waited for 'ready' event, but it's emitted as soon as one node connects, but not the other.\nIn the pr what I basically do is add 'lazyConnect' to make sure that it actually connect, but it's one and the same with the 'ready' event.\nAssume the following code:\n``` js\nvar redis = require('ioredis');\nvar instance = new redis.Cluster(hosts, { lazyConnect: true });\ninstance.connect().then(function () {\n   return instance.set(Date.now().toString(), 'somerandomvalue');\n});\n```\nI bet if you do this it will eventually throw the same error. I have to emphasize that I do wait for the ready event / connect resolve on my code\n. I'll run more tests, but I still had the same results with the lazyConnect: false, its just random and seem dependant on whether it was able to connect to a node that holds needed slots or not before the event\n. I tried turning off the lazyConnect, and still it fails from time to time\ninstance = new redis.Cluster(config.redis.hosts, config.redis.options || {});\nsome redis commands\n\nUncaught TypeError: Cannot call method 'sendCommand' of undefined\n. Ok, I've added logs to \n\n``` js\nCluster.prototype.sendCommand = function (command, stream, node) {\n  if (this.status === 'end') {\n    command.reject(new Error('Connection is closed.'));\n    return command.promise;\n  }\nvar targetSlot = node ? node.slot : command.getSlot();\n  var ttl = {};\n  var reject = command.reject;\n  var _this = this;\n  if (!node) {\n    command.reject = function (err) {\n      _this.handleError(err, ttl, {\n        moved: function (node, slot, hostPort) {\n          debug('command %s is moved to %s:%s', command.name, hostPort[0], hostPort[1]);\n          _this.slots[slot] = node;\n          tryConnection();\n          _this.refreshSlotsCache();\n        },\n        ask: function (node, slot, hostPort) {\n          debug('command %s is required to ask %s:%s', command.name, hostPort[0], hostPort[1]);\n          tryConnection(false, node);\n        },\n        clusterDown: tryConnection,\n        connectionClosed: tryConnection,\n        maxRedirections: function (redirectionError) {\n          reject.call(command, redirectionError);\n        },\n        defaults: function () {\n          reject.call(command, err);\n        }\n      });\n    };\n  }\n  tryConnection();\nfunction tryConnection (random, asking) {\n    if (this.status === 'end') {\n      command.reject(new Error('Cluster is ended.'));\n      return;\n    }\n    if (_this.status === 'ready') {\n      var redis;\n      if (node && node.redis) {\n        redis = node.redis;\n      } else if (.includes(Command.FLAGS.ENTER_SUBSCRIBER_MODE, command.name) ||\n                 _.includes(Command.FLAGS.EXIT_SUBSCRIBER_MODE, command.name)) {\n        redis = _this.subscriber;\n      } else {\n        if (typeof targetSlot === 'number') {\n          redis = _this.slots[targetSlot];\n        }\n        if (asking && !random) {\n          redis = asking;\n          redis.asking();\n        }\n        if (random || !redis) {\n          redis = _this.selectRandomNode();\n        }\n      }\n      if (node && !node.redis) {\n        node.redis = redis;\n      }\n      // this is where redis is undefined and crashes the system\n      redis.sendCommand(command, stream);\n    } else if (_this.options.enableOfflineQueue) {\n      _this.offlineQueue.push({\n        command: command,\n        stream: stream,\n        node: node\n      });\n    } else {\n      command.reject(new Error('Cluster isn\\'t ready and enableOfflineQueue options is false'));\n    }\n  }\n  return command.promise;\n};\n```\n. Also, here is the stack trace (finally managed to get it)\n1) actions#verify() \"before all\" hook: add a set of jwt tokens:\n     Uncaught TypeError: Cannot call method 'sendCommand' of undefined\n      at tryConnection (/Users/vitaly/projects/ioredis/lib/cluster.js:324:13)\n      at Cluster.sendCommand (/Users/vitaly/projects/ioredis/lib/cluster.js:294:3)\n      at Cluster.executeOfflineCommands (/Users/vitaly/projects/ioredis/lib/cluster.js:255:12)\n      at Cluster.refreshListener (/Users/vitaly/projects/ioredis/lib/cluster.js:82:12)\n      at /Users/vitaly/projects/ioredis/lib/cluster.js:227:15\n      at /Users/vitaly/projects/ioredis/lib/cluster.js:404:5\n      at run (/Users/vitaly/projects/ioredis/lib/utils/index.js:146:16)\n. Further debugging:\n1. it check target slot, it's undefined\nif (typeof targetSlot === 'number') {\n     redis = _this.slots[targetSlot];\n     console.log('clause <target slot>');\n   }\n2.  it selects random node, apparently still undefined. Do we have empty nodes array? Probably this is the only way random node will return undefined\nif (random || !redis) {\n     redis = _this.selectRandomNode();\n     console.log('clause <random node>');\n   }\nWhole trace:\nclause <target slot>\nclause <random node>\nTrace\n    at tryConnection (/Users/vitaly/projects/ioredis/lib/cluster.js:330:27)\n    at Cluster.sendCommand (/Users/vitaly/projects/ioredis/lib/cluster.js:294:3)\n    at Cluster.executeOfflineCommands (/Users/vitaly/projects/ioredis/lib/cluster.js:255:12)\n    at Cluster.refreshListener (/Users/vitaly/projects/ioredis/lib/cluster.js:82:12)\n    at Cluster.g (events.js:180:16)\n    at Cluster.emit (events.js:92:17)\n    at /Users/vitaly/projects/ioredis/lib/cluster.js:227:15\n    at /Users/vitaly/projects/ioredis/lib/cluster.js:411:5\n    at run (/Users/vitaly/projects/ioredis/lib/utils/index.js:146:16)\n    at tryCatcher (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/util.js:24:31)\n    at Promise.successAdapter (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/nodeify.js:22:30)\n    at Promise._settlePromiseAt (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/promise.js:528:21)\n    at Promise._settlePromises (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/promise.js:646:14)\n    at Async._drainQueue (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:177:16)\n    at Async._drainQueues (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:187:10)\n    at Async.drainQueues (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:15:14)\n    at process._tickDomainCallback (node.js:492:13)\n. Added keys listing to random nodes call, and as I've suspected, the nodes array is empty. Question is why.\nrandom nodes from: localhost:30001, localhost:30002, localhost:30003, localhost:30004, localhost:30005, localhost:30006\nclause 3\nclause <target slot>\nrandom nodes from: \nclause <random node>\nTrace\n    at tryConnection (/Users/vitaly/projects/ioredis/lib/cluster.js:331:27)\n    at Cluster.sendCommand (/Users/vitaly/projects/ioredis/lib/cluster.js:295:3)\n    at Cluster.executeOfflineCommands (/Users/vitaly/projects/ioredis/lib/cluster.js:256:12)\n    at Cluster.refreshListener (/Users/vitaly/projects/ioredis/lib/cluster.js:82:12)\n    at Cluster.g (events.js:180:16)\n    at Cluster.emit (events.js:92:17)\n    at /Users/vitaly/projects/ioredis/lib/cluster.js:228:15\n    at /Users/vitaly/projects/ioredis/lib/cluster.js:412:5\n    at run (/Users/vitaly/projects/ioredis/lib/utils/index.js:146:16)\n    at tryCatcher (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/util.js:24:31)\n    at Promise.successAdapter (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/nodeify.js:22:30)\n    at Promise._settlePromiseAt (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/promise.js:528:21)\n    at Promise._settlePromises (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/promise.js:646:14)\n    at Async._drainQueue (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:177:16)\n    at Async._drainQueues (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:187:10)\n    at Async.drainQueues (/Users/vitaly/projects/ioredis/node_modules/bluebird/js/main/async.js:15:14)\n    at process._tickDomainCallback (node.js:492:13)\n. @luin I assume I know what happens. During the tests I create a new cluster each time with redis-trib.rb and then connect to it. It is possible that at a certain time when ioredis requests slot information - there is none, even though connection can be established. After that it deletes all nodes and crashes. Probably this case wont really happen in production unless the whole cluster dies/resets, but I think it should be handled as an error during refresh slot call\n. @luin I've actually tested it more and pinpointed that to response from cluster that is not fully initialized. During my tests I start the cluster with redis-trib util via spawn process, then allocate the slots and its completed by the timeout, which is, of course, far from ideal. Either way, when redis receives some weird / incomplete response from a cluster, which contains 0 nodes (they were not able to connect yet) and I suppose that would still be the case for completely degraded cluster - it will make the process crash with\nUncaught TypeError: Cannot call method 'sendCommand' of undefined\n. I was thinking that it actually makes sense to have some sort of TTL on the request. Idea is that some  requests mutate state and some don't, and those that mutate, especially in microservice environments must be controlled. Because when there is a message bus like rabbitmq, services don't really know about each other, whether something had died or not, therefore it would be a great option to have a built-in function, that could pull command out of offline queue after specified time if it wasn't executed. @luin do you think it's possible to do so?\nIe use case that I assume should be the following:\nredis.set('somekey', 'somevalue').timeout(5000).then(<ok>, <err>)\nwhat really must be implement is that when we timeout is called, the cancel is issued (bluebird has cancellable promises as far as I remember) and the library takes care of pulling the command out of offline queue as long as it's there\nwhat do you think?\n. well, there is .pipeline() / multi() + .exec(). \nWhat if there is something similar in terms of semantics? Ie enter limiting mode and then exec to set it in motion. It might be a bit verbose though\n. If you leave unattended write requests that mutate state you can introduce race condition vulnerabilities - thats the problem here\n\nOn Jun 13, 2015, at 1:23 PM, Vsevolod Rodionov notifications@github.com wrote:\nI definitely have to sleep more :) Good question, I'll have a discussion with our guys.\nActually, we discussed my usecase with devs team we figured out that we need to save data even after timeout, so using only bluebird .timeout() is correct scenario. But in case we would need dropping the requests - that option would be enough. But I'd rather implement just cancellable promise anyway as far as per-request customisation is more flexible than per-connection. E.g. for my case I would rather prefer to drop read requests and leave write requests working. But for me it would just good option than killer-feature that I need to implement.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/issues/61#issuecomment-111747332.\n. Yeah, but I\u2019m concerned about a general solution for this, hence the thought\nOn Jun 13, 2015, at 1:41 PM, Vsevolod Rodionov notifications@github.com wrote:\nI know. It's only HMSETs and Redis is used as a caching reflection for other database.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/issues/61#issuecomment-111750744.\n. these options are good, but they should be usable together and probably should be located in a generic configuration. Also a good idea would be so that can accept both function and a number. When there is function - its invoked with args for the command (include the command name) and when smth that is =< 0 returned - then we have infinite attempts / no timeouts - otherwise use number as a limit\n. you cant really do that, you need to supply keys amount to defineCommand, all the keys passed to LUA script must be on the same machine and this is the only way you can use lua scripts on the cluster, you cant make calls to other machines from lua scripts either, check the tutorial on the redis cluster\n. Just try calling redis.call(\"get\", key) from lua with key that is not on an executing cluster machine. Standard multi-key cluster constraints are applicable here. So yes, I'm sure\nOn 09 Dec 2015, at 14:05, Alisson Cavalcante Agiani notifications@github.com wrote:\ncoming back to this issue, @AVVS are you sure all the keys explicitly declared on EVAL command need to be on the same machine? According to EVAL docs:\nAll Redis commands must be analyzed before execution to determine which keys the command will operate on. In order for this to be true for EVAL, keys must be passed explicitly. This is useful in many ways, but especially to make sure Redis Cluster can forward your request to the appropriate cluster node.\n\u2014\nReply to this email directly or view it on GitHub.\n. No, key must be from another machine.\n\nKEYS[1] must be on master A, \nARGV[1] should be on master that is not A, call redis.call(\"get\", ARGV[1])\n\nOn 09 Dec 2015, at 14:41, Alisson Cavalcante Agiani notifications@github.com wrote:\nredis.call(\"get\",KEYS[1]) you mean?\n\u2014\nReply to this email directly or view it on GitHub.\n. Only if all keys belong to a single machine, otherwise it wont\nOn 09 Dec 2015, at 15:04, Alisson Cavalcante Agiani notifications@github.com wrote:\nbut if it is on the KEYS array it should work, right?\n\u2014\nReply to this email directly or view it on GitHub.\n. Use prefix that gives you constant hash values, like {xxx}\nOn 09 Dec 2015, at 15:23, Alisson Cavalcante Agiani notifications@github.com wrote:\nThis seems impracticable, how should I know that in advance?\n\u2014\nReply to this email directly or view it on GitHub.\n. it's from one of the talks during nodeconfeu, probably there will be some videos on youtube. Basically whats exploited here is hash mode of Object vs optimized mode, or whatever it's called. The way the object is allocated based on the size of properties, whether some1 used delete or not, and whether property names are \"valid\" for the object. The talk was given almost a year ago, and also was on one of the jsconfs, so there should be some reference to it. Sadly cant give any more details at the moment\n. https://github.com/petkaantonov/bluebird/wiki/Optimization-killers#52-the-object-being-iterated-is-not-a-simple-enumerable more about it\n. I don't really see your point. This is a common convention and effectively does exactly the same as your \"optimized\" code. Furthermore, for the sake of it one might want to be politer when expressing his or her opinion\nOn 19 Jun 2015, at 16:57, Operations Research Engineering +Software notifications@github.com wrote:\nexports = module.exports is totally redundant, I don't care who uses it!\n\u2014\nReply to this email directly or view it on GitHub.\n. First of all code isn\u2019t mine, I just happen to not like when people use OSS and complain. Create a PR, explain the reasoning, it will get merged and everybody will be happy, just my few cents\nOn Jun 19, 2015, at 6:43 PM, Operations Research Engineering +Software notifications@github.com wrote:\nSorry, one cannot be polite when criticizing code, you just have to have a thick skin\nthe point is it took me too much time to figure out what your index.js was doing.\nit should not be that hard.\nyour code is a mess in that file, which makes me doubt the other stuff.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/issues/75#issuecomment-113694993.\n. This is related to #61, even though solves the problem differently. fixed FIFO queue is nice when you have outage and then spike of requests come in and they are flushed with an error, but only if none of them mutate state. \n\nIf you have some that mutate state (lets say we manage sessions in redis) - it may actually do plenty of harm, so gotta be somewhat careful with this\n. https://github.com/luin/ioredis/blob/master/lib/connectors/connector.js#L28 - this needs to be patched,\nerror can be triggered when network device goes down, which should trigger reconnect, not crash. I'll do something raw quickly\n. Basically problem is that error happens before you init error listener, because callback is async.\n1. stream init\n2. nextTick\n3. error\n4. never gets to .once('error')\n```\n    this.connector.connect(function (err, stream) {\n      if (err) {\n        _this.flushQueue(err);\n        reject(err);\n        return;\n      }\n  _this.stream = stream;\n\n  stream.once('connect', eventHandler.connectHandler(_this));\n  stream.once('error', eventHandler.errorHandler(_this));\n  stream.once('close', eventHandler.closeHandler(_this));\n\n```\n. https://github.com/luin/ioredis/blob/master/lib/connectors/sentinel_connector.js#L73-L74 - for sentinel this is already a sync operation\n. This looks good, gonna test it shortly and report if problem goes away\n. There are a few optimizations that can be done, especially switch-case is best changed to object-based switch clause, then it can be easier optimized by v8\n\nOn 19 Sep 2015, at 17:46, Zihua Li notifications@github.com wrote:\nJust implemented the new parser: https://github.com/luin/ioredis/blob/new-parser/lib/parsers/ioredis.js. However, the result doesn't show much difference comparing with the current parser. So I'm closing this issue before we have time to revisit it.\nThe new parser will only be faster when the replies from Redis are broken into many small frames. For example, running the benchmark using the following code:\njavascript.execute(new Buffer('$3'));\njavascript.execute(new Buffer('\\r'));\njavascript.execute(new Buffer('\\n'));\njavascript.execute(new Buffer('f'));\njavascript.execute(new Buffer('o'));\njavascript.execute(new Buffer('o'));\njavascript.execute(new Buffer('\\r\\n'));\nAnd the result will be:\nWhich is not going to happen in the real-world environment.\n\u2014\nReply to this email directly or view it on GitHub.\n. It could be that array wasnt optimized (diff types of values, not all strings for instance) or some other similar case\nOn 03 Aug 2015, at 09:56, Luigi Pinca notifications@github.com wrote:\nI don't know either, but it doesn't matter, I doubt that someone will ever use an amount of arguments that will make a difference.\n\u2014\nReply to this email directly or view it on GitHub.\n. Pooling is having multiple open connections to the same db and use them in parallel. Technically it can improve speed if the latency is a problem - ie bottleneck is network and not cpu of redis server\nOn 14 Sep 2015, at 18:09, Michael Taylor Scheer notifications@github.com wrote:\nWouldn't this kind of splitting already be handled if you have a redis cluster running? \nRelevant Redis documentation\nFor pooling, I was imagining a situation where no matter how many times you require('ioredis'), it would always use the same connection to talk to the redis server. This would create a problem for pub sub type applications though now that I think about it...\n\u2014\nReply to this email directly or view it on GitHub.\n. it does\n. all the commands are exactly the same as in the redisio docs on their website\n\nredis.expire(...)\nredis.ttl(...)\nlast arg is callback, if its not - then promise is returned\n. Thats likely a problem with setting up your cluster\n\nOn 24 Sep 2015, at 14:24, Adrien-P notifications@github.com wrote:\nMy current setup is the following:\nI have a machine on IP 10.0.0.6 which hosts a redis cluster with 3 masters (ports 7000, 7001 and 7002) and 3 slaves (ports 7003, 7004, 7005).\nThe client machine is on IP 10.0.0.7 and I pass the configuration:\nnew Redis.Cluster([{\n            port: 7000,\n            host: \"10.0.0.6\"\n        }], {db: 0});\nthe issue is that my redis node on port 7000 keeps receiving an unlimited amount of \"info\" requests but never receives the actual requests within the client code. Looking at the code I think I found the source of the issue:\nIn the method Cluster.prototype.getInfoFromNode there is a command redis.cluster('slots',... and the results of this command are like this:\n[ [ 0, 5460, [ '127.0.0.1', 7000 ], [ '127.0.0.1', 7003 ] ],\n  [ 10923, 16383, [ '127.0.0.1', 7002 ], [ '127.0.0.1', 7005 ] ],\n  [ 5461, 10922, [ '127.0.0.1', 7001 ], [ '127.0.0.1', 7004 ] ] ]\nWhat happens is that the redis node on 10.0.0.6:7000 replies with the list of nodes but with the localhost address (not the 10.0.0.6). Thus, ioredis tries to connect to theses nodes on 127.0.0.1 and keeps getting ECONNREFUSED (because the client machine does not host any redis instance).\nNote: I tried to pass all the nodes to the Redis.Cluster constructor and I experienced the same \"info\" loop issue\n\u2014\nReply to this email directly or view it on GitHub.\n. Could be a basis for #96 \n. Also we can speed it up even further by doing this:\n\n``` js\n.read = function (reader) {\n  ...\n  reply = reader.get();\n  ...\n}\n.execute = function (data) {\n  ...\n  var reader = this.reader;\n  reader.feed(data);\n  do {\n    reply = this.read(reader);\n  }\n  ...\n}\n```\n. This looks like the issue I had during the tests, and it was already fixed in the modern versions of ioredis\n. Thought was related to #56, but doesn't seem to be. Either way that was when cluster wasn't yet initialized and not all slots were allocated yet. In the recent versions I haven't seen it once and can't really try 1.7.6 at this point - these test sets are long gone :-1: \n. http://redis.io/commands/mset\nredis.mset(key, val, key, val), instead you pass an object, it's coerced to string, hence it fails.\nas far as I remember object to payload transform is applied on hmset only\n. then I'm wrong and this is a bug for sure\n. More info - this actually happened when I had a bootstrapped cluster. But then I started resharding it. At this point ioredis tried to get some info, I assume it got error that a SLOT was moved and then crashed. \n. Fixed the typeof duplication. Tests cases would, indeed, be great, but I don't have time to write them for the next week or so. If some1 wants to pitch in - that would be awesome\n. btw, any chance we get to use Map for handling nodes? This seems to be more suited towards it and is well supported in 4.x and 5.x, but this would mean 0.10.x will need to include polyfill for it\n. Do you have memory dump for this? I mean if its leaking as fast as you are saying - there should be some clear indication in it\nOr code to replicate it would be just as nice\n\nOn 16 Feb 2016, at 17:13, Ramon Snir notifications@github.com wrote:\n@luin Really strange. I installed with nvm Node 0.12.10, Node 4.3.0 and Node 5.6.0: all three are leaking. 0.10.40 isn't.\n@AVVS +1 for Map.\n\u2014\nReply to this email directly or view it on GitHub.\n. @luin the way objects are treated in node is basically the following:\n1. object created, uses optimized mode\n2. when property changes - it recreates underlaying structure of object\n3. it does 1 and 2 until there were too many mutations on this object or until delete is called at least once, then it switches to hash mode and becomes many times slower\n\nWith map it doesn't try to do these things and is more suited towards dynamic allocation of keys. I would be mainly concerned about obj[key] vs Map.get(key) performance, because this is potentially a hot place in the code. Either way it's not likely to yield anything more than a few nanoseconds, but in the example posted above it should work faster.\n@shaharmor @ramonsnir - in the posted example there are tons of strings being initialized and then they all must be disposed of, since it's utf-8 and it eats a lot of memory. so it's possible GC just doesn't kick in fast enough to dispose of all the garbage\n. key is a String - not a reference like object, therefore, copied all the time anyway. Likely it's not the case anyway\n. Great catch. Makes sence that the function is referenced and when it's never called - it appears to still be in the scope.\n. I think its a problem with the code. Redis.config is issued without await, command cant complete before .subscribe, connection is then moved to subscriber mode and cant finish it\nie I'm pretty sure if you do \nawait Redis.config()\nand only then subscribe - you should be good. Actually this is I being stupid, I've used incorrect context, therefore couldn't access this\n. @trevnorris could you give some input here? I remember your optimization talk during one of the the first nodeconfeu\n. I must be confusing this with 1-byte strings and trick to decode it to binary representation before json.parse and then creating a buffer before we put to to db. Likely doesnt apply here because of inapplicable assumptions\nThanks!\n\nOn 31 May 2016, at 20:00, Trevor Norris notifications@github.com wrote:\nHave any benchmarks been done with this? There is no API that allows passing a char* (though I wish there was). The call to JSON.parse() would automatically call .toString() on the Buffer. Example of what's happening:\nvar b = { toString: () => '{\"foo\":\"bar\"}' };\nJSON.parse(b);\n// output: { foo: 'bar' }\nSo I'm honestly unsure how one could be faster than the other.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @gbaquedano take a look at https://github.com/twitter/twemproxy, you can use this for scaling, but you would sacrifice a lot of functionality. As for connection pooling, thats a nice feature for sure, but would be somewhat out of scope here\n. let me check with the tests and get back to you, fixing this in 3.0 is fine if you say that redisOptions are actually not needed for this\n. @luin confirmed\n. as for dropBufferSupport warning - could you limit it to only 1 invocation? It's extremely annoying to have that and I don't see an extra connection being an option, since its used with a buffer only once in the 2.5k LOC application. But its still a hot place and is used very often, leading to degraded performance (spamming to tty aint good)\n. @luin that was my bad indeed - I was staring at the tests and it cleans up after every test to make the idempotent, therefore recreating the parser/connection each test. Thanks for clearing that up.\n. I'd say bench is too synthetic compared to real-world. 4MiB buffer & 100 ops in the queue could be there infinitely if app is under no load. So you absolutely must add a timed flush. Imagine what would happen if the command is stuck in the queue for minutes because the buffer doesn't reach 100th command?\n- in most real-world scenarios latency is lower than 1ms, often around 0.2-0.3ms. Given that - I'd look at this change as a highly controversial.\n- There is also a rewritten JS parser, which greatly increases performance in the low latency environments, try running bench with it\n- how are manual pipelines handled?\n\nGiven all those points auto-pipeline could be cool, but it's way more complex than this as it seems to me\n. @Salakar cool, missed sense of this line initially:\nhttps://github.com/luin/ioredis/pull/343/files#diff-bbfcd42271084cf70271e042b7b4a16eR44\n\nAm aware of the new parser also as I re-wrote it with BridgeAR, and wrote the new key slot calculator =]\n\nawesome \ud83d\udc4d \n\nManual pipelines go through the same logic and will also get buffered, there's no harm in this, it's per event loop as I said, so the manual pipeline and any other commands run in the same event loop will get sent together.\n\nThanks for clarification, all makes much more sense now!\n. @Salakar 1-2 ticks shouldn't really matter for a manual pipeline IMO, but force write is handy for such cases\n. yeah, doesn't happen all the time, but frequently enough to post it here. Thought it was a bug in my code, but that didn't happen in 1.x versions, first I noticed it in 2.2, but then rolled back to 2.1 and it was also happening there.\n. https://github.com/makeomatic/mservice/blob/master/src/plugins/redisCluster.js#L21-L82\nyou can see catchReturn - this is how I mitigate the issue for now.\n. @migg24 they aren't, but they are sent through the same tcp socket sequentially\n. Throw error in then if its empty and handle it in catch. Logic seem to be flawed though\n\nOn 06 Sep 2016, at 17:12, Dev Patel notifications@github.com wrote:\nHandle \"not found\" and \"connection/client errors\" in one place when using promise. I have a same fallback of those two scenarios but I have to use promises.\nreturn romis.getAsync(\"amazon\")\n.then(function (amazonSTR) {\n//THEN ONE\nconsole.log(amazonSTR); // will be undefined if not found\nreturn doManualDBProcedure();\n})\n.catch(function (err) {\n// CATCH ONE\nconsole.log(err);\nreturn doManualDBProcedure();\n});\nIs there any way I can handle both errors in //THEN ONE ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. doing the same operation on internal redis error (connection flipped, invalid key, etc) and missing key is somewhat weird. Key might exist just fine\n. To me it looks like cluster misconfiguration. the difference is that ioredis create a new node based on report from a cluster node, which tells to ask some new node not from the list - it tries to ask that node, gets another redirect and so on. One would assume that that nodes report wrong ip addresses for some reason and that leads to redirection loops\n\nOther possibility is that response is parsed incorrectly? But I assume it's not the case though\n. @thelinuxlich maybe you are right. The place it can happen is\nhttps://github.com/luin/ioredis/compare/v2.1.0...v2.2.0#diff-ded17b5f6b1633a10701ad5e98a4e3c8R428\n. https://github.com/luin/ioredis/compare/v2.1.0...v2.2.0#diff-ded17b5f6b1633a10701ad5e98a4e3c8R428\nFixed link. Its an error from flexbuffer lib. https://github.com/mercadolibre/flexbuffer-node/issues/2 and https://github.com/mercadolibre/flexbuffer-node/issues/3. Thanks, will do. Closed by #406 . We also might want to add to the docs that if somebody does bulk export/import and its important to increase throughput, whereas latency doesn't matter as much - noDelay should be set to false, because while this decreases latency, it reduces maximum throughput. @luin can you release the latest version?. @luin dont have write access, but probably would do the following to ensure tls support and make sure it doesn't crash here. Afaik there were some changes to TLS sockets, but this actually could make it implementation agnostic for the future, as in unix pipes might have different interface, etc:\nP.S. don't have write access to push it to the branch, nor to merge when it all passes tests\n```js\n      if (_this.options.noDelay) {\n        if ('setNoDelay' in stream) {\n          debug('disabling Naggle on raw tcp socket');\n          stream.setNoDelay(true);\n        }\n    // NOTE: can also check based on _this.options.tls,\n    // but this seems good enough\n    if ('socket' in stream && 'setNoDelay' in stream.socket) {\n      debug('disabling Naggle on tls socket');\n      stream.socket.setNoDelay(true);\n    }\n  }\n\n```. @luin you seem to be right, at least for the latest versions, methods are proxied:\nhttps://github.com/nodejs/node/blob/master/lib/_tls_wrap.js#L290-L305\nIn that case this should be good to go. bluebird is a) faster than native promises and b) has useful utils, so when you say pointless, thats highly debatable\n\nOn Feb 9, 2017, at 12:04 PM, Gregory Wild-Smith notifications@github.com wrote:\nThis would also open up dropping the pointless (given what it is used for here) Bluebird library and using Native Promises. As far as I can tell Bluebird is only used for functionality that Native Promises provides out of the box.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-278756444, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0Qq98TdLwqYZhLDB8yg7LBVtLXUmks5ra3FJgaJpZM4LCu7A.\n\n\n. I hear you, but these are not arguments, this is a personal opinion. Explain your position, tell what broken, whats working not as expected? \nAs far as I\u2019m personally aware BB follows A+ spec and adds extra methods on top, which makes promises actually useful and pleasant to work with.\nFor example, basic promises are limited to .then, .catch and .all, but it leaves out all the actual usage patterns behind. Before promises control flow libs were highly popular due to hardly avoidable callback hell,\nnow they have their niche, but so do promises (when you can actually use them for control flow, like with BB). Of course, there is async/await, but its not released yet either, and it uses try/catch semantics, and that is debatable, too\nSo once again, please facts & arguments, not tunnel-visioned statements, such as BB is bad and native is good\n\nOn Feb 9, 2017, at 3:50 PM, Gregory Wild-Smith notifications@github.com wrote:\nThe only parts it is faster on is because it doesn't follow the spec. Neither do those utils. Bluebird is NOT promises. It doesn't follow the spec. It mostly follows the spec.\nFollowing broken psuedo-implementations is an anti-pattern. Best to nip it in the bud as early as possible.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-278813909, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0e4d0aMOF0FjBntIU2xGzUZcdFczks5ra6YxgaJpZM4LCu7A.\n\n\n. Still nothing justifies that. In my view every argument here is just kudos to the clever implementation of BB promises.\n\nOn Feb 9, 2017, at 4:08 PM, Gregory Wild-Smith notifications@github.com wrote:\nit leaves out all the actual usage patterns behind\nGreat. Add those. Don't call it Promises, because it's not. Don't reimplement a (not-quite up to spec) version of Promises to add it.\nAs far as I\u2019m personally aware BB follows A+ spec and adds extra methods on top\nYeah, it doesn't. For example new Promise is an anti-pattern in Bluebird. Why? Because speed-wise it drops back down to native levels. Their benchmarks cheat a lot in this way. They also don't allocate arrays for promises' handlers. Meaning Promise branching is limited to incomplete.\nYes, it\u2019s slow. And it\u2019s slow because this effectively allocated a new function, and a new scope on each invocation. That\u2019s why it\u2019s done that way. It\u2019s an anti-pattern without any regard to bluebird, if\nnew Promise(fn [, ctx]) supported an optional context, then it would\u2019ve been possible to make this as fast or event faster. But it doesn\u2019t - hence the anti-pattern\nBasically it's a bunch of things to win micro-benchmarks while only adhering to as much of the spec as they can do be \"compliant\".\nHowever, it should really be about why does ioredis even need Bluebird?\nAnswer: it doesn't unless it's for compatibility for old versions of Node.\nThis is one thing I agree - it doesn\u2019t need it on node 4+, but a) it would be a huge breaking change b) literally any developer I know would bring BB or do a global Promise remap to bring extra functionality and save dev time\nNot to mention here is the entire list of Promise-related APIs ioredis uses:\nnew Promise\nPromise.reject\nPromise.resolve\nPromise.then\nThat's it. 100% Native compatible without using any extra library. Also 100% as fast as Bluebird because the raw reject/resolve/then calls are the same speed and Bluebird performs approx the same when used with new (instead of their promisify implementation).\nSo, long story short: why keep it? It adds nothing but bloat.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-278817877, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0dGQr7NFEH7ao4VHo4FEPnh-GfNZks5ra6ptgaJpZM4LCu7A.\n\n\n. Its a huge breaking change, because it will ultimately break all the code that relies on BB functionality.\nImagine snippet of\njs\nredis.pipeline().get(x).hmget(y, one, two, three).exec().then(handlePipeline).spread((valueX, ArrayofValuesFromY) => {\n    // stuff to do\n})\nThis won\u2019t work any longer.\nAnything with tap won\u2019t work - literally any code that was written that relied on BB utils won\u2019t work.\n\nOn Feb 10, 2017, at 10:59 AM, Gregory Wild-Smith notifications@github.com wrote:\na) it would be a huge breaking change\nAs I demonstrated: this is literally not true. ioredis uses nothing of Bluebird. No breaking changes whatsoever.\nliterally any developer I know would bring BB or do a global Promise remap to bring extra functionality and save dev time\nAh, annecdata. The last refuge of the lack of argument. I can counter with - no developer I know would willingly add a bloated library when it wasn't required. Which it isn't for 99% of promise use.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-279034258, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0fnSzb7vx0NftTHnYKlquZPLLXSNks5rbLOOgaJpZM4LCu7A.\n\n\n. it is a superset of Promises and is fully compatible, don\u2019t mix these two terms ;)\nBack to square one - if they are completely replaced it will break code and will reduce DAU one way or another, there would appear a fork for people who want it the other way.\nI can see a solution that can work both ways - use either bluebird or native promise with bluebird prevailing if it\u2019s available. That you can control - it won\u2019t be a hard dependency, but will still be available for those who prefer it\n\nOn Feb 10, 2017, at 1:23 PM, Gregory Wild-Smith notifications@github.com wrote:\nSo Bluebird isn't compatible with Promises. Great, glad we got that cleared up.\nBut fine, sure, breaking change. So is killing node < 4. Perfect time to do it then.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-279069448, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0TXn6MpDrFaOD-NT0Pvh5m9Zpxkyks5rbNU_gaJpZM4LCu7A.\n\n\n. I\u2019m pretty sure that won\u2019t work either for some people npm might return warns/errors for this. Likely it shouldnt be listed as dep at all, but there should be a wrapper that includes it if it\u2019s available. Somewhat similar to how hiredis was used earlier\n\nOn Feb 10, 2017, at 3:17 PM, Gregory Wild-Smith notifications@github.com wrote:\nThat would be cleaner. Making it an optional peer dependency would be much less bloat-y.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/408#issuecomment-279092811, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0YxI2njpAYjm2wMG1yDHW1bgcQ8aks5rbPAEgaJpZM4LCu7A.\n\n\n. https://www.npmjs.com/package/any-promise - this could work for all cases. @Salakar can I take a look at the experimental redis client with proxy? Wonder which techniques allowed for such a drastic speedup. @luin how are we on that? Node < 4 is deprecated and with new V8 we should be moving forward imo. IMO, it just stagnates growth, there were no major updates/fixes/features for many months now and the ability to drop node 4 is still something to consider. Of course, this would be a breaking change, but there is no reason to force any1 to upgrade at this point - if you cant use new version, just stick to current release, if you can - use the new one. Its actually common to only support latest LTS and beyond at this point to simplify transpiling, etc\n. one way is probably ask for a promise implementation in the constructor and fallback to default available (ie native Promise). I've actually had to hack my way through this not so long ago. In my experience often cluster is used with pre-hashed keys, and I would want the pipeline to be formed for my requests to do some batching. I know that the requests would go to the same machine, hence for that case it would be reasonable to enable pipelining. @luin what do you think?\nExample code with batching:\n```js\n/*\n * Queues more requests to pipeline\n /\nfunction addToPipeline(key) {\n  // fetchData is a lua script\n  this.pipeline.fetchData(1, key, this.omitFields);\n}\nmodule.exports.batch = function fetchDataBatch(keys, omitFields = []) {\n  const timer = perf('fetchData:batch');\n  const redis = this.redis;\n// this must include {}\n  const prefix = redis.options.keyPrefix;\n  const slot = calcSlot(prefix);\n  // they will all refer to the same slot, because we prefix with {}\n  // this has possibility of throwing, but not likely to since previous operations\n  // would've been rejected already, in a promise this will result in a rejection\n  const nodeKeys = redis.slots[slot];\n  const masters = redis.connectionPool.nodes.master;\n  const masterNode = nodeKeys.reduce((node, key) => node || masters[key], null);\n// uses internal implementation details\n  if (is.fn(masterNode.fetchData) !== true) {\n    masterNode.options.keyPrefix = prefix;\n    masterNode.defineCommand('fetchData', { lua: fetchDataScript });\n    masterNode.options.keyPrefix = null;\n  }\nconst pipeline = masterNode.pipeline();\n  keys.forEach(addToPipeline, { pipeline, omitFields });\nreturn pipeline\n    .exec()\n    .tap(timer('pipeline'))\n    .map(deserializePipeline)\n    .finally(timer);\n};\n```. it throws for lua scripts\nhttps://github.com/luin/ioredis/blob/master/lib/pipeline.js#L240-L243 https://github.com/luin/ioredis/blob/master/lib/pipeline.js#L240-L243\n\nOn Jan 25, 2017, at 8:59 PM, Zihua \u5b50\u9a85 notifications@github.com wrote:\n@AVVS https://github.com/AVVS Why not just call Cluster#pipeline() directly instead of Redis#pipeline() given all your keys belong to the same slot since your prefix them with the common {} flag.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/416#issuecomment-275308311, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0Zepds23VgyfGBULkI5KTO0gWzYYks5rWCg0gaJpZM4LT4hL.\n\n\n. @PhillippOhlandt I might be missing something, but your example contains master/slave, which is not really a cluster. Use this https://github.com/makeomatic/docker-redis-cluster for pre-bootstrapped testing cluster if you want. mine isn't - just for testing. But I've noticed there is master+slave, but no script for distributing slots (which is what makes redis a cluster) and minimal setup is 3 masters. But I might be wrong and simply missed the boot script. Well, thats the problem - you cant use ioredis cluster because it relies on slots and you simply dont have them. https://redis.io/topics/cluster-tutorial should have enough information. All you have is separate redis instances and their slaves. yeah, well, not going to happen until v4 redis cluster at least, because of a) redis cluster doesnt support docker yet (unless you do host networking) - advertised ip address would be different from the one they can talk through b) doesnt autoscale either. use sentinel, 1 master + slaves + automatic failover. Works just fine - you still can scale reads, and if need be you can have multiple sentinel groups for manual write splitting, which you were aiming to do anyway\n\nOn Feb 21, 2017, at 10:41 PM, Phillipp Ohlandt notifications@github.com wrote:\nDo you know if there is a planned release date? This is a big blocker for us because we don't want to go with a single redis instance online.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/431#issuecomment-281583567, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0eEjchW4ZSjXrdVEYo0-ch0g7_yzks5re9iFgaJpZM4L_Bne.\n\n\n. > The problem is with read-write splitting in Cluster mode with replication (not with sharded clusters, nor sentinel). When in cluster mode, it always tries to search for a slot to which a command has to be sent, and if the cluster is a replication one (not sharding), it fails because all slots are in all nodes.\nIts not a cluster - its a replicated single machine setup. Master - Slave, which should use sentinel for monitoring and automatic failover. It seems there is confusion over a cluster and sentinel.\n@mvaker what you are referring to is a sentinel based setup (should be). I've spent some time coding instrumentation trying to get least intrusive API, I've ended up monkey-patching Command.initPromise() and Script.execute()- code is located here - https://github.com/makeomatic/opentracing-js-ioredis/blob/master/index.js\nI'll add docs & more tests in the next couple of days\nCurrent implementation is lacking individual tracking for multi / pipeline calls - it will only track initiation of pipeline. After a bit more research I have an idea about using https://nodejs.org/dist/latest-v8.x/docs/api/async_hooks.html, but that really limits it to recent nodejs versions, so unlikely to be of use for the next year or so.\nBut when in the future there could literally be no need to use any wrapper methods as tracing could be performed with the async hooks completely hidden from the userland code. I don't mind the reasoning behind the change, but would hide it behind a setting just to be on the safe side. usually people bundle lodash with their app and then it's deduped. That being said the change will actually increase the final bundle size given that scenario. Change as it is will work, but, at the same time, there will be a performance penalty associated with this, as we'd have to create shallow copies of args for each command\n@luin what do you think?. I think you should simply use response transformers, that would lead to the desired result\n\nOn Aug 29, 2017, at 9:01 PM, kzz555 notifications@github.com wrote:\nHi there, I wonder if anyone would +1 if the returned value of some certain method like smembers and hgetall could be ES6 Set and Map?\nThis could be alternative like using redis.smembers().asSet() and wouldn't break the original structure.\nI'd like to make a PR if you guys agree to it.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/luin/ioredis/issues/515, or mute the thread https://github.com/notifications/unsubscribe-auth/ABol0X9WNlGfxI_T0jbkzqhezIa8K_3Qks5sdN6TgaJpZM4PG1qv.\n\n\n. So the opts I've passed initially to redis cluster constructor were:\njs\nkeyPrefix: '{xxx}',\n// pass this to constructor\ndropBufferSupport: true,\nAnd that always returned buffers for some reason, when I've changed to:\njs\n    keyPrefix: '{xxx}',\n    // pass this to constructor\n    redisOptions: {\n      dropBufferSupport: true,\n    },\nIt started working. I dont understand why it would be the case though, as the defaults were supposed to be strings anyway. I assume it could somehow be related to https://github.com/luin/ioredis/blob/master/lib/cluster/ConnectionPool.ts#L133. Probably its because 'drain' event happens before new node is added. And this is likely specific to hostname -> ip address resolution. Another log, with slightly altered ioredis (added +node/-node/drain debug logs)\n2018-09-27T16:58:41.508Z ioredis:cluster status: [empty] -> wait\n2018-09-27T16:58:41.508Z ioredis:cluster status: wait -> connecting\n2018-09-27T16:58:41.509Z ioredis:cluster:connectionPool Connecting to redis-cluster:7000 as master\n2018-09-27T16:58:41.510Z ioredis:redis status[redis-cluster:7000]: [empty] -> wait\n2018-09-27T16:58:41.510Z ioredis:cluster +node\n2018-09-27T16:58:41.511Z ioredis:cluster:connectionPool Connecting to redis-cluster:7001 as master\n2018-09-27T16:58:41.511Z ioredis:redis status[redis-cluster:7001]: [empty] -> wait\n2018-09-27T16:58:41.511Z ioredis:cluster +node\n2018-09-27T16:58:41.511Z ioredis:cluster:connectionPool Connecting to redis-cluster:7002 as master\n2018-09-27T16:58:41.511Z ioredis:redis status[redis-cluster:7002]: [empty] -> wait\n2018-09-27T16:58:41.511Z ioredis:cluster +node\n2018-09-27T16:58:41.512Z ioredis:cluster getting slot cache from redis-cluster:7001\n2018-09-27T16:58:41.514Z ioredis:redis status[redis-cluster:7001]: wait -> connecting\n2018-09-27T16:58:41.515Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-27T16:58:41.516Z ioredis:redis status[redis-cluster:7002]: wait -> connecting\n2018-09-27T16:58:41.521Z ioredis:redis status[172.18.0.5:7001]: connecting -> connect\n2018-09-27T16:58:41.523Z ioredis:redis write command[0] -> info()\n2018-09-27T16:58:41.527Z ioredis:redis status[172.18.0.5:7001]: connect -> ready\n2018-09-27T16:58:41.522Z ioredis:redis write command[0] -> info()\n2018-09-27T16:58:41.523Z ioredis:redis status[172.18.0.5:7002]: connecting -> connect\n2018-09-27T16:58:41.531Z ioredis:redis status[redis-cluster:7000]: close -> end\n2018-09-27T16:58:41.531Z ioredis:cluster:connectionPool Disconnect redis-cluster:7001 because the node does not hold any slot\n2018-09-27T16:58:41.531Z ioredis:redis status[172.18.0.5:7001]: [empty] -> wait\n2018-09-27T16:58:41.532Z ioredis:cluster +node\n2018-09-27T16:58:41.532Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7002 as master\n2018-09-27T16:58:41.532Z ioredis:redis status[172.18.0.5:7002]: [empty] -> wait\n2018-09-27T16:58:41.532Z ioredis:cluster +node\n2018-09-27T16:58:41.532Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7000 as master\n2018-09-27T16:58:41.533Z ioredis:cluster status: connecting -> connect\n2018-09-27T16:58:41.534Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-27T16:58:41.535Z ioredis:cluster -node\n2018-09-27T16:58:41.536Z ioredis:redis status[172.18.0.5:7001]: ready -> close\n2018-09-27T16:58:41.527Z ioredis:connection send 1 commands in offline queue\n2018-09-27T16:58:41.527Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:58:41.528Z ioredis:redis status[172.18.0.5:7002]: connect -> ready\n2018-09-27T16:58:41.530Z ioredis:cluster:connectionPool Disconnect redis-cluster:7000 because the node does not hold any slot\n2018-09-27T16:58:41.565Z ioredis:cluster -node\n2018-09-27T16:58:41.567Z ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting...\n2018-09-27T16:58:41.567Z ioredis:cluster status: connect -> disconnecting\n2018-09-27T16:58:41.567Z ioredis:cluster:connectionPool Disconnect redis-cluster:7002 because the node does not hold any slot\n2018-09-27T16:58:41.567Z ioredis:cluster:connectionPool Disconnect 172.18.0.5:7001 because the node does not hold any slot\n2018-09-27T16:58:41.530Z ioredis:redis status[redis-cluster:7000]: wait -> close\n2018-09-27T16:58:41.530Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.568Z ioredis:redis status[172.18.0.5:7001]: wait -> close\n2018-09-27T16:58:41.568Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.568Z ioredis:redis status[172.18.0.5:7001]: close -> end\n2018-09-27T16:58:41.568Z ioredis:cluster:connectionPool Disconnect 172.18.0.5:7002 because the node does not hold any slot\n2018-09-27T16:58:41.568Z ioredis:redis status[172.18.0.5:7002]: wait -> close\n2018-09-27T16:58:41.531Z ioredis:cluster:connectionPool Disconnect redis-cluster:7002 because the node does not hold any slot\n2018-09-27T16:58:41.531Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7001 as master\n2018-09-27T16:58:41.532Z ioredis:redis status[172.18.0.5:7000]: [empty] -> wait\n2018-09-27T16:58:41.532Z ioredis:cluster +node\n2018-09-27T16:58:41.568Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.568Z ioredis:redis status[172.18.0.5:7002]: close -> end\n2018-09-27T16:58:41.569Z ioredis:cluster:connectionPool Disconnect 172.18.0.5:7000 because the node does not hold any slot\n2018-09-27T16:58:41.569Z ioredis:redis status[172.18.0.5:7000]: wait -> close\n2018-09-27T16:58:41.569Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.569Z ioredis:redis status[172.18.0.5:7000]: close -> end\n2018-09-27T16:58:41.569Z ioredis:cluster -node\n2018-09-27T16:58:41.569Z ioredis:cluster -node\n2018-09-27T16:58:41.569Z ioredis:cluster -node\n2018-09-27T16:58:41.569Z ioredis:redis status[172.18.0.5:7002]: ready -> close\n2018-09-27T16:58:41.569Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.569Z ioredis:redis status[172.18.0.5:7002]: close -> end\n2018-09-27T16:58:41.570Z ioredis:cluster -node\n2018-09-27T16:58:41.570Z ioredis:cluster drain\n2018-09-27T16:58:41.570Z ioredis:cluster status: disconnecting -> close\n2018-09-27T16:58:41.536Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.537Z ioredis:redis status[172.18.0.5:7001]: close -> end\n2018-09-27T16:58:41.570Z ioredis:cluster status: close -> reconnecting\n2018-09-27T16:58:41.673Z ioredis:cluster Cluster is disconnected. Retrying after 102ms\n2018-09-27T16:58:41.674Z ioredis:cluster status: reconnecting -> connecting\n2018-09-27T16:58:41.675Z ioredis:cluster:connectionPool Connecting to redis-cluster:7000 as master\n2018-09-27T16:58:41.676Z ioredis:redis status[redis-cluster:7000]: [empty] -> wait\n2018-09-27T16:58:41.676Z ioredis:cluster +node\n2018-09-27T16:58:41.676Z ioredis:cluster:connectionPool Connecting to redis-cluster:7001 as master\n2018-09-27T16:58:41.676Z ioredis:redis status[redis-cluster:7001]: [empty] -> wait\n2018-09-27T16:58:41.676Z ioredis:cluster +node\n2018-09-27T16:58:41.676Z ioredis:cluster:connectionPool Connecting to redis-cluster:7002 as master\n2018-09-27T16:58:41.676Z ioredis:redis status[redis-cluster:7002]: [empty] -> wait\n2018-09-27T16:58:41.676Z ioredis:cluster +node\n2018-09-27T16:58:41.676Z ioredis:cluster getting slot cache from redis-cluster:7001\n2018-09-27T16:58:41.677Z ioredis:redis status[redis-cluster:7001]: wait -> connecting\n2018-09-27T16:58:41.677Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-27T16:58:41.681Z ioredis:redis status[172.18.0.5:7001]: connecting -> connect\n2018-09-27T16:58:41.681Z ioredis:redis write command[0] -> info()\n2018-09-27T16:58:41.682Z ioredis:redis status[172.18.0.5:7001]: connect -> ready\n2018-09-27T16:58:41.682Z ioredis:connection send 1 commands in offline queue\n2018-09-27T16:58:41.682Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:58:41.700Z ioredis:cluster:connectionPool Disconnect redis-cluster:7000 because the node does not hold any slot\n2018-09-27T16:58:41.700Z ioredis:redis status[redis-cluster:7000]: wait -> close\n2018-09-27T16:58:41.700Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.700Z ioredis:redis status[redis-cluster:7000]: close -> end\n2018-09-27T16:58:41.701Z ioredis:cluster:connectionPool Disconnect redis-cluster:7001 because the node does not hold any slot\n2018-09-27T16:58:41.701Z ioredis:cluster:connectionPool Disconnect redis-cluster:7002 because the node does not hold any slot\n2018-09-27T16:58:41.701Z ioredis:redis status[redis-cluster:7002]: wait -> close\n2018-09-27T16:58:41.701Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.701Z ioredis:redis status[redis-cluster:7002]: close -> end\n2018-09-27T16:58:41.701Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7001 as master\n2018-09-27T16:58:41.701Z ioredis:redis status[172.18.0.5:7001]: [empty] -> wait\n2018-09-27T16:58:41.701Z ioredis:cluster +node\n2018-09-27T16:58:41.702Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7002 as master\n2018-09-27T16:58:41.702Z ioredis:redis status[172.18.0.5:7002]: [empty] -> wait\n2018-09-27T16:58:41.702Z ioredis:cluster +node\n2018-09-27T16:58:41.702Z ioredis:cluster:connectionPool Connecting to 172.18.0.5:7000 as master\n2018-09-27T16:58:41.702Z ioredis:redis status[172.18.0.5:7000]: [empty] -> wait\n2018-09-27T16:58:41.702Z ioredis:cluster +node\n2018-09-27T16:58:41.702Z ioredis:cluster status: connecting -> connect\n2018-09-27T16:58:41.703Z ioredis:redis status[172.18.0.5:7002]: wait -> connecting\n2018-09-27T16:58:41.703Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-27T16:58:41.703Z ioredis:cluster -node\n2018-09-27T16:58:41.703Z ioredis:cluster -node\n2018-09-27T16:58:41.704Z ioredis:redis status[172.18.0.5:7002]: connecting -> connect\n2018-09-27T16:58:41.704Z ioredis:redis write command[0] -> info()\n2018-09-27T16:58:41.704Z ioredis:redis status[172.18.0.5:7001]: ready -> close\n2018-09-27T16:58:41.704Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-27T16:58:41.705Z ioredis:redis status[172.18.0.5:7001]: close -> end\n2018-09-27T16:58:41.705Z ioredis:redis status[172.18.0.5:7000]: wait -> connecting\n2018-09-27T16:58:41.705Z ioredis:cluster -node\n2018-09-27T16:58:41.706Z ioredis:redis status[172.18.0.5:7000]: connecting -> connect\n2018-09-27T16:58:41.706Z ioredis:redis write command[0] -> info()\n2018-09-27T16:58:41.706Z ioredis:redis status[172.18.0.5:7002]: connect -> ready\n2018-09-27T16:58:41.707Z ioredis:connection send 1 commands in offline queue\n2018-09-27T16:58:41.707Z ioredis:redis write command[0] -> cluster(info)\n2018-09-27T16:58:41.707Z ioredis:redis status[172.18.0.5:7000]: connect -> ready\n2018-09-27T16:58:41.708Z ioredis:cluster status: connect -> ready\n2018-09-27T16:58:46.713Z ioredis:cluster getting slot cache from 172.18.0.5:7002\n2018-09-27T16:58:46.714Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:58:51.714Z ioredis:cluster getting slot cache from 172.18.0.5:7002\n2018-09-27T16:58:51.714Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:58:56.714Z ioredis:cluster getting slot cache from 172.18.0.5:7000\n2018-09-27T16:58:56.715Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:59:01.719Z ioredis:cluster getting slot cache from 172.18.0.5:7001\n2018-09-27T16:59:01.719Z ioredis:redis status[172.18.0.5:7001]: wait -> connecting\n2018-09-27T16:59:01.720Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-27T16:59:01.720Z ioredis:redis status[172.18.0.5:7001]: connecting -> connect\n2018-09-27T16:59:01.720Z ioredis:redis write command[0] -> info()\n2018-09-27T16:59:01.721Z ioredis:redis status[172.18.0.5:7001]: connect -> ready\n2018-09-27T16:59:01.721Z ioredis:connection send 1 commands in offline queue\n2018-09-27T16:59:01.721Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-27T16:59:06.723Z ioredis:cluster getting slot cache from 172.18.0.5:7001\n2018-09-27T16:59:06.724Z ioredis:redis write command[0] -> cluster(slots). Ok, so digging further, it actually seems that Ready check failed is what causes it.\nI assume this happened because we started ready check and then in the middle of it issued .disconnect(). https://github.com/luin/ioredis/pull/710 - please check. @luin managed to reproduce endless reconnect, posting logs soon. So initially the cluster was failing, but then it got stuck with endless slot refresh\nRelevant redis connection options:\njs\nredis: {\n    hosts: Array.from({ length: 3 }).map((_, idx) => ({\n      host: 'redis',\n      port: 7000 + idx,\n    })),\n  },\nSmall log where it got stuck:\n2018-10-26T19:13:05.296Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:13:05.296Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:05.297Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:05.297Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:05.298Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:05.298Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:05.299Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:10.302Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:13:10.303Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:10.305Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:10.306Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:10.306Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:10.307Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:10.308Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\nComplete log from start to the endless reconnect:\n2018-10-26T19:11:31.187Z ioredis:cluster status: [empty] -> wait\n2018-10-26T19:11:31.188Z mservice:redisCluster attaching lua\n2018-10-26T19:11:31.189Z mservice:lua loading form /src/scripts\n2018-10-26T19:11:31.191Z mservice:lua attaching resolveUserId\n2018-10-26T19:11:31.194Z ioredis:cluster status: wait -> connecting\n2018-10-26T19:11:31.202Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.206Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.207Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.210Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.211Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.211Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.211Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.212Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.213Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:31.219Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:31.220Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.221Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:31.222Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.222Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.236Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:31.238Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:31.248Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:31.250Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.250Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.258Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:31.258Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.260Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:31.260Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.261Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:31.262Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.264Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.267Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.267Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.267Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.268Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.268Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.268Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.269Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.269Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.269Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.270Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.270Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:31.271Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.271Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.271Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.271Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:31.271Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.271Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.295Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:31.295Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.295Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.295Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.296Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:31.400Z ioredis:cluster Cluster is disconnected. Retrying after 102ms\n2018-10-26T19:11:31.400Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:31.402Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.402Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.402Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.402Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.402Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.403Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.403Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.403Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.403Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:31.403Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:31.404Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.404Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:31.404Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.404Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.405Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:31.405Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:31.406Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:31.406Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.406Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.408Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:31.408Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.409Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:31.409Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.410Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:31.410Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.410Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:31.411Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.412Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:31.412Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:31.413Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:31.413Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.413Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.414Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.414Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.414Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.414Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.414Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.415Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.415Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.415Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.415Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.415Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.416Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.416Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:31.416Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.416Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.417Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:31.418Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.418Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.418Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:31.418Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.418Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.418Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.418Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:31.523Z ioredis:cluster Cluster is disconnected. Retrying after 104ms\n2018-10-26T19:11:31.523Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:31.524Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.524Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.524Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.524Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.525Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.525Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.525Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.525Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.525Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:31.525Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:31.525Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.526Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:31.526Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.526Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.526Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:31.527Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:31.527Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:31.528Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.528Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.530Z ioredis:cluster cluster slots result count: 2\n2018-10-26T19:11:31.530Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.531Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.532Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:31.532Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.532Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:31.532Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.532Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.532Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.533Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:31.533Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.533Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:31.535Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:31.537Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:31.537Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.537Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.542Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.543Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.543Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.543Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.543Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.543Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.543Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.543Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.543Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.543Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.545Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:31.545Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.545Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.546Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:31.546Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.546Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.547Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.547Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:31.654Z ioredis:cluster Cluster is disconnected. Retrying after 106ms\n2018-10-26T19:11:31.654Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:31.655Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.655Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.655Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.655Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.656Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.656Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.656Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.656Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.656Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:31.656Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:31.657Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.657Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:31.657Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.657Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.658Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:31.658Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:31.659Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:31.659Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.659Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.660Z ioredis:cluster cluster slots result count: 2\n2018-10-26T19:11:31.660Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.660Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.660Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:31.660Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.660Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:31.660Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.660Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.660Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.661Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:31.661Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.662Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:31.662Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:31.663Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:31.664Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.664Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.664Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.664Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.664Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.665Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.665Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.665Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.665Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.665Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.665Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.665Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.666Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:31.666Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.667Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.667Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:31.667Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.667Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.667Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.667Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:31.780Z ioredis:cluster Cluster is disconnected. Retrying after 108ms\n2018-10-26T19:11:31.781Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:31.782Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.782Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.782Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.782Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.782Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.783Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.783Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.783Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.783Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:31.783Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:31.783Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.783Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:31.784Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.784Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.784Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:31.784Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:31.785Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:31.785Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.785Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.786Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:31.786Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.786Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.787Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:31.787Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:31.787Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.787Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.788Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.788Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.788Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.788Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.788Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.788Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.788Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.788Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.788Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.789Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:31.789Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.789Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.789Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.789Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:31.789Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.789Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.789Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.790Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:31.790Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.790Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.791Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.791Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:31.901Z ioredis:cluster Cluster is disconnected. Retrying after 110ms\n2018-10-26T19:11:31.902Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:31.902Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:31.903Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:31.903Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:31.903Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:31.904Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:31.904Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:31.904Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:31.904Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:31.904Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:31.905Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:31.905Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.905Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:31.906Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:31.906Z ioredis:cluster:subscriber started\n2018-10-26T19:11:31.907Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:31.907Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:31.908Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:31.908Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.908Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:31.909Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:31.909Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:31.910Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:31.910Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:31.911Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:31.911Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:31.911Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:31.912Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.913Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:31.913Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:31.914Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:31.914Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:31.914Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:31.915Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:31.915Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:31.915Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:31.916Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:31.916Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.916Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:31.916Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:31.916Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:31.916Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:31.916Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:31.917Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.917Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:31.917Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:31.917Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:31.918Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:31.919Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.919Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:31.919Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:31.920Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:31.920Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:31.920Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:31.920Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.033Z ioredis:cluster Cluster is disconnected. Retrying after 112ms\n2018-10-26T19:11:32.033Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.035Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.035Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.035Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.036Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.036Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.036Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.037Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.037Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.037Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.038Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.038Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.038Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:32.039Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.039Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.040Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.041Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.042Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.042Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.042Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.043Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.043Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.043Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.043Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.044Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.044Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.044Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:32.044Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.046Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:32.046Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:32.049Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:32.049Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.049Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.050Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.050Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.050Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.051Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.051Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.051Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.051Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.052Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.052Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.053Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.053Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.053Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.054Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.054Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.055Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:32.055Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.055Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.056Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.056Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.057Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.057Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.057Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.172Z ioredis:cluster Cluster is disconnected. Retrying after 114ms\n2018-10-26T19:11:32.172Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.174Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.174Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.174Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.175Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.175Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.175Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.175Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.176Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.176Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.176Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.176Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.176Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:32.177Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.177Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.177Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.178Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.178Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.179Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.179Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.179Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.180Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.180Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.180Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.180Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.180Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.181Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.181Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.182Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.182Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.182Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.182Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.182Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.182Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.182Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.182Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.182Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.182Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.182Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.183Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.183Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:32.183Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.183Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.183Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.184Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.185Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.185Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.185Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.185Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.302Z ioredis:cluster Cluster is disconnected. Retrying after 116ms\n2018-10-26T19:11:32.302Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.303Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.303Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.303Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.304Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.304Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.304Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.304Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.305Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.305Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.305Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.306Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.306Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:32.306Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.306Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.307Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.307Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.308Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.308Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.308Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.309Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.309Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.309Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.310Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.310Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.310Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.310Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:32.311Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.311Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:32.312Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:32.313Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:32.313Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.313Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.313Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.313Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.314Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.314Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.314Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.314Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.315Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.315Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.315Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.315Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.315Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.316Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.316Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.316Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.317Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.317Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.318Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.318Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:32.318Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.319Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.319Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.319Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.437Z ioredis:cluster Cluster is disconnected. Retrying after 118ms\n2018-10-26T19:11:32.437Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.439Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.439Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.439Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.439Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.440Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.440Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.440Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.440Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.441Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:32.441Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:32.441Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.441Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:32.442Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.442Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.442Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:32.443Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:32.444Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:32.444Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.444Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.445Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.445Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.445Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.445Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.445Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:32.445Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.445Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.446Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.446Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.446Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.446Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.447Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.447Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.447Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.447Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.447Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.447Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.448Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:32.448Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.448Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.448Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.448Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:32.448Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.448Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.449Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:32.449Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.449Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.450Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.450Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.571Z ioredis:cluster Cluster is disconnected. Retrying after 120ms\n2018-10-26T19:11:32.571Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.573Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.573Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.573Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.574Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.574Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.574Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.574Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.575Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.575Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.575Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.575Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.575Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:32.576Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.576Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.576Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.576Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.578Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.578Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.578Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.579Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.580Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.580Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.580Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.580Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.580Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.580Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:32.581Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.581Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:32.582Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:32.582Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:32.583Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.583Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.583Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.583Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.583Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.583Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.583Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.584Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.584Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.584Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.584Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.584Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.584Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.584Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.584Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.584Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.585Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:32.585Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.585Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.585Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.585Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.585Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.585Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.585Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.708Z ioredis:cluster Cluster is disconnected. Retrying after 122ms\n2018-10-26T19:11:32.708Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.709Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.709Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.709Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.709Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.710Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.710Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.710Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.710Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.710Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:32.710Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:32.711Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.711Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:32.711Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.711Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.712Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:32.712Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:32.713Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:32.713Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.713Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.714Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.714Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.714Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.714Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.715Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:32.715Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.715Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.715Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.716Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.716Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.717Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.717Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.717Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.718Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.718Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.718Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.718Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.718Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.718Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.718Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.718Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.719Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.719Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.719Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.719Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.719Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.719Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.720Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:32.720Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.720Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.720Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.720Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.720Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.720Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.720Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.845Z ioredis:cluster Cluster is disconnected. Retrying after 124ms\n2018-10-26T19:11:32.845Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.846Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.846Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.846Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.846Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.847Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.847Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.847Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.847Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.847Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.848Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.848Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.848Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:32.848Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.848Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.849Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.849Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.851Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.851Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.851Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.852Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.852Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.853Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.853Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.854Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.854Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.854Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.855Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.855Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.855Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.855Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.855Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.855Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.855Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.856Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.856Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.856Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.856Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.856Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:32.856Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:32.856Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:32.856Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.856Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:32.857Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:32.858Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:32.858Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.858Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:32.858Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:32.858Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:32.986Z ioredis:cluster Cluster is disconnected. Retrying after 126ms\n2018-10-26T19:11:32.986Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:32.987Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:32.987Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:32.987Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:32.987Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:32.987Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:32.988Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:32.988Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:32.988Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:32.988Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:32.988Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:32.988Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.989Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:32.989Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:32.989Z ioredis:cluster:subscriber started\n2018-10-26T19:11:32.990Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:32.990Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:32.991Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:32.992Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.992Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:32.994Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:32.994Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:32.994Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:32.995Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:32.995Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:32.995Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:32.995Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:32.995Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.996Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:32.996Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:32.997Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:32.997Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:32.997Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:32.998Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:32.998Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:32.998Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:32.998Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:32.998Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.998Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:32.999Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:32.999Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:32.999Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:32.999Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:32.999Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:32.999Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.000Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.000Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.000Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:33.000Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.001Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.001Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:33.001Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.001Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.001Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.002Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.130Z ioredis:cluster Cluster is disconnected. Retrying after 128ms\n2018-10-26T19:11:33.130Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.131Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.132Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.132Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.132Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.132Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.132Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.132Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.133Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.133Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:33.133Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:33.133Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.133Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:33.134Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.134Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.135Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:33.135Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:33.136Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:33.136Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.136Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.137Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.137Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.137Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.138Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.138Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:33.138Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.138Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:33.138Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.139Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:33.139Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:33.140Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:33.140Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.140Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.141Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.141Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.141Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.141Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.141Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.141Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.141Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.141Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.141Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.142Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.142Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.142Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:33.142Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.142Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.143Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:33.143Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.143Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.143Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:33.143Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.143Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.143Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.143Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.238Z ioredis:cluster Cluster is disconnected. Retrying after 130ms\n2018-10-26T19:11:33.238Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.240Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.240Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.240Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.241Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.241Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.242Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.242Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.242Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.242Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:33.243Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:33.243Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.243Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:33.243Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.244Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.244Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:33.245Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:33.245Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:33.246Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.246Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.247Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.247Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.247Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.247Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.247Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:33.247Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.247Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:33.248Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.248Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:33.249Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:33.250Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:33.250Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.250Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.251Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.251Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.251Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.251Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.251Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.251Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.251Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.251Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.252Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.252Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.252Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:33.252Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.252Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.252Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.253Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:33.253Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.253Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.253Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:33.253Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.254Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.254Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.254Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.387Z ioredis:cluster Cluster is disconnected. Retrying after 132ms\n2018-10-26T19:11:33.387Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.388Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.388Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.388Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.388Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.389Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.389Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.389Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.389Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.390Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:33.390Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:33.390Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.390Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:33.390Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.390Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.391Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:33.391Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:33.392Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:33.392Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.392Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.393Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.393Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.393Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.393Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.393Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:33.393Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.394Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.394Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.394Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.394Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.394Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.394Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.394Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.394Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.395Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.395Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.395Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.395Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:33.395Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.395Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.396Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.396Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:33.396Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.396Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.397Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:33.397Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.397Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.397Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.397Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.532Z ioredis:cluster Cluster is disconnected. Retrying after 134ms\n2018-10-26T19:11:33.532Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.533Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.534Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.534Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.536Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.536Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.536Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.537Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.537Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.537Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:33.537Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:33.538Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.538Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:33.538Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.538Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.540Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:33.540Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:33.541Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:33.542Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.542Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.543Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.543Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.543Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.543Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.544Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:33.544Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.544Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.545Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.545Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.545Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.546Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.546Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.546Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.546Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.546Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.547Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.547Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:33.547Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.547Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.547Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.547Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:33.548Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.548Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.548Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.549Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:33.549Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.549Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.549Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.550Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.686Z ioredis:cluster Cluster is disconnected. Retrying after 136ms\n2018-10-26T19:11:33.686Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.687Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.688Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.688Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.688Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.688Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.688Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.688Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.689Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.689Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:33.689Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:33.689Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.689Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:33.689Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.689Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.690Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:33.690Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:33.691Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:33.691Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.691Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.692Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.692Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.692Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.692Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.693Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:33.693Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.693Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:33.693Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.694Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:33.694Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:33.695Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:33.695Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.695Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.696Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.696Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.696Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.696Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.696Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.696Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.696Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.696Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.696Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.697Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.697Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.697Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:33.697Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.697Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.699Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:33.699Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.699Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.699Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:33.700Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.700Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.700Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.700Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.839Z ioredis:cluster Cluster is disconnected. Retrying after 138ms\n2018-10-26T19:11:33.839Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.840Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.840Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.840Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.841Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.841Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.841Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.841Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.841Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.842Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:33.842Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:33.842Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.842Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:33.842Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.842Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.843Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:33.843Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:33.844Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:33.844Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.844Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.845Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.845Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.845Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.845Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.845Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:33.845Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.846Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:33.846Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:33.847Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:33.847Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:33.847Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:33.847Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.847Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:33.848Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:33.848Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:33.848Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:33.848Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:33.848Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.848Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:33.848Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:33.848Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:33.848Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:33.848Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.848Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:33.849Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:33.850Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:33.850Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:33.850Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:33.850Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:33.991Z ioredis:cluster Cluster is disconnected. Retrying after 140ms\n2018-10-26T19:11:33.991Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:33.992Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:33.992Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:33.992Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:33.993Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:33.993Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:33.993Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:33.993Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:33.993Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:33.993Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:33.994Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:33.994Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.994Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:33.995Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:33.995Z ioredis:cluster:subscriber started\n2018-10-26T19:11:33.996Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:33.996Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:33.997Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:33.997Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:33.997Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:33.998Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:33.998Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:33.998Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:33.998Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:33.999Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:33.999Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:33.999Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:34.000Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.003Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:34.003Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:34.004Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:34.004Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.004Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.005Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.005Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.005Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.006Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.006Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.006Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.006Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.006Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.006Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.006Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:34.007Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.007Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.007Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.007Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.008Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:34.008Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.008Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.008Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:34.008Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.009Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.009Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.009Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.152Z ioredis:cluster Cluster is disconnected. Retrying after 142ms\n2018-10-26T19:11:34.152Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.153Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.153Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.153Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.153Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.154Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.154Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.154Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.154Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.154Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:34.154Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:34.154Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.154Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:34.154Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.154Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.155Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:34.155Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:34.156Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:34.156Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.156Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.157Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.157Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.157Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.157Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.157Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:34.157Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.158Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.158Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.158Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.158Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.158Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.158Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.158Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.159Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.159Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.159Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.159Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:34.159Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.159Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.159Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.159Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.159Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:34.159Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.159Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.160Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:34.160Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.160Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.160Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.160Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.305Z ioredis:cluster Cluster is disconnected. Retrying after 144ms\n2018-10-26T19:11:34.305Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.306Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.306Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.306Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.307Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.307Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.307Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.307Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.307Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.307Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:34.308Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:34.308Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.308Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:34.308Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.308Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.309Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:34.309Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:34.310Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:34.310Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.311Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.311Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.311Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.312Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.312Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.312Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:34.312Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.312Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:34.312Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.313Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:34.313Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:34.314Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:34.314Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.314Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.315Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.315Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.315Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.315Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.315Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.315Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.316Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.316Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.316Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.316Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:34.316Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.316Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.316Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.316Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.317Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:34.317Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.317Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.317Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:34.317Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.318Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.318Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.318Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.464Z ioredis:cluster Cluster is disconnected. Retrying after 146ms\n2018-10-26T19:11:34.465Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.466Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.466Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.466Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.467Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.467Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.468Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.468Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.468Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.468Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:34.469Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:34.469Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.469Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7002\n2018-10-26T19:11:34.470Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.470Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.470Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:34.471Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:34.472Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:34.472Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.472Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.473Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.473Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.473Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.474Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.474Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:34.474Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.475Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:34.475Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.476Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:34.476Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:34.478Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:34.478Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.478Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.479Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.479Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.479Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.479Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.479Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.479Z ioredis:redis status[172.31.0.2:7002 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.479Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.480Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.480Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.480Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:34.480Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.480Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.480Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.480Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.481Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:34.481Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.481Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.481Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:34.481Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.482Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.482Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.482Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.630Z ioredis:cluster Cluster is disconnected. Retrying after 148ms\n2018-10-26T19:11:34.630Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.631Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.632Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.632Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.632Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.633Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.633Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.633Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.633Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.634Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:34.634Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:34.634Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.634Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:34.635Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.635Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.638Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:34.638Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:34.639Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:34.639Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.640Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.643Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.643Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.644Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.644Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.645Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:34.645Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.646Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:34.647Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.648Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:34.648Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:34.649Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:34.649Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.649Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.650Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.650Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.651Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.651Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.651Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.651Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.651Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.651Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.652Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.652Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.652Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:34.652Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.652Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.653Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.654Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:34.654Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.654Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.655Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:34.655Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.655Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.655Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.655Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.806Z ioredis:cluster Cluster is disconnected. Retrying after 150ms\n2018-10-26T19:11:34.806Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.808Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.808Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.809Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.809Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.809Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.809Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.809Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.810Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.810Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:34.810Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:34.811Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.811Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:34.811Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.811Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.812Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:34.812Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:34.813Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:34.814Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.814Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.815Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.815Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.816Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.816Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.817Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:34.817Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.817Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:34.818Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.819Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:34.819Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:34.821Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:34.821Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.821Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.822Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.822Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.822Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.822Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.822Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.822Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.822Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.823Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.823Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.823Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.823Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.823Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:34.823Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.823Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.824Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:34.824Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.825Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.825Z ioredis:redis status[172.31.0.2:7001]: ready -> close\n2018-10-26T19:11:34.825Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.825Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.825Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.825Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:34.978Z ioredis:cluster Cluster is disconnected. Retrying after 152ms\n2018-10-26T19:11:34.978Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:34.979Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:34.979Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:34.979Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:34.980Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:34.980Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:34.980Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:34.980Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:34.980Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:34.980Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:11:34.981Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:11:34.981Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.981Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7000\n2018-10-26T19:11:34.981Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:34.982Z ioredis:cluster:subscriber started\n2018-10-26T19:11:34.982Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:11:34.983Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:11:34.984Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:11:34.984Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:34.984Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:34.986Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:34.986Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:34.986Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:34.986Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:34.986Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:11:34.986Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:34.987Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:34.987Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:34.987Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:34.987Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:34.988Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:34.988Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.988Z ioredis:redis status[172.31.0.2:7000 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:34.988Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:34.988Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:34.988Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:34.988Z ioredis:redis status[172.31.0.2:7000]: wait -> close\n2018-10-26T19:11:34.988Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.988Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:34.988Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:34.988Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:34.988Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.989Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:34.989Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:34.989Z ioredis:redis status[172.31.0.2:7002]: ready -> close\n2018-10-26T19:11:34.989Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:34.990Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:34.990Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:34.990Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:35.143Z ioredis:cluster Cluster is disconnected. Retrying after 154ms\n2018-10-26T19:11:35.144Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:35.144Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:35.145Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:35.145Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:35.146Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:35.146Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:35.146Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:35.146Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:35.146Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:35.147Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:35.147Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:35.147Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:35.147Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:35.148Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:35.148Z ioredis:cluster:subscriber started\n2018-10-26T19:11:35.149Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:35.150Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:35.151Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:35.151Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:35.151Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:35.152Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:35.152Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:35.153Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:35.153Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:35.154Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:35.155Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:35.155Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:35.156Z ioredis:cluster cluster state not ok (fail)\n2018-10-26T19:11:35.156Z ioredis:cluster Ready check failed (fail). Reconnecting...\n2018-10-26T19:11:35.156Z ioredis:cluster status: connect -> disconnecting\n2018-10-26T19:11:35.156Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: wait -> close\n2018-10-26T19:11:35.156Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:35.157Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: close -> end\n2018-10-26T19:11:35.157Z ioredis:cluster:subscriber stopped\n2018-10-26T19:11:35.157Z ioredis:cluster:connectionPool Reset with []\n2018-10-26T19:11:35.157Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7000 because the node does not hold any slot\n2018-10-26T19:11:35.157Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7001 because the node does not hold any slot\n2018-10-26T19:11:35.157Z ioredis:redis status[172.31.0.2:7001]: wait -> close\n2018-10-26T19:11:35.158Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:35.158Z ioredis:redis status[172.31.0.2:7001]: close -> end\n2018-10-26T19:11:35.158Z ioredis:cluster:connectionPool Disconnect 172.31.0.2:7002 because the node does not hold any slot\n2018-10-26T19:11:35.158Z ioredis:redis status[172.31.0.2:7002]: wait -> close\n2018-10-26T19:11:35.158Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:35.158Z ioredis:redis status[172.31.0.2:7002]: close -> end\n2018-10-26T19:11:35.159Z ioredis:redis status[172.31.0.2:7000]: ready -> close\n2018-10-26T19:11:35.159Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-10-26T19:11:35.159Z ioredis:redis status[172.31.0.2:7000]: close -> end\n2018-10-26T19:11:35.159Z ioredis:cluster status: disconnecting -> close\n2018-10-26T19:11:35.159Z ioredis:cluster status: close -> reconnecting\n2018-10-26T19:11:35.316Z ioredis:cluster Cluster is disconnected. Retrying after 156ms\n2018-10-26T19:11:35.316Z ioredis:cluster status: reconnecting -> connecting\n2018-10-26T19:11:35.318Z ioredis:cluster resolved hostname redis to IP 172.31.0.2\n2018-10-26T19:11:35.318Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7000 },\n  { host: '172.31.0.2', port: 7001 },\n  { host: '172.31.0.2', port: 7002 } ]\n2018-10-26T19:11:35.318Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7000 as master\n2018-10-26T19:11:35.319Z ioredis:redis status[172.31.0.2:7000]: [empty] -> wait\n2018-10-26T19:11:35.319Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7001 as master\n2018-10-26T19:11:35.319Z ioredis:redis status[172.31.0.2:7001]: [empty] -> wait\n2018-10-26T19:11:35.319Z ioredis:cluster:connectionPool Connecting to 172.31.0.2:7002 as master\n2018-10-26T19:11:35.320Z ioredis:redis status[172.31.0.2:7002]: [empty] -> wait\n2018-10-26T19:11:35.320Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:35.320Z ioredis:redis status[172.31.0.2:7000]: wait -> connecting\n2018-10-26T19:11:35.320Z ioredis:redis queue command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:35.321Z ioredis:cluster:subscriber selected a subscriber 172.31.0.2:7001\n2018-10-26T19:11:35.321Z ioredis:redis status[172.31.0.2:7001 (ioredisClusterSubscriber)]: [empty] -> wait\n2018-10-26T19:11:35.321Z ioredis:cluster:subscriber started\n2018-10-26T19:11:35.322Z ioredis:redis status[172.31.0.2:7000]: connecting -> connect\n2018-10-26T19:11:35.322Z ioredis:redis write command[172.31.0.2:7000]: 0 -> info([])\n2018-10-26T19:11:35.323Z ioredis:redis status[172.31.0.2:7000]: connect -> ready\n2018-10-26T19:11:35.323Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:35.324Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:35.324Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:35.324Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:35.325Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:35.325Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:35.325Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:35.325Z ioredis:cluster status: connecting -> connect\n2018-10-26T19:11:35.326Z ioredis:redis status[172.31.0.2:7001]: wait -> connecting\n2018-10-26T19:11:35.326Z ioredis:redis queue command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:35.327Z ioredis:redis status[172.31.0.2:7001]: connecting -> connect\n2018-10-26T19:11:35.327Z ioredis:redis write command[172.31.0.2:7001]: 0 -> info([])\n2018-10-26T19:11:35.327Z ioredis:redis status[172.31.0.2:7001]: connect -> ready\n2018-10-26T19:11:35.328Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:11:35.328Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'info' ])\n2018-10-26T19:11:35.329Z ioredis:cluster status: connect -> ready\n2018-10-26T19:11:40.332Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:40.333Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:40.334Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:40.334Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:40.335Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:40.336Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:40.337Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:45.338Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:11:45.338Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:45.346Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:45.346Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:45.346Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:45.347Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:45.347Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:50.341Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:50.342Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:50.344Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:50.344Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:50.344Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:50.344Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:50.345Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:11:55.345Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:11:55.346Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:11:55.348Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:11:55.348Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:11:55.349Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:11:55.350Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:11:55.351Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:00.351Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:12:00.351Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:00.355Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:00.355Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:00.355Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:00.356Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:00.356Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:05.320Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:05.321Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:05.322Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:05.322Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:05.322Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:05.323Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:05.323Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:10.326Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:10.326Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:10.327Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:10.327Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:10.329Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:10.330Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:10.331Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:15.331Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:15.331Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:15.336Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:15.337Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:15.337Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:15.339Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:15.340Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:20.338Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:12:20.339Z ioredis:redis status[172.31.0.2:7002]: wait -> connecting\n2018-10-26T19:12:20.340Z ioredis:redis queue command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:20.341Z ioredis:redis status[172.31.0.2:7002]: connecting -> connect\n2018-10-26T19:12:20.342Z ioredis:redis write command[172.31.0.2:7002]: 0 -> info([])\n2018-10-26T19:12:20.344Z ioredis:redis status[172.31.0.2:7002]: connect -> ready\n2018-10-26T19:12:20.344Z ioredis:connection send 1 commands in offline queue\n2018-10-26T19:12:20.345Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:20.346Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:20.346Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:20.346Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:20.347Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:20.348Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:12:25.340Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:12:25.340Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:25.342Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:25.342Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:25.342Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:25.343Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:25.343Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:30.340Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:30.341Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:30.342Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:30.342Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:30.342Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:30.343Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:30.343Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:35.305Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:12:35.305Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:35.307Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:35.307Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:35.308Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:35.308Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:35.309Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:40.305Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:40.306Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:40.307Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:40.307Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:40.308Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:40.308Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:40.309Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:45.311Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:45.312Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:45.313Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:45.314Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:45.315Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:45.315Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:45.316Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:12:50.316Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:12:50.317Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:50.320Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:50.320Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:50.321Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:50.322Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:50.322Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:12:55.321Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:12:55.322Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:12:55.324Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:12:55.325Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:12:55.325Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:12:55.326Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:12:55.327Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:00.325Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:13:00.327Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:00.328Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:00.328Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:00.329Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:00.330Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:00.330Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:05.296Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:13:05.296Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:05.297Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:05.297Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:05.298Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:05.298Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:05.299Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:10.302Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:13:10.303Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:10.305Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:10.306Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:10.306Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:10.307Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:10.308Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:13:15.309Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:13:15.309Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:15.313Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:15.313Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:15.314Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:15.314Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:15.315Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:13:20.310Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:13:20.310Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:20.312Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:20.312Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:20.312Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:20.313Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:20.313Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:25.316Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:13:25.317Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:25.319Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:25.319Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:25.319Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:25.320Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:25.321Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:30.319Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:13:30.319Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:30.320Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:30.321Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:30.321Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:30.321Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:30.322Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:13:35.290Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:13:35.291Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:35.293Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:35.293Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:35.293Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:35.294Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:35.294Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:40.295Z ioredis:cluster getting slot cache from 172.31.0.2:7000\n2018-10-26T19:13:40.296Z ioredis:redis write command[172.31.0.2:7000]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:40.298Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:40.298Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:40.298Z ioredis:cluster cluster slots result [1]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:40.299Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:40.300Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:45.302Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:13:45.303Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:45.304Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:45.304Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:45.305Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:45.306Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:45.306Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]\n2018-10-26T19:13:50.303Z ioredis:cluster getting slot cache from 172.31.0.2:7002\n2018-10-26T19:13:50.303Z ioredis:redis write command[172.31.0.2:7002]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:50.306Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:50.306Z ioredis:cluster cluster slots result [0]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:50.307Z ioredis:cluster cluster slots result [1]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:50.307Z ioredis:cluster cluster slots result [2]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:50.308Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false },\n  { host: '172.31.0.2', port: 7002, readOnly: false } ]\n2018-10-26T19:13:55.303Z ioredis:cluster getting slot cache from 172.31.0.2:7001\n2018-10-26T19:13:55.303Z ioredis:redis write command[172.31.0.2:7001]: 0 -> cluster([ 'slots' ])\n2018-10-26T19:13:55.304Z ioredis:cluster cluster slots result count: 3\n2018-10-26T19:13:55.304Z ioredis:cluster cluster slots result [0]: slots 10923~16383 served by 172.31.0.2:7002\n2018-10-26T19:13:55.305Z ioredis:cluster cluster slots result [1]: slots 5461~10922 served by 172.31.0.2:7001\n2018-10-26T19:13:55.305Z ioredis:cluster cluster slots result [2]: slots 0~5460 served by 172.31.0.2:7000\n2018-10-26T19:13:55.306Z ioredis:cluster:connectionPool Reset with [ { host: '172.31.0.2', port: 7002, readOnly: false },\n  { host: '172.31.0.2', port: 7001, readOnly: false },\n  { host: '172.31.0.2', port: 7000, readOnly: false } ]. I think this is actually related to the cluster being unavailable initially (ie in the bootstrap state) and only then moving on to bootstrapped state and being available, because a simple restart on the service allows it to connect with no problem. https://github.com/luin/ioredis/pull/741 - seems like the resolve() handler is just lost if this.disconnect(true) is issued and even though cluster goes to the ready state eventually, the connect handler never resolves. @luin what do you think about this fix? the flow that this PR tries to fix is following:\n\ncluster connects with hostnames:port\nstarts ready check\nstarts connecting to ipaddress:port\ndisconnects hostname based connection\nready check returns error due to .disconnect()\ncluster.disconnect() is called forcing connection shut down across all nodes\n\nendlessly repeats this sequence, unless ready check has not started or already finished before .disconnect on the hostname:port\nAnother fix that might be a bit more reliable is to resolve hostname to ipaddress before connecting to redis cluster, most likely forming a correct node key right away. But it would still be a problem in case that ipaddress is different from what the redis cluster returns. I think in case of only one node that node should be removed from the pool completely after the failure? Or is it not how it works - I admit I don't follow the complete logic for maintaining pool of connections yet so maybe there is something that I'm missing. BTW, in the tests 1 case seem to be particularly flaky, any idea why is that?\n```\n  1) cluster:pipeline\n       should retry when redis is down:\n      Uncaught AssertionError: expected [ null, 'OK' ] to deeply equal [ null, 'bar' ]\n      + expected - actual\n       [\n         [null]\n      -  \"OK\"\n      +  \"bar\"\n       ]\n  at /home/travis/build/luin/ioredis/test/functional/cluster/pipeline.js:207:28\n  at tryCatcher (node_modules/standard-as-callback/lib/utils.js:10:19)\n  at /home/travis/build/luin/ioredis/node_modules/standard-as-callback/index.js:20:31\n  at <anonymous>\n  at process._tickCallback (internal/process/next_tick.js:189:7)\n\n```. One other possible solution I see is resolving hostname -> ip address before connecting, since we know that redis cluster would be returning ip addresses either way, what do you think?\n@luin that way there should not be a readyCheck triggered on a node that we already know is going to be disconnected from the list, however, if it does get disconnected for some other reason - it could still be the case of endless retries with that race condition. \nAnother stab at this could possibly be delaying readyCheck until all of the nodes in the pool have \"connected\" state. @luin reworked proposed fix a bit, I think it should handle the problem a bit better\n\nchore: all literals that I've found were replaced with string constants, but probably a few more should be replaced, where there are .on/once listeners\nonce stream is up (sentinel / redis) the node key is updated if it is different, thus removing the need for the node to be disconnected\n. alright, this seem to be working rather well now, except that I still can see some issues in somewhat random places - one of the tests for monitor & max retries would bug out at times with a timeout error. However, this is also the case with non-modified code\n\nNoteworthy - updated TS to 3.x, added sourcemaps, nothing seems to be breaking, sinon to 6 (5 major versions jump), istanbul of 0.4.x to nyc with most recent versions of it. Locally tests seem to be passing well, waiting for CI. not the best piece of my code, but I think the last solution thats been implemented should be the best. Basically it waits before all nodes are in some sort of final \"pre-cluster connected\" state and only then tries to do readyCheck\nI haven't implemented cancelling promises since its not in the spec, but it shouldn't leak since it must be always resolving with any status - either connect or failure. did some more testing and that specific solution still doesn't eliminate the problem completely, guess need to make some sort of combination between them or automatic redirect of node-agnostic commands on the \"manual\" disconnect, such as the cluster status one. it will work for some cases, but I dont think its fail proof. In my opinion it would be great to somehow rework the ready check, specifically to make sure it doesn't trigger full reconnect when one node is manually disconnected due to configuration mismatch\n. > As for the rework of ready check, in which case this pull request doesn't eliminate the problem?\nI was thinking that when some node is faulty and wont let us connect for some reason its possible for command to be dispatched to it and then go in circles. They don\u2019t have support for multi-branch yet, but you can do 2 branch release with publishing from a specific branch\nIe master = no release, just tests and can manually trigger a release via semantic-release + tags (from your laptop, for instance) - thats for beta releases\nAnd once master is pushed to, lets say \u201crelease\u201d, branch - semantic will do all the automatic work\nTook a stab at it while trying out fixes for the cluster connect race conditions here: https://github.com/makeomatic/ioredis/blob/canary/.releaserc.json. ok, I'll work on that. @luin also a note - it requires GH_TOKEN and NPM_TOKEN to be set as env variables to be able to deploy to npm and github\nI'd advise to make these env variables only available on the master/release branches so that people wont be able to steal them with malicious PR. https://github.com/semantic-release/condition-travis - there is more info here on how to set things up and I do think it has the travis specific setup with encryption and stuff. https://github.com/semantic-release/semantic-release/blob/caribou/.travis.yml example of how they set it up on their own repo\nhttps://docs.travis-ci.com/user/pull-requests/#pull-requests-and-security-restrictions - this also states that only people who send PRs with write access to the repo are \"trustworthy\" and the secrets are only exposed to PRs like that. Checked the deploy - a few things - it did seem to only detect v4.0.0 as the last release, which is not correct. Can we specifically tell it to push the tag from git?\nAdd\n\"getLastRelease\": \"@semantic-release/git\" to the releaserc for that. need to make sure that .npmrc is present and I've configured it correctly (ie maybe dist-tag=next) is a wrong way to set it :) but yeah, pretty cool \ud83d\udc4d . got another idea regarding tokens, specifically not to leak them in case of problems with deps.\nsuggestions in order:\n\nadd overwrite to install script (https://docs.travis-ci.com/user/languages/javascript-with-nodejs/)\n\ninstall: NPM_TOKEN=\"\" GH_TOKEN=\"\" npm ci - ideally, but that probably wont work on older node version due to old npm\ninstall: NPM_TOKEN=\"\" GH_TOKEN=\"\" npm install\n\nsame for script itself - unset the variables since they are not required in the test\n\nscript: NPM_TOKEN=\"\" GH_TOKEN=\"\" /bin/sh -c \"npm run test:cov -- --retries 3\" - probably that would work just as good as retying the whole suite of tests\n. checked the logs and travis does dynamic install based on platform (ci / install)\nso maybe the following is fail proof:\ninstall: NPM_TOKEN=\"\" GH_TOKEN=\"\" /bin/sh -c 'npm ci || npm install'\n. > According to the documentation of Travis CI, pull requests that sent from a forked repo won't have access to the encrypted env vars. I submitted a test pull request and the NPM_TOKEN & GH_TOKEN are not logged: https://travis-ci.org/luin/ioredis/jobs/442282959#L591.\nYeah, I was thinking more in the line of malicious dependencies. Such as with the eslint not so long ago. Where a build might be triggered from a \"trusted\" person, but a package has been updated and tries to steal things. > That makes sense. We could just write the encrypted tokens as env vars to the deploy stage in .travis-ci.yml instead of configuring them in the dashboard of Travis CI that is visible by all the stages.\nagree, though I didn't find how to do that in travis. @luin it will close issues like that automatically on a successful release, btw. ok, I saw there is a git tag + draft release - was this intentional for testing or do we need to fix that?. I'm not sure, but I thin there was a way to allow exceptions to the rule, if not I'll just be extra careful not to commit directly :). released under next tag :)\n\n. Pushed a possible solution to the problem\nProblem that I'm trying to solve:\nresolve is lost when we issue .disconnect(true)\nWorkaround\nstart tracking all .connect() promises via .connectionPromise private variable, which exposes resolve and reject for the next connect promise and self-erases upon resolution\nThings to think about\n\ngarbage collection - I do seem to be cleaning up everything, but it still introduces tons of cross-references\ninstead listen to events (?), ie ready, close inside the ready handler alongside .disconnect(true)\nsimply reject() .connect() promise - that requires a breaking change to current behaviour, as one would have to implement retrying outside of ioredis\nreject & implement retrying (via bluebird-retry library for instance)\n. @luin any thoughts on that?. @shaharmor issue described here - https://github.com/luin/ioredis/issues/709. @luin what do you think of the approach, should we just redo this into event-based handling instead of chaining promises?. > Currently, the problem seems to be that we remove the close handler too early so the \"reject()\" won't be called if a disconnection happens between the \"refresh\" event and \"ready\" event. To solve that, we may move the removeListener('close') to the ready event callback, so the Cluster#connect() will be rejected in your test case (although the cluster wil be ready eventually). What do you think?\n\nI think rejecting makes sense, and then users can handle what to do - as long as promise is not stuck forever we are already way ahead of this\nAt the same time I think it would be great to add a convenience method, which would do auto-reconnect sort of thing with listening to ready/close/error events and possibly some sort of auto-retry strategy with a finite end. this is costly to access options + keyPrefix each time, imo it shoud be cached in a var. Also options should be always an object?\n. check in a cycle is not the best thing, imo we should create a function which is either a noop (gives back the input) or the transformed input. that way check is only performed once. this an be applied to all the other if (transform)\n. should be:\nvar keyPrefix = options.keyPrefix;\nif (keyPrefix) {\n  this._iterateKeys(function (key) {\n    return keyPrefix + key;\n  });\n}\nOtherwise each iteration it will look for a pointer from options to .keyPrefix and we want to avoid it\n. First idea seems cleaner, second would require a bit more code all the time. However we would want to make a single instance of a command distributor like this and I cant quite come up with the design just yet - do you have any particular implementation of this in mind?\n. for TLS stream interface would look like stream.socket.setNoDelay(), I suppose we should add extra checks here. did that because my eslint was going nuts :) since we do support only node 6+ - that shouldnt be a problem at all, and I assume once codebase has been completely migrated to ts eslint would go completely. this sometimes would throw if node that we issue quit to is already in the disconnected state (often in the tests where redis instance is down before the quit is processed). this.emit.bind could be saved as a reference, ie boundEmit\nand then we can process.nextTick(this.boundEmit, status) - that should be a bit faster. Same error returned for 3 statuses, was this intentional?. ah, didn't notice the difference at the end. It will do a dry run if branch is not master (marked in releaserc). to check that the setup is not broken (ie in case we update some of packages to incompatible versions). yeah, check the logs :). can overflow (unlikely, but still)\nhttps://github.com/fastify/fastify/blob/master/lib/logger.js#L32-L38 is an example of quick capping of the int so it runs in circles. maybe wrap it as a promise here via util.promisify (I dont remember when it became available) or just as a\nts\ndnslookup(hostname: string): Promise<string> => {\n  return new Promise((resolve, reject) => {\n    lookup(hostname, (err, address) => {\n          if (err) {\n            debug('failed to resolve hostname %s to IP: %s', hostname, err.message)\n            reject(err)\n          } else {\n            debug('resolved hostname %s to IP %s', hostname, address)\n            resolve(address)\n          }\n        })\n  });\n},. and then the code would be cleaner in the actual resolve function. ",
    "klinquist": "I would love that option!  Thank you for the reply.\n. You rock! Thank you!\n. @mertonium \nI would recommend keeping the connection open and sharing it across all of your routes.  Simply add a \"redisConnector.js\"  that exports the connection  and include it in all of your routes.\n. a-ha - I am running 1.5.3.  I will upgrade and re-open if it reoccurs.  Thank you for the quick response.\n. I have fixed this and created a pull request:\nhttps://github.com/luin/ioredis/pull/88\n. I agree with luin.\nI'm encrypting data that flows through redis and that is stored in our db.  This is how I do it:\n```\nvar crypto = require('crypto');\nvar algorithm = \"aes-256-ctr\";\nvar password = process.env.TOKEN_ENC_KEY\nexports.encrypt = function(text) {\n    if (!password){\n        console.log(\"Warning, text is not being encrypted because there is no TOKEN_ENC_KEY env var\");\n        return text;\n    } else {\n        var cipher = crypto.createCipher(algorithm,password);\n        var crypted = cipher.update(text,'utf8','hex');\n        crypted += cipher.final('hex');\n        return crypted;\n    }\n};\nexports.decrypt = function(text) {\n    if (!password){\n        console.log(\"Warning, text is not being decrypted because there is no TOKEN_ENC_KEY env var\");\n        return text;\n    } else {\n        var decipher = crypto.createDecipher(algorithm,password);\n        var dec = decipher.update(text,'hex','utf8');\n        dec += decipher.final('utf8');\n        return dec;\n    }\n};\n```\n. Found the bug, it was on my side, sorry for the trouble!  I use both sets and hsets, and I was trying to del \"prefix\", \"suffix\"  instead of \"prefix-suffix\". showFriendlyErrorStack to the rescue :)\n. d'oh.  I thought it was autoResend that wasn't working in cluster mode - turns out it's auto-resubscribe.\n. Just tested it on my side - seems to be working great!\nThank you!\n. ioredis would have to detect the input type (string/object/etc) and stringify it on the way in/parse it on the way out, so it wouldn't be faster even if it were handled automatically.  Redis simply doesn't support JSON objects.\nI believe you need to re-think your data model - storing a string of 200k rows?    Each row should be its own key (or a hash key)\n. They are async calls in which a callback is optional.  The last parameter is always the optional callback.\nYou should specify a callback if you want to check for error/results!\n. When you psubscribe, the messages are returned with three patterns - channel, pattern, and message.  Try this:\nredis.psubscribe('status_*', function(error, count){})\nredis.on('message', function (channel, pattern, message) {\n  console.log('Received message \\'%s\\' from channel \\'%s\\'', message, channel);\n});\n. @saschaishikawa  Oh yes, one more thing... when you psubscribe, you need to watch for pmessage instead of message.\nredis.on('pmessage', function(channel, pattern, message) {});\n:)\n. Makes sense... if I wanted to submit a PR, should I first to it on redis-commands?\n. I didn't actually check the version we had deployed, but it was @latest as of a few days ago, so it might have been 4.0.0-1.\nWe saw this issue every few hours.\nUnfortunately I won't be much help troubleshooting as this is a system that I can't have down :(.\n. The microservices open up several redis connections (clones of each other), but the one that does BRPOP is dedicated to BRPOP only.\nI did not notice any reconnections.  I was able to BRPOP myself manually from the redis server no problem.. Hm.. I'd have to see if that one server is failing master>slave, I haven't looked.  We aren't calling process.exit().\nUsing the official cluster, not sentinel. . qps can be ... \"tens\". I have some additional information:\nWith 4.0.0, 'psubscribe' messages were also being \"missed.\"\nOur redis instances do bgsaves often and it can block i/o.. the servers are using ~9gb ram.  Is it possible that ioredis 4.0 is emitting an error when a bgsave is happening whereas 3.2.2 isn't?. ",
    "mertonium": "Would you need to close the connections in the case where one has, let's say, an express app and different routes open redis connections? Since the app doesn't exit very often.\n. Thanks for the response @klinquist ! In my case I thought I'd done that. Then, after my question here, I realized we had an errant .quit() in the code that was closing the connection.  Got rid of it an all was well :)\n. ",
    "thelinuxlich": "processing_batch:pageviews\n  ioredis:redis write command[0] -> multi() +260ms\n  ioredis:redis write command[0] -> lrange(processing_batch:pageviews,0,99) +1ms\n  ioredis:redis write command[0] -> ltrim(processing_batch:pageviews,100,-1) +0ms\n  ioredis:redis write command[0] -> exec() +0ms\ngot_reply\nprocessed\nprocessing_batch:devices\n  ioredis:redis write command[0] -> multi() +5ms\n  ioredis:redis write command[0] -> lrange(processing_batch:devices,0,99) +0ms\n  ioredis:redis write command[0] -> ltrim(processing_batch:devices,100,-1) +0ms\n  ioredis:redis write command[0] -> exec() +1ms\nthe processing_batch message prints before running the transaction and the got_reply right after the promise resolves with the result from .exec(), processing_batch:devices is empty but is not returning\n. As I'm writing it is still stuck and there are other empty key lists to process\n. the servers are good(google cloud n1-highmem2 instances)\n. Yes it is 1.2.0\nYou know what solved the problem? Changing from promise to callback, crazy huh?\n. https://gist.github.com/thelinuxlich/c9f6c3ded8a312a78924\n. I was using bluebird and executing something like\nvar results = yield redis.multi().lrange(redis_key, 0, 99).ltrim(redis_key, 100, -1).exec()\n. no the function is a bb.coroutine\n. var run_inserts = bb.coroutine(function * () {\n    if (!running_inserts) {\n            var processing_fn = function(key) {\n                return function(err, replies) {\n                    if (replies[0][1] && replies[0][1].length > 0) {\n                        var batch = replies[0][1];\n                        //do something with the batch\n                    }\n                };\n            };\n        running_inserts = true;\n        for (var i = 0; i < keys.length; i++) {\n            var redis_key = \"processing_batch:\" + keys[i];\n            redis.multi().lrange(redis_key, 0, 99).ltrim(redis_key, 100, -1).exec(processing_fn(keys[i]));\n        }\n        running_inserts = false;\n    }\n});\n. it's actually bluebird.coroutine: https://github.com/petkaantonov/bluebird/blob/master/API.md#promisecoroutinegeneratorfunction-generatorfunction---function\nvar run_inserts = bb.coroutine(function * () {\n    if (!running_inserts) {\n            var processing_fn = function(key) {\n                return function(replies) {\n                    if (replies[0][1] && replies[0][1].length > 0) {\n                        var batch = replies[0][1];\n                        //do something with the batch\n                    }\n                };\n            };\n        running_inserts = true;\n        for (var i = 0; i < keys.length; i++) {\n            var redis_key = \"processing_batch:\" + keys[i];\n            var result = yield redis.multi().lrange(redis_key, 0, 99).ltrim(redis_key, 100, -1).exec();\n            processing_fn(keys[i])(result);\n        }\n        running_inserts = false;\n    }\n});\n. don't know if it is related, but I was executing the run_inserts function in a setInterval(run_inserts, 300)\n. yeah, if I return the code to yield the promise it stops on the second iteration. Very strange, anyway, I'm gonna close this since it seems unpredictable\n. But what should I do to avoid these errors?\n. I thought there was a configuration to avoid UNBLOCKED, for example\n. I'm also getting a lot of these:\nReplyError: EXECABORT Transaction discarded because of previous errors.\n. And by the way, if the library has the autoResendUnfulfilledCommands option enabled by default, shouldn't it resend automatically after recovering from CLUSTERDOWN?\n. Now I have 3 masters and 9 slaves, let's keep this open for some days so I can see if those errors persist. I've also configured the cluster with \"cluster-slave-validity-factor 0\" and \"cluster-migration-barrier 1\"\n. good reference: http://redis.io/presentation/Redis_Cluster.pdf\n. even after increasing slaves and changing config to be more available, I'm receiving EXECABORT ocasionally with this previousErrors:\npreviousErrors:\n [ { [ReplyError: MOVED 1684 192.168.0.1:7000]\n name: 'ReplyError',\n message: 'MOVED 1684 192.168.0.1:7000',\ncommand: [Object] },\n{ [ReplyError: MOVED 1684 192.168.0.1:7000]\nname: 'ReplyError',\n message: 'MOVED 1684 192.168.0.1:7000',\n command: [Object] } ] }\n. You mean, using multi(), right?\n. My code has one transaction:\nredis.multi().setnx(new_key, possible_new_session_id).expire(new_key, 1800).exec()\n. no, probably the multi() is not going to the right node\n. By the way, I didn't have this problem with a similar library: https://github.com/thunks/thunk-redis\nTheir API forces you to set the key explicitly on the multi() and exec() methods\n. ok, maybe refreshAfterFails = 1 can solve the issue?\n. Dont know if it is luck, but since setting refreshAfterFails to 1 no error happened\n. let's test it ;)\n. still getting MOVED errors :(\n. Great, every new release I'm always testing :)\n. No errors so far...\n. Seems fixed!\n. Bee-queue on ioredis would be awesome https://github.com/LewisJEllis/bee-queue\n. I wrote a PR but there are some small things I'm unable to work around:\nhttps://github.com/LewisJEllis/bee-queue/pull/21\n. coming back to this issue, @AVVS are you sure all the keys explicitly declared on EVAL command need to be on the same machine? According to EVAL docs:\n\"All Redis commands must be analyzed before execution to determine which keys the command will operate on. In order for this to be true for EVAL, keys must be passed explicitly. This is useful in many ways, but especially to make sure Redis Cluster can forward your request to the appropriate cluster node.\"\n. redis.call(\"get\",KEYS[1]) you mean?\n. but if it is on the KEYS array it should work, right?\n. This seems impracticable, how should I know that in advance?\n. Understood\n. I'm seeing this error too with ioredis 2.4 and redis 3.2 :((((\n. Hmm I solved this by adding \"bind IP\" to redis.conf which seems to be required on 3.2+\n. Ok, I will keep you updated\n. So today it happened again and this is the log with DEBUG:\nhttps://gist.github.com/thelinuxlich/365b8cafc98295b14077a38a44a949a7\n. I'm configuring it to connect with all the 6 nodes, and when this happens, I have to restart the whole process :(\n. Some more info: I'm using Node 5.5 and the redis operations consist of GET, SET, SETEX and EVALSHA scripts(using defineCommand)\n. Is there anymore info I can provide you to help pinpoint the issue?\n. I'm testing with older ioredis versions, let's see\n. Interesting, ioredis 2.0.0 is doing okay\n. Now I can confirm that switching to ioredis 2.0.0 totally mitigates the problem\n. After days of testing, can confirm this happens in ioredis 2.2+\n. Well, it can't be misconfiguration because  I've been using ioredis for more than a year without changes in configuration and all of a sudden when I update from 2.0 to 2.4 this began to happen.\n. ",
    "jjmr": "My becnhmark updated:\n```\n $ npm run bench\n\nioredis@1.2.0 bench /tmp/ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nioredis: 1.2.0\nnode_redis: 0.12.1\nCPU: 8\nOS: linux x64\n==========================\n                  simple set\n     102,374 op/s \u00bb ioredis\n     100,401 op/s \u00bb node_redis\n\n                  simple get\n      69,507 op/s \u00bb ioredis\n      90,876 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      10,670 op/s \u00bb ioredis\n      11,413 op/s \u00bb node_redis\n\n                  lrange 100\n      57,448 op/s \u00bb ioredis\n      67,558 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 54,293.49 ms\n```\nMy OS is Linux Mint.\nIf I run the same benchmark more than once I can see different values and in general ioredis and node_redis have similar performance in each suite (more or less)\n. redis server is in the same machine and the version is 2.8.4\n. ",
    "IncSW": "redis-cli 2.8.17\ndebian jessie\n```\n\nioredis@1.5.4 bench /home/tests/ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nioredis: 1.5.4\nnode_redis: 0.12.1\nCPU: 8\nOS: linux x64\n==========================\n                  simple set\n     133,532 op/s \u00bb ioredis\n     156,371 op/s \u00bb node_redis\n\n                  simple get\n     133,728 op/s \u00bb ioredis\n     135,891 op/s \u00bb node_redis\n\n                  simple get with pipeline\n      17,511 op/s \u00bb ioredis\n      15,210 op/s \u00bb node_redis\n\n                  lrange 100\n      90,610 op/s \u00bb ioredis\n     106,528 op/s \u00bb node_redis\n\nSuites:  4\n  Benches: 8\n  Elapsed: 53,750.60 ms\n```\n. ",
    "fantasticsoul": "\u4f60\u597d\uff0c\u6211\u672c\u5730\u673a\u5668\u662fwin7\u7cfb\u7edf\uff0c\u4f7f\u7528npm install ioredis\u547d\u4ee4\u4e0b\u8f7dioredis\u5230\u4e86\u672c\u5730\uff0c\u6211\u672c\u5730\u6709redis-server,\u7248\u672c\u4e3a2.6.12,\u8bf7\u95ee\u600e\u6837\u8dd1\u8fd9\u4e2aioredis/benchmarks/single_node.js\u6587\u4ef6\uff1f\u76f4\u63a5\u5728ioredis/benchmarks/\u76ee\u5f55\u4e0b\u8f93\u5165 node single_node.js\u4f1a\u62a5\u9519\uff1a\nReferenceError: suite is not defined\n    at Object. (C:\\Users\\Administrator\\node_modules\\ioredis\\benchmark\ns\\single_node.js:30:1)\n    at Module._compile (module.js:460:26)\n    at Object.Module._extensions..js (module.js:478:10)\n    at Module.load (module.js:355:32)\n    at Function.Module._load (module.js:310:12)\n    at Function.Module.runMain (module.js:501:10)\n    at startup (node.js:129:16)\n    at node.js:814:3\n(>_<)~~~~\n. \u8f93\u5165npm run bench\u540e\u56de\u8f66\uff0c\u4f9d\u7136\u4e0d\u6210\u529f\uff0c\u63a7\u5236\u6253\u5370\u5982\u4e0b\uff1a\n\nioredis@1.7.5 bench C:\\Users\\Administrator\\node_modules\\ioredis\nmatcha benchmarks/*.js\n\n'matcha' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\nnpm ERR! Windows_NT 6.1.7601\nnpm ERR! argv \"C:\\Program Files\\nodejs\\node.exe\" \"C:\\Program Files\\nodejs\n\\node_modules\\npm\\bin\\npm-cli.js\" \"run\" \"bench\"\nnpm ERR! node v0.12.0\nnpm ERR! npm  v2.5.1\nnpm ERR! code ELIFECYCLE\nnpm ERR! ioredis@1.7.5 bench: matcha benchmarks/*.js\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the ioredis@1.7.5 bench script 'matcha benchmarks/.js'.\nnpm ERR! This is most likely a problem with the ioredis package,\nnpm ERR! not with npm itself.\nnpm ERR! Tell the author that this fails on your system:\nnpm ERR!     matcha benchmarks/.js\nnpm ERR! You can get their info via:\nnpm ERR!     npm owner ls ioredis\nnpm ERR! There is likely additional logging output above.\nnpm ERR! Please include the following file with any support request:\nnpm ERR!     C:\\Users\\Administrator\\node_modules\\ioredis\\benchmarks\\npm-debug.lo\ng\n\u7136\u540e\u6211npm i matcha\u540e\uff0c\u518d\u6b21\u8f93\u5165npm run bench\u540e\u56de\u8f66\uff0c\u8fd8\u662f\u4e0d\u6210\u529f\uff0c\u63a7\u5236\u6253\u5370\u5982\u4e0b\uff1a\n\nioredis@1.7.5 bench C:\\Users\\Administrator\\node_modules\\ioredis\nmatcha benchmarks/*.js\n\nchild_process: customFds option is deprecated, use stdio instead.\nmodule.js:338\n    throw err;\n          ^\nError: Cannot find module 'C:\\Users\\Administrator\\node_modules\\ioredis\\benchmark\ns*.js'\n    at Function.Module._resolveFilename (module.js:336:15)\n    at Function.require.resolve (module.js:388:19)\n    at C:\\Users\\Administrator\\node_modules\\ioredis\\node_modules\\matcha\\bin_matc\nha:80:18\n    at Array.map (native)\n    at Object.runSuite (C:\\Users\\Administrator\\node_modules\\ioredis\\node_modules\n\\matcha\\bin_matcha:79:17)\n    at Program.parse (C:\\Users\\Administrator\\node_modules\\ioredis\\node_modules\\m\natcha\\node_modules\\electron\\lib\\electron\\program.js:262:27)\n    at Object. (C:\\Users\\Administrator\\node_modules\\ioredis\\node_modu\nles\\matcha\\bin_matcha:36:9)\n    at Module._compile (module.js:460:26)\n    at Object.Module._extensions..js (module.js:478:10)\n    at Module.load (module.js:355:32)\nnpm ERR! Windows_NT 6.1.7601\nnpm ERR! argv \"C:\\Program Files\\nodejs\\node.exe\" \"C:\\Program Files\\nodejs\n\\node_modules\\npm\\bin\\npm-cli.js\" \"run\" \"bench\"\nnpm ERR! node v0.12.0\nnpm ERR! npm  v2.5.1\nnpm ERR! code ELIFECYCLE\nnpm ERR! ioredis@1.7.5 bench: matcha benchmarks/*.js\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the ioredis@1.7.5 bench script 'matcha benchmarks/.js'.\nnpm ERR! This is most likely a problem with the ioredis package,\nnpm ERR! not with npm itself.\nnpm ERR! Tell the author that this fails on your system:\nnpm ERR!     matcha benchmarks/.js\nnpm ERR! You can get their info via:\nnpm ERR!     npm owner ls ioredis\nnpm ERR! There is likely additional logging output above.\nnpm ERR! Please include the following file with any support request:\nnpm ERR!     C:\\Users\\Administrator\\node_modules\\ioredis\\benchmarks\\npm-debug.lo\ng\n\u8fd9\u662f\u80bf\u4e48\u4e86\n. \n\u7136\u540e\u518d\u6b21\u6267\u884cnpm run bench\n\n\u8fd9\u662f\u4e3a\u4ec0\u4e48\u5462~~~~(>_<)~~~~\n. \n\u975e\u5e38\u611f\u8c22\uff0c\u5df2\u7ecfok\u4e86\uff0c\u63a5\u4e0b\u6765\u6211\u53bbcentOS\u8dd1\u8dd1\u770b\u770b\u7ed3\u679c\n. ",
    "zhuangya": "node 4.2.1:\nversion: pull-request #184 \n```\n12:09:59 nos:~/Projects/Github/ioredis 13 (redis-dev-dep)\n~> npm run bench\n\nioredis@1.10.0 bench /Users/nos/Projects/Github/ioredis\nmatcha benchmarks/*.js\n\n(node) child_process: options.customFds option is deprecated. Use options.stdio instead.\nioredis: 1.10.0\nnode_redis: 2.2.5\nCPU: 8\nOS: darwin x64\n==========================\n                  simple set\n     111,917 op/s \u00bb ioredis\n      87,705 op/s \u00bb node_redis\n\n                  simple get\n     107,242 op/s \u00bb ioredis\n      86,911 op/s \u00bb node_redis\n\n                  simple get with pipeline\n       9,105 op/s \u00bb ioredis\n       8,371 op/s \u00bb node_redis\n\n                  lrange 100\n      10,572 op/s \u00bb ioredis\n       8,917 op/s \u00bb node_redis\n\n                  publish\n     108,210 op/s \u00bb ioredis\n      87,479 op/s \u00bb node_redis\n\n                  subscribe\n      48,086 op/s \u00bb ioredis\n      46,994 op/s \u00bb node_redis\n\nSuites:  6\n  Benches: 12\n  Elapsed: 89,436.12 ms\n```\n. \u53ef\u4ee5\u8d34\u4e00\u4e0b\u51fa\u95ee\u9898\u7684\u4ee3\u7801\u561b\n. > I noticed people got different results in their computers/servers. However, IMO the difference is trivial since both two libraries are very fast, and neither node_redis nor ioredis will become the bottleneck in most applications.\nmake sense\n. ",
    "shershams": "Here are results with Mid 2014 MBPro 2.5 GHz i7, 16GB RAM, NVIDIA GeForce GT 750M 2GB. Running against Redis 3.0.0 on the same machine. Any special configs needed for Redis to optimize it for ioredis?\n\n> matcha benchmarks/*.js\n(node) child_process: options.customFds option is deprecated. Use options.stdio instead.\n==========================\nioredis: 1.14.0\nnode_redis: 0.12.1\nCPU: 8\nOS: darwin x64\nnode version: v5.3.0\ncurrent commit: 581524d\n==========================\n                      simple set\n          34,464 op/s \u00bb ioredis\n          49,481 op/s \u00bb node_redis\n                      simple get\n          33,857 op/s \u00bb ioredis\n          40,399 op/s \u00bb node_redis\n                      simple set\n          36,601 op/s \u00bb ioredis\n          52,078 op/s \u00bb node_redis\n                      simple get\n          41,298 op/s \u00bb ioredis\n          51,527 op/s \u00bb node_redis\n                      simple get with pipeline\n           2,769 op/s \u00bb ioredis\n           9,503 op/s \u00bb node_redis\n                      lrange 100\n          34,479 op/s \u00bb ioredis\n          65,115 op/s \u00bb node_redis\n                      publish\n          40,704 op/s \u00bb ioredis\n          49,343 op/s \u00bb node_redis\n                      subscribe\n          30,397 op/s \u00bb ioredis\n          40,692 op/s \u00bb node_redis\n  Suites:  6\n  Benches: 12\n  Elapsed: 96,849.92 ms\n\n. ",
    "curtiszimmerman": "I'm running Redis via Docker (something very close to docker run -d --name=redis -p 127.0.0.1:6379:6379 redis which automagically pulls the stock Redis image and then starts it listening locally). I'm posting because my results are much more in favor of ioredis than node_redis.\nRedis server v=3.0.7\n```\nioredis: 1.15.0\nnode_redis: 2.4.2\nCPU: 8\nOS: linux x64\nnode version: v4.3.0\ncurrent commit: 13ac1c4\n==========================\n                  simple set\n      99,635 op/s \u00bb ioredis\n      55,776 op/s \u00bb node_redis\n\n                  simple get\n      86,711 op/s \u00bb ioredis\n      53,505 op/s \u00bb node_redis\n\n                  simple get with pipeline\n       8,082 op/s \u00bb ioredis\n       7,081 op/s \u00bb node_redis\n\n                  lrange 100\n      14,140 op/s \u00bb ioredis\n       9,686 op/s \u00bb node_redis\n\n                  publish\n      92,252 op/s \u00bb ioredis\n      56,567 op/s \u00bb node_redis\n\n                  subscribe\n      41,925 op/s \u00bb ioredis\n      33,334 op/s \u00bb node_redis\n\nSuites:  6\n  Benches: 12\n  Elapsed: 92,940.75 ms\n```\n. ",
    "lwhorton": "Hah, oops. It was something simple... thanks.\n. ",
    "jfrux": "I seem to be having a lot of errors that look similar to this:\n/node_modules/ioredis/node_modules/bluebird/js/main/async.js:43\n        fn = function () { throw arg; };\nHaving to do with async.js:43\nIs this something similar to this issue?\n. I agree.\nAt least some sort of event that fires upon connection because intermittently I'm getting strange issues when server peaks and it takes it a bit longer to respond to a connection.\nIn basic tests, it is connected and ready instantly but in some cases even a few ms difference could mean whether or not it's instantiated or not.\n. Sorry about this, I did realize this was already done after I posted this.\nShould have updated my comment.\nThanks again for all you do!\n\nOn May 14, 2015, at 12:11 PM, Vitaly Aminev notifications@github.com wrote:\nhttps://github.com/luin/ioredis#connection-events https://github.com/luin/ioredis#connection-events\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/issues/30#issuecomment-102087968.\n. \n",
    "ghost": "https://github.com/thedeveloper/warlock seems to work on ioredis as well. Just pass it an ioredis client.\n. bump anyone had the time to implement this?. ",
    "ScheerMT": "Maybe including a list of working redis modules like this should be added to the documentation to gain exposure to these? \n. Wouldn't this kind of splitting already be handled if you have a redis cluster running? \nRelevant Redis documentation\nFor pooling, I was imagining a situation where no matter how many times you require('ioredis'), it would always use the same connection to talk to the redis server. This would create a problem for pub sub type applications though now that I think about it...\n. Ah, yeah. After having read over another library (seriate) which does leverage pooling in that context I should have instead thought of that.\nThanks for clearing that up :)\nWould pooling in the context of AVVS be something that could have a benefit for redis performance?\n. Thanks! :)\n. Yikes! I should have spent more time reading over the documentation. Thanks for making an awesome package :+1: \n. Two questions!\n1. Curious on how that was causing a leak...Was it because it was in an enclosing scope and the function inside of it was referencing it?\n2. There was talk about this project merging with another redis package at one point i believe... Did that fall through and you are proceeding with a v2 release seperate of that?\nNinja Edit: Noticed the merging comment at the bottom of the readme. Where does the work start with the noining of the libraries?\nAwesome work!\n. ",
    "behrad": "+1 for https://github.com/AVVS/redislock\n. ",
    "manast": "Bull uses many of ioredis features: https://github.com/OptimalBits/bull\n. @luin please, could you be so kind to share any insight you have in this issue?. thanks for the throughout explanation.  When calling defineCommand, why can't we just call SCRIPT LOAD and then after that just assume that the command is available?. Bump! :). Bump!. Bump because it is an important issue!. bump. ok, I try to test again with a reproducible environment.. bump to avoid auto close. @carly not yet. I need to provide better test code for @luin but I did not have enough time for it, I will try to prioritize it.. ",
    "zj8487": "thanks a lot\n. ",
    "faller": "https://github.com/Automattic/socket.io-redis\nvar redis = require('redis').createClient;\nvar adapter = require('socket.io-redis');\nvar pub = redis(port, host, { auth_pass: \"pwd\" });\nvar sub = redis(port, host, { detect_buffers: true, auth_pass: \"pwd\" });\nio.adapter(adapter({ pubClient: pub, subClient: sub }));\ndetect_buffers: default to false. If set to true, then replies will be sent to callbacks as node Buffer objects if any of the input arguments to the original command were Buffer objects. This option lets you switch between Buffers and Strings on a per-command basis, whereas return_buffers applies to every command on a client.\n. thx\n. ",
    "johnlogsdon": "Thanks for a really quick detailed response however can you confirm that ioredis supports the specific case I showed above as I dont seem to be able to get that to work i.e. instead of multi.mget(),incr(),incr() you can do multi([\n        [\"mget\", \"multifoo\", \"multibar\", redis.print],\n        [\"incr\", \"multifoo\"],\n        [\"incr\", \"multibar\"]\n    ])\n. You may have set a record for quickest project update! Thanks very much for doing this as I'm up and running now, all good!\n. ",
    "jedi4ever": "Alternative could be:\n- new Redis({ srv: 'redis.consul' }) : specific srv specification\n- new Redis({ host: 'redis.consul'\u00a0, srv:true }) : toggle between regular host and port or read srv record\n. great ! which syntax do you prefer?\n. ",
    "nunofgs": "@AVVS: any news on this?\n. ",
    "MrPink": "@jedi4ever did you ever get anywhere with this? . ",
    "tangxinfa": "This feature is what i am looking for.\n. I already implementing it, see https://github.com/tangxinfa/ioredis_sentinel_connector\nioredis_sentinel_connector\nBest efforts on connect slave with ioredis's SentinelConnector.\nWARNING: This module will change the prototype of ioredis's SentinelConnector.\nThere is no difference when role is \"master\", but for \"slave\" following the order:\n\n\nLocal slave.\n\n\nMaster if no slaves.\n\n\nLocal master.\n\n\nRemote slave.. @luin \nGood job. I was migrated one project to use ioredis today for access redis sentinel.\n\n\n@BridgeAR \nThanks for share this information, we depends on node_redis to call toString on values, i will check our code before use new version of node_redis in other projects.\n. Becarefull, according to http://redis.io/topics/sentinel-clients#sentinel-failover-disconnection, Sentinel failover disconnection require at least Redis 2.8.12(ioredis already point out in the documention), if you use low redis versions(like me 2.8.7), application will broken after redis failover.\nI already use \"reconnectOnError\" options to force reconnect if READONLY, and ask op to upgrade redis.. ",
    "doublesharp": "This is exactly the setup I need as well - any idea when it will be available?\n. I think there is a misunderstanding - the goal isn't to pick a Slave for failover but rather for reads. Assume there are 6 servers, all running Redis and Sentinel. \n- 1 Redis Master\n- 2 Redis Replicas (a slave, but available for failover)\n- 3 Redis Slaves (a slave, but not available for failover)\nIn the above setup with the 3 \"Slaves\" running on the application server reads can be made locally to the Slave instance while writes are made to the remote Master. It's assumed that the reads will not always be in sync with the writes based on the asynchronous Redis replication, but in cases where this is OK you get much better performance.\nI actually updated the ioredis code/tests to handle this already, and was planning on submitting a pull request. \nIt accepts either a function (as you suggested) with the available slaves provided as an argument or an array of objects with properties ip, port, and optionally prio (with a default of 1). I used ip rather than host to match the output from the SENTINEL slaves command. The first available \"preferred\" slave is returned, otherwise the default behavior of returning a random available slave is preserved. The prio isn't the actual slave priority, it is your preference priority - i want to connect to priority 1 because it is local, priority 2 because it is in my network segment, or worst case something else because 1 and 2 are not available. This would not affect failover or Sentinel functionality.\nLet me know if you see any issues with the code below in sentinel_connector.js lines 150-204, with var _this = this; at the top of the function on line 136;\n``` javascript\n// allow the options to prefer particular slave(s)\nif (_this.options.preferredSlaves) {\n  var preferredSlaves = _this.options.preferredSlaves;\n  switch (typeof preferredSlaves) {\n  case 'function':\n    // use function from options to filter preferred slave\n    selectedSlave = _this.options.preferredSlaves(availableSlaves);\n    break;\n  case 'object':\n    if (!Array.isArray(preferredSlaves)) {\n      preferredSlaves = [preferredSlaves];\n    } else {\n      // sort by priority\n      preferredSlaves.sort(function (a, b) {\n        // default the priority to 1\n        if (!a.prio) {\n          a.prio = 1;\n        }\n        if (!b.prio) {\n          b.prio = 1;\n        }\n    // lowest priority first\n    if (a.prio < b.prio) {\n      return -1;\n    }\n    if (a.prio > b.prio) {\n      return 1;\n    }\n    return 0;\n  });\n}\n// loop over preferred slaves and return the first match\nfor (var p = 0; p < preferredSlaves.length; p++) {\n  for (var a = 0; a < availableSlaves.length; a++) {\n    if (availableSlaves[a].ip === preferredSlaves[p].ip) {\n      if (availableSlaves[a].port === preferredSlaves[p].port) {\n        selectedSlave = availableSlaves[a];\n        break;\n      }\n    }\n  }\n  if (selectedSlave) {\n    break;\n  }\n}\n// if none of the preferred slaves are available, a random available slave is returned\nbreak;\n\n}\n}\nif (!selectedSlave) {\n  // get a random available slave\n  selectedSlave = _.sample(availableSlaves);\n}\n``\n. Check out the commit in my fork here: https://github.com/doublesharp/ioredis/commit/8dfbe3287208e573056c61bbac191f8d9a855b33\n. @tangxinfa @jedi4ever this is now included in 2.4.0 :)\n. @luin What do you think aboutscaleReadsfor Sentinel groups? It could possibly use a connection pool or just two connections - master for writes and a slave (by preference/priority) for reads?\n. You can usegeneric-pool` to do it if you want. I actually implemented connection pooling with a min/max of 1 to help handle lock contentions for an app I built.\n```es6\nconst Redis = require('ioredis');\nconst Pool = require('generic-pool');\nconst pool = Pool.createPool({\n  create: function() {\n    // create a new client\n    return Promise.resolve(new Redis(...));\n  },\n  destroy: function(client) {\n    // do something when destroyed?\n    return Promise.resolve(client);\n  },\n},\n{\n  min: 1,\n  max: 1,\n  autostart: true,\n});\n// acquire a client\npool.acquire()\n.then((client) => {\n  // do something with the the client\n});\n``. You could use a similar method to the above with logic in thecreate` to point each pool resource to a different read-only slave for example. You can only write to a single threaded instance so there isn't much benefit to pooling except in some unique cases.. ",
    "RobinQu": "Maybe you should take another look at what I suggested.\n. ",
    "miguelmonrealgonzalez": "Can you post an example how to make a unit test using mock_server??\nFor example, how can i test a function which save in redis a hmset. \nThe function could receive two parameters, the client redis object and an object to save as hmset\nHow I  test (Unit Test) that?\nThanks.\n. ",
    "blimmer": "The problem I ran into with the mock_server is has to do with this line. It returns \"OK\" when trying to get a key that doesn't exist, which prevents me from using it in testing. For example:\njavascript\nioredis.del('foo').then(function() {\n  ioredis.get('foo').then(function(res) {\n    // res === 'OK';\n  });\n});\n. Ah, OK. I see now. Thank you for that clarification. It would be really nice if there were an equivalent of fakeredis that worked with ioredis. Or, is there a way to make that work that I'm not thinking of?\n. Awesome @stipsan !!\n. ",
    "stipsan": "For many cases I agree running a real redis server is the wisest choice.\nAt the same time I think there are cases where the requirements of your project have different tradeoffs making an emulator a better choice.\nCases like:\n- Your workflow already use a local redis-server instance for the dev server.\n- You're on a platform without an official redis release, that's even worse than using an emulator.\n- You're running tests on a CI, setting it up is complicated. If you combine it with CI that also run selenium acceptance testing it's even more complicated, as two redis-server instances on the same CI build is hard.\n- The GitHub repo have bots that run the testing suite and is limited through npm package.json install scripts and can't fire up servers. (Having Greenkeeper notifying me when a new release of ioredis is out and wether my code breaks or not is awesome).\nI've started writing a mock-ioredis utility for one of my projects. So far I've covered just the methods that I'm using in a personal project:\n- incr\n  - hsetnx\n  - hmset\n  - sadd\n  - srem\n  - hget\n  - hvals\n  - hgetall\n  - smembers\n  - sismember\n  - hset\n  - multi\n  - exec\nI'm going to put it up on github and npm so that those of us who need it can collab on it, and the readme will make it clear that it's only meant to solve use cases where testing against a real redis-server is impractical.\n. Hey guys, I started working on it here: https://github.com/stipsan/ioredis-mock\n. Thanks guys!\nI've added a compat.md that shows the status of implemented commands.\nShould we add a reference to it here: https://github.com/luin/ioredis#running-tests so that people can find it?\n. Sure thing!\n. ",
    "assaf": "Thanks\n. 'unref' is not protocol aware, it can't infer intent. It's up to the application developer to use it correctly.\nIf you're running an HTTP server, you want to keep it alive while the socket is open, so you're not going to unref. If you have a socket for logging, you don't want that socket to keep the process from exiting, so you're going to unref it.\nRedis, like raw sockets, supports many different use cases. If you're using it for storage/cache, primarily read/write operations, then you probably want to unref. If you're using Redis for queue processing, or listening to pub/sub messages, you don't want to unref.\nSo this could be a connection open or API call on the connection, and let the application developer decide when to use this option based on use case. Or it could be handled by ioredis depending on which command is currently executing (e.g. unref for get and ref for brpop).\n\nThere's a different issue with get. From looking at the code, it looks like every command that executes will block until something happens on the socket. If the Redis server goes away (crash, network issue) nothing happens on the socket, and the command will hang forever.\nA server that shuts down will send a FIN packet which tells Node to close the connection. But if the server goes away there's no chance of that happening. TCP can handle that by detecting no activity on the socket \u2014 the server can be configured to always send something (keep-alive), and the client has a timeout \u2014 but that seems to be turned off.\nBut even when used, socket timeouts are not good enough. You want the timeout long enough so it doesn't disconnect during periods of inactivity, but you don't clients waiting forever, so you need a quick way to detect crashed servers, by having a shorter timeout on the command.\nCommonly this is done with setTimeout, so you have a timer running while executing a command and waiting for it to complete, which keeps the process running until the timeout is cleared (command completed, successfully or not). So that would fix the problem with get.\nbrpop is a bit more tricky, because you want it to wait, possibly forever, but only if the server is alive. So one option is to let TCP detect a crashed server using keep-alive and timeout, but I'm not sure if Redis supports that. The other option is to use timeouts, then do some noop to see if Redis still responding, and then repeat the command.\nThat mechanism will likely also use setTimeout, so if you have a way of dealing with crashed servers that uses timeouts, you can unref the underlying socket.\n. ",
    "walling": "I could definitely use this feature. unrefWhenDrain would also be fitting for my use-case.\n. I'm wondering if this issue should be re-opened, since I don't see the final solution implemented. This is something we could use too. Anyone care to explain or provide a code snippet for the idea behind flushing the offlineQueue in the retryStrategy?\nRight now I'm researching different strategies to harden our codebase against various failures, fx. Redis becomes unresponsible. Imho I like the timeoutPerRequest option the most, even though it might be difficult to support efficiently. If it's a global option (ie. all commands have the same timeout value) it should be possible to implement an efficient strategy.. ",
    "heri16": "@luin Could we reopen this? I need unrefWhenDrain for AWS Lambda usage.. any real workarounds?. Ioredis just does not work on AWS Lambda!. @mfulton26 that's not a good idea. That was not how we solved this. Setting that to false basically makes some redis writes pending until the next http request.. @ccs018 Yes, I am well aware of these limitations. ElastiCache ensures that both the DNS name and the IP address of the cache node remain the same when cache nodes are recovered in case of failure. \nSee here\nAdding shards while online would require a RegEx-based implementation that maps all matching auto-generated internal-DNS hostnames to the Natted ip address. The node hostnames on AWS ElastiCache can be predicted ahead of time, and this is likely the same case for other automated deployment tools also. [xxxx-cache-0001-001.xxxx-cache.xx.xxxx1.cache.amazonaws.com] With the right regexp pattern, this addresses point 1 so that we can include all possible nodes that may exist in the future, including nodes spanning across availability zones.\nTo address point 2, the cluster should be using static internal hostnames instead of static internal ip addresses. This is already the case for AWS ElastiCache. If the node is ever restarted, internal dns should be updated to reflect the new IP address, without having to update natRemap or restarting the client.\nWould we prefer a regex implementation? Alternatively, the configuration could be externalized to a JSON file with fs.watch() hot-reloading.. Just in case others are wondering why-NAT, this is to mitigate cold-start-times on AWS Lambda, in particular the slow performance when initializing new VPC-connection. This means an extra long time for ioredis to successfully connect to redis cluster because the VPC network interface requires some time to initialize and be ready.\nSee article. ",
    "jedwardsaviata": "It was simply hanging during the import. As soon as I added the try...catch then removed it, everything started working correctly, so I am unable to reproduce it now.\nHere is the code surrounding the import. There is only one module import being performed before the ioredis import:\n``` coffeescript\n!/usr/bin/env coffee\nconsole.log \"Starting script.\"\n_ = require 'lodash'\nconsole.log \"Lodash ready.\"\nRedis = require 'ioredis'\nconsole.log \"Redis module loaded.\"\ndurations = require 'durations'\n``\n. I'll admit that was what really surprised me. The fact that there was atry / catchalready in place. However, the hang actually occurs when the _hiredis_ module is being required, so the hang is within the body of thetry` portion before any error has been encountered. This may end up being something I need to raise on the hiredis-node repository instead of here if I can reproduce it.\nIn any case, here are some additional details just in case.\nHere is my modified (added try/catch and logging) version of ioredis/lib/parsers/hiredis.js\n```\n'use strict';\nconsole.log(\"hiredis> load dependencies...\");\nvar events = require('events');\nconsole.log(\"hiredis> loaded events\");\nvar util = require('util');\nconsole.log(\"hiredis> loaded util\");\nvar hiredis;\ntry {\n  console.log(\"hiredis> preparing to load the hiredis module...\");\n  hiredis = require('hiredis');\n  console.log(\"hiredis> loaded hiredis\");\n}\ncatch (error) {\n  console.log(\"Error loading the hiredis module:\", error);\n}\nvar ReplyError = require('../reply_error');\nconsole.log(\"hiredis> loaded ReplyError\");                                                                                              \nconsole.log(\"hiredis> setting up module...\");\n```\nHere is the output with the above modifications in place.\nhiredis> load dependencies...\nhiredis> loaded events\nhiredis> loaded util\nError loading the hiredis module: { [Error: Cannot find module 'hiredis'] code: 'MODULE_NOT_FOUND' }\nhiredis> loaded ReplyError\nhiredis> setting up module...\nioredis> loaded hiredis\nioredis> all dependencies loaded. Setting up the module...\nbase redis module loaded\nreply-error loaded\nbluebird loaded\ncluster loaded\nRedis module loaded.\nRedis cliet created.\n/home/jedwards/aviata/campaign-builder-import/node_modules/ioredis/lib/parsers/hiredis.js:33\n  this.reader = new hiredis.Reader({\n                           ^\nTypeError: Cannot read property 'Reader' of undefined\n  at HiredisReplyParser.reset (/home/jedwards/aviata/campaign-builder-import/node_modules/ioredis/lib/parsers/hiredis.js:33:28)\n  at new HiredisReplyParser (/home/jedwards/aviata/campaign-builder-import/node_modules/ioredis/lib/parsers/hiredis.js:24:8)\n  at Redis.exports.initParser (/home/jedwards/aviata/campaign-builder-import/node_modules/ioredis/lib/redis/prototype/parser.js:18:22)\n  at Socket.<anonymous> (/home/jedwards/aviata/campaign-builder-import/node_modules/ioredis/lib/redis/event_handler.js:24:10)\n  at Socket.g (events.js:199:16)\n  at Socket.emit (events.js:129:20)\n  at TCPConnectWrap.afterConnect [as oncomplete] (net.js:991:10)\n. ",
    "Ehesp": "Also had this issue, had to npm i --save hiredis to sort it for me.\n. ",
    "chrishiestand": "Well that's great to hear. Sorry for the simple question, but I'm relatively new to using redis and ioredis. What is the correct syntax for writing such a command?\nredis.set('mykey', 'blah', 'EX 10') gets me Possibly unhandled ReplyError: ERR syntax error\n. Ah, thank you that works. That makes much more sense :-)\n. ",
    "smithandweb": "Thanks for the reply. I'll add code as soon as I can but I'll be away from this server for about a week on a work trip. I'll update you when I get back to it. \nThanks!\n. Hey thanks for fixing this! I'll be sure to update.\nFor your curiosity this was the code that was giving me the Command queue error:\n```\n          //var monitored = false;\n        _.client.on('error', function(err) {\n            _.errorHandler(err);\n        });\n\n        _.client.on('reconnecting', function(delay){\n            logger.debug('Retrying connection in: %s ms', delay);\n        });\n\n        // setInterval(function(){\n        //     if (_.client.status !== 'ready' && monitored) {\n        //         _.client.monitor(function(err, monitor) {\n        //             monitor.off('monitor');\n        //             monitored = false;\n        //         });\n        //     } else if (_.client.status === 'ready' && !monitored) {\n        //         _.client.monitor(function (err, monitor) {\n        //             monitor.on('monitor', function (time, args) {\n        //                 if (args.indexOf('info') === -1) {\n        //                     logger.log('Redis activity: Time: %s Args: %s', time, args[0]);\n        //                 }\n        //             });\n        //         });\n\n        //         monitored = true;\n        //     }\n        // },3000);\n\n```\nThis was basically my fix for checking the ready state and if it was monitored. I tried both removeAllListeners and .off()\nThanks again for the quick fix!\n. Just confirmed monitors are working as expected. Thanks again.\n. ",
    "dg3feiko": "it works now, thanks\n. Thank you for responsive reply, but actually there are so many npm modules using load then evalsha to perform their functionalities, changing syntax is hard for them (and us). \nCould u make it support script load/eval to/from all nodes?\n. That's nice and at least makes something work at this moment\n. I didnt drill down much into the detail, but i think the command currently queued into the buffer remains a pending state until is it handled by redis server, so that there is no callback fired in the middle( i am not sure this behaviour and correct me if i am wrong). One a command or a batch of command is going to be dropped, just fire a callback with failure.\n. A lru buffer, may it be a Ring Buffer more accurately, holds the last N command that issued but not processed, once the buffer is full, the oldest serveral will be dropped\n. Actually LRU may not be the best policy for cases, but anyway there should be a way to control the growth\n. If it is full, then any incoming command will be failed immediately?\n. ",
    "tftiancai": "retryStrategy \u5982\u679c\u6ca1\u6709\u8bbe\u7f6e\uff0c\u4f1a\u4e0d\u505c\u7684\u53d1\u51faerror\u4e8b\u4ef6\uff1f\nvar client = new Redis(redisConfig.port, redisConfig.ip);\n  client.on('error',function(error){\n    console.log(\"error:\"+ error);\n  });\n\u8fd9\u6837\u4f1a\u4e0d\u505c\u7684\u6253\u5370\u51fa\nError: connect ECONNREFUSED\n. ok\uff0cclient\u653e\u7f6e\u5728request\u4e2d\nvar client = new Redis(redisConfig.port, redisConfig.ip);\n\u6bcf\u6b21\u7528\u5b8c\u9700\u8981quit\u6216\u8005disconnect\u5417\uff1f\n. \u8fd9\u4e48\u8bf4\u5e94\u8be5\u662f\u5e94\u7528\u7ea7\u522b\uff0c\u5e94\u8be5\u6709\u4e2a\u7c7b\u4f3c\u8fde\u63a5\u6c60\u7684\u6982\u5ff5\n1\u3001\u600e\u4e48\u521d\u59cb\u5316\u591a\u4e2a\u8fde\u63a5\nvar client_1 = new Redis(redisConfig.port, redisConfig.ip);\n var client_2 = new Redis(redisConfig.port, redisConfig.ip);\n ....\n2\u3001\u6bcf\u6b21\u8fde\u63a5\u90fd\u9700\u8981\u624b\u52a8\u6307\u5b9a\u54ea\u4e2a\u5417\uff1f\n. ioredis\u662fnode\u7684client\uff0c\u6700\u4f73\u5b9e\u8df5\u662f\uff1f\n1\u3001\u653e\u5728node\u7684server\u8fdb\u7a0b\u91cc\u9762\uff0c\u968f\u7740server\u7684\u542f\u52a8\u521b\u5efa\u591a\u4e2a\u8fde\u63a5\uff0cserver\u7684\u5173\u95ed\u4e5f\u81ea\u52a8\u5c06\u6240\u6709\u7684\u8fde\u63a5\u5173\u95ed\uff1b\n2\u3001\u653e\u7f6e\u8bf7\u6c42\u8fc7\u7a0b\u5f53\u4e2d\uff0c\u6bcf\u6b21\u968f\u7740\u8bf7\u6c42\u521b\u5efa\u8fde\u63a5\uff0c\u8bf7\u6c42\u7ed3\u675f\u5173\u95ed\u8fde\u63a5\uff1b\n. \u975e\u5e38\u611f\u8c22\uff01ioredis\u672c\u8eab\u6709\u8fde\u63a5\u6c60\u7684\u8bbe\u8ba1\u5417\uff1f\u5b83\u662f\u59cb\u7ec8\u53ea\u6709\u4e00\u4e2a\u8fde\u63a5\u8fd8\u662f\u4f1a\u81ea\u52a8\u7ef4\u62a4\u591a\u4e2a\uff1f\n. \u591a\u8c22\u56de\u9988\u54c8\uff01\n. ",
    "bendpx": "second option is perfect. thanks\n. ",
    "Jabher": "It's not about retry strategy. Retry strategy is used to reconnect, right?\nI'm asking about redis commands, not redis connections. E.g. webserver is\nexpected to response in 60 seconds or less otherwise data from redis is\nuseless.\nI do not want to store requests in offline queue, I need to drop them in 5\nseconds if redis connections are unavailable and apply another strategy\nfrom business logic. Failproof, man. I know redis is stable but what if\nsome weirdo infrastructure engeneer drops my cluster. (Already happened\nonce BTW)\nI can do Promise.race([redis.get(token),\ntimeoutRejectPromise(5000)]), but I still want to clear the offline\nqueue of the lib so that when connection will reestablish they will not get\ndata, occupy the network traffic and so on.\nOn Wed, Jun 3, 2015, 18:29 Zihua Li notifications@github.com wrote:\n\nClosed #61 https://github.com/luin/ioredis/issues/61.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/61#event-321986022.\n. Maybe timeout/retries count can be used like a .pipeline option? As far as\nit's rare but reasonably needed case it can have a bit complex api.\n\nOr just cancelable promise can be implemented and all the logic of\nretries/timeouts will be left for users responsibility, just note that\npromise is actually cancelable is required.\nOn Fri, Jun 12, 2015, 20:17 Vitaly Aminev notifications@github.com wrote:\n\nwell, there is .pipeline() / multi() + .exec().\nWhat if there is something similar in terms of semantics? Ie enter\nlimiting mode and then exec to set it in motion. It might be a bit verbose\nthough\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/61#issuecomment-111663515.\n. Sorry, won't be able to test it until Tuesday, I check it as soon as I can\n\nOn Sat, Jun 13, 2015, 08:00 Zihua Li notifications@github.com wrote:\n\nDoes adding an option of maxAttemptsPerRequest to the Redis constructor\nworks for you? @Jabher https://github.com/Jabher\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/61#issuecomment-111717417.\n. I definitely have to sleep more :) Good question, I'll have a discussion with our guys.\nActually, we discussed my usecase with devs team we figured out that we need to save data even after timeout, so using only bluebird .timeout() is correct scenario. But in case we would need dropping the requests - that option would be enough. But I'd rather implement just cancellable promise anyway as far as per-request customisation is more flexible than per-connection. E.g. for my case I would rather prefer to drop read requests and leave write requests working. But for me it would just good option than killer-feature that I need to implement.\n. I know. It's only HMSETs and Redis is used as a caching reflection for other database.\n. \n",
    "joshualo": "I'm experiencing a related issue where a bug in my code was sending a SET command with an empty string as the key.  In the version of nutcracker (0.3.0) that I'm using, an empty key causes the server to close the connection.  ioredis retries and reconnects to the server but when the request is reattempted, the cycle is triggered infinitely making the server unresponsive.  \nI have fixed the bug in my code and have upgraded nutcracker to version 0.4.1 which accepts empty keys, but something like a retry limit is needed to handle the rare cases where a request causes the connection to close.\n. ",
    "shaharmor": "+1 for this. Would greatly help control the priority of requests.\n. I think this is a critical feature as i have already witnessed multiple times a server going down because the queue was filling up.\nI think that having a property to limit the # of items in the queue should be sufficient.\n. I just checked and my example works :+1: \n. I don't mean to connect to multiple non-clustered instances, but that if the given hosts array is 1, it will just create a normal Redis instance.\nThat way i can use the same code for both places where i have a redis cluster and where not. (My hosts array is loaded from a remote source)\n. Maybe make it so ioredis auto-detects if its a cluster instance or not\n. hey @luin, any estimate on when v2 will be out? Anything we can do to speed it up? \n. @luin any update on this? we could really use this for scaling our cluster.\n. Not sure if this has been though about or not, but we would very much like if it would be possible to send all reads only to slaves, and not mix with master (When there are slaves available obviously).\n. there you go:\n127.0.0.1:6379> cluster slots\n1) 1) (integer) 5461\n   2) (integer) 10922\n   3) 1) \"127.0.0.1\"\n      2) (integer) 6379\n   4) 1) \"10.240.56.139\"\n      2) (integer) 6380\n2) 1) (integer) 0\n   2) (integer) 5460\n   3) 1) \"10.240.29.254\"\n      2) (integer) 6379\n   4) 1) \"10.240.78.46\"\n      2) (integer) 6380\n3) 1) (integer) 10923\n   2) (integer) 16383\n   3) 1) \"10.240.56.139\"\n      2) (integer) 6379\n   4) 1) \"10.240.29.254\"\n      2) (integer) 6380\nI think the issue happened when the cluster was not in its OK state, maybe during promotion or some other state\n. But the issue is at this line:\nhttps://github.com/luin/ioredis/blob/1.7.6/lib/cluster.js#L471\nmeaning there was no error, and the result just became not as it should.\nI don't see any check of the response in the master branch\n. It doesn't happen anymore, it only happened last night during a cluster failover.\n. I'm not sure its related to redis.\nI think that there was something in the connection, maybe load on the server, on redis, timeout, network issue, or something like that.\n. I believe this can be closed as i have yet to witness it again using the latest version\n. @luin thank you for your fast response.\nI have some additional questions:\n- When a CLUSTERDOWN error is received, does ioredis handle other hash slots as usual, or does it affect all the requests? Given that its possible to set cluster-require-full-coverage no on redis, its possible that only a part of the cluster will be down. What happens in this case?\n- Is it possible to limit the # of times a request will be retried? I have witnessed situations that the cluster was only partially down (Only some of the hash slots were uncovered), but the library was queuing up millions of requests, causing the server to fill its memory usage, eventually causing it to crash.\n. checks are failing in master branch too, so i don't think its related to this PR\n. On my localhost redis cluster testing it seems so.\n. After looking at the code a bit, i think that every command that fails calls this line: https://github.com/luin/ioredis/blob/master/lib/cluster.js#L472 which is causing a lot of callbacks and setTimeouts, instead of having a single one call all others when its done.\nAlso, this causes the refreshSlotsCache to be called many many times.\n. @luin any chance to release a version with this change?\n. Cool thanks! Also, if you can add me to the contributors list in the README in your next commit it would be great :)\n. Unfortunately its not consistent at all, it happened on 1 server out of 50, and once we restarted it it was working again.\nI think it somehow got into a bad state that caused him to stay in a Connection Closed mode.\n. I think it happens when the library didn't finish loading and we are already sending commands to it\n. I'm not setting cluster.options.enableOfflineQueue = true at all as it is the default, but maybe its possible that the library didn't yet set this property itself and then when its getting to the line you marked its seeing the value as undefined or something...\n. I actually managed to replicate it. Trying to create a simple script now\n. @luin found it. PR ready\n. There is a memory leak somewhere in the cluster module.\nRunning this vs non-cluster clearly shows it.\nI'll try to debug it and find where exactly\n. Ok i think it only happens when using a string for the host instead of an IP\n. @luin i know it shouldn't, but it is :smile: \nNot sure why exactly, but it causes the memory to increase indefinitely, until the server reaches its max memory and crashes.\n. Its also disconnecting those servers (the ones using hostname instead of IP)\n. I know for a fact that it isn't happening in older version because we are using hostnames all over the place\n. Now i'm not sure its related... using 127.0.0.1 and still happens. just slower\n. Try once with cluster & once without:\nhttps://gist.github.com/shaharmor/7fbf9ef4cf606b06c214\n. @ramonsnir i though it was related to the DNS issue at first, but i managed to replicate it without it.\nEven with 127.0.0.1, the RSS keeps rising forever (After about 1-2 minutes you are already at 300MB)\nWithout cluster, it stays steady at around 75MB\n. Locally 3.0.6, in production 3.0.7. It happens on both.\n. Yeah i'm on Node v4.3.0...\n. But it doesn't happen on stable version (From my testing), so its not related to node really..\n. @AVVS so why doesn't it happen on non-cluster / stable cluster?\n. I updated the gist: https://gist.github.com/shaharmor/7fbf9ef4cf606b06c214\nCan you try now? Or is it not because of the examples string but rather the library ones\n. Yeah i though it will :smile: \nI still think its somewhere in the library it self because it doesn't happen in older version.\n. Which node version?\nDid you run it against Redis Cluster? What configuration?\n. Don't really know what to say.. I can reproduce it each time. @ramonsnir any suggestions?\n. @luin anything?\n. Maybe we can go commit by commit and see which one was the one causing the leak.\nIts possible that its one of the libraries like bluebird or lodash too.\n. @luin good! i just found it on the flight back home too :)\n. So now that the bug is fixed, any plans to finish this PR soon?\n. Why use the javascript default? You can use hiredis and it will fallback to javascript if its not installed\n. The only thing that wasn't working is the auto\n. @luin thanks a lot, i appreciate it!\nAny specific guidelines you are following i should know about?\n. LGTM\n. Anyone ever tested this thoroughly?\n. That would be awesome \ud83d\udc4d \n. hey @luin, any chance to get this implemented?\n. @luin What do you think?\n. LGTM\n. LGTM\n. Yea i know that, thats exactly why i opened this: https://github.com/antirez/redis/issues/3346\nIn addition, its possible to build a custom version of redis and comment out this line: https://github.com/antirez/redis/blob/unstable/src/pubsub.c#L320 and that will stop propagation.\n. Following the issue i linked above, I have created a PR on https://github.com/antirez/redis/pull/3381 that enables a user to disable cluster pubsub propagation.\nI honestly don't think it will be merged because the owner of redis have different ideas for pubsub, but it still shows how it can be done.\nMy question remains: do you think its possible to change ioredis pubsub handling to subscribe/publish to a node based on the slots map?\nI understand that:\n1. it will only be effective for users that really disable cluster pubsub propagation\n2. it will require 2x more redis connections, because each node will require both a subscriber and a normal client. (Unlike today that there is just 1 subscriber)\nI just want to see if its feasible, if its something that more users other than me require, and how long you think it'll be to change.\n. I'll see if i can find some spare time for this\n. Where can we see the diff?\n. I have encountered it as well, i think it happens when calling cluster.quit() multiple times\n. Does it work in Cluster mode as well? I want to do something like this:\n``` js\nvar redis = new Redis.Cluster();\n// then sometime later\nredis.connect().then(() => ...)\n```\n. I just want to have the redis instance created in the constructor of my own class, and then make sure it connects before using it.\nWe can just wrap this line: https://github.com/luin/ioredis/blob/master/lib/cluster/index.js#L83\nin an if statement with lazyConnect no?\n. thnx @luin , didn't have any time for this\n. Its possible that during a failover to a slave, the old master will sync from the new master and cause this error to be returned, which makes the whole failover mechanism not so failsafe.\n. @luin something like this?\n``` js\nredis = new Redis(_.defaults({\n      retryStrategy: null,\n      readOnly: readOnly\n    }, node, this.redisOptions, { lazyConnect: true }));\nvar _this = this;\nredis._readyCheck(function (err) {\n  // TODO: handle error\n  _this.nodes.all[node.key] = redis;\n  _this.nodes[readOnly ? 'slave' : 'master'][node.key] = redis;\n\n  redis.once('end', function () {\n    delete _this.nodes.all[node.key];\n    delete _this.nodes.master[node.key];\n    delete _this.nodes.slave[node.key];\n    _this.emit('-node', redis);\n    if (!Object.keys(_this.nodes.all).length) {\n      _this.emit('drain');\n    }\n  });\n\n  _this.emit('+node', redis);\n\n  redis.on('error', function (error) {\n    _this.emit('nodeError', error);\n  });\n});\n\n```\nAlso, how should we handle an error in the _readyCheck function?\n. So that means that this should already be fixed? I've seen this happen in production, so its definitely an issue.\nCould it be that it happens only to slaves or something? or when using scaleReads?\n. Its also possible that it happens if the slave was once connected, but then got restarted for some reason\n. Hey @luin , I just encountered this issue again, and I think we should see how we can fix it.. Buffer changed in new Node version, need to update the library accordingly. It's not dead at all. If you have any contributions to make we will be glad to review them.\nRegarding dropping node < 4, I agree that it's a good move and we should definitely move ahead in that direction.\n@luin, let's talk and see how we can move this forward.. Sound reasonable. Care to make a PR?. slotsRefreshTimeout should be good. @luin ?. Sounds reasonable.\nI would set it on a setting which defaults to the current behavior (show the password), and in the next version change the default to hide the password (To avoid breaking the existing API)\nMaybe use **** instead of [FILTERED] too. @luin what do you think?. The issue is not with the Redis instances, its with the node instances.. bummer . Hey @ccs018, I took a look at your issue.\nThe problem is that currently, ioredis tries to refresh the slots map before actually connecting to the cluster, which causes it to fail and disconnect from the node before it managed to connect (Because you don't have the offline queue enabled).\nFrom my understanding, the redisOptions.offlineQueue is not relevant for cluster mode because when the redis node will go offline it will also be removed from the cluster nodes list, so no commands will be sent to it when its offline.\nOne possible fix for this is to move the refreshSlotsCache call to only after at least one node has been connected. @luin ?. The problem with this is that a cluster can sometimes have nodes that will never come up again, in cases such as auto scaling where a machine goes up, and when it goes down it will never return, which will cause ioredis to basically leak nodes.\nI agree that there are fixes that should be done in the way we handle cluster nodes. I have talked with @luin and we will take a look at it next week. Sorry for the delay.\nThe \"moved\" event will cause ioredis to re-fetch the nodes list, and now with my PR it will do that periodically as well, but I agree that we should probably trigger it on a -node event as well.. Thnx. Hey @ccs018, as far as I know, ioredis only refreshes the cluster nodes list when there is a slot change.\nCan you try to change one of the slots and see if ioredis reconnects to that slave?\nYou are correct that we should add a refresh interval for the cluster nodes list, I'll see if I can hook something up. Can you share how you are calling the Redis.Cluster constructor?. What awkwardness?. Hey @mgetz,\nSorry for not being responsive for a few days.\nCan you please add a test that checks the name is set for monitor clients here: https://github.com/luin/ioredis/blob/master/test/functional/connection.js. @szeist can you please add a test that checks it?. Isn't this test verifies it? https://github.com/luin/ioredis/blob/master/test/functional/connection.js#L77-L95. @szeist I still don't understand the difference between this:\nhttps://github.com/szeist/ioredis/blob/b4402b12f23b90515b1641270b38b0b77e6ed160/test/unit/redis.js#L154-L165\nand this:\nhttps://github.com/luin/ioredis/blob/master/test/functional/connection.js#L78-L95. They are fine, but I think @luin is not available. Haven't seen him in quite a while. Hey @aalexgabi , at the moment there is no such way. We'll welcome a PR if you want to make one. Cluster:\n- Refresh slots upon a disconnection from one of the cluster nodes\n- Restructure the cluster initialization code so that the initial slots refresh call will be done only after connecting to the first node. This will help with issues like #581 where the user is setting enableOfflineQueue: false which causes the slots refresh call to fail immediately and disconnect from the node.\n- Consider setting enableOfflineQueue: true explicitly on the internal cluster nodes - @luin correct me if I'm wrong, but the offline queue of the internal node shouldn't be used in cluster mode because when ever the socket will disconnect the node will automatically be removed.\nOther:\n- Remove callback support? Not sure why we have to support callbacks if we support promises. If anyone really needs callback support, they can \"callbackify\" the promises themselves. Why do we need the entire library to support it?. @ccs018 we now refresh the slots map every 5s which will find that missing node and re-join it.. Yes it was merge, and you are correct it wasn't mentioned.. @ccs018 any objections to close this PR now?\n. @jeremytm you are correct. Is this a new bug?. Maybe use _this for consistency?\n. I think this test is not well defined, as its possible that the 2nd foo will come from the line 482 and not from the retry itself.\nthis will be better tested like this:\n``` javascript\ndescribe('TRYAGAIN', function () {\n    it('should retry the command', function (done) {\n      var times = 0;\n      var slotTable = [\n        [0, 16383, ['127.0.0.1', 30001]]\n      ];\n      var server = new MockServer(30001, function (argv) {\n        if (argv[0] === 'cluster' && argv[1] === 'slots') {\n          return slotTable;\n        }\n        if (argv[0] === 'get' && argv[1] === 'foo') {\n          if (times++ === 1) {\n            return;\n          } else {\n            return new Error('TRYAGAIN Multiple keys request during rehashing of slot');\n          }\n        }\n        if (argv[0] === 'get' && argv[1] === 'boo' && times === 2) {\n          process.nextTick(function () {\n            cluster.disconnect();\n            disconnect([server], done);\n          });\n        }\n      });\n  var cluster = new Redis.Cluster([\n    { host: '127.0.0.1', port: '30001' }\n  ], { retryDelayOnTryAgain: 1 });\n  cluster.get('foo', 1, function () {\n    cluster.get('boo');\n  });\n});\n\n});\n```\n. We should export possible statuses to an enum to avoid having typos.\nsomething like:\n_this.status = Redis.READY\n. Yeah, but your IDE will probably detect it.\n. Not critical though, just saying :)\n. duplicate from two tests below, did you mean to use dropBufferSupport: false?\n. Yeah but there is another test that is exactly the same down below (Look at the last test)\n. I think just \"OK\" is good\n. rename connectionConnectHandler to connectionReadyHandler. ",
    "focomoso": "Is there any news here? I have what seems to be a very common use case that could use this timeoutPerRequest option. (redis-rb has something similar that they call read_timeout and write_timeout). We're using redis as a mem cache. We do a get on redis and if what we're looking for isn't there, we pull it from an upstream service. The problem comes when the connection to redis is lost. We have to wait for the get to timeout (10 seconds) before we know there's a problem and can switch to the upstream.\n. Thanks @luin - I ended up going with the timeout promise method, but a timeoutPerRequest would be great too. Make things cleaner. \n. ",
    "nicholasf": "@luin how far are you with the timeoutPerRequest logic? I'm wondering if there's any point in us attempting a solution over here. We need this functionality too. :) \nGreat work on the project!\n. @luin ok, thanks. Will look into maxRetriesPerRequest plus retryStrategy.\n. For the record, we are just using the the Bluebird Promise timeout. Not great to mix promises with callbacks but it's the simplest. \n. Thanks luin.. ",
    "mjh1": "@luin would setting the autoResendUnfulfilledCommands or enableOfflineQueue option to false achieve the same result as flushing the offline queue inside the retryStrategy?\nI'm curious to know exactly what was meant by \"flushing the offlineQueue in the retryStrategy\", I couldn't find example code in this thread, so maybe I'm missing something.. Thanks, looks good.. ",
    "elyobo": "Did anything ever happen with this @luin?\nIt seems like wanting the connection to generally keep retrying connections forever, but not wanting a specific request to block forever would be a common requirement.  For example, I'm using redis as a to store state for a rate limiter, I don't want an individual request to get blocked forever so I don't want unbounded retries, but I do want the connection to be tried on the next request.  It seems like the suggested maxRetriesPerRequest would do this (I can leave the retryStrategy unbounded, but limit the maximum retries on any given request).\nI don't understand how the offline queue has any bearing on this behaviour; whether requests are queued up or not doesn't matter, I just don't want a specific request to keep being retried but I do want the connection to start retrying again when the next request is made.. :tada: thanks @luin . See also https://github.com/luin/ioredis/issues/61 which seems related.. You're right, I misread the issue.. ",
    "edevil": "I've read the comments to this issue but I haven't found a way to deal with unresponsive servers other than the promise timeout. When we are using third party modules that just take a redis connection as parameter (koajs/ratelimit for example) we can't use this method.\nIt seems unlikely that I will be able to persuade all the module authors to alter the way they access the redis server to include a timeout, it would be much easier if ioredis could take a statement timeout as a config parameter and enforce this.. This PR, along with https://github.com/luin/ioredis/pull/658, seem very important to me in order to prevent an unresponsive Redis server from bringing down an application. Is there a reason for them not having been merged?. Seems like the referenced PR was abandoned. What's the current status on handling unresponsive servers?. I still think that a configurable global command timeout is useful when dealing with unresponsive servers because there are some modules that take a Redis proxy object and issue the commands themselves. All these modules would need to be updated to correctly guard for timeouts.. Is the option maxOfflineQueueSize available? It seems the PR was not merged. :(. ",
    "cvlahakis": "I stumbled on this thread today when looking into GET commands hanging due to cluster unavailability. Setting enableOfflineQueue to false produced the behavior I was looking for (fail fast).. ",
    "knguyen2525": "Oh I see. I just read the documentation more closely and get your point.\nThanks\n. ",
    "devaos": "Oh, I see what you mean.  However this commit did resolve the issue we were having.  I'll comment shortly with the debug logs so you can see what was happening.\n. Here is a snippet of the debug output before the patch.  I added a couple console lines to clarify which command was being sent and whether the stream was writable.\nioredis:redis status[127.0.0.1:6381]: close -> end +1ms\n  ioredis:redis status[10.240.14.47:6375]: connect -> ready +0ms\n  ioredis:redis status[10.240.46.240:6375]: connect -> ready +0ms\n  ioredis:redis status[10.240.75.76:6375]: connect -> ready +1ms\n  ioredis:redis status[127.0.0.1:6381]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[127.0.0.1:6381]: close -> end +0ms\n  ioredis:cluster getting slot cache from 10.240.46.240:6375 +2s\nsendCommand: command.name=cluster\nsendCommand: stream.writable=undefined\nsendCommand: this.stream.writable=true\n  ioredis:redis write command[0] -> cluster(slots) +1ms\nsendCommand: command.name=subscribe\nsendCommand: stream.writable=undefined\nsendCommand: this.stream.writable=false\n  ioredis:cluster getting slot cache from 10.240.46.240:6375 +2s\nsendCommand: command.name=cluster\nsendCommand: stream.writable=undefined\nsendCommand: this.stream.writable=true\n  ioredis:redis write command[0] -> cluster(slots) +0ms\nsendCommand: command.name=subscribe\nsendCommand: stream.writable=undefined\nsendCommand: this.stream.writable=false\n. I updated the commit with what I believe is a more appropriate fix based on your feedback.  Thanks\n. ",
    "haoxins": "yes, I know. But I think this is a very normal option.\n. got it.\n. If enableOfflineQueue is false, should we just throw error when connect failed?\n. :smile: yeah, got it. \n. ",
    "nakulgan": "Cool, I agree on both of them. I'll update the PR with changes accordingly later today.\n. Any idea what Version of Redis they're running at redistogo.com?\nSELECT '' seems to be valid on >Redis 2.4 and defaults to 0.\n. Great! Thanks for quick fix.\n. ",
    "Ajido": "Yes, it works.\nThanks!\n. I didn't know that. Thank you, I will.\n. ",
    "frankvm04": "Done.\n. Thanks for ioredis - works great!\n\nOn Jun 11, 2015, at 10:29 AM, Zihua Li notifications@github.com wrote:\nMerged. Thank you for the patch!\n\u2014\nReply to this email directly or view it on GitHub https://github.com/luin/ioredis/pull/71#issuecomment-111174048.\n. \n",
    "Bossa573": "will try clusterRetryStrategy, thanks\n. ",
    "kimmobrunfeldt": "It seems that .quit() method is not defined in the code either.\n. Ok thanks. Didn't realize that.\n. By setting the database to 0 in the url: redis://redistogo:<secret>@<secret>.redistogo.com:<port>/0 the issue goes away.\n. This might be an bug in parsing urls with trailing slash. Quote from RedisToGo support:\n\nJust wanted to send you a bit more information, as I did some digging into the driver code and I have found the source of the issue. Here is some sample code from the node shell that should explain things:\n``` javascript\nvar urllib = require('url');\nvar Redis = require('ioredis');\n// Explicit database 0\n\nvar redis = Redis('redis://redistogo:mypassword@myhost.redistogo.com:54321/0');\nundefined\nredis.get('foo', function(err, result){console.log(result);});\n{ _bitField: 1,\n  _fulfillmentHandler0: [Function: successAdapter],\n  _rejectionHandler0: [Function: errorAdapter],\n  _progressHandler0: undefined,\n  _promise0: [Function],\n  _receiver0: [Circular],\n  _settledValue: undefined }\nbar\n\n// Implied database 0\n\nvar redis2 = Redis('redis://redistogo:mypassword@myhost.redistogo.com:54321');\nundefined\nredis2.get('foo', function(err, result){console.log(result);});\n{ _bitField: 1,\n  _fulfillmentHandler0: [Function: successAdapter],\n  _rejectionHandler0: [Function: errorAdapter],\n  _progressHandler0: undefined,\n  _promise0: [Function],\n  _receiver0: [Circular],\n  _settledValue: undefined }\nbar\n\n// Force parse error\n\nvar redis3 = Redis('redis://redistogo:mypassword@myhost.redistogo.com:54321/');\nundefined\nUnhandled rejection ReplyError: ERR invalid DB index\n    at ReplyParser._parseResult (/mycomputer/vault/code/node_realm/node_modules/ioredis/lib/parsers/javascript.js:56:14)\n    at ReplyParser.execute (/mycomputer/vault/code/node_realm/node_modules/ioredis/lib/parsers/javascript.js:174:20)\n    at Socket. (/mycomputer/vault/code/node_realm/node_modules/ioredis/lib/redis/event_handler.js:88:22)\n    at Socket.emit (events.js:107:17)\n    at readableAddChunk (_stream_readable.js:163:16)\n    at Socket.Readable.push (_stream_readable.js:126:10)\n    at TCP.onread (net.js:538:20)\nredis3.get('foo', function(err, result){console.log(result);});\n{ _bitField: 1,\n  _fulfillmentHandler0: [Function: successAdapter],\n  _rejectionHandler0: [Function: errorAdapter],\n  _progressHandler0: undefined,\n  _promise0: [Function],\n  _receiver0: [Circular],\n  _settledValue: undefined }\nbar\n\n// Different parsed URLs\n\nvar p = url.parse('redis://redistogo:mypassword@myhost.redistogo.com:54321/0');\nundefined\np\n{ protocol: 'redis:',\n  slashes: true,\n  auth: 'redistogo:mypassword',\n  host: 'myhost.redistogo.com:54321',\n  port: '54321',\n  hostname: 'myhost.redistogo.com',\n  hash: null,\n  search: null,\n  query: null,\n  pathname: '/0',\n  path: '/0',\n  href: 'redis://redistogo:mypassword@myhost.redistogo.com:54321/0' }\np.pathname.slice(1)\n0\nvar p2 = url.parse('redis://redistogo:mypassword@myhost.redistogo.com:54321');\nundefined\np2\n{ protocol: 'redis:',\n  slashes: true,\n  auth: 'redistogo:mypassword',\n  host: 'myhost.redistogo.com:54321',\n  port: '54321',\n  hostname: 'myhost.redistogo.com',\n  hash: null,\n  search: null,\n  query: null,\n  pathname: null,\n  path: null,\n  href: 'redis://redistogo:mypassword@myhost.redistogo.com:54321' }\n// p2.pathname is null, won't call .split(1) and _.defaults will be called\n// See code below\n// https://github.com/luin/ioredis/blob/master/lib/utils/index.js#L254-L285\nvar p3 = url.parse('redis://redistogo:mypassword@myhost.redistogo.com:54321/');\nundefined\np3\n{ protocol: 'redis:',\n  slashes: true,\n  auth: 'redistogo:mypassword',\n  host: 'myhost.redistogo.com:54321',\n  port: '54321',\n  hostname: 'myhost.redistogo.com',\n  hash: null,\n  search: null,\n  query: null,\n  pathname: '/',\n  path: '/',\n  href: 'redis://redistogo:mypassword@myhost.redistogo.com:54321/' }\np3.pathname.slice(1)\n''\n```\n\nLooking through the documentation for the driver they list a trailing slash as a valid URL, but in this driver it is not handled correctly. Here is the example in the docs:\nhttps://github.com/luin/ioredis/blob/master/lib/redis.js#L84\nMy proposed solution would be to modify this code:\nhttps://github.com/luin/ioredis/blob/master/lib/utils/index.js#L270-L272\nif (parsed.protocol === 'redis:') {\n    result.db = (parsed.pathname !== '/')\n      ? parsed.pathname.slice(1)\n      : '0';\n}\nI hope this helps explain things but please let me know if you have any further questions.\nThank you,\n~John Moore\nRedisToGo Support\n. I was under the impression that I should disable the pipeline when I'm not doing the operation as a single line operation such as: redis.multi().set('a', 1).set('b', 2).exec()\n. Ok now I got it. Thanks :) I'll change it to use pipeline for now bu I might check the lua script at some point. \n\nFor others reading this, my first code example is wrong in many ways. This should be better(haven't tested though):\n``` js\nvar redis = require('./configured-redis-instance');\nfunction removeIndex(key, index) {\n    return withMulti((multi) => {\n        // This function needs to return a Promise but this operation\n        // happens to be synchronous\n        return Promise.resolve(\n            multi\n                .lset(key, index, 'deleted')\n                .lrem(key, 0, 'deleted')\n        );\n    });\n}\nfunction withMulti(func) {\n    var multi = redis.multi();\nfunc(multi)\n    .then(funcResult => {\n        return Promise.props({\n            exec: multi.exec(),\n            funcResult: funcResult\n        });\n    })\n    .then(result => {\n        if (result.exec === null) {\n            throw new Error('Transaction failed');\n        }\n\n        return result.funcResult;\n    });\n\n}\n```\n. ",
    "ORESoftware": "exports = module.exports is totally redundant, I don't care who uses it! :)\n. Sorry, one cannot be polite when criticizing code, you just have to have a thick skin\nthe point is it took me too much time to figure out what your index.js was doing.\nit should not be that hard.\nyour code is a mess in that file, which makes me doubt the other stuff.\n. My point is this:\n``` javascript\nvar ioredis = require('ioredis');\nconsole.log(ioredis.print());\nconsole.log(ioredis.Promise);\nconsole.log(ioredis.Cluster);\n```\nyour attaching a bunch of crap to the export object, either it's just wrong or it's bad design or both.\nyour index.js file should just look like this:\njavascript\nmodule.exports = require('./lib/redis');\nthe way it is now, when I require('ioredis'), attached to that object I am also getting Promise, Bluebird, Cluster.\nIt's just wrong guys, sorry.\n. @AVVS sure, I understand. I just want to make sure my point makes any sense before I create some PR that is useless\n. if I am wrong please let me know, if I am more correct than not, I would happy to spend 20 hours refactoring the code\n. have a look at the Express NPM module\nthey have the same thing as you\njavascript\nvar express = require('express');\nvar app = express();\nyou should make the export functionality so that ioredis =~ express\nso you can return a function from the express module, but also attach properties to that function.\nthat's why we can call express() but also we can call express.static or express.Router\nFrankly, the Express library doesn't do it right either. but you can make your index.js file simpler, by modeling it like the Express module. your library and the Express module are overcomplicating things.\nexports = module.exports ...\nthat is bad\n``` javascript\nexports = module.exports = createApplication;\n/\n * Create an express application.\n \n * @return {Function}\n * @api public\n /\nfunction createApplication() {\n  var app = function(req, res, next) {\n    app.handle(req, res, next);\n  };\nmixin(app, EventEmitter.prototype, false);\n  mixin(app, proto, false);\napp.request = { proto: req, app: app };\n  app.response = { proto: res, app: app };\n  app.init();\n  return app;\n}\n/*\n * Expose the prototypes.\n /\nexports.application = proto;\nexports.request = req;\nexports.response = res;\n```\nthis is BAD! bad bad bad \ndo this instead\n``` javascript\nfunction IORedis(){\n}\nIORedis.luin = function(){\n}\nIORedis.overcomplicateEverything = function(){\n}\nmodule.exports = IORedis;\n```\nthis is SOOOOO much more clear. Do you want to be like those assholes that wrote Express or do you want to think for yourself and help the community? :)\nAlso, I don't understand why you are exporting Promise from your library, but I guess Cluster is something special that pertains to your library and doesn't have to do with NPM cluster.\n. I don't think so, can you prove that?\n. yes syntactically it is different, but semantically it is not different\n. thanks....humma.... it seems like an on('subscribe') event would be useful to have in the library itself...am I missing something?\n. ok thanks for understanding, I look forward to the feature and let me know if you wish for me to make a PR\n. any update on this one? thanks for your help\n. for sure, will give it a shot\nOn Jan 25, 2016 7:17 PM, \"Zihua Li\" notifications@github.com wrote:\n\nA little busy these days, will get time to implement it this weekend. Pull\nrequest is welcome before that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/232#issuecomment-174796667.\n. nevermind fixed it by disabling auth\n. \n",
    "BridgeAR": "I thought about this issue before too and I have the same concerns as @AVVS that the flushed commands might do more harm than help.\nAnd even if the commands would not mutate state, they still might be harmful, as the result might be expected before handling something else.\nAnd the number of commands is really not a good indicator for memory growth. There might be a few commands that use up more memory than a whole lot of small ones.\nThe suggestion from @dirkbonhomme is likely the only \"safe\" way to handle this. The user is able to check all commands, if they exceed a specific memory amount, no matter if it's done by checking each command arg or by checking the number of commands (maybe all commands are of a fixed size). He's also able to distinguish between commands that might be harmful to throw away and between others and so forth.\nAs this feature is not yet released I suggest to remove it again and to update the docs instead to outline how to handle such situations.\n. You are right that the user should handle the error. But if you shift a value from the commands queue that alters state and other commands were already added to the command queue that rely on that altered state (this is very likely as the user would not get any error so far) it is fairly difficult to handle this properly without just flushing the whole queue and starting all over again.\nAnd the amount of commands is still not really useful. Instead I recommend to add a maxOfflineQueueMemorySize option and to collect the overall memory footprint of the offline queue (using Buffer.byteLength() should be a god enough way to measure this).\n. Right now most time is spend by concatenating the buffer chunks together. So we'd have to find a way to improve that and that's not an easy task. @luin and me decided to move the parser to a own module so we might have another look at that at some point.\n. The do while loop can't be used in this case, since not every chunk might have a return value. But a similar approach is used in the new parser module.\n. @ekousp Please use the hiredis parser. It should improve the speed significantly. Handling big data in the js parser is really slow and a known issue :/\nGoogle translator: \u8bf7\u4f7f\u7528hiredis\u89e3\u6790\u5668\u3002\u5b83\u5e94\u8be5\u663e\u8457\u63d0\u9ad8\u901f\u5ea6\u3002\u5728JS\u89e3\u6790\u5668\u5904\u7406\u5927\u6570\u636e\u786e\u5b9e\u662f\u6162\uff0c\u4e00\u4e2a\u5df2\u77e5\u7684\u95ee\u9898\n. To handle errors properly it is best to use promises. If you do not set a callback a promise will be returned by the call. The call itself is async but redis works sync so it is possible to fire multiple calls without waiting for the response but this is not recommended in most cases.\n. @tangxinfa this is not supported in node_redis either and was a bug, not a feature.  It was fixed in v.2.0.0.\n. How are you able to determine what keys you actually have in redis, if the keys parameter would be prefixed? This is not possible in that case, so if you want to have the keys command prefixed you might consider adding a arguments transformer to it to always prefix the argument. I do not recommend it though. \n. This might not be a bad idea. I have to think about it when I'm more awake though. I just realized I just wrote total nonsense somewhere else ;)\n. So after looking at it I see the following issues: if a command takes longer than 500ms on Redis, all fired commands are \"failing\". That is not intended, so the timeout should be more like 15 seconds or something really \"safe\" and some people might even want to raise that value further. It is definitely a weighing up for the user: do I want to be sure heavy commands don't trigger a reconnect or do I want to detect timeouts faster.\nThe client allows all kinds of different use cases and some might rely on heavy commands.\nBut this is not the only downside: the ping may not be fired by the user client as that client might be used as a queue with blocking commands that wait for a arbitrary time. Starting a keep alive client for each user client might not be a good option either (some people use lots of connections and there is a maximum clients per server limit) but it is the only safe way that comes to my mind.\nSadly implementing this in NodeJS would not help us all that much right now, as the feature would not be backported to the LTS versions.\nSo this might be added as a opt-in option but it is definitely not ideal :/\n. I still recommend using bytes to determine if the queue is full or not. The memory is not exact but it's not difficult to have a approximation that is about right.\n. @ScheerMT one reason why this can't be done right away is also that the clients support different things at the moment and both need to have round about the same features to make such a thing possible.\nSo reusing the same code is a step into that direction.\nTwo of the goals in this milestone are actually to use those abstractions.\n. It was late in the night when I changed that and the \"default\" parser (as stated in the README) should be chosen (so first hiredis and if not available the js parser) instead. Nevertheless I'd like to keep the warning to make sure it's properly implemented and therefore how the fallback mode works is not that important.\n@luin Sorry for not checking your code before releasing the new version, I'll do that next time! I was not aware that ioredis used the word \"auto\" and yes, my way of implementing auto was to pass null or undefined to the parser. I'm going to document that a bit clearer.\n. The parser change yesterday: http://www.commitstrip.com/en/2014/05/07/the-truth-behind-open-source-apps/ :laughing: \nv.1.3 knows 'auto' from now on too and falls back into that mode if it does not know what parser was requested instead of always choosing the JS parser (in the later case it'll print a warning though). \n. I agree that it would be nice to have a unified format. I also like the idea of @an-sh. As ioredis v.2 is not yet released it could be added there and for node_redis I'm likely going to release a v.3 alpha to get some breaking changes in there and keep support for v.2 at the same time.\nSo overall :+1: \n. You're looking for psubscribe. Be aware that you also have to listen to pmessage / pmessageBuffer in that case. \n. @christianpv with what are you compiling your code? I guess you use a bundler and it resolves all dependencies up front and therefore does not detect, that the module may actually fail to load.\n@luin in the parser v.3 hiredis is not included anymore.. @shaylevi2 I am not sure why you think v3 is much faster than v2. ioredis supports older Node.js versions than the redis-parser v3. The main performance difference is only for errors and this should not have a big influence on any application. I am closing this as I recommend against changing this right now.. The typeof is to much. Otherwise this function is always going to throw ;-)\n. As a hint: there's a easy polyfill for this and Microsoft itself does not maintain IE 8 or older, not even with security fixes on any regular Windows version. Most libraries drop support for it or have already dropped the support and the faster all developers switch to not supporting it anymore, the faster no one is going to visit any website with it anymore.\n. ",
    "mike-marcacci": "Thanks! That's what I was afraid of (but really expecting). Sounds like this will be a fun weekend challenge figuring out how to keep guarantees in the midst of cluster growth. I'll bother you on gitter if I have any specific questions :)\nI'm really impressed with ioredis btw. I'll probably start moving to it for most of my projects.\n. ",
    "tzanko-matev": "Thank you for the quick reaction.\n. ",
    "siddo420": "Looks like metroplex is using https://github.com/mranney/node_redis\n. Ok, so you're saying that metroplex is dependent on that particular node module because of the function/method it uses.\nAny chance ioredis would implement a function/feature like that?\n. Looks like metroplex is using a Redis wrapper named leverage (https://www.npmjs.com/package/leverage) around redis client and that wrapper is using send_command.\nI used the following command to list all instances where send_command is used in both leverage and redis client.\negrep -ir \"send_command\"  node_modules/metroplex/\n. Thanks.\nIs there an easy way to dynamically add a random (unknown) number of members to sorted set in a single command? I have members in an array but only want to use a single command.\nhttp://redis.io/commands/zadd\n. Thanks luin\n. ",
    "3rd-Eden": "Thank you (I'm the author of leverage, metroplex).\n. ",
    "ahkimkoo": "\u6211\u7528node-redis\u548cgeneric poll\u5c1d\u8bd5\u4e86\u8fde\u63a5\u6c60. \u5728\u9ad8\u5e76\u53d1\u7684\u60c5\u51b5\u4e0b\u6bd4\u4e00\u6b21\u64cd\u4f5c\u8fde\u63a5\u4e00\u6b21\u6216\u8005\u591a\u6b21\u64cd\u4f5c\u4f7f\u7528\u4e00\u4e2a\u8fde\u63a5\u7684\u6027\u80fd\u8981\u660e\u663e\u9ad8\u51fa\u5f88\u591a. \u6240\u4ee5\u6211\u89c9\u5f97\u8fde\u63a5\u6c60\u8fd8\u662f\u6709\u5fc5\u8981\u7684.\n. \u6211\u662f\u7ed3\u5408\u6211\u7684\u4e1a\u52a1\u4ee3\u7801\u6d4b\u8bd5\u7684. \u6211\u7684\u4e1a\u52a1\u5355\u7528\u6237\u6bcf\u6b21\u8bbf\u95ee\u8bfb\u5199redis\u7ea647\u6b21. \u91cc\u9762\u6709\u4e00\u4e9bparalle\u64cd\u4f5c, \u5728\u53d1\u90012\u4e07\u8bf7\u6c42\u7684\u60c5\u51b5\u4e0b,\n\u5206\u522b\u5bf9\u4e00\u4e2aredis\u8fde\u63a5\u548c\u8fde\u63a5\u6c60\u7684\u60c5\u51b5\u505a\u4e86\u6d4b\u8bd5, \u53d1\u73b0\u4f7f\u7528\u8fde\u63a5\u6c60\u540e\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u548c\u6700\u957f\u54cd\u5e94\u65f6\u95f4\u90fd\u6bd4\u4f7f\u7528\u5355\u4e00\u8fde\u63a5\u7684\u8981\u77ed. \u6211\u4e5f\u6ca1\u641e\u6e05\u695a\u539f\u56e0,\n\u5f53\u7136\u4f7f\u7528generic poll\u505aredis\u7684\u8fde\u63a5\u6c60\u6709\u4e00\u4e2abug, \u5f53\u8fde\u63a5\u6570\u8fbe\u5230\u8bbe\u5b9a\u7684max\u6570\u91cf\u540e, \u5f88\u591a\u8fde\u63a5\u4f1a\u50f5\u6b7b, \u56e0\u4e3ageneric\npoll\u5c06\u8bf7\u6c42callback\u653e\u5728\u961f\u5217\u91cc\u6e05\u7406idel\u7684\u65f6\u5019\u662f\u6e05\u7406\u8fd9\u4e9bcallback, \u800c\u8c03\u7528\u7aef\u4e00\u76f4\u7b49\u5f85callback.\n\u5982\u679c\u4f60\u6253\u7b97\u52a0\u8fde\u63a5\u6c60\u7684\u8bdd\u7279\u522b\u7559\u610f\u4e00\u4e0b\u8fd9\u4e2a\u9677\u9631.\n2015-07-10 9:56 GMT+08:00 Zihua Li notifications@github.com:\n\n\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u6d4b\u8bd5\u4ee3\u7801\u5417\uff1f\u6211\u5468\u672b\u8bd5\u4e00\u4e0b\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/92#issuecomment-120199700.\n. \n",
    "cfan8": "\u8fde\u63a5\u6c60\u80fd\u63d0\u9ad8\u6027\u80fd\u6709\u53ef\u80fd\u53ea\u662f\u56e0\u4e3a\u7f13\u5b58\u53d8\u5927\u7684\u539f\u56e0, \u5efa\u8bae\u628a\u5185\u6838\u53c2\u6570\u91cc\u7684TCP\u8bfb\u5199\u7f13\u5b58\u9009\u9879\u8c03\u5927\u8fd9\u6837\u6bd4\u8f83\u624d\u516c\u5e73\n. ",
    "alsotang": "\u603b\u7ed3\u6765\u770b\uff0c\u4e00\u822c\u9700\u8981pool\u7684\u60c5\u51b5\u5728\u6211\u770b\u6765\u662f\u4ee5\u4e0b\uff1a\n1. \u670d\u52a1\u5668\u5177\u5907\u591a\u7ebf\u7a0b\u80fd\u529b\n2. \u7f51\u7edcio\u592a\u6162\n\u5bf9\u4e8emysql\u548cpg\u6765\u8bf4\uff0c1\u548c2\u90fd\u7b26\u5408\uff0c\u800c\u4e14\u5927\u5bb6\u7684\u4e60\u60ef\u4e5f\u662f\u5f00\u542fpool\u6765\u4f7f\u7528\u3002\n\u5bf9\u4e8eredis\u6765\u8bf4\uff0c\u53ea\u80fd\u662f2\u8fd9\u4e2a\u539f\u56e0\uff0c\u56e0\u4e3aredis\u662f\u5355\u7ebf\u7a0b\u7684\uff0c\u5047\u5982\u4e00\u4e2ascan\u547d\u4ee4\u5360\u6ee1\u4e86cpu\uff0c\u5176\u4ed6\u6240\u6709connections\u90fd\u5361\u4f4f\u3002\u5f53\u4e0d\u4fe1\u4efb  redis client \u548c redis server \u4e4b\u95f4\u7684\u7f51\u7edc\u8fde\u63a5\u65f6\uff0c\u5c31\u9700\u8981\u7528 pool \u6765\u63d0\u9ad8 redis server \u5229\u7528\u7387\u4e86\u3002\n\u53c2\u8003\u4e0b https://github.com/coopernurse/node-pool . alright, if this pr seems ok, just ignore the .gitignore change\n2015-09-06 18:13 GMT+08:00 Zihua Li notifications@github.com:\n\n+1 for this, except .DS_Store should be ignored globally instead of being\nadded to the project's .gitignore file since this kind of files are not\nioredis specified.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/pull/140#issuecomment-138062260.\n. I thought you would do this for me...\n\nit's done\n2015-09-06 19:45 GMT+08:00 Zihua Li notifications@github.com:\n\nYou'd better amend this commit or just submit an other commit to revert\nthe changes of .gitignore so that I can merge this pr directly. [image:\n:rocket:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/pull/140#issuecomment-138075451.\n. \n",
    "zorro-fr24": "thanks for the quick response.\nYup, I had investigated redis-scanstreams and node-redis-streamify and they both seem to rely on extending node_redis's client prototype which won't work with ioredis without modification.\nI didn't need anything fancy, so I tested node-redis-streamify rather than scanstreams ( which looks like a better implementation, using proper piped streams )\nIn the case of modifying streamify, it's literally 1 line of code, or you can work around it in userland by:\n```\nvar Redis = require('ioredis'),\n    RedisStreamify = require('node-redis-streamify');\n/ Add reference to ioredis client with node_redis property /\nRedis.RedisClient = Redis;\nRedisStreamify(Redis);\n/ ... continue as normal /\n```\nIt works, but it's ugly, so I'd love to see a native implementation.\nCheers\n. Fantastic!\nI look forward to it.\n. awesome work, looks very neat - I'll give it a spin on monday - thanks!\n. ",
    "damianobarbati": "@luin is is possible to ad a promise'd version of it?\nconst result = await redis.scanAsync({ match: 'user:*', count: 100 });\nFaster when developer is expecting small set of items from a scan without using the \"evil\" keys nonetheless.. Hey guys, can you explain what's the final behaviour? \nredis.set(key, content, 'EX', timeout, function (err, res) {\n is this execute right when the key is EXPIRED?\n});. ",
    "vweevers": "In node v10 you can use readable[Symbol.asyncIterator]() to for await (x of stream).. Is socket.setKeepAlive(true) not an option? TCP keep-alive probes are very cheap and work well.. @ccs018 Oh I see, the issue is about detecting dead connections faster. That's cool. I just hope the added functionality will remain opt-in (because for my needs, waiting 10 minutes is perfectly fine) and not add side effects to an already complicated module.. @ccs018 This must mean TCP keep-alive is disabled by default (client side).\n@luin Does ioredis expose the raw socket somewhere, so that I can call setKeepAlive on it?. Using a Set to record visited keys is one solution, but having to keep all keys in memory can be a huge drawback. Another solution is to make your operations on the keys idempotent, so that it doesn't matter if you visit a key 1 time or 10 times.\nWhat the best solution is depends on the use case, and I don't think ioredis docs should focus on that. Instead briefly explain the problem and leave it up to consumers to solve it.. > I'm good with removing the storage of keys from the example entirely, simply logging from the .on('data') handler\nSounds good! That's also closer to the intention of streams. ",
    "dguo": "Sounds good. What's your opinion on giving the user flexibility for commands that have multiple keys? So something like:\njs\nvar redis = new Redis({keyPrefix: ['app1:', 'app2:', 'app3:']});\nredis.blpop('key1', 'key2', 'key3', 0);  // BLPOP app1:key1 app2:key2 app3:key3 0\nOr maybe even allow the user to dynamically set the prefix?\njs\nvar redis = new Redis({keyPrefix: 'foo:'});\nredis.blpop({keyPrefix: ['app1:', 'app2:', 'app3:']}, 'key1', 'key2', 'key3', 0);\nI'm not sure how useful this would be.\n. Ok, that's reasonable. I'll start working on it.\n. Okay, I made the two changes.\n. :smile: Awesome! Thanks for the quick responses.\n. \u4f60\u7684\u82f1\u6587\u6bd4\u6211\u7684\u4e2d\u6587\u597d\u591a\u4e86! :smiley:\n. @luin, I don't think that would work because the tranformation needs to be saved in this.args.\nHow about doing what @AVVS suggested in this way:\njavascript\n// at the top\nif (typeof transform !== 'function') {\n  transform = function (key) {\n    return key;\n  };\n}\nAnd then I can remove all the checks and always apply the transform.\n. Good catch. Thanks.\n. ",
    "Delagen": "scanstream match option does not use this prefix\n. [DEBUG] status[redis.domain.ru:6379]: connecting -> close +9ms\n[DEBUG] reconnect in 36ms +2ms\n[DEBUG] status[redis.domain.ru:6379]: close -> reconnecting +1ms\n[DEBUG] error: Error: connect EADDRNOTAVAIL 127.0.0.1 - Local (0.0\n.0.0:55277) +5ms\n[ERROR] Error: connect EADDRNOTAVAIL 127.0.0.1\n- Local (0.0.0.0:55277)\n    at internalConnect (net.js:932:16)\n    at defaultTriggerAsyncIdScope (internal/async_hooks.js:294:19)\n    at GetAddrInfoReqWrap.emitLookup [as callback] (net.js:1081:9)\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:61:10)\n. This is due \nfunction isIIpcConnectionOptions(value) {\n    return value.hasOwnProperty('path');\n}\nbecause I supply property path with null value\nFixed with remove null|undefined options before connect.\nSo it may be closed\nBut manual look like\nLocal domain socket path. If set the port, host and family will be ignored.. ",
    "skreutzberger": "Thanks, your code and explanation made it clearer to me!\nI investigated it further using your code and it seems that the connection object just contains masterNodes after 1-2 milliseconds - it is retrieved from Redis right? With your code I could reduce the waiting period from 2 seconds to 2 milliseconds.\nSo for now the solution is to wait some milliseconds (in my case just 2ms which is fine for unit tests) before flushing a cluster. But that is actually just necessary if you directly flush the cluster after connecting to it (which is unrealistic since some business logic is run in between which should give the connection enough time to get the master nodes).\nHere is my new function which flushes all master nodes of a cluster if they are existing:\n``` js\nfunction flushRedisClusterAsync(cluster) {\n    log.debug(\"flushing Redis Cluster ...\");\nvar masterNodes = Object.keys(cluster.masterNodes);\nif (masterNodes.length > 0) {\n    //log.debug(\"Found cluster nodes\", Object.keys(cluster.nodes));\n    log.debug(\"Found cluster master nodes\", masterNodes);\n\n    return Promise.each(masterNodes, function(key) {\n        log.debug(\"flushing cluster master node %s ...\", key);\n        var node = cluster.masterNodes[key];\n        return node.flushall()\n        .then(function(val) {\n            if(val === \"OK\") {\n                log.debug(\"flushed master node\", key);\n            } else {\n                log.error(\"Could not flush master node %s. Returned val:\", key, val);\n            }\n        })\n    });\n} else {\n    log.error(\"Could not flush cluster, no master nodes were found.\n\nPlease wait some milliseconds after connecting for master node detection!\");\n        return Promise.reject(false);\n    }\n}\n```\nI call that function in my afterEach Mocha function with:\nreturn Promise.delay(2)\n.then(function() {\n    return flushRedisClusterAsync(c);\n});\nI tried it now a lot with a lot of data and it work always 100%. Maybe I could remove the waiting period if I would wait for the \"ready\" event but actually I don\u00b4t know how to add an .on event to a Mocha beforeEach function with a Promise.\nAnyway, I am happy that I can now reliably flush a cluster :)\nThanks for your great library!\nSebastian\n. ",
    "0neSe7en": "\u77e5\u9053\u5982\u4f55\u8fd4\u56deScore\u4e86...\n. ",
    "kamilwylegala": "Thanks for that. I'm gonna check it for sure.\n. ",
    "johnnychq": "it helps, 3q. ",
    "andreikun": "Ahh crap. that's my bad. I also have an subscriber.on(\"pmessage\") ... where i do JSON.parse but i forgot that it will enter that closure when i publish from somewhere else. I expected the result in my Lumen installation. Sorry about that and thanks for the tip.\n. ",
    "ehacke": "Seems to work fine with pipeline enabled.\nWork-around that works for me\nI only disabled pipeline because I don't know ahead of time the commands I'll be sending in the multi block. So I can't chain them.\nFor my purposes, rather than disabling pipeline, I can just build an array of commands and submit them as a batch to the multi constructor.\n``` javascript\nredis.defineCommand('echoDynamicKeyNumber', {\n      lua: 'return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}'\n    });\nvar commands = [];\ncommands.push(['echoDynamicKeyNumber', 2, 'k1', 'k2', 'a1', 'a2']);\nif (somethingExists) {\n  commands.push(['conditionalCommand', 'arg1', 'arg2']);\n}\n...\nredis.multi(commands).exec(function(){..});\n```\n. @luin thanks! That'll work for me.\nYou can close this if you'd like.\n. ",
    "reneeichhorn": "Is there a way to check if there is currently one open/valid connection?\n. ah ok, thanks for your help. :+1: \nSo as this is the wanted behavior I will close this issue \n. ",
    "FrankFang": "\u771f\u662f\u6ce8\u91cd\u7ec6\u8282\u3002\n. ",
    "albin3": "\u5f53\u65f6\u6d4b\u4e0b\u6765\u662f\u6bd4\u8f83\u4e0d\u7a33\u5b9a\uff0c\u800cnode_redis\u662f\u572850k\u5230100k\u6d6e\u52a8\uff0c\u53ef\u80fd\u662f\u5f53\u65f6\u8d77\u7684\u8fdb\u7a0b\u592a\u591a\u4e86\uff0cCPU\u6ca1\u8c03\u5ea6\u8fc7\u6765\u3002\n\u521a\u624d\u53c8\u505a\u4e86\u4e2a\u6d4b\u8bd5\u2026\u6ca1\u51fa\u73b0\u7c7b\u4f3c\u7684\u95ee\u9898\u3002\n\u6d4b\u8bd5\u65b9\u6cd5\u548c\u7ed3\u679c\u5728\uff1a\nhttps://github.com/albin3/pubSubTest\n. ok~\n. \u611f\u89c9\u8fd9\u91cccallback\u7684\u4e00\u822c\u7406\u89e3\u662fexpire\u8bbe\u7f6e\u6210\u529f\u4e86\u6ca1\u6709\u2026\n. redis\u6709\u4e2a\u952e\u5230\u671f\u63d0\u9192\u2026\u8fd9\u4e2a?\uff0c\u9700\u8981\u81ea\u5df1\u65b0\u5efa\u4e00\u4e2aredis\u8ba2\u9605\u8fde\u63a5\uff0c\u5355\u8fde\u63a5\u597d\u50cf\u5b9e\u73b0\u4e0d\u4e86\uff0c\u53ef\u4ee5\u73a9\u4e00\u4e0b\u3002\u3002\u4e0d\u77e5\u9053\u8ddf\u5176\u4ed6\u5b9a\u65f6\u4efb\u52a1\u6bd4\u600e\u4e48\u6837\u2026\n. \u5927\u795e\uff01\ud83d\udc4d\n. ",
    "suprememoocow": "Wow, that was quick. Thanks so much for your help. \n. ",
    "fisherwj": "\u6211\u7528redis\u4f5cnodejs\u7684session\uff0c\u4f46\u662fredis\u7684server\u7aef\u4f1a\u9488\u5bf9\u95f2\u7f6e\u7684\u8fde\u63a5\u8bbe\u7f6e\u4e86\u8d85\u65f6\u65ad\u5f00\u3002 \u867d\u7136ioredis\u6709\u91cd\u8bd5\u673a\u5236\uff0c\u4f46\u662f\u5728\u91cd\u8bd5\u8fc7\u7a0b\u4e2d\u9047\u5230\u6709\u7528\u6237\u8bf7\u6c42\u65f6\uff0c\u662f\u4e0d\u662f\u4f1a\u8bfb\u4e0d\u5230session\u5462\uff1f\u6211\u770bioredis\u6ca1\u6709\u8fde\u63a5\u6c60\uff0c\u5982\u679c\u9047\u5230\u8fd9\u79cd\u60c5\u51b5\u4e00\u822c\u662f\u600e\u4e48\u89e3\u51b3\u7684\u5462\uff1f\n. ",
    "vieks": "There is absolutely any interest having connections pooling per process (application) since redis server  is single threaded and is processing sequentially all the client queries.\n. ",
    "majintao": "\u4e5f\u5c31\u662f\u8bf4\uff0cioRedis \u7684 Cluster \u6027\u80fd\uff0c\u5355Client \u8fde\u63a5\u5c31\u884c\u4e86\uff1f    \u4e0d\u5efa\u8bae\u7528\u5176\u4ed6\u7684\uff0c\u6bd4\u5982\uff1a\nhttps://github.com/coopernurse/node-pool\n. \u786e\u5b9e\u6211\u4e5f\u8bd5\u8fc7\u7528\u8fd9\u4e2ahttps://github.com/coopernurse/node-pool\uff0c  \u8fde\u63a5\u6570\u4e00\u76f4\u4e0b\u4e0d\u6765\uff01 \n. Can i user this for thread-pool?\nhttps://github.com/coopernurse/node-pool\n. ",
    "guyisra": "So no connection pooling?. ",
    "jasonkuhrt": "Ok I see, thanks.\n. ",
    "fracmak": "Thank you for merging so quickly, any idea when you'll package it up?\n. ",
    "tiefenb": "can you give me an code example? i didnt found this functions in the\ndocumentation\nAm Freitag, 14. August 2015 schrieb Vitaly Aminev :\n\nit does\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/luin/ioredis/issues/130#issuecomment-131086901.\n. thanks!\n. \n",
    "vipreshjha": "redis.expire command takes the time in Seconds or Milliseconds?\nWould be nice if this was also documented somewhere.. ",
    "DarkSorrow": "Would it be bad you think to have an option that would let it log to a master while no slave is available but still try to relog to a slave using the sentinelRetryStrategy to connect to the slave once its available?\nBecause in case the slave are unavailable application that were just reading from slave won't be able to work anymore. Maybe have an option like 'enforceRole = false'?\n. ",
    "bluekylin": "\u8bf7\u95eetransaction\u7684\u56de\u8c03\u51fd\u6570\u53ef\u4ee5\u8fd4\u56de\u547d\u4ee4\u672c\u8eab\u7684\u7ed3\u679c\u5417\uff1f\u56e0\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd4\u56deQUEUED\u6ca1\u6709\u4f7f\u7528\u4ef7\u503c\uff0c\u66f4\u591a\u7684\u5e94\u7528\u662f\u9700\u8981\u547d\u4ee4\u7684\u7ed3\u679c\uff0c\u5426\u5219\u6211\u4eec\u9700\u8981\u5728exec\u7684\u56de\u8c03\u51fd\u6570\u91cc\u53bb\u64cd\u4f5cresults\u6570\u7ec4\uff0c\u800c\u6b64\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u4e0d\u77e5\u9053transaction\u7684\u987a\u5e8f\uff0c\u4e3a\u6b64\u6211\u4eec\u9700\u8981\u7ef4\u62a4\u989d\u5916\u4e00\u4e2a\u6570\u7ec4\u6765\u77e5\u9053\u662f\u54ea\u4e9btransaction\u3002\u662f\u5426transaction\u548cpipeline\u80fd\u884c\u4e3a\u4e00\u81f4\uff1f\u662f\u5426\u53ef\u4ee5\u5728multi\u7684exec\u7684\u56de\u8c03\u4e4b\u524d\uff0c\u4f9d\u6b21\u56de\u8c03\u6bcf\u4e2atransaction\u5bf9\u5e94\u7684\u56de\u8c03\u51fd\u6570\uff1f\n. \u597d\u7684\uff0c\u975e\u5e38\u611f\u8c22\u60a8\u7684\u89e3\u7b54\uff01\n. ",
    "kunth": "\u6211\u7684\u6d4b\u8bd5\u662f\u5728redis 2.8.11\u4e0a\uff0csentinel\u53ef\u4ee5\u505afailover\u3002\u5927\u6982\u6d4b\u8fc7\u767e\u6765\u4e2acase\u6ca1\u51fa\u73b0\u95ee\u9898\u3002\u4f46\u662f\u5728\u505afailover\u7684\u65f6\u5019\uff0csentinel\u5c06slave\u63d0\u5347\u4e3amaster\u7684\u65f6\u95f4\u6bd4\u8f83\u957f\uff0c\u5927\u6982\u572831s+\uff08\u4ece\u539fmaster\u6302\u6389\u5230\u539fslave\u7684\u72b6\u6001\u53d8\u4e3amaster\u7684\u65f6\u95f4\uff09\u3002\n. Hi luin, I have the same question, I already set my ioredis version to 1.14.0 or 1.11.1, but neither does it work out. \nMy sentinel(redis) version is 2.8.19, the code as follows:\nvar Redis = require('ioredis');\nvar opts = {\n  name: 'rnode_53',\n  sentinels: [\n    //{host: 'any.org', port: 9600},\n    //{host: 'any.org', port: 9601},\n    {host: 'any.org', port: 9602}\n  ],\n  //retryStrategy: function(times) {\n  //  var delay = 2000;\n      // var delay = Math.min(times * 2, 2000);\n  //  return delay;\n  //},\n  sentinelRetryStrategy : function(times) {\n    //var delay = Math.min(times * 2, 1000);\n    var delay = 1000;\n    return delay;\n  }\n}\nvar redis = new Redis(opts);\nredis.on('error', function(err){\n  console.log('catch a error: ' + err);\n});\nI can get error from retryStrategy(if sentinel runs well and related nodes down), but not sentinelRetryStrategy(if sentinel down).\nAny suggestions?\n. yeah, I noticed that. I killed all sentinels when I try this. \n. @luin , sentinelRetryStrategy never invoked in my case. \n I had erased sensitive info(shown as any.org) and my own log info\n the debug info:\nioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[any.org:9602]: [empty] -> connecting +2ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,rnode_53) +2ms\n  ioredis:connection error: Error: connect ECONNREFUSED +1s\n  ioredis:redis status[any.org:9602]: connecting -> close +1ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[any.org:9602]: close -> end +0ms\n  ioredis:SentinelConnector failed to connect to sentinel any.org:9602 because Error: Connection is closed. +1ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 1000 +0ms\n  ioredis:redis status[any.org:9602]: [empty] -> connecting +1s\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,rnode_53) +1ms\n  ioredis:connection error: Error: connect ECONNREFUSED +1s\n  ioredis:redis status[any.org:9602]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[any.org:9602]: close -> end +0ms\n  ioredis:SentinelConnector failed to connect to sentinel any.org:9602 because Error: Connection is closed. +0ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 1000 +0ms\n  ioredis:redis status[any.org:9602]: [empty] -> connecting +1s\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,rnode_53) +0ms\n. @luin , yeah I manually set the delay to 1000 in sentinelRetryStrategy function.\nAnd I can see the log in sentinelRetryStrategy function, but why the \"error event\" never triggered?\n. Well, so there is a difference between retryStrategy and sentinelRetryStrategy. \nAs to retryStrategy, no matter what value is set to delay, the redis.on('error') wolud be triggered. However in  sentinelRetryStrategy, only set delay to non-number value would emit error .\nIs that the way?\n. Well, a little weird. Thanks very much, @luin , you save me lots of time.\n. Hi luin, redis.disconnect(true) may not work for me since\n1. As I disable the INPUT and OUTPUT packets of redis-server's port(using iptables), redis.disconnect(true) would make origin tcp connection FIN_WAIT_1 state, the connection is not closed, neither nor the new connection created.\n2. What's more, I do not want to lose any pending commands in ioredis client.\n. I had made a roughly modification which meets my own requirements(after detecting that master is down, get a new connection from sentinel, not losing any commands at the same time), but I do not kown whether it is good enough.\n. Well, I do not look deep into the code, previously what I want to do in Connector#reconnect is to replace Connector#connect. So does the Redis#reconnect. But my experiments turn out that the procedures is as follows:\n(1)The app detects that current ioredis client() cannot connect to redis\n    \u2193\n(2)client.reconnect();  // client is an obj created like var client = new Redis{sentinels: \"xxx\" , name: \"yyy\"}\n    \u2193\n(3)Redis#reconnect() // use original connector obj as a param\n   \u2193\n(4)orginal stream is copied\n   \u2193\n(5)orginal stream is destroyed, and connectionCloseHandler being called.\n  \u2193\n(6) a new Redis#Redis() object is created, and Redis#connect() beging called.\n  \u2193\n(7) All right. using netstat command we can see a new connection to current master(original slave) being established and all redis commands being executed correctly.\nThe 6th procedure may be executed many times as sentinels have not done the failover. Anyway the result turn out right after the failover.\nI am confused about why a new Redis object would be created, what I intend to do is same Redis object with a new connector(with the same stream which guarantees no losing commands ).\nI would take a good look at the related codes in later days.\n. I add two functions in this commit. I have made some test and not found any errors as far as I am concerned.\nLook forward to hearing your advice.\n. Oh yeah only stream.destroy() works.\nYou are awesome.\n. ",
    "kulicuu": "hi ( & thanks for responding to this semi-ridiculous thread )\n\nWhat's your purpose to run this module on the browser side?\n\nRight so, I do 'fullstack' design+build of complex social-media systems (more complex featurewise than Twitter or Facebook).  Realtime interaction+collaboration between multiple user-nodes is the general essential characteristic of these systems.  \nI'm looking at re-engineering some systems I built in the past (Bartr/Britvic).  Moving from ugly mess to pristine nirvana is the goal.  \nMy stack for Bartr/Britvic consisted of SocketStream (a NodeJS WebSocket server with some additional frameworky stuff), Redis, and AngularJS on the front end.  Web-app clients are in touch with SocketStream server via WS; this handles all the logic, data layer entirely Redis. \nInitially the server-side was pretty MVC flavoured, but without the direct view computations; sending just data to Angular to compute. \nI realised long ago that I could eliminate most (IO) transactions to Redis by factoring out the server Model logic into Lua on Redis.\nRecently I realised that if I had a direct connect from the browser to Redis I could cut out the middleman (WS server) entirely, provided static assets and initial application load service could be passed to e.g. a CDN.\nProbably OT, but included for context: \nmy ideas for ideal web-app are totally changed from last year.  Then it was Angular, Jade templates, Browserify.  Did some stuff with Grunt and Gulp, then early this year went to a React + Webpack build with inline CSS and markup (all CoffeeScript, no JSX); then I dropped HTML/CSS entirely and developed an all-SVG / React approach to client build, which got great results, but now I'm convinced that the future of webapp graphics presentation (talking strictly 2D) has to be WebGL (because of the performance benefits of assigning all render computation to GPU; (if there is a shim to send React.render comps to GPU that's something to look into) ).  The rise of many-core systems means we need to have a concurrency /parallel-comp strategy for the browser; hoping advanced exploit of WebWorker capabilities will be adequate.  WebRTC capabilities are too good to pass up (my ideal network topology is composition of P2P stuff and a centralised signals network (probably based on Redis/Disque)).  Want to take advantage of cognitive bandwidth of audio channel so WebAudio.  ...\nSo, the client stack is looking like Flux (no React, just Flux), WebGL, WebAudio(lots of processing via WebRTC uptake of mic channel), WebRTC for P2P, some kind of connection to a Redis/Disque grid, and that's it. Except WebWorkers.  If I can't siphon all of this stuff out to various WebWorkers, the single thread will choke on everything it's going to be attending to.\n. I never was able to realise this idea technically; I'm still not sure if it's feasible, I've never heard anyone in the Redis community mention it, though I'm not keeping up to date totally with everything going on there.\nCurrently I'm researching architecture for a commercial realtime marketplace/social system, and while I was originally going with a two-layered PostgreSQL & Redis approach, I'm looking now into RethinkDB, which would reduce complexity and time to develop.  Since RethinkDB push notifies the connected server, the realtime angle is well-covered.. ",
    "dkrieger": "@kulicuu did you end up pursuing this approach? I am looking to do the same, as I'd rather not use a websocket server when 1) I'd need to duplicate logic in redis to scale out and 2) redis is more adaptable, compatible, and performant. The only solution I've found is to use websockify as a WS -> TCP proxy, but I worry about its performance and scalability.\nEspecially with the addition of streams to redis, it seems like the way to go compared with a more conceptually-heavy and rigid solution like RabbitMQ w/ webstomp plugin. ",
    "hcburger": "I just used exactly what you suggested, and it works wonderfully. It's a really good solution to my problem.\nMay I suggest adding this to your Quick Start? \n. ",
    "AdriVanHoudt": "having the same issue here where it just does nothing when it can't connect to the redis and I use callbacks so the timeout won't work for me :disappointed: \n. @luin thanks for looking into this!. Thanks a lot! @luin . ",
    "LeDominik": "I'm using the promise-style and sadly it doesn't run into the catch() part if the connection is not possible (I simple quit redis). Working with bluebird's timeout helps, but this doesn't seem to be the right strategy as I cannot report the real reason...\n. @luin Well I did it the easy way. Same machine, OS X, switch tab, CTRL+C :smile:\n. @luin so I basically need to modifiy the retry-policy to ensure that it gives up in a reasonable time like 1sec :smile:\n. Awesome, thanks @luin I went for 500msec\njavascript\n  retryStrategy: function (times) {\n    if(times < 2) {\n      console.warn('Redis: Connection retry # %d ', times);\n      return 500;\n    }\n    return \"\";\n  }\nBut this got me thinking: Basically it should keep retrying in the background to recover gracefully when redis / network is back up. Now once the connection is closed it's done and I can basically restart my little express app. It would be great to have a real per-operation timeout that would go into catch(err), tell me the real reason (no connection) and allow me to return a HTTP 500. Then the consumer of the API can choose whether to retry...\n. ",
    "bojanbass": "Yes, every time. Do you need any other code to check the problem?\n. Oh, you helped me to solve my problem! The problem is in how do I set up redis config. I have a file called redis.js, which has this content:\n``` javascript\nvar Redis = require('ioredis')\nvar redis = new Redis({\n  port: 6379,          // Redis port\n  host: config.redisDs,   // Redis host\n  db: 5\n});\nmodule.exports = redis;\nmodule.exports.connect = function () {\n  //var redisConfig = url.parse(config.REDISURL);\n  //var client = redis.createClient(redisConfig.port, redisConfig.hostname);\n  //\n  //if (redisConfig.auth !== null)\n  //    client.auth(redisConfig.auth.split(':')[1]);\n  //\n  //client.on('error', function(e){\n  //    console.log(e);\n  //});\n  return redis;\n};\n```\nand for redis init I only include this file in my main app like this:\njavascript\nrequire('./redis');\nSo when I added console.log(self.connect.toString()), the content returned was the one from my custom connect function, so the default one was overriden. Removing my module.exports.connect function soved the problem.\nThank you for your help!\n. Just another minor question: in my case when redis server is down, I'm trying to catch errors when connection fails, so I added this code inside my redis.js module:\njavascript\nredis.on('error', function (e) {\n  console.log(e);\n});\nThe strange thing is that the number of error events executions is huge, somewhere around 250 in 1 minute. Is there anything wrong with reconnection logic or is this expected behaviour?\nThanks!\n. javascript\nError: connect ECONNREFUSED\n    at errnoException (net.js:905:11)\n    at Object.afterConnect [as oncomplete] (net.js:896:19)\n. Ok, I found out what's going on with retry logic. If I set a custom retry function like this:\njavascript\n  retryStrategy: function (times) {\n    return 1000;\n  }\nthen retry works as expected, so 1 retry each second. But if I leave the defaults:\njavascript\nretryStrategy: function (times) {\n    var delay = Math.min(times * 2, 2000);\n    return delay;\n  }\nthen the delay calculated is a very small number, starting 2, 4, 6, 8 and so on. But this represents miliseconds, that's why it will try to reconnect multiple times per second. I think the default function is missing * 1000 multiplier for the delay variable, so reconnection intervals would then be 2, 4, 6, 8 seconds, which makes more sense to me. Or am I wrong?\n. Ok, thank you for the explanation and quick response!\n. ",
    "vigneshnrfs": "How do I use the above with same score? Something like\nredis.zadd('key', score, [member1, member2, member3])\n. Thanks. That works.\n. ",
    "InstanceOfMichael": ":+1: \n. ",
    "vestly": "I am using new Redis to connect to the primary endpoint for the cluster, which always resolves to the master. However, a failover event does not trigger a disconnect of the ioredis client, so my client (an api server that is always running) continues to stay connected to the new slave and throws errors on writes. I am looking for a way for the client to recognize the error and reconnect to the new master.\n. Yes, I think that would be a great solution, especially if it includes offline queuing for minimal data loss.\n. This works nicely in my tests with ElastiCache failover. Thank you very much! \n. ",
    "amit777": "This error seems to come from the native redis module, not ioredis. Sorry for the false alarm.\n. ",
    "jsmartfo": "I think this was resolved in the newer version of ioredis - I had 1.3.6 I needed 1.8.0 \n. ",
    "bradennapier": "I think the transform argument makes more sense.  Custom commands having transformers seems like the #1 need for the transformers to have a public api in the first place.  \nCould also simply have an instance Command property\n```js\nimport Redis from 'ioredis';\nconst redis = new Redis();\nredis.Command....\n```\nThe workaround is kind of ugly and hacky feeling.\nEspecially when you consider handling of replies since we may have promise, pipeline, callback, etc etc situations.  Not even sure if that would be possible to do.. This does not appear to work with multi?  My custom command is returning a promise and executing things.\njs\nconst cmd = redis\n    .multi()\n    .del('hash1')\n    .hmset('hash1', { field1: 'value1', isTrue: 1 })\n    .hsetifget(\n      'hash1',\n      { field1: 'value1', isTrue: 1 },\n      { field2: 'value2', field3: 'value3' },\n    )\n    .then(results => {\n      console.log('Results? ', results);\n    });\nIf I add .exec then I get an error since the hsetifget always returns a promise.\nOk figured that out -- nevermind.  Was how I was transforming the custom command results that caused it.. So I was able to do this in a fairly hacky manner.  Not really happy with how it had to be done.  Ended up building a utility to make this simpler at ioredis-utils.  \nSet it up so the comments in the top control transforms if provided.  It strips the comments after parsing it. \n```lua \n--[[\n  Summary:\n    Checks if the hash with key KEYS[1] matches the KEYS.  If it does,\n    then it sets the values on the hash given in ARGV.\nSo \"hash1\", { field1: 'value1', field2: 'value2' }, { field3: 'value3' }\nif hash with key \"hash1\"\n has field \"field1\" with value \"value1\" AND\n has field \"field2\" with value \"value2\"\nTHEN\n  set \"hash1\" \"field3\" \"value3\"\n\nReturns the modified hash\n]]\n--| name:    hsetifget\n--| dynamic: true\n--| keys:    key ifMatchesThis thenSetThese\n--[[args => {\n  const keys = [];\n  let nkeys = 0;\n  if (args.length === 3) {\n    keys.push(args[0]);\n    Object.keys(args[1]).reduce((p, key) => {\n      p.push(key, args[1][key]);\n      return p;\n    }, keys);\n    nkeys = keys.length;\n    Object.keys(args[2]).reduce((p, key) => {\n      p.push(key, args[2][key]);\n      return p;\n    }, keys);\n  }\n  return [nkeys, ...keys];\n}]]\n--[[result => {\n  if (!Array.isArray(result)) return result;\n  const response = {}\n  for (let i = 0; i < result.length / 2; i += 1) {\n    const idx = i * 2\n    response[ result[idx] ] = result[idx + 1];\n  }\n  return response;\n}]]\n-- local KEYS = {\"hash1\", \"field1\", \"value1\", \"field2\", \"value2\", \"field3\", \"value3\"}\nlocal HashKey = KEYS[1]\ntable.remove(KEYS, 1)\nif #KEYS % 2 ~= 0 or #ARGV %2 ~= 0 then\n  return redis.error_reply(\"Keys and args Must be a set of key/value pairs\")\nend\nlocal CheckKeys = {}\nlocal CheckTable = {}\nfor i=1,#KEYS/2 do\n  local k = KEYS[i * 2 - 1]\n  local v = KEYS[i * 2]\n  table.insert(CheckKeys, k)\n  CheckTable[k] = v\nend\nlocal HashArray = redis.call(\"HMGET\", HashKey, unpack(CheckKeys))\nfor i=1,#HashArray do\n  local k = CheckKeys[i]\n  local v = HashArray[i]\n  if CheckTable[k] ~= v then\n    return nil\n  end\nend\n-- All values match, we can get the ARGV now\nlocal result = redis.call(\"HMSET\", HashKey, unpack(ARGV))\nif result[\"ok\"] then\n  return redis.call(\"HGETALL\", HashKey)\nend\nreturn nil\n```\n\nDefinitely does provide nice speed benefits to use lua though! :-)\n```\n-- END EXAMPLE ONE --\n { field1: 'value1',\n  isTrue: '1',\n  field2: 'value2',\n  field3: 'value3' }\nExample One Duration:  2.6271560192108154\n-- END EXAMPLE TWO --\n { field1: 'value1',\n  isTrue: '1',\n  field2: 'value2',\n  field3: 'value3' }\nExample Two Duration:  0.818884015083313\n```\n. ",
    "ColmHally": "Thanks @luin - I appreciated the quick response!\n. Yes, that's a better idea, but this.stream.address() will not work (see here - address() returns the bound address).\nthis.stream.remoteAddress and this.stream.remotePort would better suit our intentions here.\n. @luin made some changes that use remoteAddress etc. Would you mind taking a look?\n. ",
    "Adrien-P": "If anyone has managed to setup ioredis with a cluster on another machine, I'd be happy to know your configuration\n. Hi @luin \nthanks a lot for your answer. bind is definitely the option I was looking for.\nFor those interested I added this line to my redis.conf files for each node:\nbind 10.0.0.6 127.0.0.1\nnow it works well.\nOn a side note, we just moved to ioredis from node_redis mostly because of the support of Redis Cluster. The migration has been very easy\n. Hi @luin , thanks for the answer.\nActually both options make sense. For the moment I'll create a readOnly client as you suggested for my read operations. I assumed the second option was the one already in place, this is why I got mislead. I think this second option where you make a smart choice between Master/Slave depending if it's write/read would make things easier as you'd have only one redis client to use for every operation.\n. Yes I agree the readOnly option is not the best name for this. Because eventually, you can make write operations with this option too. I guess that's directly inspired from the READONLY Redis command.\nscaleReads is closer to what we want indeed. I think that's a good name as long as there is a bit of text around it to explain what it does exactly.\nPerhaps another option could be:\nreadFrom that could take the values: master or slave or all\nPersonnaly, I'd be interested in being able to pick only Slave nodes for Read operations (and never the Master node)\n. ",
    "pensierinmusica": "Thanks @luin! Would it make sense to add this example to README? If you like I can create a PR  :)\n. Done! #160 \n:)\n. Sure! Done  :)\n. Thanks! Cheers  :)\n. It makes sense.. is there a way to make the two mutually exclusive? For example, if we're using promises ignore the \"error\" event, since it should be handled by the rejection anyways.. Yep, that I thought of, thx!. ",
    "wesleymilan": "I'm receiving an error trying to set key with expires\nredis.set('key', 100, 'ex', 10);\nUnhandled rejection ReplyError: ERR wrong number of arguments for 'set' command\n    at ReplyParser._parseResult (/project/node_modules/ioredis/lib/parsers/javascript.js:39:14)\n    at ReplyParser.execute (/project/node_modules/ioredis/lib/parsers/javascript.js:139:20)\n    at Socket. (/project/node_modules/ioredis/lib/redis/event_handler.js:94:22)\n    at Socket.emit (events.js:107:17)\n    at readableAddChunk (_stream_readable.js:163:16)\n    at Socket.Readable.push (_stream_readable.js:126:10)\n    at TCP.onread (net.js:540:20)\n. ",
    "aalexgabi": "@luin  Please add this to the API documentation.\n. TL;DR Method signatures should be documented.\nDisclaimer: Maybe I didn't see the part of documentation that talks about this when I \"scanned\" it. I didn't read the entire page. I mainly used mouse scroll and Ctrl + F (searched for \"set\", \"ex\", \"command\", \"commands\").\n@luin @dirkbonhomme I would like to see explicit documentation on the existence of the set method on Redis prototype. Also I would like to see explicit documentation on the arguments each command can take even if that means just writing something like ioredis.set(String arg0,String  arg1,String  arg2...) where arg<x> is the positional argument documented in the Redis API documentation for your version available here: <link>\nThere is no reference of the set method (or any other Redis command) here: https://github.com/luin/ioredis/blob/master/API.md#new_Redis I knew the set method existed from the README.\nIt wasn't obvious to me what arguments the set command can take: I was wondering if it would be like ioredis.set(key, value, px, nx, xx) but then what about ex or ioredis.set(key, value, {px: new Date().getTime()}) but then where do I find the options names in the documentation. Then I searched online and I stumbled upon this issue where @luin gave an example. It would be nice to have that example visible somewhere in the documentation.\nThis would also reduce probability of issues like #159.\n. I just saw the commit in #160. that added:\n+// All arguments are passed directly to the redis server:\n+redis.set('key', 100, 'EX', 10);\nI didn't see that in README.\n. @luin I agree with you on having generic documentation for all versions of Redis. I suggest adding something like:\n- instance\n  - .<redis command>(<first command argument>, <second command argument>...) \u21d2 any \nIn the table of contents and as a section in the API documentation.\n. @luin I agree that Redis cluster does not work the same way as a single node and developers need to be aware of this.\nHaving two classes makes it obvious that something is different but not exactly what is different.\nThe redis-cli provides a -c option for cluster mode. I'm thinking of something similar: adding a flag clusterMode: true or followRedirects: true.\nThis would have allowed me to do something like this\njs\nredis = new Redis.Cluster([\n    {\n        port: program.port,\n        host: program.hostname,\n        cluster: program.cluster,\n    },\n]);. ",
    "magicdawn": "+1 on document for the instance methods signature\n. ",
    "arankhem": "@luin , Just wanted to know is there a way to expire hash similar to set. For example\nlet sample = {\n'field1': 1,\n'field2': 2,\n'field3': 3,\n}\nredis.hmset('test', sample, 'EX', 1000). @dirkbonhomme ok thank you.. ",
    "i5ting": "it is a common way with no readability, is it a better way for resolve this problem?\n. \u7ec8\u4e8e\u903c\u51fa\u4e2d\u6587\u4e86\uff0c\u54c7\u54c8\u54c8\uff0c\nexpire\u7528\u6cd5\u57fa\u672c\u5c312\u79cd\u5427\n- \u7f13\u5b58\uff0c\u9650\u5236xx\u79d2\uff0c\u7136\u540e\u5224\u65ad\u662f\u5426\u5b58\u5728\u5373\u53ef\n- \u5ef6\u65f6\u4efb\u52a1,\u6bd4\u598210s\u540e\u5e72\u70b9\u5565\uff0c\u548cKeyspace Notifications\u7c7b\u4f3c\n. \u8bed\u4e49\u4e0a\uff0cexpire - n\u79d2 - \u5230\u671f\u4e4b\u540e\u56de\u8c03\u5904\u7406\uff0c\u8fd9\u6837\u662f\u5408\u7406\u7684\u5427?\n. ",
    "965283058": "ioreis API\u63d0\u4f9b expire\u8fd9\u4e2a\u65b9\u6cd5\u5417\uff1f. ",
    "sunheartGH": "\u540c\u95ee\uff0credis\u5220\u9664\u63d0\u9192\u7684api\u6709\u4e48\uff1f\u67d0\u4e9b\u952e\u8fc7\u671f\u540e\u60f3\u5220\u9664\u4e00\u4e9b\u5173\u8054\u7684\u6570\u636e\u9700\u8981\u7528\u5230\u8fd9\u4e2a\u7279\u6027. ",
    "ccs018": "The anonymous callback function is executed when the SET command has been completed, not when the key is EXPIRED.  I don't believe that there is any notification available to clients when a key has been expired.. I certainly hope it's not dead.  It's the only library that supports Redis\nClusters.  node-redis doesn't support that yet.\nThough it would be nice if the various questions and issues that have been\nraised in the past 2-3 months can be addressed.  I wish I could submit PRs\nto address the issues I've run into, but I simply don't have the time to\ndelve into a library I have no familiarity with.  if it's not going to be\nmaintained, then it would be good to let the community know.  I understand\nthat there was a plan to merge ioredis & node-redis, but that doesn't\nappear to have had any activity either.\nOn Tue, Mar 20, 2018 at 4:54 PM, Shahar Mor notifications@github.com\nwrote:\n\nIt's not dead at all. If you have any contributions to make we will be\nglad to review them.\nRegarding dropping node < 4, I agree that it's a good move and we should\ndefinitely move ahead in that direction.\n@luin\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=wCT6Ly5D5OLHkl4Sb1cpdZlfijci_2OKI5Lul2KYJsk&s=kbUmzdvNIswTrwwbgWQrNvRvfmjd9UCpgYj2w2e_lBE&e=,\nlet's talk and see how we can move this forward.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_408-23issuecomment-2D374770404&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=wCT6Ly5D5OLHkl4Sb1cpdZlfijci_2OKI5Lul2KYJsk&s=Um1AasMuObdw-9f6B__vs7CpRpk2Nh3mWPCja44BidU&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa79EvY7dZoBG7Enu5HvByCSKr5bxks5tgXqdgaJpZM4LCu7A&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=wCT6Ly5D5OLHkl4Sb1cpdZlfijci_2OKI5Lul2KYJsk&s=zOqV5-5yA29mYatuFxiljOsXFqpd1LrJIyHVmrU4TzA&e=\n.\n. Interesting.  I ran your test code.  The only difference was I supplied a password in the Redis constructor.  I got:\n\nroot@redis:/usr/src/app# node multiTest.js\n[ [ 'key1', 'data1' ],\n  [ 'key2', 1, 'data2' ],\n  [ 'key3', 1, 'data3' ],\n  [ 'key4', 1, 'data4' ] ]\nThe other difference in my production code is that redisCmds was defined using let, not const as the array of commands is built dynamically using redisCmds.push().\nDid you run this with 3.1.1?  I'm also using node v6.11.1.\nUpdate: Reconfigured Redis to not use a password and re-ran test.  Same results.. I dug into the ioredis code.  It appears that the issue is in lib/pipline.js -> addBatch().  Specifically at line 185.  While this might appear to operate on a local variable, JS uses a mix of pass by reference and pass by value.  At line 184, command isn't a copy of comands[i], but rather a reference to commands[i].  The shift at line 185 actually affects the passed in array (commands).  I believe this issue could be resolved by changing line 184 to:\nvar command = commands[i].slice();\ncommand will then actually be a copy.\n\nThe shift() method removes the first element from an array and returns that element. This method changes the length of the array.\nThe slice() method returns a shallow copy of a portion of an array into a new array object selected from begin to end (end not included). The original array will not be modified.. Resolved with v3.1.2.. It appears that this PR isn't needed.  There is another commit on (June 8th) for #480 which also appears to resolve the issue and would probably explain why @luin wasn't able to reproduce it:  I'm using the released 3.1.1 and @luin was likely running off of the latest code.  3.1.1 does not include the commit for #480.\n\nIf that's the case, then this PR can be rejected.\n3.1.1 code:\n```\nPipeline.prototype.addBatch = function (commands) {\n  for (var i = 0; i < commands.length; ++i) {\n    var command = commands[i];\n    var commandName = command.shift();\n    this[commandName].apply(this, command);\n  }\nreturn this;\n};\n```\nLatest version:\n```\nPipeline.prototype.addBatch = function (commands) {\n  var command, commandName, args;\n  for (var i = 0; i < commands.length; ++i) {\n    command = commands[i];\n    commandName = command[0];\n    args = command.slice(1);\n    this[commandName].apply(this, args);\n  }\nreturn this;\n};\n```. Thanks for the updated release!. I having a similar issue, but I'm getting a different failure - the client never connects to the cluster.  See issue #581.  Maybe it's a dupe, but these need investigation.. Keep alive.. Keep alive. This issue still needs to be resolved.. Keep alive. This issue still needs to be resolved.. freshbot. Can it be clarified how these two options interact or don't interact?. It's too bad that the documentation can't be clarified.. This is clearly a defect.  I'd try to root cause it and submit a PR, but I'm not familiar enough with the code and unfortunately don't have the time to figure it out.  While the obviously work around is to not set that option, it is not at all clear what the implications are as the both exists at both the cluster level and the individual server level.  At a minimum, the documentation needs to be updated to clarify the behavior of these options for a Redis Cluster.. Commenting to keep the issue open.. @luin Any chance this will be addressed?. unstale. @shaharmor , thanks for taking a look at this.\n\nthe redisOptions.offlineQueue is not relevant for cluster mode\n\nThanks for clarifying.  The documentation is not clear which redis node options are applicable in cluster mode.  In my use case, queuing commands when offline would likely cause issues as I can have multiple clients attempting to update the same keys and if one client queued an update while its connection was down and another client was able to make another update, then when the first client reconnected it would incorrectly clobber the other client's update (the updates would be made out-of-order).\n\nwhen the redis node will go offline it will also be removed from the cluster nodes list\n\nI believe that this is incorrect behavior.  This can typically be a temporary event and ioredis should not be unilaterally removing the node from the cluster nodes list.  It should query the cluster for the topology since when the node recovers ioredis is not proactively adding the node back to the cluster nodes list.\nConsider a rolling upgrade scenario.  As each node is taken down to be upgraded, ioredis is removing the node from the cluster nodes list.  Eventually, there will be none left.  Restarting clients is not an acceptable recovery when one is trying to design a zero-outage system - which includes upgrades.. Just saw the PR - Looks like this will fix a couple similar issues.  Though I still don't believe that when a node goes offline that it should be removed from the cluster nodes list.  Just mark it as offline and periodically attempt to reconnect.. Again, thanks for digging into this.  I have some rather ugly code that periodically performs a CLUSTER NODES and compares the count with cluster.nodes and if the count is different I destroy and recreate the ioredis client.  Not the cleanest or more efficient.  Similarly, to work around the issue captured here where the client never connects - I simply have a timeout to guard against this condition and again destroy and recreate the ioredis client.  Looking forward to ripping that code out and completely rely on ioredis to maintain and automatically recover connections.. I've ended up doing that in my client app:  periodically do CLUSTER SLOTS\nand compare the number of nodes with #nodes().  If #nodes < CLUSTER SLOTS\nnodes, then I do a cluster.disconnect(true) which appears to work.  Though\nthis doesn't handle some odd scenario where a slave was moved from one\nmaster to another.  And it has the downside of temporarily making redis\nunavailable to the app.  It would be better if ioredis could handle this\nwithout causing the app to disconnect from the cluster.\nWhat I noticed is if I killed a node, ioredis would shortly after\ncompletely remove that node from it's list (#nodes().length would\ndecrement).  So even when the node recovers and rejoins the cluster,\nioredis doesn't reconnect to that node (unless a MOVED or ASK error is\nreceived which normally would not happen in a simple node failure /\nrecovery scenario).\nChris\nOn Fri, Feb 16, 2018 at 8:23 PM, Zihua \u5b50\u9a85 notifications@github.com wrote:\n\nioredis doesn't fetch all the master list periodically from the servers,\nonly when the MOVED or ASK error is received. That makes the result of\nnodes() not up to date though.\nWe can refresh the master list in the background periodically to solve the\nproblem. I'll look into that.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_585-23issuecomment-2D366408724&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=SDcwXnE76OG1Ewjg-JrcIoIkHggCrF1UimijVp6bthc&s=JKgkQtfivsppG7w6BnZTbdEAHdwFYB0pGwUypW9CguA&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa1oe30yAbTE79WUE-2Dz-2DQONs5qZdIks5tVjgmgaJpZM4SB4SF&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=SDcwXnE76OG1Ewjg-JrcIoIkHggCrF1UimijVp6bthc&s=HK3Eqr3GFzTDM8zkqeRlwOt6-Yy1BzWfzwllmgFpq3M&e=\n.\n. keep open ping.. Hi fresh bot again here. We need this!. @luin can you please re-open this issue?. Ping.  It would be good to fix this issue.  I've written some rather kludgy code in my client to periodically (~3 seconds) check if connected and if not destroy that ioredis Cluster instance and create a new one, but it isn't the cleanest solution at all.  The library really needs to handle this scenario.. Commenting to keep this issue open.. @luin Any chance this will be addressed?. unstale. Keep alive.  This issue needs to be resolved.. Keep alive. This issues needs to be resolved.. Keep alive. This issue still needs to be resolved.. freshbot. @shaharmor , thanks for the response.  I could try a slot change, but I believe this also is not immediately detected, but only if an operation results in a MOVED response.\n\nNote that I've raised a couple other related issues regarding how ioredis manages cluster connections.  In this particular issue, there is no real change in the cluster topology.  Rather one of the nodes failed and ioredis decides to never attempt to reconnect to that node.  If you query any node in cluster, that temporarily failed node was never removed from the cluster, yet ioredis decides that it is gone forever.  In this scenario, ioredis should not unilaterally remove the node from its view of the cluster, but should continue to retry re-connecting to that node.  As I noted above error may be related to the following debug output:\nioredis:connection skip reconnecting because `retryStrategy` is not a function +0m\nyet in my configuration of ioredis, retryStrategy IS a function.  I suspect that there is an error in handling the options passed to Cluster().. I could, but it's gotten rather complex as I've had to work around the various issues I've logged and no longer have the original code.  Between this issue and not handling the exception case of trying to connect to the cluster before the cluster is actually formed (startup race condition and the ioredis client hangs) and not proactively handling actual changes to the cluster topology, I've written a lot of logic to detect and recover from these conditions.\nI'll try to find some time to write a sample client, but it should be rather easy to reproduce.  Just set up a cluster, connect a client and then stop one of the nodes.  Wait a minute and then restart the node.  You'll see that ioredis does not automatically reconnect to that node.\nstartupNodes = [{host:127.0.0.1, port:6400},{host:127.0.0.1, port:6410},{host:127.0.0.1, port:6420}]\nOther nodes in the cluster are 127.0.0.1 ports 6401, 6411 and 6421 and discovered dynamically.\nNon-default options:\nenableOfflineQueue: false\nscaleReads: true\nredisOptions.connectTimeout: 2000\nredisOptions.autoResendUnfulfilledCommands: false\nredisOptions.reconnectOnError = function(err) {\n    let targetError = 'READONLY';\n    if (err.message.slice(0, targetError.length) === targetError) {\n        // Only reconnect when the error starts with \"READONLY\".\n        return 2;\n    }\n};\nredisOptions.readOnly: true\nI don't recall if I was initially using the builtin for clusterRetryStrategy and/or redisOptions.retryStrategy but you can look at #586 for a related issue.. MOVED is only applicable if slots were moved from one shard to another.  There are many scenarios where there's an issue with the current implementation where there is never a MOVED error.\nSeveral scenarios come to mind:\n1) read-only slaves.  If I want to scale my read performance and enable read operations on slave nodes, then if a slave node restarts (or more like the machine the slave node is on restarts), the slave node is removed by ioredis and never reconnects thus impacting my read performance.\n2) high availability: A slave node restarts and ioredis removes it from the cluster list.  At some point later, the master goes down.  The slave node which previously restart is elected master.  The cluster slot configuration hasn't changed, so no MOVED error will be received.  The client will never be able to access that shard.\n3) rolling upgrades.  A slave is taken down to be upgraded.  ioredis removes that node from the list.  That slave is restarted.  Next the master is taken down to be upgraded.  The previously upgraded slave is elected as master.  This is basically the same as item 2 above.. So I think the discussion has gotten off topic of the original issue.  I did a bit more digging and found that in cluster mode, if a node disconnects (e.g., the redis server restarts), that there is explicitly no attempt to reconnect to that node - even if the client specified a retryStrategy() in options.redisOptions.  Specifically in connection_pool.js:\nredis = new Redis(_.defaults({\n      // Never try to reconnect when a node is lose,\n      // instead, waiting for a `MOVED` error and\n      // fetch the slots again.\n      retryStrategy: null,\nI believe this is wrong.  The connection could be lost due do a simple node restart (machine restart, redis server upgrade, etc).  In these scenarios, there would never be a MOVED error and so we never reconnect to this node.\nFurther, the code overrides the client specified options and forces offline queuing to be enabled.\n// Offline queue should be enabled so that\n      // we don't need to wait for the `ready` event\n      // before sending commands to the node.\n      enableOfflineQueue: true,\nIt's not clear why this needs to overridden.  In my use cases, because I am using redis as a true cache, I have special handling when operations to update the cache can't complete.  I may have other clients also writing to the cache and if those commands start to get queued up and then later dequeued and executed, those commands could get executed out-of-order and the cache has the wrong data.\nThe fact that these two options are being overridden is not documented.  I'd like to see these overrides removed.. The comment about enableOfflineQueue is more of a secondary issue.  The primary problem is that retryStrategy is being set to null so that ioredis will not attempt to reconnect to that node when that node simply restarts.  That needs to change.  Assuming that there will eventually be a MOVED error is an invalid assumption.. This is a Redis API question, not ioredis.  The Redis API does not inform you which key or keys exists.  It reports how many of the specified keys exist.  See the Redis API documentation:  https://redis.io/commands/exists.  If you want to know which specific keys exists or not, then you need to check each key one-by-one.\nThough I don't understand why you would want to first check if a key exists before querying.  If they key doesn't exist, then the response to the query will be null.  Why make it a 2-step process when the query would be sufficient?. Then use the KEYS command.  https://redis.io/commands/keys\n\"keys *\" will return all of the keys in the database.\nYou can also try the SCAN command. https://redis.io/commands/keys. Probably a good idea to add an event handler and wait for cluster to emit\n'ready' before trying to use the cluster.  Waiting an arbitrary amount of\ntime may wait most of the time, but waiting until the client reports that\nit is connected to the cluster and that the cluster is ready to received\ncommands would be much more cleaner.\nOn Mon, Apr 23, 2018 at 6:31 AM, Kelvin Hu notifications@github.com wrote:\n\nI also meet this problem, my original code is almost the same as\n@JoeNyland\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_JoeNyland&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=VCduhTGKuHv--KC8VF-13L6lAgwTPLMuUA03k9Sj3Eo&s=5Inpdg1nCYNvrJ1Z7C8DZPrNGZtUPl5YLs7Ogi_ZPEs&e=\nsetInterval(function() {\n    var cluster = new Redis.Cluster(...);\n    cluster.nodes('all').forEach(function(node) {\n        node.bgsave(...);\n    });\n}, timeInterval);\nHowever, I tried something like this:\nsetInterval(function() {\n    var cluster = new redis.Cluster(...);\n// delay the command for 3 seconds\nsetTimeout(function() {\n    cluster.nodes('all').forEach(function(node) {\n        node.bgsave(...);\n    });\n}, 3 * 1000);\n\n}, timeInterval);\nThen the code never fails, so I guess there might be some initialization\njobs inside new Redis.Cluster(...), and it requires some time to\ncomplete, so immediate command execution might fail (not quite sure, just\nmy guess).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_597-23issuecomment-2D383543860&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=VCduhTGKuHv--KC8VF-13L6lAgwTPLMuUA03k9Sj3Eo&s=f4dSpU2NWE7sal6B_ZVsyLWRnhEVGEcu57jVKNBNNmM&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa7YXiwFSHZ8ucQOq-5FIWYxKx-5FD7FVks5trbupgaJpZM4SkXZf&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=VCduhTGKuHv--KC8VF-13L6lAgwTPLMuUA03k9Sj3Eo&s=j0jWfi7UGL935PaXFMZCc5IDndX5Mg-SFXLNv0YQeOw&e=\n.\n. Given that Node.js 0.10.16 is no longer a release supported by nodejs.org,\nwhy continue to support it in the library?  0.10.x when end-of-life in\n2016.  Sure, this would be a breaking change - uprev the major version and\ncall it a day.  Developers really should stay current with their\ndependencies and that includes using a supported version of Node.js if not\nthe latest version.\n\nhttps://github.com/nodejs/Release#release-schedule\nChris\nOn Thu, Mar 15, 2018 at 10:29 AM, Zihua \u5b50\u9a85 notifications@github.com wrote:\n\nPull requests for this change are welcome! However, it seems that\nBuffer.from() is not supported until Node.js 6.x, which will be a\nbreaking change if we replace new Buffer() with it since we provides\nsupport for Node.js 0.10.16. Can we check if the Buffer.from() exists,\nand if not, fallback to new Buffer()?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_599-23issuecomment-2D373417189&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=lK2s75cQzS_vmBjA4KV5QNAVPp5l14DPcXOq2Fb3dEY&s=u4QedOdtkmlG8rA1YGSJp0ccxNGAaJWPH7igx4jxQTI&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa1-2DMG-2DtykEH52Xwmsf8lsDEzotC2ks5teojygaJpZM4Snxgn&d=DwMFaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=lK2s75cQzS_vmBjA4KV5QNAVPp5l14DPcXOq2Fb3dEY&s=ZPJ6DxAIuXigeDtB6SxS0RS6ULLM-oiNOoZLRYc_zcY&e=\n.\n. Are you sure you're really using a Redis Cluster?  First, per the Redis\ndocumentation (https://redis.io/topics/cluster-tutorial), a Redis Cluster\nis required to have a minimum of 3 shards:\n\n\"Note that the minimal cluster that works as expected requires to contain\nat least three master nodes.\"\nSecond, per the Redis Cluster specifications, data is sharded based on a\nspecific hash function on the keys being used.  A Redis Cluster client does\nnot decide on its own to which shard to direct traffic to - it is\ndetermined by the keys being used.\nI highly recommend you read up on the documentation describing how a Redis\nCluster works.  What you're suggesting ioredis do completely violates that\nspecification.\nOn Thu, Mar 22, 2018 at 4:06 PM, Elliott Foster notifications@github.com\nwrote:\n\nI've noticed while load testing the ioredis cluster functionality with a\ntwo shard, three replica cluster that a significantly higher portion of\ntraffic is being directed at one master vs the other. Some of this may have\nto do with the traffic shape, but regardless I think it'd be a good\nimprovement to allow round robin node selection rather than random node\nselection as random will most likely not be evenly distributed among the\ncluster.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_608&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=YvzheuYPeIUdDU9rNOTwcml6rZ9cIodOQwja7ZCvoGo&s=kWaYvWVRvXGaHZH60q22uaMrxuOqtKMSOLvljiGSLnY&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa9ZX9HxztO-5Fq8R86I1l41DeRCBg1ks5thBJPgaJpZM4S3y10&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=YvzheuYPeIUdDU9rNOTwcml6rZ9cIodOQwja7ZCvoGo&s=a2GnoJwMm16BZFWRY5aI-h_u1y6e5hFvpGFzQZFhN_Y&e=\n.\n. Using JSON.stringify() and JSON.parse() is exactly how I'm using ioredis/redis and when I get the value and pass it through JSON.parse() I get an object.  JSON.parse() returns an object and does not modify the original input.\n\norigValue = { a: 1, b: 2 };\nkey = 'abc';\nredis.set(key, JSON.stingify(origValue));\nvalue = redis.get(key);\nparsedValue = JSON.parse(value);\nvalue is still a string.  parsedValue equals origValue.. Any update???. So you're suspecting that this is really a node issue and not an ioredis issue?  Would probably to be good to log an issue there as well.  Try to get some of those folks to re-review that commit.  This issue is going to prevent folks using ioredis to upgrade to Node 10.\nThough perhaps there was some breaking change that ioredis needs to handle.. Great investigation work!. I think part of the problem is that the RESP (Redis Protocol) is really basic and most of the commands are blocking.  I don't believe that there is any kind of sequence/transaction/request ID used.  So if the client timed out and issued another command, then the response to the previous command could first be received.  I'm not sure how that would be handled - probably not well if there is no transaction ID.  It might be safer that if a time out occurs to simply terminate the connection and reconnect.\nIt would be cleaner if ioredis set a timeout on every command issued and if the timeout expired, return an error (if callback used) or reject (if promise used), terminate the connection and automatically attempt to reconnect.. This only works if natRemap contains the entire set of nodes in the cluster.  In general, you don't need to specify all of the nodes in the nodeList argument when you do a new Redis.Cluster() and ioredis will dynamically learn the rest of the nodes in the cluster.  This solution essentially requires the client to be fully configured with all of the nodes.\nFurther, this solution does not allow for the cluster to dynamically change.\n1) The size of the cluster can not increase unless natRemap is originally set to include all possible nodes that may exist in the future.\n2) The NAT mapping is static.  If a node is ever restarted with a different IP address, the natRemap will need to be updated and the client restarted.\nFor rather static deployments, this appears to work, but the above limitations should be highlighted as this won't work in all deployments.. > Having two classes makes it obvious that something is different but not exactly what is different.\nIf that's all the developer is aware of (that there are two classes), then they're mostly likely going to use Redis incorrectly.  As @luin points out, if a developer doesn't know the differences between a standalone redis instance and redis cluster, then the developer is likely going to create problems for themselves.  In particular, multi-key operations (such as mget or multi/exec operations) can only be executed if all of the keys in those operations map to the same hash slot. This is described at https://github.com/luin/ioredis#transaction-and-pipeline-in-cluster-mode and https://redis.io/topics/cluster-tutorial#migrating-to-redis-cluster. An application may work just fine when using a standalone instance, but if you try to transparently replace that standalone instance with a cluster - and not examine the application design and use of redis - then developers will have no idea why certain operations start to fail.  You generally can't simply \"flip a switch\" and move to using a redis cluster without making sure your application is using it correctly - this goes beyond simply switching which class you use to instantiate the client.\nAny developer wanting to use redis needs to know if they are using a standalone redis instance or a redis cluster and familiar with the redis documentation that describes the differences between them.  If they're relying simply on which class is being used to instantiate the client, it's not going to go well.  At a minimum they really need to read through the redis cluster tutorial:  https://redis.io/topics/cluster-tutorial.\nIn addition, when connecting to a redis cluster, it is a good idea to list more than one node when instantiating the client.  While the client can auto-discover all of the nodes in the cluster, if the one node you've configured your client with happens to down for whatever reason (upgrade, h/w fault, etc), then your application won't be able to connect and you've a complete outage.  If you specify multiple nodes (not necessarily all, but say 3), then the chances of not being able to connect are generally drastically reduced.  So again, the application code needs to know if there is one node (standalone) or multiple nodes (cluster).. While you've got\nredis.on('error',function (err) { ... });\nThere should also be\nredis.on('node error',function (err) { ... });\nHere's all of the events that I'm catching:\nredisCluster.on('+node', function (data) { ... });\n    redisCluster.on('-node', function (data) { ... });\n    redisCluster.on('connect', function () { ... });\n    redisCluster.on('ready', function () { ... });\n    redisCluster.on('error', function (err) { ... });\n    redisCluster.on('node error', function (err) { ... });\n    redisCluster.on('close', function () { ... });\n    redisCluster.on('reconnecting', function (delay) { ... });\n    redisCluster.on('end', function () { ... });\nSee https://github.com/luin/ioredis#events.. @vweevers While a little old, see https://github.com/nodejs/node-v0.x-archive/issues/6194.  The main take away is that node.js doesn't provide sufficient configurability of the TCP keepalive functionality.  Thus we'd have to wait up 10-11 minutes before detecting that the connection has closed.  Searching through the node 10.x documentation, I didn't find anything that allows further configuring TCP keep alives to detect a broken connection any faster.. It's not sure much about detecting dead connections fast.  It's I issue a redis command (e.g., get), but if the connection dies before the response is received nothing is reported back to the client - the callback is never invoked.. Good point. Was that change merged in?  I didn't see it mentioned in the release notes for the 4.0 RCs.. No, no objections.  I'll need to test out the RC.. Are you asking how to configure the ioredis client or how to configure\nredis itself?\nOn Fri, Aug 24, 2018 at 1:46 AM Runrioter Wung notifications@github.com\nwrote:\n\n\nclient-side sharding across your nodes\nnative Redis clustering\n\nWhat are the configuration of the above two case? How difference.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_687&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=e13Hnu2YMNEuKmdvfVj5i_MDMpbZa85ak66ChZedmbU&s=RGj-ivUibCnDFF5d73J-m0VmXx96TJZrZVD9cE41v0w&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa48Yrl2-5FeHeFHYYOox0WgLnomfLKks5uT6FGgaJpZM4WK1cc&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=e13Hnu2YMNEuKmdvfVj5i_MDMpbZa85ak66ChZedmbU&s=Ctn9sFDXd0EC9rg975JX0N6hYL9l3ff0vj87THkxIGU&e=\n.\n. These are not two different options.  Redis Clustering uses client-side\nsharding.  There is lots of documentation available describing how Redis\nClustering works and how to configure Redis itself to operate in cluster\nmode.  See:\n\nhttps://redis.io/topics/cluster-tutorial\nhttps://redis.io/topics/cluster-spec\nOn Fri, Aug 24, 2018 at 11:22 PM Runrioter Wung notifications@github.com\nwrote:\n\nBoth.\n\nIf I use client-side sharding across my nodes, how to configure\n   Redis itself?\nIf I use a node with native Redis master/slave clustering, how to\n   configure Redis itself?\n\nWhat is the difference?\nThanks @ccs018\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_ccs018&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=RR67lGaSp80zBIjjK_gXcuOTQ_QTuV2sea1-NbOnt8E&s=Is3Ij5jkhhUqmkRth2msh2dOXWVsTIye7V_B92j7hE4&e=\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_luin_ioredis_issues_687-23issuecomment-2D415934388&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=RR67lGaSp80zBIjjK_gXcuOTQ_QTuV2sea1-NbOnt8E&s=Ssr9JdGsS9nx39tYfAK05ANK0VEcywARn4Raermo5zs&e=,\nor mute the thread\nhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AVxAa3c9b9RS4tIpjITNsxjX1eoSSbXKks5uUND1gaJpZM4WK1cc&d=DwMCaQ&c=q3cDpHe1hF8lXU5EFjNM_A&r=N1Msa0ZexCqrdtWLgqlbMZlNrFKF4YmyP5otZCNJ7mmERCt9HPx5t646YC5_jvy8&m=RR67lGaSp80zBIjjK_gXcuOTQ_QTuV2sea1-NbOnt8E&s=Msy6ZpKEdovPCXHCW9Q6F8DnoRZ_RuZQGkSte3z0VrU&e=\n.\n. Is emitting an error event sufficient?  In my usage, I'm always supplying a callback in the form of:\nredis.get(key, function (err, reply) {\n    if (err) {\n        // handle error\n    } else {\n        // handle data\n    }\n});\nWhile I do also handle the error event, that handling is somewhat generic.  It would be better if the callback was called with the error so that specific error handling could be invoked.. I assume that this will trigger an automatic reconnect to that server, correct?. Good, but see my follow-up to #587.  If this node was part of a cluster, retryStrategy is overridden and set to null so there would in fact be no automatic reconnect to that node.. It appears that the minimum practical value for timeoutPerCommand is 500ms.  Even if I specify timeoutPerCommand = 100, the check for a timed out command occurs every 500ms and so the command time out error won't trigger as quickly as I would expect.. Since self.lastWriteTime is updated every time a command is added to the commandQueue, if the client attempts commands fast enough (more than one every timeoutPerCommand ms), then self.lastWriteTime continues to get updated such that Date.now() - self.lastWriteTime will continue to be less than timeoutPerCommand and thus the original command will never time out.\n\nPerhaps in Redis.prototype.sendCommand lastWriteTime is only updated if commandQueue.length === 1.. This timeout guards against a socket which has no activity.  The value to use probably greatly depends on the use cases.  In a production environment, there's probably enough traffic to keep such a timeout from triggering.  But in a dev environment, there might not be any traffic for relatively long stretches of time making this an idle connection which then times out.  Setting a per command timeout seems better.\nAlso, if this socket timeout occurs, it doesn't enable specific error handling / recovery for the specific commands which timed out.\nOr were you suggesting this as an additional check and not an alternative solution?. Per #587, there will not be any automatic reconnect in a cluster environment.. ",
    "naugtur": "So for the workaround to work:\nenableReadyCheck: false in options is required, then:\n``` js\nredisInstance.on(\"connect\", function () {\n    redisInstance.auth(\"pawsword\", function (err, res) {\n        if (err) {\n            console.error(err);\n            redisInstance.disconnect()\n            process.exit();\n    }\n\n})\n\n});\n```\nBut I'd appreciate a way to catch the initial built-in auth failure. \njs\nredisInstance.on(\"error\",function(authError){})\n^would be the way to go imho\n. Well, I'm on this camp that prefers when errors have to be handled ;) but ok.\nIf I can suggest something I'd not put a space in the event name for the sake of people coming from jQuery where it means 2 events ;)\n. That doesn't look like a change that'd make all tests fail... \n. Now that the user is in control documenting the auth error event should be enough. \nFor an app that doesn't care about error handling ignoring this seems like the way to go ;)\n. :+1: \n. could it be auth-error or authError or authfailed ?\n. ",
    "dandanknight": "Hi luin, thanks for taking the time to read and reply. The branch I'm testing against is found at https://github.com/Automattic/kue/tree/b53e4751e1c86a1006380eb07ae1c378865dc7ea\n. Ahhh, ok. Seems so obvious now you explain it like that. Thank you so much for your help. I'll add a handler to the Kue error today and let you know how I get on. Thanks again. \n. Hi again, changes made, and works perfectly now. It is successfully failover to the new master after the timeout set in the config, and now, I also have a load of nice events telling me the connection status, unlike the previous module I used! Thanks for your time.\n. ",
    "MrMMorris": "oh well haven't you thought of everything.\nThanks! and sorry for not RTFM\n. ",
    "nichliu": "For customize lua function, how can we know read or write?, maybe we need another option when we define the lua function? \n. Thx, That make sense, I can use it as internal servers without password.\nBut I still think add password support for cluster will more secure, even you can require all Redis cluster nodes to use same password. \nThank you for your help.\n. works very good for me.\nThank you\n.   ioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +3ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +2ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +0ms\n  ioredis:cluster getting slot cache from xxx.xxx.xxx.xxx:6379 +1ms\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +85ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +8ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +128ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +23ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +43ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +1ms\n  ioredis:cluster status: connecting -> connect +1ms\n  ioredis:cluster status: connect -> ready +0ms\nioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: ready -> close +76ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:cluster getting slot cache from xxx.xxx.xxx.xxx:6379 +2s\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n  ioredis:cluster getting slot cache from xxx.xxx.xxx.xxx:6379 +1s\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> close +411ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: close -> end +0ms\n  ioredis:cluster status: ready -> close +0ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:cluster getting slot cache from 10.240.0.6:6379 +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +102ms\n  ioredis:cluster status: reconnecting -> connecting +1ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +0ms\n  ioredis:cluster getting slot cache from xxx.xxx.xxx.xxx:6379 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +79ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +6ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connecting -> connect +3ms\n  ioredis:redis write command[0] -> auth(aaaaaaaaaa) +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +158ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +29ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: connect -> ready +4ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +65ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +2ms\n  ioredis:redis status[xxx.xxx.xxx.xxx:6379]: [empty] -> connecting +1ms\n  ioredis:cluster status: connecting -> connect +1ms\n  ioredis:cluster status: connect -> ready +0ms\n  ioredis:cluster send 1 commands in offline queue +0ms\n  ioredis:redis queue command[0] -> get(test) +0ms\n. I can run redis-cli -c with remote cluster on the same host without any problem\n. { [ReplyError: MOVED 6918 10.x.x.x:6379]\n  name: 'ReplyError',\n  message: 'MOVED 6918 10.x.x.x:6379',\n  command: { name: 'get', args: [ 'test' ] } }\nI think what happen might be I have 2 ips for each redis node, 1 is public, another is internal, when move command to another node, it might use internal ip can't be access by outside\n. ``` javascript\nvar Redis = require('ioredis');\nvar r1 = \"x.x.x.x\";\nvar r1s = \"x.x.x.x\";\nvar r2 = \"x.x.x.x\";\nvar r2s = \"x.x.x.x\";\nvar r3 = \"x.x.x.x\";\nvar r3s = \"x.x.x.x\";\nvar r4 = \"x.x.x.x\";\nvar r4s = \"x.x.x.x\";\nvar redisNodes= [{\n    port: 6379,\n    host: r1\n},{\n    port: 6379,\n    host: r2\n},{\n    port: 6379,\n    host: r3\n},{\n    port:6379,\n    host:r4\n},{\n    port:6379,\n    host:r4s,\n    readOnly: true\n},{\n    port:6379,\n    host:r1s,\n    readOnly: true\n},{\n    port:6379,\n    host:r2s,\n    readOnly: true\n},{\n    port:6379,\n    host:r3s,\n    readOnly: true\n}];\nvar redis = new Redis.Cluster(redisNodes);\nredis.defineCommand('calc', {\n    numberOfKeys: 1,\n    lua: '.....lua code'\n});\nredis.calc(key,start,end,screen,function(err,data){\n    #process result or err\n})\n```\n. Thx. I got it.\n. I got message\n{ [ReplyError: ERR unknown command 'sentinel']\n  name: 'ReplyError',\n  message: 'ERR unknown command \\'sentinel\\'',\n  command: { name: 'sentinel', args: [ 'slaves', 'r1' ] } }\n. Thx, working good\n. ",
    "jeffjen": "Well it looks like v4.1.2 had the tls connection emit \"connect\" as well as \"secureConnect\"\nFor the node build from https://deb.nodesource.com/setup, which had version v0.10.40, it doesn't emit \"connect\", only emits \"secureConnect\".\nConsider this fix for backwards compatibility then.\n. Pushed a change to do tertiary assignment to CONNECT_EVENT.\nI know its not recommended to connect via TLS to REDIS since the support of it is minimal (need SSL proxy to do it), but hopefully this will payout in the long run.\n. Oh yes, I will submit a new change without isClosed.  I was thinking whether it needed a way to indicate it is draining.  Not looking at node source code on Readable, I don't think it will end the stream outright when there are data in buffer, right?\n. ",
    "JSteunou": "Ho thank you for that quick response, exactly what I needed!\n. after removing my package-lock.json and install from scratch all node modules instead of trusting npm on update for update and install package@version for rollback it works again... This is sad. ",
    "bkw": "Ah, thanks for clearing that up.\n. Is that also the case for the other scan commands like hscan?\nI mean, they do have a key as an argument. Couldn't that work? And please excuse my ignorance about cluster internals.\n. ",
    "geogeim": "tested with ioredis 3.1.4 and scanStream function is missing from Redis.Cluster. ",
    "AndyGura": "+1 on this. There is still no scanStream function in Redis.Cluster. However you can use cluster.subscriber.scanStream to scan on currently connected slot.. ",
    "xpepermint": "I fixed it. Because I'm accessing the remote Redis cluster I have to specify the public IP addresse inside the /etc/redis/sentinel.conf.\nsentinel monitor mymaster XX.XX.XX.XX 6379 2\n. ",
    "ekousp": "\u6ca1\u6709\u6539\u5584\u3002\n\u6211\u67e5\u770b\u4e86\u6e90\u7801\uff0c\u5c06 https://github.com/luin/ioredis/blob/master/lib%2Fredis.js#L270 \u4fee\u6539\u4e3a\uff1a\njavascript\nstream.on('data', function() {\n  console.log('data', Date.now(), arguments[0].length);\n  eventHandler.dataHandler(_this).apply(null, arguments);\n});\n\u8f93\u51fa\u5982\u4e0b\uff1a\n.. (\u7701\u7565\u90e8\u5206)\ndata 1445240539379 65536\ndata 1445240539388 65536\ndata 1445240539398 65536\ndata 1445240539409 65536\ndata 1445240539466 65536\ndata 1445240539473 65536\ndata 1445240539482 65536\ndata 1445240539490 65536\ndata 1445240539498 65536\ndata 1445240539511 65536\ndata 1445240539520 65536\ndata 1445240539527 65536\ndata 1445240539541 65536\ndata 1445240539549 65536\ndata 1445240539559 65536\ndata 1445240539569 65536\ndata 1445240539577 65536\ndata 1445240539590 65536\ndata 1445240539598 65536\ndata 1445240539610 65536\ndata 1445240539668 65536\ndata 1445240539676 65536\ndata 1445240539688 65536\ndata 1445240539697 65536\ndata 1445240539710 65536\ndata 1445240539720 65536\ndata 1445240539728 65536\ndata 1445240539742 65536\ndata 1445240539751 65536\ndata 1445240539763 65536\ndata 1445240539773 65536\ndata 1445240539784 65536\ndata 1445240539796 65536\ndata 1445240539809 65536\ndata 1445240539818 65536\ndata 1445240539882 65536\ndata 1445240539892 65536\ndata 1445240539901 65536\ndata 1445240539910 65536\ndata 1445240539922 65536\ndata 1445240539932 65536\ndata 1445240539942 65536\ndata 1445240539956 65536\ndata 1445240539969 65536\ndata 1445240539982 65536\ndata 1445240539995 65536\ndata 1445240540005 65536\ndata 1445240540019 65536\ndata 1445240540030 65536\ndata 1445240540095 65536\ndata 1445240540106 65536\ndata 1445240540116 65536\ndata 1445240540126 65536\ndata 1445240540141 65536\ndata 1445240540157 65536\ndata 1445240540167 65536\ndata 1445240540177 65536\ndata 1445240540190 65536\ndata 1445240540206 65536\ndata 1445240540216 65536\ndata 1445240540226 65536\ndata 1445240540243 65536\ndata 1445240540326 65536\ndata 1445240540336 65536\ndata 1445240540346 65536\ndata 1445240540357 65536\ndata 1445240540371 65536\ndata 1445240540384 65536\ndata 1445240540398 65536\ndata 1445240540409 14169\n\u6211\u5bf9 NodeJS \u548c socket \u7f16\u7a0b\u4e86\u89e3\u4e0d\u591a\uff0c\u731c\u6d4b\u662f\u4e0d\u662f data \u4e8b\u4ef6\u7684 chunk size \u4e3a 65536\uff0c\u5bfc\u81f4\u6bcf\u6b21 data \u4e8b\u4ef6\u53ea\u80fd\u5904\u7406 65536 bytes\uff0c\u800c\u4e8b\u4ef6\u5faa\u73af\u6bcf\u6b21\u9700\u8981 10 ~ 15 ms\uff0c\u6240\u4ee5\u5bfc\u81f4\u5904\u7406\u65f6\u95f4\u5ef6\u957f\uff1f\n. \u90a3\u80fd\u5426\u63d0\u9ad8 chunksize\uff1f\u6211\u641c\u7d22\u4e86\u4e00\u4e0b\uff0c\u6ca1\u6709\u627e\u5230\u4fee\u6539\u8fd9\u4e00\u8bbe\u7f6e\u7684\u65b9\u6cd5\u3002\n\u53e6\u5916\uff0c\u6bcf\u6b21\u4e8b\u4ef6\u5faa\u73af\u9700\u8981 10 ~ 15ms\uff0c\u8fd9\u4e2a\u6b63\u5e38\u5417\uff1f\u76ee\u524d\u60a8\u5bf9\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u6709\u4ec0\u4e48\u5efa\u8bae\u5417\uff1f\n. \u597d\u7684\uff0c\u8c22\u8c22\uff01\n. ```\n\nnode redis.js\nend 95\n```\n\n@BridgeAR @luin huge improvement. thanks!!!\n\u6240\u4ee5\u4e3b\u8981\u539f\u56e0\u662f parser \u7684\u6027\u80fd\u95ee\u9898\uff0c\u800c\u4e0d\u662f chunk size\u3002\n. ",
    "pangguoming": "sorry, \u662f\u6211\u7528\u7684Redis\u67e5\u770b\u5de5\u5177\u4e71\u7801\u3002 \n@luin  \u80fd\u7ed9\u6211\u4f60\u7684QQ\u5417\uff1f \u6211\u4ececnodejs.org \u4e0a\u95ee\u8fc7\u4f60\n. \u54e6\uff0c\u8c22\u8c22\n. ok\uff0cgot it.\n. @luin \n\u6ca1\u6709\u770b\u660e\u767d ${} \u662f\u4ec0\u4e48\u610f\u601d\uff0c\u600e\u4e48\u628a\u53d8\u91cf\u5d4c\u5165\u5230\u90a3\u4e2a\u5b57\u7b26\u4e32\u4e2d\u7684\uff1f \nvar city=\"\u5317\u4eac\";\nconsole.log('${city}.......');\n\u8fd9\u6837\u597d\u50cf\u4e0d\u884c\uff0c \u521d\u5b66\u8005\u8bf7\u6307\u6559\n. thanks.\n. ",
    "senarukana": "Then do you have any ideas that which nodejs client that supports this feature?\n. ",
    "dmfay": "Rather than make the publisher itself available across our app we've kept it inside a single module which exports a function publish (channel, message, callback) that just passes what it gets directly to ioredis (we were formerly using ortoo/node-redis-sentinel which didn't have a problem, but it's deprecated). So if our publish function is invoked with two arguments, it passes undefined on to ioredis.\nIt's easy enough to work around it by checking the callback we're passed and invoking Redis.publish with two or three arguments depending, but it is a little annoying.\n. Yeah, I tried making that change in generateFunction myself and it didn't go too well. Your example of redis.set is the sort of thing where I'd expect to have to pass an empty string explicitly, although Redis is pretty different from the relational databases I'm more accustomed to working with.\nI think it might be possible to support both cases by augmenting the definitions in commands.js: if a command is defined as taking a callback, then in situations where the number of arguments passed matches the already-defined arity but the last argument is undefined it should be safe to pop that one.\n. ah, that's too bad. At least working around it isn't too painful.\n. ",
    "rrfeng": "\u5f53\u65f6\u7528 codis \u7684\u65f6\u5019 qps \u4e0d\u9ad8\uff0c\u4f46\u662f\u4e32\u6570\u636e\u7684\u95ee\u9898\u5341\u5206\u4e25\u91cd\u3002\u6362\u6389 codis \u4e4b\u540e\u6ca1\u518d\u51fa\u73b0\uff0c\u76f4\u5230\u6628\u5929\u63a8\u5e7f\u5bfc\u81f4\u6d41\u91cf\u7ffb N \u500d\uff0c\u4e8e\u662f\u53c8\u51fa\u73b0\u4e86\u2026\u2026\n\u5982\u679c\u662f\u8d1f\u8f7d\u9ad8\u7684\u60c5\u51b5\u4e0b\u4f1a\u51fa\u73b0\u7684\u8bdd\uff0c\u90a3\u4e48\u6211\u4eec\u7528\u589e\u52a0 node \u6570\u91cf\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002\n\u53ef\u4f9b\u53c2\u8003\u7684\u503c\u5e94\u8be5\u662f\u4e00\u4e2a node \u53d1\u5f80 redis \u7684 qps \u5427\uff0c\u8fd9\u4e2a\u5e76\u4e0d\u77e5\u9053\u2026\u2026\n\u5982\u679c\u628a debug \u6253\u5f00\uff0cioredis \u8f93\u51fa\u7684 write command \u65e5\u5fd7\u662f\u4e0d\u662f\u5c31\u662f\u4e00\u6b21 redis \u64cd\u4f5c\uff1f\n. hiredis \u5e76\u6ca1\u6709\u88c5\uff0c\u4e4b\u524d\u7528\u4e86\u4e00\u4e0b\u51fa\u73b0\u4e00\u4e2a\u9519\u8bef\u6000\u7591\u662f\u5b83\u5f15\u8d77\u7684\u5c31\u5220\u6389\u4e86\u3002\n. \u8fd8\u6709\u4e00\u4e2a\u72b6\u51b5\u5c31\u662f\uff0c\u5982\u679c\u4e00\u4e2a\u8fdb\u7a0b\u51fa\u73b0\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u90a3\u4e48\u80af\u5b9a\u5c31\u4e0d\u4f1a\u662f\u4e00\u6b21\uff0c\u800c\u4e14\u8fd9\u4e2a\u8fdb\u7a0b\u5f88\u6709\u53ef\u80fd\u5c31\u4e0d\u4f1a\u6b63\u5e38\u5de5\u4f5c\u4e86\u3002\n\u53e6\u5916\u5728\u73b0\u573a\u68c0\u67e5\u7684\u65f6\u5019\u6ca1\u6709\u53d1\u73b0 redis \u91cc\u5b58\u7684\u4e0d\u662f\u9884\u671f\u7684\u503c\u7684\u73b0\u8c61\uff0c\u4e5f\u5c31\u662f\u8bf4\u53ea\u6709\u5728\u83b7\u53d6\u7684\u65f6\u5019\u624d\u4f1a\u51fa\u73b0\uff08\u4f46\u4e0d\u80fd\u4e25\u683c\u4fdd\u8bc1\uff09\u3002\n\u7a0d\u540e\u6211\u8d34\u4e00\u6bb5\u4ee3\u7801\u4e0a\u6765\u3002\n. pub/sub monitor \u90fd\u6ca1\u6709\u3002\u53ea\u6709 hash \u548c zset\uff0c\u548c get/set \u3002\n\u7248\u672c\u662f 1.7.5\n91 function getTopHotEntry() {\n 92   return getTopEntryFromRedis().then(function(topId) {\n 93     if(!topId) {\n 94       return getTopEntryFromMongodb().then(function(topEntry) {\n 95         return setTopEntryToRedis(topEntry.entry).then(function() {\n 96           return topEntry;\n 97         });\n 98       });\n 99     }\n100     return HotEntry.findOne({ entry: topId }).lean().exec();\n101   });\n102 }\n103\n104 function getTopEntryFromMongodb() {\n105   return HotEntry.findOne().sort({ _id: -1 }).lean().exec();\n106 }\n107\n108 function getTopEntryFromRedis() {\n109   return redis.getAsync(HOT_ENTRY_KEY);\n110 }\n\u8fd9\u6bb5\u4ee3\u7801\u5728\u4e1a\u52a1\u91cc\u8c03\u7528\u7684\u9891\u7387\u975e\u5e38\u9ad8\uff0cHOT_ENTRY_KEY \u4ec5\u5b58\u4e86\u4e00\u4e2a MongoDB \u7684 document id\u3002\n\u7206\u53d1\u4e32\u6570\u636e\u7684\u60c5\u51b5\u7684\u65f6\u5019\uff0c\u7b2c 100 \u884c\u4f1a\u51fa\u73b0\u5404\u79cd\u5185\u5bb9 cast to _id  \u7684\u9519\u8bef\uff0c\u6bd4\u5982 \"OK\", null, 1, \"{xxxxx: xx}\".....\u603b\u4e4b\u4efb\u4f55\u5947\u602a\u7684\u4e1c\u897f\u90fd\u6709\u53ef\u80fd\u2026\u2026\n. \u662f\u7684\uff0c\u73b0\u5728\u8fd9\u91cc\u662f node_redis\uff0c \u662f\u60f3\u5bf9\u6bd4\u4e00\u4e0b\u662f\u5426\u662f ioredis \u7684\u95ee\u9898\u3002\u56e0\u4e3a\u4e0d\u80fd\u5168\u90e8\u6362\uff0c\u5c31\u5728\u90e8\u5206\u5bb9\u6613\u4e32\u7684\u5730\u65b9\u6362\u6210\u4e86 node_redis\u3002\n\u4f46\u662f\u95ee\u9898\u5c31\u662f\u6211\u63cf\u8ff0\u7684\u90a3\u6837\u3002\n\u53e6\u5916 debug \u51fa\u6765\u7684 ioredis write command *** \u7684\u65e5\u5fd7\u662f\u4e0d\u662f\u4e00\u6761\u6307\u4ee4\u8f93\u51fa\u4e24\u6b21\uff1funique \u4e86\u4e00\u4e0b\u597d\u50cf\u90fd\u662f 2   \u6761\u4e00\u6837\u7684\u3002\n. \u5bf9\u6bd4\u4e86\u4e00\u4e0b\u6570\u636e\uff0c\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5927\u7ea6\u5728\u4e00\u4e2a node \u5206\u5230\u7684\u8bf7\u6c42\u5728 30 qps \u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u51fa\u95ee\u9898\u3002\n30qps \u8f6c\u5316\u6210 redis \u64cd\u4f5c\u7684\u8bdd\uff0c\u53ea\u6709 1 \u4e2a\u6708\u4e4b\u524d\u7684\u4ee3\u7801\u6709 debug \u65e5\u5fd7\uff0c\u5e73\u5747\u6bcf\u79d2\u4f1a\u6709 1.2w \u6b21redis \u64cd\u4f5c\u2026\u2026\n\u65e5\u5fd7\u5185\u5bb9\u662f\u8fd9\u6837\u7684\nSun, 01 Nov 2015 07:12:21 GMT ioredis:redis write command[0] -> exists(jwt:UID)\n\u7136\u540e\u8fd8\u662f\u8bf4\uff0cdebug \u65e5\u5fd7\u91cc\u5b8c\u5168\u76f8\u540c\u7684\u5185\u5bb9\uff08cmd + key\uff09\u90fd\u4f1a\u51fa\u73b0\u4e24\u6b21\u2026\u2026\u6240\u4ee5\u771f\u7684\u4e0d\u662f\u4e00\u4e2a command \u4f1a\u6253\u5370 2 \u6b21\uff1f6k \u548c 1.2w \u7684\u533a\u522b\u8fd8\u662f\u5f88\u5927\u7684\u2026\u2026\n\u95ee\u6211 1.2w/30 \u5c31\u662f\u6bcf\u4e2a\u8bf7\u6c42\u4f1a\u6709 400 \u6b21 redis \u64cd\u4f5c\uff08or 200\uff09\uff0c\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u9ad8\u6211\u4e5f\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u2026\u2026\n. ok\uff0c\u53ef\u80fd\u662f\u5f53\u65f6\u7684\u4ee3\u7801\u6709\u95ee\u9898\u6216\u8005\u7edf\u8ba1\u6570\u636e\u6709\u95ee\u9898\u3002\n\u521a\u624d\u6253\u5f00 debug \u770b\u4e86\u4e00\u4e0b 30min 6500 \u4e2a\u8bf7\u6c42\uff0credis \u64cd\u4f5c 20000 \u6b21\u3002\n\u6628\u665a\u9ad8\u5cf0\u671f\u5927\u7ea6 6-7 \u500d\u7684\u6d41\u91cf\uff0c\u4e5f\u5c31\u662f\u8bf4\u51fa\u95ee\u9898\u7684\u65f6\u5019 redis \u8bf7\u6c42\u6570\u5728 70/s \u5de6\u53f3\u3002\n. ",
    "Ovidiu-S": "Thanks for answering luin. \nCould you provide more details please ? Sharing a single connection with pubsub means one client publishes everything and is subscribed to everything.\nDo you foresee an issue with increasing the number of max connections when it is needed ?\n. Also, when using cluster, does the number of connections scale when adding more servers ?\n. Thank you again for responding. I am confused how a single connection would scale, or how to add additional endpoints to scale horizontally.\nI can see how we could use our Kafka queue to decouple, but PubSub seems nicer and faster. I don't yet picture how one connection could handle millions of topics and millions of users.\nI understand this is not ioredis specific, but you have helped by confirming there is a number of connection issue, which I appreciate. Could you suggest a better place for me to post these questions?\n. ",
    "skuridin": "@luin Sorry, it's because I did assert.ok() inside then(). My fault. \n. ",
    "ajkerr": "Oh! You are right, I thought I was using 1.10.0, but I had a typo in my package.json file and was using 1.1.0. Sorry about that.\n. ",
    "toddbluhm": "Yeah I will certainly do that. I will let you know next Monday if I run into any errors with this, and what those errors are. We run the workers over the weekend so I won't know before then.\n. Okay so here is the error that I am getting. I received this error 2 times out of all the jobs we ran +2k. Again the retry on the jobs worked just fine.\nScanStream receives an error [Error: Stream isn't writeable and enableOfflineQueue options is false]\nNote: I do have offline queue set to false as I don't want to queue any data up (our worker has a hard ram limit and its small so I can't afford to buffer/queue very many items, or it will force kill the worker immediately).\n. Yeah sounds good. Just let me know when you have this error getting forwarded to the error handler so I can catch it and then do as you suggested :smile: \n. Thanks!\n. ",
    "madeinjam": "Awesome, thanks!\n. ",
    "Gorokhov": "this seems to work properly only in the case if sentinelRetryStrategy is given as above. \nWith basic configuration ex:\nclient = new Redis({\n                sentinels: config.sentinels,\n                name: config.sentinelName,\n                connectTimeout: 1000\n            });\nthe error is never fired, maybe this is expected but not intuative.\n. ",
    "xxxman2008": "Sorry ,i mistake    \"assume\uff1a key bar (value foo) stored in MasterB , key foo (value bar) stored in MasterC\"\nshould be \"assume\uff1a key bar (value foo) stored in MasterA, key foo (value bar) stored in MasterC\"\n. I upgrade to  1.12.2 , got same result ..\nAnd, i'm  using webstorm  on  windows .....so sorry  ,i don't know  how to  set  Debug= ioredis:* ,althought  i  set it as an  windows  ENV  parameter.\n. finally \uff0ci  open debug for  ioredis.  Here is the  log:\nSee attache file .\nIORedis-DebugInfo-01.txt\n. And  my  Code for your refference.\nRedisCache.js.txt\n. Something  is  really  confused me.\nYou can see ,i  make an Option Object\nRedisCache.optionParam = {\n    connectTimeout:2000,\n    enableOfflineQueue:false,\n    autoResendUnfulfilledCommands:false\n}; \nAnd then Creat an Cluster connection  with :\nconn = new REDIS.Cluster(this.connParam,this.optionParam);\nI  think  it will  disable the \"OfflineQueue\"  , But ,from  debug log ,you can see :\nMon, 07 Dec 2015 10:53:41 GMT ioredis:redis queue command[0] -> set(bar,foo0.86925975442864)\nMon, 07 Dec 2015 10:53:41 GMT ioredis:redis queue command[0] -> get(bar)\n------------  seems  the command still  be  queued .\nAnd  sencode , i think  retryStrategy  should have an  Default value , but  we got : \nMon, 07 Dec 2015 10:53:33 GMT ioredis:connection skip reconnecting because retryStrategy is not a function\n. OK\uff0c thank you .\nAnd  i really think the options  of  single redis connection  and cluster connection  should  be (at least most of them should be ) same ( uniforme? ).\nAnd , if  option \"enableOfflineQueue\" was not  used in the cluster internally  --  in some situation ,it's waste time --- ,we can resolve the problem ,i guess.\nit just need an Event to inform user \"we cannot got the key\" ,so it's user's responsibility how to proceed. \nIn another hand , if  MasterA will never online , the command queue may crash the whole process( also ,i guess).  In the  meantime , the web user will encountered a long long time waiting...\nAnyway , thank you  . It seems  IORedis need more time to strong enough to deal with various situations. Best wishes. \nBye.\n. ",
    "roblav96": "I did that here:\njavascript\n} ).then( function ( pass ) {\n    this.coms = []\n    if ( pass.doActivesUpdate == true ) {\n        if ( _.isNull( pass.activesUpdate ) ) {\n            this.coms.push( [ 'setex', 'updates:' + req.xid + ':actives', process.env.rkeyTokensTimeout, this.stamp ] )\n        }\n        this.coms.push( [ 'smembers', 'convoys:' + req.xid ] )\n        this.coms.push( [ 'smembers', 'convoying:' + req.xid ] )\n    }\n    return redis.pipeline( this.coms ).\n    zrangebyscore( 'updates:' + req.xid + ':geo', pass.prevUpdate, '+inf' ).\n    // zrangebyscore( 'updates:' + req.xid + ':geo', 1449190842488, '+inf' ). // DEV\n    smembers( 'updates:' + req.xid + ':contacts' ).\n    expire( 'updates:' + req.xid + ':contacts', 5 ). // let it get it before we remove it\n    setex( 'updates:' + req.xid + ':prevupdate:' + req.bytes, process.env.rkeyTokensTimeout, this.stamp ).\n    exec()\n} ).then( function ( docs ) {\n    // var max = offset + 2 // +2 is the zrangebyscore and setex coms not in this.coms\n    var docLen = docs.length\n    var offset = this.coms.length\n    this.sendi = {}\n    if ( this.coms.length != 0 ) {\n        this.sendi.convoys = docs[ offset - 2 ][ 1 ]\n        this.sendi.convoying = docs[ offset - 1 ][ 1 ]\n    }\n    this.geos = docs[ docLen - 4 ][ 1 ]\n    this.contacts = docs[ docLen - 3 ][ 1 ] // check if null?\n} )\nBut as I add more updates, it gets pretty ugly and hard to manage.\n. I was afraid of that :/\nNo worries though! So I've been making due. Excellent work here!\n. You can't use multi without pipeline\nToo many negatives trip me out :X I think I need to recall the demorgans theorem. lol\nNow is pipeline set to true by default?\nAnd is this syntax correct?\njavascript\nredis.multi( [\n    [ 'set', 'foo', 'bar' ],\n    [ 'get', 'foo' ]\n], {\n    pipeline: false\n} )\nredis.exec()\nAlso wouldn't it error out that different keys are on different masters?\nThis is my cluster setup:\n```\nConnecting to node 127.0.0.1:6379: OK\nConnecting to node 127.0.0.1:6383: OK\nConnecting to node 127.0.0.1:6381: OK\n\n\n\nPerforming Cluster Check (using node 127.0.0.1:6379)\nM: 6fd3898306c1fa51dd8d7ea1bc5b1397d5f5f468 127.0.0.1:6379\n   slots:0-5460 (5461 slots) master\n   0 additional replica(s)\nM: af5e247e8c8b7834c378fa45316603ff702b359e 127.0.0.1:6383\n   slots:10923-16383 (5461 slots) master\n   0 additional replica(s)\nM: 9d11b6af92a2d8db269438a5908b2ef39c7a084a 127.0.0.1:6381\n   slots:5461-10922 (5462 slots) master\n   0 additional replica(s)\n[OK] All nodes agree about slots configuration.\nCheck for open slots...\nCheck slots coverage...\n[OK] All 16384 slots covered.\n```\n\n\n\nThis whole cluster stuff is really interesting and fun!\n. ",
    "Chandler243": "I tried to perform \nredis.spop('myset', 3, function(err, elements) {\n     console.log(elements)\n})\nAnd it complained that I had too many arguments for the 'spop' command.\nRunning the command without the number correctly returns one element and deletes it from the set.\n. Ah, looks like I need to learn to RTFM better, Thank you very much!\n. ",
    "davidgatti": "I see this same. The data is there. On Mac I replaced the local link to the local Readis with the remote one. And running a local script connected to that Redis database gives the same result. I can count, but I can't spop\n. \"I see this same\" == \"I see the same as the Desktop Manager\"\n. Will try, thank for the debugging tip.\n. ",
    "garkin": "/test/functional/transformer.js#L45\ndescribe('mset', function () {\n      it('should support object', function (done) {\n        var redis = new Redis();\n        redis.mset({ a: 1, b: '2' }, function (err, result) {\n          expect(result).to.eql('OK');\n          redis.mget('a', 'b', function (err, result) {\n            expect(result).to.eql(['1', '2']);\n            done();\n          });\n        });\n      });\n});\n. I think it's pretty inconsistent to have msetnx API differ from mset. \nSince it should be just an mset with a blackjack.\n. ",
    "fbacker": "On the redis server I used redis-cli and flushall, created the instances again.\nIf I'm using redis desktop manager from same computer that runs the node script I can get a connection to port 30001.\nBut when running the nodejs ioredis code I still get error. Any id\u00e9as what's missing? Do I need to configure redis in any other way? I've just downloaded redis, built it and started. No configuration.\nREDIS CONNECT error Error: Failed to refresh slots cache.\nnode error { [ReplyError: ERR unknown command 'cluster']\n  name: 'ReplyError',\n  message: 'ERR unknown command \\'cluster\\'',\n  command: { name: 'cluster', args: [ 'slots' ] } }\nWhen running the nodejs on same server and change the host to 127.0.0.1\nREDIS CONNECT error Error: Failed to refresh slots cache.\nnode error [Error: Connection is closed.]\nWhen looking at 30001.log\n3664:M 23 Dec 09:43:08.240 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n3664:M 23 Dec 09:43:08.240 # Server started, Redis version 3.0.6\n3664:M 23 Dec 09:43:08.240 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\n3664:M 23 Dec 09:43:08.241 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n3664:M 23 Dec 09:43:08.241 * The server is now ready to accept connections on port 30001\n3664:M 23 Dec 09:43:16.596 # configEpoch set to 1 via CLUSTER SET-CONFIG-EPOCH\n3664:M 23 Dec 09:43:16.655 # IP address for this node updated to 127.0.0.1\n3664:M 23 Dec 09:43:18.560 # Cluster state changed: ok\n3664:M 23 Dec 09:43:19.269 * Slave 127.0.0.1:30004 asks for synchronization\n3664:M 23 Dec 09:43:19.269 * Full resync requested by slave 127.0.0.1:30004\n3664:M 23 Dec 09:43:19.269 * Starting BGSAVE for SYNC with target: disk\n3664:M 23 Dec 09:43:19.269 * Background saving started by pid 3691\n3691:C 23 Dec 09:43:19.272 * DB saved on disk\n3691:C 23 Dec 09:43:19.272 * RDB: 0 MB of memory used by copy-on-write\n3664:M 23 Dec 09:43:19.362 * Background saving terminated with success\n3664:M 23 Dec 09:43:19.363 * Synchronization with slave 127.0.0.1:30004 succeeded\n. I'm running redis 3.0.6. And testing with CLI works as it should.\n[ec2-user@ip-172-31-6-207 src]$ ./redis-cli -c -p 30001\n127.0.0.1:30001> set foo bar\n-> Redirected to slot [12182] located at 127.0.0.1:30003\nOK\n127.0.0.1:30003> get foo\n\"bar\"\n127.0.0.1:30003> set hello world\n-> Redirected to slot [866] located at 127.0.0.1:30001\nOK\n127.0.0.1:30001> get foo\n-> Redirected to slot [12182] located at 127.0.0.1:30003\n\"bar\"\n127.0.0.1:30003> get hello\n-> Redirected to slot [866] located at 127.0.0.1:30001\n\"world\"\n127.0.0.1:30001>\n. Now it seems to be working. When I ran it first on local computer I saw that it kept asking for 127.0.0.1 instead of the hostname that was specified. So looked with new eyes on the code and I've somehow managed to put double array [[ ]] where I specify the hosts. Pretty emabbasing yes :(\nSo now running on same machine with 127.0.0.1 works.\nHowever when running on developer machine and using hostname it's still an issue. Is there more ports that needs to be open on the server?\nDEBUG=ioredis:* node -nouse-idle-notification -expose-gc -max-old-space-size=8192 app.js\ninfo: node environment production\ninfo: node port. 5224\ninfo: Configuration environment=production, port=5224, redisNamespace=BingoLottoChat:, mainroom=MainRoom, chatroom=ChatRoom, expireCheck=10000, expireMessage=180, expireProcess=15, facebookTimeout=900\ninfo: start worker: 0, with port: 5224\ninfo: node environment production\ninfo: node port. 5224\n  ioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +5ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +2ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +1ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30001 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +2ms\n  ioredis:cluster status: [empty] -> connecting +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +1ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30001 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +0ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30002 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\ninfo: Load server pid: 19140, port 5224\ninfo: Server is up and running\ninfo: HTTP Listening on *:5224\n  ioredis:redis status[52.11.222.96:30001]: connecting -> connect +412ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[52.11.222.96:30003]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30002]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[52.11.222.96:30003]: connecting -> connect +4ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[52.11.222.96:30001]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30002]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30001]: connecting -> connect +8ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30003]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30002]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[52.11.222.96:30003]: connect -> ready +177ms\n  ioredis:redis status[52.11.222.96:30001]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +1ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[52.11.222.96:30002]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +1ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[52.11.222.96:30003]: connect -> ready +10ms\n  ioredis:redis status[52.11.222.96:30001]: connect -> ready +5ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n  ioredis:redis status[52.11.222.96:30002]: connect -> ready +4ms\n  ioredis:redis status[52.11.222.96:30001]: connect -> ready +7ms\n  ioredis:redis status[52.11.222.96:30003]: connect -> ready +5ms\n  ioredis:redis status[52.11.222.96:30002]: connect -> ready +2ms\n  ioredis:redis status[127.0.0.1:30002]: [empty] -> connecting +157ms\n  ioredis:redis status[127.0.0.1:30003]: [empty] -> connecting +1ms\n  ioredis:redis status[127.0.0.1:30001]: [empty] -> connecting +1ms\n  ioredis:cluster status: connecting -> connect +1ms\n  ioredis:cluster status: connect -> ready +0ms\nREDIS CONNECT\n  ioredis:connection error: Error: connect ECONNREFUSED +2ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:redis status[127.0.0.1:30001]: [empty] -> connecting +1ms\n  ioredis:redis status[127.0.0.1:30003]: [empty] -> connecting +0ms\n  ioredis:redis status[127.0.0.1:30002]: [empty] -> connecting +2ms\n  ioredis:cluster status: connecting -> connect +1ms\n  ioredis:cluster status: connect -> ready +0ms\n  ioredis:cluster send 6 commands in offline queue +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:chatmessage) +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:chatmessage-confirm) +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:presence-friends) +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:user-clear) +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:process-start) +0ms\n  ioredis:redis queue command[0] -> subscribe(BingoLottoChat:process-end) +0ms\n  ioredis:redis status[127.0.0.1:30001]: connecting -> close +1ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30001]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30003]: connecting -> close +1ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30003]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30002]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30002]: close -> end +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED +1ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:redis status[127.0.0.1:30002]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30002]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30003]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30003]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30001]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30001]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30002]: [empty] -> connecting +18ms\n  ioredis:redis status[127.0.0.1:30003]: [empty] -> connecting +3ms\n  ioredis:redis status[127.0.0.1:30001]: [empty] -> connecting +3ms\n  ioredis:cluster status: connecting -> connect +0ms\n  ioredis:cluster status: connect -> ready +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED +2ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED +0ms\n  ioredis:redis status[127.0.0.1:30001]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30001]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30003]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30003]: close -> end +0ms\n  ioredis:redis status[127.0.0.1:30002]: connecting -> close +1ms\n  ioredis:connection skip reconnecting because `retryStrategy` is not a function +0ms\n  ioredis:redis status[127.0.0.1:30002]: close -> end +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: ready -> close +156ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: close -> end +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: ready -> close +5ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: close -> end +2ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: close -> end +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: ready -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: close -> end +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: ready -> close +4ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: close -> end +0ms\n  ioredis:cluster status: ready -> close +0ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: close -> end +0ms\n  ioredis:cluster status: ready -> close +0ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: ready -> close +11ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: close -> end +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: ready -> close +8ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: close -> end +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: ready -> close +8ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: close -> end +0ms\n  ioredis:cluster status: ready -> close +1ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +77ms\n  ioredis:cluster status: reconnecting -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +0ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30001 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +5ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +0ms\n  ioredis:cluster status: reconnecting -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +1ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30001 +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +29ms\n  ioredis:cluster status: reconnecting -> connecting +1ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30001]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30002]: [empty] -> connecting +0ms\n  ioredis:redis status[xxx.us-west-2.compute.amazonaws.com:30003]: [empty] -> connecting +0ms\n  ioredis:cluster getting slot cache from xxx.us-west-2.compute.amazonaws.com:30002 +1ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n. Looks like it goes from the host I wanted to 127.0.0.1 in the end?\nI've installed redis locally so now it works but might want to check into using redis cluster with another hostname.\n. ",
    "genediazjr": "No, origin and target are connected on the same redis server but on a different db\njavascript\nvar originRedisClient = new Redis({db: 0});\nvar targetRedisClient = new Redis({db: 1});\nI'm testing dump and restore from db to db.\n. The problem was due to my nodejs installation. I got it working.\n. ",
    "Salakar": "When i tried with just one cluster node, which was in a CLUSTER DOWN state it still emits ready. But there's no connections. \ni.e:\njavascript\nthis.client = new Redis.Cluster([\n        {host: '127.0.0.1', port: 6379}  // in a CLUSTER DOWN state\n      ]);\nI'm using default settings, so not touched clusterRetryStrategy yet. \n. Should it not check that the cluster can receive commands?\nThe readme says ready is emitted when the server is ready to receive commands, doesn't CLUSTER DOWN count as not ready to receive commands (as in userland ones, as apposed to info commands ioredis uses to check state)?\n. ``` javascript\nimport Redis from 'ioredis';\nconst client = new Redis.Cluster([\n  {host: '127.0.0.1', port: 7777}\n]);\nclient.on('error', function (error) {\n  console.dir(error);\n});\nclient.on('ready', function () {\n  console.log('** READY ****'); // getting spammed\nclient.get('ABCD', function (err) {\n    // never calls back - stuck in offline queue i guess?\n    console.error(err);\n  });\n});\n```\nIt also spams ready loads when cluster is down.\n\n. Ok, thanks. Other than that though, ioredis is awesome :+1: \n. Been testing v2.0.0-alpha2 clustered for a couple days now, not had any issues so far, memory issue seems to have been resolved.\nThe new scaleReads options seems to be working fine also, much more flexible than the old readOnly option.  :+1: \n. Nice! when do you think this'll get merged?\n. Just one small change that needs doing, the pipeline can use var calculateSlotMulti = require('cluster-key-slot').generateMulti; to calculate the slots for the array of keys there, then instead of checking for 'undefined' you just check if slot < 0 (returns -1 if any slot isn't the same) - then throw the 'keys must be of same slot' error. \nThe multi on the calc repo is a little bit more performant. https://github.com/Salakar/cluster-key-slot#usage\n. Hmm build test failures seem a little random and unrelated?\n. Looks good, LGTM =]\n. See the change log and follow the versioned PR's : https://github.com/NodeRedis/node-redis-parser/blob/master/changelog.md#v200---29-may-2016\nMainly: https://github.com/NodeRedis/node-redis-parser/commit/4418ac6f20b469aa7c3995953791edda94e916c2\nBut considering it's a full re-write I don't see much point comparing diffs \ud83d\ude04 \nYou can follow the original PR here: https://github.com/NodeRedis/node-redis-parser/pull/2\n. @xyzlast you're looking at the wrong repo.  https://github.com/NodeRedis/node-redis-parser\n. \nThese two failing on travis - think it's a random connection issue, seen connection is closed.\n. - It's per event loop tick so it won't just hang, buffer will always get written at the start of the next tick, even if it's not reached those limits\n- 100 commands in a single 0.2-0.3ms trip is still better than 100 round trips totalling at 20-30ms, it adds up as the benchmarks show. \n- Am aware of the new parser also as I re-wrote it with BridgeAR, and wrote the new key slot calculator =]\n- Manual pipelines go through the same logic and will also get buffered, there's no harm in this, it's per event loop tick as I said, so the manual pipeline and any other commands run in the same tick will get sent together.\n. @AVVS no worries, if we need the manual pipelines to be sent in their own .write() then we can just use the forceWrite arg I added to the proxy stream.write() function, shouldn't be needed though\n. Performance halves on remote servers, wut. Investigating...\n. @luin It's a lot more performant without cork and uncork, bench with cork/uncork:\n\nStill investigating the remote issue, seems to just make it worse... might make an issue on node repo.\n. @pavel I'm not sure if it's worth it just for local connections - would be good if I could get responses on the nodejs issues I created re this but had nothing from anyone there, it's weird how this drastically drops if remote connection. \nI have a feeling maybe it's using unix socket locally maybe, dunno, need to investigate, just not got the time at the moment. Soon \u2122\ufe0f\n. @lo1tuma I did this at the raw connection level, so not an issue, each individual server connection has it's own pipeline.\n. Closing my PR until i can re-look at this at a later date.\n. @luin Woo, looks like this is fixed in node now - https://github.com/nodejs/node/issues/5095\nMight be worth re-opening and checking the benchmarks. I tweeted both the contributors, should hopefully get their attention \ud83d\udc4d \n\n. License has been added, MIT.\n. > Bluebird is NOT promises. It doesn't follow the spec.\n@abritinthebay pretty sure @petkaantonov and the guys over at https://github.com/promises-aplus/promises-spec would disagree with you, but...  hey what do they know. \ud83d\udc4d  \nNot entirely sure why you're against it, it keeps Promises consistent across versions - there has also been changes between node 4 - 7 in the native node promises implementation, such as better stack traces in the newer versions that have not been back ported to older versions. \n\nno developer I know would willingly add a bloated library\n\nYour concerns about 'bloat' are far fetched, this is server side code: \n\nIf it was bundled for web usage then sure - I'd understand maybe.\nioredis has <10 dependencies of which most of them have 0 dependencies themselves (bluebird included) this point is moot. \nIf your opinion of bloated is 1 module with zero dependencies then you're going to have a bad time in the npm/node ecosystem - better not use browserify then anymore in your projects, it has 119 dependencies too many for you. \ud83d\udc31 \n\n\nThat's it. 100% Native compatible without using any extra library. Also 100% as fast as Bluebird because the raw reject/resolve/then calls are the same speed and Bluebird performs approx the same when used with new (instead of their promisify implementation).\n\nWould love to see your benchmarks on this, while you're at it can you benchmark memory usage.\nBack on subject:\nAs for node support, I'm all for making node 4 the minimum version.. @abritinthebay never said it can't be bloated, server side code bloat is not an issue here on this lib, I'd say educate us with your experience on open-source libs and node but I feel we'd have better conversation talking to a brick wall. \ud83d\udc4d \nBack to topic.\n@AVVS definitely should support 4+ in the next major release, had this discussion a while back with @luin - both agreed it was for the best and gives more flexibility.\nThough having said that, my thoughts are start at version 6, its now in LTS - allows us to then use things like Proxy and gain a ton of performance benefits, e.g. a redis client I wrote that made use of Proxy and various other ES6 features (and god forbid... bluebird):\n\n. > Funny, that's the impression I'm getting from you too.\nTouch\u00e9. \ud83d\ude1b \nGuess it depends when we'd want to release by, I'd suggest as soon as 4 is in maintenance unofficially drop it, it's not like anything drastic is going to change during the maintenance period really that'd break older versions? But, I can also see the need to keep it in support still until April next year. Ah, decisions.\nI did originally write the cluster slot calculator and the double ended queue implementation used here to only support node 4+ but eventually just made it backwards compat to 0.12 just to get it in for release - so I guess it'd save hassle like that and thats the main thing really for me.\nThe only major thing that'd be good to utilise that I can see missing from 4 is Proxy (but it is there behind a harmony flag), const, let, classes etc are all there.. ",
    "RyanMcDonald": "Thanks guys. That helps a lot!\n. ",
    "Volox": "Ah ok, i got the point.\nBut since i am requiring keys it would be nice/useful to use this command transparently no?\n. Ok,\nbut once i got the keys from the keys command i cannot use them coherently with the other commands.\nHere is an example:\n``` javascript\n// Just pretend that it works without promises/callbacks :)\n// prefx: \"NS:\"\nlet keys = redisP.keys( '*' ); // keys = [ \"key1\", \"NS:key2\" ]\n// This will NEVER get \"key1\" because the get is namespaced\nlet val = redisP.get( keys[0] );\n// also i cannot get \"NS:key2\" because i will get \"NS:NS:key2\" instead\n```\n. Thanks to all for the support, I see there is no easy solution for this.\n. ",
    "xpiwo": "right, looks like 10 minutes.\n. upgrading to a newer redis version also works, 2.8+ allows ping in subscription mode.\n. in theory even older redis error-response can be considered PONG.\nhowever even when pinging, ioredis just tries to ping and never gets the response back, so that does not help much.\n10 minutes is a lot of time. i guess i can keep track of the last PONG received. is there a command to force ioredis to re-connect?\nthis issue looks similar/same as #139, but i am not sure what is the suggested solution.\n. ",
    "sakthivel49": "$ node --version\nv0.10.25\n. I tested with latest version of node.js (v4.2.6) it works fine, error catch on event and process not terminated. Thanks. Any hope for v0.10.25?\n. Yes when I run the same code on cent os it works fine. Thank you for \u200byour swift \u200breply.\n. ",
    "lejoix": "Hi,\nThank you for your response. Yes I am using domains. I put it wrapped around the code in the description. As you said try catch of course doesn't work with async code, but connect-domain handles that. Or it does at least for other use cases.\n. Yes I can try, although at first sight a problem could be I am using node 4.2.1 (supposedly LTS version). To complicate things even more, the same code, same installation is working on an installation and not on another. So this of course can lead to other discussions, but I am trying a go here because it's crashing in ioredis module. Will come back with feedback\n. So, of course this part of code is working, also in my earlier tests something similar it was. However I cannot see any part of the app, that could change this and crash it in ioredis. And since the behaviour is not the same either for my local installation and for the remote intstallation ( I am managing both) I am suspecting also a versioning issue of one of the module dependencies. I am really desperate with this and depleted most of the ideas the I found on the net. The real use case is of course more complicated, I have my app separated in services, I use express router, and actually have more redis get's in the callback of redis commands.\n. OK I did some more extensive tests. It is actually related to https! I have in my app http AND https termination and it is related to https it seems, because on http it's ok but on https it's still crashing inside ioredis.Need to do some more research on this.\n. Just as an idea, do you know a better errorhandling approach for async calls than domains ?\n. Thank you, for your patience and responses. It seems that domain is not the way to go (unfortunately this has become clear only now when the app is big and complex). With a simple application, like your example I cannot crash it, but with a big and complex app, handling a lot of requests at once, if requests that throw inside come in fast enough, domain somehow \"looses\" the context, So for example if I slowly make the same requests it's fine it's caught, but if I make it really fast, after 10th or so request it crashes. That's why it only popped up now. Also it happens on http also of course. Ugh, need to find a better way then promises, I really cannot rewrite the app at this point :)\nThanks, keep it up!\n. Hi,\nThanks for the tip. So it's getting even weirder, before I saw it sent to redis with monitor command (when I was debugging the module) but now it doesn't get sent. See output below:\nioredis:redis status[:3001]: [empty] -> connecting +2s\nioredis:redis queue command[0] -> get(profile:123456789) +1ms\nioredis:redis status[127.0.0.1:3001]: connecting -> connect +1ms\nioredis:redis write command[0] -> info() +0ms\nioredis:redis status[127.0.0.1:3001]: connect -> ready +1ms\nioredis:connection send 1 commands in offline queue +0ms\nioredis:redis write command[0] -> get(profile:123456789) +1ms\nioredis:redis status[:3001]: [empty] -> connecting +2ms\nioredis:redis queue command[0] -> sismember(all_playlist_slugs:123456789,asdasd) +0ms\nioredis:redis status[127.0.0.1:3001]: connecting -> connect +1ms\nioredis:redis write command[0] -> info() +0ms\nI am making two operations to redis in this case, the first is working the second is not. \nAnother small note, is that this used to work just fine. Can't this be a compatibility issue maybe?\n. Thanks for your efforts. \nOk I checked some more and cleaned up. The weird error message was because I have a special case when starting node with env='test', but either way the problem is the same. I paste the new logs and then redis monitor logs. I have my own console logs also there\nioredis logs from node:\nDebugger listening on port 5858\n  ioredis:redis status[:3001]: [empty] -> connecting +0ms\nLoaded version: /v0 | Alias: stable | Default:true\nPLY HTTP server listening on port 3333 in development mode\nPLY HTTPS server listening on port 5433 in development mode\n  ioredis:redis status[127.0.0.1:3001]: connecting -> connect +2s\n  ioredis:redis write command[0] -> info() +1s\nCONNECTED to redis on port :3001\n  ioredis:redis status[127.0.0.1:3001]: connect -> ready +6ms\nlogged in true\n  ioredis:redis write command[0] -> get(profile:566ae33524c12ab4786768b8) +408ms\n  ioredis:redis write command[0] -> sismember(all_playlist_ids,59855178) +7s\nredis monitor logs:\n1454946391.113656 [0 127.0.0.1:50440] \"info\"\n1454946391.526532 [0 127.0.0.1:50440] \"get\" \"profile:566ae33524c12ab4786768b8\"\n1454946398.954276 [0 127.0.0.1:50440] \"sismember\" \"all_playlist_ids\" \"59855178\"\nSo the command is queued and redis monitor gets it. But no response called in node. Other commands work fine, also this command in other cases. Really weird\n. Sorry I didn't came back at this. I had to change my whole app's flow. My specific problem was that if I had multiple ioredis calls nested with callbacks, and there was a throw, I was doomed. So I had to rewrite my app to not follow this logic (handle errors differently). I would really recommend rewriting to Promises if you still are able to (I am not at this point, at least a few weeks of refactoring/bugfixing would be needed).\n. ",
    "ramonsnir": "@shaharmor What if you use dns.lookup applicatively? Does that help?\n. Weird. I have it stabilizing at 65MB after 1-2 minutes. Tested both with mainline ioredis 1.x and with this 2.x branch. With a 3-masters-10-slaves cluster (was testing something, so had it ready).\nWhat version of Redis are you using? I'm running 3.0.7 locally.\n. Also, I'm oddly still running Node 0.10.40. Did you move forwards?\n. @luin Really strange. I installed with nvm Node 0.12.10, Node 4.3.0 and Node 5.6.0: all three are leaking. 0.10.40 isn't.\n@AVVS +1 for Map.\n. Correction for 0.12: it stabilized, eventually, around 350MB.\n@AVVS I'll upload a heapdump from Node 4, but Shahar gave a link above for code to reproduce: https://gist.github.com/shaharmor/7fbf9ef4cf606b06c214\n. Explicitly calling the gc seems to resolve this. Seems that someone the gc's behavior changed during Node's iojs phase of life...\n. http://expirebox.com/download/a9ad167c59af84903ad48b58b2cab0c3.html - dump from Node 4.6, without calling gc explicitly (although, calling heapdump does some gc before it saves, but not a full gc).\n. Looks like the node list of each slot is allocated in memory A LOT of times.\n. @shaharmor If all you did was reuse the \"key\" string, then it's still leaking on Node.js 4+.\n. ioredis:redis queue command[0] -> set(quay,993) +0ms                                                                                                                                                                                                                  [534/2097]\n  ioredis:redis queue command[0] -> set(quay,994) +0ms\n  ioredis:redis queue command[0] -> set(quay,995) +0ms\n  ioredis:redis queue command[0] -> set(quay,996) +0ms\n  ioredis:redis queue command[0] -> set(quay,997) +0ms\n  ioredis:redis queue command[0] -> set(quay,998) +0ms\n  ioredis:redis queue command[0] -> set(quay,999) +0ms\n  ioredis:redis status[172.17.0.3:7000]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[172.17.0.3:7000]: connect -> ready +0ms\n  ioredis:connection send 1000 commands in offline queue +0ms\n  ioredis:redis write command[0] -> set(quay,0) +0ms\n  ioredis:redis write command[0] -> set(quay,1) +0ms\n  ioredis:redis write command[0] -> set(quay,2) +0ms\n  ioredis:redis write command[0] -> set(quay,3) +0ms\n  ioredis:redis write command[0] -> set(quay,4) +0ms\n  ioredis:redis write command[0] -> set(quay,5) +0ms\n  ioredis:redis write command[0] -> set(quay,6) +0ms\n  ioredis:redis write command[0] -> set(quay,7) +0ms\n  ioredis:redis write command[0] -> set(quay,8) +1ms\n  ioredis:redis write command[0] -> set(quay,9) +0ms\n  ioredis:redis write command[0] -> set(quay,10) +0ms\n@luin can you try to run the following docker-compose? It leaks locally. If it doesn't link to you, then I'd want to ask what Linux Kernel version you have (I'm on 4.1.13).\nhttps://gist.github.com/ramonsnir/bb7158931db43cb7185e\n. My bisect stopped on https://github.com/luin/ioredis/commit/23851692b2e0f8e7d39d42061ab586e6071ac175 @luin \n. The one I'm personally having at the moment is that my cluster spans across multiple network subnets, and the latency between them is relatively large. So my function would take the node's IP, check if it's in the \"local subnet IP range\", and return a lower priority if it isn't. This would lower significantly the read times, as otherwise reads would go to a random slave which may or may not be nearby (i.e. 50ms latency instead of 1ms latency).\nP.S.: That is too bad :disappointed: Yet another regression from classic Redis.\n. That could work, although I thought that you'd prefer to keep as much logic as possible inside the library, just letting the custom function filter out low-prioritized nodes.\nYour suggestion 100% solves the problem.\n. It bothered me too... I'll do that, and make sure the README states it clearly.\n. Thanks!\n. Yes, you are correct. I'm sadly used too much to client-side JS, and Array.isArray is unavailable in some MSIE versions...\n. ",
    "dophlin": "The problem still exists, sismember callback is never called. Any new solution?\n. ",
    "apostopher": "ok. got it! closing this issue. thanks!\n. ",
    "evantahler": "Oh, I should point out that while the rejection occurs, the message is still received:\n```\n\nnode test.js\nPUB connected\nSUB connected\nUnhandled rejection Error: Connection in subscriber mode, only subscriber commands may be used\n    at Redis.sendCommand (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/lib/redis.js:500:20)\n    at Redis.selectBuffer (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/lib/commander.js:117:17)\n    at /Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/lib/redis/event_handler.js:184:12\n    at /Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/lib/redis/event_handler.js:35:39\n    at /Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/lib/redis.js:392:7\n    at tryCatcher (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/util.js:26:23)\n    at Promise.successAdapter (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/nodeify.js:23:30)\n    at Promise._settlePromiseAt (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/promise.js:579:21)\n    at Promise._settlePromises (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/promise.js:697:14)\n    at Async._drainQueue (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/async.js:123:16)\n    at Async._drainQueues (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/async.js:133:10)\n    at Immediate.Async.drainQueues [as _onImmediate] (/Users/evantahler/Dropbox/Projects/actionhero/node_modules/ioredis/node_modules/bluebird/js/main/async.js:15:14)\n    at processImmediate [as _immediateCallback] (timers.js:383:17)\n\n```\n. Thanks for the quick patch! \n. ",
    "seunlanlege": "@luin getting this same error with this code\n\n. @luin here are the logs\n\n. ",
    "jomeier": "Found the problem in my own code:\nI was calling this code:\nredis.on('message', function(channel, message) {\n  // only got one message when updating \"Example:Meas\"\n});\nevery time when I called subscribe().\n. ",
    "mkozjak": "Thanks for responding so quickly, @luin :)\nThis works, yes. I'll try sending some info request if available, right after connecting to check up on that.\nBut why aren't you emitting a new error upon this? Or an authError? Since you emit it if the password is wrong, you should emit it when there's no password (and the server requires it).\nYou are definitely receiving 'NOAUTH' on connect. It gets to ReplyParser.prototype._parseResult. So you could handle it, maybe.\nRegards!\n. Thanks!\n. ",
    "guilhermef": "You're right, they're not instrumenting iorelic the right way.\nI've opened a ticket on NewRelic too,\nbut I just left this issue as a reminder for others.\n. I've started a PR on node-newrelic to add ioredis support: https://github.com/newrelic/node-newrelic/pull/212\nA few tests aren't passing, but I'm already trying to fix it.\n. It's done, now new relic supports ioredis, I just need them to accept the PR.\n. ",
    "pwnsrma": "Great work @guilhermef, now this support is live in node-newrelic v1.26.2\n. ",
    "ozomer": "It is very hard to implement the optimistic-lock pattern (1. watch 2. get 3. unwatch or do a multi-transaction) using ioredis or any other async library. \"Watch\" is per client, which means that if you call watch and while waiting for the promise a different part of your code executes a different multi-transaction, the other transaction will release the watch (and it will run/fail according to the changes in the irrelevant watched key).\nIn my opinion there should be an api called redis.watchLock(...) (i.e. not to break existing code that uses watch(...)), which should return a promise to a watchClient object. This object should keep a set of all the keys currently being watched (watchClient._watchedKeys), and support all the api of ioredis. Calling watchClient.watch(...) should add the new key to watchClient._watchedKeys, calling watchClient.unwatch(...) should remove the key, and calling watchClient.multi(). ... .exec()/.discard() should clean all the keys from watchClient._watchedKeys. Once watchClient._watchedKeys is empty, the watchClient should be marked disabled and all further operations should fail.\nThe \"locking\" mechanism is applied to other parts of the code that might try to call redis.watchLock(...) or redis.multi(). ... .exec() or even redis.multi({pipeline: false}). These commands promises should be postponed until the watchClient is disabled.\nThis will obviously block some of the asynchronous operation, but since watch is per-client I don't see another way to make watch useful.\nP.S. I am relatively new to redis, so if there is some ninja-trick to use optimistic-locking (other then using ioredis-lock) please let me know.. I would like to suggest a different feature that could fix the same problem: dynamically overloading the api with more functions.\nThe current api already provides a \"Buffer\" api for each function: get(...) has getBuffer(...), set(...) has setBuffer(...), hget(...) has hgetBuffer(...) etc. These functions allow to pack/unpack the value in two different ways: string (default) and buffer. I would like to be able to call something like:\nRedis.Command.setDynamicTransformer('Snappy', function decoder(compressedValue) { return uncompress(compressedValue); }, function encoder(uncompressedValue) { return compress(uncompresedValue); }), that will add a third way to pack/unpack the values: getSnappy(...), setSnappy(...).\nThe encoder transformer should operate on the \"values\" of the function, which appear in different positions in each command - for example, in set the \"value\" appears in the second parameter, in hset in the third parameter, and in mset the \"value\" appears in all even parameters. Similarly, get returns a single value to decode and mget returns multiple values to decode.\nThis solution will allow to dynamically decide in the code whether I want to use get, getBuffer, getSnappy, getMsgpack or getJson, instead of writing the transformations in one place and having to adapt it to each command by yourself.\nP.S.\nIt would also be great to be able to make the encoder/decoder functions return a Promise.. ",
    "Pyrolistical": "I have a workaround. Just create multiple instances of the redis client. I tested multiple clients in the same Javascript context and it worked. You would need to figure out what is the non-overlaping set of work in your system. If you are implementing a backend server, then the natural unit of work is a http request. I don't know the performance of creating a new redis instance per backend request. But if that becomes a problem you can always implementing client pooling.. i'll get a pr up. ",
    "jamesdixon": "I double checked and a password is definitely being set. Here's the options being passed into the constructor:\njavascript\n{ \n  host: 'redis.host.com',\n  port: '6379',\n  options: { password: 'MYPASSWORD' } \n}\nIf you'd like to try it, you can use those credentials to see if you can reproduce. There's nothing on there that's sensitive at the moment and I'll reset the key later.\nAs for Medis, my mistake - I accidentally had the SSL port entered. I verified that I am able to connect with Medis using the credentials above.\n. Note that I'm using ioredis through a hapi.js plugin called hapi-ioredis. If you're able to reproduce without the plugin, there could be another issue. However, I am able to connect to my local Redis instance, so not sure what the the problem could be.\n. Yikes. My apologies. When I read the docs, I was reading it as passing host, port, and then an options object with password, forgetting the fact that I was passing an object and not explictly passing params to the constructor. Very sorry, but thank you for your help!\n. @luin thank you! i didn't realize it would be that trivial. cheers \ud83d\udc4d . Thank you!. Not sure what happened but it's working now \ud83d\udc4d . ",
    "xavierchow": "thanks for quick response, closing.\n. ",
    "flaviolivolsi": "Thank you @luin and forgive me for opening this issue, I found that it wasn't related to redis but to the gulp task. The process.exit() prevented the callback from executing. Thank you anyway!\n. ",
    "saschaishikawa": "@klinquist, good catch! But alas, the 'message' event isn't even firing. I checked redis and the channel is definitely being published.\n. :facepalm: That takes care of it. Thank you very much for your help and contribution to ioredis. It's been very useful. :) Closing issue now.\n. ",
    "shiraaa": "Thank u very much!\n. ",
    "devnetf": "Anyone help please? \n. \nHere is the debug log. I am using the newest version: \"2.0.0-rc1\"\nThanks!\n. Thanks for the quick reply. I am not sure about the cluster setup because it is our system admins who set it up. However, when I change the version to \"1.15.1\". The exact same code works. Could it be a bug in v2.0.0? \n. ",
    "bryanlarsen": "Thanks for the hint.   I get this error:  ioredis:SentinelConnector failed to connect to sentinel vagrant:6379 because ReplyError: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command 'CONFIG SET protected-mode no' from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to 'no', and then restarting the server. 3) If you started the server manually just for testing, restart it with the '--protected-mode no' option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside.\nwhich gives me lots of options on how to fix things.   Feel free to close the bug, although I do recommend you surface this bug to the user in non-debug mode because this is a change from 3.0 to 3.2.    It's in the redis changelog, but I didn't notice it earlier.\n. ",
    "zeroone001": "(error) ERR unknown command 'sentinel'\n. @luin I know that i've got redis-cli instead of redis-sentinel\n. ",
    "fedor-wsm": "Actually, protected mode affects on sentinel and has to be added into sentinel config as well. In my case, failover did not happen due to the above error message reported by bryanlarsen.\nprotected-mode no\nMy setup is a cluster with two masters and two slaves on four servers with sentinel installed on each server.\n. @luin, I agree with you. The redis is configured on particular interface in order to communicate with other redis. The issue was that I did not know why Sentinel did not start failover, there were not any logs in its logs. I just did not know that Sentinel should have been configured with the same option becuase there is no protected-mode setting in default config of sentinel. You know sentinel runs on 0.0.0.0 and I did not expect kind of this issue.\nInitially, I removed 127.0.0.1 interface from redis configs and disabled protected-mode.\nBTW, there is no bind setting in default config of sentinel as well. There is just port setting with value 26379.\nActually, my failover did not happen, after a few minutes I got this error: \"-failover-abort-slave-timeout master\" and I have not found a description this state. I hope to figure it out anyway or will try earlier version of redis.\n. ",
    "mperham": "I'm new to the JS world and trying to track down where this error is being swallowed.  The code appears to handle the connect and error events but doesn't seem to call the Promise's reject callback when processing the error here.\nhttps://github.com/luin/ioredis/blob/master/lib/redis.js#L271\n. Ok, I'm just pointing out that the readme says to use that onPossiblyUnhandledRejection code to get unhandled errors.  It does not work for this error.\n. ",
    "NuSkooler": "My apologies -- it seems I can simply remove connectionName from options as this is causing CLIENT SETNAME ...\n. @tuananh Simply removing connectionName does the trick for me - Twemproxy + ioredis working great!. ",
    "tuananh": "@NuSkooler i tried removing connectionName and it seems to work so far. Do you have any other issue with Twemproxy and ioredis?. @luin one question: why when key undefined, you want to calculate slot anyway?. @marvel308 maybe you can use something like redlock (so you don't have to introduce sth new to the stack). each request upon writing to redis must require lock beforehand.. @luin \nI added it but it's not printing any extra error, not even the @@@ string below.\nredis.on('error', function (error) {\n  console.log('@@@@@@@@@@')\n  console.dir(error)\n}). cat node_modules/ioredis/package.json  | grep version show it's at 2.5.0. @luin it was indeed caused by another instance in another file which i forgot to remove the require. Thanks for helping me debug it.\n. my bad. Thank you.. What's the issue with eslint?. You're talking about this? https://github.com/luin/ioredis#offline-queue\n\nvar redis = new Redis({ enableOfflineQueue: false });\n. Looks like he's using a proxy in front of redis. Twemproxy and Dynomite do not support SELECT command.. @MichelDiz you need to compile redis with the module you want to use (prior redis 4). @MichelDiz i played with redisearch before and wrote a dockerfile for it. you may want to take a look and see https://github.com/tuananh/Dockerfile/tree/master/redisearch. why do you worry about the size? you're not going to build and upload it manually are you?. 300KB difference isn't much compare with your code (500KB) + container base size (few MB at least, assuming it's alpine-based).\nIf container has been dropped \n\nIn this case, the response is slow anyway. And i don't think lambda (for example) will download, unzip every single time the func execute. Fair enough.\nBtw, looks like @luin hears you :) https://github.com/luin/ioredis/pull/494. you mean for individual item in the list? no. you can only set ttl to key but not the underlying data structure . try this maybe?\nredis.on('message', async function(channel, message){\nawait myFunc()\n}. similar to #637. \"ioredis\": \"3.2.2\" and redis 4.0.9. Thank you.. ioredis is being rewritten in typescript. of course the published one will be different right?. i suppose one must submit a PR to node-redis-parser first ?. ",
    "ciarans": "@NuSkooler I cant seem to get my twemproxy working, here is my Node code\njs\nconst client = new redis({\n    host: \"twemproxy\",\n    port: 7000,\n    password: process.env.REDIS_RULES_PASSWORD,\n    enableReadyCheck: false\n});\nHere is my yaml\nyml\nleaf:\n  listen: 127.0.0.1:7000\n  hash: fnv1a_64\n  distribution: ketama\n  auto_eject_hosts: true\n  redis: true\n  redis_auth: CrazyRedisAuthPassword\n  server_retry_timeout: 1000\n  server_failure_limit: 3\n  servers:\n   - redis-slave-one:6379:1\n   - redis-slave-two:6378:2\n   - emaster.amazonaws.com:6379:3\nAny help would be appreciated. ",
    "plantain-00": "I checked the size of the full lodash: about 500kB.\nNot only backend, some client programs, for example, electron.js, can use this library too.\nI think it may reduce the size about 0.4MB.\nAn interesting thing is, the node_redis library doesn't use any lodash functions.\n. ",
    "claudio-viola": "Actually there is also another issue.\nIt seems the event ready is not being raised even with what i believe is a valid configuration....\nDo you know why this could happen or Is it a bug?\nPerhaps network level issues? Sentinels unable to communicate correctly ? \nWhy is there no error being raised?\nEDIT: The PASSWORD was not right hence the connection was never ready however no error is thrown in sentinel mode\n. Hi There ,\nso yes I have been doing investigations and I would say there are a few things that may need to be fixed or improved on:\n1) No errors being thrown when failing to authenticate in sentinels mode\n2) No errors being thrown when sentinels are not working correctly? Is quorum reached? Are they agreeing on a master\n3) If the connection is not ready (in this case due to authentication that did not work and ready event not  not set to true) any get operation should probably better fail immediately instead of being added to a queue?\nHow does the client now if there is a problem otherwise? Things keep getting added in the queue..\nioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[localhost:26379]: [empty] -> connecting +5ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,master) +2ms\n  ioredis:redis status[localhost:26379]: connecting -> connect +671ms\n  ioredis:redis status[localhost:26379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,master) +0ms\n  ioredis:redis status[localhost:26379]: ready -> close +354ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[localhost:26379]: close -> end +0ms\n  ioredis:redis status[localhost:6379]: connecting -> connect +0ms\n  ioredis:redis write command[1] -> auth(WRONG_PASSWORD) +1ms\n  ioredis:redis write command[1] -> info() +0ms\n  redis.on('connect') [ \n    connect event emitted \n]\n<---- No error thrown failed to authenticate?\nioredis:redis queue command[1] -> get(SOMEKEY) +14m  <---- Should fail immediately?\n  ioredis:redis queue command[1] -> get(SOMEKEY2) +1ms   <---- Should fail immediately?\n. Forgot to mention version 1.15.0\n. 1) Ok strangely when your password is not valid and you are not connecting to sentinels the error is emitted with the \"error\" event as well.. thaths why it was hard to spot.. I will check if I can see it in the authErrro.\n2) Ok but thats something that you could possibly achieve through sentinel commands cant you?\n3) Not really sure about point 3.. when would you disconnect and how do you know that you have to disconnect if there is a error emitted elsewhere and you are trying to do a get? \nin this case the error is an authError however connection is OK hence ioredis just queues the commands and never tell you that you will never be able to get/set since the authentication failed?\nThanks!!\n.  thanks for the explanation!\n3) I was  also actually  listening for the AuthError event but wasn't really doing anything there as I thought that failure to authenticate would not require any action aside from logging the error\nDon't you think ioredis should emit both authError and error,  or otherwise only error events?\nWhat was the reason to decide to have to separate events for authentication errors and errors?\nonAuthError() {\nemit('authError')\nemit('error)\n}\nvs\nonAuthError() { \n  emit('error',errorCode,errorMessage) \n}\n2) I haven't tried but I wonder what happens when one or more of the sentinel go down and the quorum is not reached on who is the master. Would ioredis handle this or raise an event?\n. Oh Sorry I just saw you made a PR about emitting both errors under error :) Forget previous comment!\n. ",
    "cmcewen": "Ah actually was able to get some logs with DEBUG on\n```\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[localhost:6379]: [empty] -> connecting\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[redis-sentinel:26379]: [empty] -> connecting\nFri, 06 May 2016 16:05:07 GMT ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster)\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[localhost:6379]: [empty] -> connecting\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[redis-sentinel:26379]: [empty] -> connecting\nFri, 06 May 2016 16:05:07 GMT ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster)\nFri, 06 May 2016 16:05:07 GMT ioredis:redis queue command[0] -> subscribe({q}:events)\nListening at port 80\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[192.168.185.195:26379]: connecting -> connect\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[192.168.185.195:26379]: connect -> ready\nFri, 06 May 2016 16:05:07 GMT ioredis:connection send 1 commands in offline queue\nFri, 06 May 2016 16:05:07 GMT ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster)\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[192.168.185.195:26379]: connecting -> connect\nFri, 06 May 2016 16:05:07 GMT ioredis:redis status[192.168.185.195:26379]: connect -> ready\nFri, 06 May 2016 16:05:07 GMT ioredis:connection send 1 commands in offline queue\nFri, 06 May 2016 16:05:07 GMT ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster)\nFri, 06 May 2016 16:05:07 GMT ioredis:connection error: Error: connect ENETUNREACH 192.168.180.0:6379 - Local (0.0.0.0:0)\nevents.js:154\n      throw er; // Unhandled 'error' event\n      ^\nError: connect ENETUNREACH 192.168.180.0:6379 - Local (0.0.0.0:0)\n    at Object.exports._errnoException (util.js:856:11)\n    at exports._exceptionWithHostPort (util.js:879:20)\n    at connect (net.js:847:16)\n    at net.js:933:9\n    at _combinedTickCallback (node.js:370:9)\n    at process._tickCallback (node.js:401:11)\n```\n. Yeah that makes sense - might be some momentary network problems when the container first starts, but I just thought all the connection errors were silent and the connection would be retried if no error listener was attached. This error crashes the app\n. ",
    "tutman96": "Very well, thanks for the explanation.\n. ",
    "TeeAaTeeUu": "So defaulting it to be true even for the non-sentinel connections, or removing them totally? I don't know about that, but for our use case they are useless especially with the https://github.com/redis/hiredis-node\n. Thanks for the guidance @luin!\n. ",
    "trevnorris": "Have any benchmarks been done with this? There is no API that allows passing a char* (though I wish there was). The call to JSON.parse() would automatically call .toString() on the Buffer. Example of what's happening:\njs\n var b = { toString: () => '{\"foo\":\"bar\"}' };\nJSON.parse(b);\n// output: { foo: 'bar' }\nSo I'm honestly unsure how one could be faster than the other.\n. @AVVS \n\nI must be confusing this with 1-byte strings and trick to decode it to binary representation before json.parse and then creating a buffer before we put to to db.\n\nNever thought about using it that way. If you could tell your source for the data to encode in one-byte (i.e. latin1 or iso-8859-1, not win-1252) then converting to JS string from Buffer would be faster.\n. ",
    "spiffytech": "Done.\n. ",
    "an-sh": "Here are more details/reasoning about it.\nI did not mention it explicitly, but the main point is to have an\nerror value constructed even if only one command has failed. So the\nfollowing is always true: either you have an error or all results of a\ntransaction.\nSo arguments for it:\n1. Partially applied transaction is an edge case, that should not\n   happen if Redis is used correctly (Redis is using this argument too,\n   to rationalise an absence of rollback support). Still anyone who wants\n   to handle this edge case can do it, but in an error handling code.\n2. It will actually make error handling much easier, as it will be\n   like any other node API. Something semantically resembling\n   Promise.all and an actual transaction in general. Also in terms of\n   syntax just a common use of callbacks (first argument check) or\n   promises (attaching a catch handler).\nArgument against:\n1. It is just a very big breaking API change.\n2. Pipelining is using the same API (explicitly stating that error is\n   always null), on the other hand pipelining has a different\n   semantics.\nAlso Redis docs states that it is up to a library how to handle this\nsituation:\n\nEXEC returned two-element Bulk string reply where one is an OK code\n  and the other an -ERR reply. It's up to the client library to find a\n  sensible way to provide the error to the user.\n\nProbably there is more things to consider.\nAt least could some example like this be added to the documentation:\njavascript\nredis.multi().set('key1', 'val1').hget('key1', 'val1').exec()\n  .spread( (r1, r2) => {\n    // We get here, with the following results:\n    // [ null, 'OK' ] [ { [ReplyError: WRONGTYPE Operation against a key holding the wrong kind of value]\n    //     name: 'ReplyError',\n    //     message: 'WRONGTYPE Operation against a key holding the wrong kind of value' } ]\n    console.log(r1, r2);\n  })\n  .catch(e => {\n    // We DON'T get here.\n    console.log(e);\n  });\nIn my opinion, this very non-intuitive from a promise specification\nand transaction meaning points of view (the same to a certain degree\ncan be applied to node style callbacks).\n. The first reason, there is no way to run application logic inside\nit. So there is always tradeoff to consider about getting data ahead\nof time and a redis server load (and changing more code if something\nnew is added).\nBut the main reason, there is no way an easy to use pipeline across\nevent handlers. So, for example, we have:\njavascript\nfunction handler(eventData) {\n  redis.pipeline().SOMECOMMANDS.exec().then(results => {\n    runAppLogic(eventData, results);\n  });\n}\nJust one pipeline per event (http request, websocket message, etc). If\nthere is no many events - it is ok. But if there is just:\njavascript\nfunction handler(eventData) {\n  redis.get('key').then(results => {\n    runAppLogic(eventData, results);\n  });\n}\nAnd lots of events? There is no transparent way to use a pipeline in\nthat case, beside reimplementing a command queue inside an\napplication.\nAuto pipelining will eliminate the need to consider low level things\ninside an application logic, and moreover will give an optimal result\nin any case. Like pooling in pg/mysql node clients, you can just\nenable it, and get a much better performance, without any need of low\nlevel connection stuff management/optimisations.\nAuto pipelining addressing exactly the same issue as connection\npooling(in redis case), but with a different approach. I'm not sure\nwhich one is the best. There is an open issue about Connection\nPooling, so probably these two should be merge somehow.\n. ",
    "lo1tuma": "I would also like to see a better transaction result format for the promise-based API.\nWhat do you think about introducing a custom error type, e.g. PartiallyAppliedTransactionError which could have additional fields that point exactly to the commands that have been applied or failed, for example:\njs\nredis.multi().set('key1', 'val1').hget('key1', 'val1').exec()\n    .catch(error => {\n        if (error instanceof redis.PartiallyAppliedTransactionError) {\n            console.log(error.commandErrors); // \u21e8 [null, [ReplyError: WRONGTYPE Operation against a key holding the wrong kind of value]]\n            console.log(error.commandResults); // \u21e8 ['OK', null]\n        }\n    });\n. Wouldn\u2019t you get a lot of cross-slots errors if you do multi-key ops against a redis cluster?\n. ",
    "bgrieder": "This was du to an entry expiring between an HMGET and an INCR.\nObviously the more latency, the higher chances this was to occur.\nMany Thanks\n. ",
    "hzxuzhonghu": "I know what you mean. But what I said is faillover by hand, connection between client and redis master is alive and how client know?\n. Ok\uff0c get  it. Thanks !\n. ",
    "9point6": "Ah right! no worries, I'll do that now.\n. @luin done!\n. ",
    "enst": "The example doesn't mention the Redis command \"EXISTS\".\n. ",
    "bi-kai": "@luin calling exists in a for loop, the result in callback or promise is only boolean, but how to find which key is or not exist?? Will this supply a sync way?. javascript\n_redis.keys(\"*\").then()\nThis works, my bad.. @luin \u6211\u8fd8\u4e0d\u719f\u6089es7\u7684\u7279\u6027\uff0c\u9488\u5bf9\u540e\u4e00\u79cd\u5199\u6cd5\u6709\u51e0\u4e2a\u7591\u95ee\uff1a\n1. redis.hgetall.bind\u7684.bind\u7528\u6cd5\u5728\u54ea\u91cc\u80fd\u67e5\u5230\uff1f\u76ee\u524d\u62a5\u9519\u5982\u4e0b\uff1a\n\n\u8fd9\u4e2a\u53c2\u8003async\uff0c\u6539\u6210redis.keys(\"*\").map(key => redis.hgetall(key,function(){...}))\u5c31\u597d\u4e86\uff0c\u662f\u5426\u5e94\u8be5\u8fd9\u4e48\u5199\uff1f\n\n\n\u5176\u4ed6\u8bed\u8a00\u6bd4\u5982java\u6267\u884credis api\u597d\u50cf\u662f\u540c\u6b65\u62ff\u5230\u7ed3\u679c\u7684\uff0cjs\u5f02\u6b65\u9020\u6210\u4e86\u56f0\u6270\uff1a\u4e0d\u7ba1\u7528for\u8fd8\u662fmap\uff0c\u8fd4\u56de\u503c\u90fd\u662f\u5728\u4e00\u4e2a\u56de\u8c03\u91cc\u7684\uff0c\u800c\u671f\u671b\u7684\u7ed3\u679c\u662f\u628a\u5404\u4e2a\u56de\u8c03\u4e2d\u7684\u7ed3\u679c\u62fc\u63a5\u5230\u4e00\u8d77\uff0c\u624d\u80fd\u4f9b\u540e\u7eed\u4f7f\u7528\uff0c\u8fd9\u662f\u5426\u7528pipeline\u65b9\u5f0f\u89e3\u51b3\uff1f\n\n\n\u6bd4\u5982\u67e5\u8be210\u4e2akey\u7684\u503c\uff0c\u6211\u60f3\u5148\u5224\u65ad\u5176\u662f\u5426\u5b58\u5728\uff0cfor\u5faa\u73af\u8c03\u7528exists\uff0c\u5728\u56de\u8c03\u4e2d\u53ea\u80fd\u5f97\u52300 or 1\uff0c\u6211\u5e76\u4e0d\u77e5\u9053\u54ea\u4e2akey\u662f0 or 1\uff0c\u8fd9\u4e2a\u662f\u5426\u4e5f\u4f7f\u7528pipeline\uff0c\u5c06\u7ed3\u679c\u548c\u539fkey\u6570\u7ec4\u4e00\u4e00\u5bf9\u7167\uff1f\n\n\n\u8c22\u8c22\u3002. The above discussion could be summarized into two points:\n1. async;\n2. callback: query results are in different callback;\nSo I use promise to solve async, use eventproxy to solve separated results, like this:\n```javascript\nfunction getAppInfo(appList) {\n    var _redis = getRedisObj();\n    changeDB(1);\nreturn new Promise(function(resolve, reject) {\n    getAvaliableKeys(appList).then(function(d) {\n        return getKeysData(d);\n    }).then(function(d){\n        resolve(d);\n    });\n});\n\n// get all keys.\nfunction getAvaliableKeys(appList) {\n    return new Promise(function(resolve, reject) {\n        var _aKey = [];\n        // query specific apps' new version. \n        if (appList) {\n            var _multi = _redis.multi();\n            for (var i = appList.length - 1; i >= 0; i--) {\n                _multi.exists(appList[i]);\n            }\n            _multi.exec(function(err, aExistStatus) {\n                appList.map(function(elem, idx) {\n                    if (aExistStatus[idx][1]) {\n                        _aKey.push(elem);\n                    }\n                });\n                resolve(_aKey);\n            }); // end of exec().\n\n        } else { // query all apps' new version.\n            _redis\n                .keys('*')\n                .then(function(d) {\n                    resolve(d);\n                });\n        }\n    });\n}\n\n// query key data.\nfunction getKeysData(_aKey) {\n    return new Promise(function(resolve, reject) {\n        ep.after(\"redisQuery\", _aKey.length, function(aAppInfo) {\n            resolve(aAppInfo);\n        });\n\n        _aKey.map(key => _redis.hgetall(key, function(err, oAppInfo) {\n            ep.emit(\"redisQuery\", oAppInfo);\n        }));\n    });\n\n} // end of function.\n\n}\n```\nI just wander, is there any other best practices?. @ccs018 hi, I just want to get all the key and value in a DB.\nBefore get the value, I need to know what keys it holds, some time the keys are params, I want to know there's existence for safity.\n590 . @ccs018 yes, I am using keys * now, #590.\nThe codes are quite long to solve:\n1. async;\n2. query results are in different callback;\nIf it's sync, code will be more cleaner.. \u55ef \u6211\u6d4b\u8bd5\u4e5f\u662f\u8fd9\u6837\u3002\nexec\u540e\u4e3a\u4ec0\u4e48\u8fd8\u5b58\u50a8\u7740\u8fd9\u4e9b\u6570\u636e\u5462\uff0c\u662f\u5426\u662f\u8d44\u6e90\u6d6a\u8d39\u8fd8\u662f\u6709\u4ec0\u4e48\u522b\u7684\u7528\u9014\uff1f. ",
    "dansaferide": "redis.exists('myKey').then( exists => { console.log('Does it exist?', (exists === 1)) }. ",
    "goldfire": "Working now, thanks!\n. ",
    "carterfort": "That's perfect, thank you. Sorry for the incorrect label.\n. ",
    "gbaquedano": "I was about to write a similar thread. In my scenario I have plenty of objects that get randomly updated and each one of them raises an event with the new data. I dont find an easy way to use pipeline in this scenario. I was even thinking to push all the updates to a buffer and then pipeline it all together for every fixed interval of time, but I thought someone had already faced this with ioredis.\n. ",
    "DyncKevin": "Hi, I have an question, for example, I use pipeline like this, \nredis.pipeline().set('foo', 'bar').get('foo').exec();\nwhat happen if cmd 'set' failed? does cmd 'get' continue exec? will the pipeline return or continue?\nthe 'failed' I mean in the callback, 'err' is not null. ",
    "scality-gdoumergue": "Hello,\nI would like to re-consider authentication upon redis-sentinel, because it's now available thanks to this commit, available since redis 5.0.\nThanks a lot. ",
    "oaleynik": "@luin can you please explain what do you mean under \"tunnel\"?\n. I think it makes sense (unless we talk about different things). Let me explain what I mean. To save some memory (actually - a lot of memory) prior saving value to redis I usually compress it and encode as base64 string. When I fetch it back I decode and uncompress it. In the case if original value is serialized JSON string (which is very usual) - such transformation allows to transfer and store up to ten times less data.\nWould be nice to have support of such transformation. For instance by passing snappy: true via options to Redis constructor.\nQuick example:\n``` javascript\nfunction pack (value) {\n  return new Promise((resolve, reject) => {\n    snappy.compress(JSON.stringify(value), (err, packed) => {\n      if (err) {\n        return reject(err);\n      }\n  resolve(packed.toString('base64'));\n});\n\n});\n}\nfunction unpack (value) {\n  return new Promise((resolve, reject) => {\n    snappy.uncompress(Buffer.from(value, 'base64'), (err, unpacked) => {\n      if (err) {\n        return reject(err);\n      }\n  const unpackedString = unpacked.toString();\n\n  resolve(JSON.parse(unpackedString));\n});\n\n});\n}\n``\n. @luin Yep! Exactly what I'm looking for. I was thinking to somehow use transformers for this purpose, but I didn't figure out how to friend them with promisified API and extract into something reusable. But hooks sounds great! Or maybe some plugins ecosystem when you can do something likeclient.use(redisSnappy | redisDeflate | whatever)`. \n. ",
    "haozxuan": "\u5176\u5b9e\u6211\u8ba4\u4e3a\u8be5\u95ee\u9898\u53ef\u4ee5\u6539\u4e3a\u201ccluster\u6a21\u5f0f\u4e0b\uff0c\u662f\u5426\u5141\u8bb8\u7528\u6237\u53ea\u63d0\u4f9b\u4e00\u4e2a\u8fde\u63a5\u8282\u70b9\u201d\u3002\n\u5f53\u6211\u770b\u5230ioredis\u652f\u6301\u7ed9\u81ea\u5df1\u53d1\u73b0\u5269\u4f59\u8282\u70b9\u65f6\uff0c\u6211\u7b2c\u4e00\u65f6\u95f4\u60f3\u5230\u4e86mongodb\u9a71\u52a8\u7a0b\u5e8f\uff0c\u5f53\u521d\u4ed6\u4e5f\u662f\u5141\u8bb8\u53ea\u63d0\u4f9b\u4e00\u4e2a\u8282\u70b9\uff0c\u7136\u540e\u81ea\u5df1\u53d1\u73b0\u5176\u4f59\u8282\u70b9\uff0cbut\uff0c\u597d\u666f\u4e0d\u957f\uff0c\u6ca1\u8fc7\u591a\u4e45\u5c31\u51fa\u73b0\u4e86\u5355\u8282\u70b9\u7684\u62d3\u6251\u7ed3\u6784\u548c\u526f\u672c\u96c6\u7684\u62d3\u6251\u7ed3\u6784\u4e0d\u540c\u9020\u6210\u627e\u4e0d\u5230\u4e3b\u8282\u70b9\u7684\u95ee\u9898\uff0c\u4e8e\u662f\u5b98\u65b9\u53d6\u6d88\u4e86\u53ea\u63d0\u4f9b\u4e00\u4e2a\u8282\u70b9\u7684\u65b9\u5f0f\u3002\u4ece\u6574\u4e2a\u4ea4\u66ff\u8fc7\u7a0b\u6765\u770b\uff0c\u867d\u7136\u53ea\u63d0\u4f9b\u4e00\u4e2a\u8282\u70b9\u770b\u8d77\u6765\u66f4smart\uff0cbut\uff0c\u903b\u8f91\u4e0d\u591f\u6e05\u6670\uff08\u6307\uff0c\u9a71\u52a8\u5c42\u9762\u8981\u81ea\u8bc6\u522b\u90e8\u7f72\u67b6\u6784\uff0c\u53ef\u80fd\u4f1a\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u5f00\u9500\uff09\uff0c\u6240\u4ee5\u8fd8\u662f\u66f4\u504f\u5411\u4e8ecluster\u5c31\u81f3\u5c11\u63d0\u4f9b2\u4e2anode\uff0c\u5982\u679c\u63d0\u4f9b\u4e00\u4e2anode\uff0c\u53bb\u8fde\u63a5cluster\uff0c\u8fd8\u662f\u629b\u5f02\u5e38\u7684\u597d\uff0c\u907f\u514d\u5f00\u53d1\u8005\u76f2\u76ee\u7684\u76f8\u4fe1\u8be5\u8fde\u63a5\u65b9\u5f0f\uff1b\n. ",
    "Tomino2112": "In my code I have:\nredis.on(\"error\", function(err){\n    console.log(err);\n});\nBut still my app crashes (after taking down redis server) with exception:\n```\nevents.js:160\n      throw er; // Unhandled 'error' event\n      ^\nError: Redis connection to localhost:6379 failed - connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nProcess finished with exit code 1\n```\n. Yes thats what it says in readme, yet still it is actually throwing errors which stop my app. I am not doing anything unusual, as simple as:\n```\nvar redis = new Redis({\n    port: config.redis.port,\n    host: config.redis.host,\n    family: config.redis.family,\n    password: config.redis.password,\n    db: config.redis.db,\n    showFriendlyErrorStack: (process.env.NODE_ENV !== \"production\"),\n});\nredis.on(\"error\", (error) => {\n    debug(\"Redis connection error\", error);\n});\n``\n.\"version\": \"2.2.0\"`\n. This is the whole code that I use to connect to redis. (Typescript)\n```\nimport * as Redis from \"ioredis\";\nimport {main as config} from \"../config/main\";\nimport IRedis = IORedis.Redis;\nlet debug: Debugger = require(\"debug\")(\"ipp:redis\");\n// Handle unhandled exceptions\nRedis.Promise.onPossiblyUnhandledRejection((error: any) => {\n    debug(\"Redis unhandled error \", error);\n});\ndebug(\"creating default redis connection\");\nexport const redis: IRedis = new Redis({\n    port: config.redis.port,\n    host: config.redis.host,\n    family: config.redis.family,\n    password: config.redis.password,\n    db: config.redis.db,\n    // keyPrefix: config.redis.keyPrefix, // @todo https://github.com/luin/ioredis/issues/325\n    showFriendlyErrorStack: (process.env.NODE_ENV !== \"production\"),\n});\n// Handle connection error\nredis.on(\"error\", (error) => {\n    debug(\"Redis connection error\", error);\n});\n```\n. If I kill the server why app is running, I also get list of reconnection tries but eventually app is killed. If I start the app without server running it gives just one error and dies. \nI am not using emit anywhere throughout my app\n. Here is full output from console when I kill server during app run\n```\nioredis:redis status[127.0.0.1:6379]: ready -> close +3s\nioredis:connection reconnect in 2ms +1ms\nioredis:redis status[127.0.0.1:6379]: close -> reconnecting +1ms\nioredis:redis status[127.0.0.1:6379]: ready -> close +3ms\nioredis:connection reconnect in 2ms +0ms\nioredis:redis status[127.0.0.1:6379]: close -> reconnecting +1ms\nioredis:redis status[127.0.0.1:6379]: reconnecting -> connecting +1ms\nioredis:redis status[127.0.0.1:6379]: reconnecting -> connecting +2ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +5ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +3ms\nioredis:connection reconnect in 4ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +1ms\nipp:redis Redis connection error +1ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +22ms\nioredis:connection reconnect in 4ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +0ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +3ms\nioredis:connection reconnect in 6ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +0ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 6ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +2ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 8ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +0ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\nipp:redis Redis connection error +3ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +3ms\nioredis:connection reconnect in 8ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +1ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 10ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +3ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +3ms\nioredis:connection reconnect in 10ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +2ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 12ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +4ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +5ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +3ms\nioredis:connection reconnect in 12ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +0ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 14ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +7ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 14ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +3ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 16ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +7ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\nipp:redis Redis connection error +1ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 16ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +5ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 18ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +9ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +2ms\nioredis:connection reconnect in 18ms +1ms\nioredis:redis status[localhost:6379]: close -> reconnecting +0ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +6ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +2ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 20ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +8ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +3ms\nipp:redis Redis connection error +0ms { Error: connect ECONNREFUSED 127.0.0.1:6379\n    at Object.exports._errnoException (util.js:1007:11)\n    at exports._exceptionWithHostPort (util.js:1030:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n    code: 'ECONNREFUSED',\n        errno: 'ECONNREFUSED',\n        syscall: 'connect',\n        address: '127.0.0.1',\n        port: 6379 }\nioredis:redis status[localhost:6379]: connecting -> close +1ms\nioredis:connection reconnect in 20ms +0ms\nioredis:redis status[localhost:6379]: close -> reconnecting +1ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +7ms\nioredis:redis status[localhost:6379]: reconnecting -> connecting +15ms\nioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6379 +0ms\n    [ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\nevents.js:160\nthrow er; // Unhandled 'error' event\n^\nError: Redis connection to localhost:6379 failed - connect ECONNREFUSED 127.0.0.1:6379\nat Object.exports._errnoException (util.js:1007:11)\nat exports._exceptionWithHostPort (util.js:1030:20)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1080:14)\n``\n.Exiting...listener count 1`\n. It is not really about multiple applications, its more about being able to programatically use set of keys and handle them without having to hard code them at any point.\n. I think I understand, Its no problem to just not use it. THanks\n. ",
    "hellish": "hello,\ni'm getting the error as well. I have both\nRedis.Promise.onPossiblyUnhandledRejection((error) ...\nand\nredis.on('error', function() {\n    debug('redis error caught');\n});\nThe error occurs when redis (sentinel slave) machine dies or reboots. I stay connected to the master but my app crashes with the following error\n```\nredis error caught\nevents.js:160\n      throw er; // Unhandled 'error' event\n      ^\nError: connect ECONNREFUSED 127.0.0.1:6666\n    at Object.exports._errnoException (util.js:1022:11)\n    at exports._exceptionWithHostPort (util.js:1045:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1087:14)\n```\nhas anyone solved this?. @luin i am running master at 127.0.0.1:5555 and slave 127.0.0.1:6666 and sentinel at 127.0.0.1:7000.\nthe setup works fine with the sentinel and i can connect to the sentinel with the code available in the documentation.\nvar redis = new Redis({\n  sentinels: [{ host: '127.0.0.1', port: 7000 }],\n  name: 'mymaster'\n});\nonce i kill the slave node app dies.\nIts the simplest sentinel setup with 3 instances.\nI do not know how can I give you a  reproducible example. \n. ```\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:redis write command[0] -> sentinel(sentinels,mymaster)\nTue, 31 Jan 2017 13:41:06 GMT ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ]\nTue, 31 Jan 2017 13:41:06 GMT ioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6666\nevents.js:160\n      throw er; // Unhandled 'error' event\n      ^\nError: connect ECONNREFUSED 127.0.0.1:6666\n    at Object.exports._errnoException (util.js:1022:11)\n    at exports._exceptionWithHostPort (util.js:1045:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1087:14)\nTue, 31 Jan 2017 13:41:08 GMT ioredis:redis status[127.0.0.1:7000]: [empty] -> connecting\nTue, 31 Jan 2017 13:41:08 GMT ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster)\nTue, 31 Jan 2017 13:41:08 GMT ioredis:redis status[127.0.0.1:7000]: [empty] -> connecting\nTue, 31 Jan 2017 13:41:08 GMT ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster)\n```. @luin the code u gave me runs ok. and the app is not crashing. it produces the following\nioredis:redis write command[0] -> ping() +979ms\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis write command[0] -> ping() +1s\nnull 'PONG'\n  ioredis:redis status[127.0.0.1:6666]: ready -> close +554ms\n  ioredis:connection reconnect in 2ms +1ms\n  ioredis:redis status[127.0.0.1:6666]: close -> reconnecting +0ms\n  ioredis:redis status[127.0.0.1:6666]: reconnecting -> connecting +4ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 10 +1ms\n  ioredis:redis status[127.0.0.1:7000]: [empty] -> connecting +10ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster) +0ms\n  ioredis:redis status[127.0.0.1:7000]: connecting -> connect +1ms\n  ioredis:redis status[127.0.0.1:7000]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster) +0ms\n  ioredis:redis write command[0] -> sentinel(sentinels,mymaster) +1ms\n  ioredis:SentinelConnector sentinels [ { host: '127.0.0.1', port: 7000 } ] +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:6666 +2ms\n  ioredis:redis status[127.0.0.1:7000]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[127.0.0.1:7000]: close -> end +1ms\n  ioredis:redis status[localhost:6379]: connecting -> close +1ms\n  ioredis:connection reconnect in 4ms +0ms\n  ioredis:redis status[localhost:6379]: close -> reconnecting +0ms\n  ioredis:redis status[localhost:6379]: reconnecting -> connecting +4ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 10 +1ms\n...\ni noticed two things with this:\n since i kill the slave it says All sentinels are unreachable although sentinel runs ok\n and at the end of the logs says that it tries to connect to localhost:6379 which is weird since localhost:6379 is not specified anywhere and since nothing is running there it fails to connect\nat the end it reconnects fine once the slave is up and running\n. @luin I'll check again my config. I assumed its correct since I see the following.\n```\n127.0.0.1:7000> sentinel master mymaster\n 1) \"name\"\n 2) \"mymaster\"\n 3) \"ip\"\n 4) \"127.0.0.1\"\n 5) \"port\"\n 6) \"5555\"\n 7) \"runid\"\n 8) \"73b4db1a696185bd195c0ea0f5aee86a257b193d\"\n 9) \"flags\"\n10) \"master\"\n11) \"link-pending-commands\"\n12) \"0\"\n13) \"link-refcount\"\n14) \"1\"\n15) \"last-ping-sent\"\n16) \"0\"\n17) \"last-ok-ping-reply\"\n18) \"911\"\n19) \"last-ping-reply\"\n20) \"911\"\n21) \"down-after-milliseconds\"\n22) \"9000\"\n23) \"info-refresh\"\n24) \"1008\"\n25) \"role-reported\"\n26) \"master\"\n27) \"role-reported-time\"\n28) \"61891660\"\n29) \"config-epoch\"\n30) \"214\"\n31) \"num-slaves\"\n32) \"1\"\n33) \"num-other-sentinels\"\n34) \"0\"\n35) \"quorum\"\n36) \"1\"\n37) \"failover-timeout\"\n38) \"180000\"\n39) \"parallel-syncs\"\n40) \"1\"\n```\nit finds 1 master which has 1 slave.. ",
    "mwain": "That works \ud83d\udc4d \n. ",
    "gohiei": "There are some problems in my cluster configuration.\nThe connections between nodes depends on 127.0.0.1.\nAfter I fixed 127.0.0.1 to 192.168.1.101, the problem is solved.\nBefore (cluster nodes)\nf2b443faf4e2624fa54fd6ca2a79b378b7ce9593 127.0.0.1:30004 slave 8fb9494ecd03e21a7d95e815ff427f2e3de2e14c 0 1465829431475 4 connected\nee3275cbf1c7947392f69a31526fb153fae31a29 127.0.0.1:30002 master - 0 1465829431677 2 connected 5461-10922\nd01224ff1ae0ee1affc71d2b89ebb00b31257951 127.0.0.1:30006 slave 1e7d13ff56299ff871913e6e2e1e4c0287b220dd 0 1465829431375 6 connected\n8fb9494ecd03e21a7d95e815ff427f2e3de2e14c 127.0.0.1:30001 myself,master - 0 0 1 connected 0-5460\n927c7951e5892390df848bafc0a550df35f15280 127.0.0.1:30005 slave ee3275cbf1c7947392f69a31526fb153fae31a29 0 1465829431677 5 connected\n1e7d13ff56299ff871913e6e2e1e4c0287b220dd 127.0.0.1:30003 master - 0 1465829431375 3 connected 10923-16383\nAfter\nf283f6dba4bfdbeefe7db33e8bd71546491531d6 192.168.1.101:30003 master - 0 1465829358324 3 connected 10923-16383\n412dd386085bed79ab0e33b76e82a4a31391e793 192.168.1.101:30004 slave b4c8f465245db8b204bd6c8e0f427c9810950692 0 1465829358324 4 connected\nb4c8f465245db8b204bd6c8e0f427c9810950692 192.168.1.101:30001 myself,master - 0 0 1 connected 0-5460\n17781a506cfae8bd4db929f893905ca42118e720 192.168.1.101:30002 master - 0 1465829358324 2 connected 5461-10922\n92a8f8925ba4fec80f17ea00603760c4a1af383b 192.168.1.101:30005 slave 17781a506cfae8bd4db929f893905ca42118e720 0 1465829358324 5 connected\n7378315b05b38388e293cff14588ba48b4cfc8dc 192.168.1.101:30006 slave f283f6dba4bfdbeefe7db33e8bd71546491531d6 0 1465829358324 6 connected\n. ",
    "olglay": "We are using generic-pool\n. ",
    "geuis": "thanks @luin \n. ",
    "huangyingjie": "\u767b\u5f55redis \u5ba2\u6237\u7aef\u624b\u52a8\u6267\u884c\u4e5f\u4e0d\u652f\u6301\uff0c\u5e94\u8be5\u662fredis\u672c\u8eab\u5c31\u4e0d\u652f\u6301\uff0c\u6709\u53d8\u901a\u7684\u529e\u6cd5\u5417\n. ",
    "zui-coding": "\u57fa\u4e8ejedis 2.9.0 \n\u53ef\u4ee5\u7528scan \u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e2a\u65b9\u6cd5\u4e5f\u662f\u5751\u3002\u770b\u4e86\u4ed6\u7684\u6e90\u7801 \uff0c\u7b80\u76f4\u4e86\uff0c\u5224\u65adpattern \u662f\u5426\u662f {} \u5305\u88f9\u7684\uff0c\u5982\u679c\u662f\uff0c\u5c31\u8bf4\u660e\u8fd9\u4e2a pattern \u4f20\u7684\u6b63\u786e\uff0c\u4f46\u662f \u53d6\u503c\u7684\u65f6\u5019\uff0c\u7adf\u7136 \u6ca1\u6709 \u628a{} \u53bb\u6389\u3002\u6bd4\u5982 \u6211redis \u91cc\u6709 test_1,test_2,test_3 \u8fd9\u4e09\u4e2akey\uff0c\u7528\u7684\u65f6\u5019\u4ed6\u8ba9\u4f60\u8fd9\u6837 \u7a7f \u6b63\u5219 {test_*} ,\u8981\u7528 {}\u5305\u88f9\u3002but ,\u4e25\u91cd\u7684but \u5c31\u662f \uff0c\u4ed6\u53d6\u503c\u7684\u65f6\u5019\uff0c\u7adf\u7136\u6ca1\u6709 \u628a {} \u53bb\u6389\uff0c \u76f4\u63a5\u7528{test_*} \u53d6\u503c\uff0c\u8fd9\u80fd\u53d6\u5230\u503c\u554a\uff1f\uff01 \u6211debug\u7684\u65f6\u5019\uff0c\u628a {}\u53bb\u6389\u5c31\u53ef\u4ee5\u4e86\u3002\u54ce\uff0c\u65e0\u529b\u8bf4\u4f5c\u8005\u7684\u610f\u56fe\u4e86\u3002\u54ce\uff0c\u6211\u60f3\u964d\u4f4e\u7248\u672c\uff01\uff01. ",
    "SacDin": "On client connection:\nroot@local:/home/local/app# DEBUG=ioredis:* node app.js\n  ioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:redis status[192.168.1.6:6381]: [empty] -> wait +4ms\n  ioredis:redis status[192.168.1.6:6382]: [empty] -> wait +2ms\n  ioredis:redis status[192.168.1.6:6383]: [empty] -> wait +0ms\n  ioredis:cluster getting slot cache from 192.168.1.6:6382 +1ms\n  ioredis:redis status[192.168.1.6:6382]: wait -> connecting +1ms\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[192.168.1.6:6383]: wait -> connecting +0ms\n  ioredis:redis status[192.168.1.6:6382]: connecting -> connect +6ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[192.168.1.6:6383]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[192.168.1.6:6382]: connect -> ready +3ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n  ioredis:redis status[192.168.1.6:6383]: connect -> ready +0ms\n  ioredis:redis status[192.168.1.6:6384]: [empty] -> wait +1ms\n  ioredis:redis status[192.168.1.6:6386]: [empty] -> wait +1ms\n  ioredis:redis status[192.168.1.6:6385]: [empty] -> wait +0ms\n  ioredis:cluster status: connecting -> connect +0ms\n  ioredis:redis status[192.168.1.6:6381]: wait -> connecting +1ms\n  ioredis:redis queue command[0] -> cluster(info) +0ms\n  ioredis:redis status[192.168.1.6:6381]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[192.168.1.6:6381]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(info) +0ms\n  ioredis:cluster status: connect -> ready +0ms\nOn instance failover:\nioredis:redis status[192.168.1.6:6381]: ready -> close +4m\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[192.168.1.6:6381]: close -> end +0ms\n. ",
    "etiennechabert": "@luin thank's for the explanation, it's making sense now. But I am still convinced this behavior is bad (not only because I wasted 3h of my day debugging my server on this case). Btw, IORedis is probably not supposed to avoid this case himself by changing the value of the user. I probably should create the same ticket in the official Redis repository.\n. @luin I am going to follow the evolution of this request on the official Redis repo. Btw, thank's for your quick answers.\n. ",
    "jpallen": "Wow, that was super quick! I see you've already done another release too, thanks so much :)\n. ",
    "xyzlast": "and redis-parser 1.3.0 has v8::ObjectTemplate::Set() with non-primitive values. It's deprecated. \n. ",
    "SimonSchick": "Should this be closed?. You can probably omit most of those @constructor and other class related hints.. Some of those requires can probably be moved to import (namely node and some of your own).\nEverything should probably be const. You can get rid of @public and @private and use the keywords instead.. ",
    "pavel": "@Salakar Can you just disable this by default, and add an option that enables auto-pipelining? This will allow those who need this for local connections to get the benefit from your PR?\n. @luin Please, take a look at my point of view on pipeline reuse.\nImagine this example:\n``` javascript\nfunction updateUser(client, user) {\n  client.hmset(user.key, user);\n}\nfunction updateUsers(client, users) {\n   var pipeline = client.pipeline();\n   users.forEach(function (user) {\n     updateUser(pipeline, user);\n   });\n   return pipeline;\n}\n// more functions like updateUser and updateUsers\nfunction deleteUser(client, user) { ... }\nfunction deleteUsers(client, users) { ... }\nfunction addUser(client, user) { ... }\nfunction addUsers(client, users) { ... }\n// ...\nvar db = require(\"./myDB\"); // instance of ioredis\n// express app batch processing\napp.post(\"/sync\", function (req, res) {\n  var pipeline = db.pipeline();\n  // first we fetch data from DB, by ids\n  getBatch(pipeline, req);\n  pipeline.exec().then(function (batch) {\n    // at that point pipeline has an empty queue\n    deleteUsers(pipeline, batch.usersToDelete);\n    addUsers(pipeline, batch.usersToAdd);\n    updateUsers(pipeline, batch.usersToUpdate);\n    return pipeline.exec();\n  }).then(function () {\n    // ... error handling, etc\n    res.sendStatus(200);\n  }).catch(function () {\n    res.sendStatus(500);\n  });\n});\n// single processing\napp.put(\"/update\", function (req, res) {\n  var update = updateUsers(db, req.usersToUpdate).exec();\n  update.then(function () {\n    // ... error handling, etc\n    res.sendStatus(200);\n  }).catch(function () {\n    res.sendStatus(500);\n  });\n});\n```\nYou can see, that the functions like updateUsers don't need to know about whether its client is already a pipeline or not, so they are pretty reusable.\nTo me pipeline reuse is a feasible feature. Please, let me know what you think.\n. :) This is a hypothetical example, showcasing what if ioredis supported pipeline reuse, through Pipeline#pipeline(), and clearing queue on Pipeline#exec(). It is there to show you that reuse is a feasible feature, and should be somehow implemented in ioredis.\n. ",
    "migg24": "Are they executed in a blocking way?\n. Ok then maybe latency is not a problem and the example is fine. Then the problem is our clustering of the app. If you make a request to a route that executes the set and instantly another request to another route (on a different server) that executes the get then the set may not be finished and you get an \"id not found\" error as we were sporadically getting.\n. THX for enlighting :)\n. ",
    "xingchch": "I mean if I fork two node processes. Does them can share  same ioredis?\n. I think I am misunderstand ioredis, In fact , ioredis is the redis client . not redis implemented by javascript. so forget the V8 memory limitation. the data in fact stroge in local or remote redis. not in V8 memory. \nclose this. \n. ",
    "jamesmoriarty": "@luin I would love to see this feature. In the past I've used a memecached client which supported compressions for a high traffic site. \nIt's important for better utilisation of:\n- memory\n- bandwidth\nOften people don't realise they are saturating available bandwidth transferring large fragments.. ",
    "pavanratnakar": "Thank you very much. Thank was really quick.\n. ",
    "Jokero": "For me it also looks very strange that instantiation does some connections.\nI like how it did in mongoose. You need to connect explicitly:\njs\nmongoose.connect('mongodb://localhost/myapp');\nvar MyModel = mongoose.model('Test', new Schema({ name: String }));\n// Works\nMyModel.findOne(function(error, result) { /* ... */ });\nYes, I can use lazyConnect option, but anyway it looks like antipattern. ",
    "Anandesi": "i set the bind address as 0.0.0.0 so that it can be access any where and\nenabled cluster. and formed cluster using cluster meet command\n. ",
    "PhilTheAir": "I came across the same issue only to find out the reason had nothing to do with ioredis.\nI used EventEmitter in Node.js but did not place \"eventEmitter.emit('start');\" in the last line. That is, placed \"eventEmitter.emit('start');\" in the front of many \"eventEmitter.on('...');\" codes. Hence the redis ECONNREFUSED  error...\n. ",
    "timaschew": "oh, thanks :)\n. ",
    "Gerhut": "Changed\n. ",
    "danilvalov": "@luin Thank you! It works!\nBut perhaps it is worth adding a parser for redis.client('list') from returned string:\nid=112 addr=127.0.0.1:55153 fd=6 name= age=0 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client\n. Thank you!\n. ",
    "kishorpawar": "I found this issue when I did following.\n1. Accidentally I ran FLUSHALL on redis-cli, I tried to do ctrl-d. \n2. Without stopping redis-server I copied backed up rdb to dump.rdb and restarted redis-server.  I found that the copy did not happen actually.\n3. I stopped redis-server and then copied backed up rdb to dump.rdb and started redis-server. Copy worked.\n4. Started redis-cli\n5. Ran command KEYS * and got error (error) LOADING Redis is loading the dataset in memory\n. ",
    "kaidiren": "@shaharmor So how did you deal with it finally ?. ",
    "aleemb": "\nAs for me, keyPrefix is better since the prefix only applies on the keys rather than patterns and channels.\n\nI wasn't aware of this distinction, makes sense. Thanks for highlighting it.\n. ",
    "Trenrod": "My fault. Whole issue is nonsense close/delete if possible\nWas just confused that if key not exists, promise nor callback not get called at all.\n. Got the workaround. Needed to set enableOfflineQueue = false\n. ",
    "jaylinwang": "Redis session TTL (expiration) in seconds.\n. yeah,it's right. description indeterminacy,i'm so sorry \ud83c\udf85 \n. ",
    "dvp0": "Thanks folks for your input. That solves my problem partly. \n@AVVS Any reason you find the logic flowed ? I am new to node and redis, anything will help :)\n. ",
    "bsnote": "This just forces to use internal IP and doesn't allow to connect to Redis using localhost or public IP address. I think it's better to still bind to 0.0.0.0 but use cluster meet 10.2.x.x 6379 command, which makes Redis save 10.2.x.x in nodes.conf. BTW in order to use single-node cluster, the node should cluster meet itself using internal IP address. For example redis-cli -p 6379 cluster meet 10.2.19.39 6379\nIn both cases it's not possible to connect to Redis using public IP address or use localhost address for inter-node communication. It's not a big deal as it's probably not a good idea to expose Redis to outside world anyway. It's just that the overall concept of using the same IP address for inter-node communication and user client communication looks not perfect. Perhaps it's more of a general problem in Redis.\n. ",
    "mklilley": "Thanks @luin , really appreciate your quick response,\n. ",
    "driedger": "I have a set of node applications that use argumentTransformers and replyTransformers for all get/set operations to redis. We have recently setup a redis cluster, and would like to use IORedis with some other node modules to connect to our cluster, but the existing transformers we created are breaking the 3rd party modules as the transformers are on the global namespace.\nA connection level transformer would be ideal to work around this, so I tried the override method mentioned above, which worked great for basic set and hmset operations, but it appears that the overrides are bypassed when using multi() or pipeline() commands.\nI looked into issue #347 mentioned above but it appears the hooks were not implemented in v3.0.. ",
    "antoniodimariano": "Thanks for your reply. \nThis is the debug output\nioredis:redis status[127.0.0.1:16380]: connecting -> connect +137ms\n  ioredis:redis status[127.0.0.1:16380]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,redis-cluster) +0ms\n  ioredis:redis write command[0] -> sentinel(sentinels,redis-cluster) +5ms\n  ioredis:SentinelConnector adding sentinel 127.0.0.1:16382 +1ms\n  ioredis:SentinelConnector adding sentinel 127.0.0.1:16381 +0ms\n  ioredis:SentinelConnector sentinels +0ms [ { host: 'localhost', port: '16380' },\n  { host: 'localhost', port: '16381' },\n  { host: 'localhost', port: '16382' },\n  { host: '127.0.0.1', port: 16382 },\n  { host: '127.0.0.1', port: 16381 } ]\n  ioredis:redis status[127.0.0.1:6380]: connecting -> connect +13ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[127.0.0.1:6380]: connect -> ready +20ms\n  ioredis:redis status[127.0.0.1:16380]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[127.0.0.1:16380]: close -> end +0ms\n. 127.0.0.1:16380> SENTINEL sentinels redis-cluster\n1)  1) \"name\"\n    2) \"127.0.0.1:16382\"\n    3) \"ip\"\n    4) \"127.0.0.1\"\n    5) \"port\"\n    6) \"16382\"\n    7) \"runid\"\n    8) \"68645cfb8fcf86064af1317ccbc6e714ac086988\"\n    9) \"flags\"\n   10) \"sentinel\"\n   11) \"pending-commands\"\n   12) \"0\"\n   13) \"last-ping-sent\"\n   14) \"0\"\n   15) \"last-ok-ping-reply\"\n   16) \"654\"\n   17) \"last-ping-reply\"\n   18) \"654\"\n   19) \"down-after-milliseconds\"\n   20) \"5000\"\n   21) \"last-hello-message\"\n   22) \"1841\"\n   23) \"voted-leader\"\n   24) \"?\"\n   25) \"voted-leader-epoch\"\n   26) \"0\"\n2)  1) \"name\"\n    2) \"127.0.0.1:16381\"\n    3) \"ip\"\n    4) \"127.0.0.1\"\n    5) \"port\"\n    6) \"16381\"\n    7) \"runid\"\n    8) \"96fa1356370d960810582069714971a4f625108f\"\n    9) \"flags\"\n   10) \"sentinel\"\n   11) \"pending-commands\"\n   12) \"0\"\n   13) \"last-ping-sent\"\n   14) \"0\"\n   15) \"last-ok-ping-reply\"\n   16) \"654\"\n   17) \"last-ping-reply\"\n   18) \"654\"\n   19) \"down-after-milliseconds\"\n   20) \"5000\"\n   21) \"last-hello-message\"\n   22) \"184\"\n   23) \"voted-leader\"\n   24) \"?\"\n   25) \"voted-leader-epoch\"\n   26) \"0\"\n. $ redis-cli -p 16382 sentinel get-master-addr-by-name redis-cluster\n1) \"127.0.0.1\"\n2) \"6380\"\n. yes it is.\n. ",
    "yacut": "I have same issue in k8s.\nshell\nbash-4.3# redis-cli -p 26379 sentinel get-master-addr-by-name redis-cluster\n1) \"10.90.236.83\"\n2) \"6379\"\nThe simple client with sentinel config:\n```javascript\nvar redis = new Redis({\n  sentinels: [{ host: 'redis-cluster', port: 26379 }],\n  name: 'redis-cluster'\n});\nredis.set('foo', 'bar');\n```\nThe  result is a wrong ip address: Error: connect ECONNREFUSED 127.0.0.1:6379. @leoDreamer If you have sentinel on each node, you can do it. I had this setup before and it's never works great. I think, it will be better to create one svc for master and one svc for slaves (like redis chart do), and put them both to the ioredis config.. ",
    "dmason30": "Hi @luin, \nI have a laravel application broadcasting this data to the redis cache. When the user connects and there is already data in redis under this channel I want it to, on connection, emit this data (message) to the client. \nCurrently it appears that is only emits the data (message) when channel is updated so the client sits there without any data waiting for the update.\n. ",
    "looveu": "@luin \u8c22\u8c22\u6b63\u5728\u62c6\u5206\n. ",
    "seap": "yes. I'm sure. the data stored in db is correct. \nif I only launch one and two functions 'myGet(1)', the results are all right.\nmyGet(1);\nmyGet(1);\nbut if I try to launch more functions(> 3) , the error result will be coming.  \nmyGet(1);\nmyGet(1);\nmyGet(1);\nmyGet(1);\n....\n. thanks for your feedback soon. \nI think it should be our inside proxy issue. \nIn one connection, we can send async get concurrently\nI think ioredis can make sure we can get the corresponding return value exactly in async get.\nright\uff1f\n. ",
    "lgwebdream": "\u5728\u6211\u672c\u5730\u662f\u597d\u597d\u7684\uff0c\u7f16\u8bd1\u5230\u670d\u52a1\u5668\u5c31\u4e0d\u884c\u4e86\u3002\u662f\u5b98\u65b9\u7684\u5462\u3002\nimport Redis from \"ioredis\";\nthis.redis = new Redis({\n        port: RDS_PORT, // Redis port \n        host: RDS_HOST, // Redis host \n        password: RDS_PWD,\n        dropBufferSupport: true\n    });\n. \u60a8\u7684\u610f\u601d\u662f\u6211\u7684Redi\u670d\u52a1\u5668 \u8fd8\u662f \u8fd9\u4e2a\u6a21\u5757\u5462\u3002\u6211\u7528\u7684\u662f ioredis \u76f4\u63a5 install\u4e0b\u6765\u7684\u3002\u6211\u9700\u8981\u5728install\u4e00\u4e2a redis\u6a21\u5757\u4e48\uff1f\n\u4e0d\u597d\u610f\u601d\u554a \u8ffd\u95ee\u4e86\u597d\u51e0\u6b21\u3002\n. \u6211\u76f4\u63a5\u7528redis\u8fd9\u4e2a\u6a21\u5757\u662f\u53ef\u4ee5\u8fde\u901a\u7684\uff0c\u6240\u4ee5\u6211\u5f88\u5947\u602a\u3002\u8981\u662fredis\u670d\u52a1\u5668\u6709\u95ee\u9898\u7684\u8bdd\uff0c\u90a3redis\u8fd9\u4e2a\u6a21\u5757\u4e5f\u5e94\u8be5\u6302\u5462\u3002\n. \u6211\u7528\u7684\u5c31\u662f\u76f4\u63a5\u963f\u91cc\u4e91\u7684redis\u670d\u52a1\uff0c\u5e94\u8be5\u95ee\u9898\u4e0d\u5927\u5462\u3002\n. ",
    "fghpdf": "\u7b80\u76f4\u4e86\u3002\u3002\u3002\u3002\u3002\n. ",
    "shuperry": "@luin Thks, I figure it out just now, before that, I thought getset means get a set (collection), when I checked it on redis official website, I realized I thought wrong.\n. ",
    "camdenorrb": "Indeed, I can. I also used Redis Desktop manager, and Jedis. So I am certain it's not my side.\n. Ok, recently my routers ethernet ports blew out. So I will need to wait about a week to test that out (Until the new router gets in). I will let you know how it goes then.\n. I don't get the error when I am not in loadbalancer/Cluster mode from PM2 (A process manager for NodeJs) Not sure if there is a way to fix this :C\n. ",
    "vhung92": "I also recently stumbled upon this issue when trying to connect to remote redis instances, here are the logs from node:\nDEBUG=ioredis:* node test.js\nioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\nioredis:redis status[<sentinel-host-1>:26379]: [empty] -> connecting +4ms\nioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,redis) +4ms\nioredis:redis queue command[0] -> set(cache:documents:hello,world) +1ms\nioredis:redis status[<sentinel-host-1>:26379]: connecting -> connect +64ms\nioredis:redis status[<sentinel-host-1>:26379]: connect -> ready +1ms\nioredis:connection send 1 commands in offline queue +0ms\nioredis:redis write command[0] -> sentinel(get-master-addr-by-name,redis) +1ms\nioredis:redis write command[0] -> sentinel(sentinels,redis-staging) +61ms\nioredis:SentinelConnector adding sentinel <sentinel-internal-ip-1>:26379 +64ms\nioredis:SentinelConnector adding sentinel <sentinel-internal-ip-2>:26379 +0ms\nioredis:SentinelConnector sentinels +0ms [ { host: '<host>'26379 port: 26379 },\n{ host: <sentinel-host-2>, port: 26379 },\n{ host: <sentinel-host-3>, port: 26379 },\n{ host: '<sentinel-internal-ip-1>', port: 26379 },\n{ host: '<sentinel-internal-ip-2>', port: 26379 } ]\nioredis:redis status[<host>:26379]: ready -> close +77ms\nioredis:connection skip reconnecting since the connection is manually closed. +0ms\nioredis:redis status[<host>:26379]: close -> end +0ms\nioredis:connection error: Error: connect ETIMEDOUT +3s\n[ioredis] Unhandled error event: Error: connect ETIMEDOUT\n  at Socket.<anonymous> (<path>/node_modules/ioredis/lib/redis.js:287:21)\n  at Socket.g (events.js:260:16)\n  at emitNone (events.js:67:13)\n  at Socket.emit (events.js:166:7)\n  at Socket._onTimeout (net.js:332:8)\n  at _runOnTimeout (timers.js:524:11)\n  at _makeTimerTimeout (timers.js:515:3)\n  at Timer.unrefTimeout (timers.js:584:5)\nI am able to connect to a single instance (with port 6379 ) and execute commands using ioredis but not the sentinels, although I can connect to the sentinels using medis, any idea what the problem could be?\n. I've investigated further and the issue was not with ioredis but with our setup. ioredis is able to connect to one sentinel but when it is discovering other sentinels it used the internal IP's of the other sentinels to try and connect and since those were set up to be restricted ioredis was not able to connect to them, so my issue can be considered resolved\n. ",
    "fwjieok": "\u6211\u7684\u7528\u6cd5\u5982\u4e0b\uff1a\nredis .sort(\"newuserlist\").then(function (result) {\n    console.log(result);\n}); \n\u8fd9\u6837\u662f\u53ef\u4ee5\u83b7\u53d6\u6211\u60f3\u8981\u7684\u6570\u636e\u7684\u3002\n\u7136\u800c\u5982\u4e0b\u5199\u6cd5\u662f\u4e0d\u884c\u7684\uff1a\nredis .sort(\"newuserlist get user:userid:*:username\").then(function (result) {\n    console.log(result);\n});               \n\u5728redis-cli\u4e2d\u6267\u884csort newuserlist get user:userid::username   \u662f\u53ef\u4ee5\u83b7\u53d6\u6211\u60f3\u8981\u7684\u6570\u636e\u7684\uff0c\u6211\u7684\u5199\u6cd5\u6709\u95ee\u9898\u5417\uff1f\nsort\u51fd\u6570\u5230\u5e95\u600e\u6837\u7528\uff1f\n. Thanks Very Much, I complete my function!  use:\nredis.sort('newuserlist', 'get', 'user:userid::username', 'desc', 'limit', 0 , 5);\n. ",
    "zhaozhi1989": "Network no problem, the network delay between the nodejs server and the redis server is 2 to 3 ms, just in the same second, the emergence of redis query timeout of 80 milliseconds or more, I would like to ask this and ioredis cluster package is associated with it?\n\u7f51\u7edc\u6ca1\u95ee\u9898\uff0cnodejs\u670d\u52a1\u5668\u4e0eredis\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u7f51\u7edc\u5ef6\u65f6\u662f2\u52303\u6beb\u79d2\uff0c\u53ea\u662f\u5728\u540c\u4e00\u79d2\u4e2d\u51fa\u73b0redis\u67e5\u8be2\u8d85\u65f680\u6beb\u79d2\u4ee5\u4e0a\uff0c\u8bf7\u95ee\u8fd9\u4e0eioredis\u96c6\u7fa4\u5305\u6709\u5173\u8054\u5417\uff1f\n. \u5982\u679c\u6839\u636e\u4f60\u8bf4\u7684\u5b58\u5728redis\u7ebf\u7a0b\u963b\u585e\uff0c\u6211\u5df2\u7ecf\u8ba9\u6211\u4eec\u516c\u53f8DBA\u67e5\u4e86redis\u6570\u636e\u5e93\u7684\u65e5\u5fd7\uff0c\u90fd\u6ca1\u6709\u627e\u5230\u8d85\u65f650\u6beb\u79d2\u4ee5\u4e0a\u7684\u3002\n\u5e76\u4e14\u6211\u4eec\u6709\u7528java\u8c03\u7528redis\u96c6\u7fa4\uff0c\u90fd\u6ca1\u6709\u5b58\u5728redis\u7ebf\u7a0b\u963b\u585e\u8fd9\u79cd\u60c5\u51b5\u3002\nzhaozhi198904@163.com\nFrom: Zihua \u5b50\u9a85\nDate: 2016-10-10 23:34\nTo: luin/ioredis\nCC: zhaozhi1989; Author\nSubject: Re: [luin/ioredis] ioredis cluster access to the redis database, in the same second in a number of times out of 80 milliseconds (#377)\n\u548c ioredis \u65e0\u5173\u7684\uff0c\u8d85\u65f6\u539f\u56e0\u5f88\u591a\uff0c\u6bd4\u5982\u7f51\u7edc RTT \u6bd4\u8f83\u9ad8\u6216\u8005 Redis \u7ebf\u7a0b\u963b\u585e\u3002\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Redis Cluster\u8fd4\u56de\u7684\u6570\u636e\u4e0d\u5927\uff0cvalue\u503c\u90fd\u5f88\u5c0f\uff0c\u5e76\u4e14parser\u89e3\u6790\u662f\u7528\u7684hiredis\u8fd9\u4e2a\u5305\uff0c\n\u76ee\u524dnodejs\u73b0\u5728\u7528\u7684\u662f\u5355\u7ebf\u7a0b\uff0c\u5728\u505a\u538b\u529b\u6d4b\u8bd5\u65f6\u89c2\u770bcpu\u6d88\u8017\u60c5\u51b5\uff0c\u6709\u662f\u4f1a\u8fbe\u5230100%\uff0c\n\u8bf7\u95eecpu\u8fc7\u9ad8\u4f1a\u9020\u6210redis\u8fde\u63a5\u8d85\u65f6\u5417\uff1f\n\u53e6\u5916\u60a8\u90a3\u8fb9\u6709\u6ca1\u6709express.js\u6846\u67b6\u5f00\u542f\u591a\u5c11cluster\u7684\u4f8b\u5b50\u3002\nzhaozhi198904@163.com\nFrom: Zihua \u5b50\u9a85\nDate: 2016-10-10 23:57\nTo: luin/ioredis\nCC: zhaozhi1989; Author\nSubject: Re: [luin/ioredis] ioredis cluster access to the redis database, in the same second in a number of times out of 80 milliseconds (#377)\n\u55ef\uff0c\u6392\u9664\u8d85\u65f6\u7684\u8bdd\u5c31\u662f\u7f51\u7edc\u95ee\u9898\u6216\u8005 Node.js \u8fd9\u8fb9 CPU \u963b\u585e\u4e86\u3002\u628a ioredis \u548c java \u7684\u4ee3\u7801\u653e\u5230\u540c\u4e00\u4e2a\u7f51\u7edc\u73af\u5883\u770b\u770b\uff0c\u5982\u679c\u8fd8\u6709\u95ee\u9898\uff0c\u786e\u8ba4\u4e0b\u662f\u4e0d\u662f Redis Cluster \u8fd4\u56de\u7684\u6570\u636e\u6bd4\u8f83\u5927\uff0cJavaScript \u7684 parser \u5904\u7406\u4e0d\u8fc7\u6765\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \u76ee\u524d\u670d\u52a1\u5668\u7684\u914d\u7f6e\u662f4\u683816G\uff0c\u538b\u529b\u6d4b\u8bd5\u7684\u65f6\u5019\u662f50\u4e2a\u5e76\u53d1\uff0c\u6709\u6ca1\u6709\u4ec0\u4e48\u529e\u6cd5\u80fd\u964d\u4f4eCPU\u7684\u6d88\u8017\uff1f\n. \u5b50\u9a85\uff0c\u6211\u4eca\u5929\u628anodejs\u5f00\u4e86\u591a\u7ebf\u7a0b\uff0ccpu\u964d\u4e0b\u6765\u4e86\uff0c\u4f46\u662freids\u67e5\u8be2\u8fd8\u662f\u6709\u8fd8\u591a\u8d85\u65f6\uff0c\u4f60\u80fd\u5206\u6790\u4e0b\u662f\u4ec0\u4e48\u539f\u56e0\u5417\uff1f\u8c22\u8c22\uff01\nzhaozhi198904@163.com\nFrom: Zihua \u5b50\u9a85\nDate: 2016-10-11 00:08\nTo: luin/ioredis\nCC: zhaozhi1989; Author\nSubject: Re: [luin/ioredis] ioredis cluster access to the redis database, in the same second in a number of times out of 80 milliseconds (#377)\n\u5982\u679c CPU \u8fbe\u5230 100% \u5fc5\u7136\u4f1a\u5bfc\u81f4\u963b\u585e\u7684\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "nileshmali": "Is this ready for merge?. ",
    "jeanadrien": "Yes, that can do the trick, thanks. According to this Redis ticket https://github.com/antirez/redis/issues/2942 it's not really an option that PUBLISH on a readonly slave returns an error, but they suggest another approach: to pair the PUBLISH command with a random SET command in a pipeline, and this way the master role is checked with a negligible performance cost and no data loss.\n. ",
    "ddennis": "I also had this issue - using Azure Redis Cache from a linux VM.\nCould this info maybe be part of the README under Error handling\nThank you for a great library.. ",
    "iamjochem": "hi, \nI was aware of the related issues and the relevant part of the README. I am confused about your second comment ... are you saying it is NOT good practice to use a keyPrefix? (I thought that the whole point of the keyPrefix was to promote consistent name-spacing).\nSo is your advice to not use a keyPrefix ? \n. hi @luin,\nas promised here is a PR: https://github.com/luin/ioredis/pull/523. you are correct about the custom formatters - this is what I actually currently use to solve this issue in my own project. \nthe reason I chose not to use custom formatters here is because they are applied \"module\" wide ... i.e. you may very well be overriding existing custom formatters or applying them (without asking) to all instances of \"debug\" used in a project's own code and all it's other dependencies (assuming they use the same version) ... I felt that this was something to be avoided (it is unfortunate that debug does not support custom formatters per 'instance')\nthe fix for the potential performance hit is easy enough - I pushed a commit that make the debug call a no-op (basically performing the same check as is done in the debug module itself) ... what do think? . @luin - thanks for merging, it's a goog feeling to start the week with a successful PR :-). ",
    "marshallford": "Like @iamjochem, I too am confused. From reading and watching talks about Redis I was under the impression that name spacing was recommended.. ",
    "16boke": "ioredis\u7684cluster\u662f\u4e0d\u662f\u53ea\u652f\u6301redis3.0\u4ee5\u4e0a\u7684cluster?\n\u662f\u5426\u652f\u6301codis\u96c6\u7fa4\uff1f\n. ",
    "markschmid": "Yes, official Redis instance (cluster configuration). I think it happened because the cluster was not instantiated correctly or something. After I've downgraded to ioredis@2.0.0 it did not crash anymore but showed a DENIED error (https://www.reddit.com/r/redis/comments/3zv85m/new_security_feature_redis_protected_mode/). After I've fixed the redis configuration, it worked again (also with ioredis@2.4.0).\nThere might be situations, where item can be null or undefined and maybe this could be handled in ioredis. It seemed to me that ioredis@2.0.0 did handle this differently than 2.4.0 but I might be mistaken. In any case: We like ioredis a lot and I'm happy our setup works again.\n. ",
    "allthetime": "@luin\nI just found all the events to use to detect connection errors and it looks like even though I am setting host explicitly, it is still trying to connect to localhost (127.0.0.1)\nOh.... so, I might have to re-run my cluster setup to use the proper addresses?\nThank you so much.\n. Yup that was the problem exactly. \nNeeded to use explicit/external addresses in the redis cluster node configuration instead of 127.0.0.1\n. ",
    "darrachequesne": "You're welcome!\n. ",
    "Nepoxx": "\nwhich is already supported by Redis Cluster\n\nIsn't a Redis cluster only for sharded data?\nI too, have this use case of non-sharded data that's replicated where I want writes to the master and reads to the closest instance. I'm not quite sure how to do that (mongodb does this natively and that's what I'm looking into doing).\n. I'm not sure I understand. If I keep a table with the slaves in order of ping, they're going to get selected correctly next time there's a reconnection? What about the first time?\n. ",
    "Somebi": "There is no pipeline implemented in cluster setup in many redis clients. We decided to use sentinels and replications with sharding and scaling reads with this setup would be really awesome.\n. So it works as well with replication+sentinel setup?\n. ",
    "nodo": "@luin I have a doubt regarding the override of Redis#sendCommand. Should it pick a random node? But how to know the available nodes? Wouldn't this have a performance impact.\n. Thanks @luin. Probably I am missing something, but with INFO I can get only private IP of the replicas which is not always useful. Also, INFO should be called periodically to keep the information up-to-date. What do  you think?\n. Thanks for your comment @luin.\nI am not sure if this could be a general solution. For instance, when using Elasticache it is not possible to bind the slaves to the interface. I believe this is the case with docker as well.\nDo you think this is a good enough solution anyway?\n. ",
    "toredash": "@luni,\nIsn't read write splitting supported now ? If I'm reading the docs correct( link ), I can use two redis connection with sentinels and do r/w splitting.\nvar redis_ro = new Redis({\n  sentinels: [{ host: '127.0.0.1', port: 26379 }, { host: '127.0.0.1', port: 26380 }],\n  name: 'mymaster',\n  role: 'slave'\n});\nvar redis = new Redis({\n  sentinels: [{ host: '127.0.0.1', port: 26379 }, { host: '127.0.0.1', port: 26380 }],\n  name: 'mymaster'\n});\nThen I'm able to do different calls like this:\nredis.set('foo', 'bar');\nredis_ro.get('foo');\nOr am I missing something?. Hi @willrstern,\nI'm not sure if this is a ioredis issue, but a redis sentinel configuration issue.\nYour sentinels have voting power of who is the current master and also electing a new master if the current master is down/unreachable. Your sentinels configration entry down-after-milliseconds (https://redis.io/topics/sentinel) defines \"is the time in milliseconds an instance should not be reachable (either does not reply to our PINGs or it is replying with an error) for a Sentinel starting to think it is down\".\nIf down-after-milliseconds is set to 60000, that translates to 60s. The redis-master needs to be down for 60s before your sentinels will vote for a new master. Your client application will not be able to issue commands to redis during that time.\nIf you want your queries to fail right away, I think you could disable offlineQueue as described here: https://github.com/luin/ioredis#offline-queue - I haven't tested it but I assume your client application will fail at the very moment your redis master is not functonal.\nIf you want your read operations to work while the master is down, you could use two sentinel connections (https://github.com/luin/ioredis#sentinel) where one specifies the role: slave options, while the other don't. The former will only connect to slaves instances as reported by your sentinel, and the latter will only connect to the master instance as reported by your sentinel.\nAlso have a look at Auto-reconnect and Reconnect on error (https://github.com/luin/ioredis#auto-reconnect) - those might provide an alternative approach to solve your use-case.. ",
    "dev-drprasad": "@luin is read-write splitting supported currently ?. ",
    "zdy23216340": "\u597d\u7684\uff0c\u8c22\u8c22\uff5e @luin \n. ",
    "kelchy": "\"ioredis\": \"^2.4.0\"\nno proxies, just sentinel \nredis 3:3.0.6-1chl1~trusty1\n. I can reproduce this problem 100%\n. i am trying to isolate code since project is quite big.\nif i isolate, the problem does not exist. but if i run it together with\neverything else, problem shows up 100%\nI am inclined to think that there might be other stuff causing this but it\nis difficult to trace. it only shows up during\nstartup, where the server tries to do lot of initialization like fetching a\nbunch of wsdl for a SOAP feature.\nstrange enough, when i put a 2seconds timeout before fetching from redis,\nit works. this is not ideal as i need\nthe cache as early as possible during startup. any clue on how to dig\ndeeper?\nKelvin Chua\nOn Tue, Nov 1, 2016 at 7:40 PM, Zihua \u5b50\u9a85 notifications@github.com wrote:\n\nHow can I reproduce it locally?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/issues/389#issuecomment-257546358, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGxwWCAjpFHwSVW00A3i9yN2Z-IEdyM1ks5q5yUxgaJpZM4KlzPx\n.\n. Yes it was indeed related. I was able to find the real reason.\n\nIn my code base, initialization is very heavy. so redis takes more than 1 second to emit \"ready\"\nIn this situation, somewhere somehow, when I fetch a key from redis, it returns this erroneous response. I have patched my code to wait for \"ready\" before proceeding and it works.\nhowever, I think this behaviour is incorrect in a way that ioredis should have returned error instead\nof providing\n[ [ null, 'O' ], [ null, 'K' ] ]\n. ",
    "Aditya-Chowdhry": "Recently used Ioredis cluster in production and got the above-mentioned errors. Just want to check up on the status of the issue. Is it resolved in latest version?. In my case, I am only getting this same exact error repeatedly\n\nMOVED 0 xxx.xx.xx.xx:6389\n\nwhere the cluster consists of only 4 masters, ports - 6389,6390,6391,6392\nI have another cluster consisting of master & slaves for which I am not getting any error. . @luin  No 6389 was not down. I am not able to reproduce it locally. Debugging further. Will get back if found something ASAP.. @luin Got the issue. This error that I posted comes when null cache key is passed. I wrote a rough small code to reproduce the issue. \n````\n'use strict';\nconst Redis = require('ioredis');\nconst CLUSTER_HOST = '127.0.0.1';\nconst cluster = new Redis.Cluster([{\n    port: 6383,\n    host: CLUSTER_HOST\n}, {\n    port: 6384,\n    host: CLUSTER_HOST\n}]);\nlet cacheKey = null;\nlet data = 'Hey this is test data';\nconsole.log('CacheKey', cacheKey);\nfor(var i = 0; i < 100; ++i) {\n    cluster.set(cacheKey, data, function(err, reply){\n        if(err) console.log('Error', err);\n        if(reply) console.log('Okay set done', reply);\n        cluster.expire(cacheKey, 1234, function(err, reply){\n            if(err) console.log('Error', err);\n            if(reply) console.log('Okay expire done', reply);\n            cluster.get(cacheKey, function(err, reply){\n                if(err) console.log('Error', err);\n                if(reply) console.log('Okay get done', reply);\n            });\n        });\n    });\n}\n\nError: Too many Cluster redirections. Last error: ReplyError: MOVED 0 127.0.0.1:6380\n. \n",
    "thenitai": "I'm seeing this issue with 3.2.2 in our production environment with 3 redis nodes (different IP's). ",
    "sumeet559": "The doc in the link says http://redis.io/topics/pipelining.\nI guess you need to split the task in batches.\nSnippet from the doc\nIMPORTANT NOTE: While the client sends commands using pipelining, the server will be forced to queue the replies, using memory. So if you need to send a lot of commands with pipelining, it is better to send them as batches having a reasonable number, for instance 10k commands, read the replies, and then send another 10k commands again, and so forth. The speed will be nearly the same, but the additional memory used will be at max the amount needed to queue the replies for this 10k commands.\n. ",
    "seanlindo": "Thanks for the response. I'm using ioredis for interacting with a cache now and the default keepalive settings worked perfectly-- in my case, queuing has been best left to SQS :)\n. ",
    "jehy": "@luin yeah, I can see retries happening - but how can I get error message?\nWill it be in error event if I return non-number message  from sentinelRetryStrategy?\n. Hmm, error message is error event is much less detailed, it only says Error: All sentinels are unreachable. Last error: Connection is closed.\nMessage No such master with that name would be much better. But that's at least something. Thanks!\n. I have the following config:\n\"redis\": {\n    \"sentinels\": [\n      {\n        \"host\": \"localhost\",\n        \"port\": 26379\n      },\n      {\n        \"host\": \"localhost\",\n        \"port\": 26380\n      }\n    ],\n    \"name\": \"mymaster\",\n    \"keyPrefix\": \"kbk:\",\n    \"enableOfflineQueue\": false\n  },\nAnd I think that error message should be the same for both sentinels since name is wrong.\n. Yeah, here is the last retry attempt (99 of 100):\nioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 99 +1ms\n  ioredis:redis status[127.0.0.1:26379]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[127.0.0.1:26379]: close -> end +0ms\n  ioredis:redis status[localhost:26379]: [empty] -> connecting +95ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster1) +1ms\n  ioredis:redis status[127.0.0.1:26379]: connecting -> connect +1ms\n  ioredis:redis status[127.0.0.1:26379]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster1) +0ms\n  ioredis:redis write command[0] -> sentinel(sentinels,mymaster1) +2ms\n  ioredis:SentinelConnector failed to connect to sentinel localhost:26379 because ReplyError: ERR No such master with that name +4ms\n  ioredis:redis status[localhost:26380]: [empty] -> connecting +1ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster1) +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:26380 +1ms\n  ioredis:redis status[localhost:26380]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +1ms\n  ioredis:redis status[localhost:26380]: close -> end +0ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 100 +1ms\n  ioredis:redis status[127.0.0.1:26379]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[127.0.0.1:26379]: close -> end +0ms\n  ioredis:redis status[localhost:26379]: [empty] -> connecting +99ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster1) +0ms\n  ioredis:redis status[127.0.0.1:26379]: connecting -> connect +1ms\n  ioredis:redis status[127.0.0.1:26379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,mymaster1) +0ms\n  ioredis:redis write command[0] -> sentinel(sentinels,mymaster1) +1ms\n  ioredis:SentinelConnector failed to connect to sentinel localhost:26379 because ReplyError: ERR No such master with that name +2ms\n  ioredis:redis status[localhost:26380]: [empty] -> connecting +0ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,mymaster1) +0ms\n  ioredis:connection error: Error: connect ECONNREFUSED 127.0.0.1:26380 +2ms\n  ioredis:redis status[localhost:26380]: connecting -> close +0ms\n  ioredis:connection skip reconnecting because retryStrategy is not a function +0ms\n  ioredis:redis status[localhost:26380]: close -> end +0ms\n  ioredis:SentinelConnector failed to connect to sentinel localhost:26380 because Error: Connection is closed. +1ms\n  ioredis:SentinelConnector All sentinels are unreachable and retry is disabled, emitting error...\n. Oops. My bad. Sentinel really listened only on 26379. Stupid mistake, sorry for taking your time. Thank you very much!\n. ",
    "diamont1001": "OK\u4e86\uff0c\u8c22\u8c22. ",
    "omidh28": "@luin I'm using latest one, 2.4.0. ",
    "pszabop": "My apologies.  Reading too fast. The behavior you describe is documented here:\n. ",
    "weifansym": "\u4e0d\u597d\u610f\u601d\u3002\u3002\u6ca1\u6df1\u5165\u770b\u4ecb\u7ecdreadme\u4e2d\u7684Handle Binary Data\u3002\u592a\u62b1\u6b49\u4e86. ",
    "748890753yiyi": "@luin thank you very much, the method is right.. ",
    "abritinthebay": "This would also open up dropping the pointless (given what it is used for here) Bluebird library and using Native Promises. As far as I can tell Bluebird is only used for functionality that Native Promises provides out of the box.. The only parts it is faster on is because it doesn't follow the spec. Neither do those utils. Bluebird is NOT promises. It doesn't follow the spec. It mostly follows the spec. \nFollowing broken psuedo-implementations is an anti-pattern. Best to nip it in the bud as early as possible.. >  it leaves out all the actual usage patterns behind\nGreat. Add those. Don't call it Promises, because it's not. Don't reimplement a (not-quite up to spec) version of Promises to add it.\n\nAs far as I\u2019m personally aware BB follows A+ spec and adds extra methods on top\n\nYeah, it doesn't. For example new Promise is an anti-pattern in Bluebird. Why? Because speed-wise it drops back down to native levels. Their benchmarks cheat a lot in this way. They also don't allocate arrays for promises' handlers. Meaning Promise branching is limited to incomplete.\nBasically it's a bunch of things to win micro-benchmarks while only adhering to as much of the spec as they can do be \"compliant\".\nHowever, it should really be about why does ioredis even need Bluebird? \nAnswer: it doesn't unless it's for compatibility for old versions of Node.\nEvidence: here is the entire list of Promise-related APIs ioredis uses:\nnew Promise\nPromise.reject\nPromise.resolve\nPromise.then\nThat's it. 100% Native compatible without using any extra library. Also 100% as fast as Bluebird because the raw reject/resolve/then calls are the same speed and Bluebird performs approx the same when used with new (instead of their promisify implementation).\nSo, long story short: why keep it? It adds nothing but bloat.. > a) it would be a huge breaking change\nAs I demonstrated: this is literally not true. ioredis uses nothing of Bluebird. No breaking changes whatsoever.\n\nliterally any developer I know would bring BB or do a global Promise remap to bring extra functionality and save dev time\n\nAh, annecdata. The last refuge of the lack of argument. I can counter with - no developer I know would willingly add a bloated library when it wasn't required. Which it isn't for 99% of promise use.. So Bluebird isn't compatible with Promises. Great, glad we got that cleared up.\nBut fine, sure, breaking change. So is killing node < 4. Perfect time to do it then.. That would be cleaner. Making it an optional peer dependency would be much less bloat-y.. Hmm, good point, though it's just a warning - should not throw errors in npm >= 3. In effect all peer deps are optional in npm.. That's a nice solution. I think @AVVS would still have the objection that this is a potentially breaking change for people relying on Bluebird behavior further down the chain (even if it's a bad practice).\nWhich is fair - but that is an excellent solution to solving the compatibility layer.. > it keeps Promises consistent across version\nBy having an interface that exposes incompatible APIs to Native promises and doesn't actually function according to the same rules? Ooookay.\n\nYour concerns about 'bloat' are far fetched, this is server side code\n\nMFW someone thinks server-side code can't be bloated \ud83d\ude44 If you think that... well, that's horrifying.. >  I feel we'd have better conversation talking to a brick wall.\nFunny, that's the impression I'm getting from you too. \ud83d\ude44 \nAnyhow, in more productive conversation I agree with you on the suggestion of Node 6 LTS in principle but think you might be being a bit premature.\nLots of Node 4 LTS apps are still out there and it in fact doesn't even go into maintenance mode until April of this year - and it will still be supported until April of next year.\nGiven that I'd say it's premature to drop v4 but the point you make about extra features is a great one for a future point on the roadmap.. I'd lean to initial support being 4+ for the first upgrade. Using new fancy features for 6+ can then be a SemVer Major update and drop 4 compatibility (iterative updates and development is good, right?)\nThat way 4 gets first class support of its features, it's a smaller update, it's faster to get to release, etc etc\nBest of both worlds. As you said - the main difference for 6 would be Proxy and Proxy is really a whole thing by itself. No need to complicate matters.\nMy general rule is always go for the simplest solution and iterate complexity when needed. . Incidentally - related to both Promises and Node v6 - Mikael Rogers had this to say on the perf differences:\n\"0.02ms additional overhead is that native promises clock have over bluebird on Node.js v6.\"\n- https://twitter.com/mikeal/status/834516413536628737\nSo... given who that is that would be my cite for that figure (for what it is worth, that's basically what I've seen personally too). It's functionally identical.\nNot to derail the conv again, but you did ask. \ud83d\ude04 . Reasonably put. I think it's the exact opposite of surprising as a default though - it's a native type. The default being a native implementation should never be surprising.\nThe default being a library should be surprising. Having a config option to override though is entirely reasonable.. Doh, brain fail. Read it as \"at least surprising\". Never mind!. Agree on that. It wouldn't break Node4 it would just say \"ok, we're at a new LTS release so we're going to focus on that moving forward\".\nSeems like a sane and absolutely reasonable decision from a management of resources point of view.. Given Node < 4 has been End Of Life'd since 2016-12-31 and Node 4 itself is going to be EoL by 2018-04-30 plus the last serious update to the codebase was 7 months ago and there has been no interest in moving the codebase forward: in fact hostility against it....\nI mean... I've moved on from this project assuming it's basically dead from a practical standpoint.. ",
    "elmigranto": "Every side makes valid points. From what I've seen, most libs that find themselves in similar situation provide an option for user to set custom Promise constructor while defaulting to system one.\njs\nconst redis = new Redis({\n  promiseConstructor: require('bluebird') // if omitted, `global.Promise` is used\n});\nMy personal opinion is that while Bluebird is definetly a very useful thing, it's least surprising to use global.Promise as a default. Imagine same argument about Date or Array, and I think it's quite obvious.. ",
    "connor4312": "Node 4 isn't deprecated, it's in LTS maintenance for another six months. That's a big difference! After April it'll be EOL. While I understand the desire to use fancier and faster ES6 features--I am looking forward to being able to async/await all my libraries--not all consumers have the privilege of being able to upgrade quickly. The three year release-to-EOL cycle Node has is already on the aggressive side as far as LTS (Ubuntu LTS, for instance, have a five year cycle).. ",
    "naholyr": "Just a comment about dropping bluebird:\nAbout performance: did you bench ioredis with/without bluebird, or just rely on provided micro-benchmarks? I think only the first is valuable, and should be tested.\nDropping bluebird should be considered, just replacing bluebird with i-promise for example would provide a configurable way for everyone, at no cost. I'm currently fighting with Bluebird handling \"run-away\" promises (that's a nice feature, but unwanted in this case), and I wonder how it will work when Node finally makes process crash on unhandled rejection, as this will only concern native promise\u2026 so you may suddenly have some promises that don't crash, and some that do, and that will be a surprise\u2026 if every bluebird-based module could at least provide a way to stick to native, it would be perfect for such cases.. What's the best alternative known other than util.promisifying node-redis?\nLe mar. 20 mars 2018 \u00e0 19:38, Gregory Wild-Smith notifications@github.com\na \u00e9crit :\n\nGiven Node < 4 has been End Of Life'd since 2016-12-31 and Node 4 itself\nis going to be EoL by 2018-04-30 plus the last serious update to the\ncodebase was 7 months ago and there has been no interest in moving the\ncodebase forward: in fact hostility against it....\nI mean... I've moved on from this project assuming it's basically dead\nfrom a practical standpoint.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/issues/408#issuecomment-374711027, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AANEMxYxgTAq57l8AbJLLwpAZItESsRLks5tgUy3gaJpZM4LCu7A\n.\n. \n",
    "kdekooter": "We should definitely have native promises!. @luin would it be possible for you / the committers to make a decision about this issue, and not let it be wiped out by the stale bot?\nIt is 2018 now. Promise has been a first class nodejs citizen for quite some time now. ioredis should support this so one can have a native-promise-only codebase.. Is this project still alive? Or is the stale bot running the show?. Hi fresh bot again here. We need this!. Good day folks, fresh bot again. Still interested in a fix!. Good day folks, fresh bot again. Still interested in a fix!. @jfjm could you expand a bit on how you did that? It might be useful for my project as well.. ",
    "RuofeiLv": "Thank you for replying.\nQuestion 1\nI am  gonna check the error handling topic in detail\nQuestion 2\nI am gonna check the source code about getInfoFromNode, I am curious about how it was implemented, a small question before diving into the source code -- slot cache means the cache for\nthe map between hash slots and nodes addresses, so basiclly the procedures i guess are\n\nNext time we have a new key in client, we firstly calculate the key's slot by using CRC16 of the key modulo 16384\nNow we get the hash slot and since we have the cache map between hash slots and nodes addresses, then we have the node addresses.\nClient send the requests to the that node addresses.\n\n4.Redis server get the requests from client, then the redis server has to caculate again about the hash slot of the key to check if this node can receive this request, so this recalcaulation here for hash slot is verbose and useless, right?     However, for server itself, it has to check again for data or system safety. But calculation for key's hash slot do happen twice.\nAnother question is about the data safety of the redis cluster, I use the redis for two purpose, database and the cache. For database, I store the data such as user follower-following relationship and timeline for each user, I need to ensure the 99.99% safety of the data. So If I follow the recommendation of official configuation, I need to have several slaves for each master, but do we need to turn on or turn of the persistence for master such like AOF? If I turn on the AOF, then I definitely want to restore my data from AOF because it seems to have less possibility for data lose(no more that 1 second if i set the time limit), but for turning off the AOF, the time is not guaranteed(0-N? seconds), so any ideas here?   For cache purpose, I dont' care any data lose. HAHA.\nRecommendation\nYes, I mean that is the new connection tab or new connection window function that I have found them on File->New connection tab || New connection window !\n. @luin \nBecause Node.js by default follows pipeline mode (that sends the next command without waiting for the receiving of the previous command) when sending commands.\nRegarding this sentence, I think the advantage of pipeline is to save network bandwidth, hence reducing the time this task need.  The node.js server performance is surely same in both mode(pipeline and send one by one) since it is asynchronous.\nhttp://bbs.redis.cn/forum.php?mod=viewthread&tid=826\nIn the link above, they(youku) do the same thing(forming different pipeline to different node) and then merge together(they also check the slot-node table, I think that ask, moved is a rare thing, performance should be the first), otherwise cluster for caching is not an ideal option for sizeable application. I would hack it and pull requests later.\n. ",
    "ruimarinho": "@luin do you have time for some feedback here?. No problem, I appreciate your time! Is that a simple OK or [null, OK]? From the Redis specification it looks like it's a Simple String Response. I've updated the PR to reply with an OK for now.. SGTM! Do you think this should be a separate test?. Let me know if you like the style of the new test.. @luin would you be up to a patch/minor release which includes this PR? Not urgent, only if you have spare time and agree with it. Thanks!. ",
    "chrisskilton": "I had the code wrapped in redis.on('connect'). Here's a reproducible example.\nscript:\n```javascript\n'use strict';\nconst Redis = require('ioredis');\nconst redis = new Redis();\nredis.on('connect', () => {\n    console.log('redis connected');\n    redis.info().then(info => {\n        console.log('redis.info response', info);\n    });\nredis.config('get', 'maxmemory').then(maxMem => {\n    console.log('redis.config response', maxMem);\n});\n\nredis.multi().info().config('get', 'maxmemory').exec().then(results => {\n    console.log('redis.multi response', results);\n});\n\n});\n```\n```\n$ node check-scripts/test.js\nredis connected\nredis.info response # Server\nredis_version:3.0.7\nredis_git_sha1:00000000\nredis_git_dirty:0\n...\nredis.config response [ 'maxmemory', '3145728000' ]\nredis.multi response []\n```\nThe multi does work outside the connect callback. . closing this as it's fixed. ",
    "andr0s": "This is actually a very important thing, but the official documentation doesn't seem to state it clearly.. ",
    "imsrini-m": "Getting the following issue while running the below code block.\nCode block: \ncluster.pipeline().set('foo1', 'bar').set('foo2', 'bar').exec(function (err, results) {\n    console.log(err);\n});\nError: All keys in the pipeline should belong to the same slot                                                              at Pipeline.exec (********\\node_modules\\ioredis\\lib\\pipeline.js:227:21)                           at Object.<anonymous> (********\\sample.js:30:58)                                                  at Module._compile (module.js:570:32)                                                                                   at Object.Module._extensions..js (module.js:579:10)                                                                     at Module.load (module.js:487:32)                                                                                       at tryModuleLoad (module.js:446:12)                                                                                     at Function.Module._load (module.js:438:3)                                                                              at Module.runMain (module.js:604:10)                                                                                    at run (bootstrap_node.js:394:7)                                                                                        at startup (bootstrap_node.js:149:9)                                                                                    at bootstrap_node.js:509:3. ",
    "TangMonk": "@luin sorry, that's my problem, and thanks for quickly reply! . ",
    "philraj": "Was there ever any movement on this? The ioredis API would be much nicer to use if you didn't have to explicitly check whether arguments are empty when calling commands.. Posted on DefinitelyTyped, nevermind.. Thanks for the info, I'll try to at some point but at the moment my hands are tied.. Done in https://github.com/luin/ioredis/pull/681. @vweevers That's fair, but the original example was pushing them into an array, which would use even more memory (since duplicates are kept). I'm good with removing the storage of keys from the example entirely, simply logging from the .on('data') handler, and leaving the detail about duplicates as a comment, if that works for you.. ",
    "krystianity": "@zauberpony. *rewrote problem, because it was too hard to understand... this issue is probably related to firewall issues.. ",
    "whoan": "Lol my bad. Sorry. Thanks for the clarification.. ",
    "almoghaimo": "Thanks, that help :). ",
    "PhillippOhlandt": "@luin \ndeepstream_1         |   lastNodeError:\ndeepstream_1         |    { ReplyError: ERR This instance has cluster support disabled\ndeepstream_1         |        at JavascriptRedisParser.returnError (/var/lib/deepstream/deepstream.io-msg-redis/node_modules/ioredis/lib/redis/parser.js:24:25)\ndeepstream_1         |        at JavascriptRedisParser.execute (/var/lib/deepstream/deepstream.io-msg-redis/node_modules/redis-parser/lib/parser.js:558:12)\ndeepstream_1         |        at Socket.<anonymous> (/var/lib/deepstream/deepstream.io-msg-redis/node_modules/ioredis/lib/redis/event_handler.js:107:22)\ndeepstream_1         |        at emitOne (events.js:96:13)\ndeepstream_1         |        at Socket.emit (events.js:188:7)\ndeepstream_1         |        at readableAddChunk (_stream_readable.js:176:18)\ndeepstream_1         |        at Socket.Readable.push (_stream_readable.js:134:10)\ndeepstream_1         |        at TCP.onread (net.js:548:20)\ndeepstream_1         |      name: 'ReplyError',\ndeepstream_1         |      message: 'ERR This instance has cluster support disabled',\ndeepstream_1         |      command: { name: 'cluster', args: [Object] } } }\nHowever, the cluster works fine. I already tried turning it on in the config of the master server, without any luck.. It was commented. I uncommented and now its value is yes.\nI still get the same error but now the lastNodeError is:\ndeepstreamprivate_1  |   lastNodeError:\ndeepstreamprivate_1  |    Error: Connection is closed.\ndeepstreamprivate_1  |        at close (/var/lib/deepstream/deepstream.io-msg-redis/node_modules/ioredis/lib/redis/event_handler.js:101:21)\ndeepstreamprivate_1  |        at Socket.<anonymous> (/var/lib/deepstream/deepstream.io-msg-redis/node_modules/ioredis/lib/redis/event_handler.js:81:14)\ndeepstreamprivate_1  |        at Socket.g (events.js:291:16)\ndeepstreamprivate_1  |        at emitOne (events.js:96:13)\ndeepstreamprivate_1  |        at Socket.emit (events.js:188:7)\ndeepstreamprivate_1  |        at TCP._handle.close [as _onclose] (net.js:498:12) }. I set the log level to debug on the redis master. Seems like deepstream is connecting and disconnecting all the time:\n34:M 13 Feb 11:46:33.629 - Accepted 172.23.0.7:46448\n34:M 13 Feb 11:46:33.633 - Client closed connection\n34:M 13 Feb 11:46:33.650 - Accepted 172.23.0.7:46452\n34:M 13 Feb 11:46:33.652 - Client closed connection\n34:M 13 Feb 11:46:33.830 - Accepted 172.23.0.7:46456\n34:M 13 Feb 11:46:33.918 - Client closed connection. Doesn't give me much more information. Deepstreams connector receives the error from ioredis and passes it to Deepstream, which then throws it. I also created an issue in the connector repo (linked below my initial post) with my connector configuration. . Side note: It just does it on the master redis. The slave redis doesn't even get a connection yet.. Hey @luin, I created a little demo to show the issue. You can find it here: https://github.com/PhillippOhlandt/deepstream-redis-cluster Maybe you can successful debug it.. Why is it not a cluster? Scale them up to 20 instances and you are fine. Your linked image is the same, except that it's not scalable and therefore not usable in production. . The used image handles the roles on its own. And you can just scale the master service to 3. I don't see the problem. I just want to write to master and read from slaves. . The problem is, there aren't any examples or tutorials on a proper docker setup for redis. The best image I found is the one I use. I would like to scale redis with the built in docker tools (scaling a service) but that doesn't seem to work.. Do you know if there is a planned release date? This is a big blocker for us because we don't want to go with a single redis instance online.. Does that work with docker? Do you have a example setup?. ",
    "lakamsani": "Silly mistake on my side. Thanks for catching it as well as making this library available. My initial test works now.  Do you know  ioredis has been tested/used in these three scenarios and anything to watch out for or any other thoughts on such usage. \n\nAWS ElastiCache. I read a thread about  a fix you made over a year ago related to failover with this. With the latest version of ioredis will it work out of the box with ElastiCache or any special configuration to be set when initializing? AWS gives a single end point for the full cluster. Can I just pass it to the IORedis.Cluster constructor or do I need to pass the individual node end points?\nsocket.io-redis (https://github.com/socketio/socket.io-redis). Their docs say to pass node_redis handles for pub/sub clients. And it is working well so far with AWS ElastiCache for my use case. To avoid two redis libraries in my app I 'm thinking of passing in two ioredis handles for socket-io.redis pub/sub clients. \nredlock. On this I got this basic test working I will evaluate more. Without a cluster mode client handle it fails with MOVED errors. \n\n````javascript\nvar expect = require('chai').expect;\nvar should = require('chai').should()\nvar IORedis = require(\"ioredis\");\nvar Redlock = require('redlock');\nvar redisClient = new IORedis.Cluster([{host:'host1'},{host:'host2'}]);\nvar distributeRedisdLock = new Redlock(\n  // you should have one client for each redis node\n  // in your cluster\n  [redisClient],\n  {\n    // the expected clock drift; for more details\n    // see http://redis.io/topics/distlock\n    driftFactor: 0.01, // time in ms\n// the max number of times Redlock will attempt\n// to lock a resource before erroring\nretryCount:  3,\n\n// the time in ms between attempts\nretryDelay:  200 // time in ms\n\n}\n);\ndistributeRedisdLock.on('clientError', function(err) {\n  console.log('A redlock error has occurred:', JSON.stringify(err));\n});\ndescribe('Redlock with IORedis tests', function () {\nit('should test redlock', function (done) {\nvar ttl = 30000\nvar lockName = 'update-lock:'\n// ONK-1325.\ndistributeRedisdLock.lock(lockName, ttl, function (err, lock) {\n  if (err) {\n    console.log(err.stack)\n    console.log(\"unable to get lock\")\n    return done()\n  }\n  lock.unlock(function (lockReleaseErr) {\n    if (lockReleaseErr) {\n      console.log(\"lock release failed\")\n      console.log(lockReleaseErr.stack)\n    }\n    console.log(\"it worked\")\n    return done()\n  })\n})\n\n})\n})\n````\n. preliminary testing indicates ioredis working for all 3 use cases. just passing the single AWS ElastiCache load balancer address to IORedis.Cluster and using the same in  a single element client array in Redlock. For socke.io-redis created two non Cluster IORedis clients (with createClient) for pub/sub as socket.io-redis manages cluster behavior on its own.. ",
    "jayjanssen": "Thanks for the reply.  What about this:\n```\nclient = (require 'ioredis') 'redis://127.0.0.1:6379/0', {\n    connectTimeout: 1000\n    lazyConnect: true\n    retryStrategy: () -> false\n    reconnectOnError: (err) -> false\n}\nclient.on 'error', (err) ->\n    console.error \"Event emitter: #{err.message}\"\nclient.get 'key'\n.then (value) ->\n    console.log value\n.catch (err) ->\n    console.error \"Promise error catch: #{err.message}\"\n.delay 1000, () ->\n    client.get 'key2'\n.then (value) ->\n    console.log 'Promise2 then'\n.catch (err) ->\n    console.error \"Promise2 error catch: #{err.message}\"\n```\nOutput:\nEvent emitter: connect ECONNREFUSED 127.0.0.1:6379\nPromise error catch: Connection is closed.\nPromise2 then\nIf I do the same thing without the delay, I do get the Promise 2 error properly.. Ah, sorry.  I'm not trying to be a pain, I was just seeing some issues in a more complex piece of code that I couldn't pin down.  I'm going to try again to adopt that code to ioredis and see if I can get it working and/or find something a little better to report.  Sorry for the noise.  . Ok, I'm struggling a bit with retries.  What I want is for ioredis to try to reconnect on a down redis server on every operation, but not get blocked by retries.  I've been playing with retryStrategy and reconnectionOnError, but to no avail.  The specific use case is for the second 'client.get' to reconnect and work if I bring the redis server up during the delay:\n```\nclient = (require 'ioredis') 'redis://127.0.0.1:6379/0', {\n    connectTimeout: 1000\n    lazyConnect: true\n    retryStrategy: (t) -> false\n    reconnectOnError: (err) -> true\n}\nclient.on 'error', (err) ->\n    console.error \"Event emitter: #{err.message}\"\nclient.get 'key'\n.then (value) ->\n    console.log value\n.catch (err) ->\n    console.error \"Promise error catch: #{err.message}\"\n.delay 10000\n.then () ->\n    client.get 'key2'\n.then (value) ->\n    console.log 'Promise2 then'\n.catch (err) ->\n    console.error \"Promise2 error catch: #{err.message}\"\n```\nI'm getting:\n```\nEvent emitter: connect ECONNREFUSED 127.0.0.1:6379\nPromise error catch: Connection is closed.\nRedis comes up here\nPromise2 error catch: Connection is closed.\n```\nIs there a set of ioredis options that will produce this?  Or, should I be using something like a Promise disposer instead?. All my attempts at playing with retryStrategy seem to block the app until the retries are done.  I'm fine with a background retry, but I need the workflow to continue.  In that case, it seems my only option is to use disposable connections.  . ",
    "andymans": "Fixed - just spotted redisOptions.. ",
    "agmcleod": "@luin i figured as much, just wanted to confirm :). Thanks a bunch, and sorry for flooding your inbox!. ",
    "mvaker": "The problem is with read-write splitting in Cluster mode with replication (not with sharded clusters, nor sentinel). When in cluster mode, it always tries to search for a slot to which a command has to be sent, and if the cluster is a replication one (not sharding), it fails because all slots are in all nodes.. Hello! Thanks for your early comments.\nIt is not a Sentinel based setup, we are using this setup (cluster mode dissbled): \nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Replication.Redis-RedisCluster.html\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/AutoFailover.html\nIt does pretty much the same as sentinel but amazon provides it all itself (failover, monitor, etc). So it's a redis cluster without sharding (or with all shards in the same node), but since it's elasticache it's not sentinel based.\nI have not tried the sentinel client here, but I understand it won't work either because no sentinel commands are available.\nI think that the best approach is to use the cluster mode to keep track of existing nodes and connection pool, while sending reads to random reader (slave) instances.\nThank you!. BTW my resolution in this PR is a little bit \"dirty\" and I know this can probably be done better (maybe a separate class or client..). It was just a quick approach to see if it worked and hear what are your thoughts about it.. Hi, any thoughts on this? Have you checked my last comments? I would appreciate very much any comments. Thank you. @luin @AVVS . hi @luin, I would like to discuss again this PR, so clearly this is a missing feature in this library and other people have proved it being useful for them. Thanks a lot!. ",
    "christoph-puppe": "Would love to see that. Don't need sentinel and no cluster. Just read-only slaves for 90% of the load but need to get the 10% writes to the master @luin \nMy usecase: frontend nodes are too far away for cluster, so latency is high. Solution: have a slave per frontend-instance and speed up the whole thing for reads. \n. Any news?. ",
    "fenichelar": "This is a much needed feature! The AWS non-clustered approach is much better for read heavy workflows than clustering because AWS only supports on-the-fly resizing for non-clustered Redis.. @mvaker You should probably rename this PR to \"Add AWS non-clustered replication support\" for clarity. This is not clustering.\n\nhttp://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/SelectEngine.RedisVersions.html. @luin The problem is the assumption that either Sentinel or Cluster mode are used if replication is used. Redis supports replication without Sentinel or Cluster mode but ioredis does not and therefore does not support read/write splitting for the following 3 use cases:\n\nVanilla Redis without Sentinel but with master/slave replication (which is perfectly valid for read heavy workloads)\nRedis Enterprise\nAWS ElastiCache in non-cluster mode\n\nWhile I haven't looked into the contents of the PR, I think the feature is well within the scope of scenarios ioredis should support natively.. @mvaker I said you should probably rename this PR to \"Add AWS non-clustered replication support\" but I think that is misleading, my fault. Instead, \"Add support for non-clustered replication without Sentinel\" is probably more accurate. While AWS is a use case, there are other use cases where this PR is quite useful. @STRML just brought up yet another use case.\n@luin I use Redis Enterprise and ioredis (hosted on AWS lol). At some point, I am going to have more read traffic on a single key than a single shard in Redis Enterprise can handle. So adding another shard to the Redis Enterprise cluster will not help. I will have to create a read replica of the cluster so the same key is stored on different shards. The replication is managed by Redis Enterprise, not Sentinel. Having ioredis support this natively would be very nice :). I think I'm seeing the same issue. I'm using Redis Enterprise from Redis Labs.\nWithout TLS, I can connect directly to the IP of the master node or I can connect using sentinel.\nWith TLS, I can only connect directly to the IP of the master node. When attempting to use sentinel, I see the same error described above:\nDEBUG=* node app.js\nioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\nioredis:redis status[10.0.11.99:8001]: [empty] -> connecting +2ms\nioredis:redis queue command[10.0.11.99:8001]: 0 -> sentinel([ 'get-master-addr-by-name', 'clearip' ]) +1ms\nioredis:redis status[10.0.11.99:8001]: connecting -> connect +4ms\nioredis:redis status[10.0.11.99:8001]: connect -> ready +1ms\nioredis:connection send 1 commands in offline queue +0ms\nioredis:redis write command[10.0.11.99:8001]: 0 -> sentinel([ 'get-master-addr-by-name', 'clearip' ]) +0ms\nioredis:redis write command[10.0.11.99:8001]: 0 -> sentinel([ 'sentinels', 'clearip' ]) +2ms\nioredis:SentinelConnector Updated internal sentinels: [{\"host\":\"10.0.11.99\",\"port\":8001},{\"host\":\"10.0.12.99\",\"port\":8001},{\"host\":\"10.0.13.99\",\"port\":8001}] @1 +0ms\nioredis:SentinelConnector resolved: 10.0.11.99:11000 +0ms\nioredis:redis status[10.0.11.99:8001]: ready -> close +2ms\nioredis:connection skip reconnecting since the connection is manually closed. +4ms\nioredis:redis status[10.0.11.99:8001]: close -> end +1ms. @luin Looking at the sentinel connector, it does not appear to support connecting using TLS. Would you be open to a PR to add support for TLS when sentinel is used? Also, would you reopen this issue? Thanks.. https://github.com/luin/ioredis/pull/772\n@luin Please let me know if there is something that needs changing.. Force pushed a slightly simpler version of the same thing.. Tested with Redis Enterprise and is working as expected. Redis Enterprise requires client certificates. Code used for testing:\n```js\nconst fs = require('fs');\nconst ioredis = require('ioredis');\nconst redisClient = new ioredis({\n  sentinels: [\n    {\n      host: '10.0.1.1',\n      port: 8001,\n    },\n    {\n      host: '10.0.1.2',\n      port: 8001,\n    },\n    {\n      host: '10.0.1.3',\n      port: 8001,\n    },\n  ],\n  name: 'test',\n  enableTLSForSentinelMode: true\n  tls: {\n    key: fs.readFileSync('key.pem').toString(),\n    cert: fs.readFileSync('cert.pem').toString(),\n    ca: fs.readFileSync('ca.pem').toString(),\n    secureProtocol: 'TLSv1_2_method',\n    checkServerIdentity: () => {\n      return undefined;\n    },\n  },\n  sentinelTLS: {\n    ca: fs.readFileSync('ca.pem').toString(),\n    secureProtocol: 'TLSv1_2_method',\n    checkServerIdentity: () => {\n      return undefined;\n    },\n  },\n});\nredisClient.on('error', err => {\n  console.error(err);\n});\nredisClient.on('ready', () => {\n  console.log('ready');\n  process.exit(0);\n});\n```. @luin I don't think so. In Redis Enterprise, you enable TLS per database. However, only a single Sentinel runs per node. If you have 2 database on a single node, one can require TLS and the other can not (which is what I was using for testing actually). I don't think there is an option to require TLS for the connection to Sentinel, but I'll look at the documentation again. That would be nice though because a MITM attack against Sentinel could be quite bad.. Also, I should clarify, for Redis Enterprise it is not actually Sentinel, it is Discovery Service. The Discovery Service API is intentionally compliant with Sentinel but it isn't actually the same thing. See https://docs.redislabs.com/latest/rs/concepts/data-access/discovery-service/.. It worked! While it doesn't appear to be possible to require TLS connections to the Discovery Service, it appears to be possible to use TLS to connect.\nShould I modify this PR to encrypt the connection to Sentinel / Discovery Service if TLS options are supplied or make a seperate PR?. It was easy to implement so I just added it here.\nPlease let me know if there is anything else.. This makes sense. However, what if someone wanted just the connection to Redis to be encrypted, not Sentinel? Or the other way around?\nWhat if we add enableTLSForSentinel and enableTLSForRedis? Both will default to false for this version. In v5, switch them both to default to true. Thoughts?. Makes sense. How's that look?\nTested, works as expected. Testing code above updated.. @luin Awesome. I'm trying to setup a new Redis cluster this week with TLS. When do you think a new version of ioredis could be released with this change? Thanks.. Oh, awesome!! Thanks :). Hahaha, that's what I thought too and I spent so much time trying to figure out why it wasn't working!!\nhttps://nodejs.org/api/tls.html#tls_tls_connect_options_callback\n\nsecureContext: TLS context object created with tls.createSecureContext(). If a secureContext is not provided, one will be created by passing the entire options object to tls.createSecureContext().\n\nThe entire options object is passed to createSecureContext().. The StandaloneConnector works the same: https://github.com/luin/ioredis/blob/master/lib/connectors/StandaloneConnector.ts#L45-L47. ",
    "STRML": "This is particularly useful if you just have heavy read or pub/sub flow on a local machine and need more cores to process the subscriptions.. FWIW, we worked on a fork of this PR for a while, and abandoned it in favor of just putting HAProxy in front of a bunch of read slaves, opening port 6378, and having all the pub/sub clients connect to that. That way, the adapter doesn't have to know anything (this logic is brittle, particularly in regards to reconnects).. ",
    "caffeineshock": "Hey @luin, what's your current feeling about this pull request? Do you think it will get merged into the source eventually or are you rather inclined towards the separate modules solution?\nWe are currently stuck with a cluster that is not managed by sentinel and need to use read/write splitting quite soon. If this pull request might get merged soon, we would like to use the introduced feature. Otherwise we will have to split reading and writing manually by using different clients.. ",
    "azman0101": "Is there any chance that these use-case described in this issue will be one day supported ?\nThis redis chart https://github.com/helm/charts/tree/master/stable/redis also support replication cluster.\nhttps://redis.io/topics/replication. ",
    "CQBinh": "It returns the same result with when I query via cli: x.y.z.1:6379.\nStill returns the master IP.. Update 2: one more thing, I run the app in my local machine with the production environment and everything work like charm!!!. Yeah, finally found.\nIt's my mistake when an old code use the redis module and try to connect directly to localhost.\nReplace it to ioredis instance and it work like charm :)\nAnw, thank for your quickly help.. ",
    "migounette": "@luin Thanks for this quick reply:\nEg. \n    var redisCnx = new Redis({\n        'sentinels': [ { 'host': redisInfo.hostname, 'port': redisInfo.port } ],\n        'name': config.master\n    });\nredisCnx.on('ready', function() {\n  this.serverInfo.xxxx\n}\n\n\nThis structure is a return of INFO method from a redis module not frm io-redis\nioredis\\lib\\redis\\event_handler.js\n\nif (self.options.enableReadyCheck) {\n  self._readyCheck(function (err, info) {\n    if (err) {\n      if (!flushed) {\n        self.flushQueue(new Error('Ready check failed: ' + err.message));\n        self.silentEmit('error', err);\n        self.disconnect(true);\n      }\n    } else {\n      self.serverInfo = info;\n      if (self.connector.check(info)) {\n        exports.readyHandler(self)();\n      } else {\n        self.disconnect(true);\n      }\n    }\n  });\n\nA2: In fact I am trying to identify the master IP and port, I looked at the code and I found that you already use get-master-addr-by-name, what about returning the information from the connected callback ?\nredisCnx.on('ready', function(server) { \n  if (server !== undefined) {\n       _self.logger.info(server.ip + ':' + server.port);\n  }\n\n}\nJust let me know I can create a PR if you agree.\nMy 2 cents\n. ",
    "reconbot": "Updated the PR from the review notes.. Alrighty - I'd rather put // optional but I'm not strong on it. Updated PR!. I'm seeing this too, can you link to the other open issues on this?. I was seeing this as a result of internal commands on https://github.com/luin/ioredis/pull/493 and not something I can catch.. I manually hoisted the var declarations. Isn't that automatic?. I think I misunderstood you. Yes the vars are assigned in the loop. Are you saying that not using an intermediate variable (for readability and clarity) will improve the performance?. Fixed up the semicolons =). If I can get any advice on this I'm happy to submit a patch fixing it (along with a better test). It's still an issue. Still an issue. We could copy the sentinel objects, but I get your point. I'll patch.. As a new user to sentinels and not finding any docs for it in the api.md having this clue would have saved me a bit of headache.. I didn't understand that fact. I thought sentinels overrode it. I was coming from a project that used urls.. ",
    "qixin1991": "\u4f3c\u4e4e\u627e\u5230\u7b54\u6848\u4e86\uff0c\u5e94\u8be5\u662f redis \u5728 cluster \u6a21\u5f0f\u4e0b\u4e0d\u652f\u6301\u8fd9\u4e48\u5e72\u3002 \nhttps://github.com/antirez/redis/issues/2541. ",
    "WClouds": "Meet the same error!. ",
    "SamBergeron": "Possible fix with #446 . Added a callback to silentEmit so that if none of the sentinels can connect we report an error, also added sentinelError reports to know which of the individual sentinels aren't connection if you're listening for that.. Just following up on this? Thoughts @luin ?. ",
    "sanket-naik-zocdoc": "I don't understand how name is used here.\nAlso, what does this line do? https://github.com/luin/ioredis/blob/master/lib/connectors/sentinel_connector.js#L135\nI couldn't find definition of method sentinel\n. @luin thanks for the details. One last question before I close this issue; is there a way to use any of the existing api in ioredis to scale reads if I can provide list of slaves (AWS replications)?. That is a really good alternative.. thanks for the suggestion!\nI would close this issue. ",
    "zbmowrey": "Closing issue - I was trying to access the Promise object rather than the value. Thanks for the response, the tip to check 'monitor' is something I won't soon forget! :). ",
    "cmhsu-cs": "@luin Got it. If I were to get all the values of a redis sorted set (11 million values) and 11 million hashes, would the operation be async? Meaning would it block or not block the main event loop for my Node.js server?. @luin Sorry what do you mean by the parsing process?. ",
    "achselschweisz": "I added the PR to show you that the rejections need to be handled, I don't really have the time to dig through the depths of this - Cheers!\nI'll remove the PR and convert it to a proper issue instead. ",
    "benzaita": "I am seeing this, when simulating a sudden connection drop (e.g. shutting down WiFi):\nUnhandled rejection Error: Connection is closed.\n    at close (node_modules/ioredis/lib/redis/event_handler.js:101:21)\n    at Socket.<anonymous> (node_modules/ioredis/lib/redis/event_handler.js:76:14)\n    at Object.onceWrapper (events.js:316:30)\n    at emitOne (events.js:115:13)\n    at Socket.emit (events.js:210:7)\n    at TCP._handle.close [as _onclose] (net.js:548:12)\nAnd with this coming in the future, it could be a problem:\n\nIn the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.. @luin you were correct - it was an unsubscribe() command that was sent just before disconnect().\n\nI guess it's not related to this issue then.. ",
    "Permagate": "That solves it, thanks.. ",
    "mopduan": "\u518d\u8bf7\u6559\u4e0b\uff0c\u4e5f\u5c31\u662f\u8bf4node_redis\u4e5f\u65e0\u9700\u663e\u5f0f\u4f7f\u7528quit\uff1f\n\u8c22\u8c22\uff01. \u591a\u8c22\uff01. ",
    "kakasal": "redis.on('error', err => {\n  window.location.href = \"https://stackoverflow.com?q=\"+err;\n});. ",
    "efkan": "When using async loop, I have to execute pipeline to clear Node.js stack like the following pseudo code.\n(Instead of _queue I've applied another variable  [let pipelineCmdCount = 0]  to count commands which have been put into the pipeline)\nPseudo code:\nlet pipeline = redis.pipeline()\nlet someNum = 10000\nlet i = 0\nlet test = () => { return i < someNum }\nlet repeatIt = (done) => {\n    if (someCondition == \"user\") {\n        let key = \"user:\"+ user_id\n        pipeline.set(key, someValue)\n        if (pipeline._queue.length % 1000 == 0) { // clearing v8 engine stack to prevent max stack size exceeded error\n            pipeline.exec((err, results) => {\n                pipeline = redis.pipeline()\n                setTimeout(done)\n            })\n        } else {\n            process.next(done)\n        }\n    } else {\n       let class = users[i].classrooms\n        pipeline.set(someKey, someValue)\n        let j = 0\n        let test2 = () => { j < class.students.length }\n        let repeatIt2 = (done2) => {\n            let student_id = class.students[j]._id\n            let key = \"student:\" + student_id\n            pipeline.set(key, someValue)\n            if (pipeline._queue.length % 1000 == 0) { // clearing v8 engine stack to prevent max stack size exceeded error\n                pipeline.exec((err, results) => {\n                    pipeline = redis.pipeline()\n                    setTimeout(done2)\n                }\n            } else {\n                process.next(done2)\n            }\n        }\n        let results2 = (err, results) => {\n            if (pipeline._queue.length % 1000 != 0) {\n                pipeline.exec((err, results) => {\n                    pipeline = redis.pipeline()\n                    done()\n                }\n            } else {\n                done()\n            }\n        }\n        asyncModule.whilst(test2, repeatIt2, results2)\n    }\n}\nlet results = (err, res) => {\n    if (pipeline._queue.length % 1000 != 0) {\n        pipeline.exec((err, results) => {\n            // Completed\n        })\n    } else {\n        // Completed\n    }\n}\nasyncModule.whilst(test, repeatIt, results). ",
    "sophister": "@luin  thx! BTW, when ioredis stops reconnecting according to retryStrategy, how can I make it retry again after some time when redis is up? Manually restart server is not cool you know, :). Got it, thx!. ",
    "taichunmin": "Score can be duplicate, but member should be unique. So score must be value, member should be key.\nA real use case is sort score in a game. Every player will have a score, but some people may have the same score (and have same rank). If the player reach a new score, old score should be update.. I'd like to send a PR, but I still have some question:\n\nzadd not only have 2 argument, so may I only check the last args?\nredis\nZADD key [NX|XX] [CH] [INCR] score member [score member ...]\nOrder in plain object is obvious, but I have no idea which order is better in Map:\nmyMap.set(\"member\", \"score\");\nmyMap.set(\"score\", \"member\");. I only compare the stars on there Github Repo.\nhttps://github.com/eslint/eslint - 7778 stars\nhttps://github.com/feross/standard - 11979 stars. \n",
    "yunnysunny": "If you want to add multi data to a sorted set, you may write such code:\n```javascript\nconst len = 5;//The length may be the count of some records getten from some db\nconst originalData = [\n{score:1,value:'a'},\n{score:2,value:'x'},\n{score:3,value:'f'},\n{score:4,value:'e'},\n{score:5,value:'j'},\n];\nvar params = new Array(len+2);\nparams[0] = 'a_sorted_redis_key';\nvar i = 1;\nfor (var item of originalData ) {\n    params[i++] = [item.score,item.value];\n}\nparams[i] = function(err) {\n    console.log(err);\n}\nredis.zadd.apply(redis,params);\n```\nIt is so inconvenient. @luin @yunnysunny. ",
    "lijicheng123": "`\nvar session = require('express-session');\nvar Redis = require('ioredis')(session);\nvar redis = new Redis.Cluster([{\nhost:'192.168.0.12',\nport:123\n}]);`\nthat is it ?. ",
    "tnguyen14": "Do I need for the redis client to be connected before calling app.use(session..?. ",
    "anuradhag": "On logout, getting this error, Unhandled rejection ReplyError: CROSSSLOT Keys in request don't hash to the same slot. Everything else works. Tried doing Hashtag by making the prefix to be {sess}.. Having raised this issue with them too https://github.com/expressjs/express/issues/3646. Unhandled rejection ReplyError: CROSSSLOT Keys in request don't hash to the same slot\n  at JavascriptRedisParser.replyParser.Parser.returnError (/usr/src/app/node_modules/ioredis/lib/redis/parser.js:24:25)\nat JavascriptRedisParser.execute (/usr/src/app/node_modules/redis-parser/lib/parser.js:572:12)\nat Socket.<anonymous> (/usr/src/app/node_modules/ioredis/lib/redis/event_handler.js:107:22)\nat emitOne (events.js:90:13)\nat Socket.emit (events.js:182:7)\nat readableAddChunk (_stream_readable.js:147:16)\nat Socket.Readable.push (_stream_readable.js:111:10)\nat TCP.onread (net.js:523:20). I get this above stack trace every 10 seconds, even when my app is idle. Not sure, If it is something to do with my configuration. How do I proceed further? Please help.. ",
    "weconquered": "1) 1) (integer) 5461\n   2) (integer) 10922\n   3) 1) \"192.168.30.92\"\n      2) (integer) 6379\n      3) \"33c352babcb9795381fb877376760ca46e07105e\"\n   4) 1) \"192.168.30.97\"\n      2) (integer) 6379\n      3) \"bdcb1b5d60d5a72bdfac947cedbbea93ac53f125\"\n2) 1) (integer) 0\n   2) (integer) 5460\n   3) 1) \"192.168.30.98\"\n      2) (integer) 6379\n      3) \"7326b8056677fa6121e205074a71a581df72052c\"\n   4) 1) \"192.168.30.93\"\n      2) (integer) 6379\n      3) \"f952594ea555482cd42f944572c520ffb29fc773\"\n3) 1) (integer) 10923\n   2) (integer) 16383\n   3) 1) \"192.168.30.94\"\n      2) (integer) 6379\n      3) \"7e9a018015678699ecba3d8d89b3d4ccfdb1fc53\"\n   4) 1) \"192.168.30.91\"\n      2) (integer) 6379\n      3) \"4990051c2e6f55484e07e8493cd9c5d67283f4ab\". thank you , it's ok as you say. . ",
    "atwoodjw": "You're right. Should be using then. Resolves the issue. Appreciate the help.. ",
    "damianhodgkiss": "can do.. any preferences what you'd like the option called so it keeps with your option naming?  else maybe just refreshSlotsTimeout (on the main cluster config/options). ",
    "christianpv": "This is error that I get while compiling my code:\nERROR in ./~/redis-parser/lib/hiredis.js\nModule not found: Error: Can't resolve 'hiredis' in '/Users/christianpv/homer/gapi/node_modules/redis-parser/lib'\n@ ./~/redis-parser/lib/hiredis.js 3:14-32\n@ ./~/redis-parser/lib/parser.js\n@ ./~/redis-parser/index.js\n@ ./~/ioredis/lib/redis/parser.js\n@ ./~/ioredis/lib/redis.js\n@ ./~/ioredis/index.js\n@ ./src/server/index.ts\nUpdate: @luin thanks for the quick response. I'm using my fork, and its working correctly.. @luin I'm using TypeScript, and Webpack. I will create a repo reproducing the error. \nUpdate: https://github.com/christianpv/test-ioredis-parser\nReadme has instruction to setup, and reproduce.. I understand what you are trying to said but its common to use Webpack for both.\nhttps://stackoverflow.com/a/37920356\nhttps://github.com/DxCx/webpack-graphql-server\nhttps://github.com/kriasoft/react-starter-kit/blob/master/tools/webpack.config.js#L280\nhttps://github.com/zeit/next.js#customizing-webpack-config\nThat is why Webpack provides the ability to specify the build output node.\nI have four options. \n\nContinue to use my fork, and keep it up to date until this PR gets merge.\nAdd hiredis to my dependencies.\nTry using ignore plugin but I don't know if that will work: https://webpack.js.org/plugins/ignore-plugin/\nThis PR gets merge, and I update to latest version.\n\nI hope this gets merge because I wouldn't like anyone else to get this issue. I'm going with option 2. Thanks a lot for your time!. @luin I'm not using the hiredis parser, I'm just adding it to my dependencies so that Webpack does not give me an error. Webpack includes all the modules that are require by you, and your dependencies in the final bundle.. ",
    "shaylevi2": "+1. No :( tried that\nI don't know where it's coming from.. prof.txt\nProfiler output file attached.. Why would I use that? \nNode has a built in profiler... node --prof which is statistical and way more informative.. But if I'm not mistaken, these writes seem to have come from ioredis\n72399   29.4%  write\n\u00a029443   40.7%    LazyCompile: Command.transformReply \n\u00a0\u00a029443  100.0%      LazyCompile: ~ /app/node_modules/ioredis/lib/command.js:191:19\n\u00a0\u00a026560   90.2%        LazyCompile: exports.returnReply /app/node_modules/ioredis/lib/redis/parser.js:76:32\n\u00a0\u00a026560  100.0%          LazyCompile: returnReply /app/node_modules/ioredis/lib/redis/parser.js:26:27\n\u00a0\u00a026560  100.0%            LazyCompile: execute /app/node_modules/redis-parser/lib/parser.js:521:60. @luin is there a way to push this forward? \nredis-parser 3 has far better performance.. ",
    "SimenB": "Standard uses eslint under the hood.\nI'd suggest doing eslint:recommended and prettier. Then you get warnings about code patterns, and all style is autofixed. ",
    "ProfileSeeker": "I personally prefer standard, it's more strict but I am not working on this project :man_facepalming:. ",
    "joze2016": "Yes, you're right, thanks.. Ok, we will try to implement mget ourselves.. ",
    "vdixit14": "Hi, No we are not invoking SELECT or setting DB option explicitly. We are using 3.1.1. Pastebin of our cache code https://pastebin.com/g4MbNXEk . The error message shared in the first comment is with \"showFriendlyErrorStack\" enabled. I had manually enabled and got the same message.\nExact Error Seen\nYou have triggered an unhandledRejection, you may have forgotten to catch a Promise rejection:\nReplyError: ERR SELECT is not allowed in cluster mode\nat JavascriptRedisParser.replyParser.Parser.returnError (/opt/app/node_modules/ioredis/lib/redis/parser.js:25:25)\nat JavascriptRedisParser.execute (/opt/app/node_modules/ioredis/node_modules/redis-parser/lib/parser.js:572:12)\nat Socket. (/opt/app/node_modules/ioredis/lib/redis/event_handler.js:107:22)\nat Socket.emit (events.js:107:17)\nat readableAddChunk (_stream_readable.js:163:16)\nat Socket.Readable.push (_stream_readable.js:126:10)\nat TCP.onread (net.js:540:20). We checked and no other Redis instance is running. Will wait for your feedback from looking at our code.. ",
    "Angus-Accedo": "Hi luin\nI'm a friend of vdixit14.\nWe would like to use a Master/Read Replica set for a test on its ability to handle large scale loads.\nWe can see that the documentation focuses mainly on the configuration parameters (eg ScaleReads)  for the Single and Cluster scenarios.\nCould you confirm the configuration requirements when targeting a  Read Replica set ?\nthanks.\n. ",
    "barwin": "Thanks for the quick merge!. ",
    "JohnLou": "@luin \nOK, I get it. It's a mistake in my program. \nThanks!. ",
    "gheidorn": "That was it!  Thank you so much for your help.. ",
    "rubens21": "Hi guys,\nI am happy this tip is useful. It was not catch by myself, but for my work mate. I was the responsible for finding a solution though.\nSo, If you guys don't mind I would be happy if I could work on it and create a PR. \n. ",
    "synologic": "Exactly, thanks for pointing that out ... . ",
    "chris-olszewski": "I would argue that if someone is already pulling in all of lodash instead of the functions they need, they don't care about the very minor bloat of 16 functions. \nAlso, a quick experiment on a fresh project:\nioredis 3.1.1\nyarn why ioredis\nyarn why v0.27.5\n[1/4] Why do we have the module \"ioredis\"...?\n[2/4] Initialising dependency graph...\n[3/4] Finding dependency...\n[4/4] Calculating file sizes...\ninfo Has been hoisted to \"ioredis\"\ninfo This module exists because it's specified in \"dependencies\".\ninfo Disk size without dependencies: \"240kB\"\ninfo Disk size with unique dependencies: \"6.16MB\"\ninfo Disk size with transitive dependencies: \"6.18MB\"\ninfo Number of shared dependencies: 8\nioredis git://github.com/chris-olszewski/ioredis.git#get_rid_of_monolithic_lodash\nyarn why ioredis\nyarn why v0.27.5\n[1/4] Why do we have the module \"ioredis\"...?\n[2/4] Initialising dependency graph...\n[3/4] Finding dependency...\n[4/4] Calculating file sizes...\ninfo Has been hoisted to \"ioredis\"\ninfo This module exists because it's specified in \"dependencies\".\ninfo Disk size without dependencies: \"580kB\"\ninfo Disk size with unique dependencies: \"2.18MB\"\ninfo Disk size with transitive dependencies: \"2.2MB\"\ninfo Number of shared dependencies: 23. ",
    "Siilwyn": "Yes please, I was already wondering why ioredis is so much bigger than node_redis, lodash takes up 5MB. For small projects this saves space and for projects with a lot of dependencies deduping can happen on lodash.x level.. ",
    "SamuelMarks": "Sure thing. Good to have clarification.. ",
    "miguelmota": "May be a good idea to throw a console log warning mentioning it might deprecated in the future.. ",
    "zag2art": "I'm for the PR. ",
    "mblum14": "This would be great! we were forced to talk to redis over ssh and I would love for this to be accepted! I will donate my firstborn if this makes it in.. ",
    "TomerAmirV": "Thanks!\nThat worked!\ndidn't understand that part in the documentation... \ud83d\ude05 . ",
    "chameleonbr": "Thanks!!!. ",
    "MichelDiz": "Is there any documentation that I can put together a test of this feature?\nDo I have to do a Lua Script?\nUnhandled rejection ReplyError: ERR unknown command 'GRAPH.CREATENODE'. Okay, I'm using Redis on Docker. I have to figure out how to do it through (local) Docker. I would have to create a new image in that case.. I know that Join-monster maps all Schema and hence it uses a dialect according to the type of SQL dialect. It has MariaDB, MySQL, Oracle, PG, Sqlite3.\nI'll need to set up a testing environment to test this hypothesis. Add another dialect to Join-Monster by mapping GraphQL to the Redis commands. \nI know that redis.call () is perfect for this situation. But, one detail, is it possible to use \"Multi\" within \"Redis.call\"? If not, would it be possible to implement a \"Redis.Raw ()\"?\nCheers. Before starting something, I did some mental rehearsals in this sketch, taking/using my little knowledge on the Join-Monster tool. \nThe query simplified\ngraphql\n{\n  user(id: 999) {\n    id\n    email\n    fullName\n    favNums \n    posts {\n      title\n      body\n      comments {\n        body\n      }\n    }\n  }\n}\nI think it would be a bit more complex to develop this feature. It's not impossible, but it's rather complicated (Comparing how Join-Monster produces its results.). Just analyzing this simple Query with many-to-many using simple Redis commands. It got a bit complicated, I believe. Perhaps in Graph mode (using the module) it is easier and more dynamic.\nSo. Asking for User + Fav numbers + Posts from that user and comments inside the posts from others users\ngraphql\n{\n  user(id: 999) {   # this whole Query is resolved with  .MULTI command\n    id         = HGET user:id-999 id\n    email      = HGET user:id-999 email\n    fullName   = HGET user:id-999 fullName\n    favNums    = GET  user:id-999:favNums Nums\n        # This part above could  be generated with the command bellow \n        # if requested all fields but still needs .MULTI for posts and \n        # favNums because they are in different fields/Keys.\n           // HGETALL user:id-999\n                       return = {\n                                \"id\": \"id-999\",\n                                \"email\": \"micheldiz@ea.com\",\n                                \"fullName\": \"Michel Torrado\",\n                              }\n    posts {    = HGET user:id-999:posts \n      #Need to solve a many_to_many problem here\n      #I think it would have to create a loop with SCAN inside the \n      #Join-monster and then it will try to make a new call for each \n      #post on a new MULTI command. This is just an abstract sketch.\n      title\n      body\n      comments { = HGET user:id-999:posts:${post_id:X}:comments\n        body\n      }\n    }\n  }\n}. The need with .multi is that Join-Monster could in one \"Redis batch\" request all Query needs/requests. You see? instead of send lots of little request to Redis. Maybe it's my placebo thinking that way.\nAnd it can have a \"dialect\" for each Redis relations solution or even modules. If someone uses RedisGraph it would be enough to require the dialect for this module. Or even a dialect for each style, for example a dialect for those using Ohm.. ",
    "Jonahss": "I created a specific RedisGraph client based on this.\nhttps://github.com/Jonahss/ioredisgraph. ",
    "madbean": "hello no it works, thx\ngreat ! have nice day.\nredis.geoadd(id, latitude, longitude, key);\n redis.georadius(id, latitude, longitude, distance, unit, 'WITHCOORD');. ",
    "ShadowMurloc": "@luin  \u77e5\u9053\u4e86  \u591a\u8c22\u591a\u8c22 . ",
    "southpolesteve": "I was curious how reduce would perform but it looks worse: https://jsperf.com/object-keys-reduce-vs-for-loop/1\nUpdated to use Object.keys + a for loop. Also added a test for an object without a prototype. ",
    "realknack": "@tuananh\nNot sure if I understand your question but here's how it usually works in \"serverless\" (e.g AWS Lambda):\nCode is ran in isolated container. If container has been dropped (haven't been triggered for several minutes, e.g by API call from actual user), it has to reinitialize container, download, unzip, parse & run the code again. It's called cold start & can take second(s) if zip file is too big.\nLess code == faster start == better user experience == cheaper because it's usually priced per 100ms.\n\nAnd yes, Im currently building & uploading the code manually to serverless service provider but it's only because Im testing & trying things out. I would probably have an automated dev pipeline in production.. @tuananh \nNo, it only downloads & unzips on cold start & you don't get too many cold starts if traffic is medium-high.\nYou're probably right about no huge time differences but for example, https://github.com/luin/ioredis/pull/494 - such a small change & already cuts ioredis size in bundle ~25-50% (~25% smaller if only uglified, ~50% smaller if also zipped).. Nice! Package codebase itself is relatively small & author(s), maintainer(s) probably know the best, if & which dependencies are crucial + there's definitely some extra code to support older versions of Node, not going to protest against that. \nAnyway, I feel a lot better using this package now that #494 is merged, Im going to close this issue. It doesn't look like it's something people are much worried about anyway.. ",
    "buzai": "@luin \u3000\u8c22\u8c22. @luin \u4e0d\u884c\u6587\u6863\u8fd8\u662f\u592a\u5c11\u4e86\uff0c\u8fd8\u5f97\u770b\u6e90\u7801. ",
    "LastKing": "\u3002\u3002\u3002\u6211\u8fd9\u82f1\u6587\u3002\u3002\u3002\u6211\u7684\u610f\u601d\u662f redis\u6211\u672c\u5730\u8fde\u4e0d\u4e0a\u670d\u52a1\u5668\u7684\u96c6\u7fa4\uff0c\u670d\u52a1\u5668\u7684redis \u914d\u7f6e\u662f\n\u4e24\u4e2a\u6ce8\u91ca\u7684bind 127.0.0.1 \uff0c\u6ca1\u6709\u5176\u4ed6\u7684\u3002\u8fd9\u6837\u4e0d\u5199\u96be\u9053\u4e0d\u7b49\u4e8ebind 0.0.0.0\u4e48\uff1f\n\u6211\u8fde\u63a5\u7684\u65f6\u5019\u7528\u7684\u662f\u516c\u7f51ip\uff0c\u4f46\u662f\u7206\u51fa\u6765\u7684\u9519\u8bef\u5168\u90e8\u6307\u5411\u4e86127.0.0.1 \u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48. ",
    "betweenbrain": "@luin Do you happen to have a simple example the shows\n\nHTTP callbacks in your App to see whether the request handler will keep waiting for redis commands. \n",
    "aojiaotage": "Thank you for replying!\nI'm a little confused about the response transformers you kindly mentioned. Did you mean the wrapping work I did my self after getting the result?\nIMHO, wrapping the result in a way that meets the ES standard could be a little more better than getting the 'raw' data.\n@AVVS . ",
    "kabalage": "Thank you for your response! Is the underlying connection reused for multiple subsequent timeouted BRPOP-s?. Ok, thanks!. ",
    "leoDreamer": "@yacut I have a question when connect to k8s's redis cluster , can i use svc node in ioredis config?. @yacut Thanks for reply, actually our OPS use one proxy svc node, I can't find the relevant config in ioredis. I know it now, thanks agin. ",
    "p3x-robot": "ok, thanks. I tried with removeListener, but I get this error:\ntext\n2018-1-4 17:33:53 unhandledRejection TypeError [ERR_INVALID_ARG_TYPE]: The \"listener\" argument must be of type Function\n    at Redis.removeListener (events.js:293:15)\n    at Redis.onMessage (/home/patrikx3/Projects/patrikx3/corifeus/corifeus-server/src/service/redis/communicate.js:178:36)\n    at Redis.emit (events.js:164:20)\n    at Redis.exports.returnReply (/home/patrikx3/Projects/patrikx3/corifeus/corifeus-server/node_modules/ioredis/lib/redis/parser.js:107:14)\n    at JavascriptRedisParser.returnReply (/home/patrikx3/Projects/patrikx3/corifeus/corifeus-server/node_modules/ioredis/lib/redis/parser.js:27:13)\n    at JavascriptRedisParser.execute (/home/patrikx3/Projects/patrikx3/corifeus/corifeus-server/node_modules/redis-parser/lib/parser.js:574:12)\n    at Socket.<anonymous> (/home/patrikx3/Projects/patrikx3/corifeus/corifeus-server/node_modules/ioredis/lib/redis/event_handler.js:107:22)\n    at Socket.emit (events.js:159:13)\n    at addChunk (_stream_readable.js:265:12)\n    at readableAddChunk (_stream_readable.js:252:11)\n    at Socket.Readable.push (_stream_readable.js:209:10)\n    at TCP.onread (net.js:608:20)\nOf course I use a function, arrow, asyn arrow, async function, but same error.\nHow can I remove?. Right now, I have tons of subscriber.on('message', fun), and every request,just more and more functions. How can I get rid of it?. never mind, i found it.\ni needed:\nsubscriber.removeListener('message', onMessage);. weird, because for me it works.. ",
    "7demo": "@tuananh   thank you for your answer . . ",
    "elelement": "Sorry. when I said:\n\nThis is what redisConfig.cluster contains:\n\nI meant:\n\nThis is what redisConfig contains:. OMG. I had a self developed module included in my application that was using another ioredis client instance, with 127.0.0.1 hardcoded and thus, causing the whole program to fail (without telling me where in the code). Sorry for the issue, but I really didn't know what was happening. Thanks!. \n",
    "KromDaniel": "Hey @luin , I know it's old topic, was wondering if this is possible with pipeline? sending a custom command\nThanks!\nEdit: found the solution using createBuiltinCommand. @fas3r something like\n```\n// cache me, client is ioredis instance\nconst cmd = client.createBuiltinCommand('JSON.GET');\nconst getJson = cmd.string;\nconst getJsonBuffer = cmd.buffer;\n// create pipeline\nconst pipeline = client.pipeline();\ngetJson.call(pipeline, 'argA', 'argB', 'argC');\ngetJsonBuffer.call(pipeline, 'argA', 'argB', 'argC');\npipeline.get('someThing');\n// regular use\ngetJson.call(client, 'regular', 'get', 'json');\n```\nAnother option to create commander is via:\n```\nconst Commander = require('ioredis/lib/commander');\nconst commander = new Commander();\nexport const JSONGet = commander.createBulletinCommand(\"JSON.GET\");\nexport const JSONSet = commander.createBulletinCommand(\"JSON.SET\");\nexport const JSONDel = commander.createBulletinCommand(\"JSON.DEL\");\n```. @luin  thanks, unfortunately I'm up to-date\n```\n// Type definitions for ioredis 4.0\n// Project: https://github.com/luin/ioredis\n// Definitions by: York Yao https://github.com/plantain-00\n//                 Christopher Eck https://github.com/chrisleck\n//                 Yoga Aliarham https://github.com/aliarham11\n//                 Ebrahim https://github.com/br8h\n//                 Shahar Mor https://github.com/shaharmor\n//                 Whemoon Jang https://github.com/palindrom615\n//                 Francis Gulotta https://github.com/reconbot\n// Definitions: https://github.com/DefinitelyTyped/DefinitelyTyped\n// TypeScript Version: 2.8\n/ =================== USAGE ===================\n    import * as Redis from \"ioredis\";\n    const redis = new Redis();\n =============================================== /\n```. what is your TS version? :)\nOn Sat, 6 Oct 2018, 9:00 PM \u5b50\u9a85, notifications@github.com wrote:\n\nNot sure where the problem is. Here's my setup:\n@types/ioredis: 4.0.3\nioredis: 4.0.0\n[image: image]\nhttps://user-images.githubusercontent.com/635902/46574380-b7480400-c9d4-11e8-85b4-56e062fb327f.png\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/issues/712#issuecomment-427594254, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AQbtsaGSAh3I37c_zd_T5sLuHuAcj_HGks5uiO--gaJpZM4XF31T\n.\n. It seems that the tsconfig.json file didn't have the key allowSyntheticDefaultImports set for true, now it's fixed. \n",
    "lmj0011": "js\nredis.call('JSON.GET', 'foo')\n\nredis.call()\n\nHow would one know that this method exists without it being documented in the API docs?\nhttps://github.com/luin/ioredis/blob/master/API.md. ",
    "fas3r": "Hello @KromDaniel , \nwould you mind to give an example ? I did not find how to use createBuiltinCommand with REJSON . Maybe that's because of the . in the commands.\nThanks.. Hello @KromDaniel , \nI did not know about commander I will look into this. If not I found this : https://github.com/evanhuang8/iorejson/blob/master/lib/rejson.js \nShould be easy to adapt. \nThanks.. ",
    "serv": "As per @luin 's comment, this worked well.\njs\nredis.call('JSON.GET', 'foo')\nI just wanted to note that, the result from this is a Buffer object. So in order to parse it I had to do this.\n```js\nredis.call('JSON.GET', 'foo')\n  .then(result => {\n    const stringified = result.toString('utf8');\n    const parsed = JSON.parse(stringified);\n  });\n```. ",
    "legolasNg": "sorry to trouble you, it's my fault, not the multi(). ",
    "stephan-nordnes-eriksen": "I found the issue. I had, for some reason, linked to the wrong redis instance. Correcting that successfully made the connection, with the approach above. For anyone looking to do this in the future, this would probably be the easiest way to achieve the ssh tunneling. . ",
    "pranavsreedhar92": "Thu, 05 Oct 2017 09:17:01 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:01 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:01 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:01 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:01 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:01 GMT ioredis:redis status[ip]: connect -> close\nThu, 05 Oct 2017 09:17:01 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:01 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:01 GMT ioredis:cluster status: disconnecting -> close\nThu, 05 Oct 2017 09:17:01 GMT ioredis:cluster status: close -> reconnecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster Cluster is disconnected. Retrying after 444ms\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: reconnecting -> connecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[HOST: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster getting slot cache from HOST\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[HOST]]: wait -> connecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis queue command[0] -> cluster(slots)\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[IP]]: connecting -> connect\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis write command[0] -> auth([Password])\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis write command[0] -> info()\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[IP]]: connect -> ready\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection send 1 commands in offline queue\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis write command[0] -> cluster(slots)\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip Adderess]: [empty] -> wait\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: connecting -> connect\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> connecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis queue command[0] -> cluster(info)\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: connecting -> connect\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis write command[0] -> info()\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[IP]]: ready -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[IP]]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: connect -> disconnecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: wait -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: connect -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[ip]: close -> end\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: disconnecting -> close\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: close -> reconnecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster Cluster is disconnected. Retrying after 446ms\nThu, 05 Oct 2017 09:17:02 GMT ioredis:cluster status: reconnecting -> connecting\nThu, 05 Oct 2017 09:17:02 GMT ioredis:redis status[[HOST]]: [empty] -> wait\n. cluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:6\ncluster_size:3\ncluster_current_epoch:12\ncluster_my_epoch:11\ncluster_stats_messages_ping_sent:139414\ncluster_stats_messages_pong_sent:138627\ncluster_stats_messages_sent:278041\ncluster_stats_messages_ping_received:138627\ncluster_stats_messages_pong_received:139414\ncluster_stats_messages_received:278041. ",
    "jegsar": "Interesting when I set DEBUG=ioredis:* it seems to work better most of the time but is sometimes still slow.\nHere is a log where 2.2.2.2 is the external ip of the remote redis server and 1.1.1.1 is the internal ip which is not directly accessible. What I really don't understand is how enabling debug helps the situation.\n\"C:\\Program Files\\nodejs\\node.exe\" C:\\CBDrive\\src\\circleblack-js\\node_modules\\mocha\\bin\\_mocha C:\\CBDrive\\src\\test\\api\\securities\\create-update-spec.js\nSat, 07 Oct 2017 17:29:54 GMT ioredis:cluster status: [empty] -> connecting\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis status[url:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis status[url:8001]: [empty] -> wait\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis status[url:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:29:54 GMT ioredis:cluster getting slot cache from url:8002\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis status[url:8002]: wait -> connecting\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:29:54 GMT ioredis:redis status[url:8000]: wait -> connecting\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8002]: connecting -> connect\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: connecting -> connect\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8002]: connect -> ready\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: connect -> ready\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[url:8001]: wait -> close\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[url:8001]: close -> end\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[1.1.1.1:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8001]: [empty] -> wait\nSat, 07 Oct 2017 17:29:55 GMT ioredis:cluster status: connecting -> connect\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8002]: ready -> close\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8002]: close -> end\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: ready -> close\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: close -> end\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[1.1.1.1:8002]: wait -> connecting\n[2017-10-07 13:29:55.876] [INFO] api - [POST] [/securities] [7f5c8fdf-6eae-4396-95bb-bc444e9b8f72] [Authenticating] \n[2017-10-07 13:29:55.876] [DEBUG] orm - [auth] [validateSession] [validating]\nSat, 07 Oct 2017 17:29:55 GMT ioredis:cluster getting slot cache from 2.2.2.2:8000\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: wait -> connecting\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: connecting -> connect\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: connect -> ready\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[1.1.1.1:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:29:55 GMT ioredis:delayqueue send 1 commands in failover queue\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8001]: wait -> connecting\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis queue command[0] -> cluster(info)\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: ready -> close\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8000]: close -> end\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8001]: connecting -> connect\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[2.2.2.2:8001]: connect -> ready\nSat, 07 Oct 2017 17:29:55 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis write command[0] -> cluster(info)\nSat, 07 Oct 2017 17:29:55 GMT ioredis:cluster status: connect -> ready\nSat, 07 Oct 2017 17:29:55 GMT ioredis:cluster send 1 commands in offline queue\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis status[1.1.1.1:8000]: wait -> connecting\nSat, 07 Oct 2017 17:29:55 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:05 GMT ioredis:connection error: Error: connect ETIMEDOUT\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[1.1.1.1:8002]: connecting -> close\nSat, 07 Oct 2017 17:30:05 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[1.1.1.1:8002]: close -> end\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[2.2.2.2:8002]: wait -> connecting\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[2.2.2.2:8002]: connecting -> connect\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[2.2.2.2:8002]: connect -> ready\nSat, 07 Oct 2017 17:30:05 GMT ioredis:connection error: Error: connect ETIMEDOUT\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[1.1.1.1:8000]: connecting -> close\nSat, 07 Oct 2017 17:30:05 GMT ioredis:connection skip reconnecting because `retryStrategy` is not a function\nSat, 07 Oct 2017 17:30:05 GMT ioredis:redis status[1.1.1.1:8000]: close -> end\nSat, 07 Oct 2017 17:30:06 GMT ioredis:cluster getting slot cache from 2.2.2.2:8002\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[1.1.1.1:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:30:06 GMT ioredis:delayqueue send 1 commands in failover queue\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8002]: ready -> close\nSat, 07 Oct 2017 17:30:06 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8002]: close -> end\nSat, 07 Oct 2017 17:30:06 GMT ioredis:cluster getting slot cache from 2.2.2.2:8000\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: wait -> connecting\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: connecting -> connect\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: connect -> ready\nSat, 07 Oct 2017 17:30:06 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[1.1.1.1:8002]: wait -> close\nSat, 07 Oct 2017 17:30:06 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[1.1.1.1:8002]: close -> end\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[1.1.1.1:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:30:06 GMT ioredis:delayqueue send 1 commands in failover queue\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[1.1.1.1:8000]: wait -> connecting\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: ready -> close\nSat, 07 Oct 2017 17:30:06 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:06 GMT ioredis:redis status[2.2.2.2:8000]: close -> end\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection error: Error: connect ETIMEDOUT\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8000]: connecting -> close\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection skip reconnecting because `retryStrategy` is not a function\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8000]: close -> end\nSat, 07 Oct 2017 17:30:16 GMT ioredis:cluster getting slot cache from 2.2.2.2:8002\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: wait -> connecting\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: connecting -> connect\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: connect -> ready\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:30:16 GMT ioredis:delayqueue send 1 commands in failover queue\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: ready -> close\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: close -> end\nSat, 07 Oct 2017 17:30:16 GMT ioredis:cluster command expire is moved to 2.2.2.2:8000\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: wait -> connecting\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:cluster getting slot cache from 2.2.2.2:8000\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: connecting -> connect\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: connect -> ready\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection send 2 commands in offline queue\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8002]: wait -> close\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8002]: close -> end\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8000]: [empty] -> wait\n[2017-10-07 13:30:16.472] [INFO] api - [POST] [/securities] [7f5c8fdf-6eae-4396-95bb-bc444e9b8f72] [Authenticating] Session Validated\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[1.1.1.1:8000]: wait -> connecting\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis queue command[0] -> smembers(sessions:test-token:entitlements)\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: ready -> close\nSat, 07 Oct 2017 17:30:16 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:16 GMT ioredis:redis status[2.2.2.2:8000]: close -> end\n[2017-10-07 13:30:25.894] [DEBUG] orm - [auth] [validateSession] [validating]\nSat, 07 Oct 2017 17:30:25 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:connection error: Error: connect ETIMEDOUT\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[1.1.1.1:8000]: connecting -> close\nSat, 07 Oct 2017 17:30:26 GMT ioredis:connection skip reconnecting because `retryStrategy` is not a function\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[1.1.1.1:8000]: close -> end\nSat, 07 Oct 2017 17:30:26 GMT ioredis:cluster getting slot cache from 2.2.2.2:8002\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8002]: wait -> connecting\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis queue command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8002]: connecting -> connect\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8002]: connect -> ready\nSat, 07 Oct 2017 17:30:26 GMT ioredis:connection send 1 commands in offline queue\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis write command[0] -> cluster(slots)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[1.1.1.1:8002]: [empty] -> wait\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8000]: [empty] -> wait\nSat, 07 Oct 2017 17:30:26 GMT ioredis:delayqueue send 2 commands in failover queue\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8000]: wait -> connecting\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis queue command[0] -> smembers(sessions:test-token:entitlements)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis queue command[0] -> expire(sessions:test-token:entitlements,900)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8000]: connecting -> connect\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis write command[0] -> info()\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8002]: ready -> close\nSat, 07 Oct 2017 17:30:26 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8002]: close -> end\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis status[2.2.2.2:8000]: connect -> ready\nSat, 07 Oct 2017 17:30:26 GMT ioredis:connection send 2 commands in offline queue\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis write command[0] -> smembers(sessions:test-token:entitlements)\nSat, 07 Oct 2017 17:30:26 GMT ioredis:redis write command[0] -> expire(sessions:test-token:entitlements,900)\n[2017-10-07 13:30:26.665] [INFO] api - [POST] [/securities] [7f5c8fdf-6eae-4396-95bb-bc444e9b8f72] [Authenticating] Entitlements Gathered. Looking at the log I agree though I don't understand why it would..\nI do know that when using the CLI there are no issues.\nAlso it seems to work considerably better when I have enabled DEBUG=ioredis:* which is very confusing.. ",
    "kilianc": "@luin thanks, that calls a toString on the response items but I still had to write a parser to get back objects. Thanks for the suggestion about call, very handy :D. ",
    "MR4online": "thanks! now it works. ",
    "btd": "Bump. Bump. ioredis 3.x used bluebird as Promise lib underhood (which has huge number of extensions to Promise). and at 4.x they switched to use native promises.. ",
    "Rush": "Bump - interested in it as well.. ",
    "Gappa88": "interested. ",
    "firedfox": "Bump because we are using more Redis clusters in recent projects. This problem is getting more serious.. ",
    "bheema01": "Thanks for your quick response @luin \nYour suggestion on using pubClient/subClient seems to be keeping the node app UP without shutting it down.. Another related question  - so keeping it in the same thread.\nContinuing to test the retry logic with just one redis instance and one sentinel - I understand, I need at least one redis slave for failover to happen. \nThe idea is to see the retryStrategy in action by setting the failover-timeout to 30000 and down-after-milliseconds to 3000 in the sentinel config\n``javascript\nlet redis = new IOredis({\n  sentinels: [{ host: 'abc.xyz.com', port: 26379 }],\n  name: 'master01',\n  showFriendlyErrorStack: true,\n  retryStrategy: function (times) {\n    Logger.info(Redis: Connection retry #${times});\n    if (times < 20) {\n      return 5000;\n    } else {\n      Logger.error(Redis reconnect max retry limit reached...aborting app);\n      process.exit(1);\n    }\n  },\n  sentinelRetryStrategy: function (times) {\n    Logger.info(Redis Sentinel: Connection retry #${times});\n    if (times < 50) {\n      return 5000;\n    } else {\n      Logger.error(Redis Sentinel reconnect max retry limit reached...aborting app);\n      process.exit(1);\n    }\n  },\n  reconnectOnError: function (err) {\n    var targetError = 'READONLY';\n    if (err.message.slice(0, targetError.length) === targetError) {\n      // Only reconnect when the error starts with \"READONLY\"\n      return true; // orreturn 1;`\n    }\n    console.log(err);\n  }\n});\nio.adapter(socketredis({pubClient: redis, subClient: redis}));\n```\nInterestingly, when I bring the only redis instance down, the node app seems to fail with the error message All Sentinels are unreachable. Retrying from scratch after 5000ms - even though the redis-sentinel is still UP.\nAny thoughts on what I could be missing?\n. Re-opening this issue to see if you have any thoughts on this. \nContinuing to test the retry logic with just one redis instance and one sentinel - I understand, I need at least one redis slave for failover to happen. The idea is to see the retryStrategy in action by setting the failover-timeout to 30000 and down-after-milliseconds to 3000 in the sentinel config\nCurrently using pubClient/subClient, like so\nio.adapter(socketredis({pubClient: redis, subClient: redis}))\nThe node app gets shut down when I bring the only redis instance down and the node app seems to fail with the error message All Sentinels are unreachable. Retrying from scratch after 5000ms - even though the redis-sentinel is still UP.\nAny thoughts/suggestions?\n. ",
    "penglinjiang": "\u5728\u4e3b\u5e93192.16.8.1\u4f7f\u7528\u9632\u706b\u5899\u7981\u6b62\u7aef\u53e36380\u8bbf\u95ee\uff0c\u4e5f\u4f1a\u51fa\u73b0\u540c\u6837\u7684\u60c5\u51b5\u3002\u53d1\u73b0\u6ca1\u6709\u4efb\u4f55\u9519\u8bef\u629b\u51fa\uff0cshowFriendlyErrorStack: true\u4e5f\u6ca1\u6709\u4efb\u4f55\u9519\u8bef\u3002\u4f46\u5c31\u662f\u65e0\u6cd5\u6267\u884credis\u547d\u4ee4\u3002\u6b64\u65f6\u5982\u679c\u91cd\u65b0\u8fd0\u884cnode\u9879\u76ee\uff0c\u5c31\u53c8\u80fd\u6267\u884c\u4e86. ",
    "yiting007": "+1, I have the same issue.. ",
    "admirkadriu": "@luin thank you for the replay. No it`s not disabled on my ioRedis configurations. Config contains only host, password, port and database.. ",
    "shudingbo": "me too\uff0c this problem. ",
    "up73k": "After update to v4.2.0, ioredis began to disconnect without any load operations: \n\nstack=Error: read ETIMEDOUT\n   at TCP.onStreamRead (internal/stream_base_commons.js:111:27)\n\nHow can I find any reasons of this? May be devs have some magic recipe? ). ",
    "LudovicLachance": "Why do I really dont get it??? :D Am I blind?\nWhat method in ioredis is useful to me to use redis pub/sub???. Thank you, I was blind :D. ",
    "slavaGanzin": "Thanks for clarifications. I misinterpret my results. I want to use it like:\nredis.call(`MULTI\nSET foo bar\nSET foo bar\nSET foo bar\nEXEC`)\nBut as I understand now it's redundant, because validation doesn't slowdown query execution, as I thought before.. ",
    "julienvincent": "@luin I am running into some performance issues while using pipeline.\nIn my use case I am wanting to pipe ~1000 redis commands together using something similar to the below pseudo code:\n``js\nconsole.time(\"gen\")\narr.forEach(i => {\n  pipeline.hgetall(key:${i}`)\n})\nconsole.timeEnd(\"gen\")\nconsole.time(\"exec\")\nawait pipeline.exec()\nconsole.timeEnd(\"exec\")\nExecution of the above produces timestamps roughly equal to\ngen: 900.00 ms\nexec: 55.00 ms\n```\nThe fact that it takes 900ms to loop a 1000 length array seems a bit odd to me and makes pipelining un-usable. It would almost be faster to make 1000 individual parallel reads to redis.\nIs there perhaps something I can do to increase performance over here?. ",
    "kobehaha": "\u540e\u6765\u6211\u76f4\u63a5\u6362\u7684\u9ad8\u7248\u672cnode8\u4e86. \u597d\u7684\uff0c\u6211\u8fd9\u8fb9\u518d\u89c2\u5bdf\u4e0b\u3002client<--->\u662f\u4fdd\u6301\u957f\u8fde\u63a5\u5427\uff0c\u80fd\u7528\u7c7b\u4f3c\u8fde\u63a5\u6c60\u7684\u6982\u5ff5\u4fdd\u6301\u591a\u4e2a\u8fde\u63a5\uff0c\u53ef\u80fd\u4f1a\u597d\u4e00\u4e9b. ",
    "eduleboss": "Thank you guy for your response, @luin \nTested and it works fine.. ",
    "ningkexin": "\u975e\u5e38\u611f\u8c22. ",
    "maxcheers86": "works.\nthank you!. ",
    "willrstern": "Also, here's the Redis docs that show that sleep is a good way to simulate a master hanging. https://redis.io/topics/sentinel#testing-the-failover. ",
    "super86": "I have the same problem,when the read or write actions take long time,the Sentinel will chose to switch master/slave. ",
    "Sebmaster": "@luin Could you just give some quick guidance on how you want this solved? I'd be fine with implementing it.. ",
    "liutian": "I had the same problem.. ",
    "xdsCoding": "\u6211\u4eec\u505a\u4e86\u4e00\u4e2aredis\u4e2d\u95f4\u4ee3\u7406\uff0c\u5ba2\u6237\u7aef\u662fc\u7684\uff0c\u4f7f\u7528hiredis\u5ba2\u6237\u7aef\u63a5\u53e3\u4f20\u8fc7\u6765\u7684\u539f\u59cb\u8bf7\u6c42\u5c31\u662f\u8fd9\u79cd\uff0c\u5982\u679c\u6709\u900f\u4f20\u7684\u8bdd\u5c31\u4e0d\u9700\u8981\u89e3\u6790\u547d\u4ee4\u4e86\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f20\u5230redis\u91cc\u9762\u6267\u884c\u547d\u4ee4\u4e86. ",
    "VineyShih": "I am facing the similar issue and solving it by https://github.com/luin/ioredis/issues/365.\nDuplicate of #365. ",
    "divmgl": "Can we get a response on this? I'm just now facing this issue as well. Is the solution to downgrade?. The issue was actually not with ioredis but with redfour which doesn't support using ioredis as a client. ioredis was working just fine.\nNot sure if this will help, but check that the environment variable in the context of the running application is correctly set. Usually that resolves issues for me. . ",
    "GoBrianGo": "I have the same problem,too. Is it solved?. \u77e5\u9053\u539f\u56e0\u4e86\uff0c\u56e0\u4e3aredis-server\u90a3\u8fb9\u8bbe\u7f6e\u4e8630s\u95f2\u7f6e\u8d85\u65f6\uff0c\u6240\u4ee5\u5982\u679c\u621130s\u6ca1\u64cd\u4f5c\u5c31\u4f1a\u65ad\u5f00\uff0c\u7136\u540enode redis\u8fd9\u8fb9\u56e0\u4e3a\u6709\u81ea\u52a8\u91cd\u8fde\u673a\u5236\uff0c\u6240\u4ee5\u7a7a\u95f2\u7684\u65f6\u5019\u5c31\u8fd9\u6837\u4e0d\u65ad\u91cd\u8fde\u3002\u8fd8\u6709\u4e2a\u95ee\u9898\u60f3\u95ee\u4e0b\uff0c\u6211\u7528redis-benchmark\u6d4b\u8bd5redis\u8bfb\u5199\u901f\u5ea6\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u5f53connection\u662f1\u7684\u65f6\u5019\u6bcf\u79d2\u8bfb\u5199\u6bd4\u8f83\u6162\uff0c\u53ef\u662f\u6211\u4eecnode redis\u5c31\u662f\u4e00\u76f4\u53ea\u6709\u4e00\u4e2a\u94fe\u63a5\uff0c\u53ef\u662f\u8fd9\u6837\u5f53\u5e76\u53d1\u9ad8\u7684\u65f6\u5019\u8bfb\u5199redis\u6548\u7387\u5c82\u4e0d\u662f\u4f1a\u5f88\u4f4e\uff1f\u56e0\u4e3ajava\u90a3\u4e9b\u90fd\u662f\u6709\u8fde\u63a5\u6c60\uff0c\u53ef\u80fd\u540c\u65f6\u6709\u5f88\u591a\u4e2a\ud83d\udd17\u5728\u7528\u5462\u3002. @MinStenve \u4fee\u6539\u95f2\u7f6e\u8d85\u65f6\u65f6\u95f4\u662f\u53ef\u4ee5\u7684\uff0c\u4e0d\u8fc7\u8fd9\u4e2a\u8981\u4fee\u6539redis\u90a3\u8fb9\u7684\u8bbe\u7f6e\uff0c\u6216\u8005\u5c31\u662f\u4fdd\u630130s\u7684\u95f2\u7f6e\u65f6\u95f4\uff0credis\u6bcf\u6b21\u65ad\u5f00\u540e\u4f1a\u81ea\u52a8\u91cd\u8fde\u7684\u3002. ",
    "gigi": "Same issue. Any updates?. ",
    "shaunxu": "I implemented a very simple retry mechanism in my test client and it works when a new redis instance came online. I found in https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#liveness Twemproxy team said \n\nTo ensure that requests always succeed in the face of server ejections (auto_eject_hosts: is enabled), some form of retry must be implemented at the client layer since nutcracker itself does not retry a request. This client-side retry count must be greater than server_failure_limit: value, which ensures that the original request has a chance to make it to a live server.\n\nI think retry failure commands in our client side code is correct. I might need to make my retry logic better, for example pooling all failed commands and retry them. Anyway below is my code\n```\n'use strict';\nconst uuid = require('node-uuid');\nconst Redis = require('ioredis');\nconst wait = function (ms) {\n    return new Promise(resolve => {\n        setTimeout(() => {\n            return resolve();\n        }, ms);\n    });\n};\nconst main = async function () {\n    const redis = new Redis({\n        port: 9000,\n        host: 10.211.55.6,\n        enableReadyCheck: false\n    });\nwhile (true) {\n    const key = uuid.v4();\n    const value = uuid.v4();\n\n    let result;\n    let retryCount = 0;\n    while (true) {\n        try {\n            result = await redis.set(key, value);\n            break;\n        }\n        catch (ex) {\n            console.log(`retrying... ${retryCount}`);\n            retryCount++;\n            if (retryCount < 5) {\n                await wait(100);\n            }\n            else {\n                throw ex;\n            }\n        }\n    }\n\n    console.log(`SET ${key}: ${value} => ${result}`);\n    await wait(100);\n}\n\n};\nmain().then(() => {\n    console.log(done);\n}).catch(error => {\n    console.error(error);\n});\n```. ",
    "Zetazzz": "Forgot one thing.\n\"redis.once('ready' ...\"  was not executed. \"on error\" was executed and print \"node error Error: Stream isn't writeable and enableOfflineQueue options is false\". ",
    "sushchevskiy": "Same problem. Did you get it work with SSL?. Ok, found solution =) Just set tls = true.\n{\n     port: 6380,\n     host: 'your_host.redis.cache.windows.net',\n     password: 'your_pass',\n     tls: true as any,\n}. ",
    "Bene-Graham": "@JSGuy7 No I have not gotten it to work :(\n. ",
    "tony-gutierrez": "is this a documented option? . Missing from https://github.com/luin/ioredis/blob/master/API.md#new_Redis_new. ",
    "clChenLiang": "sorry for this question. I just put the wrong place . It should be a sequelize  problem.. ",
    "garthk": "Dup of #123, closed as wontfix.. ",
    "dogabudak": "Same here;\nRedis-logs are also printing  - Client closed connection Err\nAny solutions ? \n. ",
    "qoole": "That is a really obnoxious bot. Just because someone hasn't responded doesn't mean an issue should be closed. It just encourages people to 'bump' threads to make sure the overzealous bucket-of-bolts doesn't unduly close their post.\nP.S. - bump. ",
    "stephendeyoung": "Thanks for looking into this @luin . ",
    "pr1ntr": "I made a mistake... windows doesn't give node too much memory it seems.. ",
    "tinder-xhao": "Is there any update about this issue? I'm encountering the same problem here. ",
    "mgetz": "Thanks, Shahar!\nThe monitor command does not take an options parameter. So the only way I saw to set the name (after this submitted code change) is to change the connectionName on this.redis.options and then change it back after the instance has been duplicated. I also change the retryStrategy on the monitoring instances so that it does not come back to life after I kill it. \nIdeally, the monitor command would take an options parameter. \nAnother way to do it would be to use the duplicate command and pass in options or create another instance with the monitoring settings of course.\nIs there a better way to do it?\nThanks!\nMark\n. ",
    "szeist": "I don't think so. Because of the lazy connect we need to do some redis operations to trigger the connect.\nI'll write a test that ensures this.. I created test for the plain and for the TLS connection. Can you check them out?. How about listening to the connect local event. It fired even on secureConnect and I agree with you that it's better to use here the local events instead of the underlying stream events.. Hi,\nThe unit test uses the stubbed socket and it ensures the returned promise resolved on the connect event on the socket. The functional test connects to a redis server and tries to connect an invalid port the ensure the promise resolution and rejection. (For plain connection it worked fine w/o this pull request.\nIt created in pair with the unit test for the  secureConnect socket event. The reason for I created this test to make the promise resolution unit test more complete.\nAll the best,\nIstvan. What do you think, these modification are OK or I need to change something?. Hi,\nCan we merge this in the near future?. ",
    "monkbroc": "Thanks for making the change @szeist \nI hope this gets merged soon :). Thanks @luin! I'm looking forward to the final release of 4.0.0.. This should also be changed back to 'connect' to avoid leaking a listener.. ",
    "philipgloyne": "@JoeNyland \nInterestingly I came across a very similar error today (I am also running an aws lambda, different redis host though). I took a look at the redis.status property between invocations. The first time it runs with a status: 'connecting' however, on subsequent invocations (after I had called .disconnect()) the status was 'end'. I think lambdas are caching some state between runs. Just a thought, best of luck with your issue. . ",
    "JoeNyland": "@philipgloyne The behaviour you describe (I think) is expected. If your Lambda invocation ends before the connection(s) to the Redis node(s) are closed, Lambda will freeze the state at that point in time and thaw it out on the next invocation of the Lambda.\nThis is exactly the reason why we create a new connection at the start of the function, so that every Lambda invocation uses a new (not the cached and possibly closed) connection. This is shown in the example above.. I'd like to point out that we have been able to recreate this issue outside of AWS. This was on a local Redis 3.2.10 cluster running under Docker using the above script.\nThis means that the issue is not isolated to AWS Lambda or Elasticache and is either with our code or ioredis.. @ipengyo Sorry, I don't see a solution here. Have you found a solution to this problem? If so, would you mind sharing it?. I'm commenting here to confirm that this issue is still cropping up for us.\nI'm not really sure why the bot above adds a \"wontfix\" label to an issue that hasn't had any recent activity \ud83e\udd14. @luin I see that this has a milestone set of v4, but this issue is still open. Is this issue meant to be fixed in v4.0.0 and you want us to test it or is it still being worked on? We're hitting this issue quite a lot at the moment and we're desperate to find a fix.. @luin Thanks for your input so far. We've had the following in place today:\n\nWrapped our code in a ready event listener on the Redis.Cluster instance\nEnabled the showFriendlyErrorStack option\n\nWe've got the following output from two errors in the last hour:\n2018-09-11T16:34:02.317Z    7db8e0df-b5e0-11e8-9f45-27657abca6d1    Error: Connection is closed.\nat Socket.g (events.js:292:16)\nat emitNone (events.js:91:20)\nat Socket.emit (events.js:185:7)\nat TCPConnectWrap.afterConnect [as oncomplete] (net.js:1073:10)\nTo confirm, we're also logging out the current node connection when this issue is hit and it looks like so:\n{\n  \"options\": {\n    \"retryStrategy\": null,\n    \"readOnly\": false,\n    \"host\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com\",\n    \"port\": 6379,\n    \"key\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com:6379\",\n    \"showFriendlyErrorStack\": true,\n    \"lazyConnect\": true,\n    \"family\": 4,\n    \"connectTimeout\": 10000,\n    \"keepAlive\": 0,\n    \"noDelay\": true,\n    \"connectionName\": null,\n    \"sentinels\": null,\n    \"name\": null,\n    \"role\": \"master\",\n    \"password\": null,\n    \"db\": 0,\n    \"dropBufferSupport\": false,\n    \"enableOfflineQueue\": true,\n    \"enableReadyCheck\": true,\n    \"autoResubscribe\": true,\n    \"autoResendUnfulfilledCommands\": true,\n    \"keyPrefix\": \"\",\n    \"reconnectOnError\": null,\n    \"stringNumbers\": false\n  },\n  \"domain\": null,\n  \"_events\": {},\n  \"_eventsCount\": 5,\n  \"scriptsSet\": {},\n  \"commandQueue\": {\n    \"_head\": 0,\n    \"_tail\": 0,\n    \"_capacityMask\": 3,\n    \"_list\": [\n      null,\n      null,\n      null,\n      null\n    ]\n  },\n  \"offlineQueue\": {\n    \"_head\": 2,\n    \"_tail\": 2,\n    \"_capacityMask\": 3,\n    \"_list\": [\n      null,\n      null,\n      null,\n      null\n    ]\n  },\n  \"connector\": {\n    \"options\": {\n      \"retryStrategy\": null,\n      \"readOnly\": false,\n      \"host\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com\",\n      \"port\": 6379,\n      \"key\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com:6379\",\n      \"showFriendlyErrorStack\": true,\n      \"lazyConnect\": true,\n      \"family\": 4,\n      \"connectTimeout\": 10000,\n      \"keepAlive\": 0,\n      \"noDelay\": true,\n      \"connectionName\": null,\n      \"sentinels\": null,\n      \"name\": null,\n      \"role\": \"master\",\n      \"password\": null,\n      \"db\": 0,\n      \"dropBufferSupport\": false,\n      \"enableOfflineQueue\": true,\n      \"enableReadyCheck\": true,\n      \"autoResubscribe\": true,\n      \"autoResendUnfulfilledCommands\": true,\n      \"keyPrefix\": \"\",\n      \"reconnectOnError\": null,\n      \"stringNumbers\": false\n    },\n    \"connecting\": false,\n    \"stream\": {\n      \"connecting\": false,\n      \"_hadError\": false,\n      \"_handle\": null,\n      \"_parent\": null,\n      \"_host\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com\",\n      \"_readableState\": {\n        \"objectMode\": false,\n        \"highWaterMark\": 16384,\n        \"buffer\": {\n          \"head\": null,\n          \"tail\": null,\n          \"length\": 0\n        },\n        \"length\": 0,\n        \"pipes\": null,\n        \"pipesCount\": 0,\n        \"flowing\": true,\n        \"ended\": true,\n        \"endEmitted\": true,\n        \"reading\": false,\n        \"sync\": false,\n        \"needReadable\": false,\n        \"emittedReadable\": false,\n        \"readableListening\": false,\n        \"resumeScheduled\": false,\n        \"defaultEncoding\": \"utf8\",\n        \"ranOut\": false,\n        \"awaitDrain\": 0,\n        \"readingMore\": false,\n        \"decoder\": null,\n        \"encoding\": null\n      },\n      \"readable\": false,\n      \"domain\": null,\n      \"_events\": {\n        \"_socketEnd\": [\n          null\n        ]\n      },\n      \"_eventsCount\": 5,\n      \"_writableState\": {\n        \"objectMode\": false,\n        \"highWaterMark\": 16384,\n        \"needDrain\": false,\n        \"ending\": true,\n        \"ended\": true,\n        \"finished\": true,\n        \"decodeStrings\": false,\n        \"defaultEncoding\": \"utf8\",\n        \"length\": 0,\n        \"writing\": false,\n        \"corked\": 0,\n        \"sync\": true,\n        \"bufferProcessing\": false,\n        \"writecb\": null,\n        \"writelen\": 0,\n        \"bufferedRequest\": null,\n        \"lastBufferedRequest\": null,\n        \"pendingcb\": 0,\n        \"prefinished\": true,\n        \"errorEmitted\": false,\n        \"bufferedRequestCount\": 0,\n        \"corkedRequestsFree\": {\n          \"next\": null,\n          \"entry\": null\n        }\n      },\n      \"writable\": false,\n      \"allowHalfOpen\": false,\n      \"destroyed\": true,\n      \"_bytesDispatched\": 0,\n      \"_sockname\": null,\n      \"_pendingData\": null,\n      \"_pendingEncoding\": \"\",\n      \"server\": null,\n      \"_server\": null,\n      \"_idleTimeout\": -1,\n      \"_idleNext\": null,\n      \"_idlePrev\": null,\n      \"_idleStart\": 5699454,\n      \"_consuming\": true,\n      \"_peername\": {\n        \"address\": \"10.1.1.131\",\n        \"family\": \"IPv4\",\n        \"port\": 6379\n      }\n    }\n  },\n  \"retryAttempts\": 0,\n  \"status\": \"end\",\n  \"condition\": {\n    \"select\": 0,\n    \"auth\": null,\n    \"subscriber\": false\n  },\n  \"stream\": {\n    \"connecting\": false,\n    \"_hadError\": false,\n    \"_handle\": null,\n    \"_parent\": null,\n    \"_host\": \"cluster-id-0001-001.sdbngd.0001.euw1.cache.amazonaws.com\",\n    \"_readableState\": {\n      \"objectMode\": false,\n      \"highWaterMark\": 16384,\n      \"buffer\": {\n        \"head\": null,\n        \"tail\": null,\n        \"length\": 0\n      },\n      \"length\": 0,\n      \"pipes\": null,\n      \"pipesCount\": 0,\n      \"flowing\": true,\n      \"ended\": true,\n      \"endEmitted\": true,\n      \"reading\": false,\n      \"sync\": false,\n      \"needReadable\": false,\n      \"emittedReadable\": false,\n      \"readableListening\": false,\n      \"resumeScheduled\": false,\n      \"defaultEncoding\": \"utf8\",\n      \"ranOut\": false,\n      \"awaitDrain\": 0,\n      \"readingMore\": false,\n      \"decoder\": null,\n      \"encoding\": null\n    },\n    \"readable\": false,\n    \"domain\": null,\n    \"_events\": {\n      \"_socketEnd\": [\n        null\n      ]\n    },\n    \"_eventsCount\": 5,\n    \"_writableState\": {\n      \"objectMode\": false,\n      \"highWaterMark\": 16384,\n      \"needDrain\": false,\n      \"ending\": true,\n      \"ended\": true,\n      \"finished\": true,\n      \"decodeStrings\": false,\n      \"defaultEncoding\": \"utf8\",\n      \"length\": 0,\n      \"writing\": false,\n      \"corked\": 0,\n      \"sync\": true,\n      \"bufferProcessing\": false,\n      \"writecb\": null,\n      \"writelen\": 0,\n      \"bufferedRequest\": null,\n      \"lastBufferedRequest\": null,\n      \"pendingcb\": 0,\n      \"prefinished\": true,\n      \"errorEmitted\": false,\n      \"bufferedRequestCount\": 0,\n      \"corkedRequestsFree\": {\n        \"next\": null,\n        \"entry\": null\n      }\n    },\n    \"writable\": false,\n    \"allowHalfOpen\": false,\n    \"destroyed\": true,\n    \"_bytesDispatched\": 0,\n    \"_sockname\": null,\n    \"_pendingData\": null,\n    \"_pendingEncoding\": \"\",\n    \"server\": null,\n    \"_server\": null,\n    \"_idleTimeout\": -1,\n    \"_idleNext\": null,\n    \"_idlePrev\": null,\n    \"_idleStart\": 5699454,\n    \"_consuming\": true,\n    \"_peername\": {\n      \"address\": \"10.1.1.131\",\n      \"family\": \"IPv4\",\n      \"port\": 6379\n    }\n  },\n  \"manuallyClosing\": false,\n  \"replyParser\": {\n    \"optionReturnBuffers\": true,\n    \"optionStringNumbers\": false,\n    \"name\": \"javascript\",\n    \"offset\": 0,\n    \"buffer\": null,\n    \"bigStrSize\": 0,\n    \"bigOffset\": 0,\n    \"totalChunkSize\": 0,\n    \"bufferCache\": [],\n    \"arrayCache\": [],\n    \"arrayPos\": []\n  },\n  \"prevCondition\": {\n    \"select\": 0,\n    \"auth\": null,\n    \"subscriber\": false\n  }\n}. Thanks for that. I've migrated the code to use the scanStream() function on each node instead of keys() and we're still hitting the same intermittent Connection is closed error. Here's an example:\n```javascript\nconst Redis = require('ioredis');\nconst collectNodeKeys = (node) => {\n  const stream = node.scanStream({\n    match: ':',\n    count: process.env.SCAN_PAGE_SIZE || 10,\n  });\nreturn new Promise((resolve) => {\n    let keys = [];\n    stream.on('data', (page) => {\n      keys = keys.concat(page);\n    });\n    stream.on('end', () => {\n      resolve(keys);\n    });\n  });\n};\nconst collectClusterKeys = (cluster) => {\n  return new Promise((resolve, reject) => {\n    cluster.on('ready', () => {\n      const nodes = cluster.nodes('master');\n      Promise\n        .all(nodes.map(collectNodeKeys))\n        .then((responses) => {\n          let r = [];\n          for (let response of responses) {\n            r = r.concat(response);\n          }\n          resolve(r);\n        })\n        .catch(reject);\n    });\n  });\n};\nconst interval = setInterval(() => {\n  const cluster = new Redis.Cluster([process.env.REDIS_HOST], {\n    slotsRefreshTimeout: parseInt(process.env.REDIS_SLOTS_REFRESH_TIMEOUT) || 1000,\n    enableReadyCheck: true,\n    redisOptions: {\n      showFriendlyErrorStack: true,\n    },\n  });\ncollectClusterKeys(cluster).then((keys) => {\n    console.log(Collected ${keys.length} keys);\n    cluster.disconnect();\n  }).catch((e) => {\n    clearInterval(interval);\n    cluster.disconnect();\n    console.log(e);\n  })\n}, 1000);\n```\n```bash\nCollected 1386 keys\nCollected 1387 keys\n... snip ...\nCollected 1423 keys\n/app/node_modules/bluebird/js/release/async.js:61\n        fn = function () { throw arg; };\n                           ^\nError: Connection is closed.\n    at ScanStream.Readable.read (stream_readable.js:348:10)\n    at resume (_stream_readable.js:737:12)\n    at _combinedTickCallback (internal/process/next_tick.js:80:11)\n    at process._tickCallback (internal/process/next_tick.js:104:9)\n``\n. In addition to the above, if I monitorCLUSTER NODESwhilst recreating this issue, I'm not seeing any failover events in the cluster.. @mfulton26 In production, yes we are settingcontext.callbackWaitsForEmptyEventLoop = false. With the default of this being set totrue, we couldn't get the Lambda invocation to stop cleanly without a timeout, even though we were closing the connection after the callbacks for all calls had finished. It was as if there was still stuff in Node's event loop that was keeping the process open, even though ioredis'.quit()` resolves.\nWe are only calling Lambda's callback() (which ends the invocation) once all callbacks for all Redis calls have completed, so I'm not sure I see the issue with what we are doing.\nHowever, as I said above this issue has been recreated with the default Lambda config of context.callbackWaitsForEmptyEventLoop = true so it looks like that's not the issue.. ",
    "elliotttf": "I've also been able to reproduce this problem but only in AWS.\nI believe the problem is related to the offline queue. The error originates when the close() method is called from the event_handler. The error eventually bubbles up in the redis class when flushQueue() is executed with a non-empty offline queue.\nThe commandQueue also occasionally causes this problem but it's much less frequent.. In my case, there were a couple of things that helped to mitigate the error (although it's still not completely gone):\n\nupgrade the runtime to node 8 (which you can now do on lambda)\ndisable the offline queue. Yes I'm sure. I am using an elasticache cluster with two shards and three replicas each. It seems that the three node configuration is not a hard requirement but a very strong recommendation to allow HA during node failures. Regardless, I will update my configuration.\n\nI should have been more specific in my original comment, the selection of where to send writes to isn't what I want to round robin as that is clearly determined by the cluster. What I'd like to change is the selection of which node within a cluster a read command is sent to. Currently, this is selected at random for read-safe commands. I feel allowing this selection to be round robin rather than random would allow for a more even distribution of commands within a cluster.. ",
    "ipengyo": "@elliotttf I also encountered how to solve\n. ",
    "sillykelvin": "I also meet this problem, my original code is almost the same as @JoeNyland \nsetInterval(function() {\n    var cluster = new Redis.Cluster(...);\n    cluster.nodes('all').forEach(function(node) {\n        node.bgsave(...);\n    });\n}, timeInterval);\nHowever, I tried something like this:\n```\nsetInterval(function() {\n    var cluster = new redis.Cluster(...);\n// delay the command for 3 seconds\nsetTimeout(function() {\n    cluster.nodes('all').forEach(function(node) {\n        node.bgsave(...);\n    });\n}, 3 * 1000);\n\n}, timeInterval);\n```\nThen the code never fails, so I guess there might be some initialization jobs inside new Redis.Cluster(...), and it requires some time to complete, so immediate command execution might fail (not quite sure, just my guess).. ",
    "alecbenson": "Having this issue as well. We are running on AWS, but not using Lambda. . ",
    "kierxn": "Hi @luin.\nI work with @JoeNyland so I'll answer while he isn't here.\nWe made the change you just posted a few days ago and while it's reduced the frequency of the Connection is closed. error it's not stopped it from happening.\nOur current method looks like:\n```\nconst nodes = clusterConn.nodes('master');\nPromise.all(nodes.map((\n    return node.keys(':');\n})).then((responses) => {\n  ...\n})\n```\n. ",
    "mfulton26": "@JoeNyland are you setting context.callbackWaitsForEmptyEventLoop = false? (AWS Lambda Context Object in Node.js - AWS Lambda). @heri16, you are correct, it is not a good idea. I wasn't proposing that it was. If @JoeNyland or others who are getting intermittent \"Connection is closed\" errors is setting context.callbackWaitsForEmptyEventLoop = false then such may be the cause of said errors.. ",
    "jayflo": "Not sure if this helps (or is even related to the cause), but I started seeing this error when upgrading from ~3.1.4 to ~4.3.0.  None of our infrastructure is on AWS.. Have you checked this: https://github.com/luin/ioredis/issues/365. Ah, thanks.. ",
    "sarneeh": "My case is using ioredis while doing integration tests. Every test opens and closes a connection in its run. I'm doing .quit() at the end of every test and it successfully resolves, but I still get the error for some reason.\nThe solution for me was what @elliotttf suggested: switching enableOfflineQueue to false.. ",
    "zeodtr": "@luin Thank you for your response.\nI see that's the limitation of Redis.\nI will close this issue. . ",
    "benjamingr": "@luin you can check if Buffer.from is available and if it is use it, otherwise new Buffer and .fill. \n@shaharmor since you're a maintainer here - you want to tackle this? (Feel free to say no and I'll PR). Also, I don't think you should support Node.js 0.10.16 since we (Node) no longer support it.\nIn fact, 0.10.x, 0.12.x, 1.x, 2.x, 3.x, 5.x, 7.x are all EoL, 4.x becomes EoL in ~ a month. \nTo clarify, if you don't change this then every updated node install will emit a process warning every time ioredis is used (but no code will break). @shaharmor ..... ",
    "ibc": "OK, that's not the problem, confirmed that bith PSUBSCRIBE are sent at the same time.. ",
    "ivanvanderbyl": "@divmgl did you resolve this? I'm having the same issue, as soon as the library is required it tries to connect to localhost:0\n```\n  ioredis:redis status[localhost:0]: [empty] -> connecting +0ms\n  ioredis:redis status[localhost:0]: [empty] -> connecting +2ms\nioredis:connection error: Error: connect EADDRNOTAVAIL 127.0.0.1 - Local (0.0.0.0:61722) +437ms\n{ Error: connect EADDRNOTAVAIL 127.0.0.1 - Local (0.0.0.0:61722)\n    at Object._errnoException (util.js:1031:13)\n    at _exceptionWithHostPort (util.js:1052:20)\n    at internalConnect (net.js:977:16)\n    at GetAddrInfoReqWrap.emitLookup [as callback] (net.js:1119:7)\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:100:10)\n  errno: 'EADDRNOTAVAIL',\n  code: 'EADDRNOTAVAIL',\n  syscall: 'connect',\n  address: '127.0.0.1' }\n```\nStrangely this only happens when in test mode and using Mocha/Chai.. ",
    "tomharrison": "You can be affected by this if you pass the port as an environment variable. Redis.prototype.parseOption has different behavior depending on the arity of the constructor:\nhttps://github.com/luin/ioredis/blob/622975d9d454eca6a9c495d35f5638d68e2662f2/lib/redis.js#L207\nIt tests if (typeof port === 'number') however the values in process.env are going to be strings. Try using parseInt on the port before passing it into ioredis.. ",
    "dpz3579": "thanks but i used hscanStream\ncan you look at it and tell me if it's better and optimized or should i use hscan only ?\n```\nvar name = \"MY_HM_EVT\";\nvar obj = {\n  match: '*',\n  cursor: 0,\n  count: 10\n};\nif(pattern)\n  obj.match = pattern\nif(batchSize)\n  obj.count = batchSize\nvar stream = redis.hscanStream(name, obj);\nvar keys = {};\nstream.on('data', function (resultKeys) {\n  // resultKeys is an array of strings representing key names\n  for (var i = 0; i < resultKeys.length; i++) {\n    if(i % 2){\n      // Logic as output is\n      // 1) \"806_i\" => Key (present in 'i-1')\n      // 2) \"1520508780,2273\" => value (present in 'i')\n      keys[resultKeys[i-1]] = resultKeys[i];\n    }\n  }\n});\nstream.on('end', function () {\n  cb(null,keys);\n});\n```\ni was looking for the reposnse as same standard as how the client gives for rpop and other pop function\nredis.rpop(name, function(err, result) {\n  if (err) {\n    lmessages(\"err\", err, \"result\", result);\n  }\n  cb(err, result);\n});. ",
    "ks-s-a": "@luin It's quite strange - I use docker image with redis, istead of the pure local redis server. I tested local redis-server launching and bug doesn't exist in that case (external IP, protected-mode off).\nWe with my colleagues tested reconnection with product-like environment and couldn't repeat. We use kubernetes, may be it somehow affects the bug.\nI'm not sure that I can investigate the issue further, I tried different redis options in the docker and local with the same result. It happens in docker container, but not with local redis server.. ",
    "lavarsicious": "We're definitely seeing this issue occur when using an Azure Redis instance. If I scale the service, Azure will disconnect any clients when it cuts over.. ",
    "carly": "@manast have you been able to come up with a good workaround for this issue? I'm using bull for an internal app I'm building at work. Everything works as expected on my dev box, but I'm running into this issue when I try to configure my app to redis instances on different hosts. . ",
    "stevelacy": "The latest on npm,  3.2.2. ",
    "stevehorn": "Do you need to check if the key exists before calling .del(key)?. ",
    "thetutlage": "I have the same issue, where the end event is never emitted. I tried looking into the source code, However, I cannot find any piece of code, which emits the end event. ",
    "Shahor": "I went that way indeed.\nThanks for your answer :) . ",
    "fgarcia": "the issue still needs an answer. ",
    "silverwind": "The CI failure (something about CERT_UNTRUSTED) looks unrelated.. I have a feeling that this should be a per-command option. I could certainly see using different timeouts for different commands.\nI'm currently using a wrapper function that includes a Promise constructor and a setTimeout, but seeing that ioredis does not expose a way to cancel a pending command, there is a risk that it would accumulate lots of pending commands over time and eventually exhaust the system's  resources.. I guess we could do without this new option if commands were cancellable. A common way to do this on promise interfaces is to expose a .cancel method on the promise. There's also p-cancellable which adds a bit more.. You can use whatwg-url to support older Node versions:\n````js\n\nurl.parseURL(\"redis://user::pass@127.0.0.1:6380/4?key=value\")\n{ scheme: 'redis',\n  username: 'user',\n  password: '%3Apass',\n  host: '127.0.0.1',\n  port: 6380,\n  path: [ '4' ],\n  query: 'key=value',\n  fragment: null,\n  cannotBeABaseURL: false }\n```\nIndividual URL parts should be ran throughdecodeURIComponentwhich in this case gives':pass'` for the password.. My editor automatically trimmed this trailing whitespace. If you like, I can remove this change.. \n",
    "ksmithut": "It appears that this doesn't add it to the multi pipelining. It would be great to have an answer for this, even if the answer is \"we don't support adding custom commands\".. I got it working with pipeline by adding the \"string\" and \"buffer\" from above to the Pipeline prototype by directly requiring ioredis/lib/pipeline. Gross... I'm just going to wait until you guys support the new stream commands. Awesome, thanks :). ",
    "zhoutk": "I got it.\nget a new pipeline, not to new redis connect.\nsure?. ",
    "jflebeau": "Forget this one.. ",
    "WRMSRwasTaken": "This is a bug in my opinion, or it should be clarified in the docs.\nI'd suggest adding a listener to https://github.com/luin/ioredis/blob/master/lib/redis.js#L305 waiting for the ready event before resolving the promise, when those options are set like above.. ",
    "yqsailor": "this.subClient.psubscribe('price_*', (err, count) => {\n      console.warn(`subscribe worker pid: ${process.pid} subscribe: message count\uff1a${count}`);\n    });\nthis is my code,what should i do?. the patern price_* was not work, my redis version is 2.8, but I edit \nthis.subClient.psubscribe('price_*', (err, count) => {\n      console.warn(`subscribe worker pid: ${process.pid} subscribe: message count\uff1a${count}`);\n    });\nto\nthis.subClient.subscribe('price_kline', (err, count) => {\n      console.warn(`subscribe worker pid: ${process.pid} subscribe: message count\uff1a${count}`);\n    });\nIt  can be work! what should i do?. ",
    "roccomuso": "did you solve?\n. ",
    "ManasSaxenaTil": "but even redis cli is giving this error ./redis-cli -h redis-1 -p 26379\nI got following error : Could not connect to Redis at redis-1:26379: nodename nor servname provided, or not known. got the solution - I had to uncomment following line in sentinel.conf\nprotected-mode no. ",
    "brettkiefer": "This is the most minimal repro I can get. Reproduced on Node 10.1.0 and 10.3.0 with Ioredis 3.2.2 with the following script, which establishes 3 cluster connections, burns CPU for a few seconds, then tries keys at intervals. It frequently stalls out under Node 10, but always succeeds under 9.11.1. I can't repro without a cluster.\n```\n'use strict';\nconst Redis = require('ioredis');\n// Dial up the number of Redis clusters (nRedis) to increase the chances that a\n// given run will hit the bug and get a stalled cluster\nconst nRedis = 3;\nconst redises = Array.from(Array(nRedis).keys()).map(i => ({\n  n: i,\n  redis: new Redis.Cluster([\n    { host: 'redis1', port: 6379 },\n    { host: 'redis2', port: 6379 },\n    { host: 'redis3', port: 6379 },\n    { host: 'redis4', port: 6379 },\n    { host: 'redis5', port: 6379 },\n    { host: 'redis6', port: 6379 },\n  ]),\n}));\n// Just tie up the CPU for about 3 seconds to foul up communication with the\n// clusters\nconsole.log('Start burning CPU');\nconst startBurn = new Date();\nconst randoms = [];\nfor (let i = 0; i < 5000000; i++) {\n  randoms[i] = Math.random();\n}\nrandoms.sort((a, b) => a < b);\nconst burned = new Date() - startBurn;\nconsole.log(Stop burning CPU, burned ${burned}ms);\nredises.map(({ n, redis }) => {\n  let active = false;\n  let successes = 0;\n  setInterval(() => {\n    if (active) {\n      console.log(### STUCK: CLUSTER (${n}) STALLED OUT ###);\n      process.exit(1);\n    }\n    active = Date.now();\n    console.log(Set redis key on cluster ${n});\n    redis\n      .set('test.set.key', Date.now())\n      .then(() => {\n        active = false;\n        if (++successes === 3) {\n          console.log(<<< SUCCESS, WE DID NOT STALL OUT >>>);\n          process.exit(0);\n        }\n      })\n      .done();\n  }, 1000);\n});\n```\n. It's slightly harder to get it to stall out with a single connection (dialing down nRedis to 1), but after a few runs of that on Node 10.3.0 and IORedis debug enabled, I get:\n```\nThu, 31 May 2018 20:46:52 GMT ioredis:cluster status: [empty] -> connecting\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis1:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis2:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis3:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis4:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis5:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis6:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:52 GMT ioredis:cluster getting slot cache from redis5:6379\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis5:6379]: wait -> connecting\nThu, 31 May 2018 20:46:52 GMT ioredis:redis queue command[0] -> cluster(slots)\nThu, 31 May 2018 20:46:52 GMT ioredis:redis status[redis2:6379]: wait -> connecting\nStart burning CPU\nStop burning CPU, burned 3693ms\nThu, 31 May 2018 20:46:56 GMT ioredis:cluster getting slot cache from redis2:6379\nThu, 31 May 2018 20:46:56 GMT ioredis:redis queue command[0] -> cluster(slots)\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:46:56 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:46:56 GMT ioredis:redis queue command[0] -> info()\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nThu, 31 May 2018 20:46:56 GMT ioredis:connection send 1 commands in offline queue\nThu, 31 May 2018 20:46:56 GMT ioredis:redis write command[0] -> cluster(slots)\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis1:6379]: wait -> close\nThu, 31 May 2018 20:46:56 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis1:6379]: close -> end\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis3:6379]: wait -> close\nThu, 31 May 2018 20:46:56 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis3:6379]: close -> end\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis4:6379]: wait -> close\nThu, 31 May 2018 20:46:56 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis4:6379]: close -> end\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis6:6379]: wait -> close\nThu, 31 May 2018 20:46:56 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[redis6:6379]: close -> end\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:46:56 GMT ioredis:cluster status: connecting -> connect\nThu, 31 May 2018 20:46:56 GMT ioredis:redis queue command[0] -> cluster(info)\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: ready -> close\nThu, 31 May 2018 20:46:56 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: close -> end\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: wait -> connecting\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:46:56 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:46:56 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nSet redis key on cluster 0\nSTUCK: CLUSTER (0) STALLED OUT\n```\n. On a healthy run (this is with Node 10.3.0 as well, but just on a run where we don't see the intermittent failure), we see the cluster get into the ready state instead of stalling out in 'connect':\nThu, 31 May 2018 20:56:09 GMT ioredis:cluster status: [empty] -> connecting\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis1:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis2:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis3:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis4:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis5:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis6:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:09 GMT ioredis:cluster getting slot cache from redis6:6379\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis6:6379]: wait -> connecting\nThu, 31 May 2018 20:56:09 GMT ioredis:redis queue command[0] -> cluster(slots)\nThu, 31 May 2018 20:56:09 GMT ioredis:redis status[redis2:6379]: wait -> connecting\nStart burning CPU\nStop burning CPU, burned 3359ms\nThu, 31 May 2018 20:56:12 GMT ioredis:cluster getting slot cache from redis1:6379\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis1:6379]: wait -> connecting\nThu, 31 May 2018 20:56:12 GMT ioredis:redis queue command[0] -> cluster(slots)\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis queue command[0] -> info()\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nThu, 31 May 2018 20:56:12 GMT ioredis:connection send 1 commands in offline queue\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> cluster(slots)\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis3:6379]: wait -> close\nThu, 31 May 2018 20:56:12 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis3:6379]: close -> end\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis4:6379]: wait -> close\nThu, 31 May 2018 20:56:12 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis4:6379]: close -> end\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis5:6379]: wait -> close\nThu, 31 May 2018 20:56:12 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[redis5:6379]: close -> end\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: [empty] -> wait\nThu, 31 May 2018 20:56:12 GMT ioredis:cluster status: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: ready -> close\nThu, 31 May 2018 20:56:12 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: close -> end\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: ready -> close\nThu, 31 May 2018 20:56:12 GMT ioredis:connection skip reconnecting since the connection is manually closed.\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: close -> end\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: wait -> connecting\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nThu, 31 May 2018 20:56:12 GMT ioredis:cluster getting slot cache from xx.xxx.x.xx:6379\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> cluster(slots)\nThu, 31 May 2018 20:56:12 GMT ioredis:delayqueue send 1 commands in failover queue\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: wait -> connecting\nThu, 31 May 2018 20:56:12 GMT ioredis:redis queue command[0] -> cluster(info)\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connecting -> connect\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> info()\nThu, 31 May 2018 20:56:12 GMT ioredis:redis status[xx.xxx.x.xx:6379]: connect -> ready\nThu, 31 May 2018 20:56:12 GMT ioredis:connection send 1 commands in offline queue\nThu, 31 May 2018 20:56:12 GMT ioredis:redis write command[0] -> cluster(info)\nThu, 31 May 2018 20:56:12 GMT ioredis:cluster status: connect -> ready\nSet redis key on cluster 0\nThu, 31 May 2018 20:56:13 GMT ioredis:redis write command[0] -> set(test.set.key,1527800173790)\nSet redis key on cluster 0\nThu, 31 May 2018 20:56:14 GMT ioredis:redis write command[0] -> set(test.set.key,1527800174791)\nSet redis key on cluster 0\nThu, 31 May 2018 20:56:15 GMT ioredis:redis write command[0] -> set(test.set.key,1527800175792)\n<<< SUCCESS, WE DID NOT STALL OUT >>>. This is the most minimal simulation I have been able to generate for what we're seeing for the good and bad cases in our actual application in staging on Node 10 and Cluster 3.2.2. This may boil down to a Node bug or some kind of improper usage on our end, but it seems like understanding what's causing the cluster to get stuck in 'connect' as the cluster(info) command goes through and the redis goes to 'ready' is the beginning, and that's where I'm having some trouble.. Just an update, I reproduced the issue with https://hub.docker.com/r/grokzen/redis-cluster/ and node 10.4.0 today, just copied over the script with\ndocker cp ./redisTest.js redis-cluster_1:/data/\nthen from within the container did\nwget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.11/install.sh | bash\nexport NVM_DIR=\"$HOME/.nvm\"\n[ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n[ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\"\nnvm install 9.11.1\nnvm use 9.11.1\nnpm install ioredis\nfor i in {1..10}; do DEBUG=ioredis.* node ./redisTest.js; if [ $? -ne 0 ]; then break; fi; do\nwhich all succeeded as expected, then did\nnvm install 10.4.0\nfor i in {1..10}; do DEBUG=ioredis.* node ./redisTest.js; if [ $? -ne 0 ]; then break; fi; done\nwhich broke on the first run with the expected pattern.\nThat's with config changes for the Docker cluster, so redisTest.js is \n```\n'use strict';\nconst Redis = require('ioredis');\n// Dial up the number of Redis clusters (nRedis) to increase the chances that a\n// given run will hit the bug and get a stalled cluster\nconst nRedis = 3;\nconst redises = Array.from(Array(nRedis).keys()).map(i => ({\n  n: i,\n  redis: new Redis.Cluster([\n    { host: 'localhost', port: 7000 },\n    { host: 'localhost', port: 7001 },\n    { host: 'localhost', port: 7002 },\n    { host: 'localhost', port: 7003 },\n    { host: 'localhost', port: 7004 },\n    { host: 'localhost', port: 7005 },\n  ]),\n}));\n// Just tie up the CPU for about 3 seconds to foul up communication with the\n// clusters\nconsole.log('Start burning CPU');\nconst startBurn = new Date();\nconst randoms = [];\nfor (let i = 0; i < 5000000; i++) {\n  randoms[i] = Math.random();\n}\nrandoms.sort((a, b) => a < b);\nconst burned = new Date() - startBurn;\nconsole.log(Stop burning CPU, burned ${burned}ms);\nredises.map(({ n, redis }) => {\n  let active = false;\n  let successes = 0;\n  setInterval(() => {\n    if (active) {\n      console.log(### STUCK: CLUSTER (${n}) STALLED OUT ###);\n      process.exit(1);\n    }\n    active = Date.now();\n    console.log(Set redis key on cluster ${n});\n    redis\n      .set('test.set.key', Date.now())\n      .then(() => {\n        active = false;\n        if (++successes === 3) {\n          console.log(<<< SUCCESS, WE DID NOT STALL OUT >>>);\n          process.exit(0);\n        }\n      })\n      .done();\n  }, 1000);\n});\n```\n. Unsurprisingly (this was the most suspicious-looking change in the changelog for this), this traces back to https://github.com/nodejs/node/commit/9b7a6914a7. If you build Node at the previous revision (https://github.com/nodejs/node/commit/74553465e6), this test does not repro, at 9b7a6914a7f0bd754e78b42b48c75851cfd6b3c4 it does.\nIt looks like there is some hubbub around 'close' events in the subsequent Node 10 changelogs, but concerning http, e.g. https://github.com/nodejs/node/pull/20941.\n. Yes, I'm suspecting that Node isn't emitting 'close' under certain circumstances, and that that is a bug with that commit, but it'll probably take me a little while to boil it down to something that takes ioredis out of it.\nCurrently we can work around this for the purposes of this test by handling 'end' rather than close\n```\ndiff --git a/lib/redis.js b/lib/redis.js\nindex 8870bec..39b9b53 100644\n--- a/lib/redis.js\n+++ b/lib/redis.js\n@@ -279,7 +279,7 @@ Redis.prototype.connect = function (callback) {\n   stream.once(CONNECT_EVENT, eventHandler.connectHandler(_this));\n   stream.once('error', eventHandler.errorHandler(_this));\n\n\nstream.once('close', eventHandler.closeHandler(_this));\nstream.once('end', eventHandler.closeHandler(_this));\n       stream.on('data', eventHandler.dataHandler(_this));\n```\n\n... but that seems very wrong, the 'close' handling seems more correct to me, and it looks as though that event should always be emitted.. Ah, ok, if the socket is paused there may not be a 'close', that's why it's semver-major:\nhttps://github.com/nodejs/node/pull/19241#issuecomment-371639579. ... but at the point when we get the EOF (https://github.com/nodejs/node/commit/9b7a6914a7#diff-e6ef024c3775d787c38487a6309e491dL645), the connection isPaused() returns false, so it's not that.\nThe issue is that we get into onReadableStreamEnd with the socket's this.writable set to true, so the socket's maybeDestroy doesn't call socket.destroy and the socket never emits closed.\nIt's a little hard to tell if that's intentional or not, from the PR and checkin, but it's looking more like 'not'. . There is more on what exactly it looks like is happening with the socket at https://github.com/nodejs/node/pull/19241#issuecomment-396133420 -- the long and short of it appears to be that if we hit the connect timeout and call .end before the socket is connected, the socket gets in a state that causes it to never call .destroy and thus never emit close. It might be reasonable to work around this issue in the connection timeout logic or by handling end events for the time being, as we work this out with an issue against Node.js (if they agree that the behavior is incorrect). . This will be fixed when this ships on Node 10: https://github.com/nodejs/node/pull/21290. I'll leave this open until then in case anyone else sees it.\n. @ccs018 Thanks! The fix is out with Node 10.5.0, so I'll close this.. @soulrebel What node.js and ioredis versions?. @soulrebel Is it possible you're seeing https://github.com/luin/ioredis/issues/633 ?. ",
    "neg3ntropy": "@brettkiefer Node v 6.0.11 and ioredis 3.2.2. Not the same bug.\n@luin Retrying forever is ok, but I have a log on the error event and it never fires in this case. It simply waits for the server to respond, but that never happens and there are no retries.\nI think something like this could happen anytime a server goes offline while processing a command. The reply will never come, the socket will not timeout since the outgoing data is sent and ioredis will wait indefinitely.. @livoras I don't believe that for 99.999% of the cases you would need different timeouts for different commands.\nI think that a possible solution is to use two different timeouts for connections:\n a long timeout (or no timeout) when the connection is not being used.\n a short timeout after a command is sent.\nBy timing out on data traffic at the connection level, you don't require the entire command to finish before the timeout, but just the server to start sending data, so it should not affect big resultsets.\n. @luin I could have the time to work on this. I just would like to know what would be an acceptable design so that it can be merged.. @elyobo Not really, this scenario does not trigger retries. maxRetriesPerRequest does not help with this.  @luin This should not be closed yet.\nThe failure happens when the socket is connected but the server is not responding. Probably the OS will timeout at some point, but it appears that it will take 15 minutes by default on Linux to give up on that (https://pracucci.com/linux-tcp-rto-min-max-and-tcp-retries2.html). @luin I am not sure that would be enough. You can't send pings while waiting for responses to other commands (AFAIK) and you keep the connection busy and damage performance and you don't fix the issue if it happens during a non-ping command.\n@livoras That could work, although it is not that easy to clean up afterwards.\nI am intrigued by the possibility to setup a socket timeout during commands: in this case you should get a network error and the socket should be cleaned up automatically. In case of success, you just set a new timeout to 0 to disable. Since it's all handled by Node or the OS, based on TCP progress, this would also work well for cases where the response is coming, but slowly. I just do not know if this is a legit way to use sockets.\n. how about using this: https://nodejs.org/api/net.html#net_socket_settimeout_timeout_callback to generate a timeout event from the socket if it becomes inactive while the queue is being processed?\n. I am suggesting to set this to something reasonable while we are processing commands and to disable it while the socket is not used.\nIt just seems that it can be easier to implement than setting and cancelling timeouts in the lib. And more powerful as well.. ",
    "livoras": "I came across the same problem, and I agree that the client can retry to connect the server on its own. But should this kind of retrying be separated from every individual client's request? Every request can has its own timeout and throw a timeout error:\njavascript\ntry {\n   await redis.keys('user*')\n} catch (err) {\n   // err: redis request timeout for 3000ms\n}\nOtherwise the program will be forever pending, and we have no way to kill this kind of pending.. I have a naive idea, what about simply using setTimeout to timeout a pending request?. ",
    "tomwhale": "Would be great to get this in. As it stands, when we terminated an instance running redis, ECS still tries to connect to it. We are using Redis Sentinel though.. ",
    "zluo001": "I\u2018m get the same problem, but the detail error message is\n Unhandled error event: Error: connect ETIMEDOUT\n    at Socket. (/node_modules/ioredis/lib/redis.js:289:21) \nAlso, I can access all sentinel servers by redis-cli.. ",
    "luckyscript": "sh\nioredis:redis status[10.27.18.227:26379]: ready -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.27.18.227:26379]: close -> end +0ms\n  ioredis:redis status[10.37.65.92:6379]: connect -> ready +0ms\n  ioredis:redis status[10.37.65.92:6379]: ready -> close +4s\n  ioredis:connection reconnect in 50ms +6ms\n  ioredis:redis status[10.37.65.92:6379]: close -> reconnecting +1ms\n  ioredis:redis status[10.37.65.92:6379]: reconnecting -> connecting +52ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 10ms +0ms\n[2018-07-02 09:02:14.104] [ERROR] console - [ioredis] Unhandled error event: Error: All sentinels are unreachable. Retrying from scratch after 10ms\n    at connectToNext (E:\\*\\node_modules\\ioredis\\lib\\connectors\\sentinel_connector.js:64:19)\n    at SentinelConnector.connect (E:\\*\\node_modules\\ioredis\\lib\\connectors\\sentinel_connector.js:44:3)\n    at Redis.<anonymous> (E:\\*\\node_modules\\ioredis\\lib\\redis.js:264:20)\n    at Promise._execute (E:\\*\\node_modules\\bluebird\\js\\release\\debuggability.js:300:9)\n    at Promise._resolveFromExecutor (E:\\*\\node_modules\\bluebird\\js\\release\\promise.js:483:18)\n    at new Promise (E:\\*\\node_modules\\bluebird\\js\\release\\promise.js:79:10)\n    at Redis.connect (E:*\\node_modules\\ioredis\\lib\\redis.js:250:10)\n    at Timeout._onTimeout (E:\\*\\node_modules\\ioredis\\lib\\redis\\event_handler.js:95:12)\n    at ontimeout (timers.js:475:11)\n    at tryOnTimeout (timers.js:310:5)\n    at Timer.listOnTimeout (timers.js:270:5)\n  ioredis:redis status[10.27.18.228:26379]: [empty] -> connecting +12ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,d_c10943_1) +3ms\n  ioredis:redis status[10.27.18.228:26379]: connecting -> connect +1ms\n  ioredis:redis status[10.27.18.228:26379]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,d_c10943_1) +1ms\n  ioredis:redis write command[0] -> sentinel(sentinels,d_c10943_1) +1ms\n  ioredis:SentinelConnector sentinels [ { host: '10.27.18.228', port: 26379 },\n  { host: '10.27.18.227', port: 26379 },\n  { host: '10.27.18.225', port: 26379 } ] +1ms\n  ioredis:SentinelConnector resolved: 10.37.65.92:6379 +0ms\n  ioredis:redis status[10.37.65.92:6379]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[10.27.18.228:26379]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[10.27.18.228:26379]: close -> end +1ms\n  ioredis:redis status[10.37.65.92:6379]: connect -> ready +0ms\nthanks for the response.Above is the debug log, it seems like network error, but I can reach all sentinel ip\n in fact.. Thanks for the check and your nice help.. ",
    "finalclass": "Thank you for your tips. I've followed your advice with some more extensive error handling and figured out that my app is making some additional connections during page refreshes. I have to sort that out first to see if this will solve my issues. Will let you know about my progress.. Limiting the number of connections has helped me solving these issues. In our app we were making new connections for each http request and that's why we saw a lot of disconnects when one of nodes was down.\nThe issue can be closed now. Thank you for your help.. ",
    "MinStenve": "@GoBrianGo \u77e5\u9053\u539f\u56e0\u4e86\uff0c\u56e0\u4e3aredis-server\u90a3\u8fb9\u8bbe\u7f6e\u4e8630s\u95f2\u7f6e\u8d85\u65f6\uff0c\u6240\u4ee5\u5982\u679c\u621130s\u6ca1\u64cd\u4f5c\u5c31\u4f1a\u65ad\u5f00\uff0c\u7136\u540enode redis\u8fd9\u8fb9\u56e0\u4e3a\u6709\u81ea\u52a8\u91cd\u8fde\u673a\u5236\uff0c\u6240\u4ee5\u7a7a\u95f2\u7684\u65f6\u5019\u5c31\u8fd9\u6837\u4e0d\u65ad\u91cd\u8fde\u3002\u540e\u9762\u5982\u4f55\u89e3\u51b3\u7684\u5462, \u4fee\u6539\u95f2\u7f6e\u8d85\u65f6\u65f6\u95f4?. ",
    "eladror": "No, in redis-cli it worked. \nThe error didn't accured when i was Adding .then statement to each string, or debugging it. So i assume this has something to do with the strings set at the same time. . I finally succeeded to create thie error in redis-cli, so i will ask in the redis repo. \nThank you!. ",
    "james-rabbit": "Thank you, this works in 4.0.0-1 prerelease.. ",
    "jfjm": "Redis instances can be, but Sentinels can not. The flood we experienced was with Sentinels.. Hi, although it's not in the document, but after reading your post, we decided to to create the setting for Sentinel, and it worked. Thanks! We will monitor and see if this address the connection issue we had.\nIf long live connection to sentinels is not trivial to implement, how about subscribe to Sentinel to receive any changes.. Add the following to your sentinel.conf:\ntcp-backlog num\n. ",
    "vladlavrik": "@luin Thanks for the quick response. I created a pull request on DefinitelyTyped project.. ",
    "vcfvct": "Looks like the geo related commands are not in the @types/ioredis. . ",
    "hezjing": "Good to know this, thank you!\nI think this is helpful to describe in the API too?. ",
    "ammohand": "Hi,\nThe below node code worked for me in accessing In-transit encryption and AUTH feature enabled ElastiCache Redis cluster mode enabled cluster.\nvar Redis = require('ioredis');\nvar nodes = [{\n    host: 'clustercfg.xxxxxencr.rqeqka.euw1.cache.amazonaws.com',\n    port: '6379',\n    }];\nvar cluster = new Redis.Cluster(nodes,{\n  redisOptions: {\n    password: '1234123412341234',\n    tls: {}\n}});\ncluster.set('aws', 'aneesh');\ncluster.get('aws', function (err, res) {\ncluster.disconnect()\n});\nThanks,\nAneesh Mohan. ",
    "jonmelia": "Thanks, the above config worked for us.  We edited to suit our app and this is how we now do it...  \nconst Redis = require('ioredis');\nconst config = require('../../config/environment');\nconst logger = require('../../helpers/logger');\nlet client = null;\n/\n Connection to redis and return client object\n @function connect\n @return {client} returns the redis client\n/\n/ istanbul ignore next /\nexports.connect = () => {\n  if (!client) {\n    const connection = {\n      port: config.redis.port,\n      host: config.redis.hostname,\n    };\nconnection.retryStrategy = times => Math.min(times * 50, 2000);\n\nif (config.redis.useRedisCluster) {\n  if (config.redis.tls) {\n    client = new Redis.Cluster([connection], {\n      redisOptions: {\n        tls: {},\n      },\n    });\n  } else {\n    client = new Redis.Cluster([connection]);\n  }\n} else {\n  client = new Redis(connection);\n}\n\n}\n// Set error and connection event handlers\n  client.on('error', (err) => {\n    logger.error(Redis encountered an error: ${err.message}, { stack: err.stack });\n  });\nclient.on('connect', () => {\n    const hostname = config.redis.hostname;\n    const port = config.redis.port;\n    logger.info(Connected to Redis on ${hostname} : ${port});\n  });\nclient.on('end', () => {\n    logger.info('Disconnected from Redis');\n  });\nreturn client;\n};\n/\n Closes the connection\n @function closeConnection\n/\n/ istanbul ignore next */\nexports.closeConnection = () => {\n  client.quit();\n  logger.info('Disconnected from Redis');\n};\n. ",
    "zengming00": "\u73b0\u5728\u6700\u65b0\u7684\u5c31\u662f3.2.2\u5440\uff0c\u54ea\u6765\u7684v4?. ",
    "egoarka": "@luin when are you planning to migrate code to TypeScript? I want to participate in the development. ",
    "Alphapage": "I was using a bad config. Sorry.. Finally, I found that those events aren't propagated related to https://github.com/antirez/redis/issues/2541. ",
    "khflx": "here's resolved this issue. #365. ",
    "depene": "Of course. Thanks. ",
    "scf4": "Hey guys, any update on this?. ",
    "Stono": "Piggybacking off ioredis to dynamically configure another service installed on the host which needs the master IP in a config file, but has no concept of redis sentinel. redis.on('ready', (err, data) => {\n      const master = redis.stream._peername.address;\n      console.log('ready', master);\n    });\nThis works albeit i'm depending on a side effect. ",
    "kamronbatman": "@luin, updated and ready for rereview.. ",
    "Runrioter": "Both. \n\nIf I use client-side sharding across my nodes, how to configure Redis itself?\nIf I use a node with native Redis master/slave clustering, how to configure Redis itself?\n\nWhat is the difference?\nThanks @ccs018. ",
    "renarsvilnis": "Had the same \"problem\" yesterday, solved it by a coworker who had gone through the same problem some time ago. Posting my solution, maybe it helps out someone else.\nYou need to add tls: {} to the options, which is a bit counter-intuitive given the API docs and examples.\njavascript\nconst Redis = require(\"ioredis\");\nconst redis = new Redis({\n  host: \"master.prodGETFROMENVSECRETS.amazonaws.com\",\n  port: 6379,\n  password: \"myl33tpa5w00rd\",\n  tls: {}\n});. ",
    "zsea": "javascript\nvar url = require(\"url\").URL;\nvar options= new url(\"redis://:%3a123456789127.0.0.1/1\");\nit can read password and username. ",
    "lakano": "Hello !\nWe need to use the Redis 5.0 streams commands. Currently on stable ioredis version 4.0.0, I can't uses the Redis 5.0 commands ( tested XADD in a redis.multi([ XADD ... ]).exec() )\nYour commit sent one month ago seems to fix the problem, so I would like to known when a new version of ioredis will be submitted to permit us to uses it please?. Hello!\nThanks for the version 4.0.1 published 3 hours ago.\nBut, I don't understood, I can't uses \u00ab bzpopmin \u00bb command ( added in redis 5.0 ), and it's inside the package redis-commands 1.3.5 referenced in your package.json ( https://github.com/NodeRedis/redis-commands/blob/651d1592b7103cf045e0b0be0ca0532a19e6bfe9/commands.json ) \nIs it normal or do I need to report a bug on ioredis ( or may be in redis-commands ) ?\nThanks for your help!. The last one, 1.3.5, it's why it's really strange.. :-/\nEDIT: Nervermind, it's my Docker cache, sorry for the waste of time and thanks you for the upgrade! :). Finally, I think there is a bug, I've recreated a simple example without Docker and from scratch, the bug is still present. Can you take a look on #716 please? . In my node_modules/redis-commands/package.json I have the last version, 1.3.5, but the node_modules/redis-commands/commands.json doesn't include Redis 5.0 commands.... It's not the master branch where the Redis 5.0 commands have been added in July.\nBTW, there is an error on Circle CI caused by the devDependency of ioredis: https://github.com/NodeRedis/redis-commands/compare/greenkeeper/ioredis-4.0.1\n. Same here, Redis 5.0.0 / ioredis 4.2.3 \nThe connect and ready callback events are called, but if I try to execute a command, nothing happen.\nAs @leapit , if I replace Redis.Cluster with Redis, this works.. @luin Thanks for the tip, for me this was a DNS problem in k8s! (works in single mode because only the first node was correctly resolved... ). This is an extract, until the Error: Too many Cluster redirections ( sorry for the log format )\nhttps://pastebin.com/qBWw2eux . Humm just revert back to 4.2.3, and I still have the same issue, so it's not caused by the version of ioredis (sorry for my previous wrong test !)\nDo you have any idea that could help us to solve this cluster connection problem please?. You right, this was a network DNS resolution problem ( k8s configuration related ). I close the issue, I suppose it's a redis configuration issue.. ",
    "jeremytm": "Interesting, however there are a couple of problems:\n\nThe cluster-announce-ip and cluster-announce-port options need to be specified in redis.conf, which we don't have access to per-node with elasticache.\nWe only need to remap the announced ip and port when connecting remotely via NAT. General connectivity within VPC should run as normal, so it makes more sense for implementation to be contextual, in the client.. For now we've made a fork of ioredis in our private npm, and have modified code to support what we need.. It's pretty simple, and potentially not the safest option, but it works for us. It just passes the nodes in a callback provided via connection options. This callback is free to modify the nodes used by ioredis however it pleases.\n\nhttps://github.com/luin/ioredis/pull/738\nThe callback we use is provided here:\n```\n(nodes)=>{\nfor( let node of nodes ){\n    let mapInfo = natMappings[node.host];\n    if(mapInfo){\n  let toInfo = mapInfo[node.port.toString()];\n  if(toInfo){\n\n    let [toHost,toPort] = toInfo.split(':');\n    node.host = toHost;\n    node.port = toPort;\n\n  } else {\n    throw new Error(`Port ${node.port} not found in map for ${node.host}`);\n  }\n\n} else {\n  throw new Error(`Host ${node.host} not found in map`);\n}\n\n}\n};\n```\nNat mappings would be something like:\n{\n  'rugby-cache-0001-001.xxxxxx.0001.use1.cache.amazonaws.com': {\n    '6379': 'ec2-xx-x-xx-xx.compute-1.amazonaws.com:6379'\n  },\n  'rugby-cache-0001-002.xxxxxx.0001.use1.cache.amazonaws.com': {\n    '6380': 'ec2-xx-x-xx-xx.compute-1.amazonaws.com:6380'\n  }\n}. Yeah, my bad, That's actually how ours looks. I just messed something up during copying and pasting while obfuscating the addresses.. @luin I might be misunderstanding the code, but I'm pretty sure this doesn't skip the subscriber node, and only creates a debug message.. ",
    "ioredis-robot": ":tada: This issue has been resolved in version 4.3.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.0 :tada:\nThe release is available on:\n- npm package (@latest dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.0 :tada:\nThe release is available on:\n- npm package (@latest dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.0 :tada:\nThe release is available on:\n- npm package (@latest dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.2 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.2.3 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.3.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.3.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.3.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.5.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.6.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.4.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.5.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.5.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.6.3 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.5.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.6.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.6.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.6.1 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.6.2 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.6.2 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.6.3 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.7.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This issue has been resolved in version 4.7.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.7.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. :tada: This PR is included in version 4.9.0 :tada:\nThe release is available on:\n- npm package (@next dist-tag)\n- GitHub release\nYour semantic-release bot :package::rocket:. ",
    "c1aphas": "Thank you for a quick response.\nHere is the log:\n```\n  ioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[redis01.head.rambler.tech:6380]: [empty] -> connecting +4ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,redis) +2ms\n  ioredis:redis status[10.16.36.27:6380]: connecting -> connect +7ms\n  ioredis:redis status[10.16.36.27:6380]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,redis) +1ms\n  ioredis:redis write command[0] -> sentinel(sentinels,redis) +4ms\n  ioredis:SentinelConnector adding sentinel 10.16.36.28:6380 +0ms\n  ioredis:SentinelConnector adding sentinel 10.41.36.27:6380 +0ms\n  ioredis:SentinelConnector adding sentinel 10.41.36.28:6380 +0ms\n  ioredis:SentinelConnector Updated internal sentinels: {\"sentinels\":[{\"host\":\"redis01.head.rambler.tech\",\"port\":6380},{\"host\":\"redis02.head.rambler.tech\",\"port\":6380},{\"host\":\"redis03.head.rambler.tech\",\"port\":6380},{\"host\":\"redis04.head.rambler.tech\",\"po ...  +2ms\n  ioredis:SentinelConnector resolved: 10.16.36.27:6379 +1ms\n  ioredis:redis status[10.16.36.27:6379]: connecting -> connect +5ms\n  ioredis:redis write command[0] -> info() +0ms\nRedis connected\n  ioredis:redis status[10.16.36.27:6379]: connect -> ready +2ms\nRedis ready\n  ioredis:redis status[10.16.36.27:6380]: ready -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +13ms\n  ioredis:redis status[10.16.36.27:6380]: close -> end +1ms\n\n\n\nHERE IS 60 SECS IDLE<<<\n\n\n\nioredis:redis status[10.16.36.27:6379]: ready -> close +1m\n  ioredis:connection reconnect in 50ms +1m\n  ioredis:redis status[10.16.36.27:6379]: close -> reconnecting +1ms\nRedis connection closed\nRedis reconnecting\n  ioredis:redis status[10.16.36.27:6379]: reconnecting -> connecting +51ms\n  ioredis:redis status[redis01.head.rambler.tech:6380]: [empty] -> connecting +0ms\n  ioredis:redis queue command[0] -> sentinel(get-master-addr-by-name,redis) +1ms\n  ioredis:redis status[10.16.36.27:6380]: connecting -> connect +1ms\n  ioredis:redis status[10.16.36.27:6380]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +55ms\n  ioredis:redis write command[0] -> sentinel(get-master-addr-by-name,redis) +0ms\n  ioredis:redis write command[0] -> sentinel(sentinels,redis) +0ms\n  ioredis:SentinelConnector Updated internal sentinels: {\"sentinels\":[{\"host\":\"redis01.head.rambler.tech\",\"port\":6380},{\"host\":\"redis02.head.rambler.tech\",\"port\":6380},{\"host\":\"redis03.head.rambler.tech\",\"port\":6380},{\"host\":\"redis04.head.rambler.tech\",\"po ...  +1m\n  ioredis:SentinelConnector resolved: 10.16.36.27:6379 +1ms\n  ioredis:redis status[10.16.36.27:6379]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +1ms\nRedis connected\n  ioredis:redis status[10.16.36.27:6380]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[10.16.36.27:6380]: close -> end +0ms\n  ioredis:redis status[10.16.36.27:6379]: connect -> ready +0ms\nRedis ready\n. Hm, yes\n127.0.0.1:6379> config get timeout\n1) \"timeout\"\n2) \"60\"\n```. Thank you! I'm closing this.. ",
    "simondutertre": "Sure, this code produces several connect events maybe 10% or 20% of the time on 4.0.0 ( tested a few dozen times ) and only one event on 3.2.0 ( same amount of tries ) : \n```\n\"use strict\"\nconst redis = require('ioredis')\nconst client = new redis.Cluster([{\n        \"port\": 6379,\n        \"host\": \"xxxxxxxxxxx.cache.amazonaws.com\"\n    },\n   {\n        \"port\": 6379,\n        \"host\": \"xxxxxxxxxxx.cache.amazonaws.com\"\n    }])\nclient.on('connect', () => {\n  console.log('Connect event')\n})\n```\n. @callsea1 Can you test if you get the \"Connected\", with lazyConnect true in the configuration and use the connect method manually like in the following. ( i had issue with this code sometime not triggering connection, and this is why i started using the connect event )\n```\n\"use strict\"\nconst redis = require('ioredis')\nconst client = new redis.Cluster([{\n  \"port\": 6379,\n  \"host\": \"xxxxxxxxxxxxxxxxx.cache.amazonaws.com\"\n}], { lazyConnect: true })\nclient.connect()\n  .then(() => console.log(\"Connected\"))\n```\n. here are the logs ( same code as above ), it appears that the multiple connections are caused by the node being empty, although the number of connection seem to be random ( 3 on this try, 1 on second ) :\n\nioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +0ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +4ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +2ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +2ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +1ms\n  ioredis:cluster getting slot cache from clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 +12ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> connecting +4ms\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[10.0.16.159:6379]: connecting -> connect +24ms\n  ioredis:redis write command[0] -> info() +2ms\n  ioredis:redis status[10.0.16.159:6379]: connect -> ready +5ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +41ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> close +4ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +5ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: close -> end +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +2ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +2ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.41.235:6379 as master +4ms\n  ioredis:redis status[10.0.41.235:6379]: [empty] -> wait +3ms\n  ioredis:cluster:connectionPool Connecting to 10.0.9.160:6379 as slave +1ms\n  ioredis:redis status[10.0.9.160:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.16.159:6379 as master +0ms\n  ioredis:redis status[10.0.16.159:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.48.26:6379 as slave +1ms\n  ioredis:redis status[10.0.48.26:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.86.18:6379 as master +1ms\n  ioredis:redis status[10.0.86.18:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.37.140:6379 as slave +1ms\n  ioredis:redis status[10.0.37.140:6379]: [empty] -> wait +1ms\n  ioredis:cluster status: connecting -> connect +49ms\n  ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting... +6ms\n  ioredis:cluster status: connect -> disconnecting +0ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +6ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.41.235:6379 because the node does not hold any slot +0ms\n  ioredis:redis status[10.0.41.235:6379]: wait -> close +8ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +15ms\n  ioredis:redis status[10.0.41.235:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.9.160:6379 because the node does not hold any slot +2ms\n  ioredis:redis status[10.0.9.160:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.9.160:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.16.159:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.16.159:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.16.159:6379]: close -> end +1ms\n  ioredis:cluster:connectionPool Disconnect 10.0.48.26:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.48.26:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +2ms\n  ioredis:redis status[10.0.48.26:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.86.18:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.86.18:6379]: wait -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[10.0.86.18:6379]: close -> end +1ms\n  ioredis:cluster:connectionPool Disconnect 10.0.37.140:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.37.140:6379]: wait -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.37.140:6379]: close -> end +0ms\nConnect event\n  ioredis:redis status[10.0.16.159:6379]: ready -> close +5ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +5ms\n  ioredis:redis status[10.0.16.159:6379]: close -> end +0ms\n  ioredis:cluster status: disconnecting -> close +13ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +103ms\n  ioredis:cluster status: reconnecting -> connecting +0ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +110ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +105ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +0ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +1ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +0ms\n  ioredis:cluster getting slot cache from clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 +2ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> connecting +1ms\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> connecting +0ms\n  ioredis:redis status[10.0.86.18:6379]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[10.0.16.159:6379]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.86.18:6379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +111ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[10.0.16.159:6379]: connect -> ready +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +7ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.16.159:6379 as master +1ms\n  ioredis:redis status[10.0.16.159:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.48.26:6379 as slave +0ms\n  ioredis:redis status[10.0.48.26:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.86.18:6379 as master +0ms\n  ioredis:redis status[10.0.86.18:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.37.140:6379 as slave +1ms\n  ioredis:redis status[10.0.37.140:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.41.235:6379 as master +0ms\n  ioredis:redis status[10.0.41.235:6379]: [empty] -> wait +3ms\n  ioredis:cluster:connectionPool Connecting to 10.0.9.160:6379 as slave +3ms\n  ioredis:redis status[10.0.9.160:6379]: [empty] -> wait +3ms\n  ioredis:cluster status: connecting -> connect +16ms\n  ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting... +0ms\n  ioredis:cluster status: connect -> disconnecting +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +4ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.16.159:6379 because the node does not hold any slot +0ms\n  ioredis:redis status[10.0.16.159:6379]: wait -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +10ms\n  ioredis:redis status[10.0.16.159:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.48.26:6379 because the node does not hold any slot +2ms\n  ioredis:redis status[10.0.48.26:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.48.26:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.86.18:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.86.18:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.86.18:6379]: close -> end +2ms\n  ioredis:cluster:connectionPool Disconnect 10.0.37.140:6379 because the node does not hold any slot +2ms\n  ioredis:redis status[10.0.37.140:6379]: wait -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[10.0.37.140:6379]: close -> end +1ms\n  ioredis:cluster:connectionPool Disconnect 10.0.41.235:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[10.0.41.235:6379]: wait -> close +3ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[10.0.41.235:6379]: close -> end +0ms\n  ioredis:cluster:connectionPool Disconnect 10.0.9.160:6379 because the node does not hold any slot +3ms\n  ioredis:redis status[10.0.9.160:6379]: wait -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +0ms\n  ioredis:redis status[10.0.9.160:6379]: close -> end +1ms\nConnect event\nioredis:redis status[10.0.86.18:6379]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[10.0.86.18:6379]: close -> end +0ms\n  ioredis:redis status[10.0.16.159:6379]: ready -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.16.159:6379]: close -> end +0ms\n  ioredis:cluster status: disconnecting -> close +13ms\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 104ms +107ms\n  ioredis:cluster status: reconnecting -> connecting +1ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +112ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +108ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +1ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 as master +1ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: [empty] -> wait +1ms\n  ioredis:cluster getting slot cache from clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 +2ms\n  ioredis:redis status[clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> connecting +0ms\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> connecting +0ms\n  ioredis:redis status[10.0.41.235:6379]: connecting -> connect +6ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[10.0.16.159:6379]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.41.235:6379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +119ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:redis status[10.0.16.159:6379]: connect -> ready +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0001-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +11ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +3ms\n  ioredis:redis status[clustertest-0002-001.ggdm49.0001.euw1.cache.amazonaws.com:6379]: close -> end +1ms\n  ioredis:cluster:connectionPool Disconnect clustertest-0003-001.ggdm49.0001.euw1.cache.amazonaws.com:6379 because the node does not hold any slot +2ms\n  ioredis:cluster:connectionPool Connecting to 10.0.41.235:6379 as master +0ms\n  ioredis:redis status[10.0.41.235:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.9.160:6379 as slave +1ms\n  ioredis:redis status[10.0.9.160:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.86.18:6379 as master +0ms\n  ioredis:redis status[10.0.86.18:6379]: [empty] -> wait +1ms\n  ioredis:cluster:connectionPool Connecting to 10.0.37.140:6379 as slave +1ms\n  ioredis:redis status[10.0.37.140:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.16.159:6379 as master +0ms\n  ioredis:redis status[10.0.16.159:6379]: [empty] -> wait +0ms\n  ioredis:cluster:connectionPool Connecting to 10.0.48.26:6379 as slave +1ms\n  ioredis:redis status[10.0.48.26:6379]: [empty] -> wait +1ms\n  ioredis:cluster status: connecting -> connect +16ms\n  ioredis:redis status[10.0.16.159:6379]: wait -> connecting +0ms\n  ioredis:redis queue command[0] -> cluster(info) +0ms\nConnect event\nioredis:redis status[10.0.41.235:6379]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +6ms\n  ioredis:redis status[10.0.41.235:6379]: close -> end +0ms\n  ioredis:redis status[10.0.16.159:6379]: connecting -> connect +0ms\n  ioredis:redis write command[0] -> info() +1ms\n  ioredis:redis status[10.0.16.159:6379]: ready -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[10.0.16.159:6379]: close -> end +0ms\n  ioredis:redis status[10.0.9.160:6379]: wait -> connecting +0ms\n  ioredis:redis status[10.0.16.159:6379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +1ms\n  ioredis:redis write command[0] -> cluster(info) +0ms\n  ioredis:redis status[10.0.9.160:6379]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:cluster status: connect -> ready +6ms\n  ioredis:redis status[10.0.9.160:6379]: connect -> ready +1ms\n  ioredis:connection set the connection to readonly mode +3ms\n  ioredis:redis write command[0] -> readonly() +1ms\n  ioredis:cluster getting slot cache from 10.0.48.26:6379 +5s\n  ioredis:redis status[10.0.48.26:6379]: wait -> connecting +5s\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[10.0.48.26:6379]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.48.26:6379]: connect -> ready +2ms\n  ioredis:connection set the connection to readonly mode +5s\n  ioredis:redis write command[0] -> readonly() +1ms\n  ioredis:connection send 1 commands in offline queue +1ms\n  ioredis:redis write command[0] -> cluster(slots) +0ms\n  ioredis:cluster getting slot cache from 10.0.48.26:6379 +5s\n  ioredis:redis write command[0] -> cluster(slots) +5s\n  ioredis:cluster getting slot cache from 10.0.41.235:6379 +5s\n  ioredis:redis status[10.0.41.235:6379]: wait -> connecting +5s\n  ioredis:redis queue command[0] -> cluster(slots) +1ms\n  ioredis:redis status[10.0.41.235:6379]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.41.235:6379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +10s\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n  ioredis:cluster getting slot cache from 10.0.16.159:6379 +5s\n  ioredis:redis write command[0] -> cluster(slots) +5s\n  ioredis:cluster getting slot cache from 10.0.41.235:6379 +5s\n  ioredis:redis write command[0] -> cluster(slots) +5s\n  ioredis:cluster getting slot cache from 10.0.16.159:6379 +5s\n  ioredis:redis write command[0] -> cluster(slots) +5s\n  ioredis:cluster getting slot cache from 10.0.37.140:6379 +5s\n  ioredis:redis status[10.0.37.140:6379]: wait -> connecting +5s\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[10.0.37.140:6379]: connecting -> connect +2ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.37.140:6379]: connect -> ready +3ms\n  ioredis:connection set the connection to readonly mode +20s\n  ioredis:redis write command[0] -> readonly() +0ms\n  ioredis:connection send 1 commands in offline queue +1ms\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n  ioredis:cluster getting slot cache from 10.0.86.18:6379 +5s\n  ioredis:redis status[10.0.86.18:6379]: wait -> connecting +5s\n  ioredis:redis queue command[0] -> cluster(slots) +0ms\n  ioredis:redis status[10.0.86.18:6379]: connecting -> connect +1ms\n  ioredis:redis write command[0] -> info() +0ms\n  ioredis:redis status[10.0.86.18:6379]: connect -> ready +1ms\n  ioredis:connection send 1 commands in offline queue +5s\n  ioredis:redis write command[0] -> cluster(slots) +1ms\n. \n",
    "callsea1": "I get multiple on 3.2, similar code as above, just logging on the connect event. Local server has no requests on it, yet will spit out \"New Redis Connection\" several times in a brief ten minute window.. Update, upgraded to 4.0.0 - and still am getting multiple (3 in about the same ten-minute window). Have listeners 'close' and 'connect', and this is what I see:\n[2] Closed Redis Connection\n[2] New Redis Connection\n[2] Closed Redis Connection\n[2] New Redis Connection\nSo there is some TTL on the connection and it reestablishes?. @simondutertre I have moved on \ud83d\ude02 -- but not using a cluster,  simply this:\nimport Redis from 'ioredis';\n redisInstance = new Redis(REDIS_URL, options);\nBut I tried swapping in this package: https://www.npmjs.com/package/redis\nAnd the same behavior existed, closing and re-establishing a connection. These are the two most popular libraries... \ud83e\udd14 . I tried the lazyConnect suggestion above, and the same behavior happened. this is not fixed. please reopen this issue @luin \n\n. ",
    "hhgyu": "@callsea1\nIf you modify it referring comment, Is there any problem?\n697. If you modify it referring comment, Is there any problem?\n697. @luin\nhttps://github.com/luin/ioredis/pull/697#discussion-diff-218673494R417\nThe problem occurs when querying the list of clusters, the attempt is from ioredis, is not an error, and makes it difficult to identify it when the actual error occurs.. check it ba4201cb18ac0da70e0b16e7ab1e47ce5497b9b4. added\ntryNode(index + 1);\nreturn;. @luin\nThe problem occurs when querying the list of clusters, the attempt is from ioredis, is not an error, and makes it difficult to identify it when the actual error occurs.. ",
    "stockholmux": "Seeing similar things... what's the status on this?. ",
    "szenciattila": "Thank you for the answer!\nBut the password is 100% correct and with incorrect password the error message is other.\nSo i think something is ignoring the password.. ",
    "Danver97": "Figured out after 3\u03bcs after publishing this issue (using a Redis console) that I'm running an old Redis version (pre 2.6) that not support EVAL commands.\nClosing this issue.. ",
    "DanielJoyce": "https://registry.yarnpkg.com/ioredis/-/ioredis-4.0.0.tgz#fabf1cf8724f14fd0885233cf2f4fbc6e1e59da2\nFrom our yarn.lock file.. ",
    "denigmus": "bee-queue dev2.0 will write inrespectfully to the status of redis, because ioredis emits \"ready\" without respect \"role\" or \"slave_read_only\" flags of the instance. So any app that uses that status will attempt to write to READONLY redis, not just the bee-queue.  It would be good if ioredis could check \"slave_read_only\" flag with INFO command before report ready and report readonly instead in case when slave_read_only=1.. ",
    "tvb": "@luin here you go\n2018-09-28T19:00:43.820Z ioredis:cluster status: [empty] -> connecting\n2018-09-28T19:00:43.822Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:43.823Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:43.823Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:43.825Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:43.825Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:47.540Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> connecting\n2018-09-28T19:00:47.540Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> connecting\n{\n    \"stack\": \"Error: Failed to refresh slots cache.\\n    at tryNode (/app/node_modules/ioredis/built/cluster/index.js:371:25)\\n    at /app/node_modules/ioredis/built/cluster/index.js:383:17\\n    at Timeout.<anonymous> (/app/node_modules/ioredis/built/cluster/index.js:594:20)\\n    at Timeout.run (/app/node_modules/ioredis/built/utils/index.js:144:22)\\n    at ontimeout (timers.js:486:15)\\n    at tryOnTimeout (timers.js:317:5)\\n    at Timer.listOnTimeout (timers.js:277:5)\",\n    \"message\": \"Failed to refresh slots cache.\",\n    \"lastNodeError\": {\n        \"stack\": \"Error: timeout\\n    at Object.exports.timeout (/app/node_modules/ioredis/built/utils/index.js:147:38)\\n    at Cluster.getInfoFromNode (/app/node_modules/ioredis/built/cluster/index.js:591:34)\\n    at tryNode (/app/node_modules/ioredis/built/cluster/index.js:376:15)\\n    at Cluster.refreshSlotsCache (/app/node_modules/ioredis/built/cluster/index.js:391:5)\\n    at Cluster.<anonymous> (/app/node_modules/ioredis/built/cluster/index.js:171:14)\\n    at new Promise (<anonymous>)\\n    at Cluster.connect (/app/node_modules/ioredis/built/cluster/index.js:125:12)\\n    at new Cluster (/app/node_modules/ioredis/built/cluster/index.js:81:14)\\n    at Object.<anonymous> (/app/node_modules/@myapp/myapp-core/core/redis.js:10:12)\\n    at Module._compile (module.js:652:30)\",\n        \"message\": \"timeout\"\n    },\n    \"level\": \"error\"\n}\n2018-09-28T19:00:47.573Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:47.709Z ioredis:redis status[172.16.32.178:6379]: connecting -> connect\n2018-09-28T19:00:47.709Z ioredis:redis queue command[0] -> info()\n2018-09-28T19:00:47.710Z ioredis:redis status[172.16.32.178:6379]: connect -> close\n2018-09-28T19:00:47.710Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:47.710Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:47.711Z ioredis:cluster status: connecting -> close\n2018-09-28T19:00:47.711Z ioredis:cluster status: close -> reconnecting\n2018-09-28T19:00:47.803Z ioredis:redis status[172.16.32.178:6379]: connecting -> connect\n2018-09-28T19:00:47.804Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:47.805Z ioredis:redis status[172.16.32.178:6379]: connect -> ready\n2018-09-28T19:00:47.814Z ioredis:cluster Cluster is disconnected. Retrying after 102ms\n2018-09-28T19:00:47.814Z ioredis:cluster status: reconnecting -> connecting\n2018-09-28T19:00:47.814Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:47.814Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:47.814Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:47.815Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:47.815Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:47.834Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:47.834Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:47.835Z ioredis:redis status[172.16.34.8:6379]: connect -> ready\n2018-09-28T19:00:47.943Z ioredis:redis status[172.16.32.178:6379]: connecting -> connect\n2018-09-28T19:00:47.943Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:47.943Z ioredis:redis status[172.16.32.178:6379]: connect -> ready\n2018-09-28T19:00:47.944Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:47.944Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:47.945Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:47.945Z ioredis:cluster:connectionPool Connecting to 172.16.32.178:6379 as master\n2018-09-28T19:00:47.945Z ioredis:redis status[172.16.32.178:6379]: [empty] -> wait\n2018-09-28T19:00:47.945Z ioredis:cluster:connectionPool Connecting to 172.16.34.8:6379 as slave\n2018-09-28T19:00:47.946Z ioredis:redis status[172.16.34.8:6379]: [empty] -> wait\n2018-09-28T19:00:47.946Z ioredis:cluster status: connecting -> connect\n2018-09-28T19:00:47.947Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-28T19:00:47.947Z ioredis:redis status[172.16.32.178:6379]: ready -> close\n2018-09-28T19:00:47.947Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:47.947Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:47.948Z ioredis:redis status[172.16.34.8:6379]: wait -> connecting\n2018-09-28T19:00:47.949Z ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting...\n2018-09-28T19:00:47.949Z ioredis:cluster status: connect -> disconnecting\n2018-09-28T19:00:47.949Z ioredis:cluster:connectionPool Disconnect 172.16.32.178:6379 because the node does not hold any slot\n2018-09-28T19:00:47.949Z ioredis:redis status[172.16.32.178:6379]: wait -> close\n2018-09-28T19:00:47.949Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:47.949Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:47.949Z ioredis:cluster:connectionPool Disconnect 172.16.34.8:6379 because the node does not hold any slot\n2018-09-28T19:00:47.949Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:47.949Z ioredis:redis queue command[0] -> info()\n2018-09-28T19:00:47.950Z ioredis:redis status[172.16.34.8:6379]: connect -> close\n2018-09-28T19:00:47.950Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:47.950Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:47.950Z ioredis:cluster status: disconnecting -> close\n2018-09-28T19:00:47.950Z ioredis:cluster status: close -> reconnecting\n2018-09-28T19:00:48.054Z ioredis:cluster Cluster is disconnected. Retrying after 104ms\n2018-09-28T19:00:48.054Z ioredis:cluster status: reconnecting -> connecting\n2018-09-28T19:00:48.055Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:48.055Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:48.055Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:48.055Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:48.055Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:48.160Z ioredis:redis status[172.16.32.178:6379]: connecting -> connect\n2018-09-28T19:00:48.160Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:48.161Z ioredis:redis status[172.16.32.178:6379]: connect -> ready\n2018-09-28T19:00:48.161Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:48.161Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:48.163Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:48.163Z ioredis:cluster:connectionPool Connecting to 172.16.32.178:6379 as master\n2018-09-28T19:00:48.164Z ioredis:redis status[172.16.32.178:6379]: [empty] -> wait\n2018-09-28T19:00:48.164Z ioredis:cluster:connectionPool Connecting to 172.16.34.8:6379 as slave\n2018-09-28T19:00:48.164Z ioredis:redis status[172.16.34.8:6379]: [empty] -> wait\n2018-09-28T19:00:48.164Z ioredis:cluster status: connecting -> connect\n2018-09-28T19:00:48.164Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-28T19:00:48.165Z ioredis:redis status[172.16.32.178:6379]: ready -> close\n2018-09-28T19:00:48.165Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.165Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:48.165Z ioredis:redis status[172.16.34.8:6379]: wait -> connecting\n2018-09-28T19:00:48.165Z ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting...\n2018-09-28T19:00:48.165Z ioredis:cluster status: connect -> disconnecting\n2018-09-28T19:00:48.165Z ioredis:cluster:connectionPool Disconnect 172.16.32.178:6379 because the node does not hold any slot\n2018-09-28T19:00:48.166Z ioredis:redis status[172.16.32.178:6379]: wait -> close\n2018-09-28T19:00:48.166Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.166Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:48.166Z ioredis:cluster:connectionPool Disconnect 172.16.34.8:6379 because the node does not hold any slot\n2018-09-28T19:00:48.166Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.166Z ioredis:redis queue command[0] -> info()\n2018-09-28T19:00:48.166Z ioredis:redis status[172.16.34.8:6379]: connect -> close\n2018-09-28T19:00:48.166Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.167Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.167Z ioredis:cluster status: disconnecting -> close\n2018-09-28T19:00:48.167Z ioredis:cluster status: close -> reconnecting\n2018-09-28T19:00:48.273Z ioredis:cluster Cluster is disconnected. Retrying after 106ms\n2018-09-28T19:00:48.273Z ioredis:cluster status: reconnecting -> connecting\n2018-09-28T19:00:48.273Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:48.274Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:48.274Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:48.274Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:48.274Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:48.392Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.392Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:48.393Z ioredis:redis status[172.16.34.8:6379]: connect -> ready\n2018-09-28T19:00:48.393Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:48.393Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:48.399Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:48.399Z ioredis:cluster:connectionPool Connecting to 172.16.32.178:6379 as master\n2018-09-28T19:00:48.399Z ioredis:redis status[172.16.32.178:6379]: [empty] -> wait\n2018-09-28T19:00:48.399Z ioredis:cluster:connectionPool Connecting to 172.16.34.8:6379 as slave\n2018-09-28T19:00:48.400Z ioredis:redis status[172.16.34.8:6379]: [empty] -> wait\n2018-09-28T19:00:48.400Z ioredis:cluster status: connecting -> connect\n2018-09-28T19:00:48.400Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-28T19:00:48.400Z ioredis:redis status[172.16.34.8:6379]: ready -> close\n2018-09-28T19:00:48.400Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.400Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.400Z ioredis:redis status[172.16.34.8:6379]: wait -> connecting\n2018-09-28T19:00:48.401Z ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting...\n2018-09-28T19:00:48.401Z ioredis:cluster status: connect -> disconnecting\n2018-09-28T19:00:48.401Z ioredis:cluster:connectionPool Disconnect 172.16.32.178:6379 because the node does not hold any slot\n2018-09-28T19:00:48.401Z ioredis:redis status[172.16.32.178:6379]: wait -> close\n2018-09-28T19:00:48.401Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.401Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:48.401Z ioredis:cluster:connectionPool Disconnect 172.16.34.8:6379 because the node does not hold any slot\n2018-09-28T19:00:48.401Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.401Z ioredis:redis queue command[0] -> info()\n2018-09-28T19:00:48.401Z ioredis:redis status[172.16.34.8:6379]: connect -> close\n2018-09-28T19:00:48.401Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.401Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.402Z ioredis:cluster status: disconnecting -> close\n2018-09-28T19:00:48.402Z ioredis:cluster status: close -> reconnecting\n2018-09-28T19:00:48.510Z ioredis:cluster Cluster is disconnected. Retrying after 108ms\n2018-09-28T19:00:48.510Z ioredis:cluster status: reconnecting -> connecting\n2018-09-28T19:00:48.510Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:48.510Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:48.510Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:48.510Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:48.511Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:48.619Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.619Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:48.620Z ioredis:redis status[172.16.34.8:6379]: connect -> ready\n2018-09-28T19:00:48.620Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:48.620Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:48.629Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:48.630Z ioredis:cluster:connectionPool Connecting to 172.16.32.178:6379 as master\n2018-09-28T19:00:48.630Z ioredis:redis status[172.16.32.178:6379]: [empty] -> wait\n2018-09-28T19:00:48.630Z ioredis:cluster:connectionPool Connecting to 172.16.34.8:6379 as slave\n2018-09-28T19:00:48.631Z ioredis:redis status[172.16.34.8:6379]: [empty] -> wait\n2018-09-28T19:00:48.631Z ioredis:cluster status: connecting -> connect\n2018-09-28T19:00:48.631Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-28T19:00:48.631Z ioredis:redis status[172.16.34.8:6379]: ready -> close\n2018-09-28T19:00:48.631Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.631Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.632Z ioredis:redis status[172.16.34.8:6379]: wait -> connecting\n2018-09-28T19:00:48.632Z ioredis:cluster Ready check failed (Error: Connection is closed.). Reconnecting...\n2018-09-28T19:00:48.632Z ioredis:cluster status: connect -> disconnecting\n2018-09-28T19:00:48.632Z ioredis:cluster:connectionPool Disconnect 172.16.32.178:6379 because the node does not hold any slot\n2018-09-28T19:00:48.632Z ioredis:redis status[172.16.32.178:6379]: wait -> close\n2018-09-28T19:00:48.632Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.632Z ioredis:redis status[172.16.32.178:6379]: close -> end\n2018-09-28T19:00:48.632Z ioredis:cluster:connectionPool Disconnect 172.16.34.8:6379 because the node does not hold any slot\n2018-09-28T19:00:48.632Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.633Z ioredis:redis queue command[0] -> info()\n2018-09-28T19:00:48.633Z ioredis:redis status[172.16.34.8:6379]: connect -> close\n2018-09-28T19:00:48.633Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.633Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.633Z ioredis:cluster status: disconnecting -> close\n2018-09-28T19:00:48.633Z ioredis:cluster status: close -> reconnecting\n2018-09-28T19:00:48.744Z ioredis:cluster Cluster is disconnected. Retrying after 110ms\n2018-09-28T19:00:48.744Z ioredis:cluster status: reconnecting -> connecting\n2018-09-28T19:00:48.744Z ioredis:cluster:connectionPool Connecting to redis.<<MY_DOMAIN>>:6379 as master\n2018-09-28T19:00:48.744Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: [empty] -> wait\n2018-09-28T19:00:48.744Z ioredis:cluster getting slot cache from redis.<<MY_DOMAIN>>:6379\n2018-09-28T19:00:48.744Z ioredis:redis status[redis.<<MY_DOMAIN>>:6379]: wait -> connecting\n2018-09-28T19:00:48.744Z ioredis:redis queue command[0] -> cluster(slots)\n2018-09-28T19:00:48.851Z ioredis:redis status[172.16.34.8:6379]: connecting -> connect\n2018-09-28T19:00:48.851Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:48.852Z ioredis:redis status[172.16.34.8:6379]: connect -> ready\n2018-09-28T19:00:48.852Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:48.852Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:48.853Z ioredis:cluster:connectionPool Disconnect redis.<<MY_DOMAIN>>:6379 because the node does not hold any slot\n2018-09-28T19:00:48.853Z ioredis:cluster:connectionPool Connecting to 172.16.32.178:6379 as master\n2018-09-28T19:00:48.853Z ioredis:redis status[172.16.32.178:6379]: [empty] -> wait\n2018-09-28T19:00:48.853Z ioredis:cluster:connectionPool Connecting to 172.16.34.8:6379 as slave\n2018-09-28T19:00:48.853Z ioredis:redis status[172.16.34.8:6379]: [empty] -> wait\n2018-09-28T19:00:48.853Z ioredis:cluster status: connecting -> connect\n2018-09-28T19:00:48.853Z ioredis:redis status[172.16.32.178:6379]: wait -> connecting\n2018-09-28T19:00:48.853Z ioredis:redis queue command[0] -> cluster(info)\n2018-09-28T19:00:48.854Z ioredis:redis status[172.16.34.8:6379]: ready -> close\n2018-09-28T19:00:48.854Z ioredis:connection skip reconnecting since the connection is manually closed.\n2018-09-28T19:00:48.854Z ioredis:redis status[172.16.34.8:6379]: close -> end\n2018-09-28T19:00:48.854Z ioredis:redis status[172.16.32.178:6379]: connecting -> connect\n2018-09-28T19:00:48.854Z ioredis:redis write command[0] -> info()\n2018-09-28T19:00:48.855Z ioredis:redis status[172.16.32.178:6379]: connect -> ready\n2018-09-28T19:00:48.855Z ioredis:connection send 1 commands in offline queue\n2018-09-28T19:00:48.855Z ioredis:redis write command[0] -> cluster(info)\n2018-09-28T19:00:48.856Z ioredis:cluster status: connect -> ready\n2018-09-28T19:00:49.593Z ioredis:redis write command[0] -> get(ed7fc182-5a18-4c92-8d62-7c140be54545:orders)\n2018-09-28T19:00:49.594Z ioredis:redis write command[0] -> get(ed7fc182-5a18-4c92-8d62-7c140be54545:positions)\n2018-09-28T19:00:53.857Z ioredis:cluster getting slot cache from 172.16.32.178:6379\n2018-09-28T19:00:53.857Z ioredis:redis write command[0] -> cluster(slots)\n2018-09-28T19:00:58.857Z ioredis:cluster getting slot cache from 172.16.32.178:6379\n2018-09-28T19:00:58.858Z ioredis:redis write command[0] -> cluster(slots)\nafter which the application logs are starting. @luin perhaps I don't need to use the  new Redis.Cluster with Amazon ElastiCache?. @luin bump. Can we look into this together?. We still have this issue \ud83d\ude2d. Correct. We see the same MOVED message if we connect without .Cluster config.\nProviding individual node endpoints is not an option for us.. ",
    "liam-murray-xealth": "I see a similar issue. In my case it appears to be a race condition because the connection error is intermittent. When it fails I would only see the following error message (we are using an encryption enabled elasticache cluster).\n\nError: 140434768993160:error:140940E5:SSL routines:ssl3_read_bytes:ssl handshake failure:../deps/openssl/openssl/ssl/s3_pkt.c:1216:\n\nOnce I enabled DEBUG I could see that the connection failure would always be associated with a bunch of \"Disconnect because the node does not hold any slot\" messages. (I tried enableReadyCheck set true and false and it made no difference.)\n2018-10-06T18:24:38.297Z ioredis:cluster:connectionPool Disconnect xxx.usw2.cache.amazonaws.com:6379 because the node does not hold any slot\nI tried redis-clustr with node-redis and am able to connect fine with those. . I think I found something that will help. I have been passing the single Configuration Endpoint URL to Redis.Cluster() (as the original author of this issue has been doing).\n\n{\nhost: clustercfg.mycluster.usw2.cache.amazonaws.com\nport: 6379\n}\n\nI tried instead passing the endpoints directly and it appears to work.\n\nconst nodes = [\n  {\n    host: 'my-cluster-0001-001.xxx.usw2.cache.amazonaws.com',\n    port: 6379,\n  },\n  {\n    host: 'my-cluster-0001-002.xxx.usw2.cache.amazonaws.com',\n    port: 6379,\n  },\n  {\n    host: 'my-cluster-0001-003.xxx.usw2.cache.amazonaws.com',\n    port: 6379,\n  },\n]\n\nI obtained the above by running the following (I only have one cluster with three nodes).\n\naws elasticache describe-cache-clusters --show-cache-node-info  | jq -r '.CacheClusters[].CacheNodes[].Endpoint'\n\nFunny thing is that it (\"Redis.Cluster\") sometimes does work with the configuration endpoint--it just seems to have some race condition that prevents it from always working.\nOut of curiosity I tried using \"new Redis()\" passing the cluster configuration endpoint and got: \n\nReplyError: MOVED 11958  'my-cluster-0001-001.xxx.usw2.cache.amazonaws.com':6379\n\n. ",
    "TNieminen": "Just noticed that this fix will not work on Redis engine 5.0 on Elasticache. \nDowngrade to  4.0.10 to make it work.. ",
    "dko-ahoyhoy": "Unfortunately the problem still exists. Version 4.6.2, exactly same symptoms.\nElastiCache with TLS, Engine Version Compatibility: 5.0.3. ",
    "frank-liu": "\u5b50\u9a85\u4f60\u597d\n\u6ca1\u60f3\u5230\u4f60\u4f1a\u56de\u590d\u6211\u7684\u95ee\u9898\u3002\u975e\u5e38\u611f\u8c22\u3002\n\u70e6\u8bf7\u67e5\u770b\u4e0b\u9762debug\u4fe1\u606f\uff1a\n[image: image.png]\nDebug\u8f93\u51fa\u6587\u672c\u5982\u4e0b\uff1a\nvar { StandaloneConnector, SentinelConnector } = require('./connectors');\n    ^\nSyntaxError: Unexpected token {\n    at exports.runInThisContext (vm.js:53:16)\n    at Module._compile (module.js:374:25)\n    at Object.Module._extensions..js (module.js:417:10)\n    at Module.load (module.js:344:32)\n    at Function.Module._load (module.js:301:12)\n    at Module.require (module.js:354:17)\n    at require (internal/module.js:12:17)\n    at Object.\n(/var/socketio/node_modules/ioredis/built/index.js:2:28)\n    at Module._compile (module.js:410:26)\n    at Object.Module._extensions..js (module.js:417:10)\nMany thanks for your reply.\nKind Regards,\nOn Sat, 6 Oct 2018 at 18:51, \u5b50\u9a85 notifications@github.com wrote:\n\nCould you enable the debug mode (DEBUG=ioredis:* node yourapp.js) and\npost the logs here?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/issues/714#issuecomment-427593654, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AC3r096wkt_Wbj2Q91yok98sePjn_FlEks5uiO2ygaJpZM4XJ1fg\n.\n. Thank you for your reply. My node js version is v4.2.6. I will try to upgrade it.. \n",
    "jnst": "Thanks for quick response <3\nbash\n$ env DEBUG='ioredis:*' npx ts-node sample.ts\n  ioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis queue command[localhost:6379]: 0 -> echo([ 'hello' ]) +3ms\n  ioredis:redis status[127.0.0.1:6379]: connecting -> connect +15ms\n  ioredis:redis write command[127.0.0.1:6379]: 0 -> info([]) +1ms\n  ioredis:redis status[127.0.0.1:6379]: connect -> ready +21ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[127.0.0.1:6379]: 0 -> echo([ 'hello' ]) +1ms\nhello\n  ioredis:redis status[127.0.0.1:6379]: ready -> close +33ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +34ms\n  ioredis:redis status[127.0.0.1:6379]: close -> end +0ms\nMy question was solved \ud83d\ude19. ",
    "boltzjf": "Has it been included in Redis 5.0?. ",
    "ashamia": "thanks for the quick response. \nupdated to the latest but still get an error: \"ReplyError: ERR unknown command zpopmin\"\n(other sorted set commands e.g. zrangebyscore work fine). \u2514\u2500\u252c ioredis@4.1.0\n  \u2514\u2500\u2500 redis-commands@1.4.0\n. Could you please confirm that zpopmin works with the latest ioredis/redis-commands?\nI tested the newest versions and they work with ALL sorted sets commands EXCEPT zpopmin and zpopmax!. blocking pop ops on sorted sets was only introduced in redis v5 so upgrading redis server to v5 solved it... . ",
    "prakasa1904": "when use redis-cli its working normal... . debug output:\nioredis:redis status[HOST:PORT]: [empty] -> connecting +0ms\n  ioredis:redis status[IP:PORT]: connecting -> connect +11ms\n  ioredis:redis write command[IP:PORT]: 0 -> info([]) +2ms\n  ioredis:redis status[IP:PORT]: connect -> close +4ms\n  ioredis:connection reconnect in 50ms +0ms\n  ioredis:redis status[IP:PORT]: close -> reconnecting +0ms\n  ioredis:redis status[IP:PORT]: reconnecting -> connecting +51ms\n  ioredis:redis status[IP:PORT]: connecting -> connect +4ms\n  ioredis:redis write command[IP:PORT]: 0 -> info([]) +1ms\n  ioredis:redis status[IP:PORT]: connect -> close +1ms\n  ioredis:connection reconnect in 100ms +57ms\nwhen I try this: redis-cli -h <IP> -p <PORT . Its fine\nany idea about this isue ?. fixed, my bad. My configuration is typo enableReadyChek and its just ignored by ioredis. My proxy deni info command.. ",
    "renovate[bot]": "Renovate is disabled\nRenovate is disabled due to lack of config. If you wish to reenable it, you can either (a) commit a config file to your base branch, or (b) rename this closed PR to trigger a replacement onboarding PR.. ",
    "funduck": "Well, I still have to do something about it, I'll write then.\nHow do you think it should behave? the easiest is to throw an error.. . well, that sounds right, I didn't need 'perfect' implementation and it was not hard to make one, so others can do too. \nthanks for replies!. ",
    "jakedipity": "You can safely ignore this issue. I mistakenly thought ioredis will give me an error when a transaction is invalidated by a watch command, but the result is just null as described in redis' documentation.. ",
    "kasarlashushruth": "When i use the resolved ip,\nWith out ssl its working but when i turn on ssl and try to connect it not.\nFYI, the key is client private key\ncert is client public cert\nCA is a chained cert of Client public cert, intermediate cert and trust root\ni tried passing only key and ca\nor passing key, cert and ca with out client public cert and other time with ca with client public cert\nThis is what i get if i use resolved ip\nioredis:redis status[localhost:6379]: [empty] -> connecting +0ms\n  ioredis:redis status[ip]: [empty] -> connecting +2ms\n  ioredis:redis queue command[ip]: 0 -> sentinel([ 'get-master-addr-by-name', 'name' ]) +1ms\n  ioredis:redis queue command[localhost:6379]: 0 -> set([ 'foo1', 'shushruth' ]) +1ms\n  ioredis:redis queue command[localhost:6379]: 0 -> get([ 'foo1' ]) +1ms\n  ioredis:redis status[ip]: connecting -> connect +49ms\n  ioredis:redis status[ip]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis write command[ip]: 0 -> sentinel([ 'get-master-addr-by-name', 'name' ]) +1ms\n  ioredis:SentinelConnector failed to connect to sentinel ip because ERR unencrypted connection is prohibited +0ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 10ms. Last error: ERR unencrypted connection is prohibited +0ms\nSomething went wrong  Error: All sentinels are unreachable. Retrying from scratch after 10ms. Last error: ERR unencrypted connection is prohibited\n    at connectToNext (/Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:54:31)\n    at /Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:86:21\n    at client.sentinel (/Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:117:24)\n    at tryCatcher (/Users/sredd140/Research/redis1/node_modules/standard-as-callback/lib/utils.js:10:19)\n    at /Users/sredd140/Research/redis1/node_modules/standard-as-callback/index.js:31:35\n    at \n    at process._tickCallback (internal/process/next_tick.js:188:7)\n  ioredis:redis status[ip]: ready -> close +16ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +16ms\n  ioredis:redis status[ip]: close -> end +0ms\n  ioredis:redis status[ip]: [empty] -> connecting +11ms\n  ioredis:redis queue command[ip]: 0 -> sentinel([ 'get-master-addr-by-name', 'name' ]) +0ms\n  ioredis:redis status[ip]: connecting -> connect +7ms\n  ioredis:redis status[ip]: connect -> ready +0ms\n  ioredis:connection send 1 commands in offline queue +18ms\n  ioredis:redis write command[ip]: 0 -> sentinel([ 'get-master-addr-by-name', 'name' ]) +0ms\n  ioredis:SentinelConnector failed to connect to sentinel ip because ERR unencrypted connection is prohibited +28ms\n  ioredis:SentinelConnector All sentinels are unreachable. Retrying from scratch after 20ms. Last error: ERR unencrypted connection is prohibited +0ms\nSomething went wrong  Error: All sentinels are unreachable. Retrying from scratch after 20ms. Last error: ERR unencrypted connection is prohibited\n    at connectToNext (/Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:54:31)\n    at /Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:86:21\n    at client.sentinel (/Users/sredd140/Research/redis1/node_modules/ioredis/built/connectors/SentinelConnector/index.js:117:24)\n    at tryCatcher (/Users/sredd140/Research/redis1/node_modules/standard-as-callback/lib/utils.js:10:19)\n    at /Users/sredd140/Research/redis1/node_modules/standard-as-callback/index.js:31:35\n    at \n    at process._tickCallback (internal/process/next_tick.js:188:7)\n  ioredis:redis status[ip]: ready -> close +8ms\n. ",
    "kiko35": "Ok thanks.. ",
    "galaa2011": "It's my fault~. ",
    "vflopes": "I don't know when they plan to support the x* commands, I'm trying to implement using the following snippet (hacky and dirty, DON'T USE IT):\n```javascript\n'use strict';\nconst IORedis = require('ioredis');\nconst parseObjectResponse = (reply, customParser = null) => {\n    if (!Array.isArray(reply))\n        return reply;\n    const data = {};\n    for (let i = 0; i < reply.length; i += 2) {\n        if (customParser) {\n            data[reply[i]] = customParser(reply[i], reply[i+1]);\n            continue;\n        }\n        data[reply[i]] = reply[i+1];\n    }\n    return data;\n};\nconst parseMessageResponse = (reply) => {\n    if (!Array.isArray(reply))\n        return [];\n    return reply.map((message) => {\n        return {id:message[0], data:parseObjectResponse(message[1])};\n    });\n};\nconst parseStreamResponse = (reply) => {\n    if (!Array.isArray(reply))\n        return reply;\n    const object = {};\n    for (const stream of reply)\n        object[stream[0]] = parseMessageResponse(stream[1]);\n    return object;\n};\nconst addCommand = {\n    xgroup:(target) => target.Command.setReplyTransformer('xgroup', (reply) => reply),\n    xadd:(target) => target.Command.setReplyTransformer('xadd', (reply) => reply),\n    xread:(target) => target.Command.setReplyTransformer('xread', parseStreamResponse),\n    xreadgroup:(target) => target.Command.setReplyTransformer('xreadgroup', parseStreamResponse),\n    xrange:(target) => target.Command.setReplyTransformer('xrange', parseMessageResponse),\n    xrevrange:(target) => target.Command.setReplyTransformer('xrevrange', parseMessageResponse),\n    xclaim:(target) => target.Command.setReplyTransformer('xclaim', parseMessageResponse),\n    xinfo:(target) => target.Command.setReplyTransformer('xinfo', (reply) => parseObjectResponse(reply, (key, value) => {\n        switch (key) {\n        case 'first-entry':\n        case 'last-entry':\n            if (!Array.isArray(value))\n                return value;\n            return {\n                id:value[0],\n                data:parseObjectResponse(value[1])\n            };\n        default:\n            return value;\n        }\n    })),\n    xack:(target) => target.Command.setReplyTransformer('xack', (reply) => parseInt(reply)),\n    xlen:(target) => target.Command.setReplyTransformer('xlen', (reply) => parseInt(reply)),\n    xtrim:(target) => target.Command.setReplyTransformer('xtrim', (reply) => parseInt(reply)),\n    xdel:(target) => target.Command.setReplyTransformer('xdel', (reply) => parseInt(reply))\n};\nlet isPrepared = false;\nmodule.exports = () => {\nif (isPrepared)\n    return void 0;\n\nisPrepared = true;\n\nObject.keys(addCommand).forEach((command) => {\n    const {string, buffer} = IORedis.prototype.createBuiltinCommand(command);\n    IORedis.prototype[command] = string;\n    IORedis.prototype[command+'Buffer'] = buffer;\n    addCommand[command](IORedis);\n});\n\n};\n```\nBut the xreadgroup with BLOCK option is not working with Redis Cluster \ud83d\ude22 \nI'm getting the following error: Too many Cluster redirections.\nWell, this library needs a heavy refactoring job and a better documentation... There're parts of the code that brings some really bad anti-patterns like the \"sendCommand\" function of the Cluster class.\nAnyway, if anyone want to collaborate with some snippet to support stream commands I'm able to test them.. I've developed this script to add stream commands to ioredis (I have this project that uses Redis streams: warshipjs):\n```javascript\n'use strict';\nconst IORedis = require('ioredis');\nconst parseObjectResponse = (reply, customParser = null) => {\n    if (!Array.isArray(reply))\n        return reply;\n    const data = {};\n    for (let i = 0; i < reply.length; i += 2) {\n        if (customParser) {\n            data[reply[i]] = customParser(reply[i], reply[i+1]);\n            continue;\n        }\n        data[reply[i]] = reply[i+1];\n    }\n    return data;\n};\nconst parseMessageResponse = (reply) => {\n    if (!Array.isArray(reply))\n        return [];\n    return reply.map((message) => {\n        return {id:message[0], data:parseObjectResponse(message[1])};\n    });\n};\nconst parseStreamResponse = (reply) => {\n    if (!Array.isArray(reply))\n        return reply;\n    const object = {};\n    for (const stream of reply)\n        object[stream[0]] = parseMessageResponse(stream[1]);\n    return object;\n};\nconst addCommand = {\n    xgroup:(target) => target.Command.setReplyTransformer('xgroup', (reply) => reply),\n    xadd:(target) => target.Command.setReplyTransformer('xadd', (reply) => reply),\n    xread:(target) => target.Command.setReplyTransformer('xread', parseStreamResponse),\n    xpending:(target) => target.Command.setReplyTransformer('xpending', (reply) => {\n        if (!reply || reply.length === 0)\n            return [];\n        if (reply.length === 4 && !isNaN(reply[0]))\n            return {\n                count:parseInt(reply[0]),\n                minId:reply[1],\n                maxId:reply[2],\n                consumers:(reply[3] || []).map((consumer) => {\n                    return {\n                        name:consumer[0],\n                        count:parseInt(consumer[1])\n                    };\n                })\n            };\n        return reply.map((message) => {\n            return {\n                id:message[0],\n                consumerName:message[1],\n                elapsedMilliseconds:parseInt(message[2]),\n                deliveryCount:parseInt(message[3])\n            };\n        });\n    }),\n    xreadgroup:(target) => target.Command.setReplyTransformer('xreadgroup', parseStreamResponse),\n    xrange:(target) => target.Command.setReplyTransformer('xrange', parseMessageResponse),\n    xrevrange:(target) => target.Command.setReplyTransformer('xrevrange', parseMessageResponse),\n    xclaim:(target) => target.Command.setReplyTransformer('xclaim', parseMessageResponse),\n    xinfo:(target) => target.Command.setReplyTransformer('xinfo', (reply) => parseObjectResponse(reply, (key, value) => {\n        switch (key) {\n        case 'first-entry':\n        case 'last-entry':\n            if (!Array.isArray(value))\n                return value;\n            return {\n                id:value[0],\n                data:parseObjectResponse(value[1])\n            };\n        default:\n            return value;\n        }\n    })),\n    xack:(target) => target.Command.setReplyTransformer('xack', (reply) => parseInt(reply)),\n    xlen:(target) => target.Command.setReplyTransformer('xlen', (reply) => parseInt(reply)),\n    xtrim:(target) => target.Command.setReplyTransformer('xtrim', (reply) => parseInt(reply)),\n    xdel:(target) => target.Command.setReplyTransformer('xdel', (reply) => parseInt(reply))\n};\nlet isPrepared = false;\nmodule.exports = () => {\nif (isPrepared)\n    return void 0;\n\nisPrepared = true;\n\nObject.keys(addCommand).forEach((command) => {\n    const {string, buffer} = IORedis.prototype.createBuiltinCommand(command);\n    IORedis.prototype[command] = string;\n    IORedis.prototype[command+'Buffer'] = buffer;\n    addCommand[command](IORedis);\n});\n\n};\n```\nYou just need to require it and execute the function before any call to instantiate a Redis os Cluster class.\njavascript\nrequire('./ioredis-streams.js')();\n// now you have the x* commands. Fixed by PR https://github.com/luin/ioredis/pull/749. Question 3: nope, ioredis does not have a connection pool. Think about each client instance of Redis class from ioredis as an open TCP socket (or Unix) wrapped by functions to interpret the Redis protocol, connectivity, offline queueing and all Redis commands through an asynchronous API. That's what ioredis is (:\nAnything more than this will be out of the scope of a good client library/package. If you want to forget about handling clients connections, search about \"sockets pool\" it'll fit really well in this case.. ",
    "matthax": "This is a feature I would be extremely interested in seeing. When I have some free time I will take a look to see what I can dig up - maybe the solution is as simple as a simple interval.\nIf enough people are interested maybe we could have this feature sponsored?. So I ended up having to do this when writing something for a test project. \nI took what I had done and I've rewritten it and dropped a few of the features so that I could share it here, take a look at let me know if this works for you:\nhttps://gist.github.com/matthax/d87c0b1bc0d381c7549f89cbc3bd1941. ",
    "zhongshixi": "\nI think you should always use a single redis instance to communicate with redis if your nodejs is a single instance.\nyou should always use the existing connection ( like connection pool for database), since creating and closing connection will add overhead\nthat is a good question, unfortunately I do not have knowledge to answer it now, probably someone will answer it for you. \n",
    "sulthan-ahmed": "thanks @zhongshixi \ud83d\udc4d \nI'll wait for someone to answer question 3 before closing this issue. Thanks @vflopes . ",
    "alansmith18": "\nYou can disable the clusterRetryStrategy option. See readme for details.\n\nthank you . ",
    "nicokaiser": "Update: this can also happen on reconnect, e.g. if the DNS for one cluster node is not available anymore. I consider this as critical, as error handling in these cases is not possible and ioredis stops reconnecting without the application noticing.\nioredis should either emit an \"error\" event in these cases, or \u2013\u00a0like in version 3 \u2013\u00a0retry connecting following the clusterRetryStrategy.. ",
    "hendrixroa": "Review security groups inbound rules, or check if is accessible your cluster . @MishUshakov  Try delete the networks and replace redis section by:\nyml\nredis:\n    image: redis:latest\n    command: [\"redis-server\", \"--bind\", \"redis\", \"--port\", \"6379\"]\nThat works for me. . ",
    "jordantdavis": "I do not believe this is an issue with security groups.  I am able to connect to the cluster and obtain a certificate.  \nThe issue is that the connection to the cluster is being made using an IP address instead of the given hostname and the certificate does not contain the IP address as an alternative name, so the TLS hostname verification step fails.  I can get around this issue with a custom resolveStartupNodeHostnames, but it is less secure.. Thank you for the suggestion @luin.  I gave it a try with my sample code above, but it looks like I'm still unable to successfully connect to the cluster via TLS.  I'm still getting the following error: \n[ioredis] Unhandled error event: ClusterAllFailedError: Failed to refresh slots cache.\nI added a console.log to the dnsLookup override and I'm not seeing any console output, so the override does not seem to be having an effect.  It looks like my example code is failing to establish a TLS connection with the given \"cluster config\" node and it is subsequently failing to run the CLUSTER SLOTS command.  \nI think this may be because there is a separate, un-overrideable DNS lookup that is performed on the given \"cluster config\" node hostname and that IP address is then used to attempt a TLS connection with the \"cluster config\" node.  I know it is possible to connect to the Elasticache \"cluster config\" node using a hostname instead of an IP address (with my rough change to the library above), but I'm not sure if that is something Redis is capable of or just Elasticache.\nI'd be happy to help add the functionality to make that initial connection using a hostname instead of a looked up IP address if you think it fits here.\nThoughts?. I added a console.log to the function you referenced:\n```\ndnsLookup(hostname) {\n    return new Promise((resolve, reject) => {\n        console.log(this.options.dnsLookup.toString());\n    this.options.dnsLookup(hostname, (err, address) => {\n        if (err) {\n            debug('failed to resolve hostname %s to IP: %s', hostname, err.message);\n            reject(err);\n        } else {\n            debug('resolved hostname %s to IP %s', hostname, address);\n            resolve(address);\n        }\n    });\n});\n\n}\n```\nand I see this output:\n```\n\nencrypted-ec-ioredis@1.0.0 start /Users/jdavis/workspace/experiments/encrypted-ec-ioredis\nnode index.js\n\n(address, callback) => callback(null, address)\n(address, callback) => callback(null, address)\n(address, callback) => callback(null, address)\n(address, callback) => callback(null, address)\ntest\n```\nwhich is expected.  That is also the function I provided in the dnsLookup options override.  Additionally, I have ran the same script (with override) many times tonight and I have not seen the same error message as last night.  I have not seen the script error out a single time tonight either.\nThat is what you expected to see, correct?. Just adding some additional logs I see when running the script with dnsLookup override:\n```\n\nencrypted-ec-ioredis@1.0.0 start /Users/jdavis/workspace/experiments/encrypted-ec-ioredis\nnode index.js\n\n[ioredis] Unhandled error event: ClusterAllFailedError: Failed to refresh slots cache.\n    at tryNode (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:319:31)\n    at /Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:335:21\n    at Timeout.redis.cluster.utils_1.timeout (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:551:24)\n    at Timeout.run (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/utils/index.js:150:22)\n    at ontimeout (timers.js:438:13)\n    at tryOnTimeout (timers.js:300:5)\n    at listOnTimeout (timers.js:263:5)\n    at Timer.processTimers (timers.js:223:10)\ntest\n```\nSometimes I will see that error, but eventually it connects to the cluster.. > That should be another issue I think. Does it accurs without dnsLookup?\nI do not remember seeing that error without dnsLookup.  In those cases, I override the checkServerIdentity function in the redisOptions.tls config to ignore the hostname verification for the certificate (the original problem + workaround).\nIf this is something you would like help looking into just let me know!\nI appreciate you donating your time to helping me find a solution to my issue.  Thanks!. ",
    "jordandavis-asurion": "The reason the dnsLookup override was not working is because I placed it inside the redisOptions field instead of at the top level of options.  I also did not previously see that the dnsLookup override is used in the resolveStartupNodeHostnames function within the cluster connection logic, but I can confirm that my override happens.  My code now looks like this:\n```\nconst Redis = require('ioredis');\nconst nodes = [{\n    host: 'clustercfg.xxx.use1.cache.amazonaws.com',\n    port: '6379',\n}];\nconst options = {\n    dnsLookup: (address, callback) => callback(null, address),\n    redisOptions: {\n        tls: {}\n    }\n}\nconst cluster = new Redis.Cluster(nodes, options);\ncluster.set('aws', 'test');\ncluster.get('aws', function (err, res) {\n    console.log(res);\nif (err) {\n    console.error(err)\n}\n\ncluster.disconnect()\n\n});\n```\nWhen I run this code sometimes it connects and runs successfully.  Most of the time, however, I receive debug logs like this:\n```\nioredis:cluster status: [empty] -> connecting +0ms\n  ioredis:cluster resolved hostname clustercfg.xxx.use1.cache.amazonaws.com to IP clustercfg.xxx.use1.cache.amazonaws.com +3ms\n  ioredis:cluster:connectionPool Reset with [ { host:\n  ioredis:cluster:connectionPool      'clustercfg.xxx.use1.cache.amazonaws.com',\n  ioredis:cluster:connectionPool     port: 6379 } ] +0ms\n  ioredis:cluster:connectionPool Connecting to clustercfg.xxx.use1.cache.amazonaws.com:6379 as master +2ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379]: [empty] -> wait +0ms\n  ioredis:cluster getting slot cache from clustercfg.xxx.use1.cache.amazonaws.com:6379 +5ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379]: wait -> connecting +1ms\n  ioredis:redis queue command[clustercfg.xxx.use1.cache.amazonaws.com:6379]: 0 -> cluster([ 'slots' ]) +0ms\n  ioredis:cluster:subscriber selected a subscriber clustercfg.xxx.use1.cache.amazonaws.com:6379 +0ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: [empty] -> wait +1ms\n  ioredis:cluster:subscriber started +0ms\n  ioredis:redis status[aaa.bbb.ccc.ddd:6379]: connecting -> connect +986ms\n  ioredis:redis write command[aaa.bbb.ccc.ddd:6379]: 0 -> info([]) +1ms\n[ioredis] Unhandled error event: ClusterAllFailedError: Failed to refresh slots cache.\n    at tryNode (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:319:31)\n    at /Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:335:21\n    at Timeout.redis.cluster.utils_1.timeout (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:551:24)\n    at Timeout.run (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/utils/index.js:150:22)\n    at ontimeout (timers.js:438:13)\n    at tryOnTimeout (timers.js:300:5)\n    at listOnTimeout (timers.js:263:5)\n    at Timer.processTimers (timers.js:223:10)\n  ioredis:cluster:connectionPool Reset with [] +1s\n  ioredis:cluster:connectionPool Disconnect clustercfg.xxx.use1.cache.amazonaws.com:6379 because the node does not hold any slot +1ms\n  ioredis:redis status[aaa.bbb.ccc.ddd:6379]: connect -> ready +85ms\n  ioredis:connection send 1 commands in offline queue +0ms\n  ioredis:redis queue command[aaa.bbb.ccc.ddd:6379]: 0 -> cluster([ 'slots' ]) +0ms\n  ioredis:redis status[aaa.bbb.ccc.ddd:6379]: ready -> close +2ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +2ms\n  ioredis:redis status[aaa.bbb.ccc.ddd:6379]: close -> end +0ms\n  ioredis:cluster:subscriber subscriber has left, selecting a new one... +1s\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: wait -> close +1ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +1ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: close -> end +0ms\n  ioredis:cluster:subscriber selecting subscriber failed since there is no node discovered in the cluster yet +1ms\n  ioredis:cluster status: connecting -> close +1s\n  ioredis:cluster status: close -> reconnecting +0ms\n  ioredis:cluster connecting failed: Error: None of startup nodes is available +0ms\n  ioredis:cluster Cluster is disconnected. Retrying after 102ms +103ms\n  ioredis:cluster status: reconnecting -> connecting +0ms\n  ioredis:cluster resolved hostname clustercfg.xxx.use1.cache.amazonaws.com to IP clustercfg.xxx.use1.cache.amazonaws.com +0ms\n  ioredis:cluster:connectionPool Reset with [ { host:\n  ioredis:cluster:connectionPool      'clustercfg.xxx.use1.cache.amazonaws.com',\n  ioredis:cluster:connectionPool     port: 6379 } ] +176ms\n  ioredis:cluster:connectionPool Connecting to clustercfg.xxx.use1.cache.amazonaws.com:6379 as master +0ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379]: [empty] -> wait +105ms\n  ioredis:cluster:subscriber a new node is discovered and there is no subscriber, selecting a new one... +105ms\n  ioredis:cluster:subscriber selected a subscriber clustercfg.xxx.use1.cache.amazonaws.com:6379 +0ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: [empty] -> wait +0ms\n  ioredis:cluster getting slot cache from clustercfg.xxx.use1.cache.amazonaws.com:6379 +2ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379]: wait -> connecting +1ms\n  ioredis:redis queue command[clustercfg.xxx.use1.cache.amazonaws.com:6379]: 0 -> cluster([ 'slots' ]) +0ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: wait -> close +0ms\n  ioredis:connection skip reconnecting since the connection is manually closed. +106ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: close -> end +0ms\n  ioredis:cluster:subscriber selected a subscriber clustercfg.xxx.use1.cache.amazonaws.com:6379 +1ms\n  ioredis:redis status[clustercfg.xxx.use1.cache.amazonaws.com:6379 (ioredisClusterSubscriber)]: [empty] -> wait +0ms\n  ioredis:cluster:subscriber started +1ms\n[ioredis] Unhandled error event: ClusterAllFailedError: Failed to refresh slots cache.\n    at tryNode (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:319:31)\n    at /Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:335:21\n    at Timeout.redis.cluster.utils_1.timeout (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/cluster/index.js:551:24)\n    at Timeout.run (/Users/jdavis/workspace/experiments/encrypted-ec-ioredis/node_modules/ioredis/built/utils/index.js:150:22)\n    at ontimeout (timers.js:438:13)\n    at tryOnTimeout (timers.js:300:5)\n    at listOnTimeout (timers.js:263:5)\n    at Timer.processTimers (timers.js:223:10)\n  ioredis:cluster:connectionPool Reset with [] +1s\n  ioredis:cluster:connectionPool Disconnect clustercfg.xxx.use1.cache.amazonaws.com:6379 because the node does not hold any slot +0ms\n  ioredis:connection error: Error: Client network socket disconnected before secure TLS connection was established +1s\nevents.js:167\n      throw er; // Unhandled 'error' event\n      ^\nError\nEmitted 'error' event at:\n    at emitErrorNT (internal/streams/destroy.js:82:8)\n    at process._tickCallback (internal/process/next_tick.js:63:19)\nnpm ERR! code ELIFECYCLE\nnpm ERR! errno 1\nnpm ERR! encrypted-ec-ioredis@1.0.0 start: node index.js\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the encrypted-ec-ioredis@1.0.0 start script.\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /Users/jdavis/.npm/_logs/2018-12-17T03_10_37_935Z-debug.log\n```\nDid you see anything like this?  Is my new code any different from the code you tested with?\nThanks!. ",
    "emadum": "Hi @luin, I came across this ticket while evaluating ioredis for use with Elasticache with cluster mode disabled, and can provide some more details.\nElasticache supports two types of replication, Cluster Mode enabled/disabled. See here\nWith cluster mode disabled, attempting to connect via ioredis's Cluster module yields the unsurprising error of:\nERR This instance has cluster support disabled\nHowever, it would be nice if we could bypass the majority of the cluster-mode code, and just take advantage of the connection management and read-write splitting features.\nThis could potentially be achieved by manually specifying the master/slave roles in the nodes array when initializing the cluster. E.g.,\nconst cluster = new Redis.Cluster(\n[\n  {host: 'r/w primary endpoint', port: 6379, role: 'master'},\n  {host: 'r/o slave endpoint 1', port: 6379, role: 'slave'}\n  {host: 'r/o slave endpoint 2', port: 6379, role: 'slave'}\n],\n{scaleReads: 'slave'}\n);. ",
    "20k-ultra": "I ended up not using a cluster setup until I have more time to research how to do this correctly. I believe the issue is you're suppose to bind your cluster nodes to 127.0.0.1 as mentioned in the issue commented by @jayflo.. ",
    "libook": "Thanks for your time for replying this.\nIt's a big job. If you feel hard, many hands make light work.\nHave a good day!. ",
    "mishushakov": "Hey, @luin thanks for quick reply, I will try it and comment back in the issue in a second. Thank you. Still fails, but now with this error:\n[ioredis] Unhandled error event: Error: getaddrinfo ENOTFOUND redis redis:6379\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:57:26). Ok, then I will try to hack around on my docker machine. Thank you. ",
    "skyzenr": "Hi @Pistacchione ,\nI faced the same problem few days ago.\nTo the best of my knowledge, the SCAN operation iterates over all keys on a single Redis node.\nHence, I think you need to get all nodes first and then use scan on each of them.\nTake a look at #175 or https://paluch.biz/blog/162-iterate-over-all-keys-in-a-redis-cluster.html.\nHope it helps.\nRoberto. ",
    "fossabot": "Your license scan is passing -- congrats!\nYour badge status is now updated and ready to merge:\n. ",
    "WuStardust": "OK, then that makes sense. \nThanks~. > so how to fix this ? @btd\njust use the native promise handler\n```\nprocess.on('unhandledRejection', (reason, p) => {\n  console.log('Unhandled Rejection:', reason, p);\n});\nprocess.on('uncaughtException', (reason, p) => {\n  console.log('uncaught Exception:', reason, p);\n});\n```. ",
    "RifeWang": "so how to fix this ? @btd . ",
    "natesilva": "This issue hasn\u2019t been fixed, stalebot. Please keep it open.. I can\u2019t reproduce the error now. We are using v4.5.1 and I was able to reproduce it recently with a minimal test case, but now I can\u2019t. \ud83e\udd37\ud83c\udffd\u200d\u2642\ufe0f Closing, will reopen if it happens again.. ",
    "nolimitdev": "@natesilva \nI found this bug today and I wanted to fill new issue with title Subscribe in cluster causes Unhandled error event after node is disconnected but I found this very similar issue. I think it is the same. I think this issue should be re-opened. I reproduced this error on ioredis v4.5.1 on linux debian and windows too with identical result so I will demonstrate for example result from windows.\nIt is not possible to avoid \"Unhandled error event\" in combination redis cluster + used subscribe + cluster node or all nodes goes down. There is no bug in non-cluster version but only in cluster version. I will show you results for non-cluster version (single node on port 6379) and cluster version (with 2 nodes on ports 7008 and 7009). I also tested two alternatives \"A\" without subscribe and \"B\" with subscribe.\n\nNon-cluster (OK)\nStart redis single node, then run node non-cluster.js and few seconds later shutdown node...\n```\n// file non-cluster.js\nvar ioredis = require('ioredis');\nvar redis = new ioredis({host: 'localhost', port: 6379});\nredis.on('error', function(error) {\n    console.log('Redis error:', error.message);\n});\nredis.on('ready', function() {\n    console.log('Redis is ready');\n});\n// I will test \"A\" alternative without this subscribe code and \"B\" alternative with it\nredis.subscribe('item', function(error, count) {\n    if (error)\n        console.log('Subscribe error:', error.message);\n    else\n        console.log('Subscribe count:', count);\n});\n```\n// A - without subscribe:\n\nRedis is ready\nRedis error: read ECONNRESET\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\n...\n\n// B - with subscribe:\n\nRedis is ready\nSubscribe count: 1\nRedis error: read ECONNRESET\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\nRedis error: connect ECONNREFUSED 127.0.0.1:6379\n...\n\nThere is no \"Unhandled error event\" in non-cluser version for both alternatives.\n\nCluster (BUG)\nStart redis cluster, then run node cluster.js and few seconds later shutdown both cluster nodes...\n```\n// file cluster.js\nvar ioredis = require('ioredis');\nvar nodes = [{host: 'localhost', port: 7001}, {host: 'localhost', port: 7002}];\nvar redis = new ioredis.Cluster(nodes);\nredis.on('error', function(error) {\n    console.log('Redis cluster error:', error.message);\n});\nredis.on('ready', function() {\n    console.log('Redis cluster is ready');\n});\nredis.on('node error', function(error) {\n    console.log('Redis cluster node error:', error.message);\n});\n// I will test \"A\" alternative without this subscribe code and \"B\" alternative with it\nredis.subscribe('item', function(error, count) {\n    if (error)\n        console.log('Subscribe error:', error.message);\n    else\n        console.log('Subscribe count:', count);\n});\n```\n// A - without subscribe:\n\nRedis cluster is ready\nRedis cluster node error: timeout\nRedis cluster node error: timeout\nRedis cluster node error: timeout\nRedis cluster node error: Connection is closed.\nRedis cluster error: Failed to refresh slots cache.\nRedis cluster node error: timeout\nRedis cluster node error: timeout\nRedis cluster error: Failed to refresh slots cache.\nRedis cluster node error: timeout\nRedis cluster node error: timeout\nRedis cluster error: Failed to refresh slots cache.\nRedis cluster node error: timeout\nRedis cluster node error: timeout\n...\n\nNo \"Unhandled error event\" in cluster version without subscribe.\n// B - with subscribe:\n\nRedis cluster is ready\nSubscribe count: 1\nRedis cluster node error: timeout\nRedis cluster node error: timeout\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7001\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7001\n   at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\nRedis cluster node error: timeout\nRedis cluster error: Failed to refresh slots cache.\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7001\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7001\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\nRedis cluster node error: timeout\n...\n\nThere is \"Unhandled error event\" in cluster version with subscribe alternative although all errors all correctly handled including that in subscribe callback. I think that it is unhandled by mistake somewhere in ioredis lib not by lib consumer. I also tried to listen for all cluster events but there is always this unhandled error.\n. Maybe it will be usefull - similar Unhandled error events from linux - debian:\n\nRedis cluster is ready\nSubscribe count: 1\n[ioredis] Unhandled error event: Error: write EPIPE\n    at exports._errnoException (util.js:1050:11)\n    at WriteWrap.afterWrite [as oncomplete] (net.js:814:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7009\n    at Object.exports._errnoException (util.js:1050:11)\n    at exports._exceptionWithHostPort (util.js:1073:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1093:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7009\n    at Object.exports._errnoException (util.js:1050:11)\n    at exports._exceptionWithHostPort (util.js:1073:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1093:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7009\n    at Object.exports._errnoException (util.js:1050:11)\n    at exports._exceptionWithHostPort (util.js:1073:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1093:14)\n[ioredis] Unhandled error event: Error: connect ECONNREFUSED 127.0.0.1:7009\n    at Object.exports._errnoException (util.js:1050:11)\n    at exports._exceptionWithHostPort (util.js:1073:20)\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1093:14)\n.... \n",
    "darweshsinghcgi": "This is an issue with a lack of process.env availability for the webapp, and not with ioredis. Closing.. ",
    "lvgithub": "get  commands  pending ,wait forever\u3002 not catch any error info\nioredis version: 3.2.2\nnode version: v10.3.0\n```\nconst Redis = require('ioredis');\nconst redis = new Redis({\n    axRetriesPerRequest: 10\n});\n// ,\nredis.get('1')\n    .then(data => console.log(data))\n    .catch(err => console.log(111111111111111111, err))\nredis.on('error', err => console.log())\n```. retryStrategy option is sucess, but i wan't to call redis.connect() manually.\ni hope can throw error,when maxRetriesPerRequest ,and will auto reconnect \u3002\n\n\n.  THANKS!. ",
    "akaNightmare": "@luin going to create a PR. @luin also I can do that ES6 Proxy\n```js\nconst Redis = require('ioredis');\nconst redis = new Proxy(new Redis(), {\n  get(target, propKey) {\n    return function(...args) {\n      const context = args.pop();\n      console.info(redis ${propKey}, { action: propKey, context });\n      return targetpropKey;\n    };\n  },\n});\n(async () => {\n  console.log(await redis.get('124', { user: 1 }));\n  process.exit(0);\n})();\n``\nhowever, in your example I need pass some last argument (context), could you please is it ok or not?. it can be replaced withif (!isNaN(retryDelay) && retryDelay >= 0) {`. resolved. resolved. ",
    "jacoor": "Thanks, yes, that will do.\nW dniu sob., 19.01.2019 o 05:58 \u5b50\u9a85 notifications@github.com napisa\u0142(a):\n\n@luin commented on this pull request.\nIn lib/cluster/index.ts\nhttps://github.com/luin/ioredis/pull/779#discussion_r249234534:\n\n@@ -212,7 +212,7 @@ class Cluster extends EventEmitter {\n     if (!this.manuallyClosing && typeof this.options.clusterRetryStrategy === 'function') {\n       retryDelay = this.options.clusterRetryStrategy.call(this, ++this.retryAttempts, reason)\n     }\n-    if (typeof retryDelay === 'number') {\n+    if (typeof retryDelay === 'number' && retryDelay !== NaN) {\n\nSorry for the late response. @types/ioredis module has been updated to\nsupport returning a null to stop reconnection, thus I think this pull\nrequest can be closed.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/luin/ioredis/pull/779#discussion_r249234534, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA8YSGYjZZ8vso0qPrTwDA-XcfA4YXupks5vEqXtgaJpZM4Z6zMs\n.\n-- \nJacek Osta\u0144ski\nInternet Evolution - Strony i serwisy Internetowe\n608-230-735\n. well this could also be solved with simple retryDelay >= 0.  Now negative number just keeps retrying..                 clusterRetryStrategy: (times) => {\n                    if (times > 3) { // stop every 3 times, retry next time.\n                        return;\n                    }\n                    return Math.min(100 + times * 2, 1000);\n                }\n\nwith this retry strategy and strict ts_config I get following error:\nArgument of type '{ clusterRetryStrategy: (times: number) => number | undefined; red\nisOptions: { maxRetriesPerRequest: number; connectTimeout: number; lazyConnect: false; reconnectOnError: (err: Error) =\n\nfalse | 2; }; scaleReads: string; }' is not assignable to parameter of type 'ClusterOptions'.\n  Types of property 'clusterRetryStrategy' are incompatible.\n    Type '(times: number) => number | undefined' is not assignable to type '(times: number) => number'.\n      Type 'number | undefined' is not assignable to type 'number'.\n        Type 'undefined' is not assignable to type 'number'.\n\nWith the change I could simply put NaN and be done with it.. ",
    "privateOmega": "@luin Thanks for the important information. I got the commands back and is being executed on same db, but I found out that there's a word joiner unicode character \\xe2\\x81\\xa0 present in the strings which are actually present in the db. But there's no extra unicode character in my string before I pass it to zrevrange().. @luin do you know what could be the reason for it?. @luin yes ofcourse, after identifying that an extra unicode character is present, I got the idea that it had nothing to do with neither redis client nor redis itself. I was asking that doubt in general.. ",
    "calummoore": "Incredible \ud83e\udd2a\ud83e\udd29! Thanks for resolving this so quickly @luin @tuananh. It's fixed the issue for me.. ",
    "apmcodes": "Oops ... all the commands results are returned as an result array of the format [[error, result],[error, result]] ... . ",
    "r3b-fish": "It's possible solution but I think instantiation of subscriber should be more elegant\ndiff\ndiff --git a/lib/cluster/ClusterSubscriber.ts b/lib/cluster/ClusterSubscriber.ts\nindex 11c1e0b..c176bec 100644\n--- a/lib/cluster/ClusterSubscriber.ts\n+++ b/lib/cluster/ClusterSubscriber.ts\n@@ -71,7 +71,8 @@ export default class ClusterSubscriber {\n       password: options.password,\n       enableReadyCheck: true,\n       connectionName: SUBSCRIBER_CONNECTION_NAME,\n-      lazyConnect: true\n+      lazyConnect: true,\n+      tls: options.tls\n     }). ",
    "sawan-rana": "@luin Yes, result of sentinel sentinels mymaster  gives 127.0.0.1. @luin Thanks!. ",
    "worotyns": ":+1: . ",
    "paulmelliere": "I created a branch with some failing tests demonstrating the issue: https://github.com/luin/ioredis/compare/master...paulmelliere:test/commandQueue-after-reconnect\nAs a workaround, we've disabled autoResendUnfulfilledCommands for now.. ",
    "yiwong2001": "Do you support scan, llen methods for cluster?. Hi @luin ,\nCan I bother for one question?  \nIt seems scan cmd can only return keys from one shard in cluster environment.  How can I get all keys in redis cluster?\nIn my test, I can see that the keys are on different shard nodes, when I query the primary endpoint it is returning the value from one node and not the other.\nDo you have some good idea how to handle it?  \nThanks.. Hi Luin,\nthank you for your reply.\nIs it same way for shards? \nI tried to configure each of shard urls instead of the primary endpoint. It seems I can get results by scan from different shards (can not if I use primary nodes), but I can not get results by mget with returned keys.\nvar nodes = [\n{\nhost: 'test4abc-0001-001.jwkwktaab.0001.euw1.cache.amazonaws.com',\nport: 6379,\n},\n{\nhost: 'test4abc-0002-001.jwkwktaab.0001.euw1.cache.amazonaws.com',\nport: 6379,\n}\n];\n//nodes = [{host:'test4abc.jwkwktaab.clustercfg.euw1.cache.amazonaws.com',port:6379}];. ",
    "iatsiuk": "Thank you for your detailed explanation.. ",
    "adamhathcock": "This PR broke you like it did me: https://github.com/DefinitelyTyped/DefinitelyTyped/pull/33315\nPin to 4.0.8 instead of updating to 4.0.9. This still isn't good enough. The Cluster interface in 4.0.8 has everything the non-Cluster interface has.  I'm using del and exec and multi stuff on both.\nThe type must have shared a real interface. Should I submit a solution? Also, this churn in interface should warrant semantic version changes more extreme than a patch.. ",
    "nickroberts": "I created a new PR to add the get / set methods: DefinitelyTyped/DefinitelyTyped#33655. ",
    "amelialaundy": "Awesome thanks so much @adamhathcock I obviously missed that when looking at the PR diffs. \nThanks @nickroberts for quick fix!. ",
    "Eywek": "We were using multi before the requirement of using Redis Cluster, and sometimes we send a large amount of commands to the redis instance so multi is useful.\nSo, if I use a command like zadd with multiple keys (not in the same slot), ioredis will distribute commands to the correct node for each key?. ",
    "joshhopkins": "Thanks for the quick response. Revisiting the logs and it turns out that another service was trying to connect and empty an offline queue, while being pointed to the wrong env variables. If the correct password was more complex and took a little longer to authorise, it was just enough time for the other client to attempt to connect and throw the error (I guess). My bad!. ",
    "ynomura-md": "I will cancel this PR once.\nI write the test and put it out again.. I'm sorry. It is typo. fixed.. ",
    "shults": "Thanks for reply. Fixed.. "
}
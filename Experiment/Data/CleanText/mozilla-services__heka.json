{
    "crankycoder": "i think i've covered all the comments on the pull request - can you take another look at this?\n. thanks for the feedback rob!\n. this code fell out of date\n. This seems to conflict with issue #231 which requests to update a command line argument on the man page.  Do we want command line arguments?\n. Do we still need this since the AMQPOutput and AMQPInput have landed?\n. https://github.com/mozilla-services/heka/pull/206\n. Merged with https://github.com/mozilla-services/heka/pull/199\n. There's something subtly different between what's in the hg repo for Go 1.1.1 and what's in the prepackaged release.  \nA quick check of the file contents shows some differences between the full hg repository and the release package. \nBumping the hg revision in our Makefile to the latest go 1.1.1 release @ a7bd9a33067b works just fine, but when I drop the release version of go1.1.1 in place into build/go, I get the following errors:\n```\n2013/06/21 23:43:23 Plugin 'stoppingOutput' error: Max retries exceeded\n2013/06/21 23:43:23 Plugin 'panic' error: PANIC: runtime error: invalid memory address or nil pointer dereference\nSIGSEGV: segmentation violation\nPC=0x0\nsignal arrived during cgo execution\ngithub.com/mozilla-services/heka/sandbox/lua._Cfunc_lua_sandbox_init(0x119000c0, 0x11900030, 0x11900030)\n        github.com/mozilla-services/heka/sandbox/lua/_obj/_cgo_defun.c:73 +0x2f\ngithub.com/mozilla-services/heka/sandbox/lua.(LuaSandbox).Init(0xc2026cc630, 0x3db520, 0x0, 0x0, 0x0, ...)\n        github.com/mozilla-services/heka/sandbox/lua/_obj/lua_sandbox.cgo1.go:182 +0x90\ngithub.com/mozilla-services/heka/pipeline.(SandboxFilter).Init(0xc2026df2a0, 0x2b8260, 0xc20165ebe0, 0x0, 0x0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/sandbox_filter.go:83 +0x25a\ngithub.com/mozilla-services/heka/pipeline.func\u00b7105()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/filters_test.go:84 +0x5b6\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc20165ec80, 0x0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x62\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc2026df310)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc2000ecf60, 0xc2026df310)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x4b\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc2000ecf60)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc2000ecf60, 0x456210, 0x28, 0xc20165ec80)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x51\ngithub.com/mozilla-services/heka/pipeline.func\u00b7108()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/filters_test.go:89 +0x319\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc2026cc4e0, 0x0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x62\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc2026df230)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc2000ecf60, 0xc2026df230)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x4b\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc2000ecf60)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc2000ecf60, 0x3f0b30, 0xf, 0xc2026cc4e0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x51\ngithub.com/mozilla-services/heka/pipeline.FiltersSpec(0xc2000ff940, 0xc2000ecf60)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/filters_test.go:111 +0x1fd\ngithub.com/rafrombrc/gospec/src/gospec.func\u00b7009()\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 +0x57\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc2026de360, 0x0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x62\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc2026df1c0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc2000ecf60, 0xc2026df1c0)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x4b\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc2000ecf60)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc2000ecf60, 0x4f0ca4, 0x35, 0xc2026de360)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x51\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).execute(0xc2000dd730, 0x4f0ca4, 0x35, 0x47d340, 0xc2000ecf60, ...)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 +0xac\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).startNextScheduledTask(0xc2000dd730)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/runner.go:82 +0x134\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).startAllScheduledTasks(0xc2000dd730)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/runner.go:55 +0x37\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).Run(0xc2000dd730)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/runner.go:47 +0x25\ngithub.com/rafrombrc/gospec/src/gospec.runAndPrint(0xc2000dd730, 0x47d370)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/main.go:54 +0x13f\ngithub.com/rafrombrc/gospec/src/gospec.MainGoTest(0xc2000dd730, 0xc2000f6360)\n        /Users/victorng/dev/heka-build/src/github.com/rafrombrc/gospec/src/gospec/main.go:39 +0x25\ngithub.com/mozilla-services/heka/pipeline.TestAllSpecs(0xc2000f6360)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/all_specs_test.go:56 +0x22d\ntesting.tRunner(0xc2000f6360, 0x67dfa0)\n        /usr/local/go/src/pkg/testing/testing.go:353 +0x8a\ncreated by testing.RunTests\n        /usr/local/go/src/pkg/testing/testing.go:433 +0x86b\ngoroutine 1 [chan receive]:\ntesting.RunTests(0x47d578, 0x67dfa0, 0x1, 0x1, 0xc2000c5e01, ...)\n        /usr/local/go/src/pkg/testing/testing.go:434 +0x88e\ntesting.Main(0x47d578, 0x67dfa0, 0x1, 0x1, 0x681d60, ...)\n        /usr/local/go/src/pkg/testing/testing.go:365 +0x8a\nmain.main()\n        github.com/mozilla-services/heka/pipeline/_test/_testmain.go:51 +0x9a\ngoroutine 2 [syscall]:\ngoroutine 4 [syscall]:\nos/signal.loop()\n        /usr/local/go/src/pkg/os/signal/signal_unix.go:21 +0x1c\ncreated by os/signal.init\u00b71\n        /usr/local/go/src/pkg/os/signal/signal_unix.go:27 +0x2f\ngoroutine 11 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc2000e0d00)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 12 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*LogfileInput).Run(0xc20018a900, 0xc2001234d0, 0xc200189e10, 0xc2000ddaa0, 0xc200189e00, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:142 +0x4c3\ngithub.com/mozilla-services/heka/pipeline.func\u00b7146()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input_test.go:109 +0xa5\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7147\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input_test.go:111 +0xc0e\ngoroutine 13 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc2000e0dd0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 14 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc200212000)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 15 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*LogfileInput).Run(0xc200136bc0, 0xc2001234d0, 0xc2001897c0, 0xc2000ddaa0, 0xc2001897b0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:142 +0x4c3\ngithub.com/mozilla-services/heka/pipeline.func\u00b7098()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/filemonitor_test.go:109 +0xa5\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7099\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/filemonitor_test.go:111 +0xc1d\ngoroutine 16 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc2002120d0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 17 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc200212270)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 18 [chan send]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).ReadLines(0xc200212340, 0x42ec10, 0x1c, 0x7dff01)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:408 +0x2b3\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc200212340)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:309 +0x112\ncreated by github.com/mozilla-services/heka/pipeline.(*FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 19 [select]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc200212410)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:304 +0x1de\ncreated by github.com/mozilla-services/heka/pipeline.(FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 20 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*UdpInput).Run(0xc20087e390, 0xc2001234d0, 0xc200835a60, 0xc2000ddaa0, 0xc200835a50, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:244 +0x167\ngithub.com/mozilla-services/heka/pipeline.func\u00b7114()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:165 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7115\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:166 +0x1f6\ngoroutine 21 [finalizer wait]:\ngoroutine 24 [chan send]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).ReadLines(0xc2002128f0, 0x42ec10, 0x1c, 0x7dbf01)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:408 +0x2b3\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc2002128f0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:309 +0x112\ncreated by github.com/mozilla-services/heka/pipeline.(*FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 23 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*LogfileInput).Run(0xc2008942c0, 0xc2001234d0, 0xc2007e81d0, 0xc2000ddaa0, 0xc2007e81c0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:142 +0x4c3\ngithub.com/mozilla-services/heka/pipeline.func\u00b7142()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:472 +0xa5\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7143\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:474 +0x722\ngoroutine 25 [chan send]:\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).ReadLines(0xc2002129c0, 0x42ec10, 0x1c, 0x7daf01)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:408 +0x2b3\ngithub.com/mozilla-services/heka/pipeline.(FileMonitor).Watcher(0xc2002129c0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:309 +0x112\ncreated by github.com/mozilla-services/heka/pipeline.(*FileMonitor).Init\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/logfile_input.go:495 +0x1f4\ngoroutine 29 [semacquire]:\nsync.runtime_Semacquire(0xc2000003a8)\n        /usr/local/go/src/pkg/runtime/zsema_darwin_amd64.c:165 +0x2e\nsync.(WaitGroup).Wait(0xc200108f70)\n        /usr/local/go/src/pkg/sync/waitgroup.go:109 +0xf2\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).Run(0xc200108f50, 0xc2001234d0, 0xc200c93550, 0xc2000ddaa0, 0xc200c93540, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:505 +0x1fd\ngithub.com/mozilla-services/heka/pipeline.func\u00b7123()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:259 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7124\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:260 +0x221\ngoroutine 28 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7037()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:71 +0x5f\ncreated by github.com/mozilla-services/heka/pipeline.(*GlobalConfigStruct).ShutDown\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:72 +0x64\ngoroutine 30 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).handleConnection(0xc200108f50, 0xc200821f00, 0xc200c93730)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:424 +0x33c\ncreated by github.com/mozilla-services/heka/pipeline.(TcpInput).Run\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:503 +0x250\ngoroutine 31 [semacquire]:\nsync.runtime_Semacquire(0xc200000db8)\n        /usr/local/go/src/pkg/runtime/zsema_darwin_amd64.c:165 +0x2e\nsync.(WaitGroup).Wait(0xc2015d6250)\n        /usr/local/go/src/pkg/sync/waitgroup.go:109 +0xf2\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).Run(0xc2015d6230, 0xc2001234d0, 0xc200c93aa0, 0xc2000ddaa0, 0xc200c93a90, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:505 +0x1fd\ngithub.com/mozilla-services/heka/pipeline.func\u00b7134()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:371 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7136\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:372 +0x4f8\ngoroutine 34 [semacquire]:\nsync.runtime_Semacquire(0xc200000a88)\n        /usr/local/go/src/pkg/runtime/zsema_darwin_amd64.c:165 +0x2e\nsync.(WaitGroup).Wait(0xc2015d6950)\n        /usr/local/go/src/pkg/sync/waitgroup.go:109 +0xf2\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).Run(0xc2015d6930, 0xc2001234d0, 0xc200c93fb0, 0xc2000ddaa0, 0xc200c93fa0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:505 +0x1fd\ngithub.com/mozilla-services/heka/pipeline.func\u00b7131()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:342 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7133\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:343 +0x49e\ngoroutine 33 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).handleConnection(0xc2015d6230, 0xc200821f00, 0xc200c93c80)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:424 +0x33c\ncreated by github.com/mozilla-services/heka/pipeline.(TcpInput).Run\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:503 +0x250\ngoroutine 37 [semacquire]:\nsync.runtime_Semacquire(0xc2008b72f8)\n        /usr/local/go/src/pkg/runtime/zsema_darwin_amd64.c:165 +0x2e\nsync.(WaitGroup).Wait(0xc2008d2bf0)\n        /usr/local/go/src/pkg/sync/waitgroup.go:109 +0xf2\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).Run(0xc2008d2bd0, 0xc2001234d0, 0xc200c938f0, 0xc2000ddaa0, 0xc200c938e0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:505 +0x1fd\ngithub.com/mozilla-services/heka/pipeline.func\u00b7128()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:311 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7130\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:312 +0x48f\ngoroutine 36 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).handleConnection(0xc2015d6930, 0xc200821f00, 0xc200c933c0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:424 +0x33c\ncreated by github.com/mozilla-services/heka/pipeline.(TcpInput).Run\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:503 +0x250\ngoroutine 38 [chan send]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7129()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:317 +0x4c\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7130\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:318 +0x530\ngoroutine 39 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).handleConnection(0xc2008d2bd0, 0xc200821f00, 0xc200c93cb0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:424 +0x33c\ncreated by github.com/mozilla-services/heka/pipeline.(TcpInput).Run\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:503 +0x250\ngoroutine 40 [semacquire]:\nsync.runtime_Semacquire(0xc2008b75a0)\n        /usr/local/go/src/pkg/runtime/zsema_darwin_amd64.c:165 +0x2e\nsync.(WaitGroup).Wait(0xc2000f0560)\n        /usr/local/go/src/pkg/sync/waitgroup.go:109 +0xf2\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).Run(0xc2000f0540, 0xc2001234d0, 0xc2008d33f0, 0xc2000ddaa0, 0xc2008d33e0, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:505 +0x1fd\ngithub.com/mozilla-services/heka/pipeline.func\u00b7125()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:280 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7127\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:281 +0x48f\ngoroutine 41 [chan send]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7126()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:286 +0x4c\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7127\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:287 +0x530\ngoroutine 42 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(TcpInput).handleConnection(0xc2000f0540, 0xc200821f00, 0xc2008d35d0)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:424 +0x33c\ncreated by github.com/mozilla-services/heka/pipeline.(TcpInput).Run\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:503 +0x250\ngoroutine 43 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*UdpInput).Run(0xc20230d360, 0xc2001234d0, 0xc2008d3a20, 0xc2000ddaa0, 0xc2008d3a10, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:244 +0x167\ngithub.com/mozilla-services/heka/pipeline.func\u00b7118()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:202 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7120\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:203 +0x200\ngoroutine 45 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*UdpInput).Run(0xc2016a6d50, 0xc2001234d0, 0xc200c93190, 0xc2000ddaa0, 0xc200c93180, ...)\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs.go:244 +0x167\ngithub.com/mozilla-services/heka/pipeline.func\u00b7116()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:186 +0x88\ncreated by github.com/mozilla-services/heka/pipeline.func\u00b7117\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/inputs_test.go:187 +0x492\ngoroutine 57 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 56 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 58 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 59 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 60 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 61 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 62 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 63 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 64 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 65 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 66 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 67 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 68 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 69 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 70 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 71 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 72 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 73 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 74 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 75 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 76 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 77 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 78 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 79 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 80 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 81 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 82 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 83 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 84 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 85 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 86 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 87 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 88 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 89 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 90 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 91 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 92 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 93 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 94 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 95 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7032()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:160 +0x15d\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/decoders.go:171 +0xb8\ngoroutine 98 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7037()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:71 +0x5f\ncreated by github.com/mozilla-services/heka/pipeline.(*GlobalConfigStruct).ShutDown\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:72 +0x64\ngoroutine 100 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7037()\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:71 +0x5f\ncreated by github.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown\n        /Users/victorng/dev/heka-build/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:72 +0x64\nrax     0x262db9\nrbx     0x11900030\nrcx     0x7c07c8\nrdx     0x7c0760\nrdi     0x264c29\nrsi     0x119000c0\nrbp     0xb028ce20\nrsp     0xb028cdf8\nr8      0x4\nr9      0xb4e1e792\nr10     0x0\nr11     0x1\nr12     0x3b3b80\nr13     0x94e5c9c927e8\nr14     0x119000c0\nr15     0x7ffffffe6c9eae1b\nrip     0x0\nrflags  0x10206\ncs      0x2b\nfs      0x0\ngs      0x0\nFAIL    github.com/mozilla-services/heka/pipeline       0.627s\nmake: ** [test] Error 1\nvictorng at Victors-MacBook-Air in ~/dev/heka-build/master ! ()\n[2] $                                                                \n```\n. This only seems to be occurring on OSX.  It might be related to: https://code.google.com/p/go/issues/detail?id=5726\n. I've confirmed that this issue on OSX is due to https://code.google.com/p/go/issues/detail?id=5726 .  \nIf you patch heka-build's Makefile like this: https://gist.github.com/crankycoder/5859517 you will be able to reproduce the error using the mercurial checkout.\n. Superceded by Pull Request #317.\n. Docs are updated now.  I didn't see any other occurances of LogFiles anywhere.  I think this should be ok.\n. I've move the max_message_loops into the configuration file under the [hekad] section now.  The -help will now only show -config and -version options.\n. I've applied changes as per your review, but I'm running out of file handles when I run the testsuite, I'm not quite sure what's going on - it's seems like an interaction between pipeline/inputs_test and pipeline/filemonitor_test. \nIf I temporarily bump the file handle limit up with \"ulimit -n 1000\", then the tests run fine.\nI've checked for any os.Open and os.OpenFile calls with unmatched Close() calls, but the only one I see is FileMonitor::OpenFile which closes files on the stopchan signal.\nBecause of that - I think the problem is that I've moved the call of \"go fm.Watcher()\" from the FileMonitor::Init to  FileMonitor::Run, but I don't see where in the inputs_test the stopchan gets closed.\n. I've added an explicit close on the stopchan in the test for inputs_test for reading logfiles which seems to fix the problem - mostly. I'm still getting intermittent failures in the test suite - but this time in the pipeline/decoders_test which doesn't make sense to me.\nAs usual, bumping up the filehandle limit seems to workaround the issue. \n. The last changeset in this patch relies on https://github.com/rafrombrc/whisper-go/pull/1 being merged into whisper-go.  It seems to resolve the problem of running out of file handles.\nlsof doesn't show me using any more than 21 filehandles using the script below when running the full test suite.\n[1] $ while true                                                                                                                                           \ndo\nLSOF=lsof -c goif [ -n \"$LSOF\" ]; then\necho \"==============================\"\necho $LSOF | wc\nfi\ndone\n. I've reworked the handling of SIGUSR1 and the ticker code so that the ticker will only emit to dashboard and SIGUSR1 only emits reports to stdout now.\nCommand line options have been reverted back after review comments from @rafrombrc \n. @rafrombrc i think that's it\n. @rafrombrc, I've reworked this patch to handle EOF a little better so that the last incomplete log line is passed along if the second Stat() triggers an error or if logfile rotation has been detected.\nhttps://github.com/mozilla-services/heka/compare/44c54d0ebe53f904350d02d825b430143724bff5...features;issue_246#L1R416\nThe check for logfile rotation has been moved back below the ReadString() loop as well.\n. @chamaken i finally see what you're trying to do.  \n@rafrombrc  - I've added a comment here https://github.com/mozilla-services/heka/blob/945f0ddee1ec2993dda09147984e898648ae0e56/pipeline/logfile_input.go#L411 which makes things clearer why this will handle file rotation better.  If there is any partial line data and the file has been rotated, data will still be pushed down into the NewLine channel\n. i'm seeing some odd behavior while testing where logfiles are sometimes re-read.  closing until i can resolve the issue.\n. I'll add those extra options in the sample [hekad] section.  I also noticed I forgot to move the command line arguments from the man page section of the documentation into a global configuration section.\n. Closing this while I rewrite a minimal subset of XPath instead.\n. whoops. meant to close the pull request.\n. i'm going to close this as there's quite a bit or jsonpath syntax that i don't handle.\n. @rafrombrc When the JSONPath resolves to a slice or a map type, I just serialize back to a JSON string now.  Until yesterday, it would've blown up.\nAs for keeping message_fields, I'm in favor of doing something better than what we have now, I'm just not sure what that thing is going to look like.\nI think I've hit all the comments on the code review for now though.\n. I'm going to pull in the XML Decoder into this branch as I'm consolidating the Timestamp and Severity sections for the Regex, Json and XML decoders.\n. nevermind - i'll do the consolidation work in the xmldecoder branch.\n. @rafrombrc the helper has been renamed and docstrings are added now\n. syslog outputs vary for each *nix pretty widely.  I'm porting the grok patterns for syslog in logstash over using LPeg.  \nI think if people need to customize their syslog decoding more - this offers them a bit more flexibility rather than having to recompile heka.\n. I'm closing this as https://github.com/mozilla-services/heka/pull/468 supercedes it.\n. @rafrombrc I've fixed the race conditions now.  I think I've covered all the other nits in the review. \nThat said - the race detector doesn't show any problems anymore in the ProcessInput, but I am showing races in other parts of the pipeline.  I'll open a separate ticket to take a look at those.\n. This should clean up the intermittent timeout errors in pipeline/process_chain_test and make the tests run faster.\nI've decreased all the timeouts to use milliseconds instead of full seconds for the speedups.\nThere was an intermittent failure terminating a process before the timeout was reached.  I've mitigated that by increasing the timeout to a full 30 seconds as there is a large variance in the amount of time needed to kill the process.  On my laptop, I get between 5-8 second runtimes for the pipeline tests.\n. @rafrombrc this should be ok now. I've added docs that clarify how to bind the syslog_decoder with LogfileInput. I tried wiring up the decoder with UdpInput and had the MsgBytes vs Payload problem first hand. :(\n. We're going to wait until #494 lands so that we can properly handle UDP syslog messages.\n. Closing this while I'm adjusting the patch to play nice with the new sandbox build.\n. That would be fine by me. :)\n. The server over at http://www.inf.puc-rio.br seems to be back up now, but so the commit in c984742 may not be necessary.  That said, how reliant do we want to be for library code that resides on other people's servers?\n. @trink can you take another look?  I've resolved the problems you listed in the review. thanks!\n. I actually didn't think of this as an input - I thought of it as a thing that configured ProcessInput plugins. :)\nLet me rework this into a ProcessDirectoryInput.\n. I've added an extra struct to capture status and error messages, and a goroutine to monitor a channel for those messages now.\nhttps://github.com/mozilla-services/heka/blob/a52f49cc7c68c6725ff08ccf7f21d9088b34b47e/pipeline/logfile_input.go#L147\n. Unfortunately, I can't because the recovery happens during Init().  At that point, I don't actually have an InputRunner yet as Run() hasn't been called. \n. I misread the code and thought hostname was in the docs incorrectly.  I've readded it.\n. I'll add /var/run/hekad/seekjournals/logfile_inputname.log as the default, if the path doesn't exist, I'll just get heka to bomb out.\n. thanks!\n. i've removed the duplicate test now\n. doh.\n. This format looks reasonable - I was thinking that people might paste into something like Excel, but if it's used in real-time, this seems much nicer.\n. go is magic. :)  \nFiles ending in a recognized OS will be conditionally compiled automatically.\n. I completely missed this.  Sorry!  I've added some reflection code to capture the struct fields and TOML tags if available now.\n. Nice.  I've modified the WhisperOutput with this optimisation as well.\n. You're right.  The plaintext protocol allows multiple lines coming in as long as each has a \\n attached to the end (https://github.com/graphite-project/carbon/blob/master/lib/carbon/protocols.py#L69)\nI've updated the code to handle multi-line inputs and use a single TCP connection to send all the lines out at once.\n. The kludgey wait came back when I introduced the local copy of the ticker from https://github.com/mozilla-services/heka/pull/278#discussion_r4849186\nBesides that, the ticker_interval now works as expected and the default of 10 seconds is back.\n. Clever.  :)\n. these aren't actually test files.  :)  The dashboard writes out these exact files later on.\n. oops. thanks.\n. good idea, i've added it in now.\n. That seems reasonable.  I've got bits of #309 implemented now, so I'll just remove this default value.\n. Had to play with this one a bit.  Turns out go wants: path.Join(string(os.PathSeparator), \"var\", \"run\", \"hekad\", \"whisper\"). \n. You're right, i can just close the stopChan for both the monitor and the input at the same time.  Changing this now.\n. oops. thanks\n. Ah, I missed this, sorry.  I keep thinking defer runs when the variable goes out of scope which is wrong. thanks.\n. yep.  pulling all of this out as i need it for the PayloadXmlDecoder as well\n. This definitely looks like a bug after looking at the docs and tests.\n. Good catch, thanks. \n. Missed removing that while debugging. \n. Hmm.. I think I'm ok as the reset() is only called by ProcessInput in the top level RunCmd() function.  I think a race condition could happen if reset() was called between CommandChain::Start() and CommandChain::Wait() though.  In anycase, I'll do a run through the race-detector.\n. no_error isn't even needed.  I'm just going to exit the goroutine entirely after pushing the error onto the CommandChain's done channel.\n. I've added some information about which command step failed now.\n. I've dropped the unused error code now.\n. I'm adding the binary name and the command line args to each of these kinds of errors.  That ought to clear things up.\n. ..and I'm wrong. There's a race condition here.  Yay for tools smarter than me.\n. Good point, I've gotten rid of the Init() for both the ManagedCmd and CommandChain now.\n. haha.  oops.\n. dammit!  i knew i forgot something.  coming up.  :)\n. ",
    "bbangert": "Looks good to me.\n. Looks fine.\n. We now have a runner abstraction that implements this pattern.\n. Ok, that should do the trick, tested with qualified names and it all looks good now. Thanks!\n. Implemented.\n. This actually wasn't feasible given the way godoc works and is intended for library docs, not runtime docs.\n. Implements Issue #28 \n. Looks good, added one note on a bit that's not needed.\n. CI is now setup here: https://ci.mozilla.org/view/Services/job/Hekad/\nHowever, builds are broken until ops installs cmake per bug:\nhttps://bugzilla.mozilla.org/show_bug.cgi?id=859448\n. We have builds, and they work!\n. Looks fine.\n. I've committed a fix that substitutes an absolute path during the build process which fixes this.\n. Transform Filter for logfile lines needs to occur in a decoder to avoid slowing down the router during its regex match. Does this mean we remove TransformFilter and replace with a decoder? If so, how does one associate log lines with the appropriate decoder? Should tests be written for the filter when it might be changed to a decoder?\n. This has been resolved by the plugin restarting feature.\n. Nice catch, I've updated the docs!\n. This is actually pretty complex because formatting an arbitrary message appropriately to send to CloudWatch requires quite a bit of config, perhaps even writing a short lua script on how to create such a message. The input on the other hand is a bit simpler.\nCan you indicate how you would expect to format a message into a Cloudwatch PutMetric call? What kind of message and from what source would be collected? Would you want to collect a bunch of metrics and use a single PutMetric call?\n. This has now been added to heka-mozsvc-plugins.\n. It's in heka-mozsvc-plugins docs, which are separate. I'm actually not sure where they're built at the moment.\n. There doesn't seem to be any NagiosOutput tests?\n. Looks fine.\n. We'll need tests and documentation updates before we can accept the pull request.\n. Looks good, now it just needs tests and documentation!\n. Oh, one other thing that occured to me, it might be useful to have a decoders option that will transform the message into a structured heka message, take a look at the logfile input and amqp input for an example of a decoders option and how the message would be altered.\n. Looks good.\n. Looks good.\n. Why was this re-opened?\n. Ah, ok, so LogfileInput is specifically for logfiles in the context that logfiles are ascii and regex parseable. As this issue is about LogfileInput, I'm closing it, I've opened #363 for the FileInput.\n. Rename it to FileInput to match FileOutput?\n. Other than that one if difference, it looks good to me.\n. r+\n. Looks good, r+.\n. This is the fairly common method documented most everywhere (the Go standard library http lib does this as well). If there's another more recommended approach for Go, has anyone documented it anywhere?\n. Agreed, was just referring to the fact that everyone that uses Go's HTTP library is going to suffer from this (and that's pretty much everyone using Go for HTTP).\n. Indeed, would you like to remove the use of the goroutine and benchmark it before/after to see what affect it has on the ability to soak up statsd messages?\n. Did you have any time to check out what the performance is like with just the single goroutine?\n. r+\n. Looks good to me, this is a big deep change though, so I'd recommend trink or vng also review it.\n. I am working on this right now, there's an issue_372 branch for it. We're going to make it available both as a plugin and as a separate executable capable of reading a stream of files. The user gets to declare what parts of the filename are which date parts and how they should be sorted. Example shown with the structure here:\nhttps://github.com/mozilla-services/heka/blob/features/issue_372/logstreamer/filehandling.go#L289\nHoping to land a first working version of the logstreamer this week.\n. This has been merged.\n. r+\n. Other than possibly adding another test to exercise slightly more code, r+\n. r+\n. r+\n. Ok, besides for the doc changes to reflect the backward incompatibility bits and the crypto/subtle tweak, everything else looks fine.\n. A big part of this is to have a way to see that its dead without utilizing the likely dead pipeline... the assumption is that it could already be dead and therefore the report messages will never go through either. If it freezes up suddenly, the report message will never be sent, but it would still be useful to see who possibly ate the messages.\n. Assuming it doesn't die immediately, it could be handy to have that data in the report message as well. :)\n. r+\n. Because the filemonitor has this bit:\ngo\n    parseFunction func(fm *FileMonitor, isRotated bool) (bytesRead int64, err error)\nwe can't interrupt it until it decides its read as much as it feels like. After this run, it currently saves the location, so we can save between files, or after its read as much as its going to read.\nTo address this, we'll need to update the interface such that it reads smaller chunks, and anything supplying a parseFunction will need to be updated.\n. No one at the moment is working on it. A PR would be welcome, one thing to consider is that due to gzip checksum changes some check should be made before using gzip to ensure its not the file currently being written to.\n. Yup, seeing this as well. Looking into a fix.\n. How large is the file? At the moment logstreamer requires 500 bytes or more in the file being read before it can save its position. Perhaps this should be lowered to 250.\n. I did fix a separate bug related to this recently, which seemed to address this. Can you try the latest dev?\n. Ok, at this time I don't see this error. Though another set of fixes was done to incrementally save record blocks to ensure some overread situations don't occur. Can you confirm this isn't happening in the latest dev?\n. I'd like to close out remaining Logstreamer bugs if there are any, I can't reproduce this anymore (it did show up before several of the recent fixes). Can you confirm whether you still can reproduce this error? Thanks.\n. Closing this as no one can reproduce it and I haven't heard back otherwise.\n. Interesting, maybe its a bash vs. zsh thing. In that case, perhaps the script should be modified to be a bit pickier about using the right shell to execute.\n. I've been using the procstat filter as the gist shows. It gets the information we need so that was as far as I took it. I assume by 'needs work' you're referring to extracting some of the other data in there?\n. @steverweber Not at the moment. The main reason I wanted per-cpu was to try and determine if our daemon wasn't fully utilizing cores equally (or failing to use a few entirely). This works fine for our use-case, so I have no plan to return to it right now.\n. @steverweber sure, sounds good\n. \"sent received\"?\n. It's impossible that Header_JSON might not be there?\n. Maybe return a []byte rather than do the in-place modify using a pointer to outBytes?\n. Why are these pointers?\n. The '_ =' isn't needed.\n. If there's new options, make sure to update the appropriate doc section here:\nhttps://github.com/mozilla-services/heka/blob/dev/docs/source/configuration.rst#command-line-options\n. Nope, it was mainly to prevent my own mental overhead while I was moving things around and as a form of documentation.\n. A Shutdown message, or have the runner's drop an error. If the runner drops an error, the LogError takes care of indicating which plugin threw the error as well which seems handy.\n. Plugin's need to call InChan() and get a channel back immediately before they can proceed. If this doesn't use a goroutine, the method call will block forever until the other side takes the pack, which it can't do since the runner will block then on starting the plugin...\n. Oh, right, I see what you're saying.\n. I believe the reason I did it this way, is because its possible that upon immediately starting, the plugin may die again (connection fizzles). If the packet was pulled and put on this channel, then the plugin dies, the packet is then lost. The goroutine method here guarantees that the packet to be retained must be pulled off before it'll consider it \"used\".\n. Also, the channel has to be closed after its consumed, if it returns a channel of 1 with 1 packet, the plugin will never exit even though this channel will never get anything on it again...\n. TOML is not case sensitive, so specifying the annotation is only needed if the name is different from the variable name.\n. Should prolly use 'n' instead of 'self' as most of our code is now switching to the more idiomatic method target naming.\n. How long does http.NewRequest take to return if the remote server is down? I see there's a Transport option to set the response header timeout, but its not clear what the default timeout is.\n. I've seen some reasons they don't use 'self' or 'this' (due to how its a target reciever, not an 'object'), in the Go community though, using self/this is the bad practice as its not really idiomatic Go.\n. Why not just have:\nfor plc = range inChan {\nwhich will go through until its closed.\n. We've been trying to go for more idiomatic Go style, which generally uses the first initial or two of the type, rather than self/this.\n. TOML is case insensitive, so in this case specifying the annotations for the lower-case isn't needed.\n. Why are ir/h references saved to the struct? It doesn't look like they're used outside the Run where they're already in scope.\n. While this causes the Run to exit, the Monitor is left running, the monitor should be shut down as well by flagging it using a channel.\n. This should be removed to avoid lots of debug statements.\n. The hostname should be set on the struct rather than h to avoid having to call PipelineConfig for every single message.\n. Spinning up a goroutine and copying the data over isn't likely to be a big performance win over just handling the construction of the new pack here and injecting it.\n. Rather than this being a func, it should create a timer with time.After which returns a channel. It can then use a select to toggle between the close channel (which will be used by HttpInput to flag the monitor to close), and the timer. This way the monitor will exit quickly when shutting down in a graceful manner.\n. The IR logger is generally intended for useful human readable messages, not general debugging, so most of these should be removed (except the informative ones like Starting/Stopping).\n. Shouldn't this be switch i { case 0:, etc.?\n. Testing for the nil value of \"\" rather than doing a len() would be preferred.\n. These unindents, is that replacing tabs with spaces?\n. There's no C lib that does protobuf?\n. Why? Couldn't a few more checks be made in the test suite? For example, after it spins up, ensure that the logList has all the files in it that should be there, and that pipeline config had the AddInputRunner calls made. That would at least ensure that scanPath ran properly.\n. This should be documented in the \"Developing heka plugins\" section as well, just like the restarting interface is documented so that someone creating a heka plugin/decoder knows its available.\n. Use atomic int32's here to avoid the need for locking?\n. There's an exceptionally good reason to have multiple fallbacks. How come this change removes the fallback capability? How is this handled in a similar style?\n. Every single backwards incompatible change needs to be documented clearly. In Sphinx there's a directive specifically to indicate changes that occur between versions which should be used to reflect this. Also, the CHANGES logfile needs to reflect very clearly the large backwards incompatible change being done here.\n. This needs to use crypto/subtle for digest comparisons to avoid timing attacks.\n. If case is 0, default will not actually run, because case 0 will tell it to do nothing here. Default only runs if no case handles it.\n. Ok, this backwards incompatibility change should be documented in the CHANGES file to clearly indicate it breaks all prior heka configurations. All the places this config value changed in the rst docs should be updated with a .. versionchanged:: directive to indicate the config parameter has changed.\nHere are the sphinx parameters that will come in useful here:\nhttp://sphinx-doc.org/markup/para.html\n. Cool.\n. Thankfully switching this is a very small tweak. :)\n. The AMQP input was only expected to accept protobuf serialized, or plain text messages. If you use 'plain-text' then you're expected to decode them yourself.\n. I looked at LogError and LogMessage.... they just call log.Print, and if you need the printf you send it a sprintf... so that didn't seem an improvement. But yea, we should have a unified way so at least we can easily pipe it elsewhere later. Obviously I didn't want to generate a new message to report this.... cause I'm assuming that may be broken.\n. It's being sliced to the same start/end point?\n. Not sure about 'wins' phrasing, minor nit... maybe 'first-success'? Though it is longer to type. Maybe the shed should be green...\n. For a flag like this, maybe a const iota that the mdStrategies map translates it to? Feels cleaner than passing around a string type flag.\n. Why not remove anyMatch, test only for first-wins, then test for the CascadeStrategy being 'all' below on 173?\n. This also applies to the logfile input (where I copied these portions from).\n. We could, I considered that but it seemed simpler to tell the user to keep them lower-case rather than create dupe sets of the maps and copy all the values back and forth.\n. It's perhaps not well named. Verify hash only returns false in a single case (which is intended). That's if its able to open the file and cannot locate the existing hash. If its unable to open the file, it can't say for sure that its not the same file anymore than it can say it is the same file (except that we do have a FD by this point if you look at the context for the call to VerifyFileHash).\nIf VerifyFileHash kicks out false, then we attempt to scan all the files to see where we actually should be (if we got moved). Technically we only verify the file hash when checking to see if there's a newer file available (which we only do if we've hit EOF on the file handle).\nDo you have a suggestion for the function that is named better given the the goal is to only return false if we can be certain that the file now at this location does not match the hash?\n. This is an important point, maybe call it out with a .. warning or note directive?\n. ",
    "rafrombrc": "Turned out to be a glitch when the timestamp happened to land on a whole second. Before the addition of the trailing \"Z\", this would cause an error that could be caught, so we could check for the error and try a full second time format string if we got one. Adding the \"Z\" caused this case to generate a panic, however.\nThe resolution was to change the client to send a timestamp w/ the trailing zeros to microsecond resolution when the time lands on a full second. This change has been made to metlog-py here: https://github.com/mozilla-services/metlog-py/commit/5bac1f9717ff36d9b311df0b96575144c9b981ec\nThis also negates the need for custom time format string, and for the \"full second\" test, so this change has been made to heka as shown here: 9c743abf6a3da16702f92c43d355b2fb83720988\n. \"filter chains\" are a deprecated concept, this issue is obsolete.\n. Overall I think this looks great! I made some comments, most are nits. Nice work! Tests needed, of course, but we already knew that.\nOne bigger picture thought comes up for me: we know that one of our use cases is going to be using Heka as a log file transport, reading files from the file system and sending them through unchanged. It looks we can handle that fairly efficiently, since you can register a log file w/ no decoder and no text processing will happen. I'm wondering if we might not see some benefit from not being limited, in those cases, to one Message per log line? Probably a premature optimization at this point, just capturing and sharing the thought.\n. Functionally this all looks great, but I've got some design thoughts to mention for discussion before merging.\nFirst, in the client code there is the idea of an encoder separate from the sender, just as input and decoder are separate on the server side. The design is a bit naive (i.e. it might be better to have the sender instantiate the encoder and use channels to pass the message along like the inputs do) but even so it'd be nice to leave some more loose coupling btn the transport and the encoding.\nAlso, in the interest in eating our own dogfood, it'd be nice if we could actually use the client code in our output, instead of hand-rolling the TCP and serialization code yet again. This will require removing the alias of the Message type (so we're all using message.Message), and it might impact the design of the client, but that shouldn't be a problem.\n. r+\n. Added first batch of panic recovery code, merged in 280364fa4a85564ae459e8339f699692e23d048f.\n. Okay, I did a quick audit of all of the calls we make into all of the Heka plugin APIs in the pipeline package, and put defer / recover blocks in as much as possible. Last relevant change: d2e76b49889d6725531e9936732356ff6604de93.\n. The current situation (see cb3893d42adf8804485b8736dea359f153c86d94) is that we're correctly setting the hostname value when we receive messages from a TCP connection. Unfortunately, in many cases this is immediately overwritten by whatever is specified when the message is decoded.\nI'm thinking it might be a good idea to store the remote host name that we get from the connection in a different field on the message struct (ConnectionHost, maybe?), making sure that sandboxed Lua code doesn't have the ability to edit this value. The ConnectionHost may or may not match the hostname that is extracted from the message contents, and if it doesn't match this may or may not mean that spoofing is happening; we would just report our findings and leave it to the downstream consumers of the message to make that decision.\nIt might even make sense to store a sequence of connection hosts, to track the path a message takes through multiple Heka nodes. This re-introduces a spoofing vector, though, b/c we can only verify the remote host attached to the current connection, the message might be lying about the previous hosts that it visited. Even so, this might be acceptable for many use cases. Definitely open to thoughts and ideas about how to allow as much verification as possible.\n. Oh, it should be noted that the reason we only capture the remote address on TCP connections is that UDP remote hosts are at the packet level, not the connection level like TCP. Go defines the PacketConn.ReadFrom method that will provide the remote address for each packet received, but we'll need to make some changes to our parsing setup to be able to make use of this.\n. Both the TcpInput and UdpInput already support HMAC signing, see http://hekad.readthedocs.org/en/latest/configuration.html#tcpinput and http://hekad.readthedocs.org/en/latest/configuration.html#udpinput.\n. Oh, we have support for it in our client, I didn't realize we weren't exposing that in the TcpOutput. It'd be easy to add, we'd just have to check for signing info in the config like the TcpInput already does, and then pass it in (instead of nil) when we ask for our ProtobufEncoder here: https://github.com/mozilla-services/heka/blob/dev/pipeline/outputs.go#L110.\n. This all looks good at first glance. I'm curious about the capture group comment, though. Are you saying that to get at any capture group data that the filters will have to re-apply any regexes that might be in the config?\n. Right, if I'm understanding correctly this matches what I was thinking. I did figure that the interface would need to be changed slightly so that the matching data could be passed in to filters or outputs, this might be something we put in the pipeline pack.\nThe motivation for this is code reuse, configuration sanity, and performance. If nearly every filter and output already has a \"message_matcher\" setting to indicate which messages it should match on, it feels pretty awkward to have to put a separate \"payload_match\" (or whatever) option on some of them which contains an almost identical regex, which would have to be run separately, just to extract data that could have been extracted and passed along the first time. The message matcher would never actually generate new messages, it would only be passing along match group data to filters and outputs which would then do the work.\nOr maybe I don't fully understand and we're talking past each other...\n. It's worth noting that a filter can already call helper.Output() to cache a reference to a specific output at the start of the Run method. This doesn't help w/ the MGI case, however, b/c the output runner's Deliver method needs a PipelinePack, but the MGI only hands out MsgHolders. If we consolidate the message wrapper then we could make it so filters can hand messages directly to outputs this way.\n. The StatPacket channel is available through the StatMonitor, and the StatFilter is already using it. There's also issue #119 which covers further development of that API.\n. I've started work on adding a bit more panic protection on the features/safer-plugin-config branch of heka.\n. First stab at basic panic protection merged: 280364fa4a85564ae459e8339f699692e23d048f\n. Whoops, this isn't done, and is still desired. I referenced this ticket from pull request #101, which was merged a while ago, but that was merely a reference, the pull request didn't actually resolve the issue.\nTo clarify, there are a number of command line options to hekad that should be settable in the hekad config file, since they're really global hekad config options, but currently you can't set them in the config file at all. I'm open to either removing the command line options altogether, or to keeping them and saying that the command line flags will trump what is in the config file.\n. No, we don't need to keep the command line arguments, but we do need to update the documentation to remove references to the command line settings and to add info about [hekad] config section. And, in doing that, we need to make sure the max_message_loops option is covered, since it isn't now, according to #231.\n. Too vague to be actionable.\n. The potential scope of this issue is very large. For the 0.2 milestone we want the minimum useful implementation: upon receiving a specific control message, hekad will generate an output message of a specific type, payload will contain (at a minimum) the name and length of all of the plugin input channels, and any other channel lengths that are indicative of the current state of the heka pipeline.\n. Unfortunately, due to a git hiccup, the features/queue-report branch was fast-forward merged into the dev branch, so there's no single commit to point to that contains the merge of this functionality. The commit representing the HEAD at the time of the ff merge (i.e. the point where the merge happened) is e3c51b74fab7760a78dee29a24c1b903ddb2ed42.\n. On Fri 31 May 2013 11:09:06 AM PDT, Victor Ng wrote:\n\nDo we still need this since the AMQPOutput and AMQPInput have landed?\n\nYes. Well, sort of. This ticket is probably too broad. AMQPOutput and \nAMQPInput provide one mechanism for hekad <-> hekad transport. But we \nhaven't yet tackled reliability btn heka clients and hekad. Plus, I \ndon't think AMQP is where we want to stop; I'd still like to see Heka \nprovide native reliable delivery options.\nNone of that needs to land in 0.3 though. I'll remove this issue from \nthe 0.3 milestone, and on the next triage I'll split it up into a \ncouple of more specific bugs.\n. Major overhaul of plugin dev docs merged to dev in c59769016c0577b7735cb03486b952a2b53a7916.\n. Not clear yet that this is worth it. My yak shears are a bit dull.\n. Closing this b/c we're gradually phasing out the idea of a formal Heka protocol.\n. Closing this issue as too general, we can later open other issues for specific requirements.\n. Agreed that this is too complex and too inefficient. Your original \nsuggestion that the plugins hold on to a pool of decoders for reuse is \nprobably a good one. This is inching in the right direction, though, so \nI'm going to add a couple more tests and then merge, and will reopen \nanother ticket for revisiting this to simplify and improve performance.\nOn Fri Mar 29 14:57:29 2013, Mike Trinkala wrote:\n\nThe decoder clean up looks good but the decoder management seems\noverly complex and inefficient. Every TCP connection will spin up a\ndecoder for each encoding type (yes there are only two but it highly\nunlikely more than one will ever be used). When the connection is\nclosed they are all thrown away and a new set is created for the next\nconnection.\nOn another note: from a reporting perspective it may be interesting to\ntrack how many message have passed through a channel since a queue\ndepth of 0 cannot tell you if it is just fast or unused.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/85#issuecomment-15662647.\n. Okay, made another pass at this, w/ the following improvements:\n- Decoder life cycle management is now handled by DecoderSource (i.e. decoderManager) objects instead of spread about among the config and pluginRunnerBase.\n- Heka gives each InputRunner exactly one DecoderSource when the InputRunner is created. Input plugins can get the DecoderSource from their InputRunner, and can get Decoders themselves from the DecoderSource.\n- decoderManager objects will hold on to stopped decoders and will try to reuse the ones that they are holding on to before they create new ones.\n. First major batch of doc strings merged to dev in 5faac4c65c7664cee8e7a88df44740f8e3a5f674.\n. Bit more merged to dev in ad9e6bee6201dfa5919ab1e32bd37f29c0fe6a27. Only files that aren't yet doc stringed are the sandbox.go files (awaiting changes that are being made there) and transform_filter.go (which I'd like Ben to do since he could explain it better than I could).\n. Last bits of sandbox related files are updated in 5a1b3161e426a7ed4dc81d16b414ccd1f135b031 and be1188cf0d044742d1d96d969a69828663cd2be3. transform_filter.go docs are tracked in issue #129, closing this one.\n. Overall I think this all looks pretty good. As you mention, there's definitely a lot more duplication than I'd like btn the config's filter creation and the sandbox manager filter's filter creation, so I'd like to at least open up a ticket that we should try to refactor that stuff so we can remove the duplication. I don't think that should be a blocker, though. Mainly I'd just change the APIs a bit so we don't expose the filter creation and removal through the main PluginHelper interface. Those just seem like too destructive and too infrequently needed to have showing up at the top level of our plugin developer APIs.\n. Okay, found the place where I wasn't recycling message holders, fixed in f4f65df4db48df07d2022630bfa6d3948959ac8b. Have realized that it's really time for us to get rid of message holders altogether, though, and just use PipelinePacks everywhere. Going to open a new ticket and a cut a branch for that work.\n. Trivial to add, thx to arbitrary plugin self-reporting functionality we put in: 32a71b622a1c756939ccf310a274904c2b07234a\n. Implemented on the features/no-messageHolder branch, opened pull request 93.\n. 3a75660d8759ebeccd46aae840c4e0a07dcff79b\n. 45de5dcbf58370af3fce8aca4102a4cccce8f041\n. CMake replaces perl in this commit: https://github.com/mozilla-services/heka-build/commit/b5e7b251e5f7f34e3e00ecfe4b2ac95194a75ad7\n. Resolves issue #63.\n. Moved all heka package commands into cmd subdir, and moved all heka-build scripts into the scripts folder, along w/ necessary adjustments to Makefile and scripts. Merged in c45a87559b1700fe634701f44a96b6c86ed92d66 and https://github.com/mozilla-services/heka-build/commit/5608ea03a6dfeb65d6413a913886575f9587746e\n. Bit more thought here: we could get rid of the MessageGenerator altogether, and replace it with a new method in the PluginHelper api::\n\nPipelinePack(loopCount int) (pack *PipelinePack)\nThe returned pack would have have loopCount value of the passed in value + 1. We would also change the Router API so that instead of dropping packs onto the InChan directly, you have to call a method, which would check the loopCount value and reject if the count were too high. Such a check would also be added to the *foRunner.Deliver() method.\n. Looking at the all the data, w/ the larger message sizes Heka is still pushing ~10Gbps through its pipeline. The profiles imply that a lot of improvement could be made by reducing mem copies, but we're fast enough already that we can deprioritize this against other improvements.\n. Okay, well, turns out Ben did this last night when I wasn't looking. The work is spread out over a handful of commits: d74ef7a7757a814ad8dbeebacdbfc41a77870fa6, 4722a48175812be6eebb1a1c69f70fd3cfb54253, 8197322d8432794a40df000cabc4a279fbbfdda0, 7b9ee0c277e13b1d6f3bda69a1d439da6e5ea2f5.\n. Ended up converting StatMonitor into it's own input plugin called StatAccumInput, which provides a StatAccumulator interface that other plugins can use to submit Stat objects.\n. Good point, closing this issue, captured the new work that needs to happen in #166.\n. Urgh. Tried repeatedly to get this to happen again, but every single time Heka would fail to start (which is fine behaviour) rather than running and failing to shut down cleanly (which is not). Am closing this issue until and unless it rears its ugly head again.\n. @trink You think this is reasonable to get in before 0.2 final?\n. Merged in 703bd251738d15da028a85f07823d7ce562c6f30 and https://github.com/mozilla-services/heka-build/commit/b5e7b251e5f7f34e3e00ecfe4b2ac95194a75ad7\n. As long as we either a) provide benchmark suites that demonstrate that the performance impact is negligible or b) provide a knob to turn it off.\n. As long as we either a) provide benchmark suites that demonstrate that the performance impact is negligible or b) provide a knob to turn it off.\n. @trink Can you provide a bit more description on this one?\n. Okay, great. Slotting this for 0.3, although we don't necessarily have to have all of those features by then.\n. MPLv2.0 (https://www.mozilla.org/MPL/2.0/). It's in every source file (e.g. https://github.com/mozilla-services/heka/blob/master/pipeline/doc.go) but i'll add a LICENSE.txt to the top to make this more visible.\n. Added LICENSE.txt. in 00b92ff2f1d2a267f9477c6e31fc0183b6840719.\n. We'd like to be able to do secure message transport w/o the need for an external service, so I'm leaving this ticket open, but I wanted to mention that we do now support AMQPS (i.e. AMQP over SSL) which could be used for secure heka to heka message transport.\n. It's now possible to make TLS connections directly from a TcpOutput to a TcpInput. Of course, the TcpOutput is brain-dead enough to be hardly useful, but that will be addressed soon, see #355. \n. Sorry about that, the packages were generated on an Ubuntu 12.04 machine. We're working on putting together some other systems, and our release process in general, so we can provide better compatibility.\n. The most recent deb packages (from 0.6 on) have been built on a debian host, this issue should be resolved.\n. Started this process in pull request #170 by converting the TransformFilter into a LoglineDecoder, so the back pressure from a slow decoder will only impact the inputs that are using the decoder. Might end up pulling more of the functionality out so that it can be shared by other decoder implementations, but this resolves the initial issue. \n. #170 merged in f3aa9f1\n. Merged to dev in 4800ca95dbdcc32d27aa327a6a617ce77fec6237.\n. Okay, all done w/ the review. Just small finishing touch tweaks, the overall functionality and implementation look great! Very exciting to have this land!\n. We've made Go a pre-req rather than downloading it for folks, but the change has been made.\n. I don't know where it's been tested. But we're not providing Go at all \nany more, users have to install it themselves (although if they already \nhave one that we installed from an earlier heka build that will be \nused), so it should be fine as long as you have a working Go installed.\nOn Fri 28 Jun 2013 05:03:45 PM PDT, jbonacci wrote:\n\nWell, thanks for patching this so we can install/use on OS X. I assume\nsomebody checked this out on OS 10.7 as well as OS 10.8...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/180#issuecomment-20220769.\n. I don't think they're actually built into a ReadTheDocs site or anything at the moment, unfortunately. You can see them in place on github, though: https://github.com/mozilla-services/heka-mozsvc-plugins/blob/dev/docs/config.rst\n. This has been implemented by @bbangert and merged into the heka-mozsvc-plugins repo: https://github.com/mozilla-services/heka-mozsvc-plugins/commit/5a485b8998c8a1d329eb390708b08817a5aa8f4e\n. Done: https://github.com/mozilla-services/heka-build/commit/86c994a133ac2c5878df4667b16a30467ebce66c\n. The config seems a bit baroque to me. This is in large part b/c things get trickier when a single LogfileInput is reading multiple input files. I had a chat w/ @bbangert and he didn't object to removing this feature, so I've opened #216, and am closing this pull request for now. After #216 is implemented, we can merge it back to this branch, simplify the implementation, and open a new pull request. \n. Jenkins is now auto-building again. \\o/\n. Switched to synchronized access in 18c655afa934fb19ca8c134fbb9e0e5804ee4fa7.\n. This is duplicated in #1025, closing.\n. Using a channel to send a stop message instead of mutating a boolean flag will resolve this.\n. Switched to using the channel that we already had, merged to dev: 98ac0d3d5c49bc25a0c1900ceefab1ff3fa547f2\n. I don't think we have to make the ReportingPlugin interface mandatory. If there's something we need from every single input, then we can just iterate through all of them and extract what we need. If getting the data requires knowing details of the input's implementation, we can add methods to the Input interface that we can call to get the data.\n\nThis is definitely important, though, so I'm setting this to the 0.3 milestone.\n. This looks good, but we'll want to update the docs too, configuration.rst at the very least, might be worth a grep to see if it's mentioned anywhere else.\n. r+\n. I don't like duplicating data either, but I feel pretty strongly that this is a good idea. In real world usage, the payload for statmetric messages are not that short; every single timer usually generates five lines of text. Also, there are lots and lots of partial match values due to the statsd naming conventions. Finally, it's much more straightforward for someone to extract structured data from structured message fields than it is to have to do string parsing to do so.\nI've set it up so that the StatsdInput is configurable; it can either put the data in the payload, or the message fields, or both. The default, for now, is both, but I'm open to changing this. I'm also open to profiling, of course, to see what the performance difference is. But string parsing would have to be consistently much faster, even in cases where there are 100 or more statistics, before I'd be convinced that using the message fields is a bad idea.\n. StatMonitor has been converted to StatAccumInput, and it supports emit_in_payload and emit_in_fields config options. Defaults to emit_in_fields == true, emit_in_payload == false, but you can reverse those values to return to the original behavior, or set them both to true to duplicate the data in both formats.\n. Whoops, my bad, opened this against master instead of dev. Closing to re-open correctly.\n. Fix merged in de02f63. \n. Okay, it's reviewed. Overall it looks great, nice work! The only critical notes I have are either trivial stylistic issues or what look like what were meant to be temporary code-removal-via-commenting-out that you accidentally committed and pushed. Make those tiny fixes, make sure the tests work, and merge away. :)\n. We can take a look at making changes here, but part of this has to do w/ maintaining compatibility w/ existing statsd / graphite software stacks. Statsd sends, and graphite expects, every single metric to be sent through every single time. This maintains parity w/ the way that the whisper time series files work.. they're always a fixed size, and always include data for each metric for every point in time.\nIt may make sense to maintain this behavior in cases where we're trying to interoperate w/ graphite and/or whisper files, but change it when we're expecting the data to stay within Heka's world, but unless this develops into measurable operational impact it's not likely to be very high on the priority list.\n. Good catch. Forcing the user to convert from decimal to octal seems burdensome, so I'm changing the config option to a string that will be parsed as an octal integer.\n. This looks great, thanks for the submission, @chamaken!\n@crankycoder, My only thought, implied in my line note above, is that the decision about whether or not to discard a line should probably be made by the decoder, not by the input itself. Open to discussion on that if you disagree, though.\n. Yup, this has always been on the table, but we hadn't actually opened a ticket for it, thanks for doing so. This should be extremely easy, since the payload of the 'statmetric' messages that Heka generates for stats data is already in exactly the format required by carbon, all we have to do is send this over a socket to support carbon's 'plaintext' input mechanism.\n. Yay! This was one of the very next items on our list, we're very glad that you've gotten it started! :)\nThere are a few tweaks we'd like to make to support meshing a bit better w/ the Heka side of the code. I'm going to review the diff and will make line notes to capture any thoughts I have. We'd also like to add tests. If you've got the time / inclination to work on this further and make the changes, great! If not, please let us know, so we can run with it.\n. Okay, I'm done w/ my initial review. Again, it'd be great if you could let us know whether or not you're up for making these changes, or if you'd rather we take it on.\nThanks for your valuable contribution!\n. Yeah, great! In the meantime I'm testing out your fork by hand to see if an ElasticSearch noob like me can get it working.\n. Okay, there are still a number of bugs to be fixed, improvements to be made, and tests to be written.  However, I've done some hand testing and have verified that it's somewhat usable, and folks will want to play with it so I'm thinking we should merge what we have, and cut new branches for future work.\n@tlrx Can you edit this pull request so that it's based off of the 'dev' branch instead of 'master'? Then I'll do a final once-over and hopefully will be able to merge it back. Thanks!\n. Oh, there's no \"edit\" button for you near the top right of the page, in the shaded area that shows you the branches? If not, then yes, please close this one and reopen one against our dev branch. Thanks!\n. On the same machine, a LogfileInput can consume log files generated by a FileOutput, it just tails the file like it would any other. If the idea is to send data to another Heka instance rather than to the file system, then the FileOutput wouldn't be used.\nI'm not certain, but it seems like you're asking for an efficient way for one Heka (H1) instance to send text data to a different Heka instance (H2), where H2 then does any necessary parsing and processing. Is this right? If so, I'd say this is a duplicate of #165. I'm closing this issue under that assumption. If I'm wrong, please re-open w/ more detail as to the requirement. :)\n. Hrm... not clear to me that these should be separate. There's nothing \nregex-specific in LogfileInput any more, IIRC, all of that has been \nmoved to the decoder.\nOn Fri 02 Aug 2013 11:23:59 AM PDT, Ben Bangert wrote:\n\nAh, ok, so LogfileInput is specifically for logfiles in the context\nthat logfiles are ascii and regex parseable. As this issue is about\nLogfileInput, I'm closing it, I've opened #363\nhttps://github.com/mozilla-services/heka/issues/363 for the FileInput.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/261#issuecomment-22024094.\n. This seems reasonable. I'd be curious whether it'd be worthwhile for performance reasons to notice when the common case (i.e. a simple string match at the end of each record) was in use and to not use regular expressions if so.\n. This would be a big win, but it might be a bit tricky to get right. Seems we'd need a way to distinguish btn messages where reusing the original encoding is safe (i.e. they haven't changed at all) and ones where it isn't. Did you have ideas on how to make sure we can trust the encoding?\n. Resolved in #1406.\n. Resolved in #305.\n. Sorry, this one fell through the cracks.\n\nThanks for the fix! I'd be happy to accept it, but our master branch is for released code only, active development happens against the dev branch. I'm closing this pull request, please feel free to open one against the dev branch and I'll merge.\n. Good point. I'll change the implementation.\n. Resolves #272 \n. Hrm. We already have plugins w/ a custom data flow and non-obvious dependencies. The StatAccumInput is one such plugin. I know we've been back and forth on this a bit, but I don't love that it's an input, b/c it will always be closed before the filters, which means that during shutdown we will pretty much always have to deal w/ data that can't be delivered. As of right now we put the burden on the client plugins to deal w/ this (i.e. the API tells you that the data was not accepted).\nI agree that the \"source filters\" approach (where you can declare certain filters as dependent on other filters) I was experimenting with was too complicated, though.\nWhat if we made it such that all filters that have message matchers are always closed before all filters that don't have message matchers? Maybe w/ a separate waitgroup and everything. The assumption there would be that if a filter doesn't accept messages from the router, then it's almost certainly getting its input from other filters that do accept messages from the router, and these would all be closed and flushed before the non-matcher filters get shut down.\n. @jivid Sorry, we're working on contributor guidelines that will help orient folks, that should be coming soon. For now, I can let you know that active Heka development happens on the 'dev' branch, that's where you'll find logline_decoder.go. From the root of a heka-build checkout running 'make dev' should switch you, or you can just 'cd src/github.com/mozilla-services/heka; git checkout dev; git pull origin dev' to check out the dev branch and make sure you're up to date.\n. On Sat 29 Jun 2013 08:18:50 PM PDT, Divij Rajkumar wrote:\n\nI've written what I think should fix this issue, but I'm having issues\nwith running tests on this. According to the docs, I should use flood,\nbut no flood binary got installed with |make dev|, so I tried running\nthe main.go file in |cmd/flood/|. But that gives me this error:\nflood(dev)  $go run main.go -config=\"flood.toml\"  -test=\"default\"\n2013/06/29 23:12:23 Error creating sender: dial tcp 127.0.0.1:5565: connection refused\nexitstatus 1\n\nThis is just telling you that the flood config you're using (the \n\"default\" test in the \"flood.toml\" file) is expecting Heka to be \nlistening for TCP connections on port 5565, but it's not there. Maybe \nhekad isn't started, or you don't have a TcpInput configured to listen \non port 5565?\n\nI went over the heka docs on\nhttp://heka-docs.readthedocs.org/en/latest/, and that told me I can\nuse |make test| to see if my copy of heka doesn't have errors, but\nwhen I try to run that, I see:\n/usr/local/go/src/pkg/github.com/mozilla-services/heka/sandbox/lua/lua_circular_buffer.c:9 6c: No such file or directory: ctype.h\nmake: ***[bin/hekad]  Error 2\nCan anybody help with this?\n\nI'm guessing that maybe there's a required dev package that you're \nmissing on your system? I don't know what OS you're running, but on my \nLinux Mint 14 (which is basically equivalent to Ubuntu 12.10) it looks \nlike I have a /usr/include/ctype.h file by virtue of having the \nlibc6-dev package installed.\nHope this helps,\n-r\n. Oh, re: the 'flood' binary above, forgot to mention that we do have a \"make flood\" target that will generate the binary you. We also have \"make sbmgr\" to generate the sandbox manager 'sbmgr' binary.\n. Awesome, thanks! :) Looking at it now.\n. Okay, it took a while b/c I was multitasking, but those are all the notes I have on the original submission. They're all tiny little nits, over all the changes look like a big improvement. Thanks again!\n. This is great, thanks! Do you mind writing a test for it? Here's one that's very similar to what you'll need:\nhttps://github.com/mozilla-services/heka/blob/dev/pipeline/decoders_test.go#L141\n. Whoops. I guess you never actually ran the tests, or even tried to compile, since it didn't compile, and even after it did the tests didn't pass. Not a big deal, I fixed it up in 51c589e9e122c78dd9386d29eb5dabdd534edea2, but next time please make sure you at least run both make and make test from the root of heka-build to verify all is well before submitting the request. Thanks!\n. This looks great! The only note I have is that I'd love to see more options specified in the [hekad] section of the sample config, and then some assertions to test that we got a populated struct. Once you add that, feel free to merge both this and the corresponding heka-build.\n. +1 to building the catch-all functionality directly into the MultiDecoder, that's better than putting the burden on the user to provide their own catch-all decoder.\n. JSONPath is probably worth considering as a mechanism for describing how the arbitrary JSON should be mapped to a message.\n. This might be useful: http://blog.labix.org/2013/06/07/efficient-xpath-for-go\n. The sample-config.toml file in the heka folder (https://github.com/mozilla-services/heka/blob/dev/testsupport/sample-config.toml) is a suitable sample.\n. The fixes all look great. There's one small bug and one behaviour change I think makes sense, then I think we're ready. :)\n. Resolved in 6a54b8341371cb6cdbb851034163bd1f296018bf (#325). \n. Okay, I've been playing with this, and, while it's likely to further evolve, I think it's in good enough shape to get merged.\nHowever, actually switching to this requires that we adjust our CI set up to the new build process, or it will be broken. @bbangert can you work w/ @trink towards getting https://ci.mozilla.org/job/Hekad/ set up to use this build mechanism instead of heka-build?\n. This is great, and it comes with tests, to boot. Awesome!\nMy only request is that you name the new config option something slightly more descriptive ('ESIndexFromTimestamp', maybe?) and add it to the config documentation at https://github.com/mozilla-services/heka/blob/dev/docs/source/configuration.rst#elasticsearchoutput.\nThanks!\n. Bump for @benmmurphy. I'm hoping you can take 5 minutes to change a variable name and add a line to the documentation before I merge. Thanks!\n. Okay, I've made all of the in-line comments I have to make. They're all small issues, mostly related to copy-paste artifacts, overall it looks great, thanks!\nThe only general question I have is that it looks like it is possible for someone to specify a JSONPath expression that lands on a nested object instead of a numeric value or a string. Do we know what happens in that case? Is it an error condition? Should it be?\n. @trink Excellent point. We definitely need to support more sophisticated transformations of extracted data than just inserting that data into an otherwise static string. This is true across all of our Payload*Decoder plugins; I've already had users ask whether they could do more with log file regex captures than just insert them into message fields.\nI'm not sure about getting rid of message_fields, though. For one, using message_fields maintains parity w/ the PayloadRegexDecoder, which needs a separate data structure since the json_map analog is just a regex w/ capture groups. Second, message_fields can maybe be expanded to do what we want. I'm thinking maybe message_fields could grow support for specifying functions to be called.\nIn any event, I'm inclined to leave message_fields in for now, but to open a separate ticket that deals w/ getting robust, consistent data transformation support across PayloadRegexDecoder, PayloadJsonDecoder, and PayloadXmlDecoder.\n. All of your fixes look great! My only remaining comments are about the naming of the new helper struct you defined, and to add a couple of docstring comments.\n. Another (possibly better) idea: extend the message matcher syntax to support the use of inline templates (using Go's text.template package, with the template namespace pre-populated with the capture results. Adding custom functions to a template would still require a recompile, but there's at least a bit more flexibility out of the box, and over time we can grow a set of utility functions (over and above the default set) that is made available to all of the templates.\n. This problem also might be better solved by #370.\n. Closing this since #370 has been implemented. Needs moar docs, though.\n. Brainstorm: I was thinking it might make sense to write a struct that was a wrapper around a buffered channel. Feeding a pack in would block until after the pack was written to the stream; the inject method would return a channel that would be closed when pack was safely persisted. Then the pack is dropped on the buffered channel for delivery. We'd need to figure out how to signal when the pack was safely delivered; Recycle as it's currently used isn't quite right, b/c a pack is usually recycled before the data is actually written out, but we might be able change that a bit to work.\n. @tgulacsi The missing ctypes.h issue is almost certainly b/c you're missing a dev package on your system, probably 'libc6-dev'. Try installing that and re-running the build. \n. Thinking upon this further it's clear that there are two separate use cases for wildcarded or globbed file paths:\n1. Multiple log files, all of the same format, but each representing a separate data stream. An example would be HTTP server log files from a number of different hosts, possibly represented by a path such as /var/log/hosts/*/access.log.\n2. A single data stream that is spread across multiple files that are meant to be read in series. An example would be an application log that automatically creates a new timestamped log file every hour, represented by a path such as /var/log/app/YYYY-MM-DD-HH.log.\nThese two cases are quite different, and will probably need to be solved separately. This issue (i.e. #344) is specifically related to the first case. I propose that we solve this problem by creating a new input plugin that will dynamically spin up new LogfileInput instances as needed, each identically configured (except for the file path, of course).\nI've opened #372 to capture the other use case.\n. Nope, no good reason. StatAccumInput used to only emit stats w/ that prefix, but it now supports different namespaces. Removing this restriction now.\n. This is useful, thanks for starting it! I made some notes in-line... in particular, I'm pretty sure the handling of the [hekad] section is broken. Extra bonus points would be given for adding a sample config directory to heka/testsupport and then adding at least a minimal test for your code in cmd/hekad/config_test.go.\n. Okay, just one trivial change, twice, and we're ready to merge. I like it much better w/ a single config option, thanks for that.\n. Yes, we should definitely base these off of the heka root folder. Even so, changing from /var/run to /var/cache is an improvement, so I think we should go ahead and merge this now.\n. We definitely will be keeping the TcpOutput, but it's marginally useful until we add some disk queuing w/ reconnect. The problem w/ #600 is that the data that triggered the TcpOutput write failure will be lost, probably along w/ other data that's already in the Heka pipeline. I expect disk logging and reconnect support to land some time in the next 6-8 weeks, possibly sooner.\n. Just a reminder that we already support restarting plugins w/ exponential backoff feature, ideally we'd reuse some of that code rather than duplicating functionality.\n. No, not suggesting you do, but it still might be worthwhile to share some of the backoff timing code. It might not, also, but we won't know if we don't check.\n. Finally had a chance to look at this. I think this is a reasonable requirement which is worth a bit of work to meet. I don't think a UseTypeField is the right approach, though, I think we'll want something w/ a bit more flexibility built in. One option might be for us to allow the use of Go's text templates (http://golang.org/pkg/text/template) in the type_name and index values, where the templates have access to data that's extracted from the message that is generating the record. Evaluating this for each message might be pretty expensive, though, so we'd want to make sure to not incur any performance penalty when templating wasn't being used.\n. If this meets your use case I'm definitely open to the approach... it's much simpler and less work than embedding the template stuff. Am looking at the details to make some comments on the code now.\n. @bijohnvincent This is a reasonable request. It has a much better chance of not getting lost if you open an issue in the tracker, though; comments on closed pull requests don't have much future visibility.\nThanks!\n. Closing since the use case can be met w/o trouble using existing inputs.\n. Okay, further investigation reveals that systemd is a bigger PoS than I even imagined, and it actually will require a custom journalctl input just so we can ensure that we invoke w/ the --show-cursor option, and that we capture and persist the cursor line when we're finished streaming. Although not sure what we're supposed to do if the shutdown isn't clean.\n. Answer: Because the inputs are putting the raw data there. Which leads to the next question: Why are the LogfileInput and HttpInput plugins putting the raw data into message.Payload?\nAnswer: Because that way there's a useful, functional message object from the get-go, even if no decoder is used or all of the decoders fail. This way processing of the raw data can be delegated to Lua filters later in the pipeline even if decoding doesn't happen, or in addition to the decoding, as long as the decoder doesn't overwrite the original payload.\nClosing this issue b/c we decided in a Heka project meeting that a) there's some validity to the original justification and b) our eventual plan to use byte slices and arrays for the payload would remove the drawbacks to using the immutable and non-recyclable strings that are currently in play.\n. More random thoughts on message pooling:\n- In addition to performance, resident memory size is also likely to be impacted by whether or not we recycle.\n- It'd be possible to have more flexible pooling, where instead of blocking we create new packs when the pool is exhausted, freeing them up again when things slow down enough for the recycle channel to be full.\n- Pooling provides a handy built in resource constraint; if we allow the set of packs to dynamically grow, we'll want to introduce some other measure to make sure back-pressure builds as needed and we don't just create packs until we run out of memory.\n. Closing this in deference to #261, b/c the LogfileInput can serve both cases. Renaming LogfileInput to simply FileInput might be a good idea, but that can be a separate issue.\n. I wonder if this shouldn't be a more general SandboxDecoder which exposes an entire message and allows a specific set of mutations on the message during the process? We expose the LPEG library, decoders are free to use it, but that's a subset of the use case.\n. The Payload...Decoder names are intended to indicate that the decoder expects the input data to be in the message payload. I'm proposing that the scope of this plugin be expanded to allow for sandboxed code to be access the entire message to be able to perform arbitrary transformations.\n. Those are all valid use cases, but I lean towards starting out by supporting just a couple of them, we can add more as needed on a different ticket. The two cases that we need to support immediately are as follows:\n- [ ] filename patterns like [\"appname.log\", \"appname.log.0\", \"appname.log.1\", \"appname.log.2\", ..., \"appname.log.N\"] where \"appname.log\" is always the file that is currently being written to by the running service, and the others roll over through the number sequence as they get older.\n- [ ] filename patterns like [\"appname-YYYY-MM-DD(-HH).log\", \"appname-YYYY-MM-DD(-HH).log\", ..., \"appname-YYYY-MM-DD(-HH).log\"] where the file names all embed a timestamp representing when the contents of that file were written. The file being written to changes w/ each interval, and older files might be deleted but they're never renamed. We should support flexible timestamp formats, as always.\n. Thanks for the code! whisper-go was merged, and the heka build has been updated to use the newer code, pull request #377 merged in 549e5b2. If you can rebuild and verify that this fixes the problem, we can close this issue. If you confirm that there are problems w/ the aggregation, that should go in a separate ticket. :)\n. Merged in e9f49796dec35cd901034035e048a36e9268a1f4.\n. Okay, the whisper-go fix has been merged and heka has been updated to use the newer revision, can you verify that this resolves the issue? Thanks!\n. Unfortunately, the current design doesn't lend itself to this, b/c decoders don't work w/ streams, they expect that the data representing a single message has already been extracted from the raw data that the input has received. So for now this would require a separate SyslogInput that would understand the syslog protocol and would tease apart the messages for decoding.\nThis isn't the first time that the idea of a decoder handling a raw stream has come up, though; we were just talking about this possibility this week in the context of having a single heka message spread across multiple incoming message packets, in cases where we might want to send a large message using a transport that has a limited packet size. Leaving this open as evidence for another use case.\n. Whoops, sorry, dropped the ball on updating this ticket. I believe that this was resolved in 8942cd54810970308b7cf3632e8c3646e1d3b999, which added the ability to set the format option to \"payload\" and have the message payload passed directly to ES. No validation is done, it is assumed that the payload will contain valid JSON that ES will be able to handle.\nClosing this ticket, please reopen if I've misunderstood and this doesn't actually resolve your issue.\n. This code looks fine, thanks. There are some other pieces that should be taken care of before merging, though:\n- Adding a test or two to elasticsearch_test.go.\n- Updating documentation in configuration.rst.\n- Recording changes in CHANGES.txt.\n. Okay, this is looking good and is ready to be merged except for the fact that it no longer merges cleanly, and therefore can't be verified by TravisCI. I'd be happy to resolve the merge conflicts myself, but since I can't actually push to your repo it's a bit of a PITA. If you merge dev back to your branch, resolve any conflicts, and we can verify that our build is still green then I'll finally finish the merge.\nThanks for working with us!\n. I don't think this is a real failure. sigh\n. As usual, my only notes are nits, nothing worth blocking on. The only comment I have other than the notes I've already made is that it's a bit weird to me that there are implied default values for delimeter and delimeter_location but those values aren't returned as part of the default LogfileInputConfig. There might be a good reason for this I'm not seeing, though.\n. I'm +1 to adding diagnostics; those would be useful in a variety of situations. Would like to avoid the API change if possible, I like the idea we were discussing in IRC of having an optional proxy channel that pulls from the real RecycleChan, timestamping the packs in the process.\nI'm -1 to trying to do any self-healing for now.\n. Okay, final thoughts on this merge:\n- I agree with Ben that this combines too many disparate changes into a single pull request. It's a bit more work, but it's better for the entire team process to break things apart. The changes to resolve #308 could have been done first, at least. It's possible to cut a branch from another branch, too, so you can still work on dependent changes w/o being blocked by the first change being merged.\n- I think we should change things before the merge such that the ProtobufDecoder is not automatically created, simplifying the docs and removing special case code and behavior.\n- I think that the AMQPInput should be changed, and that we should revisit the relationship btn MsgBytes / Message / Payload concepts, but that those should be done separately from this pull request, attached to tickets of their own.\n- We should draft a message to the heka mailing list before this gets merged, explicitly warning people about the b/w incompatible changes that have been made, and pre-warning them about the ones that will be coming soon.\n. Hrm, I had no problems building with Go 1.2 on Ubuntu 13.04, although I'm now using 1.2.1. For anyone who's on a Debian-based distro I strongly recommend using 'godeb'; that will download specific versions of Go and package up and install a local deb package for you.\n- http://blog.labix.org/2013/06/15/in-flight-deb-packages-of-go\n- https://github.com/niemeyer/godeb\n- http://floaternet.com/golang/godeb\n. I think this sounds like a great idea. Explicitly supporting the gathering of process and system metrics is definitely a part of Heka's intended scope. Doing this in a consistent, cross-platform way is tricky, but delegating to external processes can solve a fair number of problems, and it looks like go's os.exec package provides a reasonable cross-platform way to run the processes, get the results, and even chain multiple processes together. The command output should be placed in a message payload, so the PayloadRegexDecoder could be used to further extract information, as needed.\nI'd recommend explicitly supporting multiple processes chained together in the input config, so Windows folks don't have to jump through hoops to set up their own pipes.\n. @bzub There are some good ideas to steal from tcollector, thanks for the link. It's very useful to specify the jobs to run w/ a directory structure, so jobs can be managed w/o having to edit configuration or restart Heka. However I'd recommend we approach this the same way we do w/ log file parsing. We have a LogfileInput that is used to load a single log file stream, and then we have a separate LogfileDirectoryManagerInput that will watch a directory and will transparently spin up LogfileInput instances based on the contents of that directory.\nSo in this case I'd suggest we start w/ a ProcessInput that will start a single process (or pipeline of processes), using configuration to specify the run interval heuristics. Then, once that is tested and working reliably, we can add a ProcessDirectoryInput that would watch a folder and dynamically spin up ProcessInputs as needed based on the directory contents.\n. The pipe package looks nice, but I'd like to avoid introducing more external dependencies unless there's a really compelling reason to do so. In this case, it looks like os.exec gives us everything we need, including the ability to chain multiple processes together, so I'd lean towards just using the stdlib.\n. It's not exactly what you're asking for, but it's already possible to have your heka config across multiple toml files. If you start heka w/ the -config option pointing to a directory rather than a single file, then all of the files in that directory will be assumed to be toml files and they will all be loaded. Loading will happen in alphabetical order, with later files taking precedence over earlier ones in cases of conflict.\n. Closing this b/c the current solution has proven sufficient for all of the use cases that have come up.\n. See PR #838.\n. Encoder plugins are now implemented.\n. Just an FYI that 0.4 release will be cut next week, so this should be fixed up very soon if you want it in there. :)\n. Merged to dev (w some doc tweaks) in f72ca85560ea82f30d071d8de790d4ee1a71d4bf.\n. Yep. Currently sending SIGUSR1 to a running Heka process will generated a report and spit it to stdout even in most cases where the pipeline is jammed. The only situation that will truly block, AFAICT, is when the injectRecycleChan is completely empty.\nThat's a real concern, though, so I've opened #423 to request that we fix this so a report can always be generated.\n. Fixed in #1307.\n. This seems to be a legitimate test failure in the TravisCI, want to get that resolved before merging (or even reviewing, since it might change the code somewhat). Are you getting the failure locally?\n. Okay, well, I'm guessing it was just Travis being hosed again, since I brought the merge up to date and the tests are passing w/ no problems. Merging now.\n. Merged to dev in 31052b6.\n. This issue has been made obsolete by the LogstreamerInput, which does indeed write to the journal periodically even when there isn't a clean shutdown or an EOF.\n. Awesome, thanks for the catch.\n. Good improvement, much appreciated. Would you mind adding a note to the \"Bug Handling\" section of CHANGES.txt? Once that gets pushed we'll merge. Thanks!\n. Closing this b/c we can't reproduce and there's been no further activity. Other Windows build and test issues are currently being resolved, please feel free to try building the current versions/0.4 branch and open another ticket if the build fails. \n. Resolved in #831.\n. This looks great. I made a tiny little nit-picky note re: function return value order, if you can change that and then add a note about this to the CHANGES.txt file this will be ready to merge. Thanks!\n. p.s.: If you can get that turned around within the next 12 hours or so it will make it into the 0.4 release that will be out tomorrow or Friday.\n. Whoops, sorry, the release has been delayed due to some work that still needs to be done on the updated dashboard, and as such I've been distracted. Thanks for the updates, merging now.\n. Nice work!\n. Are you sure you've managed to deploy the latest code from dev? You're crashing at the same place, but there's now an explicit nil check right in just that spot, see c5b4a1c959fe6ed7034ca47dd53937db8f4bb5e6. I was able to successfully reproduce the original problem locally and have verified that it's no longer happening for me, so I'm pretty sure that the issue has been resolved.\n. Ah, right, that's b/c there was a change (unrelated to your issue) to \nthe Lua bindings that require a 'make clean' before the next build \nattempt.\nYou can either run 'make clean' from the build folder, or you can \ndelete the build folder entirely and run \"source build.sh\" from the \nheka checkout root.\nOn Wed 16 Oct 2013 10:03:06 AM MDT, Mark Melling wrote:\n\nSorry Rob, my fault, I hadn't.\nNow when I do a build I get an error:\n./build.sh\n-- sphinx-build was not found, the documentation will not be generated.\nSubmodule 'docs/source/_themes/mozilla' () registered for path\n'docs/source/_themes/mozilla'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mark/heka/build\n[ 20%] Built target heka-mozsvc-plugins\n[ 20%] Built target whisper-go\n[ 20%] Built target go-uuid\n[ 20%] Built target gomock\n[ 20%] Built target go-simplejson\n[ 20%] Built target raw\n[ 20%] Built target amqp\n[ 20%] Built target go-notify\n[ 20%] Built target toml\n[ 20%] Built target goprotobuf\n[ 20%] Built target slices\n[ 20%] Built target xmlpath\n[ 20%] Built target g2s\n[ 20%] Built target gospec\n[ 20%] Built target sets\n[ 20%] Built target goamz\n[ 20%] Built target GoPackages\n[ 20%] Built target lua-cjson-2_1_0\n[ 20%] Built target lpeg-0_12\n[ 20%] Built target lua-5_1_5\n[ 20%] Built target heka_source\n[ 20%] Built target message_matcher_parser\n[ 20%] Built target mocks\ngithub.com/mozilla-services/heka/sandbox/lua\n/tmp/go-build394740442/\ngithub.com/mozilla-services/heka/sandbox/lua/_obj/lua_sandbox_private.o:\nIn\nfunction require_library':\nheka/src/\ngithub.com/mozilla-services/heka/sandbox/lua/lua_sandbox_private.c:1027:\nundefined reference toluaopen_cjson_safe'\ncollect2: ld returned 1 exit status\nmake[2]: * [CMakeFiles/hekad] Error 2\nmake[1]: * [CMakeFiles/hekad.dir/all] Error 2\nOn 16 October 2013 16:49, Rob Miller notifications@github.com wrote:\n\nAre you sure you've managed to deploy the latest code from dev? You're\ncrashing at the same place, but there's now an explicit nil check\nright in\njust that spot, see\nc5b4a1chttps://github.com/mozilla-services/heka/commit/c5b4a1c959fe6ed7034ca47dd53937db8f4bb5e6.\nI was able to successfully reproduce the original problem locally\nand have\nverified that it's no longer happening for me, so I'm pretty sure\nthat the\nissue has been resolved.\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26430478\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26431683.\n. That does seem odd, but there's probably something different in the way \nthe messages are being structured. Maybe the message is the entire UDP \npacket payload w/ no trailing newline? In any event, you'll probably \nhave to look at exactly what's being sent over the wire to figure out \nwhat's going on.\n\nOn Wed 16 Oct 2013 10:54:06 AM MDT, Mark Melling wrote:\n\nOK, that's better, thanks.\nI can now send data over udp and see that appearing in hekad\nThis is probably something I'm doing wrong, but interestingly, although I\ncan use \"nc\" to send data, it's not quite working as expected when\nforwarding messages using rsyslog.\nSo when I forward syslog messages over tcp and have a config of:\n[TcpInput]\naddress = \":4881\"\nmessage_matcher = \"TRUE\"\nparser_type = \"token\"\ndelimiter = \"\\n\"\nThen everything is good, syslog messages appear in hekad\nBut when I switch to udp with a config of:\n[UdpInput]\naddress = \":4881\"\nmessage_matcher = \"TRUE\"\nparser_type = \"token\"\ndelimiter = \"\\n\"\nI don't see anything in hekad (having checked that they are being sent\nover\nudp from rsyslog). If I change the delimiter to some other character\n(that\nI know will be in the syslog messages), then I will see syslog\nmessages in\nhekad. Seems strange that it would work over tcp with a \"\\n\"\ndelimiter, but\nnot with udp.\nOn 16 October 2013 17:08, Rob Miller notifications@github.com wrote:\n\nAh, right, that's b/c there was a change (unrelated to your issue) to\nthe Lua bindings that require a 'make clean' before the next build\nattempt.\nYou can either run 'make clean' from the build folder, or you can\ndelete the build folder entirely and run \"source build.sh\" from the\nheka checkout root.\nOn Wed 16 Oct 2013 10:03:06 AM MDT, Mark Melling wrote:\n\nSorry Rob, my fault, I hadn't.\nNow when I do a build I get an error:\n./build.sh\n-- sphinx-build was not found, the documentation will not be\ngenerated.\nSubmodule 'docs/source/_themes/mozilla' () registered for path\n'docs/source/_themes/mozilla'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mark/heka/build\n[ 20%] Built target heka-mozsvc-plugins\n[ 20%] Built target whisper-go\n[ 20%] Built target go-uuid\n[ 20%] Built target gomock\n[ 20%] Built target go-simplejson\n[ 20%] Built target raw\n[ 20%] Built target amqp\n[ 20%] Built target go-notify\n[ 20%] Built target toml\n[ 20%] Built target goprotobuf\n[ 20%] Built target slices\n[ 20%] Built target xmlpath\n[ 20%] Built target g2s\n[ 20%] Built target gospec\n[ 20%] Built target sets\n[ 20%] Built target goamz\n[ 20%] Built target GoPackages\n[ 20%] Built target lua-cjson-2_1_0\n[ 20%] Built target lpeg-0_12\n[ 20%] Built target lua-5_1_5\n[ 20%] Built target heka_source\n[ 20%] Built target message_matcher_parser\n[ 20%] Built target mocks\ngithub.com/mozilla-services/heka/sandbox/lua\n/tmp/go-build394740442/\ngithub.com/mozilla-services/heka/sandbox/lua/_obj/lua_sandbox_private.o:\nIn\nfunction `require_library':\nheka/src/\ngithub.com/mozilla-services/heka/sandbox/lua/lua_sandbox_private.c:1027:\nundefined reference to `luaopen_cjson_safe'\ncollect2: ld returned 1 exit status\nmake[2]: * [CMakeFiles/hekad] Error 2\nmake[1]: * [CMakeFiles/hekad.dir/all] Error 2\nOn 16 October 2013 16:49, Rob Miller notifications@github.com\nwrote:\n\nAre you sure you've managed to deploy the latest code from dev?\nYou're\ncrashing at the same place, but there's now an explicit nil check\nright in\njust that spot, see\nc5b4a1c<\n\n\nhttps://github.com/mozilla-services/heka/commit/c5b4a1c959fe6ed7034ca47dd53937db8f4bb5e6\n\n.\n\nI was able to successfully reproduce the original problem locally\nand have\nverified that it's no longer happening for me, so I'm pretty sure\nthat the\nissue has been resolved.\n\u2014\nReply to this email directly or view it on\nGitHub<\n\n\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26430478>\n\n\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n\u2014\nReply to this email directly or view it on GitHub\n<\n\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26431683\n\n.\n\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26432211\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26436587.\n. Done w/ initial review. The comments are mostly nits, except for the race condition inquiry.\n. Awesome, thanks! Except we actually want to merge this to versions/0.4 first, and then to dev. I'll do that merge now.\n. This looks good, other than the doc and folder name notes mentioned above.\n. I think this is a red herring, something that pops up sometimes when a \nmocked interface changes. Try deleting the mock_decoder_test.go file \nand see if the problem persists.\n\nOn Fri 08 Nov 2013 09:02:13 AM PST, Mike Trinkala wrote:\n\n2013/11/08 08:40:56 wrong type of argument 0 to Return for\npipeline.MockDecoderRunner.Decoder: pipeline.MockDecoder is not\nassignable to pipeline.Decoder\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/495.\n. Thanks, reviewing now!\n. Overall this looks great, nice work. :) There are a few things that will need to happen before I can merge, however:\n- [ ] The config change I mentioned in the line note needs to be implemented.\n- [ ] Some relevant tests need to be added to the FileOutput tests in the outputs_test.go file.\n- [ ] The changes need to be recorded in CHANGES.txt.\n- [ ] The documentation needs to be updated, in configuration.rst at least.\n\nThanks again!\n. I think flush_operator using \"AND\" and \"OR\" seems fine. Can't think of anything better, anyway.\n. Trying to do a bit of clean-up in the pull requests. Are you going to be able to make changes based on the notes above? If so, any ETA? Thanks!\n. I do think the feature is worthwhile and would love to merge it, yes. It's not urgent, 2-3 weeks is fine. Writing tests isn't very hard, there are already a lot of existing tests for you to model from, and if you can catch us in the #heka channel on irc.mozilla.org (we're in there most regular workday hours, California time) we'll be happy to help you out.\nThanks!\n. Hi again. Just wanted to let you know that I'm hoping to get this merged soon, so I'm going to make a branch of this to pick up where you left off. Thanks for the contribution!\n. Improved things a bit by picking a few low hanging fruit in 001c6e286a3384e321d5e459c941728ddfbd64c9. Leaving this open b/c @crankycoder is working on cleaning up a few more.\n. Resolves #500.\n. Note: the failing build is only b/c the mozsvc tests are failing. There is a related mozsvc pull request (https://github.com/mozilla-services/heka-mozsvc-plugins/pull/17); merging that should get this build to work again.\n. Overall this looks good, thanks for the contribution! My notes upon review:\n- I made a line note w/ a slight edit to the docs.\n- The tests are failing (see travis link above), and it looks directly related to your change; gomock is expecting the MultiDecoder tests to call LogError(), but it's not happening, probably b/c the LogError config isn't being set up right. It's very strange that you can't run the tests, we'll definitely want to work that out. Are you running 'make test' from within the build directory? What happens if you run ctest? What about go -v test github.com/mozilla-services/heka/pipeline?\n- Thinking more about this, I realize that we probably also want to give folks the option of not deleting the messages that fail parsing, so they can be examined later if necessary. That can be a separate ticket, though, I've opened #530 to capture this.\n. Not sure what's going on with your tests there, I've never seen anything even remotely close to that output. I was able to clone your repo, checkout your branch, and run the tests just fine. They failed, however, with the same failure that was showing up in the original Travis build failure.\nResolving what's wrong with your test setup is definitely the first order of business here. I'm happy to help you debug the issue if you can catch me on IRC. :)\n. r+ on this one w/ the exception of what I think is a comment typo. Double check that and then merge at will. :)\n. Punting on this for now, possibly forever depending on the on-going value of a pure Go message templating system vs the use of sandbox decoders.\n. Related: #541\n. It's not really a priority for us to add alternate parsing code to the whisper output. The issue is that you don't want to write the payload out to ES, right? Using the 'clean' message format, you can set up the ES output to not emit the payload field.\n. Overall this looks great, thanks for the contribution! I made one small slightly bike-sheddy note in line. Also please add an entry to CHANGES.txt. Resubmit when that's done and I'll merge.\n. Going through open pull requests. Any time to make the small change I requested and add a note to the changelog?\n. Heh, I have no interest in making you feel pressure, and this isn't an urgent need; wrapping it up over the holidays is fine. But unless I'm missing something, the change I'm suggesting shouldn't take much time at all. I'm just asking that all of the places where it says return stats, err (or return stats, fmt.Errorf(errFmt, line)) be converted to return nil, err (or return nil, fmt.Errorf(errFmt, line)), so we're not returning an incomplete stats slice when an error has occurred.\nAlso, if you're really overwhelmed and can't pick this up, just let us know and we can take it over. In any case, we appreciate all of your contributions! :)\n. Are you referring to the fact that the 0.4.0 and 0.4.1 packages are using different package names? If so, this is a known but resolved issue, it's actually 0.4.0 that has the wrong name. See http://is.gd/NOkRRw.\nIf (.deb package building novice that I am) I'm misunderstanding this report and it's about something else, then can you please re-open and provide a bit more information on what exactly is the problem?\nThanks!\n. Okay, I think I understand now. What through me was that your original pasted command was already using the different file name, not the heka-0_4_1-linux-amd64.deb in use on our site. I tried running changestool on a downloaded deb file and verified that the file names in the changes output don't match the name of the uploaded file. Reopening, we'll see if we can't coerce cmake into generating the expected file names.\n. I don't know whether or not Mozilla has a long history w/ cmake, I just know that Heka just recently switched to cmake since it's pretty much the only build system that consistently works across Linux, OSX, and Windows. And we've got too many other things to work on for another change of build system to happen right now.\nThere's hope, however. I've opened a pull request (https://github.com/mozilla-services/heka/pull/555) with a kludgey fix that will add a 'deb' target to the Makefile on Linux machines, if dpkg is available. Running make deb instead of make package will generate deb files with the right file name. Unfortunately, make package will still generate debs with the wrong file name, but not much we can do about that.\nIn any event, once my PR is reviewed and merged I'll be able to cut a 0.4.2 release and generate new package files. The next 4 days are a holiday weekend here in the U.S., so it'll likely be Monday or Tuesday before that happens.\n. 0.4.2 release has been cut, deb files now have the expected name.\n. This seems to be a cmake/cpack bug that has been resolved only in the very latest release (see http://public.kitware.com/Bug/view.php?id=13609). Currently trying to get cmake 2.8.12.1 installed on my build machine to see if I can produce fixed rpms.\n. Was able to build a new RPM that successfully installed on a fresh Amazon Linux instance after a) using cmake 2.8.12.1 and b) explicitly excluding the conflicting directories from the spec manifest: ec69a80979ecc4b38c58f2056814757d81e7a55c. Will release 0.4.2 with fixed RPMs soon.\n. 0.4.2 released.\n. You're right that this is related to #550, @whd, but unfortunately for now we can't fail quite as simply w/ the AMQPInput as we can with the TcpInput. There are some cases where an AMQPInput w/o a decoder will work fine, and others where it won't, and we don't know what case we're dealing with until we actually start to process traffic.\nWe can improve the default situation, though, and probably should before 0.5 release. Opened #553 to capture this. In the meantime, we'll better document that you should use ProtobufDecoder if using AMQP as a transport btn Heka instances.\n. Documentation fixes landed in 0.4.2.\n. Yup, this is a docs bug, the decoders are no longer provided. The docs specific to each input were updated to reflect this (see http://hekad.readthedocs.org/en/v0.4.0/configuration.html#tcpinput) but we missed the intro sections where the above quote came from. We'll get that fixed.\n. Docs updated. We'll work on making more sane defaults in 0.5.\n. Ah, nice, thanks for the tips. Changing now.\n. Yes, the MultiDecoder tests should be rewritten to not depend on anything defined outside of the pipeline package, but it didn't seem worth the effort at this moment.\nAs for the all_spec_test files, I think we need to leave them as is, for a couple of reasons:\n- It's not possible to access the *_test.go files in one package from another package, so there would be no way to load a different package's test specs.\n- It's nice to be able to use (for instance)  go test -v github.com/mozilla-services/heka/plugins/amqp to run only the amqp related tests, this only works if the specs are actually defined in that package.\n. Oh, wanted to add a note that the config tests, like the MultiDecoder tests, are in a different package. This is b/c it's obviously much easier to test config when there are plugins actually defined that the tests can configure. This seems to me less egregious than the MultiDecoder issue, might not ever be worth the effort to change, but wanted to call it out regardless.\n. Oh, sorry, I'm with you now. Yes, you're right, putting those in a \nseparate file is probably overkill when there's only a single spec. \nI'll consolidate some of them before I merge.\nOn Mon 02 Dec 2013 10:47:12 AM PST, Mike Trinkala wrote:\n\n556 (comment)\nhttps://github.com/mozilla-services/heka/pull/556#issuecomment-29644027\nThe tests would still live in the package... just wouldn't need two\nfiles for a single test spec is all I meant.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/556#issuecomment-29645332.\n. You caught me before I had pushed the merge of these updates back to the dev branch, it's done in 22805f8.\n. I considered setting it to 7, but thought it might be useful to distinguish btn one that was explicitly set and one that was left as the default value. I don't feel strongly, though, am willing to change.\n\nGood point re: protobuf definition, I'll add it.\n. Thanks so much for the contribution! In addition to the line notes above (which you're awesomely responding to about as quickly as we can give them) we'll need you to update CHANGES.txt changelog and the configuration.rst file in the docs to reflect the changes. An additional test or two to flex the new feature would be great, too.\n. One final note... the http plugin tests are failing b/c they haven't been updated to match the new code. You can see it in the travis results, hopefully you can reproduce the test failures locally. Please find us in #heka if you need help getting the tests running.\n. #578 was closed because we actually are now using the env_version field for something similar to its original intent. Closing this since it was dependent on #578 being implemented.\n. @skurfuerst Sorry for the headaches. The issue you were seeing building v0.5.1 was probably related to the SIGUSR1 stuff. That got merged to dev in #786 but didn't make it to the versions/0.5 branch. If you want to build against the 0.5 tree, you should be able to just copy that file into place to get the build working.\n. Yup, verified that versions/0.5 branch will build if you just copy the pipeline/pipeline_signals_freebsd.go file from the dev branch. Closing this bug, since @skurfuerst can build on freebsd 10 and I was able to build on 9.2.\n. Urgh. Yes, we know that our support for externals in our build is sub-par at the moment. It's not driven by the same use case, but we have another issue open that, if implemented, I think would prove useful here: #393. This is on our list for the next release, should hopefully be implemented within the next handful of weeks. If your current fix is working for you, then I'd keep that in place until we get something more robust rolled into the build. I'll leave this open (and linked to the other issue) so that when we tackle it we'll make sure to keep your use case in mind as well.\n. This is a useful tool, thanks! Some initial thoughts:\n- I know we set the precedent by creating CLI utilities with names like \"flood\" and \"sbmgr\", but I think that we're choosing names that are too generic, the names should indicate that the tool is related to Heka. I'm open to ideas, but \"heka-inject\" is the best I can come up with.\n- This should be documented alongside the flood docs in docs/developing/testing.rst.\n- The tool is already useful so I don't want to block on this one, but it occurs to me that it'd be awesome to be able to have the message definitions come out of a config file instead of having to be specified on the command line each time, similar to how flood works. I don't want to remove the command line flags, just add the ability to save a file full of test messages for distribution and reuse.\nResolve the first two of these and we can merge. Once it's merged, we can open an issue to capture the config file feature request.\n. Hey! So I just manually launched the Travis build and it seems to still be failing. If you fix either the tests or the code so the build is green I'll be happy to review.\n. Did a bit more digging. Turns out the failure is b/c the message text passed in to the testSendMail function doesn't match the expected text. Here is a relevant traceback:\n```\nthor [heka] git:christianvozar-features/smtp-output-subject ~/go/heka/build $ go test -v github.com/mozilla-services/heka/plugins/smtp\n=== RUN TestAllSpecs\n2014/01/22 13:10:15 Missing EXPECT() for invoked function: *pipelinemock.MockOutputRunner.LogError([Expected Received From: heka@localhost.localdomain\nSubject: Heka [SmtpOutput]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr], Received From: heka@localhost.localdomain\nSubject: Heka [SmtpOutput]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr])\n/home/rob/go/heka/build/heka/src/code.google.com/p/gomock/gomock/controller.go:112 (0x50c624)\n    google.com/p/gomock/gomock.(Controller).Call: stack := debug.Stack()\n/home/rob/go/heka/build/heka/src/github.com/mozilla-services/heka/pipelinemock/mock_outputrunner.go:55 (0x5c548f)\n    com/mozilla-services/heka/pipelinemock.(MockOutputRunner).LogError: _m.ctrl.Call(_m, \"LogError\", _param0)\n/home/rob/go/heka/build/heka/src/github.com/mozilla-services/heka/plugins/smtp/smtp_output.go:130 (0x439dd5)\n    com/mozilla-services/heka/plugins/smtp.(*SmtpOutput).Run: or.LogError(err)\n/home/rob/go/heka/build/heka/src/github.com/mozilla-services/heka/plugins/smtp/smtp_output_test.go:97 (0x43ab1b)\n    com/mozilla-services/heka/plugins/smtp.func.002: smtpOutput.Run(oth.MockOutputRunner, oth.MockHelper)\n/usr/local/go/src/pkg/runtime/proc.c:1394 (0x418680)\n    goexit: runtime\u00b7goexit(void)\nexit status 1\nFAIL    github.com/mozilla-services/heka/plugins/smtp   0.007s\n```\nAnd here is some debug output the show the actual difference:\n```\n    len(msg):  197\n    len(results[0]):  207\n    msg:  From: heka@localhost.localdomain\n    Subject: Heka [SmtpOutput]\n    MIME-Version: 1.0\n    Content-Type: text/plain; charset=\"utf-8\"\n    Content-Transfer-Encoding: base64\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr\nresults[0]:  Received From: heka@localhost.localdomain\nSubject: Heka [SmtpOutput]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\n\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr]\nexit status 1\nFAIL    _/home/rob/other/go/heka/plugins/smtp   0.009s\n\n2014/01/22 13:22:49 Missing EXPECT() for invoked function: *pipelinemock.MockOutputRunner.LogError([Expected Received From: heka@localhost.localdomain\nSubject: Heka [SmtpOutput]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\n\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr], Received From: heka@localhost.localdomain\nSubject: Heka [SmtpOutput]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\n\nV3JpdGUgbWUgb3V0IHRvIHRoZSBuZXR3b3Jr])\n/home/rob/go/heka/build/heka/src/code.google.com/p/gomock/gomock/controller.go:112 (0x50ca14)\n    google.com/p/gomock/gomock.(*Controller).Call: stack := debug.Stack()\n/home/rob/go/heka/build/heka/src/github.com/mozilla-services/heka/pipelinemock/mock_outputrunner.go:55 (0x5c587f)\n    com/mozilla-services/heka/pipelinemock.(*MockOutputRunner).LogError: _m.ctrl.Call(_m, \"LogError\", _param0)\n/home/rob/other/go/heka/plugins/smtp/smtp_output.go:130 (0x439dd5)\n    (*SmtpOutput).Run: or.LogError(err)\n/home/rob/other/go/heka/plugins/smtp/smtp_output_test.go:101 (0x43af0b)\n    func.002: smtpOutput.Run(oth.MockOutputRunner, oth.MockHelper)\n/usr/local/go/src/pkg/runtime/proc.c:1394 (0x418680)\n    goexit: runtime\u00b7goexit(void)\n\n\nexit status 1\n\n```\nIn the above output, note that len(msg) (i.e. what we receive) == 197 while len(results[0]) (i.e. what we expect) == 207, and that the difference seems to be that \"Received \" is at the beginning and \"]\" is at the end of the expected value but not the passed in argument.\n. It's not exactly the same, but I think most of the use cases for this are adequately handled by the recently added support for use of environment variables in the TOML. Closing this issue.\n. LogfileInput is now deprecated, and output formatting is now handled by Encoders. The PayloadEncoder is what would be used to spit out any log data in the message payload, and it provides an option to either insert a newline or not.\n. Resolves #483\n. Definitely don't want to change hekad to heka. Auto-complete isn't that big of a deal, I don't want to gratuitously change things out from under people.\n. Okay, this looks good, thanks... just add a note to the 'Backwards Incompatibilities' section of CHANGES.txt and I'll merge.\n. Also, please add a small item to the \"Bug Handling\" section of CHANGES.txt file. :)\n. I won't close this issue just yet, but it's likely that the use cases served by the StatFilter will instead be handled by SandboxFilters once #724 is implemented.\n. Again, thanks for this submission. In addition to the in-line comments made above (and any more that might be the result of further review), before this can be merged we'll need some accompanying edits to the documentation (see configuration.rst) and the changelog (see CHANGES.txt), as well as additional tests that flex the new behavior a bit.\n. Merged (with further refinements) in e107936.\n. Closing this bug b/c it's too vague. There is some progress being made on the deb front, thanks to @mikn (see #1085) we're nearly to the point of having it create a new user and include an init script along w/ working config. If you'd like to request specific improvements be made to specific package formats, please open a new issue. Thanks!\n. r+, once there's a changelog entry.\n. Agreed that this is a horrible kludge, was never intended to be more than a stop-gap measure, and I don't blame you at all for not wanting to merge it.\nBut if we don't merge this then we should probably once again move the location change for the lua_* folders back to $BASE_DIR. As it stands now, our packages install to /usr/share/heka/lua_modules, but the sandbox plugins all default to looking for their modules in $BASE_DIR/lua_modules, so the default setting is essentially broken for all but the most trivial of lua code.\nFor a real solution I propose that we eliminate the base_dir setting and replace it with global lib_dir and cache_dir settings. lib_dir will point to the read-only base (i.e. /usr/share/heka on unix) and cache_dir will point to the writable base (i.e. /var/cache/hekad). We also do away w/ the auto-scanning magic of treating relative paths differently and replace it with explicit interpolation, i.e. users will be expected to specify paths like '${cache_dir}/whisper/...' and '${lib_dir}/dasher' in the config. We update all of the default config options accordingly (so default modules_dir setting for the sandbox plugins becomes '${base_dir}/lua_modules'), and we change GetHekaConfigDir to do interpolation instead of prepending to relative paths.\n. Yes, explicitly setting the modules_path for each plugin is the obvious choice. I was hoping to not need it in what will be the sample config we provide in our AMI, but I'm over that.\n. Is this still happening for you? I can't reproduce.\n. Docs have been updated.\n. Resolves #160.\n. Resolved in #965.\n. Thanks for this contribution! Some notes:\n- As you have it, there's nothing preventing a user from specifying one of the UDP options for a TcpInput, and vice versa. We definitely don't want to allow this.\n- In fact, it might be better to remove the \"net\" setting and replace it with a \"use_ipv6\" boolean, default false.\n- Changes to plugin configuration require corresponding doc changes (in configuration.rst for now, although that page will be broken up into several separate pages soon).\n- All changes require an update to the changelog (CHANGES.txt).\n- Code should be accompanied by tests. A fair number of tests already exist for both the tcp and udp input, so it should be pretty straightforward to add one or two more to exercise your config code. Guidance available in #heka on irc.mozilla.org or on the mailing list (https://mail.mozilla.org/listinfo/heka).\n. Added the flag for Windows and Unix, see #697. Tried to get it working on OSX, but it turns out ld on OSX no longer supports the \"-s\" flag, instead you have to use a separate \"strip\" command that you run on the binary after it's already been generated. On OSX the debug symbols only add about 10% to the binary size (debug = 11M, stripped = 10M), whereas on Linux they add about 50% (debug = 15M, stripped = 10M), so I'm just going to punt on OSX and call this done.\n. Sorry it took so long to review this. Part of the reason was that we've recently improved the TcpOutput to support queuing to disk and reconnecting w/ slow back-off, to be able to handle minor drop-offs w/o data loss, and I was considering how we might structure things so the CarbonOutput could use the same logic.\nIn the end, we've decided that we probably implement some API changes to make this a bit easier, but it won't be happening until after the 0.5 release. For now I'll merge this, since a naive reconnect strategy is still better than no reconnect strategy in most cases.\n. I have not much to add re: direct to stackdriver vs. cloudwatch, but I will say that we almost certainly don't want a stackdriver specific output. AFAICT stackdriver (like countless other services) supports use of HTTP for pushing data into their system. What we really want is a general purpose HTTP output that is flexible enough to support stackdriver, geckoboard, and any other high value use cases we can identify.\n. @christianvozar You're definitely hitting on some of the pain points, but we have alternate plans to resolve these issues. Before the 0.6 release we plan on introducing Encoder plugins, which will be analogous-to-but-the-inverse-of the Decoder plugins we already have. Encoders will take a pack as input and will generate raw bytes as output.\nAnd, as with Decoders, we'll implement a SandboxEncoder that lets you actually do the work in Lua. Rendering arbitrary JSON will be easy, you just pull the data you need out of the message and stuff it in a Lua table (i.e. an associative array, same as a Go map or a Python dictionary) with the structure you want, and call cjson.encode(your_table). Types will work, and it will be much more flexible and much faster than the reflection based stuff that Go has to do.\nThe ultimate goal is to have Input and Output plugins concerned only with the low level details of the transport. All parsing and serialization will be done with Decoder and Encoder plugins. The HttpOutput will still need to be flexible enough to handle the various requirements that our endpoints will demand, but dealing with formatting the data to be shipped to those endpoints will not be within its scope.\n. Need to update CHANGES.rst. Also a test or two, although I realize the SandboxFilter doesn't have a data preservation test. Extra points for adding one for the SandboxFilter as well. ;)\n. Right. From this side we don't need to test any serialization edge cases or anything; those tests belong in lua_sandbox. But it would be nice to verify that setting preserve_data to true actually invokes the serialization and puts the file where we expect it to.\n. Overall this looks pretty good. Need to update CHANGES.rst, and also add some tests to the test spec to show that it's working. There are existing tests that set up an HTTP server and verify that the right request is generated, should be able to use that as a model for setting up a one for a POST request.\n. Resolves #637 \n. I've got a couple of general notes before I make some in-line comments:\n- I'm maybe willing to be convinced otherwise, but I'd rather see this as a ProcessDirectoryInput rather than having it inserted into the global config like you've done. Every other mechanism for inserting data into Heka is encapsulated in an input plugin somehow, don't see why this should be different.\n- It looks to me like the directory structure is being loaded at startup, but then never again. Ideally I think the set of jobs would be reloaded periodically, so you can just drop scripts into the folder at any time and they'll get picked up at the next reload w/o need to restart hekad.\n. We're in agreement there, I just thought of it as an input that configured ProcssInputs, analogous to the relationship btn SandboxFilter and SandboxManagerFilter.\n. I'm picking this up to free Vic for other things, closing this b/c the next one will be opened from a different branch.\n. This looks fine, just need to add a small entry to CHANGES.txt. Thanks!\n. This is a helpful contribution, thanks! Before I can merge it, though, we'll need to add some supporting stuff. First, all changes need to be recorded in the CHANGES.txt changelog. Second, the documentation for the StatAccumInput (i.e. docs/source/config/inputs/stataccum.rst) needs to be updated to include the new configuration option and explain when a user would want to use it. Finally, some tests should be added to verify that the code is doing what it's supposed to do. It should be pretty easy to add a test spec to the existing suite of tests in pipeline/stat_accum_input_test.go.\n. r+, delta a changelog entry.\n. Oops, sorry, I already fixed those typos and merged the other PR, thought you'd have noticed. Sorry about the confusion.\n. Further debugging revealed that the heka_report.json was growing unendingly w/ an ever increasing number of running ScribbleDecoders. Looking at the code and discussing w/ @ipmb, it seems as though the client that is connecting w/ the TcpInput is establishing a new connection for each message sent. Each new connection spins up a new TcpInput goroutine, which in turn spins up a new decoder, but we're never cleaning them up.\nIt's nicer for clients to use connection pooling rather than closing and reopening connections for every message, but I'm sure we'll have to frequently deal w/ misbehaving clients, so we need to start timing out our connections and making sure that the TcpInput and any associated decoder goroutines are shut down if they stay inactive for too long.\n. Update: Yes, the above assessment is correct, I've been able to consistently reproduce the problem locally. I've got a fix that seems to be working, just have to finish up tests and docs, will have a PR ready for review tomorrow.\n. This looks great (can't believe I missed this the first time round :P), just need to update the changelog at CHANGES.txt and the docs at docs/source/tls.rst. Thanks!\n. I noticed that, and actually considered calling it out. I see why you think it's more clear, I'm on the fence whether it's worth it to deviate from the settings that Go uses. And I guess since I'm on the fence I'd lean towards sticking w/ the Go versions. If I thought the changes were clearly far and away better it'd be worth it, but in this case I think it could go either way, so maybe we should change it to match the Go version. Thanks for checking. :)\n. Visibility into subdecoder processing, success, and sampled timing numbers is available now via #917. Information about memory usage isn't possible with current design, but will come when we redesign the reporting infrastructure (see #918). Closing this issue.\n. This looks great, thanks! I just suggested one small change, when that's in I'll be happy to merge.\n. I think it needs to be tweaked, but not necessarily rethought. My current thinking is that we implement the statsd parsing in a decoder (as suggested by #686) and that the StatAccumInput is changed to not be a plugin at all, but simply a StatAccumulator that's always available.\n. It's worth noting that the data representation (i.e. Stat structs vs. Message structs), the routing mechanism (i.e. handed to the Router vs. handed directly to a StatAccumulator), and the API that we expose to the Lua sandbox are largely separate concerns. We won't start sending Stat structs through the Router, of course, but other than that we have a lot of flexibility. It's not a problem to have inject_message special case either the data structure or the routing of a particular message type, for instance.\n. Meant to cut this from the versions/0.5 branch and not dev, fixing now, new PR coming soon.\n. It can't be in /tmp b/c some data that is written there needs to actually stick around through restarts and such. We can work on getting the RPM to create a heka user and setting the directory ownerships correctly, though.\n. @levb It is explicitly configurable. Both base_dir and share_dir are settable in the [hekad] section of your config. On my phone right now or I'd link to the docs, but pretty sure it's in there in the \"global\" section. \n. @mreid-moz .heka-cache is a fine ad-hoc convention, but an rpm package should generally put things in established system locations, imo. I'm still inclined to add a heka user and set the folder privs appropriately.\n. Urgh, didn't mean to close the issue, and meant to say 'deb' instead of 'rpm', although the same principle applies to both. Clearly I shouldn't be trying to github from small handheld devices. :P\nAlso, checked and found that base_dir and share_dir are indeed documented at the bottom of the global config section: http://hekad.readthedocs.org/en/latest/config/index.html#global-configuration-options \n. Hrm. Can't figure out how you might have ended up with that in your file hash from looking at the code. Is it consistently reproducible when you run the same files through the same config? If so, could we get a copy of the files to try to reproduce ourselves?\n. Nope, not at the moment, any work you want to put towards it would be awesome. This is a big enough change that I'd want it to land on the dev branch for eventual inclusion in 0.6, rather than on versions/0.5 which is just for bug fixes at this point.\n. @whd Do we know if this was ever resolved? Is this still rearing its ugly head?\n. \\o/\n. Just a note to make it explicit that the solution implemented in PR #751 is not intended to resolve this issue. I'm +1 to only loading files of a specific extension (or set of extensions, or...), I just thought there was enough pain here to want a fix in 0.5.1, and didn't want to make a breaking change in such a minor bugfix release.\n. Hrm. I can't reproduce this on my local machine. Does this happen in every case where there's not a listening ES cluster, or does it only happen under certain circumstances?\n. Oh, DNS failing to resolve is promising. I'll give that a go.\n. Still can't reproduce. I've tried using host names that don't exist, verifiying that my DNS wasn't resolving them. I've even tried using an EC2 instance that was stopped. In every case Heka shuts down cleanly. ElasticSearch might be a red herring. Can you reliably reproduce? If so, we might need to get your full config.\n. I haven't been able to reproduce, and the timeout that was added in #769 may have resolved this. Closing until and unless this pops up again.\n. Agreed. Especially since ElasticSearch actually has a built in mechanism for handling this, where you set up a single 'non-data' node that handles all of the HTTP requests, which in turn delegates to a cluster of data nodes that will handle all of the indexing (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-node.html). We're using this internally at Mozilla, it's all behaving as advertised.\n. Also, I'd be very grateful if you could make this fix against the versions/0.5 branch so we can get it in to v0.5.2. :)\n. Re-implemented this fix against the versions/0.5 branch so it will land in 0.5.2.\n. Urgh, forgot to update the docs.\n. Also, the commit history on this PR is a bit whack. I'd be very grateful if you could fix it up so there aren't any extraneous commits attached to it, ideally just a single commit that reflects the diff btn dev and your branch. Thanks!\n. Merged into versions/0.5 in #775.\n. Further clarification: The upstream module introduces new embedded variables which can be used in nginx's log_format specification, see http://nginx.org/en/docs/http/ngx_http_upstream_module.html#variables. The request is that the nginx parsing grammar to grow support for these variables.\n. Closed in #782.\n. Overall this patch is great, my only in-line notes are about naming conventions and making sure the docs are clear that TLS isn't yet supported. Also, I think it might be worth it to add some validation code to the Init() method to return an error if someone tries to specify a local_address value if use_tls is set to true.\n. No, rebasing not required.\n. Overall this looks good, thanks for the contribution! In addition to the better contextualized error messages that I mention in my other comment, there's one other issue that comes up for me. Is it absolutely required that the pid file path be specified on the command line, or would it be okay to instead have it be a config option in the global [hekad] config section?\nIf the config option is okay, I'd much rather go that route. It wouldn't be a lot of work, you'd just add the option to the HekadConfig struct in https://github.com/mozilla-services/heka/blob/dev/cmd/hekad/config.go, which is exposed in main.go as the config variable. We already have a ticket open for allowing config options to be overridden by command line flags (#594) so once that was resolved we would regain the ability to specify the pid file on the command line.\nFinally, I see your note that you didn't modify CHANGES.txt. In addition to recording your work there, any command line or config changes need to be recorded in the documentation. Feel free to ask if you need help finding the right docs to update or need help making the necessary changes.\nThanks again!\n. This all looks great! Only thing I missed last time around is that you should add the license block (w/ (c) 2014 and yourself as the contributor, of course) to the top of the new go files you introduced. Be careful to leave a blank line btn the last line in the license block and the package declaration so the license block doesn't show up as package documentation in godoc. :)\n. Thanks! The files aren't actually all identical, the windows file is slightly different b/c syscall.SIGUSR1 doesn't exist on windows.\n. This doesn't seem to be an ongoing issue, not sure Heka should be changed at all, closing.\n. @mmerickel Not sure why you have the impression that there's resistance to this feature. I'm +1 to adding it, and I don't think anyone is strongly against it. I understand your desire to skip the filesystem, I was just inquiring as to whether or not you could use UDP over the loopback interface rather than through a datagram socket, since the former is working today and the latter isn't.\nRegardless, this is a reasonable feature request and one that shouldn't be terribly difficult to add. If you're interested in tackling it we'd be happy to have your contribution. If not, then we'll get to it when we can. Unfortunately some other priorities are on our plate right now, so it'd probably be at least a week or two before we get to it.\n. I know you've marked it as a work in progress, but I assume you opened the PR so you could get some eyeballs on it, so I'm going to go through and make comments on what you've got so far. Thanks!\n. Closing this since it's been superseded by #1085.\n. Documentation pull requests are welcome, but since we have such an extensive set of plugins that can be used as examples this isn't very high on our priority list, sorry. \n. This looks great overall, just the one small readability nit.\n. It's also worth nothing that, once you've got your environment set up, you can using 'go build', you just have to use the Go import path instead of the filesystem path.\nIn other words, after you've run \"source build.sh\" once, the following will work:\n$ go build github.com/mozilla-services/heka/cmd/hekad\nThe following, however, will not:\n$ go build /path/to/heka/build/heka/src/github.com/mozilla-services/heka/cmd/hekad/main.go\nUnfortunately, go run only supports the filesystem spelling, it doesn't work with the import path. So the workaround for avoiding the full source build.sh dance is to do this:\n$ go build github.com/mozilla-services/heka/cmd/hekad\n$ ./hekad\nClosing this issue b/c, nice as it would be, it's not a priority for us to spend time getting go run working.\n. This looks great, thanks! Would you mind adding a note to the bug fix section of CHANGES.txt?\n. Oof, hate to bounce it back to you again, but this PR is for the dev branch, which will eventually become the 0.6 release, but you updated the changelog for 0.5.2. Can you move the entry to the 0.6 section?\n. We don't have any Kafka plugins in the core, but Shopify has written a Kafka client for Go called Sarama (https://github.com/Shopify/sarama), and some other folks have used that to write a simple KafkaOutput for Heka (https://github.com/genx7up/heka-kafka).\n. Sorry it took so long for me to review this. This looks great, the only thing it needs is an entry in the 'Features' section for 0.6 in CHANGES.txt file. Thanks!\n. Currently both the ElasticSearchOutput and NagiosOutput plugins make use of HTTP as a way to ship data to an external source, so if you need something very quickly you can look at those for guidance. We do want to have a general purpose HttpOutput, however, especially now that we will have Encoder plugins to make it easy to generate arbitrary JSON from Heka messages.\nGetting a generic HttpOutput right requires some care, though. We want to make sure that it provides enough flexibility to work in most use cases, but that it is still simple enough to be approachable in the basic cases. I plan on tackling this pretty soon, definitely want it to land in a 0.6 release.\n. Also HipChat, per IRC request (https://www.hipchat.com/docs/api). \n. @secretmike Noted, although w/ the caveat that I don't want to significantly delay the simpler use cases with a static URL, so we might iterate by supporting static URLs first and adding dynamic URLs in a second pass.\n. Basic HttpOutput supporting only static URLs and request headers defined in the TOML config merged in #941. Batching can be achieved for now by use of a cooperating filter plugin, if desired. More features coming soon.\n. @whd Is this issue still happening?\n. Confirmed that this is still happening. The issue seems to be that when the AMQP connection drops, the AMQPOutput gets unrecoverable wedged. First step is to fix the output so it fails more gracefully, exiting when the connection drops so the plugin retry feature can be used to try to re-establish the connection. This will bridge the gap until the point where buffering is available to all outputs.\n. Also an entry needs to be added to the changelog in the features section for 0.6. \n. Thanks! Can you add a small note in CHANGES.txt to the 'bug fixes' section for 0.6?\n. Hrm. Generally we haven't done auto-escaping like this, but instead we've extended the TOML implementation so that single quoted strings will be treated as raw strings. I think, then, that the original configuration wouldn't cause a panic if you had done the following:\nini\n[ZST Request Log]\ntype = \"LogstreamerInput\"\nlog_directory = 'D:/NSX/'\nfile_match = \"zst_requests.log\"\nWe've also filed an issue against the TOML spec to have this accepted as a part of the standard, see https://github.com/mojombo/toml/issues/188. Unfortunately that ticket has been open for over a year w/o any activity, so it's not clear if it will actually be accepted.\n. Oof. Bummer. Okay, merging.\n. Resolved in #1296 & #1380.\n. Resolved in #845.\n. I was working on this in parallel, have an equivalent fix along w/ a set of SandboxDecoder tests in a separate branch, PR coming soon.\n. Looks like the actual go binary isn't on your PATH. It will probably \nwork if you run:\n$ export PATH=\"/usr/local/go/bin:$PATH\"\nand then try again.\n-r\nOn Sat 17 May 2014 10:40:52 AM PDT, Gaurav Pruthi wrote:\n\nGetting err while installing heka\npruthi@pruthi-linux:~/heka$ source build.sh\nCMake Error at\n/usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:91\n(MESSAGE):\nCould NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH) (Required\nis at\nleast version \"1.2\")\nCall Stack (most recent call first):\n/usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:252\n(_FPHSA_FAILURE_MESSAGE)\ncmake/FindGo.cmake:32 (find_package_handle_standard_args)\nCMakeLists.txt:23 (find_package)\n-- Configuring incomplete, errors occurred!\nmake: *** No targets specified and no makefile found. Stop.\nFollowing are the env variables:\npruthi@pruthi-linux:~/heka/build$ env | grep GO\nGO_PLATFORM=linux\nGOBIN=/home/pruthi/heka/build/heka/bin\nGOARCH=i386\nGOROOT=/usr/local/go\nGOOS=linux\nGO_ARCH=i386\nGOPATH=/home/pruthi/heka/build/heka\nGOPLATFORM=linux\nGO_VERSION=1.2\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/849.\n. This still looks like go isn't on your path. Please verify that typing go version on the command line, without specifying a full path, will work. If it does work, then from the same shell immediately try to run source build.sh. AFAIK the only way you can get the error you're seeing is if a go binary can't be found on the executable path.\n. Apologies for dropping the ball on responding to this. No, there's no reason for not allowing gauges to be floats, other than lack of awareness of the original author of this code (i.e. me). I'd be happy to give this a review if it gets opened as a PR. Please don't forget to update the CHANGES.txt file.\n\nThanks!\n. The output is in protobuf format, but it's got a header (which is also protobuf encoded) and some framing. Lets say you've loaded a Heka data stream into a buf variable, the following should work:\n``` python\nfrom heka.message_pb2 import Header, Message\nimport struct\nbuf = \"\"\nbuf[0] contains delimiter\nbuf[1] contains header length\nheader.message_length contains message length\nh_len_str = buf[1]\nh_len = struct.unpack(\"B\", h_len_str)[0]\nh_buf = buf[2:h_len+2]\nhdr = Header()\nhdr.ParseFromString(h_buf)  # now you have a populated header object\nm_buf = buf[h_len+3:h_len+3+hdr.message_length]\nmsg = Message()\nmsg.ParseFromString(m_buf) # now you have a populated message object\n```\nThis gives you the first message and header in the stream. The next header starts just after, so in the example above you could set buf = buf[h_len+hdr.message_length+4:] and start all over.\nThis code is only partially tested, but hopefully it gets you going.\n. Fleshed out the message documentation to include a bit of info about the header and the framing of our streams, see http://hekad.readthedocs.org/en/latest/message/index.html#protobuf-stream-framing. That seems sufficient to me for closing this issue.\n. Another possible option would be to do the zlib (de)compression in Go instead of Lua. How easy or viable that is would depend on the exact use cases. It'd be pretty easy to implement an unzipping decoder, for instance, and a MultiDecoder could be used to then do additional Lua based processing. MultiDecoder comes w/ a performance cost, though, so this might not be ideal if you need really high throughput. (Although we haven't profiled what exactly is causing the MultiDecoder to slow things down, it might be easy to improve.)\n. Probably worth doing some profiling to get an idea re: where the time is being spent. There might be some low-hanging fruit fixes that would significantly improve MultiDecoder throughput.\n. And yes, good point re: sandbox usage, but there are still times when writing Go code is a reasonable choice. Not trying to dissuade you from the Lua zlib stuff altogether, just making sure all the options are being considered. :)\n. Oof. Unfortunately, Github's releases page automatically includes zip and tarball file archives of the source code, but our build system doesn't work unless you actually have a git clone. IOW, those source downloads on the release page aren't going to work.\nYou can get a working checkout to build the v0.5.2 release w/ the following steps:\n$ git clone https://github.com/mozilla-services/heka.git\n$ cd heka\n$ git checkout v0.5.2\nThen you should be able to continue with the source build.sh command as described in the install documentation.\n. If you'd like to omit any of the default plugins when compiling Heka you can remove or comment out the appropriate import line from the cmd/hekad/main.go file (https://github.com/mozilla-services/heka/blob/dev/cmd/hekad/main.go).\nThis will make the size of the generated binary slightly smaller, and it will probably cause the resident size of the running executable to be a bit less, but it will likely not impact Heka's performance at all.\nHope this helps.\n. The embedded JSON use case makes sense, we're interested in getting this to land, but I'm afraid your timing on this PR is less than ideal. The MessageFormatter stuff is going away, to be replaced by the newly introduced Encoder plugins, see https://mail.mozilla.org/pipermail/heka/2014-May/000111.html.\nA reworking of the current \"clean\" and \"logstash_v1\" formatters into Encoder plugins should be landing on the dev branch tomorrow. Once that's in you can give this another go.\nIt should also be noted that there will be more changes coming soon, see #865. In fact, it's not unlikely that the Go implementation of these encoders will be supplanted by Lua code running in a SandboxEncoder. Even so, it's good for this to land, with tests, so we can know to replicate embedded JSON support in any new implementation.\n. Don't quite understand what this issue is asking for. The way ElasticSearch formatting is happening is changing significantly from v0.5.2 to v0.6 (see https://mail.mozilla.org/pipermail/heka/2014-June/000119.html), and indeed the lines on the dev branch linked above are no longer what they were when this issue was created. Closing the issue, please reopen another with a better description of what is being requested if the issue isn't resolved by the ES formatting overhaul. Thanks!\n. Agreed. It's worth noting that you can technically achieve what you want already by using a SandboxEncoder, you'll just have to write a small amount of Lua code to extract the data you want to use from the message object. Leaving this open, though, until we decide how we want to handle this in the long run. See also #865.\n. In case you do decide to use a SandboxEncoder, here's an example for reference: https://github.com/mozilla-services/heka/blob/dev/sandbox/lua/encoders/es_payload.lua\nThis encoder expects the JSON that is being sent to ElasticSearch to be in the incoming message's payload, so all it's doing is extracting the payload and then prepending the appropriate BulkAPI indexing information. In your case, instead of just grabbing the message payload, you would call read_message as many times as needed to extract all of the information you want to get, populating a Lua table with the results. Then you'd call cjson.encode to convert the Lua table to the JSON you want to output, finally prepending the BulkAPI indexing JSON just like the one linked above.\nHope this helps!\n. @brokenjacobs We don't support this directly right now, but it doesn't look like it'd be too terribly hard to do. I know I've seen this request before, but I can't find an open ticket for it. Is there one that search isn't finding? If not, can you open a new issue so we don't lose the request?\n. Okay, I made a bunch of inline comments pointing out specific issues. From a bigger picture perspective, I think a production ready IRC output is going to need to be quite a bit more robust than this. Instead of just stuffing commands out over a connection and hoping they succeed, we'll want to be listening to the server responses and verifying that things are actually working. This is tricky due to the async nature of IRC communication. Also, we'll want to handle error cases intelligently. Since folks will likely be using this for important notifications, we won't want to just drop messages on the floor b/c the connection faltered for a second or two. It probably makes sense to make use of the BufferedOutput to store the messages in a disk queue before sending them out the way the TcpOutput does.\nI'm still open to merging this, eventually, but there's a fair amount of work to be done yet, so it makes sense to close it for now and you can reopen it if and when you're ready for another round of review.\nP.S.: Thanks so much for your contribution. I know you're just getting started w/ Go, hopefully I haven't demoralized you w/ all of my comments and nitpicking. I've been thorough so you can have as much feedback as possible re: how to improve. Also, I should probably mention that an IRC output seems deceptively simple, but to do it well actually requires a fair amount of care.\nHope this is useful!\n. Yes, closing since the desired behaviour can be achieved w/ ElasticSearch config.\n. Overall this looks great, thanks! I made one small note, the only other thing I'd ask is that you add a note to CHANGES.txt, and add yourself to the contributors list at the top of the source files.\n. Resolves #875.\n. Yup, typo bug. Thanks for the fix. :) Do you mind adding an entry to the 'Bug handling' section for version 0.6 in the CHANGES.txt file? \n. Cool, so if you add the entry to the CHANGES.txt file, commit it, and push it up to your repo, it will show up in this pull request. Once that's in I'll merge this back to the main repo.\n. Plus maybe a changelog note re: the bugfix.\n. Yup, heka-docs was removed b/c it was stale and of marginal use. Removed the link from the README. Thanks for noticing!\n. Also need to add a note to the \"Features\" section of the CHANGES.txt file for the 0.6 release notes.\n. When running w/ source or ., the script is executed by and in the invoking shell's environment, AFAIK the script itself has no say about what shell can be used. I think we should identify which shells support which syntax and make note of it in the docs.\n. There's nothing actionable about this issue, so I'm closing it. As @trink indicates, there's no need to add an EncodedPayload field to the message b/c the output generated by an encoder is meant to be delivered to the outside world by an output plugin, there's no need for it to be written back to the message object. We may at some point change the payload from string to []byte, but that would be for performance reasons (i.e. to reduce mem copies) and not related to encoding.\nIn general, questions such as this are better asked on the mailing list than on the issue tracker. Thanks!\n. Okay, after digging into this, trink and I realized that we were going about it slightly the wrong way. We're changing the contract, so now encoder plugins should never do any stream framing, and should always only return the bytes representing the serialization of a single message.\nFraming will now be handled by the outputs. Every output will support a use_framing config option. And instead of accessing the encoder directly, outputs will call an Encode method that is being added to the OutputRunner interface. The OutputRunner will use the encoder to generate the raw message bytes, and if use_framing was set to true it will add the stream framing.\n@ianneub This is relevant to your use cases, calling you out here to make sure you don't miss this.\n. Resolved in #928.\n. Also would probably be a good idea to have the output expect [][]byte (rather than just []byte) to support cases where an encoder needs to generate multiple output blobs, such as when the amount of data being processed is too big for a single cjson serialization pass (see comments on #1037).\n. This looks good, but it seems like with just a bit more effort we could support all of the field types and not just string. Need a writeRawArrayField analog to writeStringArrayField, and to repeat the conversion from field.GetValue().(<type>) to field.GetValue<type>() (w/ the associated length check and etc.) for the other conditions. :)\n. Apologies for taking so long to review this, it slipped through my mental cracks. Looking now.\n. Similar to and related to #944.\n. Resolved in #1039.\n. I think this is a bad idea, in that it will be a much bigger support headache dealing with new Heka users who don't understand why their text files have weird characters in them than a timesaver from adding a line to our own configs.\n. I've seen a handful of folks who's first attempt at getting started w/ Heka is having it read a file in on one side and write the same file out on the other, just to verify that it's working.\n. I agree with you that the current implementation is not flexible enough. And I agree that we should provide a mechanism the output to extract request data from the encoder's byte array.\nI disagree with moving the current options into a Go encoder implementation, though. That provides no benefit and in fact makes things less flexible. How does the Go encoder know what data it's supposed to be extracting from the messages?\nI also agree that the batching is suboptimal. I only added it so that this output could possibly serve as a replacement to the ElasticSearchOutput, which uses an identical batching mechanism. With ES, as with many other cases, batching is less about HTTP traffic than it is reducing load on the listening service. For batching we need to consider how much of it we want to support within the output / encoder, as opposed to just telling folks to use filters to do batching. AFAICT there aren't any batching operations that couldn't be achieved by having a filter do the work, occasionally emitting single messages w/ the entire request contents in the payload. My only concern w/ that approach is the message size limit.\nFinally, after considering this further, I'm back to thinking that we should go ahead and land this in 0.6. I think that, in the static URL and headers case, having the settings in the output is the right choice. Thus adding the ability for the request info to come from the encoded data would be an additive change, not a breaking change. I can remove the batching feature from the output, which will force people to use a filter for batching unless and until we provide a better solution.\n. Is this still happening? If so, is rsync being used to append to the log files? If yes, then the workaround might be to use rsync --inplace; for more details please see this thread: https://mail.mozilla.org/pipermail/heka/2014-August/000172.html\n. Apparently my sacrifice of 3 chickens and a goat to the Logstreamer gods was sufficient.\n. I don't understand. The Windows batch file is executed, not sourced into the env like it is on Linux and OSX. Does an analogous change to the Windows file even exist? \n. Okay, first pass of the review is done. I made a bunch of comments, but except for the JoinAndPart stuff all of them are small nits, overall this is very nice work. Thanks!\n. Need to add a note to CHANGES.txt, and unfortunately due to sphinx weirdness we need to add a reference to the new doc in index_noref.rst as well as index.rst.\n. Resolved in #968.\n. Looks great, thanks for the clean-up!\n. The first option that you're considering is a no-go; we definitely don't want to be pushing the message objects further along. In fact, we're considering changing the API so that outputs don't receive messages at all, but instead only get []byte slices directly from the encoder (see #930).\nYou're right that the data is raw bytes by that point, but the receiver is careful to only place full message encodings into the slices that go out over the batchChan. One possibility would be to make sure that the committer never writes a partial batch, since the end of a batch will always correspond to the end of a message.\nIf you really want the file writing batch size to be of a finer grain than the batchChan batch size (i.e. you really want to be able to write partial batches out to disk) then you'll need to use framing. You could consider using the same framing that we use elsewhere, described here and implemented here. I'd lean towards the first choice, myself, to avoid all of the extra mem copies that the use of framing would likely introduce.\n. We do plan on adding this feature, but we're going to keep it simple, supporting rotation based on time intervals only, emitting files with timestamps embedded in the filename.\n. Probably not, alas... I believe @4r9h might take a look at this soon, but I don't know what the timing will be, and there are other issues that are higher priority that we're currently tackling.\n. Targeting a 0.9 release by then end of next week, or first week of Feb at the latest.\n. Here are my thoughts on how this should work:\n- We support a limited number of time intervals: hourly, every 4 hours, every 12 hours, or daily. I'm not married to that exact set, but what's important is that an interval goes evenly into 24 hours.\n- If you use rotation, regardless of when you start Heka, the files will be named relative to midnight of the day. So if you pick hourly, file rotation happens on the hour, you don't end up rotating files at 42 minutes after every hour, or something like that. If you pick every 4 hours, it happens at midnight, 4am, 8am, 12pm, 4pm, 8pm, etc.\n- If Heka starts and a file already exists for the current interval, the existing file should be appended to.\n- If rotation is in use, then the output file name can support Go's time.Format syntax to embed timestamps in the filename. This should allow folks to do nested folders just by putting a path separator in the filename. So to achieve what @bbinet asked for above, you'd use a setting of path = \"2006/01/02.log\".\nI think this is useful enough w/o being so heavy as to force us to add a new dependency for parsing cron format, etc. Does this all make sense?\n. Yeah, we rescan for existing directories. Mainly that's to support new folders showing up inside the root, but it used to work for the root folder (I tested) and I don't see any reason to change the behavior. Should work fine to check for the doesn't exist error and then reset the logRoot value if that's what we have, fail out if it's some other error. Then we just need to make sure the symlink check happens in the rescan code as well.\nI would have just fixed it myself but I found this error when I was testing my (nearly finished) globals restructuring, which is a conceptually small but lines-of-diff huge change, didn't want to mix this in with that.\n. Thanks. This should be resolved as of #1388.\n. +1, maybe just git mv sandbox_plugins_test.go all_specs_test.go since the spec declarations and getTestMessage are all that's left, and that's what we've named similar files everywhere else.\n. Realized it was probably better to render byte fields as base64 encoded instead of as a string, and I needed to add tests. Will be reopen soon.\n. You're using the 0.6 release, I'm guessing? The AMQPInput settings were changed to use underscores in the config names (like the rest of the Heka plugins do) after the 0.6 release, the spelling you want is here: http://hekad.readthedocs.org/en/v0.6.0/config/inputs/index.html#amqpinput\nAlso, while I had already specified that the hekad.readthedocs.org site should default to showing the 0.6 docs, for some reason this wasn't working and the latest dev docs were being shown instead, which is probably what tripped you up. I've since respecified that 0.6 should be the default, which seems to now be working again. Sorry for the confusion.\n. Yes, thank you. We've had reports of similar issues before, but I was never able to reproduce them. Thanks to your clear instructions I'm able to reproduce this issue on the 0.6 release. I'll do further testing and will work on getting this fixed up immediately.\n. Closed in #1014.\n. Okay, made all the in-line comments that I have to make. Other than that, this needs to be recorded in the changelog, and the docs need to be updated to reflect the new behavior. This will include putting the can_exit option in the Common Filter Parameters and Common Output Parameters sections, as well as browsing through the docs to find places where plugin restarting and exiting behavior is mentioned, to make sure that any statements correctly reflect the new reality.\n. Closed in #1050.\n. Hrm... the motiviation for this is to make it easy for folks to have a matcher of \"Logger != 'heka'\" to avoid all internal heka messages, for some definition of \"internal\". I'm not sure I'd draw the lines in the same place as you have.\nSpecifically, I think that all of the sandbox related messages (heka.control.sandbox, heka.sandbox_), and the heka.terminated messages should have Logger == \"heka\", with the data you put in the Logger value as an additional field. The counter-output, statmetric, and various http_ messages I agree should not have \"heka\" as the Logger value. For counter-output, though, I'm -1 on having two different types. By far the most common use case is to have the counter data sent to stdout, and it's awkward to have to specify two types.\n. I still lean towards practicality over purity w/ the CounterFilter's message types, I don't think the change is actually worth the config breakage it introduces. I won't block it, though; I'm okay w/ changing as long as we can use the Logger as the matcher and don't have to start using compound message matchers to catch the output from a single counter.\nI also still think that in most cases people will want to be skipping the sandbox output messages when they're omitting messages, but I can live with message_matcher == \"Logger != 'heka' && Type != 'sandbox.output'\". \n. This is something we'd like to ultimately support, but there are no concrete plans in place currently. We've got some ideas on how to get much better performance out of a single node, we'll probably explore those avenues first.\nAs for running multiple applications through Heka, depending on what the data is, how much is being generated, and countless other variables it may be fine to have several apps feeding a single Heka instance, or it may make sense to have different data going to different upstream Heka instances. At Mozilla we tend to take a tiered approach, where we have Heka running on every endpoint node, gathering system and application data from the node and performing the first level of processing and aggregation. These endpoint nodes will then feed into an aggregator that serves the data center / availability zone, which performs the next level of aggregation. And these in turn feed into a global aggregator. Sometimes these aggregators can be shared across multiple apps, but for higher volume apps (e.g. Firefox Accounts) they'll have their own dedicated aggregators.\nI'm closing this ticket b/c there isn't really anything actionable here. Hopefully this is helpful. If you have specific questions or would like suggestions re: how to tackle your particular architecture, and whether or not Heka is a fit for it, please feel free to ask either in the #heka channel on irc.mozilla.org or the mailing list. Thanks! \n. This might prove useful, not worth removing.\n. In order for this to work you'll also have to change the import path from \"github.com/ecnahc515/go-ircevent\" to \"github.com/thoj/go-ircevent\" wherever it occurs in the code.\n. Is this still an issue?\n. @jdoranster You're right that it's an obvious one, but apparently you have the unfortunate distinction of being bitten by it. We definitely plan on resolving this soon, but unfortunately there are a couple of other tasks we have to complete before we'll be able to tackle it.\nYour proposed workaround certainly seems reasonable. You want to try it out and let us know whether or not it works for you?\n. Made two comments (one of which you've already addressed) and also was thinking that it'd make sense to add a note to the docs to mention that password should be included in the channel name for protected channels, maybe use one in the example.\n. I think I've fixed this one, although there are definitely others. Debugging them is hard b/c they only happen on Travis, and only intermittently, so it's not easy to figure out if a particular change actually resolved an issue. I'll close this one and will reopen if it shows up again.\n. r+ on this too. Not merging in case you want to update the lua_sandbox SHA.\n. Resolved in #1052.\n. Sorry, this isn't something we're going to tackle. Doing so would add a great deal of complexity to the code that handles the message_fields parsing. It is already possible to achieve what you're trying to do with the use of SandboxDecoder and a little bit of Lua code. If you'd like some help figuring out how to do so, please feel free to ask for assistance in the #heka channel on irc.mozilla.org, or on the Heka mailing list: https://mail.mozilla.org/listinfo/heka\n. This looks great... maybe one test to demonstrate that it works in plugins/config_test.go?\n. Actually, we do have config tests, it's just that they're not in the pipeline folder b/c in order to test more realistic configs we need to be able to refer to plugins that are defined in the plugins package. You can see our existing tests in the config_test.go file. \n. The test TOML files that we use in our config tests can be found in the plugins/testsupport folder.\n. Overall this looks great! Made some small nit-picky comments. Chance's point about tests is valid, as mentioned you should be able to add those pretty easily to plugins/config_test.go, feel free to ping us in IRC if you need help there. Also, please add this as a bug fix to the 0.7.1 section in CHANGES.txt, and add yourself to the contributors list at the top of the files that you touch. :)\n. Incorporated these notes, cleaned it up a bit more, wired up docs and changelog, and submitted in new PR (#1079).\n. Made a couple of notes about choosing better variable / method names; fix those, resolve the merge conflicts that have come up, and add a note to the changelog, and this will be ready to merge.\n. Yes, this is still happening: https://travis-ci.org/mozilla-services/heka/builds/81886220\n. I noticed this and originally wanted to preserve the original behavior, but it turned out to be more trouble than it was worth. I agree that we probably should have put a note about this in the breaking changes section of the changelog, however. Leaving this open until we get that added.\n. Overall this looks pretty good, nice work! I made all the comments I have to make on the first pass.\n. Fixed in #1070. Thanks for the report!\n. Hi @klauspost \nThanks for your contribution. As you might have already noticed, I took your code and improved it a bit; improved performance, adhered to field naming standards we use elsewhere, added tests and docs, etc. For a few reasons, however, we've decided not to include this in the core. One of the main reasons is that you can already accomplish everything this encoder can do, and more, by writing a small amount of Lua code and using a SandboxEncoder (see https://github.com/mozilla-services/heka/blob/dev/sandbox/lua/encoders/alert.lua for a simple example).\nI still think this is a useful plugin, though, and think it would make sense to package it up in its own repo in such a way that it can easily be built into Heka by anyone who would like to do so. Is it okay with you if I package it up like that?\n. Merged in 7aebf25ef1c82595918375e55394ac7b12d87c38.\n. In response to your other question, the two cases are significantly different. You would add a new filter to the configuration if you want to get another, separate graph showing up in the dashboard. You would add new stats and stat_labels to the config if you want to add additional statistics to the same graph.\n. Yes, that text seems right, and yes, you're correct that it doesn't matter whether or not there is overlap.\n. Merged in 7aebf25ef1c82595918375e55394ac7b12d87c38.\n. Unfortunately, this is not feasible. In some cases the config options will impact whether or not the preservation_version should be updated, but changes to the Lua code also might impact the preservation_version, and there's no reliable way for us to be able to figure out whether or not the shape of the data has changed.\nAn incomplete version of this functionality might be possible, but tackling this isn't really on our current trajectory.\n. It's not completely abandoned, but it's not the highest priority for us right now, and it sems that @ecnahc515 no longer has time to work on it, so it's being tabled for now.\n. @bbinet No, the statmetric_influx encoder is meant to handle the heka.statmetric messages generated by the StatAccumInput. It expects to find statsd-style time series data in the specific format that the StatAccumInput generates when emit_in_fields = true is set in the config, and generates a new entry in InfluxDB for each data point. This encoder, on the other hand, makes no assumptions about what is in the message, it just iterates through all of the message fields and formats them all into a single InfluxDB entry.\n. @bbinet You're right that a single HTTP request does usually generate multiple metrics, but then again a single HTTP request is usually represented in Heka as a single Heka message containing all of the relevant data. If you were using this encoder, all of the data from the request would be extracted from that single message and sent through to InfluxDB in a single request. There's still value in supporting additional batch, of course. Currently you could handle some use cases by using a filter to accumulate and format data and then periodically emitting an aggregate message containing the entire batch, which would then be picked up by an HttpOutput. You wouldn't use this encoder in that case, however.\n@mikn Yes, those are good points. You could always tweak the encoder source to add any custom behavior, of course, but those are probably generally useful enough that it's worth adding them to the encoder config. I'll add that behavior to this implementation.\n. @bbinet This isn't the best venue for having this conversation. I'll reply one last time, but please bring any further questions to the mailing list. :) Yes, you'll need a custom filter, although someone is working on something that might be considered a generic batching filter:  https://github.com/michaelgibson/heka-protobuf-filter\nIt's not ready for prime time just yet, though. A custom filter wouldn't be hard to write. You wouldn't need a different InfluxDB output. In fact, we don't even have an InfluxDB output, we just have an HttpOutput. The only thing you might need is a different encoder, which handles converting the incoming messages to the JSON that InfluxDB will accept. I'd be happy to provide a bit of support on tackling these problems, but, again, that would need to be brought to IRC or the mailing list, it doesn't belong on this pull request. Thanks.\n. Thanks for catching this! I'm closing this PR b/c I've opened another one (which also includes a changelog entry) against the versions/0.7 branch, so it will land in a 0.7.2 bugfix release, as opposed to dev, which will end up being the 0.8 release: #1082.\nCheers!\n. Relevant: #721 & #717.\n. I checked this out to give it a test run and immediately learned that (on my Linux Mint system, anyway) if debhelper isn't installed then I can't even build Heka at all, even if I'm not trying to run make deb. The basic . build.sh should work regardless of whether the deb building requirements are installed.\n. After installing debhelper I was able to build heka w/o trouble, and then make deb ran and successfully generated a deb package. Installing the package failed, however, w/ the following output:\nthor [heka] git:mikn-dev ~/go/heka/build $ sudo dpkg -i heka_0.8.0_amd64.deb \ndpkg: error processing archive heka_0.8.0_amd64.deb (--install):\n parsing file '/var/lib/dpkg/tmp.ci/control' near line 6 package 'heka':\n `Depends' field, invalid package name `libc6\\': character `\\' not allowed (only letters, digits and characters `-+._')\nErrors were encountered while processing:\n heka_0.8.0_amd64.deb\n. Testing the latest changes now, thx for the ping.\n. Okay, I was able to give it a spin. I'll start by saying that this is a big improvement, it's awesome that you're tackling it, thanks! Overall things are looking great, I only found two final issues:\nFirst, unfortunately Heka is currently using Go's default logger, which means that all of the output is going to stderr. However, /var/log/hekad.log is only getting stdout, so almost none of the data is getting captured. I've just updated (and reduced in scope) an ancient bug to make sure we fix this (#105), but for now it'd be best if both stdout and stderr are routed to the /var/log/hekad.log file.\nSecond, when everything is installed we end up w/ an init script that expects an /etc/hekad.toml file to exist, but we don't actually have that file in place. I put the minimum viable config file in a gist: https://gist.github.com/rafrombrc/cb9ec5b81cd82d3330b9. The absolute least we can do is to have this file end up living at /etc/hekad.toml.\nEven better than this, however, would be if that file could end up living at /etc/heka/00_hekad.toml, and the init script can be edited to use DAEMON_ARGS=\"-config=/etc/heka\". This will make it easy for us to add extra files with additional, optional configuration, such as ##_statsd.toml.example to fire up a statsd server, ##_nginx.toml.example to parse nginx log files, etc. Users can just copy or move them to remove the .example suffix and they'll be picked up when next reloaded.\nYou up to that?\n. Okay, we're very close. Here's what I notice w/ the latest round of testing:\n- If we're going to put the sample config files in /usr/share/heka/examples, then we don't need the .example suffix. Heka will only load files ending in .toml, so we need to append a different suffix if the files are in the conf.d directory, but if they're elsewhere they can just end in .toml so they can be copied into place w/o needing a rename.\n- There's inconsistency btn using sudo /etc/init.d/heka start and sudo service heka start. The former will run as the heka user and will use -config=/etc/heka/conf.d, while the latter runs as root and still uses -config=/etc/hekad.toml.\n. I don't know why you're defending the 00-hekad.toml file name. I don't have an issue with that. In fact, I think that was originally my suggestion.\n. No worries, I was just confused b/c you seemed to be arguing a point that wasn't in contention. For the record, my original suggestion was that there would be one file called 00-hekad.toml in the /etc/heka folder, and then we would add additional files with names like 10-statsd.toml.example and 20-nginx.toml.example. I still think this would be best, b/c people would be very likely to see the example files, and activating a given file would be a matter of simply renaming it remove the \".example\" suffix.\nYou suggested moving the example files to /usr/share/heka/examples, since that more closely matches debian's usual way of doing things. I'm fine with that, but in that case the files should have names like 10-statsd.toml and 20-nginx.toml, so they can be activated by simply copying them from /usr/share/heka/examples to /etc/heka/conf.d, without needing to rename them at all. We're in agreement on that, yes?\n. I think what we have now is fine... let's get everything working w/ the basic, minimum viable setup, before we worry about putting any sample config files in /usr/share/heka/examples.\nI just tested the latest changes, and it seems like the upstart stuff still isn't quite right. Using the init script works to start, stop, and show status, but when I use service heka start I don't end up w/ a running hekad process.\n. Closes #739.\n. Unfortunately, this isn't a priority for us, and the Heka team is too busy working on the core product to have any time left for focusing on the various different distributions. I'm happy to review and provide feedback, but for this to land it's going to need someone to step up and finish it up.\n. Superseded by #1513.\n. Sure, PRs are always welcome, and you'll get better visibility and feedback in a PR vs in this bug. :)\n. This looks like a fine patch, thanks for the contribution. I'll be happy to merge it, but before we do we need to get a note added to the \"Features\" section of 0.8.0 in CHANGES.txt, and the new setting needs to be added to the elasticsearch.rst file in the documentation. Can you make those changes, commit and push them, and then make a comment on this PR so we know it's ready for another look?\nThanks!\n. Right, I don't imagine this as useful for any fancy use cases, just the simple case where message data needs to be injected into static text for output. I still think it's useful for a subset of users who won't climb the Lua learning curve, but I'm happy to package it up as an external package and not have it land in the core.\n. This is definitely something that lots of folks are interested in, and I'd love to see this functionality in the core. It's unfortunate that Go's static linking requires this (and the go-dockerclient dependency) to bloat the executable size whether someone is using the feature or not, but even so I think this is enough in demand to make it worth it.\nSo I'm inclined to say yes, we do want to include this in the core, but I'd like to include a build flag in our cmake set up to make it easy to turn off in cases where someone wants to build a leaner version without the docker support. We already have something similar in place w/ the INCLUDE_SANDBOX and INCLUDE_MOZSVC options in CMakeLists.txt. Do you think you're up to putting all of that together into a PR?\n. Actually, this isn't exactly what I had in mind. The build flag stuff is the right idea, but I want to roll this directly into the core Heka repo. That would involve adding a heka/plugins/docker folder to the heka repo, rather than having a separate heka-docker repo. We also need to add documentation for the new plugin to the sphinx docs. You can see the INCLUDE_GEOIP flag in CMakeLists.txt for an example of a core plugin being loaded conditionally; that one is based on existence of a specific .h file instead of explicitly setting the cmake value, but it's the same idea.\nAlso, it occurs to me that this plugin is pulling in logs from docker containers using logspout, but there's probably also a need for a different plugin to pull container stats from cadvisor. That doesn't effect the behavior of this plugin at all, except that it might be a good idea to rename it to DockerLogInput or DockerLogspoutInput so it can be distinguished from any future docker related inputs we might develop.\nI realize that merging this in to our repo and writing the docs and etc. is a bit of work. If you're up for it, awesome, but if not just let us know and we can pick it up when we have the cycles.\nRegardless of how you want to proceed, this has already a valuable contribution. Thanks!\n. It is possible to set @type to match the coordinates _type, yes. You shouldn't need to export interpolateFlag, encoders.go, elasticsearch.go, and coordinates.go are all in the same package, so they share the same global scope and visibility. Does ES typically expect the @type value to match the coordinates _type?\n. I'm ambivalent about this. It'd be awesome to see Heka packages end up in Debian repositories, obviously. But I'm loathe to take on more distro-specific stuff in the core. We decided to use CMake for our build system because it allows us to reliably produce statically linked binaries ready for consumption on all of our targeted platforms (including Windows). That of course gives us CPack, which lets us provide \"minimum viable product\" binary packages for the most widely used OSs, for nearly free. This is great, but it was not and still is not our intent to take on the task of maintaining distro-quality binary packages for all of the different build targets we support; we just don't have the resources to handle it.\nWith that perspective in mind, then, I'd like to avoid having multiple build systems. I'm thrilled about the ways that you've already improved the deb packages that CPack generates (thanks!), since they Just Work\u2122 and they don't negate, interfere with, or risk causing any confusion with the rest of the build process. Adding an additional Makefile to the repo root that routes around the existing build system entirely does potentially cause confusion. Plus it opens a door to enthusiasts of other platforms asking to do something similar.\nIf you really need to automate a Heka deb build that routes around the existing CMake/CPack infrastructure, it should be easy enough to package that as a separate repo that downloads the Heka source as its first step, no?\n. To be clear, I'm +1 to work being done to improve Heka's packaging, whether that's for deb-based Linux distros or any other platform. But the current CMake / CPack build system is the only build infrastructure that we're going to have in the actual core Heka repository. If CPack can be made to generate deb packages that are suitable for official Debian repositories, great. If not, then any efforts to construct an alternate build process to do so will still be welcome, they'll just have to live in a separate repository and won't be part of the core Heka project.\nClosing this issue, since improving the quality of CPack's generated deb packages is already covered by #1085.\n. Yes, this is much closer to what we're looking for, although there are still some tweaks I'd make to the packaging; I think a single docker package for this and any other docker-specific plugins we may add is fine, for instance. And of course we'll need to dig in to see why the build is failing on Travis, it seems the GeoIP extensions are causing problems, not sure if that's a transient Travis issue or if the changes you made are actually interfering somehow.\nIn any event, this is a good start. Moving forward, I'd be happy to dive in and give detailed code review, if you want to keep working on it. I'd also be happy to just take what you've got here and make the desired changes myself, if you'd rather move on. Just let me know so I know how to proceed.\nThanks!\n. Update: I am able to reproduce the build failure on a clean checkout, so it seems that something in your changes are interfering w/ the GeoIP build. I'm going in to a meeting right now, but will continue to investigate a bit later.\n. Okay, took a while but I was able to debug the build failure. We want to remove the quotes around the ${TAGS} variable in the go install command on L154 of CMakeLists.txt. Turns out cmake will quote the spaces in the TAGS value, so when the quotes are there it reads -tags=\"\\ geoip\\ dockerlog\" which causes the geoip tag to end up mangled. Removing the quotes gives us -tags=\\ geoip\\ dockerlog which works as expected.\nThis is as much review as I'll do at this point, let me know if you'd like to continue working on this, I'll do a more detailed review; if not, I'll happily take this and run with it.\nCheers!\n. Okay, done with the initial pass of in-line comments. Overall it looks pretty good, but there are a bunch of smaller things to fix up. Unfortunately it seems like there's a bit of impedance mismatch btn how the attacher code is supposed to work in the context of Logspout vs how we're going to want to use it here in Heka.\nAlso, we'll definitely want to add an entry in the CHANGES.txt changelog file. And, finally, it would be most excellent if there were tests. Unfortunately, that will probably be quite a bit of work. To successfully test, we'd want to mock out the docker client, but they've exposed that as a struct instead of an interface. Luckily, we're only using methods on the client, we don't do any direct attribute access, so we could define our own interface containing those methods, and then we could write tests where we inject a fake implementation so we can exercise our own code. Don't suppose you're up to tackling that?\n. Yup, no worries. If it's more than you can tackle, then just take care of the code fixes I suggested and I'll add tests when I merge. Thanks!\n. Okay, I've finally taken a look at this, and have even played around with it a bit. It seems to be working quite well, nice work!\nUsing a channel to send back the errors is indeed a good approach, but I agree that we should just be sending errors across rather than the struct. Even just sending a string would be better, there's no reason I can think of to wrap a string in a struct, but you might as well just go with the error.\nAs for the restarting daemon issue, that's a tougher one. I don't love relying on ping, but I can't find any better way to do it, so I think your idea is probably best. I think exiting Run is probably the right choice; by adding a CleanupForRestart() method to the input it will be able to use the restart functionality and it will reconnect when this happens (see http://hekad.readthedocs.org/en/v0.7.2/man/config.html#configuring-restarting).\n. Okay, this is killing me. I tested w/ the last round of updates and data would still stop flowing if the Docker daemon restarted. So I grabbed the code and started experimenting. Ping doesn't work as a test, b/c the amount of time the daemon doesn't respond to pings is so small that even testing 10 times per second wasn't enough to always catch it. It's slightly better if we use a different API call (such as ListImages), but even then it's possible that the daemon could finish being restarted between checks, so we never see the failure and we end up dropping the logs.\nSo I tried a different approach. I discovered that the logs would start flowing again if we unregistered the Docker client's event listener channel and then reregistered it right away. I started watching for intervals where no new logs came in, and if we got nothing for a configurable stretch of time I'd reload the event listener. This caused me to bump into bugs in the Docker client. I fixed the most obvious bug, but even then it turns out that the client will leak goroutines every time the event listener is reloaded, possibly leading to unreasonable resource consumption if there are large gaps where logs aren't being generated. Fixing the goroutine leak won't be easy, b/c the goroutines are blocked on trying to close a net resource while exiting, but the connection won't close b/c the socket is still in use elsewhere.\nI've lost two days on trying to resolve this so far, and I'm starting to think that this approach might need to be rethought altogether. Logspout is originally meant to run inside a docker container; it might make sense to stick with that approach and figure out a more robust way to get the aggregate log data out of the Logspout container and into Heka.\n. Okay, so I took a step back and have made some progress. Turns out the docker client does get notified when the daemon goes down, in the form of UnexpectedEOF error on the event listener socket. The client wasn't doing anything to propagate this information back to the downstream subscribers, however. I've changed the client to close the registered event channels, which gives the listeners an opportunity to act on the information, and have fixed up the plugin so it works as desired. I'll open a PR with go-dockerclient to see if they'll merge my changes back.\n. Okay, I've taken this code, done additional work (including switching the dependency to a go-dockerclient fork I made), and have opened a new PR. Closing this in lieu of #1145. \n. Currently the TcpOutput routes all outgoing messages through an on-disk buffer. This, combined w/ the TcpOutput's reconnect logic, allows the output to survive broken connections and upstream downtime without losing any data. This disk buffering logic is already abstracted out into a separate entity so it can be used with other outputs (see https://github.com/mozilla-services/heka/blob/dev/pipeline/buffered_output.go).\nOur current plan is to, before the Heka 1.0 release, make this BufferedOutput functionality available to every output plugin, so all a user would need to do is set an option such as use_buffering = true in the config to turn it on. If a messag fails to be delivered, the buffer cursor won't advance, causing the same message to be retried until such time as the data is delivered.\n. Related: #930\n. For anyone following, ElasticSearchOutput now supports output buffering as of #1278. Our plan is to make this available to all output plugins by 1.0 release. Closing this bug as supplanted by #1378.\n. Minimum viable solution is the ability to specify a maximum disk cache size, and to shut down Heka if the cache grows to that size. Even better would be to add another setting specifying what should happen when maximum size is reached, w/ shutting down as one option, and no longer caching (i.e. dropping data on the floor) as another, until such time as the cache shrinks to below some level.\n. Possible third option: have the output no longer pull from the input channel, which will cause the router to hang, causing Heka to apply back-pressure and stop processing messages entirely until the cache shrinks.\n. @arrrrgh Sounds good to me.\n. This all seems right to me. Nice sleuthing! Thinking through the ramifications of this... in cases where we have an EOF followed by a successful read, we're not resetting the EOF flag. This would have a performance impact, b/c we're generating a new position hash and we're checking for newer file availability when we really don't need to be doing so.\nLuckily, I don't think it's causing any other issues. If there's more stuff being written to the current file, then presumably l.NewerFileAvailable() will be returning false. And we always perform a read on the current file, whether priorEOF was true or not, and if that read is successful we return early. But still, there's no need to be doing extra work before each read when we don't expect that we're actually at the end of the file.\nIdeally this fix would be made against the versions/0.7 branch, so it can land in the 0.7.3 bugfix release. Would you mind closing this PR and opening another one against versions/0.7? Also we'll need to have an entry added to the 0.7.3 \"Bug Handling\" section in the CHANGES.txt file.\n. Fixed way back in #1214.\n. @validname We support a variety of file systems and operating systems, we can't rely on implementation specific details such as inodes for the Logstreamer behavior.\n. Go does support variations on that theme: you can check the os using runtime.GOOS, and you can use build flags and \"magic\" file names to have some code only build on some platforms. But no, we don't want to maintain separate, platform-specific code paths except when there's no other option. Besides, even on linux different file systems can act differently.\n. Closed in #1190.\n. Yup, I'm working on getting the 0.8 release out the door at this moment, and then I'll be leaving shortly afterward, I'll review this tomorrow.\n. Resolved in #1152.\n. This code all looks fine, but the usage is a bit confusing and unwieldy. I think much better would be a way to specify in the actual roc anomaly_config whether or not the thresholds should be graphed, and if it's set to true then we automatically render the thresholds on the main output graph, rather than requiring the user to hand-construct a different URL.\n. This seems reasonable, and the code overall looks good. My only notes are small nits:\n- \"cutted\" isn't actually a word. The past tense of \"cut\" is.... \"cut\". Yes, English is weird. So we should change the config option and variable names to use \"cut\" instead of \"cutted\". \"truncated\" would also be acceptable, but \"cut\" is probably better since it's a shorter word.\n- The new config option would need to be documented in the logstreamer.rst file.\n- A note on the change would need to be added to the CHANGES.txt for the 0.8 release. \n. @validname Yes, that would be an appropriate name.\n. Done.\n. Resolved in 28ffad108ea83f534d464138bd97c9a28752685c.\n. This looks great, just needs a changelog entry, since massive improvement in certain types of regex matches is worth calling out to users.\n. Sorry to take so long to get to this. This code looks great, but I'm wondering if you'd be able to add some test code to demonstrate that it works. Should be pretty easy to add to what we already have in encoders_test.go. Thanks!\n. Resolved in #1174.\n. Not sure what \"read a bit on the getting started files\" means, but the \"getting started\" section of the docs (http://hekad.readthedocs.org/en/v0.7.2/getting_started.html) goes into considerable detail describing every section of a fairly extensive, useful Heka configuration. The complete configuration is included at the end of the page (http://hekad.readthedocs.org/en/v0.7.2/getting_started.html#tying-it-all-together), and every single piece of that final configuration is explained separately in the text above.\nThat's by far the best walkthrough of an initial Heka setup that we have. I strongly recommend you take your time to go through that and make sure you understand all of the concepts that are covered. If you have specific questions on any part of that documentation, or about any other particular behavior you'd like to see, please ask on the mailing list or in the #heka channel on irc.mozilla.org.\nThanks!\n. This looks good. I thought you'd have to clone the command chain again, but then I realized that in the multiple run case we were never actually using the original (uncloned) command chain, so that was unnecessary. Only thing missing is an entry in CHANGES.txt, which will have to go in the 0.9 section that I'm about to add to the dev branch.\n. Looks good, can you please add an entry to CHANGES.txt?\n. This looks great, can you add a bug fix entry to CHANGES.txt?\n. The PR is against the dev branch, which is what will eventually become 0.9.0. If you want this to land in the 0.8.1 release then you should close this PR and open another one against the versions/0.8 branch.\n. Resolved in #1177 \n. Resolved in #1176.\n. In addition to the small doc and setting name notes I made, I think this PR should be closed and another should be opened against the versions/0.8 branch instead. This is sort of a feature, but it resolves a bug that is actively impacting users, so I'd rather see it land in 0.8.1 than in 0.9. Thanks!\n. This change actually has no effect. Go doesn't prioritize select statements based on the order of the cases, you'll need to use nested selects or a select w/ a conditional closure returning a channel as described in this thread: https://groups.google.com/forum/#!topic/golang-nuts/ChPxr_h8kUM\n. Resolved in 6222cf03d453a839b9584c1d7b8f84ffe31c508e by moving errChan processing into a separate goroutine.\n. @trink No, it's annoying. I've changed the sourcegraph settings so hopefully it won't spam us any more. If it continues, I'll deactivate it altogether.\n. It's anecdotal evidence, but the one time I tried building Heka w/ CMake 3.0 (while investigating #991) it worked just fine. It spat out one new warning message, but it still produced a working binary, so I'm hopeful we'll be able to straddle the CMake versions.\n. Ah, permission issues are causing this to fail on Travis. I'll get it fixed up.\n. In addition to the inline comment I made, this will need an entry in the CHANGES.txt file. Thanks!\n. I went ahead and resolved the conflicts and merged. Thanks for you help!\n. This feature is reasonable, and the code looks fine, but before it can be merged the docs need to be updated in the elasticsearch.rst file, and you'll need to add an entry to the features section of CHANGES.txt for 0.9. Thanks!\n. Actually, in this case I don't think you need add_external_plugin at all, since there are no plugins to load in this repo. All you need is git_clone(https://github.com/garyburd/redigo master) and the repo will be cloned into the go env. But I strong recommend using a specific SHA instead of master.\n. @arrrrgh Yup, it did occur to me that a goto would work, and I'd be open to using one in this case. Personally, though, I probably wouldn't. The amount of code duplication required to avoid it is minimal; AFAICT we're just talking about having two calls to QueueRecord (one initial one outside of the loop, and a second one inside a loop that is hit if-and-only-if the first one fails and we want to block).\n. Comment to try to force a cache invalidation on the diff view, please ignore.\n. @markabey In the case you're describing, Heka will hold on to the deleted file when no replacement file has been created, but it should notice that there's a new file as soon as one is created in the original location.\n. I'd like to see the case where ticker_interval is 0 special cased so the output doesn't spin up a separate goroutine, but instead just synchronously sends each message like the original implementation.\nAlso, while maybe you handled the ticker_interval value yourself so you could handle the 0 case correctly, that value is intended to be used w/ the channel returned by the OutputRunner's Ticker() method. If you only spin up a separate goroutine in cases where the ticker_interval is >0, then you can switch to using the ticker channel that Heka provides.\n. ticker_interval is a special config option that Heka itself defines. You can include it in your config struct if you want to set a default value, but even if you don't it's automatically supported, as you can see by its inclusion in the \"Common Output Parameters\" docs: http://hekad.readthedocs.org/en/v0.8.0/config/outputs/index.html#common-output-parameters\nWhen ticker_interval is set, then you can call OutputRunner.Ticker() and you'll receive back a channel that will automatically send an event every specified ticker interval (type <-chan time.Time). I'm just saying you should use this channel rather than interpreting the ticker_interval value yourself w/ an ad-hoc implementation.\n. Merged into dev in 6f0c474eb382130da7f33e9153c3161e1914ad36.\n. Hrm, I can't reproduce this issue. If I send messages with request_time and upstream_response_time through an ElasticSearchOutput using ESJsonEncoder the fields make it through to ES just fine.\nIt's possible to use the ESJsonEncoder with a LogOutput and/or a FileOutput, which will let you see the raw JSON you're sending to ElasticSearch either in the console or in a file. I recommend trying that to see whether or not the fields are showing up in the generated JSON; if so, then the issue is on the ES side.\n. Closing this issue for lack of activity. Hopefully the issue has been resolved. If you're still having this problem, please re-open the issue and include the results of my debugging recommendation. Thanks!\n. I haven't been able to reproduce this, nor have we had any other reports of this behavior. Are you still experiencing issues?\nClosing, please reopen if this is still a problem.\n. @starchou You can run tests in a functional Heka build by first activating the environment (using source env.sh) and then using the full path to the go package. In your case the command would be go test -v -run=TestInit github.com/mozilla-services/heka/sandbox/lua.\n. @dcarley Thanks for providing such detail in your description of the issue, and thanks for submitting a patch! I've dug in a bit and I think you're mostly right. The parser function will read until one of two things happen: the stream parser returns an error or the input receives a value on the stop channel.\nThe most common situation by far will be an io.EOF error being returned, in which case the underlying Logstream will perform a save, but it won't reset the record count. If it's a different error, or a stop channel event, then save might not have happened. So I think this PR goes just a bit too far; we can omit the save when there's a io.EOF error, but we still want to reset record count and, in other cases, we still want to save.\n. This can land in 0.8.1, yes. It would make things a bit easier for me if you wouldn't mind closing this PR and opening a new one against the versions/0.8 branch. It's easier for me to forward merge from that branch (which will become 0.8.1) to dev (which will become 0.9) than it is for me to cherry pick from dev back to the 0.8 branch.\nAlso, the last request I have is that you add an item to the 'bugfixes' section of the CHANGES.txt changelog for 0.8.1 mentioning that you've removed extraneous journal saves when logstreamer hits EOF.\nThanks again!\n. The re-org of the input docs works for me in principle, but there are some issues that come up. The big one is that Sphinx gets confused when a target reference is included in more than one place, which bites use b/c Sphinx is generating both our man pages and our HTML docs. Our previous strategy was to have two copies of the index pages for each plugin section, one index.rst and one index_noref.rst. The first has the target refs and is used by the HTML docs, the second does not and is used by the man page docs. By moving the target refs back into the actual included pages, you reintroduce the ref duplication, which is likely to break links in the HTML docs. :(\nIf we can resolve that issue, I'm +1 to reorganizing all of the plugin sections to match what you've started w/ the inputs.\n. Oops, apparently I missed that you already dealt w/ this in the index_noref.rst w/ the start-line directive. Please ignore.\n. Okay, I made one small note re: changing \"Log Streamer\" back to \"Logstreamer\". I don't want to merge until we convert the other plugin sections to match what you've done w/ the inputs, though. Thanks for your help!\n. I think there's some value here, but there are some implications that we need to consider before we jump in and make this change. First, in many (but unfortunately not all) cases we want the data to end up in the payload if there's no decoder (which is easy), or if decoding fails (which isn't quite as easy, but is still possible). The tricky bit would be deciding when the data should be in the payload, we'd probably end up w/ new conditional code replacing old conditional code.\nAlso we have to consider current use cases where people are chaining multiple decoders together. Currently it tends to work reasonably well b/c if someone is using protobuf that's typically the first decoder in the chain, subsequent decoders can extract the data from the message struct. This breaks if every decoder always expects to find the raw data in MsgBytes.\n. Closing this b/c splitters now allow data to be written either to the message payload or to pack.MsgBytes, configurable on a per-splitter basis.\n. Right, b/c ticker_interval only supports second resolution, and we wanted the FileOutput flush_interval to support millisecond resolution.\n. HttpOutput also doesn't use ticker_interval. It makes HTTP requests synchronously, i.e. requests are made as data comes in to the plugin. We might add batching support directly to the output at some point, but for now if you want to batch data or control the timing of your HTTP requests you can use a filter to do the aggregation and periodically emit the data that the HttpOutput can consume.\n. Closing since this isn't actually tracking any current work.\n. Sorry for the delay in responding... things are busy before the holidays and this one fell through the cracks. Anyway, what's going on here is a deadlock btn the StatFilter and the StatAccumInput. You're ending up w/ a situation where all 100 packs in the input pool (i.e. associated w/ inputRecycleChan) are sitting in the StatFilter matcher and input channels. The StatAccumInput is blocked b/c it can't get any new packs, since the input pool is empty. The StatFilter is blocked b/c it's waiting for the StatAccumInput to respond, so it's not processing any new messages, so input pool stays empty, everything is hung.\nTechnically this could happen at any time, but in my testing I found that it only happens when the ElasticSearchOutput is included and the maxprocs is left at the default of 1. The ES output is much slower than the other plugins, so I think what's happening is that the scheduler is trying to reduce context switching by giving the StatFilter a bigger time slice than it would in other cases, which causes the StatFilter to drain the input pool all in one go. When I switch to maxprocs = 2 or higher, the plugins can be processing in parallel and the deadlock doesn't seem to happen.\nAnyway, we don't want to rely on the idiosyncrasies of the scheduler to protect us from deadlocks. A better solution is to either increase the pool size, or (probably a better choice) decrease the size of the input channels, so that even when the StatFilter's input channels are full there are still more packs available to the StatAccumInput. If you reduce the plugin_chansize to 30 (see http://hekad.readthedocs.org/en/v0.8.1/config/index.html#global-configuration-options) the problem should go away.\n. Not sure how you have Heka installed, but it seems likely that Heka can't find the static dashboard files it needs. When Heka starts up, it will copy the static files from the static_directory to the base_dir. static_directory defaults to ${SHARE_DIR}/dasher, and share_dir defaults to /usr/share/heka.\nIf you've installed from source, the easiest thing to do is probably to run make install from the build directory and then configure share_dir at the generated build/heka/share/heka folder in your [hekad] section.\n. Is this still happening for you?\n. It is not fixed in 0.8.3. It is fixed in 0.9 and later. 0.9 came out almost a year ago, it is highly recommended you upgrade.\nIf upgrading is not an option for you, you can fix the issue by building from source applying the necessary change to the v0.8.3 tag. We have no plans of making any further 0.8.x releases.\n. First, some notes on what you have above, if we were going to continue in that direction:\n- You'd want to reuse the original pack you were handed as the first pack in the return slice.\n- You'd have to implement the SetDecoderRunner() method (see https://github.com/mozilla-services/heka/blob/versions/0.8/pipeline/plugin_runners.go#L362) to get a handle to the DecoderRunner and then use runner.NewPack() to get additional packs to populate.\nThere are some catches, though. The first is that the Decoder API is a bit more naive than we'd like, in that you have to accumulate all of your packs before you send any of them along, which means that a single in-pack that generates hundreds of outpacks will exhaust the pack supply, unless you explode the configured pool size, which will also increase memory usage. The second is that ultimately you'll probably want to use a Lua sandbox to do the actual parsing, so in this case it would make sense to do the splitting and the parsing all in one go, rather than having to rely on a MultiDecoder.\nOne workaround for the decoder API issue is to do this work in a filter instead. If you don't specify a decoder, then the original message will be passed to the router unchanged. You can catch it in a filter, split it and do the parsing in a sandbox, and then emit messages. The filter API is smarter, in that it lets you inject packs as you populate them, so they can be recycled and returned to the pool for reuse while you're still processing. This introduces a risk of router back-pressure if the parsing is very slow, but as long as the filter can process the messages faster than they're coming in, this won't be a problem.\nThe better solution is one that is not yet ready. Currently some of the other inputs (e.g. LogstreamerInput, TcpInput, UdpInput, others) require you to specify a \"parser_type\", which actually handles the splitting of stream data into separate record before the messages ever get to the decoder. HttpListenInput, unfortunately, doesn't support this. But we've realized that this is something that should be available to all input plugins, so we're introducing a new plugin type called \"splitters\" that will be available to every input, much in the same way that decoders are currently. I've already started this work on a local branch, it should be landing on the dev branch for initial testing in early January.\nOnce this is available, then it will be trivial to specify a TokenSplitter with the default delimiter of \"\\n\", and then your decoder will get each log line separately, you'll no longer need to worry about dealing w/ the splitting yourself.\nHope this helps.\n. Related: #1228 \n. Closing to reopen after merging latest changes.\n. Our strategy for dealing with this has been to use JSON strings in the config:\ntoml\n[custom_filter]\n    [custom_filter.config]\n    fields = \"{'label_1': 1548, 'label_2': 454}\"\nThen you can use cjson to deserialize this into a Lua table in your sandbox code.\n. Overall this looks pretty good, thanks for your contribution! In addition to the in-line notes, it would be great if you could add a note to the CHANGES.txt changelog file, as a new feature for the 0.9 release. Cheers!\n. Sorry for the delay, but the holidays are over and I'm getting through the backlog now. This is a simple fix, looks great, just needs an entry in the bug fixes section of CHANGES.txt for 0.9 and I'll be happy to merge.\n. Here at Mozilla we use preserve_data = true extensively with SandboxFilter plugins and all works as expected. It's worth noting that only global values will be preserved, so any variables defined as local will not be preserved.\nIf you're still having issues, please include the source code of the plugin that you're using (or just the name, if you're using one that ships with Heka), along with the full TOML configuration that you're using so we can help debug.\nPlease note, however, that the Heka core devs are on holiday until Jan 5, so help will likely be much slower than usual.\n. Oops. I tested this locally after purging this library from my system, I thought, but apparently I didn't remove the library entirely. Apologies. I'll release a 0.8.2 binary either today or tomorrow.\n. Okay, v0.8.2 is now out, with the GeoIP Decoder removed from the binaries. Docs have also been updated to reflect that users must build from source if they want to use the GeoIP stuff.\n. This looks great, and seems to work based on my initial testing. Would you mind adding an entry to the 0.9 features section of the CHANGES.txt file? Thanks.\n. You can work around this by using parser_type = \"regexp\" and delimiter = '$', which will cause the entire UDP packet to be used as the message payload. I'm currently working on introducing 'splitters' as a new plugin type, which will replace the parser_type functionality, when that is complete there will be a NullSplitter to use in these cases.\n. Closing this, splitter plugins are now available on the dev branch, using a UdpInput w/ a NullSplitter behaves as expected (i.e. the entire contents of the UDP packet are put into the payload of the message).\n. Fixed when parser_type was replaced by splitter plugins.\n. Yes, verified that the test is failing even on the versions/0.8 branch, which is very strange. Looking into it now.\n. This looks good, but a few support details need to be handled:\n- Add an entry to the 0.9 features section of CHANGES.txt.\n- Update ElasticSearchOutput documentation to reflect the new config option.\nPossible for you to add these?\n. The missing piece to be able to develop w/o having to commit is the use of the special :local tag, which lets you build with a copy of your repo that is on your hard drive. This is documented in the installing section (see the pull-quote note in http://hekad.readthedocs.org/en/v0.8.1/installing.html#building-hekad-with-external-plugins), but not very clearly. Leaving this ticket open until the docs are tweaked to make this more clear.\n. Unfortunately this isn't very high on our priority list, and we have limited engineering resources, so it's not likely that we'll be tackling this any time soon.\n. Thx for the note. You're right this should be documented, leaving this open until I get a chance. In the meantime, though, I'll let you know that a decoder's Decode method (or a SandboxDecoder's process_message function) should not be invoked by multiple goroutines. Similarly, in a SandboxFilter, only one of process_message or timer_event will be called at a time, never will they be invoked concurrently, so plugin authors don't have to worry about race conditions.\nIt should be noted, of course, that it's possible for someone to write an input plugin very poorly, so that it manages its own decoding and concurrently calls Decode from multiple goroutines. They would have to go out of their way to do so, however; using the encouraged APIs will not lead to this result, and this would absolutely be considered a bug in the input.\nHope this helps.\n. SandboxFilters can inject a message w/o timer_event. The inject_message and inject_payload functions are available in process_message, there's nothing preventing you from using them, although if you inject more than one per input message you'll need to adjust the max_process_inject setting (see http://hekad.readthedocs.org/en/v0.8.2/config/index.html#global-configuration-options).\nIt should be noted, however, that SandboxFilters can only create new messages, they can't modify existing ones. Once a message gets to the router it should never be modified, b/c it might be concurrently processed by multiple filters and/or routers. Mutations would then cause race conditions.\n. Hrm. This isn't really the direction I'd want to go for the Heka core, no. Instead, if HttpListenInput grew the ability to store arbitrary HTTP headers as fields on the generated message, then all of the rest of the behavior (rejecting messages, pre-parsing, anything else) can be done in the decoder layer. We could provide a Lua module that has Heroku-specific stuff, which could be used as a standalone decoder or in conjunction w/ other code that is doing further parsing, etc.\nDuplicating behavior across multiple plugins, blurring the distinction btn the various plugin types responsibility domains, and compiling in vendor specific Go code are all things we want to avoid.\n. Unfortunately there's a missing piece to closing this loop. We have an encoder that will take graphite style data (embedded in heka.statmetric messages) and serialize that into JSON suitable for InfluxDB, but we don't yet have an encoder that will serialize our cbuf time series data into InfluxDB compatible JSON. We do have an encoder that serializes cbuf data into Librato compatible JSON, however, see http://hekad.readthedocs.org/en/v0.8.2/config/encoders/index.html#cbuf-librato-encoder.\nIn order to get this done, the cbuf parsing code in the Librato encoder should be pulled out into a separate Lua module for reuse, which would then make it quite easy to write an alternate encoder that instead generates InfluxDB compatible JSON.\n. I've nearly got this finished, but am having problems testing w/ the InfluxDB sandbox, and am pretty sure that the issue is on the Influx side. I expect to have it resolved early next week.\nIn the meantime, there's one other option I forgot to mention. We have an encoder that will extract the fields from any Heka message and serialize them into InfluxDB data points. If you set up an HttpOutput to catch the messages generated by the various file polling inputs, you might be able to achieve your original goal.\n. Use an HttpOutput pointed at InfluxDB, w/ message_matcher =  \"Type == 'stats.loadavg' || Type == 'stats.memstats' || Type == 'stats.diskstats'\". Also define an InfluxDB Schema encoder, and use that as the encoder for your HttpOutput.\nAlso it should be noted that the statsd setup mentioned doesn't require a separate install, other than statsd clients. Heka itself serves as the statsd server.\n. Yes, typo. Already fixed on the dev branch:\nhttp://hekad.readthedocs.org/en/dev/config/encoders/index.html#schema-influxdb-encoder\n. Glad to hear. :)\n. Closing due to lack of activity. If this is still occurring, please provide config and sample data to help reproduce the problem. Thanks.\n. Sorry to take so long to look at this. Finally getting to it now.\nUnfortunately, I think this isn't the right approach. I don't think there's much win to having two different loggers for each type of PluginRunner. The only thing it buys us is the ability to inject type into the logger itself, but that's easily enough just written into the output text in the various LogError / LogMessage methods.\nI'd have LogError just use the global logger, as it's already doing. At startup we can create an alternate logger for stdout and store it in the GlobalConfigStruct, and every runner's LogMessage method can use that.\n. Superseded by #1282.\n. Unfortunately we've never been able to actually see it happen. Are you still using Heka? If so, has this recurred at all?\n. Thanks for the response. Since I can't reproduce the issue, I'll need your help to debug, hopefully you can provide me the info I need to get to the bottom of things.\nObviously it's clear that it's the extra trailing } causing the problem. When you hit this issue and Heka doesn't start, do you remove the } character to get past it? When you do, have you noticed whether or not the rest of the seek journal is correct? In other words, does Heka pick up parsing the log files at the correct location? If you don't remove the trailing }, how do you restart?\nAdditionally useful information would be your complete Heka configuration, what version of Heka you're using, on what platform, and whether or not you built Heka yourself or are using one of the binary package downloads.\nThanks!\n. The mailing list (https://mail.mozilla.org/listinfo/heka) is a better venue for asking support questions like this. The issue tracker is for reporting bugs and making feature requests. Please resend this inquiry to the mailing list. Thanks.\n. Thanks so much for this contribution! Sorry to be slow to respond.\nThis is a great catch, I definitely want to land it, but I'm thinking it might be better to use quoted-printable instead of base64, since that makes the raw version more human readable. Would you mind changing that?\nAlso, it would be great if you could add a note to the \"bug handling\" section for 0.9 in the CHANGES.txt file.\nThanks!\n. Oof re: no encoder. Didn't check that, sorry. No, I don't want to add another dependency. We can stick w/ base64 for now.\n. Okay, I made my first pass through. Overall, this is solid code, thanks so much for contributing it, and for staying engaged and integrating all of my feedback. It's very much appreciated!\nIn addition to the small in-line notes I made, there's another thought I have. Right now we're always spinning up a committer goroutine, no matter what. When buffering is in play, though, the committer goroutine isn't strictly necessary, since the buffered output already has its own separate goroutines that isolate the message intake from the outgoing ES requests. I couldn't say whether this has any impact w/o testing, though, and since this is all going to change when the buffering stuff is redone it's probably not worth worrying about too much.\nBeyond that, we need to update the docs to reflect the new plugin options, and to add an entry to the features section of the 0.9 notes in CHANGES.txt.\nThanks!\n. Good point, maybe it is worth making the change then. I'll leave it up to you.\n. Hi... Sorry for taking so long, I've had my head down trying to wrap up some major changes before I go on vacation for a week in 4 days.\nYour changes were pretty good, but my goal was to have either a committer goroutine or the BufferedOutput in play, not both. I took the work you had done and tweaked things a bit so this is the case, and cleaned things up just a bit. I also started pushing the batch record count through the batchChan to the committer so we didn't have to worry about touching that variable from separate goroutines.\nAnyway, it's merged now, and it seems to be working w/ my superficial local testing. Give it a spin and see how it works. And thanks for contributing!\n. This looks great, thanks! Can you add an entry to CHANGES.txt? I think it should go in the \"backwards incompatibilities\" section since it's a fairly significant operational change.\n. I'm confused. Go gives us -help and -h for free, so this doesn't seem to add any new features. I also don't see how this resolves #1239, since typing a bare hekad will still fail as before, yes?\n. Ooops, my bad, I misread the diff, now I see that you removed the \"no flags\" check, this does resolve #1239. But the other part of my comment still stands; we get -help for free, so we just need to remove the entire if flag.NFlag() clause.\nAlso, please add a bug fix note to CHANGES.txt. :)\n. I'm fine w/ this feature, but in addition to the performance note I made, we'd want to have the new config option documented in the opening comment (which gets pulled in to the docs). You added it to the example config, but you didn't explicitly list it in the prose part of the docs. Also we'd want to add a note the to \"features\" section for 0.9 in the CHANGES.txt changelog file.\n. Closing due to lack of interest... ;)\n. This issue tracker is intended for bug reports or feature requests, it's not a good venue for getting support. To ask questions you'll get a much better response either in the #heka channel on irc.mozilla.org (esp during California working hours) or on the mailing list. I recommend you re-ask this question on the mailing list.\nThe short answer, though, is that to use those encoders you'd want your data to already be parsed from the payload and stored in message fields. Usually this happens in a decoder on the way in. You could do this in a SandboxEncoder, though. I wouldn't be able to help you w/ the 500 error w/o considerably more information.\nAlso, I'm not sure if you really are intending to send data from logstash to elasticsearch going through AMQP and heka along the way, but if so that seems... excessive. Usually you'd use either logstash or heka, I can't think of a reason why you'd want to use both.\nAnyway, please ask further support questions in a different venue. Thanks! :)\n. This is intended behavior. I've finished and merged your delete fields patch, so this should be resolved.\n. Overall this looks great, thanks! In addition to the in-line comments I made, it'd be great if you could add a note to the \"features\" section of CHANGES.txt for 0.9. Cheers!\n. @inthecloud247  I wouldn't usually think of batching as part of an encoder's job. It might be a part of an output's functionality, or it might be done using a filter (i.e. a filter can collect a bunch of incoming messages and periodically emit an aggregate message with the batch). If there were specific reasons why either of those would be difficult, and an encoder resolves those, issues, then maybe, but it smells a bit off to me.\n. Yes, this is the reason. At Mozilla we don't run Heka in production on Windows, and all of our Heka instances use Lua extensively, so unfortunately we can't say how stable it would be. If you experiment, we'd love to hear your results.\n. This work has moved back to the back-burner, closing for now.\n. Unfortunately I don't think this is the right approach. The entire point of the synchronous_decode option is to avoid using multiple goroutines, and paying the associated syncronization and scheduling costs. Usually async decode is a better choice, but sometimes when MAXPROCS=1 synchronous decoding will be more efficient because goroutines can't be run in parallel so the cost of context switching doesn't provide any gains.\nInstead I think the InputRunner should notice when a synchronous decoder is in use, and it should add the instantiated decoder to the global set of decoders so reporting will happen as it does with the async decoders.\n. MAXPROCS = 1 doesn't have to mean that there's only one core, it could be that a box's primary job is something else and the admin only wants Heka to be consuming a single core.\nAs for the other issue, we could also just rename allDecoders to asyncDRunners (or something) and add a separate syncDecoders attribute to track all the synchronous decoders, the reporting code could iterate through both of them.\n. DRY? Not sure how storing two related-but-not-identical data points in two different struct attributes can be construed as repeating ourselves. Anyway, DRY is a useful concept, but hardly an inviolable law. There are often times when repeating a small amount of code is a better idea than jumping through awkward hoops (or incurring unnecessary performance penalties) to avoid doing so.\n. This looks fine, but the docs will need to be added before we can merge it to dev. Also we'll want to add an entry to the \"bug handling\" section for 0.9 in the CHANGES.txt changelog file. Thanks!\n. I'm +1 in principle to having a separate, shared lua_sandbox package. I'm definitely a strong +1 to having both Hindsight and Heka living on a machine together and not interfering with each other. What I want to avoid is adding yet more friction to the already somewhat burdensome process of getting started with a Heka build.\nIn 0.9 (IIRC) we switched from a monolithic Heka binary to using a separate lua_sandbox shared library. We gained a lot from that, but we also saw a considerable increase in installation support requests. My understanding is that if we move lua_sandbox to a separate package, folks building Heka for the first time will have to now complete two separate builds, with probably a bit of hand wiring to get them to work together. \nAn alternative would be to update the build to work with the new setup, grabbing both repos, building everything correctly, generating multiple binary packages with dependencies set up, etc. This is not a small amount of work, however.\nAny ideas re: how to best resolve this, or (even better) help updating the Heka build to smoothly handle a separate sandbox build is welcome. :)\n. @bbinet I wish I could agree with you, but in my experience the amount of friction in the build experience has a direct impact on whether or not folks decide to use Heka, and also on the amount of support requests that we get.\nBesides, using the official packages doesn't even solve everything. What we're saying is that the Heka package would depend on a separate lua sandbox package, which means that the sandbox package should be a package dependency. Getting CPack to handle all of this correctly is a non-trivial amount of work. I'm in support of that work being done, but I'm not in support of moving the sandbox to a separate package until that work has been done.\n. Closed in #1340.\n. Resolved in #1345 and #1358.\n. I knew I was leaving out the type setting code. Having all of the messages of type \"NetworkInput\" didn't seem very useful to me. I feel similarly about heka.tcpinput and heka.udpinput; there's not really much value to it. Still, it probably does make sense to set the message type to something... I'm thinking using the name of the input is probably the best bet. I'll make this change, update the changelog, and merge.\n. I fixed the tests, made one small tweak, and merged this to dev. Sorta hosed the merge / rebase, though, so it didn't close this automatically.\n. Resolved in #1406.\n. Fixed in #1353.\n. Yeah, this seems reasonable. To merge we'd need a couple of tests, an entry in the CHANGES.txt changelog, and documentation added to the esjson.rst file. Thanks!\n. Closing until revisiting w/ different approach.\n. 0.9.1 will be released tomorrow. You can build from the versions/0.9 branch to get the fix sooner if needed.\n. Unfortunately, I can't reproduce this error. Originally I could reproduce using the 'testdata' as suggested in the issue, but then #1369 fixed the issue and it's no longer happening. I've tried a few different variations (multiple lines that were too long, long lines interspersed with short lines, etc.) but no matter what I do Heka doesn't misbehave. I'm doing this using versions/0.10 branch.\nFor those of you still seeing the issue, can you try using the 'testdata' as specified above to see if that still causes the issue. I won't be able to do anything about it until I can understand exactly what circumstances are causing the problem.\n. One of the major benefits of using Lua is the wide array of existing libraries available, many of which are written in C and will only work with Lua or LuaJIT. A number of the existing Lua modules that we've written for use within our sandbox are also written in C. Switching to a less mature implementation that eliminates many of the benefits of our current environment is a non-starter.\n. Okay, merged the two HTTP related stream splitting implementations into one.\n. Fixes #1226.\n. Indeed.\n. The diagnostic messages you've got are saying that messages are being fed into your output plugins but they're never being freed up. Whenever a message (aka a \"pack\") is fed into a filter or an output plugin, Heka records that information. When the message has finished processing and is recycled for reuse, this is also recorded. Your output is saying that 100 messages have been fed in to the LogOutput, but even a full 2 minutes later they have not yet been recycled, which is clearly no good, 2 minutes is an eternity of time for a plugin to process a message.\nThe LogOutput code for processing a message is very minimal. The message is encoded, either an error is logged or the encoding output is logged, and the pack is recycled. My guess, then, is that something is happening inside the encoder such that the Encode() call is hanging, or at least is taking a very long time to return, gumming up the works.\nYou seem to be using a custom encoder. Is the source code to that available anywhere?\n. You've got a pack.Recycle() call inside the for loop that's iterating through your message fields. This is wrong for a couple of reasons. First, encoders shouldn't be recycling packs at all; the output plugin \"owns\" the pack, so recycling should happen in the output. Second, even if it were appropriate to be recycling the pack inside your encoder code, any given plugin should only recycle a pack one time. Recycling the same pack over and over again is definitely going to cause problems, possibly including (but not limited to) the idle pack messages you're seeing in your output.\nTry removing that line from your code and see if the problem goes away.\n. OpenTSDB might be causing some problem for you, but I don't think that the \"Diagnostics\" errors that you're seeing are related. The fact that the # of packs trapped in LogOutput and HttpOutput are different, even though they have the same message_matcher, and the fact that LogOutput seems to have more packs trapped than HttpOutput, even though it's clearly doing much less work, all point to something inside of Heka and, more specifically, something inside of your Encoder.\n. The only issue that wasn't addressed in the comments on #1296 is the no-op vs error comment, and I'm not sure I agree that deleting a value that doesn't exist should be a no-op instead of an error. I think it would be very confusing to silently not-fail if someone tries to delete a non-existent array index in a field, for instance. Maybe we need to add the ability to check for field existence?\n. Okay, I re-added the test that was removed. I also changed the behaviour so deleting a non-existent field is a no-op. Trying to delete an out-of-bounds field or array index for a field that does exist is still an error, however, and I added some tests (and fixed some bugs) to show that.\nDoes that behaviour work for you?\n. This might be resolved by #1375. In any event, it's definitely the same issue, the splitter is trying to use its SplitterRunner to log the error but none has been set. Can you test #1375 to see if this resolves?\n. Thanks for tackling this, and for explaining your ideas in such detail! My thoughts on what you've proposed:\n- The changes from 0.8 to 0.9 are significant enough that it probably doesn't make sense to try to clutter up a single encoder's code base with both implementations. I'd say scrap the post_v09_api setting and just implement the new stuff as a separate encoder.\n- Given that, database should no longer be an optional setting.\n- retention_policy would remain optional.\n- I'm starting to think maybe we've taken the wrong approach by having an exclude_base_fields setting, and that you might be following that up with the map_fields_as_tags setting. Instead, it might make sense to just have skip_fields and tag_fields settings, but to allow a couple of \"magic\" values such as **all_base** (for both) and **all** (for tag_fields only). Need to also think about how skip_fields and tag_fields interact with each other.\n- timestamp_precision seems reasonable.\n- A decoder for InfluxDB query responses also seems reasonable, but deserves a separate ticket.\nHope this is useful.\n. Yes, I think this is outside the scope of the encoder. Batching is best done in either a filter or, in some cases, as a part of the output itself.\nAlso, closing this issue since the write encoder has been merged. We can open other issues to track any additional pieces if necessary.\n. Thanks for catching this. I'd like to merge this to the versions/0.9 branch, instead of dev, so it will get into next week's 0.9.1 release. Also I'd like to add an entry to the 'bug handling' section of the CHANGES.txt changelog file for 0.9.1. If you're willing to close this PR and open another one against the versions/0.9, great. If not, please let me know and I'll make the changes there myself.\n. Thanks for the detailed bug report! I've verified that the sync.Once issue, will make sure it's resolved in next week's 0.9.1 release.\nThe other issue seems to be the result of some ambiguity on what max_retries means, exactly. The current behavior is assuming that if the plugin fails to start (i.e. Init() returns an error) then the retry count should increment, but if the plugin actually initializes and starts then it's considered a success and the retry count is reset. The docs aren't specific on this, however, and it's not clear that this is more useful than your interpretation, which is that the retry count should continue to increment regardless of whether or not the plugin is successfully restarted. I'll see about changing things so your use case can be met, will try to get that into 0.9.1 also.\n. Okay, I've changed the ProcessInput to consider an early (i.e. pre-Heka shutdown) exit of a process when ticker_interval is set to 0 to be an error. This matches what the documentation implies, and should have a side effect of causing the RetryHelper not to reset its retry count. This is on the versions/0.9 branch, is it possible for you to test and verify?\n. Unfortunately, turns out we can't trust HTTP response codes because ES will often return 5xx errors when the problem is with the submitted JSON. I've opened a PR (#1479) that will drop records in all cases where the HTTP request seems to have succeeded. Retries will still happen if there the server is down or there are network errors.\n. This looks awesome, and I'd love to get it into 0.9.1... would you be amenable to closing this PR and opening an identical one against the versions/0.9 branch?\n. Closed and reopened against versions/0.9 branch in #1416.\n. Does this happen spontaneously while Heka is running, or did this happen when coming back up after a restart?\n. Okay, I've dug into the relevant code and am confused about how this is happening. The line of code that I linked to is the only place where buffer files are being deleted. Immediately after the file is removed, the new checkpoint file is being written. The only way I can see that we'd end up with a missing log file like you're describing is if the Remove call is successful but the immediately following writeCheckpoint call is failing. If that's true, though, the entire ElasticSearchOutput should be exiting, which would show up in the logs and, most likely, cause Heka to shut down.\nI'm hoping you can provide more information about what set of circumstances are causing this issue. Are you seeing Heka crash at all? Is it exiting cleanly? Are there any indications in your log files that something is going wrong? It'd be easy enough to work around to the issue, but I really want to make sure we understand what's going on before throwing kludgey solutions at the problem.\nThanks.\n. Hrm. I'm glad to hear that you're able to fix something in your setup to minimize / effectively eliminate this problem, but it's not immediately clear to me how shutdowns, even if they're happening far more often than they should, can land us in this situation. I'd like to understand exactly what's happening, so I hope you don't mind me digging a little deeper.\nThere is a select statement on L224 that is checking for shutdown, in the form of closure of the buffer's stopChan. If we get past this point, then there isn't any other place in the code where a shutdown sequence would interrupt the execution until a possible retry wait loop on L272, or until the next iteration of the outer loop comes around.\nSo AFAICT a shutdown signal shouldn't interrupt Heka between the file remove and the checkpoint write. What would cause this result is if the file remove is succeeding but the checkpoint write operation is failing and returning an error, but it's not clear to me how a shutdown signal might trigger a write failure. If there is a write failure, that should be making its way to the logs. Were you seeing any output in the logs before or during shutdown, prior to the restart failures, that indicate that the checkpoint write was failing?\n. Yes, a kill -9, a panic, or some other hard crash could cause hit at exactly the wrong moment and leave you with the file missing but the checkpoint still pointing to it. That seems rare enough to not likely happen multiple times in a single week, though.\nAnyway, I guess I'll (unsatisfyingly) close this issue. Thanks for your feedback. :)\n. This looks reasonable, but it needs to have an entry in the CHANGES.txt changelog. Also, I'd love to see an update to the ProcessInput docs to clarify this behavior, explicitly stating that any time a process exits w/ an error it will cause the input to exit, triggering a shutdown or the retry behavior.\n. It seems to me like you've sussed things out correctly. I think it's reasonable for us to stop overriding the retry settings on the nested ProcessInputs, I'm not sure why that was happening in the first place, especially since it wouldn't actually have any impact on the behaviour, as things are currently.\nI'm +1 to changing the InputRunner so that transient inputs support the retry behaviour. We just have to be careful that if a transient input exits b/c the number of retries has been exceeded that it does not cause Heka to shut down the way a non-transient input would.\n. This looks great, very close to being ready to merge, thanks for your dedication (and patience with my code review).\nAs for the timestamp precision issue... ugh. I see that cjson has an encode_number_precision setting, but it's max value is 14, and that's not enough. I think for now we might have to omit those precision options. We probably also want to file a ticket InfluxDB re: accepting scientific notation values, since that's a valid part of the JSON spec.\n. @intjonathan I've been trying to reproduce this locally and haven't been able to. I'm wondering if you might be able to help debug? What would be awesome is if you could add a memprof = \"/path/to/mem.prof\" setting to the [hekad] section of your config so we can capture a memory allocation profile. Then, when the memory usage grows, stop hekad, move the profile out of the way so it doesn't get overwritten when you restart. It's important to capture when the usage is high so we the leaky allocations will be more likely show up in the profile.\nNo worries if this isn't possible, but if it is the data would be very helpful. Thanks!\n. I'm pretty sure that the DashboardOutput related leaks would only happen when the router is backed up... the DashboardOutput spins up a goroutine to generate the report messages every ticker_interval. When the router is backed up, or when there are no packs available in the injectRecycleChan, then these goroutines will be stuck and never exit. If this state continues, goroutines will accumulate and slowly drain system resources.\n. @mattupstate Have you concretely linked it to the DashboardOutput? If so, I'd love to work with you to try to debug the issue.\nThere are many things that might cause the router to back up, but in general the answer is that one or more of the plugins attached to the router (i.e. filter or output plugins) is either processing messages so slowly that its input channels back up to the router, or it's not recycling messages when it's finished with them, so that the messages are never freed up to be reused.\nIn this particular case the original thread (linked in my opening comment on this issue) was referring to a situation where Heka was freezing and the DashboardOutput was concretely linked to a memory leak. Since I've been unable to reproduce a DashboardOutput memory link under normal conditions, and it's very clear to me how such a leak would happen if Heka was wedged, my current hypothesis is that the original user's leak was a secondary symptom of the wedging.\nThis is still just a hypothesis, though, any additional information that can be gathered would be helpful.\n. In the \"Heka is wedged\" case I was describing above, there would be hundreds or possibly thousands of active goroutines, all hung up in the report.go code like the two goroutines you pasted above. If you send a SIGQUIT signal to the hekad process while it's in the high-memory use state you described you'd see this, in case that helps with the debugging process.\n. @trink I fixed them, edited the changelog, and integrated the docs into the right places before merging, see dda6279716c01a6545fc16e7b2416d7df244cba3.\n. Yeah, that's b/c I can't push my changes back up to @steverweber's branch, to which this PR is connected. But I'll often pull down a remote branch to a short lived branch on my local machine, make an additional commit to tweak some things, and then merge that branch back to dev and push. Github notices that all of the commits in the PR have landed and marks the PR as merged automatically.\n. I'll go ahead and fix up these last issues and get this merged, no action needed.\n. Well, I'm a bit confused by your error message, not because of the error itself but because of the line number. The statmetric_influx.lua code hasn't been touched in over 7 months, and it only has 63 lines in it, while your error is happening on line 92. Have you customized the encoder at all?\nAnyway, it looks like the cjson.encode call is failing due to NaN or infinity values in the output table. You might want to see what the data is that's causing the problem, because your input data might be bunk. But you can probably get around it by changing cjson's encode_invalid_numbers setting. Can you try adding cjson.encode_invalid_numbers(\"null\") just below the require \"cjson\" statement to see if that helps?\n. Oh no! This is the wrong one... from IRC:\nRaFromBRC sounds good, although i'd love if you could close #1393 and reopen a PR against versions/0.9 first :)\nI said #1393 in IRC, although that's the bug number, the PR I was referring to is #1403. In any case, I was talking about the UDP packet size issue, not the sync decoder reporting, which I'm happy to have land on the dev branch, and in fact has already been merged.\nI'm sorry for the misunderstanding, and extra sorry that it means that you wasted some time working on this. :P\n. Yup, that's a typo. I've just pushed a fix, will open a PR for it now. Thanks for catching it!\n. It's sort of both a bug and expected behavior. Your config not working is expected behavior, since you're trying to use a decoder called \"RsyslogDecoder\" but you haven't defined a decoder with that name. Getting a panic is a bug, though, b/c the correct behavior would be for Heka to just emit an error message to that effect and exit cleanly. You didn't include any of the output prior to the panic, but I'm guessing there was a line right before the panic that said Input 'syslog_udp' error: decoder 'RsyslogDecoder' not registered.\nI'll work on fixing the panic. In the meantime, though, you should be able to work around this issue by adding a config section like the following:\n``` ini\n[RsyslogDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/rsyslog.lua\"\n[RsyslogDecoder.config]\ntype = \"RSYSLOG_TraditionalFileFormat\"\ntemplate = '%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n'\ntz = \"America/Los_Angeles\"\n```\nThe details of what should be in the RsyslogDecoder section varies based on your exact requirements, of course, but you have to have something there or you'll see the panic.\n. It's provided by Heka, but for it to be useful it requires an rsyslog template to be provided so it knows the exact format the input is in, so it knows how to parse it correctly. The only decoder that's automatically made available by Heka is [ProtobufDecoder], any others need to be specified in the config.\nAnyway, closing this issue since the panic has been fixed as of #1451.\n. Sorry, clearly I've been neglectful. I'm afraid this isn't really the approach we want to encourage. The PayloadRegexDecoder hasn't been removed from the tree because sometimes it fills a gap for some folks, but we generally don't recommend its use; a proper LPEG grammar in a SandboxDecoder is nearly always a better choice.\nIn (slightly) better news, PR #1760 contains an implementation of a generic syslog decoder which is meant to be able to attach to a syslog stream and parse all of the log data coming through. That PR has also been languishing, however, I'll bump it up higher in my queue and will try to get it merged by the end of this month.\nSorry again for not responding.\n. This is great, thanks! I'd love to have this fix land in the 0.9.2 release, would you be willing to close this PR and open a different one against the versions/0.9 branch, adding an entry to the \"Bug Handling\" section for 0.9.2 in the CHANGES.txt changelog?\nIf you can't get to this in the next 24 hours or so that's fine, just please let me know so I can take care of it.\n. So you know, the mailing list is a better venue for asking questions, we try to keep the issue tracker focused on actual bug reports and feature requests.\nTo answer you, however, it's possible to inject SandboxFilters from a remote machine into a running Heka instance. For this reason, SandboxFilters are assumed to sometimes be running untrusted code, and there are a number of restrictions placed on them to try to prevent nefarious actors from DoS'ing or otherwise interfering with Heka's intended behavior. SandboxDecoders can only run code that actually lives on the file system of the Heka server, so it is assumed to be more trusted code and thus has more complete control over the generated output.\nI'm not sure what you mean by \"Should I create a decoder that matches... \". More information about your use case would be valuable. But, again, the mailing list is a better venue. Thanks.\n. Hrm... in my testing I'm seeing some file descriptor usage, but nowhere near enough to come close to the fd limit. How many files exist in the file stream?\n. I'd still like to know how many files exist in your file stream, but I've also added another commit to the logstreamer_rotate_fix branch that might help if my guess that there are a lot (i.e. thousands) of files is correct. Can you pull down the latest changes, recompile, and report back to let me know if it helps? Thanks.\n. Okay, #1454 fixed the panic, and #1480 causes the deliver_incomplete_final setting to apply to non-streaming, packet-based inputs, which provides a mechanism for capturing trailing records or records with no delimiter at all. \n. Resolved in #1480.\n. Resolved in #1462.\n. Hi! Thanks for the detailed question, and for being willing to contribute your work back. First, I should say that generally the mailing list is a better venue for questions such as this than the issue tracker, which is more focused on keeping track of bugs and feature requests.\nTo quickly answer your question, however, the third option is definitely what I would recommend. The StatFilter works, but for a number of reasons is not very efficient so it would likely fall behind if the throughput starts to pick up. Anyway, if InfluxDB is your target, there's not much value in generating stats data in the graphite format, so I don't see why we'd want to get the StatAccumInput involved at all.\nIf I were doing this, I'd probably write some Lua code to do the parsing and deploy that in a SandboxDecoder, and then a bit more code to do the aggregating and deploy that in a SandboxFilter. Not knowing the stats up front shouldn't be a problem. As long as you can write a grammar to describe the format that the stats are in, you can parse them and extract whatever information they contain.\nThis isn't exactly what you want, but I recently wrote an example SandboxFilter that listens for specific data in a Heka message and periodically generates graphite format aggregations of that data, to replace a StatFilter / StatAccumInput setup that was running too slowly. Your details are different, but the basic pattern of receiving data in the process_message function, storing it in data structures, and then periodically emitting that aggregate data in a new message in the timer_event function would be the same. Hopefully you find that somewhat useful.\nGood luck!\n. Resolved in #1473.\n. This is a nice fix, thanks. Before I can merge it, however, we need to have an entry added to the 'bug handling' section for 0.10.0 in the CHANGES.txt changelog, and we need to fix the test that is now failing. Looks like the test is failing b/c the error you added is now getting raised somewhere, should be easy to fix by changing the offending test to have a non-zero ticker interval.\n. Are you seeing this on your local machine? We've been seeing it intermittently on Travis for a long time (see #1041) but in all of that time I've never been able to see it happen on my local machine. If you're actually seeing it happen directly then maybe we'll have a better chance of tracking it down.\n. Okay, closing this as a duplicate, please use #1041 for any follow ups.\n. @k2xl My understanding is that when the HTTP request is successful but Lucene doesn't like the JSON to be indexed ElasticSearch will return a 500 response code. Do you know of any documentation that clearly specifies which response code ES will return, and in what conditions? If so, I'd be happy to adjust this to do the right thing as often as possible.\n. Fixed in #1831.\n. Oops, I see you made the change recommended by one of my notes, but which was actually overridden by a subsequent note that gives us what I think is a cleaner result. Sorry about that. :(\nIn any event, overall this looks pretty good. Before merging it needs the change that I spelled out in my second note (i.e. do all the work in LogstreamLocationFromFile instead of checking for a zero value after the return). Also, the LogstreamerInput documentation needs to be updated, and an entry needs to be added to the new features section of the CHANGES.txt changelog for 0.10.\nThanks!\n. I'm seeing the exact same failure as Travis when I run the tests locally... are you still not seeing the failure, @hoffoo?\n. I've dug into this a bit and have found that the feature as implemented doesn't work, and the rabbit-hole is a bit too deep for me to go down right now. Closing this PR, leaving the issue open so that we can hopefully come back to it soon.\n. Added.\n. Sorry... yes, I would consider supporting a config option that caused Heka to unlink a udp socket if it exists, as long as the default behavior doesn't change.\n. Overall this looks good. All that's missing is a changelog entry (easy) and integration with the existing documentation (still easy, but a bit more work).\nFor documentation, you'll want to take a look at the documentation comments for the other modules, and how they're all woven together into the sandbox docs in the modules.rst file.\nThanks!\n. I've added a changelog entry and merged this back to the versions/0.9 branch so it can land in the 0.9.2 release. Thanks!\n. When building Heka we made design choices to allow for the possibility of other sandbox engines, such as Javascript or Python. It's still theoretically possible for someone to implement another sandbox engine, and if someone stepped forward to do so then we would take care to make sure that the Heka core didn't prevent that effort from succeeding.\nIntegrating another sandbox engine would be a lot of work, however, and that's work that the core Heka team has no intention of tackling. Over time more and more of the functionality that makes Heka valuable has been implemented in Lua. So much so that we're event experimenting with using the Sandbox in non-Heka contexts (see https://github.com/trink/hindsight). Even if all of the sandbox's functionality were duplicated in a different scripting language, it would likely not perform as well as the Lua implementations.\nWhat are the use cases you have for which you're interested in extending the current sandbox implementation?\n. Re: not recovering from the queue being full... that issue was previously reported, should be fixed as of 0.9.2 release. Are you running 0.9.2? If not, can you upgrade and see whether the problem is resolved?\n. I've taken a look at the panic traceback and I'm not quite sure what's going on there. Is that something that happened when you were trying to shut down? The following line of code seems to be where the panic is happening:\ngo\nmessage.NewInt64Field(msg, \"SentMessageCount\", atomic.LoadInt64(&b.sentMessageCount), \"count\")\nIn order for the panic to happen either msg, b, or b.sentMessageCount would have to be nil. AFAICT b.sentMessageCount can't be nil, b/c it's not a pointer. If msg were nil, then I would have expected the panic to happen sooner in the calling code in elasticsearch.go. If b (the buffered output itself) were nil then there would have been big problems much sooner.\nMy guess is that things were wedged because of the no-recovery bug, you initiated a shutdown, and the shutdown caused either msg or b to become nil in a race condition with the ReportMsg method, generating the panic. That's just speculation, though.\n. @mattharden No need, actually. This encoder is going away entirely, to be replaced by one that implements the newer InfluxDB \"Line\" protocol, as supported in #1595. I'll close this one now. Thanks for checking!\n. I'd love to support this. Unfortunately, it's quite a bit trickier than you might expect. First, ProcessInput supports piping multiple commands together, so you'd have to make sure you handle all of the cases where an error happens somewhere along the chain. Second, and more importantly, process return codes don't work similarly across all platforms.\nEven so, it would be possible to check whether the user is on a Unix-like OS, and, if so, capture the exit code from the last executed process in the chain, storing it in a message field for any stdout and/or stderr messages that are generated. It's a bit of work, though, and it's not at the top of our priority queue right now, so it will either have to wait a bit or be done by an outside contributor.\n. I've tried to reproduce this but am having trouble doing so. Can you provide more detail about the configuration that you're using, including some sample data that triggers the incomplete_final sending? Thanks.\n. Overall this looks great, and tested well, thanks for cleaning this up! The only thing, other than the config tweak I already commented on, is that we want to add something to the CHANGES.txt changelog about this. I think there should be an entry in the 'backwards incompatibility' section for the 0.10.0 release explaining that the debian packaging setup is now different, ideally explaining which prerequisite debian packages need to be installed before the new deb generation code will work.\nThanks!\n. Fixed in #1516.\n. @jotes Not sure what you're getting at, both of those are input plugins and don't solve the original problem at all.\n@cruatta You're right that Kafka or NSQ or another message queue would be a viable solution, but of course that entails running another service.\nImplementing an output that feeds raw data to clients that are pulling from a listening port is reasonable, I think. We've got a back-log of features to implement, however, and this isn't on the short list, so it might be a while til we get to it.\n. Nothing in particular I can think of. You probably want to look at the UdpOutput for inspiration.\n. Fixed.\n. Thanks!\n. Travis build is failing.\n. I'm up for making 0.10, targeter for release by the end of this month, require Go 1.4 or greater.\n. You'll need to bump the go revision in the .travis.yml so TravisCI will use 1.4. when it tries to build. :)\n. This looks great, thanks! Before I can merge it there needs to be an entry added to the 'features' section for version 0.10.0 of the CHANGES.txt changelog, and the docker_log.rst documentation file needs to be updated to include the new setting.\n. Overall this approach seems sound. There's some commented code left in there that needs to be removed. The only real concern I have is related to the syscall.WaitStatus stuff in the initDelivery function. Will that work on all platforms (i.e. Linux, OSX, Windows)? If it doesn't work on Windows, will it fail over gracefully? \n. Overall this looks good, just need to add ESLogstashV0Encoder to the list of changes in the changelog, and update the docs for PayloadEncoder and ESLogstashV0Encoder to reflect use of the strftime format.\n. Work in progress... documentation is still coming, just getting this up so code review can start while docs are being written.\n. Travis seems to be failing randomly at this point, tests are consistently passing locally. This is ready for final review.\n. Great, thanks... I've found some issues in my own testing w/ the ElasticSearchOutput, working on those now, will get all this resolved and merged ASAP.\n. @jetpks  I assume you're talking about the \"Source code (tar.gz)\" link that is on each release page, is that right?\nUnfortunately, we don't create that link, nor do we want it to be there. Github generates it automatically, along with the similar \"Source code (zip)\" link that is also there. If there was a way I could remove it from the releases page, I would. We have no intention nor any desire to do source tarball releases.\nWe do provide 32- and 64-bit binary tarball releases for Linux. Those include the documentation, but no source code.\nIf you want to build from source you should clone the repository from git. That's the only supported build mechanism. I'm sorry the source tarball thing has misled you, but AFAIK there's nothing we can do to get rid of it. If you know any different, or if I'm misunderstanding your issue, please let me know.\n. This looks great, just needs an entry in the \"bug fixes\" section for v0.10.0 in the CHANGES.txt changelog. Thanks!\n. Hrm. I think the fix might be as simple as changing the zero-length record check from if len(record) == 0 to if len(record) == 0 && !sr.needData.\nCan you try that and see if it resolves things?\n. The \"send idle stats\" behavior doesn't originate with Heka. My understanding is that most statsd servers behave this way, i.e. once a statistic is seen once, the statsd server will continue to send that statistic forever regardless of whether or not new data arrives. Some statsd servers (including Heka) have added support for omitting idle stats, but generally that is not the default behavior.\nIn other words, my understanding is that Heka is just matching what is standard, expected behavior for a statsd server. Do you know what the default behavior is for Etsy's statsd server?\n. Right. Our goal is to be able to work as a drop-in replacement for other statsd servers, so we're going to follow their lead on this one, sorry.\n. This isn't really the best venue for such questions, the mailing list is better for support requests like this. We're not actually using Influx at Mozilla, so I'd have to set up a rig to test this, but for debugging I'd recommend breaking your experiment into two parts. First you can replace the HttpOutput with a LogOutput using the same encoder; this will show you exactly what is going in to the HTTP requests that the HttpOutput would be making. Then you can try to use curl or some other HTTP tool to send that output into InfluxDB, to see if you can get the data in directly.\n. I actually meant you could try LogOutput with InfluxdbLineEncoder so you can see the output that the Influx decoder is generating, like so:\ntoml\n[LogOutput]\nmessage_matcher = \"Type =~ /stats.*/\"\nencoder = \"InfluxdbLineEncoder\"\nGetting blank lines w/ a PayloadEncoder just means that you're getting messages with no payload, which isn't necessarily a problem, since the message's data may be stored in other fields.\n. Nope, that's not what's expected. I just tested the same config locally and am also seeing no output. Then I tried again w/ an RstEncoder and verified that the messages do actually contain data. I'm unfortunately about to wrap up for today, but I'll take a look tomorrow to figure out what's going on.\n. Okay, #1622 has been merged. @steverweber, is it possible for you to build Heka from the versions/0.10 branch to test the latest fixes?\n. Yup, it's working for me, I'm now seeing valie output, running the same config that was generating emtpy strings for me before. Hoping to have another release (either another beta or 0.10.0 final) next week, closing this issue in the meantime.\n. I'm testing out this branch, but when I use it and then try to run the build_docker.sh command it fails with the following output:\n... lots of elided output ...\nStep 10 : COPY . /heka\n ---> Using cache\n ---> b14a45d99230\nSuccessfully built b14a45d99230\nSending build context to Docker daemon 15.36 kB\nSending build context to Docker daemon \nStep 0 : FROM mozilla/heka_base\n ---> b14a45d99230\nStep 1 : RUN mkdir -p /heka_docker\n ---> Using cache\n ---> 000d455d6a01\nStep 2 : RUN cd /heka/build && make deb\n ---> Running in 3b2ae3a57bea\n/bin/sh: 1: cd: can't cd to /heka/build\nThe command '/bin/sh -c cd /heka/build && make deb' returned a non-zero code: 2\nShould this be working?\n. The original intention of the Docker setup was that it would create two containers. The first was for the build, which would install and build Heka from source, generating a deb package. This container would in turn create a second container, for deployment, into which the deb package would be installed.\nI'm not rigidly attached to the binaries and the second container getting generated immediately, but I am rigidly attached to the documentation matching reality, and that the docs include clear steps that will get users all the way to deployment. Right now the Docker build instrux in the readme (visible here: https://github.com/mozilla-services/heka/tree/dev/docker) still say that running build_docker.sh will complete the build and generate both containers, and the shell script assumes that's what it's trying to accomplish. Removing the build.sh call broke build_docker.sh's assumptions, so now running build_docker.sh exits with an error.\nI'd be open to any reasonable solution, but I can't merge this until the docs describe how to get to a successful deployment. :)\n. Yes, in general I'm fine with it. As long as I can read the Docker readme and successfully use those instructions to get all the way to the deployed Heka container without it feeling terribly burdensome, I'll be happy.\n. Bump. Just wondering if you are indeed planning on finishing this up so the docs and the actual behavior are in alignment, @mattrobenolt . :)\n. This work was further tweaked and merged in #1696. Thanks!\n. This looks good, and despite the (broken, and now deactivated, sadly) Travis failures the tests are all passing. Before I can merge, however, the docker_log.rst documentation file needs to be updated to describe the new setting's behavior, and an entry needs to be added to the CHANGES.txt changelog. Thanks!\n. Sorry to take so long to get to this... things have been a bit crazy for me lately. Thanks for your contribution!\nAnyway, I didn't love the two mutually exclusive topic options in the first place, and this feels like it goes a bit farther in the wrong direction. I'd rather see all of the settings reduced down to a single topic setting, where that setting supports variable interpolation. So your example might look like topic = \"heka-%{Fields[ContainerName]}\", or even topic = \"heka-%{ContainerName}\", where the var name is assumed to come from the dynamic fields if it's not one of the base schema field names.\nAlso, any changes to plugin configuration settings need to be reflected in both the documentation and the CHANGES.txt changelog.\nThanks!\n. I'm a very strong +1 to moving the shared functionality into a module and having a separate encoder for each endpoint. Sounds like a great idea. I haven't looked at your PR yet (am just now surfacing after Monitorama and traveling home) so I don't know what's going on in there, but I should point out that I've already created a graphite module with some utility functions to help generate graphite formatted stats from statsd-like input, which may be useful in some way: https://github.com/mozilla-services/heka/blob/dev/sandbox/lua/modules/graphite.lua\n. Actually it's likely dev and not master (dev is the default branch, and where most new development takes place).\nI've given feedback on all 3 of @mattrobenolt 's PRs (thanks for your contributions!) and hope to get them merged very soon.\n. You have to set the appropriate environment variables to point to your Go environment before any of the mingw32-make commands will work. On Windows the build.bat script does this for you, or you can do it without building by running the env.bat script. Are you doing this?\n. Yes, there were some final steps left out of the instructions for the source build. I've updated the docs and have opened a PR (see #1579). I'll close this issue once those get merged and the docs are available on readthedocs.org.\n. The updated docs are now showing up on readthedocs.org: http://hekad.readthedocs.org/en/dev/installing.html\n. @haf Yes, these steps have been tested. You have to source the build.sh file first, then make install will work from within the build folder.\n. We use the issue tracker for specific, actionable bug reports and feature requests. General support requests are better directed to the mailing list (https://mail.mozilla.org/listinfo/heka). Thanks.\n. It's too vague to be a feature request. There are a number of different strategies that can be taken to get data from Heka into Greylog, and this issue is a request for information on how to approach that problem rather than an actionable issue that can be resolved by code. A feature request would provide a description of a specific encoder plugin that you were interested in seeing, for instance, preferably with some detail re: any particular config settings that might be desirable.\nAlso, I should warn you that the core Heka team isn't using Greylog, and we have lots and lots of priorities of our own improving the Heka core, and improving integration with tools that we do actually use, so this isn't likely to leap to the top of our priority queue any time soon. We'll happily provide support and guidance to any community members who want to tackle the problem, however.\n. You can get around this by setting up a translation map for the \"Index\" value to map the \"missing\" value to the highest possible value, e.g. 99999. Details and an example config are in the Logstreamer docs.\n. Ultimately, you can use the message fields however you like. Heka's core message schema is loosely based on a syslog-ish data format (see https://tools.ietf.org/html/rfc5424#section-6) where payload maps to \"MSG\", or \"a free-form message that provides information about the event.\"\nWhen dealing with logging data, usually the raw contents of the log data (be it semi-structured text, JSON, whatever) will be in the payload field, while any structured data extracted from that that payload would be stored in separate message fields. This is a de facto, rather than de jure standard, however, you're free to use the available data fields however you see fit.\nAs for representation, no, Heka doesn't do any automatic conversion of values based on the representation of a field; representation just treated as an opaque piece of string metadata for the field. It is entirely possible, though, for Heka users to write filters that make use of the representation value to do such conversions manually.\n. Okay, first pass through the code is finished, all comments made inline. Thanks so much for tackling this!\n. P.S. Make sure you read through all of my comments before tackling the changes suggested in any of them. I made a suggestion later (in the \"Okay, I think I see why you...\" comment) that supersedes the earlier ones, would hate for you to not notice that and do work that you would then need to immediately undo. :P\n. I'm interested in accepting this, yes, but I'd want to see a few changes to the behaviour first:\n- The already existing fields setting should take precedence. That is, if \"Fields\" is not included in the original setting's list, then no dynamic fields will be included no matter what the dynamic_fields value is. If \"Fields\" is included in the original setting and dynamic_fields is empty, then all of the dynamic fields will be included. If \"Fields\" is included in the list and dynamic_fields is not empty, then only those listed in dynamic_fields will be included. Hopefully that makes sense.\n- There's no need for DynamicRawBytesFields, b/c the raw_bytes_fields setting already only applies to dynamic fields. The behavior of the raw_bytes_field won't change; if a field is in the raw_bytes_field list and it is going to be included based on the other config settings, then it will be included as a raw field. If a field is in the raw_bytes_field list but it's not going to be included, then it will have no effect.\n- The same behaviour should be added to the ESLogstashV0Encoder.\n. Hey! I'd like to have this land so that it can be in the 0.10.0 beta release, which I'm hoping to get out this week. I'm wondering if you have some time available to a) make the changes requested in my comment above and b) close this PR and reopen another one against the versions/0.10 branch (instead of dev).\nIf you're too busy to tackle that today or tomorrow, no problem, just let me know and I'll take care of making the changes and getting it merged to the right place.\nThanks!\n. Awesome, thanks! Don't forget to move this work to a new branch cut from versions/0.10. That also means you'll have to close this PR and reopen a new one that merges back to versions/0.10 instead of dev. :)\n. The issue happens if you explicitly use an empty directory as the config setting, or if you point it at an empty TOML file. I've opened #1740 which should resolve.\n. Yeah I think it was only happening when the empty TOML file was in a directory and Heka was pointed at the folder for its config.\n. Closing this b/c I'm cherry-picking the commit into the versions/0.10 branch so it can be in the next release.\n. First, I'm terribly sorry for taking to long to review this code. Thank you for your contribution! This looks very useful, and overall things look pretty good. In addition to the inline comments I made, it would be great if you could add an entry to the CHANGES.txt changelog file, and extra great if you could add the little pieces needed for the decoders to show up in the documentation.\nTo add them to the docs, you just add a stub .rst file for each decoder (this one for the nginx access log decoder is a good example) and then add them to the index.rst and index_noref.rst index pages.\nThanks again for your work!\n. Everything looks good with the changes you made, thanks! Sorry to move the goalposts, but in addition to Trink's point above about making the extract_quote function local in your decoders, there's one more comment I forgot to make the first time around.\nThe other HTTP server log decoders (nginx, apache are all using a similar message schema to represent each HTTP request handled by the server. This is helpful because then those messages are in the format expected by the HTTP Status Graph filter. It would be great if you could match that message schema, then the Status Graph filter would also work for graphing and anomaly detection based on IIS server logs.\n. @dkolli Yes, that's what I meant. I've actually done most of this work, as well as added some tests for the decoders. You can see what I've done in the iis_uls_decoders branch that I just pushed up to our repo. I'm just trying to track down some actual IIS and ULS log files so I can run them through to make sure I didn't break anything in the process of making my changes.\nOr maybe you could grab my branch and try them out on your log files?\n. I did, unless I missed something...\n. Did you read the comment that was linked above? We have no intention nor desire to provide a source tarball release, but the github releases UI exposes such a link and there's no way we can remove it. I'll happily provide support to anyone who is interested in generating an official Heka package for any widely used distro, but we don't plan on spending any cycles on fixing a source tarball download that is outside of our control.\n. Yup, the issue here is that the default isn't documented. I've fixed the docs and opened PR #1619.\n. #1648 has been merged to both dev and versions/0.10 branches. Any possibility you can build from the versions/0.10 branch and test to see if the problem is resolved?\n. Awesome, thanks for checking, closing this issue.\n. Resolved in #1631.\n. Thanks for cleaning up my clean-up. :) I've also fixed up the documentation links so the docs you wrote will show up correctly on the docs site.\n. Fixed in #1682.\n. Closed in #1795.\n. This looks great, thanks! A couple more details need to be handled before it can be merged, however. First, we need an entry in the Backwards Incompatibilities section for version 0.11.0 of the CHANGES.txt changelog. Second, the documentation needs to be updated in the stataccum.rst file.\n. Docs looks good, thanks! Unfortunately, however, I ran the tests against your branch and one of the stat accumulator tests is now failing. Sorry for not checking that before... :P\n. You can run ctest from the repo checkout directory, or make test from the build folder if you've already set up the environment using . env.sh. If the environment is set up, you can also use the go tool to just run specific package tests, e.g. go test -v github.com/mozilla-services/heka/pipeline.\n. This should be resolved on the versions/0.10 branch as of #1696 being merged back. Closing this issue, please test and re-open if the problem isn't resolved. Thanks!\n. There's little risk here, I'm happy to have this land in 0.10, but that'll require closing this PR and opening a new one against the versions/0.10 branch. Thanks!\n. This looks great, thanks! The only requests I have are an entry added to the CHANGES.txt changelog, and the addition of a couple of tests so we have a sanity check that the decoder is working. Adding tests for your decoder should be very straightforward, there are plenty of existing ones for you to use as a model. Just add yours in a new C.Specify call alongside the rest of them: https://github.com/mozilla-services/heka/blob/dev/sandbox/plugins/sandbox_decoders_test.go#L300.\n. Okay, I think @tianlin had the correct diagnosis, although the change I made was slightly different. Anyway, I've pushed what I think will fix this issue to a branch called buffer_reader_race_fix. Is it possible for either of you to build from that branch and test to see if the problem is resolved?\n. @wolkykim Hrm. I'm pretty sure that the change I made (which is now merged into the versions/0.10 branch) resolves the issue that @tianlin reported. The fact that @tianlin is reporting that the issue is resolved implies that either a mistake was made somewhere and you're not actually running the new code that I wrote, or that you're seeing an error that looks similar but actually has a different cause.\nIf you're absolutely positive that you're running the latest code from the versions/0.10 branch and are still seeing the error, we're going to need to dig in more deeply. If this is so, I'd love for you to open another issue with details about what your configuration looks like, when the error happens, and anything else that you think might be relevant or helpful to the debugging process.\nThanks!\n. Unfortunately this is harder than it should be. Heka gets data from Docker using a vendored version of the Logspout library. The Logspout API uses a channel to feed log data back to the caller, one line of text at a time. The Listen method takes a chan *Log, where Log is a struct that contains log data as the Data element.\nIn Heka, we create a message for each Log struct that we get back, putting the Data element in the message payload. Ideally each Log struct would contain an entire log event, but we don't live in an ideal world, and it turns out the log data is always split on newlines. This means that in order to support splitters, we'd have to pull the data from the Log channel and expose it over an io.Reader, which would likely cost us performance. It also makes it much harder to expose the fields data from the Log struct in the message fields, as we're currently doing.\n. The code from #1648 has been merged to both dev and versions/0.10 branches. Any chance you can build from the versions/0.10 branch and test to see if the issue is indeed resolved? Thanks!\n. Great, thanks, closing this issue.\n. Hrm. This almost entirely reverts 0495c2c44e5be4f3c18ecad75e609f8accef5e3c, which IIRC was introduced b/c the return code from the command chain wasn't always available when the decorator was running. Your commit message implies the waiting was unnecessary. Was there a problem that the earlier commit was solving, and, if so, does this reversion re-introduce the original issue?\n. I'm going to go ahead and merge this since #1620 and #1644 are causing real world pain, we can open a new PR with a different fix if the original bug resurfaces.\n. Overall I like where this is going. :)\n. This looks great, thanks for sharing! I made some notes that unfortunately add a bit of work to this. Hopefully you're up to tackling it. If not, please let us know. :)\n. First, sorry for the delay, I'm on a 3-day work week wedged in between two chunks of time away from computer, so things have been (and will continue to be) moving slowly in Heka-land.\nBeen thinking about this more and it occurs to me that the best choice is to add a field_map setting that lets the user specify which JSON fields should be mapped to which message fields. The user will provide a mapping where the keys are the (correctly capitalized) Heka message field names, and the values are the names of the JSON keys that should populate the message fields.\nThe map_fields option could stay, so if map_fields was false, field_map would have no effect. If map_fiels is true and no field_map is specified, the default field_map would be used, which might look like this:\njson\n{\"Payload\": \"payload\",\n \"Uuid\" : \"uuid\",\n \"Type\": \"type\",\n \"Logger\": \"logger\",\n \"Hostname\": \"hostname\",\n \"Severity\": \"severity\",\n \"EnvVersion\": \"env_version\",\n \"Pid\": \"pid\",\n \"Timestamp\": \"timestamp\"}\nCould even support multiple values on the right side for inconsistent JSON, so this would be valid:\njson\n{\"Payload\": [\"Payload\", \"payload\"],\n \"Uuid\" : [\"Uuid\", \"uuid\"],\n \"Type\": [\"Type\", \"type\"],\n \"Logger\": [\"Logger\", \"logger\"],\n \"Hostname\": [\"Hostname\", \"hostname\"],\n \"Severity\": [\"Severity\", \"severity]\",\n \"EnvVersion\": [\"EnvVersion\", \"env_version\"],\n \"Pid\": [\"Pid\", \"pid\"],\n \"Timestamp\": [\"Timestamp\", \"timestamp\"]}\nThis gets rid of the iteration through the JSON fields and the need to lowercase the field names. The only awkward part of this is that the sandbox config doesn't support nested TOML sections, settings can only be boolean, number, or string values. So field_map will have to be a JSON string that the decoder parses to get the mapping.\nHopefully this all makes sense.\n. The master branch should always have the code for the last non-beta release, which is currently 0.9.2. I just pulled down a fresh checkout from master and it built through w/ no problems. The issue you're seeing above matches what I would expect if you try to use the same checkout across multiple version builds w/o clearing the source files out of the way. Sometimes go files are renamed, and the build folder ends up w/ two versions of what are essentially the same file, resulting in \"redeclared\" errors. You can get around this by running make clean-heka before make (from within the build directory, after the environment has been activated) to clear out and re-sync the heka source code, or, if you want a bigger hammer, by deleting the build folder altogether and re-running . build.sh to pull everything down.\nAnyway, closing this issue b/c I don't think there's really a problem. Please reopen if you verify that there is an issue w/ a fresh checkout of master.\n. One second is the maximum interval between retries. After the retry interval hits one second, retries will continue, but the interval between retries will not continue to grow. Heka will continue to call ProcessMessage with the same PipelinePack once per second until ProcessMessage returns a different result. Do you have any suggestions for how to word this so it's more clear?\n. I agree w/ Trink. It might be surprising the first time or two, but for someone who builds Heka frequently the current behaviour is more convenient. \n. Re: the first issue, if your disk is full all bets are pretty much off, the behaviour at that point is undefined.\nRe: the second, while it sounds nice to try to do some automatic disk space checking and etc., we have no intention of tackling that problem. There are so many edge cases across various platforms, file systems, etc., this feels like a major time sink for an almost guaranteed imperfect solution. I'd much rather leave things as they are now, where the job of managing queue sizes vs drive space is left explicitly to the user.\nI'm possibly willing to consider special-casing disk full errors, so that Heka triggers the queue full behaviour when that happens, but that's not really very high on our priority queue, so it would probably have to come from an outside contributor.\n. I've cherry-picked your commit and merged it into the versions/0.10 branch so it will go out in the next release. The change will be merged back to dev soon, closing this PR. Thanks!\n. Thanks! I've cherry-picked your commit and merged it into the versions/0.10 branch so it will go out in the next release.\n. Your best option, by far, is to fix your data source.\nThere's no way to do what you want, currently. I'd be open to considering a contribution where the plugins optionally add a message field containing the remote address. This would need to happen in the pack decorator and, as such, would only apply when use_message_bytes is not set to true. Then it would be possible to use a decoder to copy the remote address value to the hostname field.\nOr, you could fix your data source.\n. This looks great. I'd love to have this land in 0.10.0... would you mind closing this PR and re-opening it against the versions/0.10 branch? Also would be lovely to have a note in the Features section for 0.10.0 in the CHANGES.txt changelog. Thanks!\n. Okay, cherry-picked this, added changelog entry, and merged to versions/0.10 in 51eab33391cdc11cc12dc867c93ef4602088ccb2.\n. Well that's embarrassing. :P Fixed.\n. Added changelog text and merged this into the versions/0.10 branch so it will make it into the next release.\n. I'm open to the idea of a generic CSV/TSV decoder. I'd imagine the config would contain a list of the column names. If a column name matches one of the static message fields (e.g. Payload, Hostname, Logger, etc) it would be written there, otherwise a dynamic field would be created.\n@timurb PayloadRegexDecoder might work, but it's not recommended. Even for a one-off, I'd strongly suggest using a SandboxDecoder and an LPEG grammar for handling this type of parsing.\n@trixpan I don't understand the ProcessInput suggestion. How the CSV data gets into Heka is immaterial. Using a TokenSplitter that splits on a comma doesn't help us, either; that would mean that each column in the CSV data would be written to a separate Heka message. The desired behaviour is that each line in a CSV file would be written out to a single Heka message.\n. @timurb had it right. TcpInput defaults to using the HekaFramingSplitter, which expectsthe protobuf-based record delimiter that wraps native Heka streams. You're going to want TokenSplitter to split on newlines or RegexSplitter for multi-line records.\n. And yes, mailing list is a better venue. Thanks. :)\n. @ChristianKniep You probably need to run make install and then set your share_dir in the global [hekad] config section to be /path/to/heka/build/heka/share/heka.\n. Is there anything showing up in the \"Termination Reports\" section of your dashboard UI?\n. Setting can_exit to false means that Heka itself will shut down if the filter exits, which unfortunately is probably not what you want. Some of Heka's plugins support restarting if they fail, but that hasn't been added to the sandbox plugins. :P\n. No, that's your best bet. Be great if you could open a bug re: adding restart support to the SandboxFilter though, so we don't forget about it.\nAnyway, closing this issue since it seems like both of your initial questions were answered.\n. I don't have any particular preference with regard to which strftime implementation we use. If the cactus team is willing to accept PRs to add better resolution then great, and if not then I'm open to switching to a different library that does. Unfortunately we don't have any cycles internally to tackle this right now, so the effort will have to be driven from outside if it's to happen soon.\n. Sorry, but since Logstash is maintained by the ElasticSearch company, they have much more interest in very tightly coupling the two. Heka supports ElasticSearch, but is less tightly coupled and, as such, we're unlikely to support alternate means of setting up ES templates when there are other viable ways to accomplish the task such as that described by @acesaro. \n. All of the files that are not in source control but are instead generated by the build are available in the build directory after a successful build. So if the Heka repo is checked out to /mozilla-services/heka, then message_matcher_parser.go will be available at /mozilla-services/heka/build/heka/src/github.com/mozilla-services/heka/message/message_matcher_parser.go.\n. I'm afraid there's not nearly enough information here for us to be able to help.\n. That page is only intended for use as part of the Unix man pages. For HTML docs, the plugin documentation is split out by plugin type, and each plugin gets its own page, all of which are clearly linked in the navigation.\n. The issue here is that CarbonOutput was written a long time ago and hasn't been updated to support encoder plugins; instead it assumes that there is Graphite format string data in the message payload, which is not the case here. You're right that the plugin should either be updated or removed, w/ users pointed to the other available outputs for sending data to carbon.\n. Yes, I'd love to have your help keeping up w/ the latest Sarama changes! Unfortunately, I think moving from Sarama 1.5 to Sarama 1.6 will have to be a separate PR, though, since this particular code base has already been extensively load tested in our deployment environment, and I don't want to rock that boat by changing the underlying code out from under it.\nAlso, you'll note that I'm still using a fork of Sarama. The only changes in my fork are related to test code; we wrote tests for Heka that make use of Sarama'a MockBroker, back when that was public and available for import. That has since been made private, and I didn't have the time to reimplement our testing infrastructure, so I forked the repo and made the mock stuff public. There are no changes of any sort to the code that actually runs in Heka, however, so using the stock Sarama should work until you try to run the tests.\n. @andremedeiros Okay, this has now been merged back to Heka's main dev branch. I'd love to get a PR from you upgrading to Sarama 1.6. The merged code is using a SHA from the versions/1.5 branch in my fork of Sarama. As mentioned above, this is identical to the v1.5.0 release tag except that the mocks that Sarama defines have been made public.\nSimilarly, I've created 1 versions/1.6 branch, which I cut from versions/1.5 and then merged in the upstream v1.6.0 tag. I've verified that this matches the 1.6 release, except for the public mocks. In the Heka build, you can refer to the latest SHA on that branch to upgrade Heka to using Sarama 1.6.\nI'd love for the mocks to be made public, but I understand why they became private in the first place. It's pretty ugly to have a bunch of test-related code cluttering up the public APIs. In Heka we've worked around this in a few places by making mocks available in a separate package. (Note you won't find the pipelinemock package in the source tree, since it's generated by our build.) This has some issues, though, b/c of course you can't get at package-private code from an external package. Not sure what the best resolution is.\n. Unfortunately, I'm currently seeing unacceptably poor throughput when I test locally with Go 1.5 on Linux. Also, we've seen Heka crash under load testing due to stack frame boundary panics, again with Go 1.5 on Linux. So we're not yet ready to commit to using Go 1.5 for all of our builds.\nStill, I'll do some testing on Windows before the next release. Even if we're still using Go 1.4 for the other builds I'll likely end up putting out Windows binaries built w/ Go 1.5, albeit with a \"use at your own risk\" warning.\n. We're still building Heka w/ Go 1.4, because with 1.5 we see worse throughput as well as intermittent segfaults. Unfortunately, the Go change that alleges to fix the cgo issue that burns us landed after 1.4, so we can't yet offer a Windows build that we can stand behind.\nWe have longer term plans to move a lot of the sandbox interaction functionality that's currently happening in Heka's Go code into C as a part of the sandbox project itself, which would both improve performance and greatly reduce the required surface area of the cgo interactions. Much less cgo means we will likely be routing around the underlying issue altogether. When this is done we'll likely upgrade to a more recent Go, and we should be able to again start releasing Windows binaries.\n. This looks great, just a few points:\n- would love to have a CHANGES.txt update\n- would love to have it opened against versions/0.10 branch so it will land in the next release\n- you could actually avoid the label if you changed for true to for ok, I don't feel strongly enough to force it, though, up to you.\n. Yes, exactly.\n. The code implementing the core functionality here looks quite good to me. I've tested both decoders and they seem to be working well. Thanks for your valuable contribution!\nOther than the small notes I made in-line to the code, there are some non-code related updates needed before this is ready for merging:\n- Need an entry in the features section for v0.10.0 in the CHANGES.txt changelog.\n- Need to add a stub file for each decoder that pulls the doc comments into the HTML and man page documentation in the docs/source/config/decoders folder. You can look at one of the other sandbox decoder pages in there for guidance.\n- Need to add an item to the index.rst and index_noref.rst files in the same docs/source/config/decoders folder above, and also an entry in the docs/source/sandbox/decoder.rst file (sorry for the duplication).\nThanks!\n. When buffering is turned on for a plugin, that plugin uses the disk buffer instead of the input channel to store the backlog of messages to be delivered, and the input channel will only have capacity of 2. ElasticSearchOutput has use_buffering set to true by default, so the inchan capacity of 2 is expected.\n. Unfortunately, there was already an open PR (#1703) with an upgrade to Sarama 1.5, along with updating all of the tests, so you've duplicated work that was already done. :P Sorry about that!\n. Oops, I see you have already opened another issue. There isn't really enough detail here, though. What plugin are you using that's causing the failure. Also, you say the code base is 0.10.0b0, but that you applied the code fix for #1639... not quite sure how that works. If you're building from source, the best course of action would be to make sure you pull the latest updates from the versions/0.10 branch and build that.\n. queue_max_buffer_size is replaced by max_buffer_size... max_file_size is something else. Even so, point taken, updated in c0122d6.\n. We have ongoing issues with intermittent Travis failures, mostly related to (test only) timing issues that are hard to debug because they only happen in Travis and not locally. I've resolved some of them recently, but there are still more. I re-ran the tests and they passed the next time through, so you're fine there.\nThis change looks great, thanks for your contribution! The only extra piece needed is an entry in the CHANGES.txt changelog in the \"bug fixes\" section for v0.11.\n. That's correct, mercurial is no longer required.\n. Starting work on this fix this week.\n. I've pushed a queue_full_shutdown_fix branch with a change (ef12c9090b04e55cda5d7fcbaf6b72089f4976d9) that I think will resolve this issue. Can you pull and build that branch to see if it works for you?\n. @ioc32 This is impacting you too... any chance you can build from my branch and see if it helps?\n. @ZeusF1 It's known that this bug exists in v0.10.0b1, the proposed fix is currently on the queue_full_shutdown_fix branch, which I hope to be merging to versions/0.10 branch today or tomorrow, to go out in a v0.10.0b2 release later this week or early next week.\n@timurb I'm sorry, I'm having a hard time keeping track of what you're talking about. It seems like you're describing a different issue altogether. My testing is showing that this patch is improving the reported issue, which matches the experience of @nathwill, so I feel confident that it's helping in at least some capacity. It might make sense for you to wait until this is merged and the v0.10.0b2 is released, then you can test that out and see what your behaviour is. If there's a problem, you can file a new bug, explaining from zero what behaviour you're expecting, and how that differs from what you're seeing. There's too much in here already for me to be able to track multiple concerns with this one bug.\n@nathwill Yes, exiting w/ non-zero seems reasonable. I'll look into adding that to this before it gets merged.\n. Closed in #1792.\n. I don't love love love this. Ultimately, I feel like all plugin types should grow some more advanced logging control, specified by a [PluginName.logging] config section, and enforced in the plugin runners' LogMessage and LogError methods. That would give the ability to turn off writing errors to StdErr for all plugins, not just decoders.\nUnfortunately, that's a bigger job, and it's not likely to be tackled soon, and this meets an immediate need, so I'm still inclined to merge it. Only two small changes:\n- I think print_decode_failures (and the associate variable names) should be changed to log_decode_failures to keep the naming consistent with Go's log package and the LogMessage and LogError API.\n- I'd like to see an entry in the \"Backwards Incompatibilities\" section of the changelog calling out that the DecoderRunner interface changed SetSendFailure to SetFailureHandling.\nThanks!\n. Needs changelog entry. :)\n. I recommend using Go 1.4.3... I see very poor multi-core performance and sometimes strange panics on 1.5 so far.\n. Fixed in #1887.\n. Overall this looks great, thanks! I made a bunch of comments, but they're pretty much all for small stylistic issues.\nIn addition to the in-line comments, we'll want to have an entry added to the CHANGES.txt changelog, and the lovely documentation comment you've provided will need to be wired up to the config and sandbox sections of the Sphinx documentation.\n. @timurb As I said in the mailing list thread, read_message(\"raw\") is safe to use once the message has hit the router. Filter, output, and encode plugins all happen after the router, so read_message(\"raw\") is what you want to use.\nAlso, using tonumber is fine, although if you break the not skip_fields[name] part into a separate if statement you can put that one first, so tonumber doesn't get called when the field is being skipped.\n. @simonpasquier Yes, your assessment is absolutely correct. The PluginMaker was updated to no longer cache the config structs that have been populated by the TOML, but the plugin runners' restart code wasn't updated to account for this. Your fix is correct, maker.PrepConfig() should be used instead of maker.Config() in all of the restart loops. I'll update the code and will get the fix into the v0.10.0b2 release that I'm hoping to get out the door tomorrow.\n. This should be resolved w/ the merging of #1764 and #1794.\n. A few notes:\n- We still have a few tests that intermittently fail on travis, but there have been some fixes on the 0.10 branch that reduce the number greatly\n- I know the tests were passing on OSX when I was doing 0.9 development, at least before releases. I did, however, verify that the 0.9 branch's sandbox tests seem to be failing on OSX right now due to linking issues. However, 0.9 is no longer under active development, so this isn't a high priority. 0.10 branch tests are all passing for me on OSX.\n- I've checked out your branch and verified that the tests are passing on Linux.\n- I'm happy to merge this, but there are no plans at this point for another 0.9 release. Are you willing to close this PR and open another one against the versions/0.10 branch? It should also include an entry in the 'bug handling' section of the CHANGES.txt changelog for the 0.10 release.\nThanks so much!\n. First, thanks so much for your contribution, and I'm very sorry to have taken so long before looking at it.\nUnfortunately, I'm not going to accept this PR for merging. We very much try to discourage decoders from being written in Go. I'd be much more interested in a SandboxDecoder that provided similar functionality, using either https://github.com/antirez/lua-cmsgpack or one of the native Lua msgpack implementations to do the heavy lifting. Even better would be an implementation that provided a pure Lua implementation, but which would make use of the much faster C-backed version if the module was available.\nBecause it's not possible at this time to implement a splitter in Lua, I'd be willing to consider merging a separate PR that only included the MsgPack splitter, but it would need to also include documentation and an entry in the CHANGES.txt changelog.\nThanks again.\n. This isn't on our short list. It would mean the output holding open a possibly unboundedly growing set of file descriptors, which could certainly create problems. Not categorically against it, but not likely to be working on it any time soon.\n. First, thanks so much for your contribution, and I'm sorry it's taken me so long to get around to looking at it. Things have gotten much busier for me, and my time to work on Heka core has diminished lately.\nWith that being said, I'm afraid I can't accept this PR. The performance impact of changing to synchronous delivery is too high, and this would greatly interfere with the real world requirements we have for Heka/Kafka throughput in Mozilla. I might be willing to consider a PR that allows synchronous delivery as a configurable option, with async delivery being the default, but removing async delivery altogether is not a viable option.\n. The test is failing b/c you're now making a new InputRunner.Deliver call, but you didn't tell the MockInputRunner in the tests to EXPECT that call. Are you able to run the tests locally? You can do so (once the environment is activated using . env.sh by running make test from within the build folder or using go test to run them from a single package (e.g. go test -v github.com/mozilla-services/heka/pipeline).\n. Your changes to the StatAccumInput tests were correct and resolved the original problem. This is a different issue, happening in plugin_runners_test.go, because those tests use a StatAccumInput as part of the test structure. StatAccumInput's changed behaviour is causing the teardown there to now happen incorrectly.\nI've got the code checked out and can reproduce the problem, hopefully I can resolve it quickly.\n. Closed in #1795.\n. Please use the mailing list for support questions; the issue tracker is meant for tracking bug reports and feature requests.\n. Oh, sheesh, I merged this, forgetting that a more comprehensive feature that covers this use case had already been merged to the dev branch (see #1667). I'll revert this, and will cherry-pick #1667 so that the fix shows up in the versions/0.10 release.\n. This issue has been resolved on the versions/0.10 branch, as of PR #1682. The fix will go out in the 0.10.0b2 release which I'm hoping to get out the door this week. Note that this is not actually related to the issue described in #1738, but that is being currently resolved and the fix will also be included in the 0.10.0b2 release.\n. This code looks great, it's definitely easier to follow than the use of the flushManual channel with the weird blocking in it that I had in there originally. I was trying to avoid the need for a channel put for every single message, but I think that the simplification of the code is probably worth it. I don't suppose you've done any throughput comparisons from before and after this change?\nA couple of notes:\n- I think recvChan should just be chan MsgPack, not chan *MsgPack. MsgPacks are very small, so there's not a big cost in mem copies, and if we don't use pointers then we're guaranteed that all MsgPack allocation will be on the stack, avoiding any garbage collection.\n- This fix should be landing on the versions/0.10 branch instead of dev, so we can make sure it gets into the v0.10.0b2 release.\nI am a bit concerned about the original issue, however. The idea is that ProcessMessage should not be getting called from more than one goroutine. Any plugin should have either ProcessMessage or TimerEvent called at any one time, never both, and never any one of them multiple concurrent times. If ProcessMessage is in fact being invoked from multiple separate goroutines, then that bug needs to be found and fixed or else there will be additional problems down the road. I'll poke around and see if I can find any place that might be happening. Any help you can offer tracking down the issue would be appreciated.\n. Closed, see #1795.\n. Okay, I did some further testing and verified that the problem is not that ProcessMessage is being called multiple times. This is a relief, because that would point to much bigger issues.\nWhat's happening is that there's a (obvious in retrospect) race condition where the flushTicker might fire while ProcessMessage is running. The flushManual hack prevents both flushes from happening at the same time, but it doesn't prevent a flushTicker flush from happening concurrently with an outBatch array append, leading to the corrupted records. Your change to the code resolves this issue.\nI'll go ahead and get this merged tonight. Thanks for your help.\n. Not sure how you're running the tests, or if that's even what you're doing, but in order to do anything you first need to set up your GOPATH correctly. The easiest way to do that is to source the env.sh file at the repo root: source env.sh or . env.sh, depending on your shell. After you do that, you can run tests by invoking the correct package path, e.g. go test -v github.com/mozilla-services/heka/pipeline or go test -v github.com/mozilla-services/heka/plugins/tcp.\n. First, there's not enough information here for me to be able to answer your question; I'm honestly not sure what it is you're trying to accomplish. Second, we use the issue tracker for bug reports and feature requests. For support we ask that you use either the mailing list or the #heka IRC channel on irc.mozilla.org. Best results in the IRC channel will typically come from asking during normal California working hours, otherwise the mailing list is probably a better choice.\n. It's not a feature request unless you know that the feature doesn't already exist. The only way to find that out is to ask whether or not it's possible and, if it is, how to do it.\nIn this case it is possible, you just have to put the lua plugins you want to include into the source tree (e.g. decoders in the sandbox/lua/decoders folder, filters in the sandbox/lua/filters folder) before you run cpack or make package to build your packages.\n. Fixed in #1831. Thanks so much to @tclh123 for submitting the code. :)\n. Support questions should be directed at the mailing list, the issue tracker is for bug reports and feature requests.\n. No, this is correct. MAX_RECORD_SIZE is MAX_MESSAGE_SIZE plus MAX_HEADER_SIZE, i.e. the maximum size of a framed message. When loading data from the outside world, as LogstreamerInput is doing, it may include framing, and the entire message/header combination needs to fit in the buffer. While flowing through Heka, however, there is no framing, so MAX_RECORD_SIZE is what applies. The queue_buffer code only deals with messages (no headers), so MAX_RECORD_SIZE is appropriate there.\n. This is awesome, thanks so much!\n. Relevant: mozilla-services/lua_sandbox#118.\n. Yes, this is a known issue. Unfortunately, github automatically adds those \"source tarball\" links to each release, and AFAIK there's no way to remove them. If it were possible to remove the links altogether I'd definitely do it.\n. The issue tracker is for bug reports and feature requests. Support requests should be directed to the mailing list: https://mail.mozilla.org/listinfo/heka\nThanks.\n. I'm guessing this is supposed to be a feature request? In that case, it should be opened as an issue, and not a pull request. This is a PR to merge our dev branch into the versions/0.10 branch, which should never happen. Closing this, please reopen as an issue.\nThanks!\n. \"Exactly once\" delivery semantics is for all intents and purposes impossible. Heka doesn't yet achieve its goal (hindsight does a better job here), but in all cases Heka will choose an \"at least once\" strategy over one of \"at most once\".\nIn other words, yes, in pathological situations you might see the same message(s) delivered multiple times. That's by design, because the assumption is that it's better to get everything with possible duplicates than to miss events with no way to tell that anything has been lost. If duplication isn't acceptable in your use case, the recommended approach is to make sure you pass along each event's UUID and use this to perform de-duplication at the destination.\n. I'm afraid this isn't the right way to get a new plugin into the Heka core. What you'll want to do is to actually integrate your plugin into the Heka code tree and open from your repo to this repo that shows all of the code that your repo contains. Here's an example of a PR that introduces a new Go plugin into the Heka code base:\nhttps://github.com/mozilla-services/heka/pull/1491\nThat one does a pretty good job, in that it updates the documentation and the changelog in addition to providing the implementation. It doesn't have any tests, however, which would be even better.\nPlease compare that PR to this one, hopefully that will help you see how to approach this differently.\nThanks for your efforts!\n. I'm wary of this one, especially since it's named \"MultilineSplitter\" when it may or may not be the right choice for all multi-line splitting. You say that the \"other options for supporting them don't work as well as [you]'d like\". I assume that the RegexSplitter is the other option to which you're referring. Can you explain exactly what it is about the RegexSplitter that's causing you pain, and how this splitter resolves the issue?\nThanks!\n. I don't have any specific comments on the implementation yet, I haven't dug in; I'm working my way through your DockerLogInput overhaul first. In general, though, I'm averse to having two splitter implementations that are so similar to each other. If at all reasonably possible, I'd much rather find a way to combine them into a single splitter which provides the functionality you need but which doesn't sacrifice any performance for the simpler cases already supported by the RegexSplitter. When I do get to reviewing this PR, I'll be doing so with an eye for how we might merge the two.\n. I've looked at this (and your explanation for this) a little bit more deeply, and I still don't understand what this provides that the RegexSplitter doesn't. Either I'm not understanding what your splitter provides, or you're not fully understanding the RegexSplitter behaviour, not sure which. I'll respond to your 4 points here:\n1. Despite the perhaps poorly named delimiter_eol setting, RegexSplitter doesn't match only on the beginning or end of a line, it matches the beginning or end of a record. The regex you use can match multiple lines. The regex can match the entire record, if need be, as long as you use a match group to make sure that what you want to keep is included in the output record.\n2. I don't understand this. The record generated by the RegexSplitter will contain everything not matched by the provided regular expression plus the contents of any match group in the regular expression, with the match group data at either the beginning or end of the record depending on the delimiter_eol value. How does it miss the first line in a stacktrace?\n3. I also don't understand this. What do leading characters (or lack thereof) have to do with finding the text that denotes a record boundary?\n4. The purpose of the match group is specify what part, if any, of the matched regular expression should be included in the resulting record. If your regular expression doesn't contain a match group, then the text matched by the regex will be dropped from the resulting record. If it does contain a match group, then the text matched by the group will be included in the resulting record. All text in between the matched delimiter will always be included in the resulting record. I suppose we could add support for multiple match groups here, but as it is you can always put a match group around your entire regex to make sure you get all of it, and then parse out the information that you care about in the decoding layer.\nTo restate, the existing RegexSplitter is meant to let you define a regular expression that will match the minimum identifiable record boundary. The generated record will contain all of the data in between those boundaries, however, not just what was matched by the regular expression. In fact, the regular expression match won't even be included, unless you specify a match group.\nHopefully that clarifies the behaviour of the existing splitter. But maybe you already understood that, and your implementation meets some use cases that RegexSplitter doesn't. If that's so, I'd love to get a more detailed explanation of exactly what those are, preferably with a set of records that I can examine for which the RegexSplitter is not sufficient. \n. Okay, now I understand the disconnect... it seems daft in retrospect, but I didn't keen on to the fact that you're trying to capture records of different types, i.e. multiline tracebacks embedded within log records that are only a single line.\nIf there were some way to get the framework to emit a timestamp (or other consistent piece of text) before every record, then RegexSplitter will work, it doesn't care about what's in the records, just that it found a delimiter. But I realize that might be hard with tracebacks, since they're generated by error conditions, so you probably don't get much control over what they look like.\nI have a few more thoughts on this, but have to go into a meeting now, more soon.\n. Sorry for the delay, my time for Heka stuff is pretty minimal these days.\nMy first thought is that my preference, from an end user perspective, would be to add support to the TokenSplitter and RegexSplitter to add an optional 'traceback' regex. Those splitters would behave exactly as they do now (w/ no performance loss) if there was no traceback regex. If one was included, they would use it to check for tracebacks along the way.\nThat might be more work than you're up for, though. I'm willing to include this splitter, but would want it to be renamed. MultilineSplitter is confusing, b/c it implies you need this to split any input with multiline records... that's part of what threw me, anyway. MultiPatternSplitter, maybe? TracebackSplitter? Anyway, there are a few other things that would also need to happen:\n- Add documentation to our Sphinx in the docs directory, clearly describing the use case and the performance trade-offs.\n- Add an entry to the 'features' section for 0.11 in the CHANGES.txt changelog.\nI'll also make some in-line comments, am reviewing the code now.\n. I can live with either MultiPatternSplitter or PatternGroupingSplitter, w/ slight pref for the first, as long as the use case is clearly explained in the docs.\n. First, thanks so much for your contribution, on this and the other PR, your work is very much appreciated. Overall this code looks great; I've made a bunch of comments inline, but as you can see they're mostly smaller issues, although a couple of them might have a bigger functional impact.\nI have one more monkey-wrench to throw into the mix, however. I've just recently gotten a request from inside Mozilla to investigate the possibility of getting the DockerLogInput to be able to maintain its location in the containers' log streams, so Heka can shut down and restart without necessarily losing log records from the container. The Docker API's logs endpoint, supported by fsouza's Go client, provides the since parameter that could be used for this purpose.\nI don't necessarily expect you to tackle that, but if it's something that would be useful to you as well and you feel like working on it while you're in there, then support for using logs instead of attach would be awesome. If not, then you can just clean up the work you've already done based on my comments, we'll merge it, and treat the stateful log tracking as a separate issue.\nOh, one more note: it'd be great if you could add a record to the CHANGES.txt changelog for the v0.11 release. Thanks!\n. In addition to removing the unrelated docker changes that got included in this PR, it needs an entry in the \"Backwards Incompatibilities\" section of the CHANGES.txt changelog for the 0.11.0 release.\nThanks!\n. I'm sure that if you look carefully through the rest of your startup log, you'll see that there is other text somewhere that explains in more detail what plugin is causing the error.\n. To clarify, this message is generated at the end of the start up process, showing you a count of how many errors were generated. If you look through the earlier messages, you should see some output generated by the plugin that caused the error with more details about what the specific problem is.\n. While I very much appreciate your contribution, and especially that you were thoughtful enough to include tests, I'm afraid I'll have to pass on this PR. Nearly all text parsing in Heka is done using Lua and LPEG in SandboxDecoders. Our LPEG test site even has a simple example of CSV parsing. \n. The main reason we don't allow hostname to be set in SandboxFilter plugins is as a security measure against dynamically injected filter code. In that scenario, we're allowing possibly untrusted code to run in the pipeline, and we don't want to allow a nefarious actor to be able to spoof other hosts.\nI'm up for allowing hostname to be modifiable from non-SandboxManaged filters (i.e. this.manager == nil), however. If someone is getting Lua code onto your host's file system and editing your config to use it, you probably have bigger problems than hostnames being spoofed in your Heka message... ;)\nAlso we'll want to add a note to the Features section of the CHANGES.txt changelog for the 0.11 release, and I'm pretty sure there's a place or two in the documentation that would need to be changed to reflect that non-dynamically injected filters allow hostname to be set.\n. I've cherry picked this commit (and added a changelog entry) onto the versions/0.10 branch so the bug fix exists there as well. Thanks for your contribution!\n. (see 3473f50a6cdd39d7d69eec16e870ea8fdd652df1)\n. Closes #1809.\n. Test was failing b/c sandbox config expects number values to be type int64 or float64, unspecified is type int. I fixed this, changed the name of the setting from separator_char to separator, and added a changelog entry. Merging now. Thanks for tackling this!\n. Thanks so much!\n. In the future, please direct support requests to the #heka channel on irc.mozilla.org or the mailing list.\nIn this case, you're probably using both tls = true and [HttpListenInput.tls] in your config, so it's complaining (correctly) that you're using tls twice. The correct setting is use_tls = true.\n. I'll go ahead and merge this, but I'll note that, as much as possible, decoders should be written so that when a message represents an HTTP request it uses the default schema expected by the http_status filter. It's meant to be a de facto standard for how HTTP requests should be represented in Heka message format.\n. Fixed in #1887.\n. This is great, but I don't want to introduce another dependency, would rather just embed the rate limiting code into Heka directly (w/ credit and link to original source, of course).\n. Oops, good catch. Got bit by an overzealous search-and-replace. Thanks.\n. Yes, sorry, I misinterpreted the thread as an r+ from Trink, so I merged. Happy to have it land in Heka for now.\n. Thanks for your contribution, but I'm going to leave it as is. We used to just have . build.sh, as you do, but we ended up w/ people commonly misreading it, using ./build.sh, and showing up in IRC asking why it wasn't working. We changed it to source build.sh while still including a reference to . build.sh for the folks who aren't using bash and we haven't had any confusion since. \n. I cherry-picked the commit back to the versions/0.10 branch.\n. After you run '. build.sh' from the root of the repo, then a 'build' directory containing a Makefile will be created. When the build finishes you will be in the the 'build' folder.\n. I know I said that I was open to including this in the Heka core, but I'm having second thoughts. I don't really want to add additional dependencies unless it's for an extremely common use case. I'd rather see this packaged up as a separate package that can easily be included in a Heka build using the plugin_loader.cmake stuff.\nAlso, for future reference, if a plugin is to be included in the Heka core, it needs to go in the changelog, and the documentation needs to be updated.\n. I'm not quite sure what's going on here. I'm not necessarily opposed to merging this, but the dev branch is building just fine for me, and I also don't understand why the Travis build is failing. What's the build failure you're seeing?\n. Overall this looks pretty good. I've added a few comments. The only additional note I have is related to the docs. Currently you've got the docs in the comments of the Lua file, and then another copy of them in the docs folder. For the other sandbox plugins, we just pull the Lua file comments into the generated docs so we don't have to maintain two copies. You can look at the apache access decoder page to see how it works.\n. The bind_query_log.rst file shouldn't contain any of the actual documentation, it should all be contained in the Lua comment. Sphinx pulls in all of the text in between two specially constructed (and explicitly specified) comment markers in the lua file. Please look carefully at the apache access log doc source and lua code to see what I mean.\n. Nope, this is fine. Thanks for your patience!\n. Sure. :+1:\n. Thanks for your contribution. In addition to the inline comment I made, you'll want to update the documentation (see esjson.rst and eslogstashv0.rst files in the docs/source/config/encoders folder) and add an entry to the CHANGES.txt changelog.\n. Great, thanks!\n. In contrast to python docstrings, godoc wants to have the explanatory comments immediately before the entity they're describing.\n. Seems weird that you'd need to dereference config here, none of the other plugins are doing so. Maps are reference objects by nature, you rarely need to use pointers to a map.\n. Is there a risk that Fields might not contain a \"name\" key? If it doesn't, Fields[\"name\"] will be nil and the type assertion will probably result in a panic.\n. First (and this took me a while to internalize) in general the Go idiom for getting a pointer to a zeroed struct is to use \"new\", i.e. self := new(StatsdOutput). Second, if we're not doing anything w/ the statsdClient in this constructor, then why do we have it? The plugin bootstrapping code can just call new(StatsdOutput) itself.\n. Again, new(SimpleT) is the Go-ish way to do this. But don't worry, I know you stole this code from me... ;)\n. So are you trying to blame me for something horrible, or are you just selling yourself short? ;)\n. Super helpful that you added this script, thanks!\n. Awesome, I'd been meaning to add this!\n. graterd is actually defunct now, it's just a config file against hekad. We should probably include a sample config file for folks to use to get started. Also, after the install, you can run the code using \"$GOPATH/bin/graterd\" (or, now, \"$GOPATH/bin/hekad\") instead of needing to use \"go run\".\n. I've changed the metlog-py client to only ever send UTC timestamps, w/ the \"Z\" at the end per RFC3339, so I think we can do away w/ heka/time altogether for now.\n. Seems like maybe we want this error output to come after the type assertion, since there a many cases other than \"nil\" which will cause key to be invalid. You can use the \"key, ok := msg.Fields[\"name\"].(string)\" spelling to explicitly check if the type assertion is valid, preventing a panic which will happen if you don't check the second return value.\n. Map lookups are relatively expensive vs. other types of access (struct values, sequential arrays, etc) so we want to avoid doing them multiple times if possible. It's possible that the compiler will optimize this away in some cases, but I don't really know if that's the case, it'd probably be best to assume that it will not. If you want to avoid having to allocate and GC a new variable for every message (which is A Good Idea\u2122) you could put another variable in the StatsdOutput struct and reuse it for each message. This is safe b/c we're now creating a separate instance of each output struct for each individual pipeline pack, i.e. each output struct instance will only be used for one message at a time.\n. See above re: avoiding panics in the type assertion and avoiding unnecessary map field lookups.\n. Yet more awesomeness you're adding to the overall setup. You da bomb!\n. There's also a statsd output which also has a StatsdWriter. This will prolly happen a lot. Names are hard. :(\n. This seems weird. Isn't that just the same as return makeRunner?\n. Should we be putting the outData on the dataChan even if PrepOutData returns an error?\n. Not a big deal, but I notice us python guys tend to forget about \"switch\", which I think would be more idiomatic here.\n. I've taken to using pack for this variable name. I mean, don't get me wrong, I like alliteration and all, but all those \"p\"s somehow seem to make the code harder to read...\n. begin is already 0, isn't it?\n. I think this can go in an else clause from the if above; fm.seek[fileName] will already be offset otherwise (that's where we got it).\n. Should we check for an error again here and return it if we find one, in case our file descriptor has somehow gotten truly fubar'ed?\n. Might be nice to have one or both of these configurable. With these defaults, of course.\n. Nice use of for var == val, I forget that this is an option.\n. TODO?\n. Pretty sure this will cause lots of GC churn. Maybe messages need to have a Zero method too?\n. Good fix.\n. Good catch. It wasn't possible when I first wrote that line of code, but since I've changed the decoder mappings to come from config then it actually is possible. As I type this I also realize that I didn't actually set up any default decoders, so people have to specify this in the config even if they want the obvious choice of mapping JSON to our JsonDecoder and protobuf to our ProtobufDecoder. Maybe we should add such defaults?\n. IIUC then val := reflect.ValueOf(*matchLayout) would be equivalent to this line. I wonder if it would be more efficient...\n. Oh, duh, this is only happening at startup, isn't it? Never mind.\n. This isn't part of a generic interface, is it? Any reason to not make the contract more explicit by using []byte as the arg type?\n. Isn't it true that if we don't have to increment/decrement the ref count unless the chain is actually a match? Seems like we could avoid a lot of possible lock contention in cases where there are lots of chains but only a few of them match each specific message.\n. Ah, right. LGTM, then, merging now.\n. Mainly b/c this was converted from the RunnerPlugin pattern where we had to use pointers due to the way that zeroing the data objects worked. And we'll probably end up needing something similar if we support a batch setup that works even when the data might be in a struct instead of in a []byte.\nThat remains to be seen, though.\n. Yup, this is part of what I've been hanging back from doing. Planning \non closing the input channel to signal a stop event.\nOn Tue 05 Mar 2013 02:07:20 PM PST, Mike Trinkala wrote:\n\nIn pipeline/outputs.go:\n\n+func (o FileOutput) Start(wg sync.WaitGroup) {\n-   o.wg = wg\n-   wg.Add(1)\n-   go o.receiver()\n-   go o.committer()\n  +}\n  +\n  +func (o FileOutput) receiver() {\n-   var pack PipelinePack\n-   var err error\n-   var ok bool\n-   ticker := time.Tick(time.Duration(o.flushInterval) * time.Millisecond)\n-   outBatch := make([]byte, 0, 10000)\n-   outBytes := make([]byte, 0, 1000)\n  +\n-   stopChan := make(chan interface{})\n\nWe should probably move away from a global stop and have the ability\nto stop and restart individual components i.e. add Stop() to the\ninterface.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/39/files#r3252192.\n. This is a teeny little nit but standard Go idiom is to put this on a single line:\n\ngo\nif _, err := os.Stat(path); err == nil {\n    return true\n}\nreturn false\nGiven that, it might not even be worth a separate function.\n. Or it might, if you're using it in a compound condition like this one. ;)\n. A bit confused about what's happening here... won't (*headerBytes)[2:2] always be an empty slice?\n. A method like this is where go's idiom of declaring the return var in the fn signature is nice.\n. A config file named \"trink\" isn't going to make much sense to a random person perusing our source code. We should put this (and some other) in a better location, w/ a descriptive name so folks can use it to bootstrap their own configurations.\n. You've currently got 3 different files all redefining the main function in the main package. I've verified that this doesn't work by trying to run: go install github.com/mozilla-services/heka/flood and compilation failures, whereas it used to compile and install the flood binary. AFAIK each separate binary that you'd like go to generate needs to be in its own package.\n. Right, got it. The slice is empty, but it's still tied to the underlying array. n/m\n. I'd rename this to simply Filter for parity w/ the nearly identical Output method above. (Yes, I know there's technically a difference btn a Filter and a FilterRunner, but once they've been bundled together and started up they're so tightly coupled that I usually just think of them as a unit.)\n. I'd remove these from this interface altogether. I think these are a bit too \"bad-touch\"-y to expose at such a high level of visibility. You can leave the methods on *PipelineConfig and access the config object off of the pipeline pack that you get (more on that below).\n. Okay, I went through the code below again and realized that there's exactly one place (the this.restoreSandboxes call in SandboxManagerFilter.Run() where you don't have access to a pipeline pack from which to get the config object. I can see two choices:\n- Cheat and use a type assertion at that point (i.e. pConfig := h.(*PipelineConfig)).\n- Add a PipelineConfig() method to the PluginHelper API so you can just get at it more easily. If we do this, we might even just remove the Config attribute from the PipelinePack struct. It doesn't really belong there anyway.\nI'm leaning towards option 2.\n. Ah, right, fair enough. It's not important to remove the config \nreference right now. We might change it moving forward, though, esp if \nwe do what we've talked about before w/ having two separate \nPipelinePack pools, one for the main loop and one for the MGI. If we do \nthat, it might make sense to have each pack have a reference directly \nto the recycle channel, since it might be a different channel for \ndifferent packs.\nOn Mon 08 Apr 2013 07:41:31 AM PDT, Mike Trinkala wrote:\n\nIn pipeline/config.go:\n\n@@ -45,6 +45,9 @@ type PluginHelper interface {\n    PackSupply() chan PipelinePack\n    Output(name string) (oRunner OutputRunner, ok bool)\n    Router() (router MessageRouter)\n-   FindFilterRunner(name string) (fRunner FilterRunner, ok bool)\n-   AddFilterRunner(fRunner FilterRunner) error\n-   RemoveFilterRunner(name string) bool\n\nThe PipelinePack still needs access to the RecycleChan. So the Config\ncannot be removed without adding a helper interface or reference to\nthe RecycleChan.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/87/files#r3695433.\n. This is true, but the problem already exists on dev, so it's not a blocker for this merge. Issue #97 opened re: the PipelinePack exhaustion issue.\n. Yup, I knew that. Not ideal, but wasn't really sure how else to handle it w/o getting into bigger changes than I wanted to right now. Open to suggestions.\n. This and all of the sender related code below is a lot of duplication of what's in the client package. There's nothing fixed w.r.t. the client package implementation and/or design... seems like we should be able to find a way to change client so that we don't have all the duplication?\n. Urf. This points to a design flaw. Decoders should have a handle to their DecoderRunner so they can call runner.LogError like the other types do.\n. Whoops, actually, never mind. For some reason I misread that as outputting a log message directly rather than returning it. But, since the error is being returned to the DecoderRunner, which is then calling self.LogError, we don't need to put the identifier text around it, that will be done for us. Which should reduce this entire function to simply return json.Unmarshal(pack.MsgBytes, pack.Message).\n. Same here... the runner will add decoder name to the error output.\n. Just curious: Was there a problem from running RemoveFilterRunner during shutdown, or did you just realize that it was unnecessary?\n. Not a big deal, but I've typically stored the runner and the helper as attributes on my plugin structs (when Run is called) rather than passing them around as method arguments.\n. I know we're logging a message from the PipelinePack call when the loop count is too high, but we probably want to do so from here (via fr.LogError()) instead or too, so folks have more clues re: which plugins are actually causing the problem.\n. Also, isn't it true that msgLoopCount is an unrelated value here, just set to the loop count value of the last message we happened to process? I imagined that new filter-generated messages would typically be fetched using h.PipelinePack(0).\n. And, as mentioned above, it might make more sense to just use h.PipelinePack(0).\n. Same notes as above (i.e. maybe use 0 arg, if there is an error log here too).\n. Pretty sure this was only used internally to the MGI implementation, so it can also be removed.\n. Hooray for getting rid of so much code!\n. Grr. We should really hack gomock to actually regenerate the same output given the same inputs. \n. Might be more semantically clear to only remove the filter from the config if Run returns an error, i.e:\n\ngo\n    if filter, ok := foRunner.plugin.(Filter); ok {\n            if err = filter.Run(foRunner, h); err != nil {\n                    h.PipelineConfig().RemoveFilterRunner(foRunner.name)\n            }\n    } else if output, ok := ....\nThis should also let you remove the if Globals().Stopping check from within RemoveFilterRunner.\n. From within a plugin or a runner we should use runner.LogError() which will include the plugin name for us.\n. I'm thinking it might make sense to remove Router() from the PluginHelper API altogether, so folks don't accidentally subvert the self-injection check by grabbing the router themselves. In this runner code, we can still get to it via helper.PipelineConfig().router. Note this would mean the InputRunner would also have to grow an Inject method, which would do nothing more than put the message on the router's channel.\n. Do we still need to store this on the router?\n. We should use fr.LogError() to log a notice here re: which plugin is exceeding the loop count.\n. I'm more concerned about that case w/ the Lua plugins than the Go ones, but this is fine.\n. Ugh. So much dependence on Globals().Stopping smells a bit funny to me, but yeah, okay.\n. Maybe it makes sense to set some defaults here?\n. fmt.Sprintf is fine so far, but I suspect we'll want to switch to using Go's html/template moving forward.\n. This would probably read more cleanly if we constructed a map and then used the json package to serialize it instead of doing string manipulation. Not sure what the performance impact would be, though, probably not worth it if it's a lot slower.\n. Good question. I think reserving a block, maybe up to 20. We don't want to reserve too big a number, though, otherwise we'll be creating overly large encoder lookup arrays w/ a bunch of empty values.\n. Yeah, Ben's gonna fill this in, it's tracked by issue #129.\n. Seems reasonable to me. Eventually we probably want to be able to define a root hekad working directory and have all of the plugin folders relative to that, but for now just setting them consistently is fine.\n. Typo\n. Right here I would add a note that the above steps only have to be done once, to set up the communication btn hekad and sbmgr, and the rest of the steps should be done for each individual filter. Might even be worth separating them a bit and starting the numbers over at one, to highlight how few steps must be taken to add a filter.\n. Actually, as of right now, master should stay w/ 0.3 as the version. I've realized the way the branches are set up now is less than ideal, but I'm not going to change this until we cut a versions/0.3 release branch, so master will never reflect a 0.2 release.\n. If we're dynamically generating the lua_sandbox.go file (which I don't love, but I don't see a better option ATM), it'd be great to add lua_sandbox.go to .gitignore so it doesn't pollute our diffs and etc.\n. Typo.\n. The method name is a bit vague, I think; there's nothing obviously linking it w/ the restart functionality. What if we called it RestartCheck() (restart bool), w/ the return value giving the plugin a chance to tell us whether it really wants to restart or not?\n. Debug output?\n. I'm thinking all of the *Wrapper attributes should maybe be made private. They're really just an implementation detail re: the way that we initialize plugins, and they probably make the public API more confusing while giving us less flexibility to change how config works.\n. The last sentence seems a bit unclear. Maybe change it to something like this?:\nPlugins specify that they support restarting by implementing the Restarting interface (see :ref:restarting_plugins).\n. s/governs/govern/\n. Can we rename this to CleanupForRestart maybe? That links the name of the method to the interface that it's providing, otherwise it's not very obvious that implementing Cleanup implies that it's a restartable plugin.\n. Not a big deal, but we don't actually need the label here, do we?\n. I'm thinking it's probably smart to have Shutdown take a message, so we can start telling folks why their Heka just quit.\n. You shouldn't need the goroutine if you give retainChan a buffer of 1.\n. Right, either is good. Although even if the plugin logs an error, it \nmight be nice to have a shutdown message that says \"Heka shut down due \nto  failure\" or something.\nOn Wed May 15 09:35:58 2013, Ben Bangert wrote:\n\nIn pipeline/pipeline_runner.go:\n\n+\n-       // Are we supposed to stop? Save ourselves some time by exiting now\n-       if globals.Stopping {\n-           return\n-       }\n  +\n-       // If its a lua sandbox, we let it shut down\n-       if _, ok := foRunner.plugin.(*SandboxFilter); ok {\n-           return\n-       }\n  +\n-       // We stop and let this quit if its not a restarting plugin\n-       if recon, ok := foRunner.plugin.(Restarting); ok {\n-           recon.Cleanup()\n-       } else {\n-           globals.ShutDown()\n\nA Shutdown message, or have the runner's drop an error. If the runner\ndrops an error, the LogError takes care of indicating which plugin\nthrew the error as well which seems handy.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/177/files#r4238493.\n. If you use a buffered channel (buffer size one), then the channel put \ndoesn't block:\n\n`retainChan := make(chan *PipelineCapture, 1)`\nOn 5/15/13 9:37 AM, Ben Bangert wrote:\n\nIn pipeline/pipeline_runner.go:\n\nfunc (foRunner foRunner) InChan() (inChan chan PipelineCapture) {\n-    if foRunner.retainPack != nil {\n-        retainChan := make(chan *PipelineCapture)\n-        go func() {\n-            retainChan <- foRunner.retainPack\n-            foRunner.retainPack = nil\n-            close(retainChan)\n-        }()\n-        return retainChan\n-    }\n\nPlugin's need to call InChan() and get a channel back immediately before\nthey can proceed. If this doesn't use a goroutine, the method call will\nblock forever until the other side takes the pack, which it can't do\nsince the runner will block then on starting the plugin...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/177/files#r4238521.\n. Closing a channel after you put something on it is fine, a channel \nwon't actually be closed to the code pulling from the channel until all \nof the contents that were on the channel when it was closed are \ndrained. The other issue is a good point. You could check to see if \nthere's a pack on the channel, of course.\n\nAll in all it's not that big a deal. Not using a goroutine appeals to \nme b/c a) spinning up goroutines are cheap, but not free and b) when \nyou spin up the goroutine, it's quite likely that the code that puts a \npack on the channel and the code that pulls from the channel will run \nat about the same time, causing lock contention for the channel. If you \ndon't, then clearly there won't be any lock contention. It's not like \nthis is in a tight loop or anything, though, so I'll leave it to you.\nAnd with that, I'm out the door to catch an airplane. See ya next week!\nOn Wed May 15 09:44:10 2013, Ben Bangert wrote:\n\nIn pipeline/pipeline_runner.go:\n\nfunc (foRunner foRunner) InChan() (inChan chan PipelineCapture) {\n-   if foRunner.retainPack != nil {\n-       retainChan := make(chan *PipelineCapture)\n-       go func() {\n-           retainChan <- foRunner.retainPack\n-           foRunner.retainPack = nil\n-           close(retainChan)\n-       }()\n-       return retainChan\n-   }\n\nAlso, the channel has to be closed after its consumed, if it returns a\nchannel of 1 with 1 packet, the plugin will never exit even though\nthis channel will never get anything on it again...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/177/files#r4238660.\n. great catch, thanks! but we also want to recycle the pack if there is an error & we continue the loop. mind adding that too?\n. If I'm understanding correctly, MatchRunners are only placed on these channels when they're going to be removed from the set of available runners. Be nice to rename these methods (and the corresponding struct attributes) to reflect this (by prepending Closed, maybe?). \n. Would it make sense to use maxJitter (in milliseconds) as the max value on the rand.Int() call, instead of having the jitter length weighted towards the max?\n. If there's only a single channel you're listening on and no default clause then you don't need a select, see http://play.golang.org/p/VG5BQlH7ug\n. Do we want some error output here?\n. I think this is another case where you don't need the select. Which would then mean you don't need the label, although that's not a big deal.\n. 'EnvVersion' is short for 'envelope version' and isn't really related to the heka release version at all. The current value of 0.8 is maybe not the best choice (it's left over from the numbering started when we were still \"metlog\") but we shouldn't be changing it unless we're changing the actual structure of our message objects.\n\nThis opens up a bigger issue re: how to manage compatibility through protocol and data model changes, which we should probably think through a bit more carefully than we have.\n. Severity and Pid will automatically default to 0. The clarity of explicitly setting them might be worth the (probably minimal) overhead of making unnecessary function calls, though. I'll leave it to you to decide.\n. I think we maybe want to use os.TempDir() instead of hard-coding '/tmp' here.\n. Total nit, but you can get rid of both of the vars above by using := instead.\n. Generally we're trying to avoid using log.Printf directly. Would it be hard to expose access to the associated LogfileInput's InputRunner so you can call ir.LogError() (or ir.LogMessage()) instead?\n. Have we removed this config option?\n. A bit more clarity, and a typo fix:\n\"Full filesystem path to a journal file that will be used to keep track of the last read position. This enables Heka to...\"\n. When there's only a single case and no default clause, you don't need a select at all, the channel read will just block until a message comes through or the channel is closed.\nBut in this case you don't even need that, b/c you can do it all w/ a single for loop over the range of the channel:\ngo\nfor msg := range lw.msgChan {\n        if msg.err {\n                 ir.LogError(fmt.Errorf(msg.msg))\n        } else {\n                ir.LogMessage(msg.msg))\n        }\n}\n. This all works well, but I'm wondering if it might not be more straightforward to just pass the InputRunner to the FileMonitor and let them call LogError and/or LogMessage directly?\n. Also, thinking about this a bit more, we should clearly define what the default behavior is here. In an ideal world, there would be a global heka runtime folder (e.g. /var/run/hekad/...), and the seekjournals would default to automatically putting themselves in a specific place relative to the global heka root. The world isn't ideal, however, and heka doesn't yet support the notion of a \"global heka runtime folder\", but we should still probably pick a reasonable default for this.\n. Doesn't change the functionality at all, but shorthand for the above is:\ngo\nif err = fm.recoverSeekPosition(); err != nil {\n        return\n}\n. This is meant to replace the LogfileInput test that's in the inputs_test.go file, right? If so we should remove that one.\n. Hrm. I see your point here. Even so, I'd like to avoid having an add'l goroutine for each LogfileInput just for this message passing; it makes more to noise to sift through when trying to extract signal from a dump of hekad stack traces, for no clear benefit. How about we add a Log(msg string, isErr bool) method to the LogfileInput? Then we pass the input to the FileMonitor and the monitor always uses the Log method.\nInside the Log method we can check to see if we have an InputRunner yet. If yes, we call ir.LogError() or ir.LogMessage() right away. If no, then we store the messages (maybe in a channel, although even just a slice is probably good enough). When Run() is called (i.e. when we are first handed our InputRunner) we can iterate through the slice and output all of the messages. This would eliminate the need for a long running goroutine, and should also mean we can get rid of the FileMonitorMessage struct.\n. Ideally this would be a configurable setting, but I'd settle for at least using a package wide SAMPLE_DENOMINATOR constant here so the value is a bit less buried in the code.\n. One neat way that you can do this is to use the same stopChan in both places and then close the channel to signal the stop instead of sending a message over the channel. If you close the channel instead of send over it you can broadcast to every listener on the channel simultaneously, perfect for one time 1:N messaging like this. \n. Maybe rename this to 'fd' since it's no longer plural?\n. Whoops, this shouldn't get merged.\n. And empty return would work here, since ok is already false.\n. Clever way to handle all the conditions.\n. Comment?\n. Usually we declare an err variable like this in the function signature. Then 3 of your 4 returns could just become bare return.\n. And more places where you don't need to add the return value.\n. I'd change:\n\"to either the web or stdout.\"\nto:\n\"to either an outgoing Heka message (and, through the DashboardOutput, to a web dashboard) or to stdout.\"\n. This is a pretty efficient way to represent the report data that lends itself pretty well to subsequent machine parsing, but I don't think it's ideal for our situation. The main use case for the stdout report is a situation where a) Heka message delivery is hosed and therefore the dashboard (which depends on message delivery through the router) doesn't work and b) an ops person is trying to figure out what's going on. Which means that ideally the user would be able to see what's going on with just a visual scan of the output.\nI'd prefer a format where the component name is on one line, and there are subsequent indented lines for each pertinent data point for that component, clearly labeled. Something like this:\ninputRecycleChan:\n    InChanCapacity: 100\n    InChanLength: 99\ninjectRecycleChan:\n    InChanCapacity: 100\n    InChanLength: 98\nRouter:\n    InChanCapacity: 50\n    InChanLength: 0\n    ProcessMessageCount: 26\netc.\n. Are these files supposed to have platform build flags so each is only used on a specific platform?\n. BTW improvements on this theme are welcome. My main concern is human parse-ability.\n. Does it make sense to discard the last line in a file b/c it's not terminated w/ a newline character? Seems like it might still be useful to pass on whatever content we did get.\n. Couple of places where \"pipe delimited\" should be changed to \"plain text\".\n. Debug droppings?\n. Seems like maybe reportStdOut() should just be formatTextReport(), returning the string, and we can do the pc.log() call here, i.e.:\ngo\npc.log(pc.formatTextReport(report_type, msg_payload))\nThen it would be possible to get a text formatted report without actually sending it to stdout, were that ever desired.\n. Is there a reason for making all of these values global to the entire package? For other plugins, we've just hard-coded the configuration defaults into the ConfigStruct() method, or stored constant values as attributes on the plugin struct.\n. We're not strict about 80 character widths, but we do try to keep our lines from growing indefinitely to the right if we can help it.\n. The or.LogError() call that this error will eventually be delegated to already includes the plugin name and that it's an error in the output, you can just emit the minimal clear error message.\n. Was surprised to see doBulkRequest spun up in a goroutine. The idea is that the slow operation will happen directly in the committer method, so if it becomes really slow it will apply the appropriate back-pressure back to the receiver (since the batchChan will block) and then further back out the system.\nAs it is now, don't we risk overwriting the array underneath the outBatch value before it's done being used, if doBulkRequest is very slow and the flush interval is small?\n. We're trying to keep all plugin generated error messages going through the runner's LogError method, rather than just outputting straight to the console. For one, it means your plugin's name will be output for you. More importantly, though, it makes it easier for us to do other things (like, oh, generate Heka messages, maybe) with the log output down the road.\n. I've been updating the year when I notice.\n. These values (and a couple more that I think you may have missed) are actually embedded in the PluginGlobals struct, as the TOML metadata for the struct fields. Is there a way we can read the list from there rather than keeping another copy of it?\n. This is the only time we use the 'payload' variable. Maybe we save a mem copy by just splitting directly from the pack?:\ngo\n    pack = plc.Pack\n    lines := strings.Split(strings.Trim(pack.Message.GetPayload(), \" \\n\"), \"\\n\")\n    pack.Recycle()\n. We're opening a connection (and registering a deferred) for each metric we have. Is that how carbon works, or can we just open a single connection and send all of them at once? \n. We're sending data out, not listening.\n. Unnecessary if statement.\n. Can't we just have this default to 10 like it did before?\n. Not a big deal to change this, but I've taken to assigning to local vars and using those inside of loops rather than making repeated API calls. It's not about performance, most of the function calls would probably end up in-lined anyway, but it does make it easier to use mocks in the tests since you know you'll only have to expect one call rather than N calls.\n. Heh, yeah, this is exactly what I mean. If you assign the tickchan to a local var outside the loop you won't need the AnyTimes() call.\n. So nice to be able to remove this kludge! \\o/\n. Oh. Heh. Whoops. Pay no attention to the conflicting feedback! ;)\nI guess if we really wanted to get rid of the time.Sleep() we could make it a struct var instead of a local var. I also think it's about time to head down to the hardware store... my yak razor might need a bit of sharpening.\n. Am I right in thinking this would be a pathological condition? If so, maybe we should emit a log message.\n. I just added a folder_perm option to the WhisperOutput, might want to steal it and use it here too so we can let folks use folder permissions other than 700.\n. We don't really want this in git, do we?\n. Is it worth deleting these test files when we're done?\n. Plugin code should use the plugin runner's LogError() or LogMessage() methods when logging. It automatically includes the plugin's name, and will make it easier to later support log messages generating heka messages in addition to writing to stdout.\n. We defined the variables outside the loop so they don't have to be re-allocated on the stack or redefined in the scope for each cycle of the loop.\n. Is there a reason to return ns here? The caller is already holding it, and we don't seem to be using this return value anywhere.\n. Maybe this should just delegate to ns.EmitInPayload() and ns.EmitInField() rather than duplicating the conditional check? These functions are small enough that they'll almost certain be in-lined by the compiler...\n. Er, d'oh! I knew that. :P\n. The JsonDecoder will only work for a JSON encoded Heka message... i.e. it won't work for arbitrary JSON. I've created #309 to capture that we want a JsonPayloadDecoder that can map arbitrary JSON to a message struct, but it doesn't exist yet. And even when it does, it won't really be useful unless someone has specified the right mapping from JSON values to message fields.\nAll of this is a long-winded way of saying maybe we shouldn't have a default decoder value.\n. I've been prefixing the message types that heka itself defines w/ a heka. prefix to minimize namespace collisions.\n. Since there's only one decoder in play here, it might make more sense to drop the pack on the DecoderRunner's input channel rather than doing it by hand here. This code was probably modeled after the LogfileInput, which does the decoding by hand b/c it supports multiple decoders and might need to iterate through them. Once there's a MultiDecoder (see #307) then the LogfileInput can also be switched to support only a single decoder, and it can use the input channel as well.\n. deferred code won't run until the function actually returns, but this response body is being used again and again within a loop. Maybe we should be calling resp.Body.Close() each time, when we're actually done using the response body, rather then in a defer?\n. Shouldn't this be a continue instead of a return?\n. Inputs always need a Stop() method so heka can tell them when it's time to stop, but closing the stopChan should be enough to stop the monitor, right? Is there ever a case where we actually need to use this method, and to send a stop signal as a message over the channel rather than a channel close?\n. I think we want path.Join(\"\", \"var\", \"run\", \"hekad\", \"whisper\") so we can still let the OS specify the path separator rather than doing it by hand.\n. Been thinking about this a bit more. It seems reasonable to me that someone might want to pass the returned HTTP response body through unchanged, maybe to play w/ it in a lua filter or something. So I'm thinking if the decoder name is specified but the decoder doesn't exist, then we should raise the error just like you're doing here. But if the decoder name is not set (i.e. conf.DecoderName == \"\"), then we shouldn't fail, we should instead just inject the pack directly into the router. Docs would need a tweak to reflect this too.\n. IIUC these are the same channel, you only need to close it once. In fact, I think the second close call would raise a panic.\n. s/packages/package/\n. This paragraph was copied from the LoglineDecoder, needs to be updated for PayloadJsonDecoder.\n. Here I would replace \"Heka\" with \"PayloadJsonDecoder's json_map config section\".\n. Maybe would should default to ISO8601 format for the TimestampLayout here? (see http://stackoverflow.com/questions/10286204/the-right-json-date-format#15952652)\n. s/regex/json/\n. More copy-pasta comments that need editing.\n. And here.\n. This chunk of code is duplicated from PayloadRegexDecoder, maybe it should be separated out into a utility function, alongside ForgivingTimeParse?\n. I know this is also copied over from PayloadRegexDecoder, but I'm wondering if the else isn't a mistake in both places. It reads to me like we're supporting a severity value only if we don't capture a timestamp, which seems weird? Am I reading correctly? Is that expected behavior?\n. Again, this is duplicated from PayloadRegexDecoder, might make sense to put the severity map lookup stuff in a utility function.\n. LoadHekadConfig creates a new, default-value initialized instance of the config object every time it's called. Wouldn't that mean the last file in the slice is the only one that will have any impact, i.e. any relevant hekad config in the earlier files would be lost?\n. Also, we should use path.filepath.Join here instead of just concatenating the strings, it will make the config more forgiving.\n. Same note here re: using path.filepath.Join.\n. Go's path package deals only w/ forward slash separated paths. To get the separator right across platforms you'll want to use the path.filepath package: http://golang.org/pkg/path/filepath/#Join\n. Here too we'll want path.filepath.\n. Not sure we should be bailing out of the loop here. I'll admit it's a bit weird, but technically it should work fine to have a [hekad] section spread across multiple files; if any value is defined more than once then the last one evaluated will be chosen. Go's ioutil.ReadDir always lists the files in the same (alphabetical) order, so the same config will always produce the same result.\nThis would match the behavior of the rest of the config sections; there's nothing preventing someone from putting the same section title in two different files, and currently the code would load them in order, merging them into one config, with values from the later files \"winning\" over values from earlier ones in cases of conflict.\nThis might even become a useful testing and debugging tool. Users could temporarily add a \"zzz.toml\" file where they overwrite config that was defined earlier; deleting the file would return the config to its original condition.\n. I think you accidentally a word.\n. TOML is case insensitive, but I've been using all lowercase in the docs for consistency.\n. It's a bit lame that Heka makes you do this. We gotta come up w/ a better way to manage logging from plugin Init() methods. Out of scope for this pull request, just ranting.\n. We've recently removed all of the generic panic recovery code from Heka, since it wasn't really protecting us and in fact just makes debugging harder when there's a real problem. Merging from dev into this branch will remove the instances that already existed, but this one will need to be removed by hand.\n. I'm a bit confused about what's happening here. If none of the decoders succeeds, then we're changing the message type and then are re-routing the message back through the same MultiDecoder. It seems to me that in most cases this would cause an infinite loop.\nIn general, the \"catch-all\" idea is a good one, but it gets more complicated if we introduce the ability to run all decoders, instead of stopping at the first success. I'm thinking that for now we should maybe just remove this feature. Users can do it themselves with a custom decoder for now, we can work out if there's a reasonable way to do it for them later.\n. So does this mean we only support single character tokens? Is this for performance reasons?\n. Seems a bit odd that this is a method instead of just exposing the delimiter attribute directly. I can understand using a method so we stay parallel w/ the RegexParser, which does more work, but if that's the case we should maybe return an error here too so we have the same function signature.\n. This took me a few times of reading to parse correctly. How about replacing \"The value is always...\" with the following?:\n\"NOTE: If the parser_type option is set to message.proto, then the decoder setting will be ignored and a ProtobufDecoder will always be used.\"\n. Or even better, just mark this option as \"Only used for token or regexp parsers.\" and enforce that in the Init() method (i.e. parser_type = message.proto and a specified decoder value yields a config error) for one less foot target in the world.\n. It smells a bit weird to me that we're actually expecting a decoder named 'ProtobufDecoder' to exist in the Go code. Maybe we should special case it somehow so that we always have access to one if we need one?\n. Urgh. I hadn't realized that ProtobufDecoder would always get created, even if another name was explicitly used, but there it is in the code, plain as day.\nThe original intent was to have the most common useful setup be there by default, but to allow the users to override that w/ custom decoder setups if they wanted to. Sounds like we succeeded w/ the first goal, but failed somewhat w/ the second.\nNow that the decoder usage patterns have changed a bit (i.e. there's no more \"expected set of decoders\" implied by a Heka protocol enum), I'm inclined to agree that we should stop creating the ProtobufDecoder by default and require that it be explicitly added and used, like the rest.\n. +1 to requiring users to specify the decoder in all cases.\nAnd yes, we should revisit how exactly we're using MsgBytes and Payload. The original idea was that MsgBytes (if used) would contain a binary representation of the entire message object, hence the name. This also explains why arbitrary text, JSON, and XML data was put in the Payload instead; that's data contained within a message object, not a representation of a full struct.\nThat model has probably outgrown it's usefulness, though. Maybe it makes more sense to replace \"MsgBytes\" w/ \"RawMsgInput\", which would always be populated w/ the raw, chunked input data for the given message, whether or not it represented the entire message object?\n. Urf. It pains me to see us putting using log.Printf for such information. I know that we don't yet have a better story for generating output that doesn't come from within a plugin (i.e. no runner exposing LogError and LogMessage) but I still think we should move in the right direction. Maybe this could populate a data structure w/ the raw output data and pass that data structure to another method which handles the output? It would also be good to get this info into Heka's internal report which can be written to stdout or the DashboardOutput.\n. That's true re: LogError and LogMessage, but the point isn't what they do today, the point is that we can (and will, eventually, see #105) change the behavior in the future w/o having to separately visit hundreds of different places where log.Print is in use.\nAnd yes, we want to be able to get the data out even when message delivery isn't working. But when message delivery is working, which is most of the time, messages are a very useful mechanism. That's why the Heka internal report supports both... it will send a message, typically, but SIGUSR1 will always cause the reporting output to be sent directly to stdout.\nSo my original opinion still holds; I think we should package this diagnostic info up in a data structure, and then pass that to a method. That method could use log.Print to send the output directly to stdout, and/or it could make sure the data shows up on Heka's internal report, which is always available to the user w/ SIGUSR1, regardless of whether or not messages are flowing.\n. This fine for now, but looking at it I'm realizing that we'll ultimately want to take this even farther. A central registry that maps parser type identifiers w/ their actual implementations would prevent each parser-supporting input from having to duplicate the above block of code.\n. This test seems flawed; wouldn't it only work w/ id's that contained just a single interpolation w/ no extra text? Some other thoughts:\n- Feels weird to me that interpolation would remove the \"%{\" & \"}\" values in cases where the field name wasn't found. Seems like the ctrl chars should be left in the string to make it clear that interpolation didn't happen.\n- Rather than using a bespoke comparison to check for interpolation failure, interpolateFlag could return a value that tells whether or not there were any interpolation failures.\n. I considered that, but a) you'll lose a tiny bit of context in the error message, b) it was a bigger change from what was already in place, and c) (most importantly) this is all going to be revisited and changed more radically as soon as #424 is implemented, so I just left it as is for now.\n. Yup. The slice still has capacity that can be written to. pbuf[0] points to the same item in the same underlying array as outBytes[message.HEADER_DELIMITER_SIZE].\n. Do we still need to be populating this?\n. This doesn't accomplish the same thing. anyMatch means that at least one of the decoders actually succeeded. In firstWins cases, anyMatch isn't needed b/c a success returns immediately, but in all cases, we need anyMatch to tell us that (for instance) the first of three decoders succeeded. \n. I don't love 'first-wins', either, but I've tried a couple of the other phrases on, and none of them really seem any better. Leaving as is for now.\n. Added a commit that introduces an iota. That improves the comparison, we can check against the int constant rather than a string, but we still have to use the string type in the config or else the config validation won't work.\n. It's a very minor style nit, but it feels weird to me to use an upper case name for a variable that's not available outside the package, and also to be shadowing the WantsName interface. Might it be better to use lower case for your local var here? i.e.:\ngo\nif wantsName, ok := plugin.(WantsName); ok {\n    wantsName.SetName(sectionName)\n}\nThere's one other case of this same code below, other than that r+.\n. Same note here w.r.t. return value order.\n. When a function returns a value and a possible error, the common golang idiom (and what we do throughout the rest of the Heka code) is to return the value first and the error second. So this would be\ngo\nfunc parseMessage(message []byte) (Stat, error) {\n. Again w/ return value order.\n. Missing some punctuation here? Seems like there should be a period after \"stdin\" and a comma after \"stop\" (\"ProcessInput will stop, logging the exit error\" has a pretty different meaning than \"ProcessInput will stop logging the exit error.\")\n. While there are some differences in the setup for each of these tests, it looks like a lot of it is the same across all three of them. It'd be easier to see exactly what was and wasn't shared across the tests if the common config and EXPECT() calls were moved out to the top level ProcessInput spec instead of being repeated in each separate test spec.\n. Seems a bit weird to be printing output here.\n. Probably a good idea to at least check that the error text is what we expect, maybe even that we get the number of errors that we think we should.\n. We use an Init method as part of Heka's Plugin interface to define the contract needed to take part in Heka's config engine. This isn't really a common Go idiom, however. This is underlined by the fact that there's no value to the returned error variable... we return nil every time. Maybe this can be replaced by a NewManagedCmd function that accepts any initial values, sets them, performs the necessary initialization, and then returns a pointer to the created ManagedCmd object?\n. Why not just mc.Cmd = exec.Cmd{...}?\n. Why bother returning an error value if we never actually return an error?\n. This comment seems superfluous.\n. Might be include which command triggered the error in the error that we return?\n. A bit confusing to read since we use no_error (i.e. a negatively formed error flag) alongside Go's idiomatic err value (i.e. a positively formed error flag). Let's use hasError so err != nil means that hasError = true.\n. Again, maybe we should decorate the error message w/ some context information before passing it out?\n. And again here.\n. Typo\n. Might reset() be getting called while the existing done and Stopchan values are being used in a different goroutine? Might not be a bad idea to run the race detector (http://blog.golang.org/race-detector) on this code to see if it gives us any warnings.\n. I haven't complained about the use of underscores instead of most of our code's standard of camel case for the local variable names, but I'll draw the line at using all lower case in this method name. Please rename to writeToPack. :)\n. This is meant for production use, we probably don't want to put it in a folder named 'testsupport'. Maybe use ${HEKA_PATH}/sandbox/lua/decoders/syslog_decoder.lua instead?\n. I think you accidentally a word there.\n. Nice docs! Just needs a full sample config section. :)\n. This configuration seems a bit opaque, plus it's needlessly backwards incompatible. Instead of merging the two values into a single option, you can restore the FlushInterval option and add a separate FlushCount one.\n. These methods aren't being exposed in an interface at all... does it make sense to just make the router, inputRecycleChan, and injectRecycleChan attributes public instead?\n. Do we really want to remove the 'sandbox' identifier here? This would mean that a sandbox filter could generate message types that match heka's internally used message types, like 'heka.all-report' and 'heka.sandbox-termination'.\n. Can we only initialize the timer if flushInterval > 0, instead of initializing it and then stopping it if flushInterval <= 0?\n. What about cases where the flush operator is AND but the flushInterval is 0? Those should be flushed as soon as msgCounter >= o.flushCount too, right?\n. There are LogError and LogMessage methods on the OutputRunner that you should call rather than using the log module directly. These methods will automatically prepend some information about which plugin is generating the log message. Plus, in the future we might provide the ability for Heka to turn its own logging output into Heka messages, if you use the API we'll be able to do this. :)\n. Although, actually, I'm not sure that we even want this much log output coming from this plugin. It's useful for debugging, but in most cases it's probably too much log noise.\n. A bit more right hand-holding here would help orient people more effectively. I'll take a stab:\nThe \"representation\" is where you can say something about what the data in this field is representing. This information might provide cues to assist with processing, labeling, or rendering of the data performed by downstream plugins or UI elements. Examples of common usage follow:\n. Ah, right, the timer variable is a pointer. It would work like I described if you declared it as a time.Timer directly, see http://play.golang.org/p/spCUUzg7au.\n. Heh, oops. Yeah, it'd be nice if github had a way to mark pull requests as \"reviewed\", \"resubmitted\", etc., so we would know when to look at it.\n. I'm not too worried about that. If I'm explicitly disabling the interval, then there's only one flush variable in play. When you only have one variable, AND and OR are equivalent. Or they're equivalently nonsensical, which amounts to the same result. :)\n. This is clear, thanks! I think we might want to tweak the phrasing a bit to feel more consistent w/ the rest of the text. How about this paragraph:\nIf decoding fails for any reason, then Decode should return a nil value for the PipelinePack slice, causing the message to be dropped with no further processing. Returning an appropriate error value will cause Heka to log an error message about the decoding failure.\n. Shouldn't this be \"comment out if using <0.4.0\"?\n. I don't want to put a lot more effort into them, no. Not quite ready to deprecate them yet, though, either. I'll make the config validation changes (slaps forehead).\n. Removing that line is technically an API change, which seemed like overkill for test fixes. But the chance that someone out in the wild is creating a GlobalConfigStruct object without calling DefaultGlobals() is practically nil, so you're right that it's probably safe to axe. \n. This changes the behavior, right? IIUC, we'll no longer track a branch if something like 'dev' or 'master' is used as the git_tag. Is this intended? If so, maybe we should change all of our pegs to tags or SHAs so we don't imply that we can follow a branch.\n. We are starting to ship Lua decoders with Heka; we've nearly finished \nimplementing a syslog decoder in lua that will ship w/ Heka. We're \nstill ironing out all the kinks, but hopefully over time the corpus of \nLua plugins that you get out of box will be considerable.\nOn Fri 22 Nov 2013 10:05:20 AM PST, Jordi Boggiano wrote:\n\nIn docs/source/configuration.rst:\n\n\nrepresentation.\n  +\nIf the field specified a type, but the extracted value cannot be converted\ninto that type, decoding will fail and the message will be dropped. It is\npossible to specify an optional type by appending a question mark after\nthe type name, e.g. Requests|count|int?. In these cases, if the\nconversion fails decoding will be considered to have succeeded and the\nmessage will still be processed, but the specified field will not be\ncreated on the message object.\n  +\nConsider the following sample message_fields section::\n  +\n[my_decoder.message_fields]\nType = \"%Type%Decoded\"\nRequests|count|int = \"%Requests%\"\nResponseCode||int? = \"%RespCode%\"\n\n\nThat's probably true, but indeed the \"lua in toml\" thing crossed my\nmind earlier, because it's a bit of a pain to have many files to deal\nwith, until I move it all to puppet configs anyway. It's a dumb gut\nfeeling thing, but I think for newcomers it's a bit nicer if they\ndon't have to dive into lua sandboxes right away. You're probably\nright though. It'd just be nice to identify common lua scripts at some\npoint and turn them into go decoders (if we can make them as\nperformant as the lua ones, which I'd hope is possible), or at least\nship some lua ones with heka, just to set common patterns and avoid\nforcing everyone to maintain a copy of the same stuff.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/531/files#r7863479.\n. I think it makes more sense to return nil for the stats value whenever we return an error, since returned stats aren't meant to be used. Also, there may be valid stats later in the list than the one that failed, we don't want to imply that the returned set of stats is complete. This applies to this line and several others below.\n. Whoops, yes, I meant to remove it. Good catch.\n. Strange to set this in the test and then not use it. Would be much better to remove this from the 'decodes simple messages' spec and instead add another spec sets this to true and then explicitly tests the new functionality.\n. Wait, is this saying that we're not planning on letting folks use regular expressions at all within Lua code? I'm all for encouraging LPeg, but I never imagined that regexes wouldn't even be an option. Am I misunderstanding?\n. Embarrassed to admit that I forgot that we had covered this already. This will work, we'll just maybe want to keep the PayloadRegexDecoder around a bit longer, until we've built up a pretty good set of reusable grammars for popular log file formats.\n. You can mock the SendMail function by defining a SmtpOutput.sendFn attribute. Init() would set it to smtp.SendMail, in your tests you can override it w/ your own implementation.\n. Can probably remove these if the SendMail stub you write is synchronous.\n. No, running the tests w/ the spec commented out proves nothing. Putting it in there proves that your plugin delegates to the smtp library, like it should.\n. That's what I assumed, which is why I mentioned that replacing it w/ a SendMail stub that isn't actually making a connection means you don't need this.\n. I'm okay w/ leaving in some commented tests for convenience testing w/ a real mail server, but I still want this plugin to have unit tests. Our automated test suite is for testing our code, not the stdlib. As long as we check to make sure that the SendMail function is being called w/ the parameters that we expect it to be called with, I'm willing to assume that the stdlib will actually do what it's supposed to be doing.\n. This comment is a bit confusing, since IIUC \"original\" in the comment doesn't match \"original\" in the code. Maybe change it to \"s.pack.Message's headers\"?\n\nr+ other than that.\n. For backward compatibility (and easier to use in the common case of a single URL) it'd be great to leave the Url config option and add Urls as a separate alternative. Also, and this is nit-picky, Go's idiom (which we've been pretty good at using in Heka, modulo the AMQP plugins) is to camel case var names, where even acronyms only have the first letter capitalized. Would appreciate use of Urls instead of URLs.\n. Users wouldn't be repeating themselves; they'd use either url = \"someurl\" or urls = [\"someurl1\", \"someurl2\"]. Init() function would validate that only one or the other was set. A little more work in the code to make it a little bit nicer for the user makes sense to me here. :)\n. Internally we'd do something like set hi.urls = []string{hi.conf.Url} if the singular config option was chosen, or hi.urls = hi.conf.Urls if not.\n. If you add a SetName(name string) method to *HttpInput, then Heka will pass in the name of the input plugin before config happens. You can store this and then (before the for loop) set a logger value to fmt.Sprintf(\"HttpInput: %s\", hi.Name) and then pack.Message.SetLogger(logger) to distinguish btn multiple HttpInputs running in the same Heka.\n. Worth mentioning that the setting is configurable:\n\"\"\"Data is always fetched using HTTP GET and any non-200 status generate a message with a configurable \"error severity\" (defaults to 1 or \"alert\") and are not fatal for the plugin.\"\"\"\n. \"\"\"Errors returned from HTTP GET such as inability to connect to remote host generate message with the specified error severity and of Type \"heka.httpinput.error\".\"\"\"\n. Go always does assignment, argument passing, and channel puts by copy rather than by reference. Since a MonitorResponse.ResponseData could potentially be pretty large, it'd be better to minimize copies by using chan *MonitorResponse and generally working w/ pointers to MonitorResponse structs rather than the structs themselves.\n. You don't have to change it, but note that you can convert ..., respChan chan MonitorResponse, errChan chan MonitorResponse, ... to ..., respChan, errChan chan MonitorResponse, ... (or, even better ..., respChan, errChan chan *MonitorResponse, ...).\n. It's worth noting that these objects will probably be created on the heap and then garbage collected each time. HttpInput will probably be executing infrequently enough that this isn't a big deal, but if we were doing thousands of these per second we'd want to consider recycling these data structures. No change is needed here, just drawing attention.\n. Seems like wedging this into add_external_plugin is a bit awkward... the SHA is meaningless, the inclusion of https:// is meaningless but (if I'm reading correctly) required. What I'd rather see is a separate dev_external_plugin function that takes a single argument of a filepath. And, if possible, the build should scan the externals folder on each build, automatically calling dev_external_plugin for each sub-folder; then you could just clone a repo into the externals folder and run make, no need to edit the config at all.\n. Rather than removing this altogether, it'd be better to put it in the defer, so defer pprof.StopCPUProfile() becomes:\ngo\n    defer func() {\n        pprof.StopCPUProfile()\n        profFile.Close()\n    }()\n. +1 to changing the executable names, but there's not a lot of value in changing the variable names here. \"FLOOD_EXE\" and friends are unambiguous in the context of a Heka specific build file, and this removes a number of noise-like changed lines below.\n. Similarly, I think it's fine to leave the default TOML file names as is. Not a huge deal, it's just fewer changes that users have to deal with.\n. We should probably use the plugin name here so we can distinguish btn multiple HttpListenInputs.\n. I don't see any reason to have the for or the select here, if we're just blocking on stopChan I think we can just replace the entire above block with <-hli.stopChan.\n. Whoops, SetName is only necessary if you absolutely need to have the plugin name in the Init() method, befrore the plugin has actually started up. After the input has started you can get the name from the InputRunner: hli.ir.Name().\n. Won't we be closing cmdin twice? Once here and once when the function returns due to the defer above?\n. Submitter remains the same for the lifespan of the process, right? Seems like we should be setting the submitter in the Init() method rather than performing this check for every message.\n. Great, thx for the clarification. :)\n. \"machine's\"\n. \"it's\"\n. Include link to Go docs that tell you what the duration parser accepts.\n. journal_directory is listed twice. Also it's not clear to me in either of the descriptions exactly what the default value will be.\n. All of the strings in this list should be match names form the file_match regex right? That should be explicitly stated here. Maybe something like: \"When using sequential logstreams, the priority option specifies which of the regular expression match values should be used to determine the order that the stream's files should be parsed. Terms earlier in the sequence have a higher priority order than terms later in the sequence.\"\n. Of course, if my understanding is wrong then we shouldn't use that text, but we should still clarify a bit where the values come from and how to use them.\n. Other two options end with a \".\", this one doesn't.\n. This paragraph leaves me confused. Can we try a different approach to explaining this? Here's a stab:\nA \"log stream\" is a single, linear data stream that is spread across one or more sequential log files. For instance, an Apache or nginx server typically generates two log streams for each domain: an access log and an error log. Each stream might be written to a single log file that is periodically truncated (ick!) or rotated (better), with some number of historical versions being kept (e.g. access-example.com.log, access-example.com.log.0, access-example.com.log.1, etc.). Or, better yet, the server might periodically create new timestamped files so that the 'tip' of the log stream jumps from file to file (e.g. access-example.com-2014.01.28.log, access-example.com-2014.01.27.log, access-example.com-2014.01.26.log, etc.). The job of Heka's Logstreamer plugin is to understand the file naming and ordering conventions for single type of log stream (e.g. \"all of the nginx server's domain access logs\"), and to use that to watch the specified directories and load the right files in the right order. The plugin will also track its location in the stream so it can resume from where it left off after a restart, even in cases where the file may have rotated during the downtime.\n. Can we lowercase the keys and just specify that they're case insensitive?\n. \"... instead of Monday you can configure it with a custom mapping.\"\n. A bit more detail here orient people more quickly: \"Keys are the name of the match variables, values are the portion of the filename that was matched.\"\n. Also might be good for readability to define StringMatchParts before MatchParts in the struct, since MatchParts makes a bit more sense once you've oriented to where the matches are coming from.\n. typo\n. Seems weird that an error opening the file returns true, same as if the file hash actually succeeds. Won't we risk swallowing errors here? Is this intended behavior?\n. Confused by these comments. First one says \"... check if we have a newer file\". Next one says \"We do have a new file, ...\". But err = nil is the only statement btn the two comments. \n. Hrm. Maybe call it \"FileHashMismatch\", and return true if the hash is a confirmed mismatch, false otherwise?\n. Typo: \"fluctuated\".\n. I'm not sure how much of a risk this really is, but I know that Go's race detector will complain about the use of t.stopping across multiple goroutines like this. We've used an internal-to-the-plugin stop channel in most other places.\n. Accessing a variable from more than one goroutine w/o any synchronization is a race condition, due to memory caching the other goroutines might never be notified that lsi.stopped has been set, see http://golang.org/ref/mem#tmp_9.\n. Shouldn't this be os.Stdout?\n. This offset is in bytes? Looks to be so. Should probably mention that here. Ultimately it'd be nice if we had an offset in records, too, but that is more work since it requires some amount of parsing (splitting) to accomplish, so no need to wait on merging this.\n. Maybe just pass hi.conf in here instead of having such a long argument list?\n. These scripts require python and non-stdlib python modules. I think we should be trying to make these as minimal as possible. I'd rather see us have some trivial shell scripts (i.e. #!/usr/bin/env sh) that emit some data. And maybe we check GOOS and use an alternate test directory containing .bat scripts which emits exactly the same data on windows.\n. We explicitly got a ProcessInput wrapper, so in this case we know that we're supposed to be holding a ProcessInput plugin. If we change to using a ProcessDirectoryInput we can be in the same package as the ProcessInput so we can cheat and use a type assertion to a ProcessInput struct and we won't need to do any of the interface checks that are following this.\n. I don't think this interface is necessary, see below re: ProcessInput type assertion.\n. You're right, we should definitely be including our sandbox decoders and filters here on this page. I've opened issue #678 for that, don't think it needs to hold up this merge.\n. Whoops, I meant to change that in the build, too. But on further thought it's probably not worth it, I'll change these back to dasher.\n. What's wrong with that? There's got to be some default home for the lua code, under the share dir seems to me like the right choice. Open to alternative suggestions.\n. I'll verify, but I'm pretty sure that packaging systems keep track of their resources at the file level and won't delete directories containing files that were not put there by the package. In any case, the behavior won't be any worse than it already was, since the base_dir is usually put in place by the package as well.\n. We should establish a de facto standard here and start using --[[ begin-doc and end doc --]] as the delimiters, so we don't break if someone uses a multiline comment somewhere else in their lua script. \n. \"upstream\" is just one word.\n. We should add a note that input messages must contain the status code in a Fields['status'] int field.\n. Fair enough, thx.\n. typo: \"... in the script or an invaild assumptions about the data ...\"\nAlso, technically you want e.g. (which means \"for example\", effectively) instead of i.e. (which means \"in other words\").\n. No need to use append to dynamically allocate our slice when we know how big it's going to be ahead of time. I think the following untested code is equivalent, but more efficient:\ngo\n        cumulativeValues := make([]float64, count)\n        cumulativeValues[0] = timings[0]\n        for i := 1; i < count; i++ {\n                cumulativeValues[i] = timings[i] + cumulativeValues[i-1]\n        }\n. Why do all of our API calls require a timestamp to be submitted? If, as the docs imply, it's always supposed to be the current time, can't we just grab that ourselves? And use a different mechanism (an optional boolean argument, maybe?) to signal bypassing the throttling?\n. typo: \"and\" -> \"an\"\n. Typo: \"test\" -> \"text\"\n. Typo: \"test\" -> \"text\"\n. Same typos as above.\n. It seems to me that the SetDeadline() calls should be happening immediately before this clientConn.Do() call. The first time through it's not so bad, but for subsequent requests the deadline is set before returning from this function. Theoretically we could end up burning through our timeout interval before the next call to Index() even happens.\n. We don't need this else clause, do we? The deadline is always set immediately before the request now.\n. We've been trying to be consistent w/ the use of underscores as word separators when naming Heka's config options. Can you change this to local_address?\n. Similarly, within Go code we're trying to stick to Go's idiomatic naming conventions, which use camelCase for variable names. Please rename to localAddress. :)\n. You would change the spelling required in the config's TOML file by the use of an attribute tag here, i.e. toml:\"local_address\".\n. Be nice to have a note here that this doesn't yet work w/ TLS connections.\n. With this error and all of the others below I suspect that the default message will not have enough context for the user. It would be better to generate a message that gives the user more information, e.g.::\nlog.Fatalf(\"Error extracting process id from pidfile '%s': %s\", *pidPath, err)\n. We should make it explicit in the docs that if the pidfile option is omitted then a pid file will not be generated.\n. Forgot to add the underscore here.\n. Why []interface{} here and not []string?\n. Heka has two special directories, a base_dir (or working dir) and a share_dir (see the last two items here: http://hekad.readthedocs.org/en/latest/config/index.html#global-configuration-options). Heka should have write access to the base_dir, read only access to the share_dir. Rather than hard-coding the full path, it's a better idea to use one of these as the base default location for your files. In this case share_dir would probably be the right choice, since I don't think we want Heka scribbling on the database file at all.\n. I guess it doesn't matter too much, since the end result of each error showing up one at a time will be the same, but it seems weird to set this error only to have it potentially shadowed by the TargetField check below. Maybe just change this err = to a return?\n. This can just be return ld.gi.GetRecord(ip); no need to declare a var if you're not going to do anything with it.\n. Why swallow panics? That seems like a bad idea...\n. This should just be var buf bytes.Buffer, no need to instantiate one here only to overwrite it with the one you're instantiating in the GeoBuff method.\n. For strings it's more idiomatic to use if conf.DatabaseFile == \"\" rather than checking for length > 0.\n. It makes more sense to open the database file, and fail if we can't, here in the Init() method rather than in Decode.\n. Actually, the use of fmt.Errorf is weird here, b/c you don't even have an original error from which to format... see the note below.\n. I'd redo the last two field validations like so:\ngo\n        if conf.SourceIpField == \"\" {\n                return errors.New(\"`source_ip_field` must be specified\")\n        }\n        if conf.TargetField == \"\" {\n                return errors.New(\"`target_field` must be specified\")\n        }\n        ld.SourceIpField = conf.SourceIpField\n        ld.TargetField = conf.TargetField\n. Existence of SourceIpField was verified in the Init method, you don't need to check for it again here.\n. You can prevent a panic on failed type conversion here by using ip, ok = ipAddr.(string).\n. Again, all of this file opening and validation should happen in Init, by this point we should assume we have the file open and we can use it.\n. This was also verified in the Init method.\n. Not a good idea to swallow the error here.\n. Not the hugest deal, but we're trying to standardize on underscore separators in our config settings, would prefer the toml tag to use \"pid_file\".\n. Clearly we don't want to hard-code this into CMakeLists.txt in the main repo.\n. @trink knows cmake and cpack better than I do, but I think that there's a slight problem w/ this approach. Once you install a file, it's there for all of the builds, not just the one that triggered the install. That means that the debian specific init and config files will end up being added to the tarball, and even to an RPM if rpmbuild happens to be available on the system.\n. It's a small stylistic nit, but I think this code would be more readable if you put a return 0 after items[item] = i + 1. That would let you get rid of the else statement and remove a layer of nesting for the rest of the function.\n. This won't always work, since it's entirely possible someone could be using a different share dir on their machine. Would it be possible to produce an extremely stripped down .dat file, one that just contains the entries that you use in the test? Then we could actually include it in the source code, and instead of overwriting the share_dir you could specify the full path to the included file as the DatabaseFile value on the plugin.\n. None of the other config options on this plugin include a field tag, so for consistency this one shouldn't either. If it were to include a field tag, it should adhere to the standard we use throughout the rest of Heka of all lower-case and underscore separated, i.e. queue_ttl. I don't think we should do this now, though; at some point in the future we should go through and change all of them at once.\n. I see why you did it, but I'm still not sure I like the interface{} use here. Maybe use int32 and say that negative is infinite? Set a default value of -1 to the ConfigStruct method and the default behavior will be what we want.\n. This code will panic in the unlikely event that path is less than 3 characters long. It'd be safer to use strings.HasSuffix(). You could use path/filepath.Ext(), but that'd be slightly less efficient (probably not a big deal here) and more code (less code is nearly always better).\n. We've already set reader = fd if !gzipped above, so we can get rid of this if and just use n, err = reader.Read(buf) every time here.\n. Ah, okay. Maybe then you should specify the Globals().ShareDir as /foo/bar/baz to reinforce that it's a bogus path that we're not even using, and then add a check after you call ConfigStruct() to make sure that conf.DatabaseFile points where you think it should.\n. Hate to be so nitpicky, but this can just be return strings.HasSuffix(path, \".gz\").\n. Putting the lock here protects against race conditions if the ReportMsg method is called twice concurrently (which could happen if SIGUSR1 is sent at the same time Heka's internal report is being generated for the DashboardOutput). The more likely race condition, though, is that ReportMsg is executing concurrently with the rescan case in the Run method, w/ both pieces of code trying to access li.logstreamSet at the same time. To protect against that you'll need to use the lock in the rescan case as well. Really, though, it should probably be a RWMutex, w/ rescan using a write lock and ReportMsg using a read lock.\n. Saying it \"is not really a valid argument\" isn't actually a valid argument. It might provide some wins, it might not, it might provide wins in some circumstances and actually slow things down in others. Without data and benchmarking we don't know.\nRegardless, decoders are implemented the way they're implemented, currently, and it would be a significant breaking change to undo that. Not the biggest fish we have to fry, IMO. And certainly not one that should happen as a part of this already very large PR.\n. Good point, I'll dig into this.\n. On further digging, in order to copy the message headers from the original message it'd be necessary for the Go implementation of InjectMessage to deserialize from protobuf, copy the message headers, and then reserialize to protobuf. This isn't worth the performance hit, so we're going to leave the behavior as is, if users want to populate the message headers they can do so by hand in the Lua code.\n. This isn't deprecated. Deprecated means \"this still exists but it won't for much longer, so you should change\". We didn't deprecate, we just broke shit, so we should remove 'format' from the docs altogether.\n. We don't rigidly enforce an 80 char limit, no, but I think things become considerably less readable when lines get over about 100 chars. I'll usually line wrap large comment blocks and documentation prose at 80 chars, but that's an artifact wrapping policy.\n. sigh\nokay.\n;)\n. Since the HLL gives us a count and we don't have to keep our own tally does it makes sense to move this to the timer_event?\n. I don't think so. First, as of the last update they're not identical; I realized that the timestamp format wasn't being used by the logstash v0 version at all, so I removed it from the config. Second, Go's serialization stuff doesn't play well w/ embedded anonymous structs, so AFAICT there's no easy way to have plugins use a shared base but add a custom extra setting or two. We might be able to hack up the TOML code to start supporting embedded structs, but unless we do that I think a bit of duplication is the best choice.\n. There are two issues with the use of stopChan here:\n- Channels are typically used to communicate between separate goroutines. It's a code smell to be putting and pulling onto the same channel in a single function.\n- In the <-stopChan case below, all you're doing is setting ok to false. But this if condition only fires if ok is already false.\n  In other words, you can get rid of stopChan altogether and just change the if clause to:\ngo\n    if !ok {\n        break\n    }\n. This is a goroutine leak, every time a new connection is established there's a new goroutine that runs forever.\n. If you only have a single case there's no need for a select statement.\n. I'm guessing this is here to give the IRC server a chance to handle the requests we've sent. This is pretty imprecise and brittle, though, it'd be much better to actually parse the server responses to confirm things have worked as we expect.\n. This is fine, but it's a bit more idiomatic to use the := spelling when you're doing allocation and initialization all in one go. Usually you'd see this written like so:\ngo\n    var pack *PipelinePack\n    ok := true\n    inChan := or.InChan()\n    stopChan := make(chan bool, 1)\n(Although the stopChan isn't needed at all, as mentioned below).\n. If the encoder.Encode call returns an error we're just silently dropping the message.\n. Returning from the Run method means shutting down the output altogether. And unless your output is set up as a restarting plugin (see http://hekad.readthedocs.org/en/latest/developing/plugin.html#restarting-plugins), this means shutting down Heka altogether. Probably not what we want from a simple network hiccup.\n. I thought about that, but ts_from_message will never change btn calls, so the condition will ensure that ns will either always be set or never be set.\n. I realize it's unlikely (and pathological) for this to not be a TCPConn, but it's still maybe a good idea to use the alternate type coercion spelling:\ngo\n    if t.config.KeepAlive {\n        tcpConn, ok := conn.(*net.TCPConn)\n        if !ok {\n            return errors.New(\"KeepAlive only supported for TCP connections.\")\n        }\n        tcpConn.SetKeepAlive(t.config.KeepAlive)\n        tcpConn.SetKeepAlivePeriod(...etc...)\n    }\nThis will cause Heka to shut down cleanly w/ a useful error message instead of panic if a non-TCP connection slips in there somehow.\n. I'd like to say we should change this spelling too, but this one is trickier b/c returning an error here will just cause the BufferedOutput to retry the connection, which is not what we want.\n. +1.\n. I'd prefer an even more precise message: \"Decoder sandbox has terminated\".\n. We're actually using it in SetDecoderRunner. I guess I could grab the names from the md.Config.Subs value, since they match the order of the decoders in md.Ordered, but it didn't seem worth it to me to remove this. I'll make that change if you feel it's worth it.\n. Need to remove the last sentence in this paragraph, not only because it has a copy/paste typo but also because it's no longer accurate.\n. Spelling typo: beginning\n. I appreciate your clever trick here which allows you to have a single array processing function for all of the types. I'm not to keen on the use of reflect, though. Go's reflection is notoriously slow, and this manual string JSON generation is already slow enough.\nPlus, you already know the type of field you're dealing with, b/c the only place this function is called is from inside the code below where we're switching on message type. Why don't you just pass the type in to the array function so you don't have to duplicate your efforts using reflection?\n. Some of your functions have been defined as methods on an IrcOutput type, while others are just functions that take a IrcOutput argument, and there doesn't seem to be any consistency in which are which. Either is fine, of course, but generally we go with methods unless it really feels like a top-level API function that isn't specifically related to the type in question.\n. Emitting which channel is causing the problem will make this much easier to debug.\n. Also notice that you're passing the runner around a bunch. When I need access to the runner across many different plugin methods I usually just add it as an attribute on the plugin struct at the beginning of the Run method so I don't have to pass it around.\n. This always means we should exit the loop, right? Maybe add a comment here to indicate that the killProcessing channel should only ever be closed, and that if someone actually sends a value over the channel it's a bug?\n. I know we were talking about how this would likely cause a flood if the queues were backed up at all. Not sure if we should do something about it. We could maybe check the length of the backlog queues and do something else (send a \"N IRC messages dropped\" message or something?) if they're over a certain size. Open to other ideas...\n. JoinAndPart is global to the output, you can put this check outside of the loop instead of checking it multiple times. In fact, this makes this callback a no-op, maybe we just don't register the callback at all if JoinAndPart is true? Or maybe we need a flag to confirm that the connection has happened for the JoinAndPart case?\n. I like how you've defined an error struct here for embedding error information. Lots of runner.LogError(fmt.Errorf(...)) calls littered throughout our code base that should be changed to use this idiom.\n. All four of those callbacks are identical. You should just have one implementation, give it a name, and reuse it.\n. This function always uses the same timer instance, but it's called from two different callbacks. Isn't it possible that multiple callback invocations could happen at the same time, causing timer calls to interfere with each other?\n. This is a bit of an arbitrary stylistic nit, but generally I think it's nice to have the Init() method up at the top of the plugin implementation, and the Run() method just under it, so someone reading the source can quickly find Heka's entry points into the code w/o having to pop around the file so much.\n. Neither the conf nor the conn temporary vars are being used at all, you can assign directly to the struct attributes.\n. If we can't encode the message we've got nothing to send to IRC, we need to recycle the pack and continue to the next loop iteration.\n. Hrm, I think the timing is wrong on the JoinAndPart stuff here. We're joining right when we put the message on the outqueue, and then we're parting after the message is sent. There's latency before the message actually goes out, what if we get two messages in before the first goes out? When the first message is delivered we'll part the IRC channel, and then we won't be in it when the second message comes up for delivery.\nFor JoinAndPart to work correctly I think we'll need to join immediately before sending the message, send it, and then immediately part.\n. s/2012/2014/\n. s/2012/2014/\n. I know that it may be considered good form, but we're not splitting up local imports vs. stdlib imports anywhere else in our code base, and there's not much value to it, so let's not start.\n. s/2012/2014/\n. +1 to not putting everything in one giant file, but all of the other plugins have the plugin struct, the config struct, and the ConfigStruct() method in the same file as the rest of the plugin implementation, I'd like to maintain that pattern. Also, it's nice to have constructors (like your NewIrcConn function, below) immediately following the struct or interface definition of what they return (your IrcConnection interface, above).\n. s/2012/2014/\n. It's not possible to concurrently fail joining two different channels? Not being rhetorical, honest question.\n. I actually like having the entire plugin implementation in a single file.\n. Then maybe we should drop JoinAndPart, b/c right now AFAICT you might part from a channel when there's still a message intended for that channel waiting in the outqueue. Am I wrong about that?\n. Doesn't this break sandbox persistence for this filter entirely? Seems like we should maybe persist some feature flag and only bump the preservation version if the flag hasn't been preserved.\n. Duh. Sorry, I was getting my wires crossed btn what was coming from config and what was coming from persistence, and I was thinking we were going to be incrementing the persistent value every time, which would clearly not work. Thanks for clarifying.\n. This comment is no longer valid.\n. This is a bug b/c there's no way to distinguish btn a false that was explicitly specified in the config and a false b/c the user didn't specify a value. You need to declare PluginGlobals.CanExit as a *bool instead of a bool so you can tell the difference btn a nil and an explicit false (see PluginGlobals.UseFraming).\n. \"All PluginRunners either can or cannot stop without causing Heka to shut down.\"\n. Not sure it makes sense to remove this. An error in NewRetryHelper means that the one of the specified time durations couldn't be parsed, which means that there's a config error, and for every other config error we bail out and shut down.\n. Did you mean to overwrite the err variable declared in the function's return values? And did you mean to comment out the LogError call?\n. Same as above re: err value from outside the closure and commented LogError call.\n. A bit hard to tell, but it doesn't look like you're actually using the pConfig struct attribute at all, but are instead passing it around as a function argument when needed. Can we remove it?\n. We've already set foRunner.canExit to true if pw == nil in getWrapperAndRun, and SandboxFilters don't support the Restarting interface, so I think we can get rid of this special case check here.\n. I really don't like this pattern of overwriting a package global to inject behavior. I'd rather see it as an attribute on the DashboardOutput struct.\n. Again here.\n. Slightly weird to refer to the same value using two different spellings in three lines (conf.Channels vs. output.IrcOutputConfig.Channels).\n. There are two other places where output.Join() is called, do we know if those are using the right spelling of the channel names for protected channels?\n. Influx is fine w/ non-numeric values, but I'm not entirely sure if it will correctly handle bytes, so I'll put in a check to exclude them.\n. Yup, I started w/ dashes, changed them to underscores, but missed a couple. Will fix.\n. Yeah, this plugin (and its docs) were merged to dev, but not to the branch that these docs are on. It should start working once this branch is merged to dev, but I'll make sure to test ahead of time.\n. You don't have to go through the entire plugin creation process to get the subdecoders, you can get them from the tomlSection using a couple of type assertions. The following should work:\ngo\nfunc subsFromSection(section toml.Primitive) []string {\n        secMap := section.(map[string]interface{})\n        var subs []string\n        if _, ok := secMap[\"subs\"]; ok {\n                subsUntyped, _ := secMap[\"subs\"].([]interface{})\n                subs = make([]string, len(subsUntyped))\n                for i, subUntyped := range(subsUntyped) {\n                        subs[i], _ = subUntyped.(string)\n                }\n        }\n        return subs\n}\nWith that you should be able to call subs := subsFromSection(section.tomlSection) inside your loop to accomplish the same thing w/ less work.\n. Typo (extra \".\")\n. rankFunc := func(...) is fine here.\n. Ah, right. Don't mind me, then.\n. I'd probably put it in config.go, myself, but I don't feel strongly.\n. Remove debug output.\n. More debug output.\n. Best to do the read_config stuff outside of process_message, so it only happens once at startup time instead of once for every message.\n. The length operators (i.e. #columns and #values) are calculated each time they're used, so it's probably better here to use absolute values for the first several, and then increment an index counter by hand when adding the dynamic fields at the end.\n. I think this should have a more specific name, e.g. stoppingMutex.\n. I think this name should be different, too. We already have a Shutdown method, so it's weird to have a shutdown method that does something different. setStopping, maybe?\n. Remove commented code.\n. Wrong comment.\n. I'd really like to see a SHA1 checksum check here to make sure the binary is right before we try to use it.\n. I have at times gotten intermittent failures building w/ make -j8 on my linux box, I think b/c some steps start before a dependent step has finished. I'm wary of turning this on here.\n. Again, it would be lovely to do some checksum verification before trying to run the binary here. :)\n. Is there a way to have this support multi-file configs?\n. Typo: missing space.\n. your -> you're. Twice.\n. your -> you're.\n. And again.\n. Does http://localhost/kibana not work?\n. Dockers -> Docker's\n. The phrasing here is a bit awkward. Maybe the following::\nAnd the ticker_interval option is specifying that our filter will be emitting an output message containing the cbuf data back into the router once every second. This message can then be consumed by other filters and/or outputs, such as our DashboardOutput which will use it to generate graphs.\n. great.\n. Yes, good change, but maybe also strengthen the other assertion:\nThe user owning the hekad process requires read access to this folder, but should not have write access.\n. Yeah, I think we can live with ignoring backfilled data for now. And I agree that cbufd data would be more robust, but unfortunately that's a less valuable use case. Most people starting with this will be using filters that emit cbuf data, not cbufd, and that's the data they're going to want to be sending.\n. Add newline. :)\n. Need to add a note here about permissions, suggesting the use of sudo and/or linking to instrux for giving non-root access.\n. The only reason I went with it is to match the lenience of what is accepted in the Pid field, which also seems weirdly fishy to me. I'll remove it.\n. I left it in b/c I have a vague memory that there was a reason it was put there in the first place. Unfortunately that was well over a year ago and I don't remember the specifics. I'm willing to remove it, but not in this PR which was opened against versions/0.7, since (small as it is) it's technically a breaking change.\n. Hrm. I appreciate that you went to pains to not break backwards compatibility, but I actually think it probably makes more sense to default to match the Logstash V0 behavior. I'd say we should call  this use_message_type, like so:\n- use_message_type (bool):\nIf false, the generated JSON's @type value will match the ES record type specified in the type_name\n  setting. If true, the message's Type value will be used as the @type value instead. Defaults to false.\nThen we'll also want to add an entry to the CHANGES.txt changelog file recording this as a breaking change for the 0.8 release.\n. You're ignoring the returned error value here. If an error is returned, interpType will be the empty string, we maybe want to fail over to the uninterpolated value?\n. go-dockerclient is a transitive dependency, not a Heka plugin, so you can use the git_clone command here instead of add_external_plugin.\n. Add .. versionadded:: 0.8 here.\n. It's super clunky, I know, but we also need to add the same include (minus the _config_docker_log_input reference label) to the index_noref.rst file which will be included in the man pages.\n. Hrm... not too stoked on this library's use of its own assert function, which delegates directly to log.Fatal. A dependency library ideally wouldn't even be directly writing to stderr or stdout (since the app may have its own way to managing log output), much less killing the entire app. Ideally this function would instead return an error, which allows Heka itself to decide what to do about the failure. \n. Here's another direct use of log.Fatal, no bueno. I haven't dug in far enough to know what it means if the events channel is closed, but, again, we should be delegating back to Heka when we have failure cases, not unilaterally shutting everything down. \n. Looks like this could be changed to a sync.RWMutex, and we can use RLock and RUnlock in the places where we're just reading from the data structure rather than mutating it. This will reduce some unnecessary blocking.\n. Same here, we'll get less blocking using sync.RWMutex.\n. Most of the other packages that use this dot import idiom used to actually be a part of the pipeline package, it's better to remove the dot and explicitly use pipeline in the source. :)\n. It's generally considered better practice to use the attribute name spelling:\ngo\n    return &DockerLogInputConfig{\n        Endpoint: \"unix:///var/run/docker.sock\",\n    }\n. Might be better to create the stopChan in Init(), just in case Stop() gets called during initialization, before we get here.\n. This is where you'd check for NewAttachManager returning an error, which you'd then return from the Run method.\n. If err is not nil, then hostname is already the empty string. You can either use hostname, _ := os.Hostname() or (even better) explicitly set the hostname to \"unknown\" (or some other sentinel value) when there's an error.\n. I don't think you need a direct handle to the decoder at all, more on this below.\n. Rather than calling decoder.Decoder() directly, you can just drop the pack on the DecoderRunner's InChan. You'll still need to check to make sure you have a decoder, of course, and if you don't you'll still call ir.Inject(), but you don't need to manage your own set of packs, and you don't need to worry about recycling or logging the error message, the DecoderRunner will handle all of that if necessary.\n. Need to add the License block here, you can copy / paste the standard MPL header from any other source file.\n. I usually indent subsections as a visual indication that it's a subsection and doesn't represent a separate plugin.\n. Oof. With the original path, I get the error if I cd docs and then make html. But with the second one, I get the error if I run make package or make dev. :( I'll see if I can resolve.\n. We should rename this to max_msg_size so it's more clear we're talking about an upper bound.\n. This should mention that we're setting the size of the message buffer, and that anything over the specified max size will be truncated.\n. This is slightly unclear since it seems to contradict itself. I'd reword slightly:\nThe variables are restricted to Type, Logger, Hostname, Payload, or any of the message's dynamic field values. All dynamic field values will be converted to a string representation. Field specifications are the same as with the :ref:message_matcher e.g. Fields[foo][0][0].\n. Missing a \"for\".\n. Typo.\n. Might not make enough difference to be worth it, benchmarking is in order, but since all of these values are already typed we could create the fields directly w/o having to resort to NewField's use of reflection.\n. SimpleT is just a hack to work around the fact that gospec doesn't pass Go's *testing.T into the spec code, but gomock needs it. You can get rid of this and just use the t variable that your test is handed.\n. Same note re: no need for SimpleT.\n. This is unsafe. If for any reason the returned JSON doesn't contain an \"errors\" value, then response_body_json[\"errors\"] will be nil and the type assertion will panic. We'll want to use the errors, ok := response_body_json[\"errors\"].(bool) spelling.\n. Most everything in this PR looks good to me, except this loop here. There are a few issues:\n- In normal operation, the non-error case is vastly more common than QueueRecord returning an error. It seems weird then to set up and break from a for loop for every message. I'd rather see the first attempt to queue the message happen outside of the loop, even if it means a little bit of code duplication.\n- In the blocking case, this will be a very tight loop that will spin away and chew up CPU time. At the very least there should be some sleeps in there, but it might even make sense to use a RetryHelper.\n- In the exit case, exiting the plugin is not the same as exiting Heka. Heka might shut down, but an output can be configured to exit safely (which actually would probably be okay in this case) or to continually restart itself (which would not be the desired behavior). If we really want to shut down Heka, we need to call Globals.Shutdown().\n- In the drop case, packs have to be recycled or they will never be released back to Heka for reuse.\n. Whoops, sorry, I missed that the pack would be recycled when the loop exits, so you can ignore the last bullet point. :)\n. I realize you didn't introduce this bug, but we should be explicitly returning nil here, since err might be containing a transient error from a message encoding failure.\n. This can be an else attached to the first if clause, don't need a separate if.\n. Heka supports Linux, OSX, and Windows, separator should vary across platforms. Also, generally using '+' for string concatenation is inefficient. Probably makes more sense to store the messages in a []string and use strings.Join to combine them before sending.\n. Needs slight tweaks, and should include the units, the default, and explain what 0 means:\n\"Defines maximum queue buffer size, in bytes. Defaults to 0, which means no max.\"\n. This is good, just slightly awkward wording. I'd rename the option to \"queue_full_action\", and reword the following text to:\n\"Specifies how Heka should behave when the queue reaches the specified maximum capacity. There are currently three possible actions:\"\n. No need to call LogError, returning the error from Run will already cause the error to be logged.\n. SetPipelineConfig only needs to be implemented if you need access to the pConfig before the Run method is called. Once the Run method is called you have the PluginHelper, and you can just call PluginHelper.PipelineConfig() to get what you need.\n. Go's idiom is camel case, so we'll want outputBlock.\n. Hrm. There's some ambiguity here. If the user sets a max_retries value, then we'll need to decide what to do if the retries are exhausted, btn either shutting down or dropping messages. I can see two possible solutions:\n1. Leave the queue_capacity_reached (or whatever we call it) options as they are and don't let the user set the retry options, so we can force it to always keep retrying forever.\n2. Remove the \"block\" option, and instead use the presence of a retries section to specify whether or not any blocking will happen. If there's no max_retries, it will block forever. If max_retries is set, then if it gets hit we either drop or exit depending on what was specified for queue_capacity_reached.\nI'm leaning towards #1 since it's a simpler solution, and easier to explain to the user.\n. If Wait() returns an error it means that we've exhausted the specified number of retries, as mentioned above. Currently if that happens we'll drop messages on the floor.\n. I don't think we need to log every time the queue fails, seems like it will be too noisy for not much gain.\n. Let's not log every dropped message. We're already logging that we've hit the queue capacity, this will just mean 2 log entries for every message.\n. I'm loathe to overwrite the Type header since it may already contain useful information before the decode failure. Including the error message as a separate field is a great idea, though, I'll add that.\n. I assume you're talking about the ProtobufDecoder case here, in which case you're spot on. Including the failure message will make it a bit more useful. Even if it's not terribly useful, though, I think we still want to support it for consistency's sake. Also, it's possible to use TcpInput with other decoders, in which case the failed contents are in the message.\n. I'm open to revisiting this if we do decide it's worth it, but I don't think it should be a part of this PR.\n. Everywhere else in the docs we refer to \"logstream\" as a single (invented, admittedly) word, I think we should leave this as \"Logstreamer Input\".\n. %v will output the stat values, converting to string where necessary, whereas %s will generate an error for non-string attributes of the struct. If we're going to change it at all, using %+v to include the field names is a better choice.\n. We don't really need to keep a separate count of the bad lines, do we? We can just check len(bad_lines) instead.\n. Probably a good idea to capture this value here (and below) and check that it's what we expect.\n. Done.\n. Since count is now being manipulated from multiple goroutines, we have to start using the synchronized atomic API for all access and mutation.\n. Which means we'll probably want to define is as int64 instead of just int.\n. Same re: count.\n. Yup, as you discovered, the other goroutines might cache the value they have and not get updates when the mutation happens elsewhere. For higher level data structures there's RWMutex to distinguish btn read locks and write locks, but in this case the atomic API should suffice. \n. I know you copied this from the TcpOutput, but I'm realizing that it's a bit off. First, an error returned from Run will automatically get logged, so there's no need to log it inside the Run method. Second, that error will be a low level one returned from an os.OpenFile call, so we should probably decorate it a bit to add some context for the poor folks who have to figure out why Heka just shut down on them. I recommend replacing both of these lines with:\nreturn fmt.Errorf(\"can't create new buffer queue file: %s\", err.Error())\n. Would appreciate a doc comment here to explain this function's contract, esp what the returned bool means.\n. For performance reasons, this string.gsub action should happen only once, before the fields loop, since it will give the same result every time. Also, it should only happen if the user specifies a prepend_fields value, we shouldn't waste cycles on a gsub call if nothing was specified.\n. Actually, neither panic nor LogError is ideal here, what should happen is that the plugin should exit, returning an error from the Run method. Same is true for the panic that's already in there. Doing this will require restructuring the plugin a bit.\n. It would definitely be better to have the file rotation triggered by a timer rather than performing  a calculation on every write, yes.\n. Why? It's the first word in the sentence.\n. Fair enough. I can reword it to be a bit more clear.\n. Yeah, we haven't been writing signed messages to disk, but in the previous implementation the LogstreamerInput was explicitly ignoring the authentication. I put this in so it'd be possible to replicate the prior behaviour.\nYou're right re: the duplicate header decoding. It's technically possible to cache the header from a successful FindRecord call and reuse it in UnframeRecord, since those calls are made serially from within the same goroutine. We could make that an explicit part of the contract, so folks know it's safe to store struct attributes on their splitter btn the two calls. But signed messages are used so infrequently it didn't seem worth it. I'll happily update if you feel differently, though. :)\n. Oh, sorry, that was supposed to be \"again call SplitStream again again\". ;)\nFixing, thanks.\n. The string.gsub call will return the same result each time, in a single process_message call. It should happen outside the fields loop.\n. If use_subs is false then field_name is going to be the same every time. Is that what we want?\n. This should be called outside of process_message with the other read_config call so it doesn't have to be done for every message.\n. So it would be consistent w/ the \"nan\" strings that are already in the data structure. Moot if we replace it w/ the grammar, though.\n. valuesTmp isn't needed, you can mutate the values map directly in the loop. \n. This worries me a bit. pack.Zero() sets MsgBytes length to the full capacity, and our code generally copies into MsgBytes and then resizes down, rather than resizing up and then copying. I'm okay w/ changing this, but I'd rather do it in a separate PR, since it requires digging into all of the places where we set pack.MsgBytes to make sure we don't silently truncate b/c the length is too short.\n. One place where it happens is my fault, it just landed. I'm pretty sure I tried to make this change earlier and hit a side-effect somewhere else, too, but I just searched through the code and couldn't find anything.\nI'm fine w/ trying this out, but the code I linked to above should be changed so the resize happens before the copy, and the Zero method should be changed to match the new pack behavior.\n. A config setting being required should mean that no default is provided, but that the plugin will error out at startup time if the value isn't specified.\n. Dangling reference to post_v09_api which is no longer relevant.\n. Also, is it worth specifying what an empty value actually means, with regard to InfluxDB retention policy?\n. typo: retentionPoilcy -> retentionPolicy\n. Is this actually \"all fields\" or should it be \"all non-numeric fields\"?\n. It's not quite clear from the docs so far whether skip_fields and tag_fields impact each other. If a field is added to skip_fields and tag_fields, will it show up in the tags?\n. This says the default is nanoseconds, but the code below seems to be using a default of milliseconds.\n. This would be more readable with separators (i.e. \"1,000,000\") or, even better, as 1e6.\n. Here's where you enforce the database value:\nlua\nlocal influxdb_db = read_config(\"database\") or error(\"`database` setting required\")\n. timestamp_divisor value doesn't change for the lifetime of the plugin, should be calculated outside of process_message rather than repeated for each message.\n. You don't have to track place manually, you can just use for place, field in ipairs(used_base_fields) do.\n. Oof. I thought I had extracted the message_interpolation stuff out into its own separate module so the various sandbox plugins wouldn't have to reinvent this wheel again and again. Turns out I did do this work, but I did it on a branch that hasn't landed, for reasons completely unrelated to the message interpolation stuff.\nThe commit in question, which includes changes to existing plugins and documentation, can be found at 22501291d264572ba8348f0c75f881787b5b755d. It's a big ask, but might you be willing/able to extract the bulk of that commit and add it as a commit to your branch, and then change your code to use the shared implementation?\nIf that's too much for you (and I understand if it is), then I can pull that commit out and get it merged back to dev, and once that's landed you can rebase against dev and then update your code to use the shared implementation.\n. You're or'ing the same value against itself here.\n. Looking at this further, it occurs to me that you know whether the **all_base** and **all** keys exist at config loading time. We can convert these key lookups to boolean values and save a few cycles, rather than having to perform the key lookup multiple times inside this loop for every message.\n. Here's another place where a boolean could be set at config loading time, and checked before the individual field lookup to avoid unnecessary hash checks.\n. Typo &{Hostname} -> %{Hostname}\n. Ignore, I'll fix this and merge...\n. It was actually @RobCherry who did the work of teasing that stuff out... I assumed he was working with you, but I guess not? Anyway, thanks to both of you for helping out! :) \n. You can just refer to the message field interpolation docs here, like we do in the cbuf_librato docs.\n. It's a small win, but it'd be better to put the tag_fields_all check first to skip the hash key lookup when we know we're using all fields.\n. Typo (Jeka -> Heka)\n. PipelineConfig.makers is protected by a RWLock, you should RLock and RUnlock around your usage. (Bootstrap code doesn't do so b/c there's only one running goroutine at that point.)\n. And I realize you've copied this code from the foRunner loop... it has the same error. :P\n. And another bug that you've brought in from the foRunner code... we should be checking for globals.IsShuttingDown() in the initLoop, otherwise we might end up stuck here forever.\n. Since Lua strings are immutable, this does some unnecessary allocation and will likely cause unnecessary GC (see http://stackoverflow.com/questions/1405583/concatenation-of-strings-in-lua). You can avoid this by using add_to_payload:\nlua\nlocal payload = read_message(\"Payload\")\nadd_to_payload(idx_json, \"\\n\", payload)\nif not string.match(payload, \"\\n$\") then\n    add_to_payload(\"\\n\")\nend\ninject_payload()\n. You actually put this in the wrong order, we want the final \"\\n\" to be added to the payload after the first batch of data. I've fixed it up and will merge, however.\n. Why add the empty line?\n. Yes, you'd want to check for empty Filename and Hash attributes as well. I'd add an IsZero() method to the LogstreamLocation struct and that checks all three values and use else if position.IsZero() instead.\n. Actually, scratch that. I think it'd make more sense for the LogstreamLocationFromFile to accept the initialTail value as an argument. If initialTail == true and there's no journal file it should return the LogstreamLocation value corresponding to the end of the stream.\n. In here you want to check to see if the splitter supports the ReportingPlugin interface, calling splitter.ReportMsg(pack.Message) if so.\n. s/Log/Event/\n. I think it makes more sense to change the default from \"name\" to \"\" and disable the name prefix if they leave it blank. Note that this would mean you should make a note in the \"Breaking Changes\" section of the CHANGES.txt changelog for the 0.10.0 release.\n. By getting rid of this variable definition you've made the scope of the name_prefix defined below to be global. You'll want to at least keep a local name_prefix declaration here.\n. The two changes above can be replaced by:\nlua\nlocal use_subs\nif name_prefix_orig != \"\" then\n    if string.find(name_prefix_orig, \"%%{[%w%p]-}\") then\n        use_subs = true\n    end\n    name_prefix_orig = name_prefix_orig..\".\"\nend\nThen the rest of the code below should work as desired.\n. parser_type is no longer a supported config option, it's been replaced by splitter plugins. If you just remove the parser_type and decoder settings entirely, Heka will choose the right defaults.\n. Add mention of ESLogstashV0Encoder.\n. Typo, missing a space after the \"to\".\n. Typo, missing a space after the \"to\".\n. Not sure I understand. Can't we use a different character for escaping? Back-slashes aren't allowed in a topic name, but they could still be used as an escape character, right? I might be missing something, though...\n. This function doesn't seem to add much to the interpolate_from_msg function to which it delegates. The string.find check might save a few cycles in cases where there's nothing to interpolate, but even if that's worthwhile it might be better off just being added to the original function.\nAm I missing something re: why it makes sense to have a wrapper function?\n. Much more efficient to check the if  outside the loop rather than inside.\n. Seems like this should probably document the attribute portion of the API, since AFAICT this module won't really work correctly unless you set the attribute values that it expects. Also it would be nice to explicitly define the settable variables in the code below, with a comment explaining that the values are expected to be mutated by calling code prior to any use of the API functions.\n. Feels a bit strange to me that most of the config values are pulled out in the encoder code, but there are a few oddballs that are extracted by the module code. Maybe these should be turned into a part of the attribute API for consistency?\n. Same as above re: read_config.\n. Okay, I think I see why you did it this way, since you're using these values in the used_base_fields and field_map calls happening at module scope, and you need to make sure you have the config values even before they might be set by the client code. I'm still a bit wary of it, though.\nMaybe instead of having the client code set attributes on the line_protocol module, we can have a line_protocol_config function that the client code must call before calling any other functions in the module, passing the needed values in as separate arguments, or maybe embedded in a Lua table. That smells a bit better than setting module-level attributes from outside, even if internally that's still what we're doing. It would also mean that all of the read_config calls can be moved to the encoders.\n. Shouldn't this include Logger?\n. It might involve a small amount of code duplication, but I think this could be made more efficient by doing the tag_fields_all or tags_fields_all_base check outside of the loop. If the check is true, then you iterate through the entire base_fields_list like you're already doing. If false, then you only need to iterate through the base_fields_tag_list. Does that seem right to you?\n. That last sentence is no longer true. :)\n. Same re: the last sentence.\n. Need to add documentation for the set_config function.\n. The module supports Graphite stat format in addition to InfluxDB... maybe 'stat_formatting' or something similar? \n. Sounds great. And thanks so much for staying with us through all of the feedback. :)\n. Yes, dev branch will help you here. The read_message(\"raw\") call technically works on 0.9.2, but it doesn't give you the desired result b/c the pack.MsgBytes value (which is what read_message(\"raw\") is accessing) isn't guaranteed to have the protobuf encoding for the message. This changed when #1406 was merged to dev, which ensures that once a pack hits the router, the protobuf encoding will always be available.\n. I think \"Fields\" should be changed to \"DynamicFields\", so there's a more clear link btn the value that is here and the dynamic_fields setting that it's connected to. I was originally thinking we could leave \"Fields\" in as a deprecated option for one release cycle, for b/w compatibility, but I'm not sure it's worth it...\n. Forgot some debug output.\n. It seems like it might not be uncommon for there to be a large number of fields, with only a few of them being sent to ES. I think it might make more sense, then, to iterate through e.dynamicFields and grab all of the field values by name, rather than iterating through all of the fields and then checking to see if each one is in the list.\n. Ignore me. You fetch fields by name in go code by using message.FindAllFields, but I looked at the implementation of that method and realized that it's just iterating through the fields every time you call it, so it doesn't help after all. :P\nGiven that, though, I wonder if it doesn't make sense to merge both of the loops (i.e. len(e.dynamicFields > 0 and not) into one. It means an extra 'if' check on each iteration of the loop when there are no dynamic fields, but that's not a lot of cycles, and you can pre-compute e.dynamicFields > 0 before the loop so you're just checking a boolean value.\n. Other than changing \"Fields\" to \"DynamicFields\", the behaviour would be identical. So including \"DynamicFields\" in the list without setting a separate dynamic_fields setting would include all of them, just like it does now.\nI'm okay w/ breaking the config, as long as we explicitly fail... i.e. Heka should fail to start if \"Fields\" (or anything other than the set of supported values, actually) is in the list. What I don't want to do is have Heka start up and silently do the wrong thing.\n. yup, good catch. i'll push my commit to heka-mozsvc-plugins repo and will get this fixed.\n. fixed.\n. Hrm, I'm a bit confused about this. decode_message(read_message('raw')) is for cases when we want to deserialize the protobuf encoded message bytes that are stored on the pack. Not sure why we'd want that to happen if no rsyslog template is provided... wouldn't we still be expecting to use the text data from the message payload?\n. We'll get less GC churn if we define msg outside the process_message function, like so:\nlua\nlocal msg = {\n    Timestamp = nil,\n    Payload = nil,\n    Hostname = nil,\n    Type = \"iis\",\n    Fields = nil,\n}\n. Initializing as above let's us remove this line.\n. Looks like a [hekad] accidentally got pasted in here.\n. Need to document the payload_keep setting.\n. These work, but for Windows paths you might prefer using single quotes, which indicate literal strings that don't require escaping, as described in the TOML spec.\n. Any reason for num to not be local?\n. Same here, I don't see any reason for grammar to be global, and variable references will be resolved more quickly if it's local.\n. That [hekad] from above belongs here. :)\n. Same here re: local num.\n. Should add a note about the payload_keep setting.\n. Heading is here for an example message of what this decoder would generate, but the example itself is missing.\n. Same re: missing example message.\n. grammar can also be local.\n. Define this outside process_message and the same memory space will be reused for every message, less allocation and GC.\n. Tiny little style nit: when looking up values with a static table key, the usual Lua idiom is dot notation, i.e. if json.type then instead of if json[\"type\"] then.\n. Rather than hard-coding a check for 'type', 'severity', and 'hostname', I'd like to see a data structure that contains all of the reserved message fields. From the message schema docs, the fields I think we want to check for are uuid, timestamp, type, logger, severity, payload, env_version, pid, and hostname. I imagine iterating through the top level keys of the deserialized JSON, lowercasing each value, and checking to see if it's in the set of default fields. If so, try to convert to the right type (timestamp in particular will need to be handled carefully) and then set the value on the msg, removing it from the source table as you're already doing.\n. It doesn't look like table_to_fields is accounting for keys that already contain a \".\" in the name, which would then cause problems if someone tried to reverse the operation. Probably a good idea to tweak table_to_fields to optionally replace any \".\"s with \"_\"s in the key names, and then adding some tests with nested objects and dotted key names to verify that things come out right.\n. Good point. I was intending to be able to at least get the nesting levels correct, not necessarily have the reversed names match exactly, but escaping is probably a better idea.\n. This is a static value, you want to define it outside of process_message or else it will need to be allocated (and subsequently GCed) for every message.\n. Not quite sure what's going on in here. Seems like we're lower-casing the values in the field_type_map table, but we hard-coded those values, we could have just made them lower case in the first place.\nWhat we really want to do is to iterate through the top level values in the json table. We lower case every key, check to see if the lower-case value matches one of the lower-case message field names, with the right type. If so, that data gets written to that message field instead of as a dynamic field.\nThis will clearly cost some cycles, especially when the incoming JSON structure is large, so it might be nice to be able to turn it off with a config setting.\n. Sorry, been on vacation (and blissfully off-line) for the last week... and am currently in for 3 days before being out for another short jaunt. Yes, I think the config option is a fine idea, once I get caught up on the email backlog I'll circle back around and review the latest code.\n. This still doesn't make sense. A single input / splitter combination will either store the received data as a string in the Payload field of a message struct (i.e. pack.Message.Payload), or as binary []bytes data in the MsgBytes attribute of the pack itself (i.e. pack.MsgBytes). This is specified by the splitter's use_message_bytes config setting, see http://hekad.readthedocs.org/en/v0.9.2/config/splitters/index.html.\nAt the point when this decoder gets the message, it will contain the raw log line in the message Payload. AFAICT, there will never be a case when using read_message('raw') makes sense. If the input has set a programname field on the message, then the field is already set, you shouldn't have to do anything in the decoder to make this happen.\n. The UseMsgBytes config option is automatically supported by Heka for every splitter, and you're not changing the default, so I don't think there's any reason for you to have a UseMsgBytes option. Which in turn means you don't even need a custom config struct.\n. We should probably keep this value in the table and explicitly propagate the incoming message's Hostname value since in case the input has already set it to a useful value by the time splitting happens.\n. Here's where you'd add msg.Hostname = read_message(\"Hostname\").\n. Good point. Done.\n. You can return a second argument that will be logged as an error message, be nice to add those to the error returns.\n. Might be nice to support Payload, which would override payload_keep like Type overrides type.\n. I think we're leaking values through here. If one message has a value that comes from the field map but the next message is missing that key in the JSON, then unless I'm missing something the msg value will still hold the previous message's value. We should add all of the possible fields in use to the msg initialization, and then be sure to explicitly nil out or set defaults for all static message fields that we don't get from the input JSON.\n. Another place we can return an error message.\n. If we define msg entirely within process_message then it will be allocated every time process_message is called and it will go out of scope every time process_message exits, leading to a lot of unnecessary GC churn. When we define it at the module scope, initializing all of the keys we're going to want with a nil value, then the memory space for the Lua table will be created once and reused each time. It does require us to clear the values out each time to prevent the values from leaking through from one message to the next, but the performance improvement is worth it.\n. I think we can do this without the additional loop, can't we? For static fields, the only values that will ever be set are ones that are in the field map. If it's not in the field map, it's initialized as nil and never changes, so we don't have to reset it each time. If it is in the field map, then we can check if the value is in the JSON. If so, we set it to the extracted value; if not, or if the type is wrong, we set it to nil.\n. Do we have to get rid of type? If type is set but Type isn't used, then msg.Type will be initialized once at the beginning and never change, just as with nil for the other values. If Type is used, then the msg.Type will be overwritten with either the JSON value or nil, which matches the documented behaviour.\n. If you really want to default to type when Type fails to return a value then it could be special cased in the loop, would only cost one string compare per field that is set. I don't feel strongly about what the behaviour should be here.\n. Yes, sorry the docs could use quite some improvement here. We provide a date_time module in the Lua sandbox that allows you to parse date strings if you provide the correct strftime format representation string. Calling date_time.build_strftime_grammar returns an LPEG grammar that does the parsing for you. It returns a Lua table that can be turned into ns-since-epoch using date_time.time_to_ns. You can add an optional timestamp_format config setting where users can specify the strftime format string. Lack of a timestamp_format setting would presume the default timestamp format of ns-since-epoch, i.e. exactly what we have now.\n. I don't understand what your statement about \"splitting and decoding phase are tight together\" means. Let me step back a bit and explain how Heka works here in more detail:\nWhen a Heka input gets data from the outside world, it is either in native Heka protobuf format or it isn't. If the data is in native Heka protobuf format, then it contains an entire Heka message. When the data is decoded, a fully populated Heka message is the result. In this case, Heka will store the raw protobuf data in the pack.MsgBytes attribute. The decoder (usually a ProtobufDecoder) will grab the data from pack.MsgBytes, will deserialize it into a Message struct, and will store that Message struct in the pack.Message attribute, overwriting any message data that may have already been there.\nIf the data is not in native Heka protobuf format, then it is clearly in some other format. Usually this format is a string of some sort. In these cases, Heka creates a mostly-empty Message struct, and stores the data in the Payload field of the Message struct. The decoder must access the message payload to get the raw input. It will then parse this input, and write the extracted data back to the same Message struct. It can populate both base fields and dynamic fields. It may overwrite the original payload, or it may not; many decoders in fact have a payload_keep setting that lets you configure whether or not the original payload will be preserved.\nNote that in this second case, pack.MsgBytes doesn't come in to the picture at all. If the splitter did not have the use_message_bytes option set to true, then the decoder can not trust the data that is in pack.MsgBytes. There will be nothing there. All of the useful data will be in the Message struct. If the input has set dynamic message field values, those will be set on the Message struct.\nWhen you call read_message(\"Payload\") or read_message(\"Fields[foo]\"), you are saying \"give me the [Payload|dynamic field 'foo'] value from the pack.Message struct\". When you call read_message(\"raw\"), you are saying \"give me the binary protobuf data from the pack.MsgBytes attribute\".\nGiven all of that, here are the things that smell weird about the code that prompted this whole thread:\n- All of our existing decoders expect to find their input data either in the pack.MsgBytes attribute or already in the pack.Message struct. There are no other decoders that sometimes use one, sometimes the other. There might be valid use cases for it, but they're rare, and IMO would need to be defended.\n- Your conditional above is based on whether or not an rsyslog_template value was provided and, if so, whether or not it correctly compiled down to an LPEG grammar. Even if you did intend to have a decoder that was expecting its input data in two different locations, that is a horrible way to signal that behaviour.\n- Folks usually use syslog for text data. I've never seen anyone use syslog as a transport for binary Heka protobuf data. It's possible, I'm sure, and someone might do it, but AFAIK it hasn't happened yet.\nFor all of these reasons, it seems to me that read_message(\"raw\") isn't the right choice here, and that you don't quite understand what it implies. If I'm wrong about your understanding and it is your explicit intention to be handling binary protobuf data in this decoder, then I still think it's probably a bad idea.\nMaybe it would help if you'd explain what behaviour you'd like to see. If the syslog template compiles, then you're parsing the input and distributing the values throughout the message you're emitting. If there's no template, or it doesn't compile, then... what?\n. You can return a string error message as the second return value here, would be nice for users to know that the input failed to match the grammar.\n. Should this be stats.netdev to distinguish it from the messages generated by the other decoder?\n. Again, it'd be great to return an error message here.\n. read_message(\"raw\") always access pack.MsgBytes. That's exactly what it does, no more, no less.\nWhat's probably confusing you is that Heka will always create a protobuf encoding of every message and store that in pack.MsgBytes before delivering the message to the router. This means that (unless there are bugs) you can always trust read_message(\"raw\") to give you the correct data in any plugin that is after the router, i.e. any filter, encoder, or output plugin. Inputs and decoders cannot make this assumption, however.\nAll message fields that are set by an input are available in the decoder, but you have to use read_message(<fieldname>) to get them one at a time, you can't use read_message(\"raw\") except in very specific cases.\n. Yeah, I did some profiling a while back and found that the balance seemed to tilt towards string.format performing better than concatenation when the string count got to about 5. Not sure if real world usage would match that. I'm happy to settle on string.format for anything over a single concatenation.\n. STDOUT and STDERR\n. Also, was there a reason why you didn't propagate the behaviour down to the LogOutput?\n. There is no fieldfix filter, this must be a reference to a custom filter that you're using internally?\n. You can't return here, you're not inside a function. At module scope, you can use the error() function to exit and return an error message. A better way to code this would be to remove L90-92 and update L82 to:\nlua\nlocal tag_prefix = read_config(\"tag_prefix\") or error(\"`tag_prefix` setting required\")\n. Seems like these three vars should all be local.\n. Seems weird to me that this would default to false. I'd think it more likely that folks will want to use the timestamp from the messages.\n. Timestamp is a static field, not a dynamic one, so there's no way it can return a nil value. Which means that AFAICT ts will never be nil, this line isn't needed.\n. When returning an error code it's a good idea to return an error message as the second return var, like so:\nlua\nif not msg.Fields then\n    return -1, \"Malformed message (no fields)\"\nend\nHowever, again, while a message might not have any fields, it should always have a Fields attribute, so I don't think this check is even necessary.\n. Lua idiom is to use dot notation when looking up static table keys, like so:\nlua\n    local name = field.name\n    local value = field.value[1]\n. Don't need the ~= nil comparison.\n. Nil comparisons are usually written using not, unless you explicitly want to differentiate btn nil and false values, so in this case you can use if type(value) == \"number\" and not skip_fields[name] then.\n. Our policy is to only use the .. concatenation operator for single concatentations. Any time you'd use more than one .., you should use string.format instead. \n. tags.host\n. This doesn't seem to be used anywhere, should be removed.\n. Believe it or not, Lua actually has to count the number of items in an array each time. Since we're adding metrics in a tight loop like this, it'd make sense to store and increment a separate variable for the buffer length, so it doesn't have to be recomputed each time around.\n. Our convention is to put the PRESERVATION_VERSION stuff as the first lines of the module, immediately after the documentation comment. Also, you'll want to add in a value that you can increment in the source code, for times when changing the source code of the encoder invalidate the saved data. You can see an example of how this should be done in the HTTP Status filter.\n. Okay, my bad, I thought read_message(\"raw\") would always return at least an empty Fields table. In that case then just make sure to return an error message as specified above.\n. Wouldn't this return an error even during normal shutdown? We need a way to distinguish btn the AMQP channel closing unexpectedly and it closing as expected b/c the Stop method was called.\n. That's fine, but the purist in me is still a bit put off by returning an error when there isn't really an error case. I'll add some code so the Stop method sets a flag, which the Run method can check on the way out. If the flag is set, we'll return nil; if not, we return the error.\n. We've got a RetryHelper that does exponential back-off with jitter that we use elsewhere throughout Heka. Someday we might even add support for the helper noticing when a shutdown is happening to abort without latency. Regardless, it'd be great if you could use that in here rather than hand-rolling.\n. Here and in a couple of other places it looks like AttachManager.Run is logging an error and then returning the error, but then the input itself returns the error, which will cause Heka to log the error a second time.\n. Oops, I misread... you're _not returning err here, so there won't be duplicate logging. But the order will likely be a bit confusing. I'd recommend logging the err returned from withRetries and then returning the giving up message so it shows up afterwards. Same with above.\n. This comment is misleading, RemoveEventListener doesn't actually close the channel, it only removes it from the set of listeners.\n. I'm a bit confused. If withRetries(m.attachAll) returns an error in the Run method, then Run itself returns the error which AFAICT will cause the plugin to exit. But if it returns an error here in the restart method, then we just log the error and take no further action, everything keeps running as usual. Am I understanding this correctly? Is this actually the behaviour that we want?\n. There are a lot of places where an error is logged before being returned. With the withRetry decorator pattern, it's a bit hard to keep track of where all of the error logging happens, but I'm pretty sure that there are more duplicates than just the ones already mentioned. Generally I recommend pushing the logging to the topmost layer that touches the error.\n. And again w/ logging ambiguity.\n. Not only a leak, but in handleDockerEvents we call restart if m.events is closed, and then restart calls RemoveEventListener(m.events). If there's a case where RemoveEventListener might again call close, then we'll panic.\n. To be clear, I think the existence of the withRetries decorator is fine, as long as it uses RetryHelper internally.\n. Looks like you were trying to move the headers  piece down here to the bottom, but you instead moved the headers description and the following body label.\n. This is unrelated and shouldn't be included in this PR.\n. Everything in this file is also unrelated and shouldn't be included in this PR.\n. Same here and in the next file, none of the docker stuff is relevant to this PR.\n. Seems like this should be a config option on the plugin rather than a hard-coded constant.\n. Not sure why this constraint is here. In the RegexSplitter, the capture group specifies which part of the regex match, if any, should be included in the record that is emitted. AFAICT you're always throwing the delimiter away, so this has no impact.\nUltimately, it would probably be better to match the behaviour of the RegexSplitter, to enable people to keep the delimiter in the record if desired.\n. Sorry for showing up late here, and having feedback that undoes some of the work that you've done. However, I don't think we should be writing any of the stats values to fields here in the input plugin. That work would be better done in a SandboxDecoder. You can parse the JSON and extract the values from there to add to the message you're injecting.\n. Again, I think all of the field marshalling stuff should be removed. Lua is a better bet for this type of stuff, no reflect needed.\n. Small nit, maybe, but I'd rather see the message string set in a value at package scope and fmt.Sprintf used to substitute in the event values.\n. If err != nil then done will be closed twice, we'll panic.\n. This might be another small nit, but the defers in the next few methods feel like overkill. In many cases the defer is only one line away from where a non-deferred call would be placed.\n. This break statement (and the one below on L61) will only break from the select, which means that AFAICT the for loop will never exit.\n. I'm not positive, but from looking at the graylog client code it looks like this call might block forever, which means Heka shutdown would hang if nothing was coming over the connection.\n. AFAICT this line will never be hit, but maybe I'm missing something?\n. I know there are plenty of examples of this littered throughout Heka code, but it's actually not recommended to use bare returns except for very short functions where the return var declaration is clearly visible from the return call, if then.\n. This never changes, it can be set outside of the process_message function so it's not reinitialized for every message.\n. You can return a second value containing an error message, I'd recommend it here.\n. Add newline at the EOF.\n. Add newline at EOF.\n. No, I'm not talking about the error() function, I'm talking about the return -1. You can return multiple values, and if you return a string for the second value it will be used as the error message. There are a couple of examples in the json decoder.\n. Not sure what the value of replaceDotsWith function is over the stdlib's strings.Replace:\nstrings.Replace(f.GetName(), \".\", replaceDotsWith, -1)\n. ",
    "trink": "The alias was already removed, and the header constants were move to the Message package to avoid a circular import and that appears to be where my refactor stopped ;)\nI assume you are referring to the client::senders::SendMessage and Pipeline::createProtobufStream code both encoding the stream header.  I don't think we want the header in the msgBytes so unless we want to change some interfaces/struct the best we can do is break out the header encoding into a common function (which I should do) but I get the impression you had something more in mind.\n. +r\n. The group capture comment:\nIf the matcher creates a map of key value pairs, that data structure can be used to create a new message (like the textparser decoder and stat_filter do).  However, if a new message is not created then the data has to be passed along side the message through the pipeline requiring changes to the filter and output interfaces (to handle the special capture map).\nI guess I don't understand the use case where the matcher would manipulate the contents of a [immutable] message using the boolean match string,  If this requirement is only about code reuse and using the same syntax then: \n\"PayloadMatch\": \"TIMESTAMP (?P\\S+) (?P[^[]+)[(?P\\d+)](?P[^:]+)?: (?P.*)\",\nwould have to be rewritten as a boolean expression\n\"PayloadMatch\": \"Payload =~ /TIMESTAMP (?P\\S+) (?P[^[]+)[(?P\\d+)](?P[^:]+)?: (?P.*)/\",\nThe only advantage I see here is the ability to capture from multiple fields at once \"Payload =~ /capture_exp1/ && Fields[demo] =~ /capture_exp2/\" but I don't think that is needed.\nAlso, if the above expression was passed to the matcher, it would mainly be confirming the format of the payload matched as opposed to matching on some specific content.\n. I cluttered up stat_filter.go before the Filter interface redesign; the 4 functions at the bottom of the file are no longer needed and can be removed.\nThe benchmark test results look good, I also ran flood with a Lua sandbox filter; the throughput is unchanged (the new flow is working well). The code changes match our Friday discussion... lets merge.\nr+\n. r+\n. Something like this would also address a lot of performance/memmove issues we are seeing  http://kentonv.github.io/capnproto/index.html\n. The lazy decoding will not be implemented.\n. MGI removed in Issue #107 \n. This is a very easy way to take down/cripple Heka and it should be addressed in the 0.2 release (at least for sandbox filters).  I don't think the MGI is the place to do it since the cycle can be as long as the number of running plugins. A->B->C->D->A and unless we store that chain in each new message the MGI will not be able to detect the loop.  In the sandbox case this could be prevented by rejecting a message_matcher that accepts messages from another sandbox that does not exist (i.e. A would not be able to subscribe to D in the above example) but this is easily thwarted by stopping and restarting A with an updated message_matcher.  Not letting sandboxes 'subscribe' to other sandboxes would solve the problem but that also prevents some valid and useful scenarios.\n. 37d26f6f5cea473e0788007c1fb78247f14e9d49\n. Restriction by size (number of bytes) is in bc822149203df914e624378b29991767ac1911c3\nRestricting by the number of message per time period may still be required\n. I could add field the standard message header, basically a source/client id, but without a single use case within Mozilla it would be pre-mature.\n. Improvements have been made to the dashboard although it is still a basic debugging UI.  This issue is being closed out since it is too generic.  Specific improvements to the UI should have new issues created.\n. https://github.com/mozilla-services/heka/commit/a4d291a6b1feaa2f3b6d2b55d8214b41d1457781\n. See #355 \n. bc822149203df914e624378b29991767ac1911c3\n. MGI was removed in Issue #107\n. I had a good talk with tnorris about V8 and agree with his recommendation of \"Don't do it!\"\nNot scalable: amount of memory required, each sandbox would be its own process\nA lot of work to wire it up for no benefit (10 lines of js sandbox code instead of 10 lines of Lua code)\n. bc822149203df914e624378b29991767ac1911c3\n. bc822149203df914e624378b29991767ac1911c3\n. decoders are not removed from decoderRunners when they are destroyed\n. The report message looks fine\n. The decoder clean up looks good but the decoder management seems overly complex and inefficient.  Every TCP connection will spin up a decoder for each encoding type (yes there are only two but it highly unlikely more than one will ever be used).  When the connection is closed they are all thrown away and a new set is created for the next connection.  \nOn another note: from a reporting perspective it may be interesting to track how many message have passed through a channel since a queue depth of 0 cannot tell you if it is just fast or unused.\n. The relative path is no longer supported in the cgo LDFLAGS directive.  The only option that seems to work is using a fully qualified path.  Attempting to set the library path using the CGO environment variables or passing it in the Go build flags have all failed.\nlua_sandbox.go line 18:\ncgo LDFLAGS: -L../../../../../../bin -lsandbox -lm\n. After working more with the sandbox termination code (and having many of my sandboxes terminated),  I don't think additional restrictions are necessary.\n- it will cause some confusion\n  - users have to keep track of which managers have which settings\n  - in could make it impossible to run a sandbox under all/any managers due to the varying restrictions\n- the maximum resources are still relatively small; memory is probably the biggest issues and it can be capped by limiting the number of sandboxes that can run under a manager.\n- if the users sandbox 'almost' fits within the tightened restraints the most common fix I have seen is to loosen the restraints (which eventually creep up to the maximum)\n. The SandboxManagerFilter should be able to specify the maximum SandboxFilter limits. Limit values provided in the dynamic sandbox submission should be ignored (i.e. like filename)\n@rafrombrc \n. On the Windows side it looks like we will be using mingw/msys so we can probably just stick with make.\n. 37d26f6f5cea473e0788007c1fb78247f14e9d49\n. Moving this to 0.2 so we can have some data analysis and visualization when running stand alone\n. FYI: the default test is pushing about 7.5 - 8.0 Gbits/sec, the simple test pushes 2.9 - 3.0 Gbits/sec on my Thinkpad x230. I added the throughput numbers to the flood output since only looking at messages/sec can be misleading.  Issue #112 \n. Wrong branch, issue #116 was created instead\n. Not sure what you are looking for here.  If UUID is not set (a valid state when first creating a message) GetUUIDString will return an empty string which is correct. \nThe other way to get an invalid UUID is from decoding a protocol buffer message with a bogus UUID byte array. If it is less than 16 bytes GetUuidString will return an empty string. If it is 16 bytes it will put it in UUID format (although it still may not be a valid UUID). In this case the consumer (InputFilter) can decide if it should be discarded.\n. I pulled LPEG 0.12 into the Heka sandbox to do some experimentation.  It works great, however I don't have a nice way to integrate it into Heka:\n1. The Lua sandbox is currently only exposed as a plugin filter.  Heavy parsing in a filter is strongly discouraged since it can backup the entire system.  In the case of a sandbox it will most likely mean its termination.\n2. Exposing it as a decoder plugin would solve the backup problem and would work well when you have access to the entire message. i.e., when paired with something like the HTTPInput plugin.  However, it may not work as well when processing a stream of data since the input plugin would have to do some of the parsing work to expose a proper/complete message.\n3. Exposing it as an input plugin while preserving the sandbox integrity would require a data stream interface to/from Go/Sandbox which would be very inefficient. In addition LPEG would have to be customized to work with streams (fetch/wait for more data mid-match).\n4. Inputs and decoders are not dynamically loadable so Heka would have to be stopped and restarted after the grammars were deployed.\n5. The LPEG captured table could be serialized into a protobuf message in the C layer and then decoded in Heka.  Realistically this is how issue #148 would be implemented (serialize/encode/decode) not very light weight though.\n6. Unless you were parsing some one off format there is probably a Go parser for it and you wouldn't need to write a PEG; instead you would most likely create a new Heka input plugin in Go.  The downside: you have to rebuild, redeploy, and restart Heka.\nI took a quick look at pego: it would need a bunch of work/testing and the grammars are not dynamic so it would have the same downsides as item 6.\nThese are some large and unlikely changes but if we revisit it in the future here are my recommendations:\nItem 5 - get some benchmarks on Heka message creation from Lua\nItem 4 - at this point make all plugins dynamically loadable\nItem 2 - expose an LPEG enabled sandbox as a decoder plugin\n. With the completion of #148 and #275 it is very little code to incorporate LPEG into the sandox.  Reopening since I have tested code that does it.\n. See #370 regarding item 2 above\n. It depends if we want to make Heka 'go gettable' by moving the Lua sandbox out into a different package.  Lets discuss what that would entail, I am unclear how that would work seamlessly.\nBuilding it all in one package isn't bad, the doc generation and the python code are causing the biggest issues at the moment.\n. #156\n. For use in #151 \n. Not required for 0.3, the type and name attributes are proving sufficient at the moment\n. Recording the amount of time each plugin holds onto a message is expensive (on my system ~40ns per timer) so instead only sampling is done and restricted to the matcher and SandboxFilter where it is required to detect\nproblematic code.\n. Message counts were added to the router and sandbox plugins. They can be added to other plugins as desired using a ProcessMessageCount field in their ReportMsg function.\n. - Ability to see what sandboxes/versions are running on all Heka instances along with their state\n- Ability to deploy/remove sandbox filters to/from a machine/machine class.e. web servers, db servers.\n- Ability to synchronize or peg particular instances/versions to a particular machine/machine class\n- Ability to apply machine specific configuration options on deployment i.e. a set of hostname matchers\n. Possibly making the output filter look like a Nagios Agent (NRPE/NRDP/NSClient++)\n. Used a Nagios passive check and the external command interface via HTTP to propagate alerts.\n. See #253\n. Closed as part of Issue #253 \n. We would only need to add a message header (handful of bytes) to use the current transport. We would also need to define encoding types for the various formats.\n. Addressed by #381, TCP/UDP Inputs can now take a stream of any type of data.  You could simply open a local log file and write it out to a socket as-is.\n. Update: the results are the same in Go 1.1.1\nThe benchmark tests are actually equal or better on Windows when compared to Linux.  The issue appears to be with the Go scheduler and the recommendation, for now, is to run with maxprocs=1 on Windows.\n. After running some more tests the low throughput appears to be an artifact of testing with the loopback interface on Windows 7.  The throughput of the interface is much lower than on Linux.  I was also able to observe an occasional 5 second cut out as described here: http://support.microsoft.com/kb/2020447. \nIt would be interesting to run this test on the Fast TCP Loopback in Win8/2012 Server: http://blogs.technet.com/b/wincat/archive/2012/12/05/fast-tcp-loopback-performance-and-low-latency-with-windows-server-2012-tcp-loopback-fast-path.aspx.\nIt is still unclear why the performance degrades when increasing the maxprocs but it doesn't appear to be the primary reason for the low throughput.\n. FYI: I am not seeing the issue on Ubuntu 12.04\nWhat version of the GCC compiler are you using?\n. Yeah, testing against a mock http client is pretty useless so it is manually tested against the real thing.\n. The channel buffer depth fluctuates wildly, it can hit capacity without there being a real issue and therefore cannot solely be used as a condition for termination.  Experimentation with some other metrics/combinations is necessary.\n. I hate duplicating data.  Before this is committed we should verify that fetching multiple fields from the message is actually faster than parsing the relatively short payload string.  I may have to add a profile flag to the sandbox because the random sampling can take a while to build an accurate picture.\n. My preference is structured data too.  The profiling option is in (well pending review) so we can get some measurements each way.\n. After doing a number of experiments I would propose the value_format enum be deprecated and a new optional string field 'representation' be added.  Since it is a free form string field the user would have to ensure consistency with its use but it is much more flexible and easier to use then a formal schema.  The representation can be used for parsing (date/time strings, ip addresses etc...), display output (markup, labels), and aggregation (combining two duration fields that use different units)\nNewField(\"address\", \"test@example.com\", \"email\")\nNewField(\"address\", \"192.168.1.1\", \"ipv4\")\nNewField(\"address\", \"http://example.com\", \"uri\")\nNumeric representations would most likely use SI units:\nNewField(\"ResponseTime\", 0.012, \"s\")\nNewField(\"ProcessTime\", 12, \"ms\")\nNewField(\"ResponseSize\", 12643, \"B\")\nImpact: \n- Breaking change to the message.NewField API.  The third parameter would change from an enum value to a string.\n- Non-breaking change to the sandbox set_header API the type parameter would become representation and a new optional parameter aggregation_method would be added (consisting of none|sum|min|max|avg)\n. ?? LogFileInput cannot consume a protobuf file stream\n. It was reopened because the LogfileInput still cannot process a protobuf stream file written by FileOutput.  We need to fix it or make a dedicated ProtobufFileInput plugin.\n. I have a local branch that can identify and cluster the common match statements that can fail a match expression but the router would need to be completely redesigned and the hierarchy would need the ability to re-organize at run time as dynamic plug-ins are added/removed.  This should be postponed until we actually need it.\n. Not needed.\n. See #330 Which use a sandbox plugin as an alternate way of handling multi-line input by taking the individual lines and sessionizing them into a single entry.\n. Here is how splunk allows you to specify it:\nhttp://docs.splunk.com/Documentation/Splunk/latest/Data/Indexmulti-lineevents\n. Splunk seems overly complex, how about this\ndelimiter = \"regex\"\ndelimiter_location = \"start|end\"\nA single capture group can be specified to preserve the delimiter (or part of the delimiter).  The capture will be added to the start or end of the log line depending on the delimiter_location variable.  This should be sufficient for most use cases. When a start delimiter is used the last line in the file will not be process (since the next record defines its end)\nexample 1 (above)\ndelimiter = \"\\n(#\\s+User@Host:)\"\ndelimiter_location = \"start\"\nexample 2 (above)\ndelimiter = \"\\n(\\d{4}-\\d{2}-\\d{2})\"\ndelimiter_location = \"start\"\nstandard log line\ndelimiter = \"\\n\" # discard the delimiter (no capture group)\ndelimiter = \"(\\n)\" # keep the delimiter at the end of the record\ndelimiter_location = \"end\"\n. Current ReadString Parser: Elapsed Time 26554475217 Lines 28442305 Lines/sec 1.0710927166729104e+06\nThe conversion from a byte array into a string is where we are taking the bigger hit, the parsing is pretty quick\nRegex parser \"\\n\": Elapsed Time 32083724240 Lines 28442305 Lines/sec 886502.6013576034\nRegex parser \"\\n\": (no string conversion) Elapsed Time 28071119160 Lines 28442305 Lines/sec 1.013223050990034e+06\nRegex parser \"(\\n)\": Elapsed Time 34598266236 Lines 28442305 Lines/sec 822073.129502812\n. After a protobuf  message is decoded nothing should be touching the msgBytes or changing the decoded message at that point so a flag on the pack should suffice.  If the message is altered for some reason , the flag should be cleared forcing the message to be re-encoded on output.  If the message did not originate from a protobuf the flag is never set and the current message will automatically be encoded on output (and the flag set if encoding it back to msgBytes).  However, re-encoding to msgBytes creates a race condition (currently each output encodes to its own stream).\n. The dot does not match a newline by default so the string will not match to the end and the match will fail\n. r-\nIt would be better to pass in the time zone name and use ParseInLocation() otherwise it would be quite difficult to manage time changes i.e. PDT to PST\n. A Filter/Output without a matcher seems like a very bad thing. In this case a custom data flow has to be created to utilize these plugins complicating shut down and creating non-obvious dependencies.  Why not just dis-allow having no matcher set?  I guess I am missing the no matcher use case.\n. r+\n. r+\n. It seems like instead of having a dummy catch all decoder which does no data extraction it would be cleaner if the MultiDecoder had a catchall property (bool).  If the order array is exhausted (no matches) and catchall is set to true the message would be routed into the system and the the message type consistently set to something like \"%DECODERNAME%.catchall\".  If the order array is exhausted and catchall is set to false the message would be discarded (optional error logging would be good property to have here too). To insure consistent message type naming out of multiple sub decoders we may also want to prefix the message type with the decoder name like the catchall example or allow the user to specify the prefix.\n. http://code.google.com/p/go/issues/detail?id=5986\n. The workaround was to create our own sqrt function; leaving this bug open since it should be removed when the Go bug is fixed.\n. Moved the sandbox out of Heka issue #464, so this has been cleaned up.\n. Why did we create or own JSONPath syntax/notation?\n. More generally; any Go code can create a message greater than MAX_MESSAGE_SIZE. Internally this is fine, the transports are where it would have to be handled.  We have a couple of options:\n1) Increase the max message size\n2) Split large messages into multiple pieces and re-assemble on the other side (the only real advantage here is large message could be sent over UDP but there is no guarantee all the pieces would make it)\n3) Discard large message because they violate the limit \n. The json_map and message_fields seem a bit tedious when the json map alone could do the job.  The message_fields do allow you to add some static data to the capture but I am betting if you needed to transform the json data you would need something more robust anyway.\n. #826 \n. I would say no on the Linux build constraint (XML parsing is needed on Windows too). \nBrainstorming: \nWith the new build system we could detect libxml2 and if it exists install the plugin code to the Go workspace.  The files would have to be placed in a different directory or the build would have to cherry pick files in pipeline to install.  Also, the all_specs_test.go would have to be auto generated (I am assuming we want to keep everything in the pipeline package to avoid a bunch of test issues).  A similar approach could be applied to for the Sandbox (#343) using a configuration flag as opposed to library detection.  The down side is that build configuration could become as involved as hekad configuration  but it would be custom tailored to what you have installed and want to use (at this point we could probably generate a nice configuration template too).\n. I am not sure how optional Lua will be going forward, we are looking to leverage it for some additional tasks, like decoding, in the future.  What is the particular issue you are trying to solve?  Build issues? Size issues?\n. Can you send us the platform information and the output from the failed build?  Thanks.\n. The new build system should fix that https://github.com/mozilla-services/heka/tree/features/cmake.  It hasn't been merged yet because we are changing our CI system but you can try it out.\n- clone a clean copy of the heka repo\n- cd heka\n- git checkout features/cmake\n- . build.sh\nFull instructions can be found here (FYI: it also changes the way third party plugin are included):\nhttps://github.com/mozilla-services/heka/blob/features/cmake/docs/source/installing.rst\n. Typically it means the generated file lua_sandbox.go was manually deleted and this is how cgo blows up.  You could try running 'cmake ..' in the release directory to regenerate it.\n. The reason it is not finding Go is because it requires a released version of Go 1.1 or greater.  The dev builds only have a SHA1 version tag and we cannot determine if it is compatible (I just tweak the check when I am testing with a dev release of Go) .  As for the prerequisites yes there is a full list in the install documentation link above.\n. If we were to custom tailor the target executable by building only the plugins that a particular environment consumes it shouldn't be limited to only sandbox plugins.\nAs for the Lua specific concerns:\n- Having Lua statically linked into Heka contributes to less than 2% of the executables overall size.\n- If you don't configure the sandbox/sandboxmanager you are not exposing it.\n- If there are build issues, the build should be fixed (Lua itself builds just about anywhere, and the C99 sandbox should build everywhere Go builds, again if there is an issue it should be logged and corrected)\n. I just took a look at the LogfileInput and realized the current design has a number of assumptions that are incorrect and will make real time monitoring or data sessionization very difficult for the user.\nAssumptions:\n1) Log files are independent entities that have no data or time correlation\n- simple failure use case: correlation of Apache access and error log to determine the number of errors in a session.  The error event my be processed before the access log session event even though the error was written to disk at a later time. These logs need to be multiplex together. However, even if the input could correctly multiplex the files by event time stamp (which it doesn't have access to) they would have to be using the same decoder to ensure the order is preserved into the router and the chances of only needing correlation between logs in the same format is pretty slim.\n2) Messages are passed to filters in the order they are logged\n- This non deterministic for multiple log files\n- The bigger problems arises with Issue #344 when a wild card represents a time series of rolled logs. Since the input doesn't account for rolled log ordering by creation time or naming convention i.e., YYYY-MM-DD-HH-AAA-BBB and has no way to multiplex overlapping logs/events the data stream into Heka will be a scramble set of events. \nIdeally input to Heka would be in identical order whether Heka was running on the box all day or if it was started at the end of the day. Without this consistency the sliding window views, injected messages, and alert analysis would be vastly different from run to run.\n. Clarification: #2 above was referring to wildcarding the logfile name /mylog/*.log\n. Is this the default we want on Windows too?  We probably want to base it off of Heka root Issue #331.\n. Tasks:\n1) add a message queue to preserve the data when the endpoint is down\n2) add reconnect capability with a backoff to re-establish the data flow\n. Wasn't going to restart the plugin, that is pretty heavy for a moment of lost connectivity.\n. Yeah, lets get the various sample configs rounded up and put in the examples directory then the installation/package creation can just pick up that directory.\n. It really depends on what the data looks like (data size, repeated strings etc) and how quickly it is recycled.  In some very basic tests the recycler makes things a little slower (by a few percent) than just allocating new objects.  However, there are two cases where the recycler could be useful in the future.\n1) if we alter the message structure to use byte arrays instead of strings and modify the Go protocol buffer code to reuse the preallocated memory.\n2) if we start tracking when Heka is done processing a message for things like checkpointing and delivery confirmation.\n. Leave it for now and revisit it when items 1 or 2 change.\n. The name was just to stick with the Payload...Decoder convention but SandboxDecoder works for me too.  However, there is a very high probably that LPEG will be used even though the native parser is very nice.\n. It already does :)  And the filter version can too with some restrictions as described in https://hekad.readthedocs.org/en/latest/sandbox/lua.html#api\n. I think we need better defined requirements before building this out.  Here are some use cases that come to mind:\n1) Sequentially numbered logs without any time information embedded in the name (usually when rolled by size, typically only a single active file is being written).  Processing the stream in order is easy but starting from a specific time (i.e. 24 hours ago) becomes more tedious.\n2) Time based logs that contain data based on the write time (typically a single open file that contains any data received during that period (regardless of when it was created)).\n3) Time based logs that contain data based on its creation time (meaning multiple logs could be open/written at the same time).\n4) Compression and archiving when logs are rolled.\n5) Directory based logging structures.\n6) Out of order events.  We should process any single data source in the order it was received/written.  However, if we have multiple active inputs we should process the oldest record first. Unless there is a very large backlog of old data being processed this will have little impact on data latency.  Real time plugins will still have to be defensively programmed for out of order events but the rough ordering should reduce the execution of the defensive code.  Scope-wise this is bigger than just the LogfileInput and multiplexing all inputs may be desirable.\nCommon log name variables: year, month (number, name, abbreviation), day, hour, week, day of the week, sequence number, max sequence number.  Some examples:\nxyz-2013-08-26.log // day\nxyz-2013-08-26-11.log // hour\nxyz-2013-08-26-11-001-006.log // fractional hour sequence/max sequence (log rotated every 10 minutes)\n2013/08/26/xyz-11.log // day directory, hour log\n2013/08/26/xyz-001.log // sequential daily log\naccess.001.log // sequential log\n2013/Aug/08/xyz-11.log // day directory, month abbreviation, hour log\n2013/August/08/xyz-11.log // day directory, month name, hour log\nxyz-2013-W35-1.log // week/day of the week support?\nxyz-2013-238 // ordinal daily log support?\nNeeded configuration options to support the use cases above:\n1) layout specification\n2) timezone specification\n3) start time i.e., (process all logs within the last 24 hours that haven't been processed, just start with new data etc...)\n. Should update CHANGES.txt also.\n. LogfileInput has a similar problem #263 that I am working on now.  \nTo correctly parse out the records in a stream the input is really doing the work of the decoder (or at least a portion of it).  If you look at TCPInput it finds a message, copies it into the MsgBytes buffer, and passes it off to the decoder on a channel to simply call unmarshal.  This abstraction actual slows the system down and offers no benefit since messages from a single source cannot be decoded in parallel because it could change their order.  I would vote for an interface that can be embedded in each input that outputs a fully formed messages to the router.  \nThe TCPInput would no longer dynamically figure out which decoder to use, it would just be preconfigured and the different stream types would use different ports i.e. heka protobuf stream, generic log stream, syslog etc.  This would also address the rigidity around the MessageEncoding protobuf header (we could simply remove it).  This design would also eliminate the decoder runners and pool in the current implementation.\n. Redesign TCPInput to work more like LogfileInput\n- Refactor out the common stream parsing components from each\n- Add a decoder configuration parameter\n- Remove the encoding type from the protobuf header\n. In #418\n. The generated import statement only includes the root directory since heka-mozsvc-plugins doesn't use nested packages.\nimport (\n     _ \"github.com/tgulacsi/heka-plugins\"\n)\nAn option could be added to include sub packages\ni.e. add_external_plugin(git https://github.com/tgulacsi/heka-plugins master email http twillo mantis)\nimport (\n \"github.com/tgulacsi/heka-plugins/email\"\n \"github.com/tgulacsi/heka-plugins/http\"\n \"github.com/tgulacsi/heka-plugins/twillo\"\n \"github.com/tgulacsi/heka-plugins/mantis\"\n)\n. Dupe of #71 \n. They were in LogfileInputConfig but they needed to be replicated in the StreamParsers also (if you were using it outside ConfigStruct like a lot of the unit tests do).  I didn't like having it in two places and the StreamParser is the source of truth. \n. Yes, you can create a plugin filter to sessionize the individual service messages into a single message for down stream consumers.\n. see #362 \n. Thanks, I will update the PR on Tuesday since I don't think it will be blocking anyone over the holiday. It will also give Rob a chance to go through it and sign off on the backward incompatible changes.\n. Refactoring this PR\n. As per https://hekad.readthedocs.org/en/latest/installing.html you need to source the build script \". build.sh\".  This will properly setup the Go environment the build needs.\n. To access the environment variables from cmake you need $ENV{VARNAME}.  BTW what is the go executable in your /usr/bin ?\n. Missed the client NewJsonEncoder and flood references\n. https://github.com/rafrombrc/heka/tree/encoders\n. Already entered as #411\n. I have been reading the syslog RFCs, rsyslog, and syslog-ng documentation.\nI don't believe Logstash/Grok is a good reference implementation for our syslog decoder.  The Grok regex's are more of a hack (inefficient in places and incorrect in others).  What we need is a grammar that describes the standard pieces as per http://tools.ietf.org/html/rfc5424#page-8 (grammar thoughtfully provided :)) and extensions for the template properties i.e. 'pri-text', various time/date formats etc.\nSome options:\n1. Ideally the Heka decoders would be based on the syslog templates; generating a parser to match the configuration.\n   -  rsyslog\n     See string based templates: http://rsyslog-5-8-6-doc.neocities.org/rsyslog_conf_templates.html\n     Template properties: http://rsyslog-5-8-6-doc.neocities.org/property_replacer.html\n   - syslog-ng\n     See Chapter 11 Manipulating Messages: http://www.balabit.com/support/documentation/syslog-ng-ose-3.5-guides/en/syslog-ng-ose-v3.5-guide-admin/pdf/syslog-ng-ose-v3.5-guide-admin.pdf\n2. Scaling back item 1 and just creating grammars to support the internal standard templates would leave us with something like this to convert by hand.\n   - rsyslog\n     - FileFormat - \"%TIMESTAMP:::date-rfc3339% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n\"\n     - TraditionalFileFormat - \"%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n\"\n     - ForwardFormat - \"<%PRI%>%TIMESTAMP:::date-rfc3339% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%\"\n     - TraditionalForwardFormat - \"<%PRI%>%TIMESTAMP% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%\"\n     - StdSQLFormat - \"insert into SystemEvents (Message, Facility, FromHost, Priority, DeviceReportedTime, ReceivedAt, InfoUnitID, SysLogTag) values ('%msg%', %syslogfacility%, '%HOSTNAME%', %syslogpriority%, '%timereported:::date-mysql%', '%timegenerated:::date-mysql%', %iut%, '%syslogtag%')\",SQL \n   - syslog-ng\n     - default - ${ISODATE} ${HOST} ${MSGHDR}${MSG}\\n.\n. That setup works well for me, here is a 32 bit package: http://people.mozilla.org/~mtrinkala/heka-0_4_0-windows-386.zip.  I am using the tdm-gcc-4.7.1-2.exe install of MinGW\n1) remove the build directory if it exists\n2) make sure your GOROOT and PATH are pointing at the 32 bit version of Go\n3) run build.bat\n. This is more of a diagnostic/debug feature.  The use case it was developed for was better resolved by fixing the data.  So until we come up with a must have use case I think we should just leave it in this branch.\n. The telemetry team needs a way to generate their data visualization from the metrics messages they create.  Iterating the fields in the first message from each logger allows the plugin to capture all the metadata for setting up the circular buffer and eliminates the manual duplication and maintenance of this data.\n. PR #509 \n. Missing existing functionality\n- More detailed diagnostic data provided by each specific type of plugin should be available\n- On the main page the plugin names should link to the more detailed diagnostics\n- Sandbox outputs should have a dedicated page per plugin with a TOC.\n- There is no termination report for failed sandboxes, they just disappear.\n- No log scale presentation option (useful when the data columns have widely varying scales)\nNew useful functionality\n- Ability to display the code and configuration of a running sandbox\n- Change the time units for graph titles: 3600 second aggregation for the last 1440 hours -> 1 hour aggregation for the last 60 days\n. We should probably just pass the entire pack down and allow read_message() to accept \"raw\" as a parameter.\n. Yeah 'mingw32-make clean-heka' regenerated the mocks and fixed the problem.  I must have switched branches without cleaning between builds.\n. This test in invalid on windows and is now skipped.\n. Dupe #151\n. https://github.com/mozilla-services/heka/pull/938\n. ``` lua\n-- move the locals and message table out of process_message, they only need to be setup once and this will save a bunch of processing and GC\n-- the timer_event() function is not required in a decoder\nlocal countField = read_config(\"count\")\nlocal fields = {[countField] = 0}\nlocal msg = {\n    Type = read_config(\"type\"),\n    Hostname = read_config(\"hostname\"),\n    Logger = read_config(\"logger\"),\n    Severity = 6,\n    Fields = fields\n}\nfunction process_message ()\n    fields[countField] = tonumber(read_message(\"Payload\"))\n    inject_message(msg)\n    return 0\nend\n```\n. @tdavis That is a different issue, for now you need to add the values individually with AddValue()\n. https://hekad.readthedocs.org/en/latest/message/index.html\n. #826 \n. The comment was meant for Christian when testing (if 0.4.0 was being used, since require \"string\" is not supported). It shouldn't have been part of the commit though, so I removed it.\n. Creating a LuaSandbox decoder to do the same task I am able to process a one million line log file in about 17s, using the multi-decoder above it takes 61s.  The multi-decoder isn't the only tool in box and it is almost never the best one.  To put it in perspective the Lua sandbox has to copy the JSON, decode it, re-encode it as protobuf, and decode it back into a Heka message and it is still over 3x faster.\n. flood.toml still has references to the json encoder, other then that it looks good.\n. #687\n. http://public.kitware.com/Bug/view.php?id=12997\n. FYI: It broke with the decoder pool removal.\n. Don't bother with the shell script and just set the filename in the call to cpack. i.e.,\ncpack -G DEB -D CPACK_PACKAGE_FILE_NAME=heka-0_5_0-amd64.  Also remove the DEB from the original configuration to prevent the incorrectly named file from being generated.\n. r+\n. Still a little weirdness (but nothing that needs to be addressed now):\n- multidecoder code/test is separate directories... saw the comment (when we decide what is happening with the PayloadDecoders this will work itself out)\n- there are a number of all_spec_test files that will probably only every contain a single entry (consolidate? or leave for consistency?)\n. https://github.com/mozilla-services/heka/pull/556#issuecomment-29644027\nThe tests would still live in the package... just wouldn't need two files for a single test spec is all I meant.\n. Unless you have big plans for the protobuf decoder; now almost all decoders support multi pack output ;)\n. - Since we are stating it should be used as an RFC 5424 severity 7 would suffice.\n- We should default it in the protobuf message definition also\n. r+ when the change log conflict is resolved\n. The external packages that we do manage (now over 20) are installed locally in the Heka build directory so as to not interfere with anything else on the system.  I don't believe this is practical with the above packages and system level components should be explicitly managed by the user.  Although providing a set of brew commands, apt-get commands, etc in the install documentation or possibly standalone dependency bootstrap scripts could be beneficial.\n. Starting to use the envelope version with some FxA message since they have schema/api version numbers and this is a good place to put it.\n. That is because Heka consumes a specific lua_sandbox tag which hasn't been rev'ed to included that change yet.  You can update externals.cmake to pull a64295de9a17db40a864ef42af02c6bf0770169e (it will be part of my next PR)\n. Closed see #393 \n. This in now what my alerts look like:\nReturn-Path: heka@localhost.localdomain\nX-Original-To: trink\nDelivered-To: trink@example.org\nReceived: from localhost (localhost [127.0.0.1])\n    by localhost (Postfix) with ESMTP id CE6DAC20593\n    for ; Fri, 14 Feb 2014 08:56:28 -0800 (PST)\nFrom: heka@localhost.localdomain\nSubject: Heka [HekaAlert]\nMIME-Version: 1.0\nContent-Type: text/plain; charset=\"utf-8\"\nContent-Transfer-Encoding: base64\nMessage-Id: 20140214165628.CE6DAC20593@localhost\nDate: Fri, 14 Feb 2014 08:56:28 -0800 (PST)\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\n. When I use a real mail client it works (it is just a little more of a pain when dealing with the raw mail files), but I am seeing some messages with corruption too.\n...\"pid\":4137,\"hostname\":\"trink-x230\"}\u001e\u021d]ZY\b\ufffd\ufffd\ufffdPR\u064dQ\ufffd\u0015\u0014\ufffd\ufffdZL\u0012\ufffd\u074c\ufffd\u058a\u049d\ufffdOH\ufffd\b\ufffd\u001a[Y\\\ufffd\u0018[\n. I've merged in dev and will apply the fix for the content if you don't mind.\nhttps://github.com/mozilla-services/heka/tree/smtp-output-subject\n. The token parser keeps the delimiter (for historical reasons and now decoders relying on that behaviour).  The default parser_type = \"regexp\" behaves as you desire.\n. LogfileInput has been removed,\nLogstreamerInput should be able to be paired with a decoder plugin to accomplish this.\n. See https://github.com/mozilla-services/heka/issues/355\n. As Rob stated will keep the TCPOutput and redesign it for reliable delivery.\n. I would rather people just fix their data :) but r+\n. Dupe #598 \n. Duplicate #372 \n. Duplicate of #407 - (there is a partial solution in place)\n. Backout https://github.com/mozilla-services/heka/pull/624 when done.\n. Not really loving the hard coded path default.  This would actually break every config I have (most have no path set and expect everything to be under the base_dir).  If I want a shared set across multiple base directories I just symlink them in (that doesn't work with dasher so I have to use a fully qualified path which has always been awkward)\nFor filters and decoder the path relative to the base_dir is often used too \"lua_filters/common.lua\".   If they were hard code the config files become more brittle.  #628 doesn't fully solve the problem unless we scan both the base and the shared directories looking for a match in the relative path case or have a special token \"<share>/lua_filters/common.lua\"\n. +1 on the real proposal\nFor the quick fix, I would just adjust the config file to use fully qualified paths on the shared decoders/filters and explicitly set the module_directory for each sandbox plugin (tedious yes, but better than my install kludge or hard coding a fully qualified path in the code).\n. Just wanted to let you know that I'm hoping to get this merged soon, so I'm going to make a branch of this to pick up where you left off. Thanks for the contribution!\n. See #687 \n. As Rob pointed out in IRC \"for tcp you could also use tls w/ client certs\".  This would definitely be better than signing each message to prove it is from a known source.  Traditionally signed message are used for Heka control messages (i.e. remotely starting and stoping sandbox filters from anywhere).   Thinking about it now I am not sure what use case having TcpOutput sign every message would serve when we now have the client cert option for accepting data from untrusted networks.  Plus, routing gets a little weird with signing since each filter can only subscribe to a single signer (it really only make sense for control messages).\n. Need to change docs/installing.rst too.\n. You didn't have to update the cmake example usage (but that is ok). \n\"Go 1.2 or greater (1.2 recommended)\" is a little strange I think \"Go 1.2 or greater\" will suffice.\n. #499 #501\n. I don't think this will work for default syslog-ng or rsyslog output since they aren't rfc 5424 compliant.  It would need to explicitly support the hardwired templates in #432 (option 2) or more generally support any template (option 1).  I realize that this would require complete redesign. If it is something you are interested in we can work on option 1 together. If you are not really interested or don't have the time feel free to decline. it will be a decent amount of work but it is a very cool approach.\n. See #671\n. Previously the preservation file was always deleted on plugin unload but this behavior changed when the preservation files were moved.  This is definitely the safest thing to do, should we restore that behavior instead?\n. Restored the intended behavior of not preserving state between unload/load.\n. The decoder.rst documentation should be updated (the preserve documentation can be grabbed from filter.rst)\n. The actual preservation validation was moved to lua_sandbox I got sick of managing it in two places.  I probably should have left the check to at least verify the file was written.\n. Ha, never mind, just merge if with dev and we will re-review\n. Merged with dev and opened PR #688 \n. Everything should really be considered debug if severity in not explicitly specified.  That way In production you can just blindly drop the debug logs.  If someone wants their data visible in production they would have to opt-in by setting the severity to something higher.\n. Closed with #773 \n. See #638 \n. Closed with #687\n. Ideally we would have load, unload, and replace where the sandbox in swapped out with no data loss. The sandbox can easily do it, but our Heka architecture makes it difficult.  The swap could also back pressure the entire system so it would raise some security/stability concerns.\n. We actually need to address this for static sandboxes also.  I propose adding a global data version variable.  The restoration would check it and if the versions match the data is restored, if not the restoration is aborted.  It will make plugin rollouts with schema changes much less painful.  I was considering a user provided data migration call if the versions mis-matched but realistically it probably isn't worth the effort for users to write.  This doesn't address the dynamic 'replace' scenario so data loss in this case is likely.\n. Dupe #625\n. #717 should address the naming issue, which should correct this bug too, closing as a dupe.\n. The decoder name would have to be re-creatable on startup so the preservation file could be reloaded, something like 'InputPluginName_DecoderName\" and for TcpInput 'InputPluginName_SourceIp_DecoderName\".  (and that is why decoders weren't allowed to have state in the past ;))\n. #721 \n. The persona code was extended and generally useful so it was added to the common_log_format module.\n. The Heka statsd implementation in general has to be rethought.  As per #686 (StatsdInput should really be treated as a decoder) and StatsAccumInput is actually doing the work of a filter, not an input.  Yeah we may take a performance hit putting it [stat message] into a Heka message first but I would be nice to see the actual numbers before extending this one-off design.\n. [IRC chat summary]\nI questioned why we need a separate message format that bypasses the router.\nAnswer 1: So we wouldn't exhaust the Heka message pool when a single message is turned into many.\nAnswer 2: It should perform better (but we don't have any measurements).\nDoes the message pool buy us anything (#362). If so then answer 1 is fine, if not is there a large performance impact standardizing on a Heka message and always using the router?  I would prefer to standardize on a single internal data representation so we don't bloat the sandbox API e.g. inject_message, inject_stat, inject_widget etc...  If we cannot standardize on a single internal data representation maybe we have the wrong representation.\n. Deferring until after 0.6 since it is not blocking and we have some design changes to test out first.\n. Rob and I were discussing that too.  However, I don't think we want the helper to take over the Plugin's Run() responsibility. \nLooking at the current usage the input side has 2 operations QueueRecord and RollQueue.  The output has Go routine and some reporting data.  I think something like the following could suffice.\n``` go\ntype BufferedOutput struct {\n    sentMessageCount    int64\n    readOffset          int64\n    parser              MessageProtoParser\n    writeFile           os.File\n    writeId             uint\n    readFile            os.File\n    readId              uint\n    checkpointFilename  string\n    checkpointFile      os.File\n    queue               string\n    name                string\n}\ntype BufferedOutputSender interface {\n    SendRecord(record []byte) (err error)\n}\nfunc NewBufferedOutput(queue_dir, queue_name string) *BufferedOutput \nfunc (b BufferedOutput) Output(sender BufferedOutputSender, outputError, outputExit chan error, stopChan chan bool)\nfunc (b BufferedOutput) QueueRecord((record []byte) (err error)\nfunc (b BufferedOutput) RollQueue() error \nfunc (b BufferedOutput) ReportMsg(msg *message.Message) error\n```\n. Yeah that will allow us to move the protobuf encoding into QueueRecord and eliminate some duplicate code.\nAs for changing the parser we don't want to go there it should always queue messages in a protobuf stream format (allowing heka-cat to work with any queue).  Once a message is de-queued we are introducing an encoder to allow message transformation into whatever form required by the final output.\n. Looks good will you update the CHANGES.txt adding a note to the Features section.  Thanks.\n. 014/04/18 01:29:19 Plugin 'FxaSandboxManager-AlertTest' error: exceeded InjectMessage count\npanic: runtime error: close of closed channel [recovered]\n    panic: runtime error: close of closed channel\ngoroutine 21 [running]:\nruntime.panic(0x8484a0, 0xffb875)\n    /usr/local/go/src/pkg/runtime/panic.c:266 +0xb6\nb.com/mozilla-services/heka/cmd/hekad/main.go:162 +0xa6f\ngoroutine 3 [syscall]:\nos/signal.loop()\n    /usr/local/go/src/pkg/os/signal/signal_unix.go:21 +0x1e\neb0, 0x5f490b)\n    /usr/local/go/src/pkg/net/tcpsock_posix.go:233 +0x47\nnet.(_TCPListener).Accept(0xc210000050, 0x7f6807ea9cf0, 0xc211accbf0, 0xc211adb400, 0x0)\n    /usr/local/go/src/pkg/net/tcpsock_posix.go:243 +0x27\nnet/http.(_Server).Serve(0xc2100b8af0, 0x7f6807e98938, 0xc210000050, 0x0, 0x0)\n    /usr/local/go/src/pkg/net/http/server.go:1622 +0x91\nnet/http.(_Server).ListenAndServe(0xc2100b8af0, 0xc2100b8af0, 0x10134a8)\n    /usr/local/go/src/pkg/net/http/server.go:1612 +0xa0\ncreated by github.com/mozilla-services/heka/plugins/dasher.(_DashboardOutput).Init\n    /home/ec2-user/heka/build/heka/src/github.com/mozilla-services/heka/plugins/dasher/dashboard_output.go:143 +0x723\n. Opened #750 to address a related but different issue.\n. 712bcf3506c3e6e7b6d836079ce168519600c4da\n. The Apache version will be a bit more work, but we will get a strftime format grammar generator too.\n. strftime - https://github.com/mozilla-services/lua_sandbox/pull/9\nApache - https://github.com/mozilla-services/lua_sandbox/pull/10\n. #757 \n. Tests using the ipv6 grammar (or even the host grammar) outside of the clf module do not exhibit this behavior.\n. Working as designed.\n. Leaving open until it is merged back into dev\n. Please update the tests and CHANGES.txt like in #595.\nThanks.\nTrink\n. The pid is not part of the standard but it is common practice to output it in the message. Technically the [xxx]: should be considered part of the message content but we are capturing it in the tag (this should be fixed in the syslog grammar). I agree pulling it out into the pid would be more useful and that change will be made in rsyslog.lua.\nIn rfc 5424: \"The TAG has been split into APP-NAME, PROCID, and MSGID.  This does not  totally resemble the usage of TAG, but provides the same  functionality for most of the cases.\" \nin rfc 3164: \"The TAG is a string of ABNF alphanumeric characters that MUST NOT exceed 32 characters.  Any  non-alphanumeric character will terminate the TAG field and will be assumed to be the starting character of the CONTENT field.\n. PR #772 \n. app-name is defined as per rfc 5424\n[\"app-name\"]            = nilvalue + printusascii^-48,\nWhat you get with %syslogtag% is programname otherwise we are back to the original output \"name[222]:\" since app-name matches the entire tag string.\n. Correct (even though their docs state otherwise).  In practice having programname contain anything but a SP, ':', '[' (I would be surprised if they enforce the length limitation too) would work better.\nhttp://rsyslog-5-8-6-doc.neocities.org/property_replacer.html\n\nsyslogtag     TAG from the message\nprogramname   the \"static\" part of the tag, as defined by BSD syslogd. For example, when TAG is \"named[12345]\", programname is \"named\".\n\nhttp://www.ietf.org/rfc/rfc3164.txt (BSD syslogd)\n\nThe TAG is a string of   ABNF alphanumeric characters that MUST NOT exceed 32 characters.\n. Yeah, length is not limited\nhttps://github.com/rsyslog/rsyslog/blob/3d4496c583a4a7b16da3d2c4fc2f7c55ecde45e1/runtime/msg.c#L1944\n\nAnd we would have to include a '/'  so printable excluding SP, ':', '[', '/'\nhttps://github.com/rsyslog/rsyslog/blob/3d4496c583a4a7b16da3d2c4fc2f7c55ecde45e1/runtime/msg.c#L1347\n. dev now pulls in these updates\n. This is typically done with filters in conjunction with dasboard output that way the output is available over the network (usually for some kind of mashup/visualization) and obviously locally too.  I would be curious what checks you are running against the local Heka output the cannot be performed within Heka itself.\n. We haven't had a need. Our standard use case is to write syslog data to disk and process it from /var/log using Heka's file input and our remote cases are handled by via the network protocols. Something like logger can send its messages directly to Heka so we have had no need for domain socket support. Feel free to create and issue or submit a pull request, thanks. \nTrink \n----- Original Message -----\n\nFrom: \"Michael Merickel\" notifications@github.com\nTo: \"mozilla-services/heka\" heka@noreply.github.com\nSent: Sunday, April 13, 2014 10:52:36 PM\nSubject: [heka] Add DGRAM support for unix sockets to create /dev/log (#790)\n/dev/log is the socket used by syslog and subsequently the logger command. It\nuses a DGRAM-configured unix socket, rather than the currently supported\nstreaming socket.. Can I ask why this isn't already supported by heka? As a\nlogging library I would've thought that syslog support would be one of the\nfirst features added as it is used for auth/sudo/postfix and many other\nsystem services and the lack of the feature does make me question whether\nI'm thinking of heka as a proper replacement for rsyslog and other logging\nsolutions.\n\u2014\nReply to this email directly or view it on GitHub .\n. What is preventing you from forwarding the local syslogd messages to a remote Heka (or from having the client(s) log directly to the remote Heka)? You would need a config like the one below on the Heka receive side and you wouldn't need any Heka instance on the sending side. \n\n[hekad] \nmaxprocs = 4 \n[Dashboard] \ntype = \"DashboardOutput\" \naddress = \":4352\" \nticker_interval = 10 \nworking_directory = \"dashboard\" \nstatic_directory = \"/work/git/heka/dasher\" \n[RsyslogUdpInput]\ntype = \"UdpInput\"\nparser_type = \"regexp\"\ndelimiter = \"$\"\naddress = \":5559\"\ndecoder = \"RsyslogDecoder\"\n[RsyslogTcpInput] \ntype = \"TcpInput\" \nparser_type = \"token\" \naddress = \":5560\" \ndecoder = \"RsyslogDecoder\" \n[RsyslogDecoder] \ntype = \"SandboxDecoder\" \nscript_type = \"lua\" \nfilename = \"lua_decoders/rsyslog.lua\" \n[RsyslogDecoder.config] \ntemplate = '<%PRI%>%TIMESTAMP% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%' \ntz = \"America/Los_Angeles\" \n----- Original Message -----\n\nFrom: \"Michael Merickel\" notifications@github.com\nTo: \"mozilla-services/heka\" heka@noreply.github.com\nCc: \"Mike Trinkala\" trink@mozilla.com\nSent: Monday, April 14, 2014 11:01:48 AM\nSubject: Re: [heka] Add DGRAM support for unix sockets to create /dev/log\n(#790)\nI'm attempting to avoid writing certain log files to disk and thus shipping\nthem through syslog and directly out over the network is something I need.\nUnfortunately I'm not a go developer yet so hopefully someone else tackles\nthis but I may find the time. Thanks for the response.\n\u2014\nReply to this email directly or view it on GitHub .\n. Part D is not a generic summary of A and B; it totally removes the syslogd component making Heka the only component in the logging stack. \n. The zero config TLS was not the issue.  We just needed some additional AWS SMTP credential setup and address verification.\n. This example has already been corrected in the dev branch. https://github.com/mozilla-services/heka/commit/4d434d073fe238bc56c76928144df42cbdbc55df\n. It is meant to represent your sandbox script.  There are several example scripts but I don't think we want to actually include them in the package.  https://github.com/mozilla-services/heka/blob/dev/sandbox/lua/testsupport/counter.lua is the closest to CounterFilter but it is not something you would run in production.\n. Correct, or place your script in the lua_filter directory.  \n\nI am leaving this open, we should replace it with a more useful example file (like processing an Nginx log file and aggregating the HTTP status codes).\n. @whd  This is only happening in production.  What are the permissions on the directory.  The FileOutput is creating a \".hekad.perm_check\" file to test write permissions.  The deletion of that file is failing causing the outputter to throw a fatal error.\n. ReportMsg runs after Run() has exited and the sandbox pointer has already been nil'ed\n. Endpoints to test/evaluate for the initial release:\nElasticSearch (replacing the current Output plugin)\nInfluxDB\nGraphite (Event Messages)\nGeckoboard (Dashboard pushes)\n. @thedrow  I would agree if there was something truly unique about about a specific HTTP interface but ES is pretty basic.\n. AmqpInput is not stopping/exiting on shutdown and circus is killing Heka.\n. @whd just saw a hang on shutdown with the AmqpOutput plugin too\n. strftime_specifiers[\"m\"]  has already been fixed\nhttps://github.com/mozilla-services/lua_sandbox/pull/23/files#diff-fc47f530db990d1b9de7124dca26c153L164\nbut I missed the strftime_specifiers[\"F\"] - fixing now\n. if you need to access it now you will have to update externals.cmake with 3a274d6f7dd7edfc8345b7e90286f78d17025f8f.  We will update heka/dev with the correct tag once the pending sandbox changes are merged in.\n. dev now pulls in the update\n. As per @rafrombrc \n11:07 @RaFromBRC we didn't deprecate it, and, while i strongly support moving folks towards LPEG as much as possible, it's still gonna see use\n11:08 @RaFromBRC yeah, but it's the only one that people actually use... and when someone is trying to get started, and we're not around in IRC to help them out, it's the difference btn them getting somewhere and them not getting somewhere\n11:11 @RaFromBRC and if anyone is using it and they complain about performance we'll tell them to stop using it, and we'll help them w/ LPEG\n11:15 @RaFromBRC we're not keeping it for the performance, we're keeping it to reduce the getting started friction for people\n11:15 @RaFromBRC there are orders of magnitude more folks who can get started w/o support from us using regex vs. being forced to use lpeg\n11:16 @RaFromBRC even if lpeg is a better choice, we won't get to show people that it is if they bounce off before they even get started\n11:19 < trink> RaFromBRC: I get it and if it does help them have a greater appreciation for LPeg later... even better\n. This issue has been resolved use the dev branch or the v0.5.2 tag.\n. Also, it is unlikely we would want to include zlib compression in the generic sandbox so it is something you would have to fork.\n. Guess what the telemetry team needs/requested...\n@michaelgibson did you do any performance testing\n. The new sandbox (in 0.9) allows the use of shared libraries; for gzip I can now just drop this .so https://github.com/vincasmiliunas/lua-gzip in the lua_modules directory.\n. If the Heka message grows beyond the maximum serialization size it will not be output. This is expected behavior.  To reproduce it just stuff >65KiB into the payload field and try to send it.  Looks like we need to correct the error message (flip the variables) so I am leaving it open.\n. 5ad5501ce277ad1afafe921e31f6876f3e800775\n. dupe, this bug was fixed a long time ago but 0.5.x doesn't pull in the latest tag. Use the dev branch or update the tag in the cmake/externals.cmake file.\nhttps://github.com/mozilla-services/lua_sandbox/pull/15\n. Not sure I am liking the names: ESCleanJsonEncoder and LogstashV0Encoder. 'Clean' doesn't have any useful meaning here and Logstash doesn't convey that this is actually an Elastic search encoder.\nESJsonEncoder\nESLogstashV0Encoder\n. Yeah, %s is an extension and not part of the standard.  We should look at all the extensions and evaluate if they can safely be added.\n. Clarification: Updating the grammar is fine and the tests will confirm os.date can handle or no-op them (if it cannot it would be an existing bug anyway)\n. Took a quick look at the ES docs.  This would be a one time setup and not something that has to be evaluated per message.  Also any single message may not contain the entire schema (so you would have to do something like the heka_message_schema debug filter).  I am not seeing this as a good candidate for including in the ESOutput plugin.  However, creating something like heka_message_schema with an ES mapping output could be useful to boot strap new setups.\n. All json encoding in now done explicitly by the user with cjson in 0.6\n. +1 that is a better solution than silently ignoring userdata objects on inject_message... and better than having to iterate the entire structure to nil them via https://github.com/mozilla-services/lua_sandbox/blob/dev/modules/util.lua#L14.\n. It can now be accessed by reading the raw message, \nlua\nlocal msg = decode_message(read_message(\"raw\"))\nif msg.Fields then\n    for i, v in ipairs(msg.Fields) do\n    -- process fields\n    end\nend\n. Thanks for the contribution and hanging in there :)\n. We should just implement https://github.com/mozilla-services/heka/issues/750\n. d41d8cd98f00b204e9800998ecf8427e is the md5sum of an empty file; your download is failing.\n. It is the rsyslog string template format.  http://rsyslog-5-8-6-doc.neocities.org/rsyslog_conf_templates.html\nI will add the link to the docs.\n. I agree this is not ideal but it is an improvement.  \nThe only problem I see is with CASC_FIRST_WINS.  We always force a timing sample on the first message (to have a ballpark number) but not all filters run on the first message and it could be a very long time before we ever get a timing sample.\n. Encoders output a byte array as the result of the message transformation (i.e., the result is not written back to a Heka message)\nAs for any of the string types in the Heka message they were chosen more as a matter of semantics.  A string field should contain a utf-8 string and a byte array, binary data.  However in Go this creates certain issues and we are considering converting all strings to byte arrays for better memory reuse and less overhead in the CGO data marshalling but we haven't run the numbers to see how much it actually buys us yet.\n. What version? The output that was provided does not match any recent release of the log.cc code.\n. The unit tests have to be corrected and any bugs fixed before we will release a Windows package.\n. Forgot to make a note on the functions: writeRawField is only used to write out the two integer fields (should probably be renamed and take an integer argument) and strangely envversion (which should really be using writeStringField)\n. @rafrombrc\nThis is a step in the right direction, there is still a bunch of refactoring that could happen but I certainly don't want to dump it on ianneub i.e.,\n- factor out the raw bytes check\n- better protection against field array access (it is possible to create a message that would panic the encoder (true with the existing version too))\n- remaining items in #865\n. Not a problem, I am modifying the change log for another PR now so  will correct it.\n. Add support for the majority of formats:\nGIT\n\nssh://[user@]host.xz[:port]/path/to/repo.git/\ngit://host.xz[:port]/path/to/repo.git/\nhttp[s]://host.xz[:port]/path/to/repo.git/\nftp[s]://host.xz[:port]/path/to/repo.git/\nrsync://host.xz/path/to/repo.git/\n\nAn alternative scp-like syntax may also be used with the ssh protocol:\n\n[user@]host.xz:path/to/repo.git/\n\nFor local repositories, also supported by git natively, the following syntaxes may be used:\n\n/path/to/repo.git/\nfile:///path/to/repo.git/\n\nThe ssh and git protocols ~username expansion in NOT supported:\n\nssh://[user@]host.xz[:port]/~[user]/path/to/repo.git/\ngit://host.xz[:port]/~[user]/path/to/repo.git/\n[user@]host.xz:/~[user]/path/to/repo.git/\n\nHG\n\nlocal/filesystem/path[#revision]\nfile://local/filesystem/path[#revision]\nhttp[s]://[user[:pass]@]host[:port]/[path][#revision]\nssh://[user@]host[:port]/[path][#revision]\n\nSVN\n\nhttp[s]://[user[:pass]@]host[:port]/[path]\nsvn://host/repo\nfile://local/filesystem/path[#revision]\n. I haven't seen anyone use it for text output yet and am betting it will be the exception.  I know Ops uses it exclusively with Protobuf output in over 20 places. Also with heka-cat a protobuf file stream is very convenient to work with.\n. So don't default the common case and make the one off noob case the standard.  Now that is a bad idea.  In any case whd has already created a PR with the use_framing config change so we won't hose production.\n. Since we don't call or.Encode we never really verify that framing is indeed added (in the file output test, the tcp output test uses the buffered output which has a framing test).  We should just add a similar framing test to the plugin runner.  Other than that it looks fine.\n. As-is the HttpOutput design seems too rigid. Fixed URL, headers, username/password, limited supported methods, and a GET request that is unable to utilize any message data.  To make those dynamic in a Go plugin would require an awkward configuration and tight coupling with the encoder. I propose moving most of the HttpOutput's functionality to an Encoder.  The byte array returned from the encoder would be an HTTP request in wire format (this would address all of the rigidity in the current output and simplify its configuration). The output would convert the byte array back to a request object, establish a connection, and deliver the request.\n\nWe can create a Go HttpEncoder that handles the basic cases with a few knobs (i.e., what HttpOutput does now). I would like to see measurements of the batching benefits v.s. say persistent connections and if batching proves beneficial create a better solution where we won't lose data on shutdown when an endpoint is unreachable.\nAs for a SandboxEncoder, this approach will allow for maximum flexibility in controlling the output format. It also automatically addresses data persistence (if batching in the encoder).  If batching proves to be beneficial we may want to increase the size of the output buffer (encoders only).  It would also be useful to add some HTTP utility functions to the sandbox e.g., base64 encoding. \n. The Windows side should be kept in sync.\n. You may still only want to setup the environment and not build also mingw32-make supports -j\n. Merging so it can be updated it in the malicious data protection PR.\n. ### Structured Data\nThese tests were only run in the confines of the Lua sandbox and not tested in the Heka pipeline.  I also created an LPeg version of the tnetstrings parser for comparison with a native Lua parser (https://github.com/jsimmons/tnetstrings.lua).  The execution times of the Lua implementations are basically identical with the main difference being the lines of parsing code: 40 (LPeg) vs 134 (Lua).\nTested Structure\nJSON\n[2,\"test\",\"it\",99,{\"y\":2,\"x\":1,\"subt\":{\"y\":4,\"x\":3}},[1,2,3,\"abc\",{\"obj\":99},true]]\ntnetstrings\n1:2#4:test,2:it,2:99#43:1:x,1:1#1:y,1:2#4:subt,16:1:x,1:3#1:y,1:4#}}40:1:1#1:2#1:3#3:abc,11:3:obj,2:99#}4:true!]\n| Parser | time (\u00b5s) | mem KiB | Lua Instructions |\n| --- | --- | --- | --- |\n| cjson | 8 | 50 | 18 |\n| tnetstrings (non LPeg) | 49 | 64 | 62 |\n| tnetstrings (LPeg) | 49 | 76 | 60 |\nSummary\nAt 6x faster, less memory utilization, and better human readability: stick with JSON.\nDelimited Data\nThese tests were performed by converting an FxA Auth Server log (1459605 lines) to a delimited format and comparing the processing time through the Heka pipeline (input->decoder).\nSample Log Line\nJSON\n{\"name\":\"fxa-auth-server\",\"hostname\":\"ip-172-31-26-89\",\"pid\":2495,\"level\":30,\"op\":\"request.summary\",\"code\":200,\"errno\":0,\"rid\":\"1398723492702-2495-14315\",\"path\":\"/v1/recovery_email/status\",\"lang\":\"en-US,en;q=0.5\",\"agent\":\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:29.0) Gecko/20100101 Firefox/29.0\",\"remoteAddressChain\":[\"71.165.123.102\",\"172.31.14.51\",\"127.0.0.1\"],\"t\":2,\"uid\":\"2fd4cd12345678a484e3f808ff2ccce6\",\"msg\":\"\",\"time\":\"2014-04-28T22:18:12.704Z\",\"v\":0}\nDelimited\n2014-04-28T22:18:12.704Z|FxaAuth|request.summary|0|ip-172-31-26-89|71.165.123.102,172.31.14.51,127.0.0.1|/v1/recovery_email/status|200||Windows 7|30|0|2|2fd4cd12345678a484e3f808ff2ccce6|fxa-auth-server|Firefox|1398723492702-2495-14315|29\nSummary\nThe delimited file size is almost cut in half and it is processed 1.46x faster than the JSON file.  However, the format is more rigid and error prone and the gains aren't really great enough to justify switching at the moment.\nOptimizations to our existing data\nA big performance hit in processing our textual data (JSON or not) is the date/time parsing and conversion.  Switching from the time variable from\"time\":\"2014-04-28T22:18:12.704Z\"' to '\"time\":1.398723492704e+18 in the FxA logs increase processing speed by 1.18x.\n. Is the intent to allow Heka to start before the directory or symlink exists? If so the symlink change needs to be made elsewhere otherwise we just need a cleaner error message on exit.  As for the panic an empty log_directory in the pre PR code will cause it too.\n. If we want to handle it in the Lua sandbox we should pull in: http://www.inf.puc-rio.br/~roberto/struct/\n. The struct library was added to the sandbox here: https://github.com/mozilla-services/lua_sandbox/pull/52\nAnd pulled into Heka here: f0f1e449510ca902e676a52083c0f28c6beb66da\nClosing this issue since we don't have any plans to create a NetFlow decoder but the struct library should make processing any binary data much easier.\n. There should be unit tests for the decoders too.  \nI have been pretty lax about sandbox filter tests (it mostly comes down to comparing cbuf and many times variable cbuf output) and it has just been easier to manually verify and visually inspect the graphs when changes are made but that will eventually bite me.\n. We probably want to make sure this gets built by Travis too.\n. What version of mingw?\n. I am using:\n- gcc-core (gcc-4.7.1-tdm64-1-core)\n- binutils (binutils-2.22.90-tdm64-1)\n- mingw64-runtime (mingw64-runtime-tdm64-gcc47-svn5385)\nThe subscript 'i' is an int so it is unclear why you are seeing a compiler error.\n. Looks good but will you add a note to the CHANGES.txt file.\nThanks,\nTrink\n. Why was this closed?\n. Our use of Logger is pretty consistent but Type is ad-hoc and fixing it is a breaking change. i.e., all Types should have the \"heka.\" prefix removed.\nFor Logger == 'heka'\n- heka.plugin-report\n- heka.input-report\n- heka.inject-report\n- heka.router-report\n- heka.all-report\n- heka.memstat\nFor Logger != 'heka\"\n- heka.control.sandbox (Logger == 'heka-sbmgrload') - change type to \"sandbox.control\"\n- heka.counter-output (No Logger (should use plugin name)) - Change Type to \"counter.output\" and \"counter.summary\"\n- heka.statmetric (No Logger (should use plugin name))\n- heka.httpdata.request (Logger == plugin name)\n- heka.httpinput.data (Logger == url)\n- heka.httpinput.error (Logger == url)\n- heka.terminated (Logger == plugin name) - change type to \"plugin.terminated\"\n- heka.sandbox.{user specified type} (Logger == plugin name) - change Type to \"sandbox.type\"\n- heka.sandbox-output (Logger == plugin name) - change Type to \"sandbox.output\"\n- heka.sandbox-terminated (Logger == plugin name) - change Type to \"sandbox.terminated\"\n. I would also consider changing all the Heka reporting Types to: report.*\n. The standard for Logger has become the plugin name (the source of the data, with the main exception being the LogStreamer where the stream name is more applicable).\nAs far as the sandbox mapping the only ones I would agree with are terminated and control since they are the only messages that can/should be considered internal.  The Logger should remain as-is in the other cases, the types should lose the 'heka.' prefix and switch to a dot notation.\nThe purpose of Type is to reflect what is in the contents of a message and the count filter outputs two different formats so there are two unique types.  If we were consuming the message we would have to test the contents to determine the correct type. This would serve as a bad example of how to use Type and be one of the first things a new user sees.\n. ,,,\ngoroutine 74 [runnable]:\nruntime: unexpected return pc for runtime.cgocallbackg called from 0x0\nruntime.cgocallbackg()\n        /usr/local/go/src/pkg/runtime/cgocall.c:244 +0x4c fp=0x7f4b54498a48 sp=0x7f4b54498a38\ncreated by github.com/mozilla-services/heka/pipeline.(*foRunner).Start\n        /home/wes/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:467 +0x96\n. try:\nmingw32-make clean-heka\nmingw32-make\n. r+\n. Thanks for the PR, ideally the newline would only be added if it doesn't exist. There is no guarentee that blank records won't cause problems with ES in the future (or even now, since there are no tests that cover it).\n. Something like this should do the trick.\nlua\nlocal payload = read_message(\"Payload\")\nadd_to_payload(idx_json, \"\\n\", payload)\nif 10 ~= byte(payload, #payload) then -- define byte outside of process_message like this: local byte = string.byte\n   add_to_payload(\"\\n\")\nend\ninject_payload()\n. This is really a bug in the input that created the Payload.  The es_payload.lua encoder simply adds the bulk header information appends whatever is in the payload (no validation or checks) and passes it to the output.  If the payload was mal-formed in other ways e.g., missing a closing brace on the JSON it would also fail to load into ES (the newline is really no different, it is a format error). The data error should be fixed at its source.\n. make install\n. This specification came from the source not the rfc:\nhttps://github.com/rsyslog/rsyslog/blob/35ef2408dfec0e8abdebd33c74578f9bb3299f20/runtime/msg.c#L1347. But, it looks like that only applies when emulating/building the string. Any value stuffed in during message construction (by irqbalance in this case) is simply used.  As far as removing the slash it is safe (i.e. it won't break the syslogtag grammar).\n. https://github.com/mozilla-services/heka/issues/978\n. https://github.com/mozilla-services/heka/pull/1388\n. Would recommend adding the source port to the decoder name but not using it in the preservation filename.\n. Thanks, here is some feedback on the decoder.\n- The json.decode should be protected by a pcall, same for inject_message. \n- There is no reason to re-encode the json table again, just read the payload into a variable and assign it back to msg.Payload.\n- The Timestamp value is invalid (so the current time is always used), Timestamp should be the number of nano seconds since the Unix epoch.  Also you can just use the date_time module for parsing/conversion https://github.com/mozilla-services/lua_sandbox/blob/dev/modules/date_time.lua#L218 (strftime grammar with a time_to_ns call after a successful parse would probably be the easiest).\n- The message Hostname only needs to be set once in the initialization list.\n- The fields table should be initialized once with the config constants (msg_facet, msg_server).  The other fields should be initialized with nil to avoid the table resizing when processing the first message.\n. @lyrixx yeah just visual inspection of the code\n. I would be a -1 on this.\n1) It is of limited usefulness since what you can do with the template is pretty rigid and any extensions require a recompile.\n2) The entire message has to be copied to the template map every time\n3) Metadata is not accessible\n4) Multiple fields with the same name are not supported\n. Not going to build this out as a feature, at best it would be a one off debugging aid.  At least it is captured in the repo now.\n. @ecnahc515 I wouldn't split it into two filters at most I would spilt it into two separate graphs in this filter but that just complicates the code and I am not sure what issue it addresses?  Download size, graph scale?\n. https://github.com/mozilla-services/heka/issues/972\n. https://github.com/mozilla-services/lua_sandbox/commit/efa4a21a8ef8b4c35a4ad9a025588f339986730a\n. https://github.com/mozilla-services/heka/issues/972\n. The point here was to share some of the debugging code we use (very rarely now).  Adding it as a feature is unlikely since there are no plans to create a similar tool for MWW at the moment (so it is just a one off). The dashboard modifications, the propogation of additional metadata with the cbuf are all out of scope.  If it doesn't add enough value as-is please close the PR and associated bug.\n. I would prefer truncate/truncated to cut.  \nIdeally this would all be handled by the stream parser as opposed to having to add this functionality individually to each input plugin.  That would be a lot easier when we turn the parser into its own plugin type.\n. You could use something like keep_truncated_messages\n. Old\n2: BenchmarkMatcherStartsWith    5000000           366 ns/op\n2: BenchmarkMatcherEndsWith  5000000           473 ns/op\nNew\n2: BenchmarkMatcherStartsWith   100000000           27.1 ns/op\n2: BenchmarkMatcherEndsWith 100000000           27.9 ns/op\n. Would have been nice if I got the bug number correct in the commit message ;)... sigh\n. This warning should be cleaned up too\nchecking consistency... /work/git/heka/build/heka/src/github.com/mozilla-services/heka/docs/source/config/inputs/docker_log.rst:: WARNING: document isn't included in any toctree\n. Remove TimestampLayout reference in the scribble decoder\nhttps://github.com/mozilla-services/heka/blob/dev/plugins/scribble_decoder.go#L24\n. Duplicate of #1110\n. ```\n[syslog_udp]\ntype = \"UdpInput\"\nnet = \"unixgram\"\naddress = \"/tmp/socksample\"\nparser_type = \"regexp\"\ndelimiter = \"$\"\n[RstEncoder]\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"RstEncoder\"\n```\n. Closed with the addition of the NullSplitter in 0.9\n. Should be solved with the latest commit. Please test, thanks.\n. If you mutate the message with inject_payload or write_message the UUID will remain intact but when you create a new message with inject_message you get a new UUID (this is intentional). I would recommend that you move your UUID to a message field and use that down stream. When we start relying on the UUID header for de-duplication we will probably reserve it for internal use (and prevent user modifications where we can enforce it). \nRelated/See also (regarding multiple inject_payloads): https://github.com/mozilla-services/heka/issues/1129. \nTrink \n----- Original Message -----\nFrom: \"Denis Shashkov\" notifications@github.com \nTo: \"mozilla-services/heka\" heka@noreply.github.com \nSent: Wednesday, November 5, 2014 7:17:29 AM \nSubject: [heka] Question: how to keep UUID on first inject_message() call (#1171) \nHello! \nI wrote a patch to Logstreamer to generate message UUID as hash from it's payload, rather as always random. It allows us to check if message goes through heka and gets into the the log storage. But I've found out that sandbox decoder always overwrites original message UUID. \nI remember that Rob Miller wrote a letter , in which he said that first call of inject_message() from lua sandbox just reuses all old message structure. \nBut there is a phrase in inject_message() description: UUID is automatically generated, anything provided by the user is ignored. . Indeed, this is what actually happens in function serialize_table_as_pb() . \nThen, that new UUID overwrites old from original message in function InjectMessage() \nWas it done for some special purpose or accidentally? \nI want to change this logic in function InjectMessage(). Won't it be a big mistake for me if write something like this: \n--- sandbox_decoder.go.orig 2014-11-05 16:04:32.000000000 +0600 +++ sandbox_decoder.tmp.go  2014-11-05 20:44:58.000000000 +0600 @@ -188,9 +188,15 @@ if original == nil {\n                original = new(message.Message)\n                copyMessageHeaders(original, s.pack.Message) // save off the header values since unmarshal will wipe them out -           } -           if nil != proto.Unmarshal([]byte(payload), s.pack.Message) { -               return 1 +               if nil != proto.Unmarshal([]byte(payload), s.pack.Message) { +                   return 1 +               } +               // keep original UUID +               s.pack.Message.SetUuid(original.GetUuid()) +           } else { +               if nil != proto.Unmarshal([]byte(payload), s.pack.Message) { +                   return 1 +               } }\n            if s.tz != time.UTC {\n                const layout = \"2006-01-02T15:04:05.999999999\" // remove the incorrect UTC tz info \n? \n\u2014 \nReply to this email directly or view it on GitHub . \n. Short answer:  It hasn't really been an issue and therefore not a priority.  If Heka dies a few messages could be replayed especially from the LogstreamerInput (but if it is not a protobuf stream you could duplicate messages and they would have unique UUIDs so you still wouldn't catch them).  In most cases a crash would result in some message loss.\n. I am not seeing the value of sourcegraph here.  Anyone else finding the link above really useful?\n. Yeah, we can leave the cmake_minimum_required at 2.8.7 for a while (and just clean up the warning for now)\n. kafka_input/output have os.Hostname calls too\n. Did you add it as a dependency in the cmake file?  I assume you are using the plugin_loader.cmake\n. Sorry, yeah the add should be for your input plugin. You will need to add it, clone its dependencies, and tie it to your input. The full change should be modeled after this\nhttps://github.com/mozilla-services/heka/blob/dev/cmake/externals.cmake#L170-L172\n. In the current dev branch you can set the output limit larger than 63KiB.\n. This would actually make things worse from a memory and cpu consumption standpoint and still not allow you to support larger messages.  In the current dev branch you can set the output_limit above the default and do what you want with no changes to the encoder.\nNote: The MAX_MESSAGE_SIZE would have to be increased if you want to use this larger output in a Heka protobuf stream which is not the case here; it is just a general warning to anyone exceeding the default output_limit.\n. No problem, glad it is working for you.\n. Close with https://github.com/mozilla-services/heka/pull/1205\n. Your are exceeding the output_limit.\nbtw the error messages have been improved in the dev branch, see:\nhttps://github.com/mozilla-services/heka/issues/1156\n. We use the LogstreamerInput heavily on Centos 6&7 without issue.  Will you provide additional details like your configuration file, Heka version, and Centos version?\n. To make this work correctly in all cases we would have to change the protocol by returning an application level ACK after an entire messages has been received and then advance the checkpoint.  Currently, when the TcpInput socket is closed the next send by TcpOutput will still be reported as successful and then the subsequent send will fail with a connection closed error (in effect skipping the message).\nSee #355 for more history. Since it seems we are stuck with it; we can document it as-is (a simple way to stream data to/from Heka with no delivery guarantees) or evolve it into a full blown reliable transport. \n. Agreed, it is just something to think about and since I ran the tests I wanted to capture it somewhere.  Changing payload to []byte and leaving everything else the same is another alternative (from a performance angle). \nOn the dual homed raw data front: Telemetry wants the message size bumped up to 32MB and it would be less than optimal to be carrying around two large buffers for each message.\n. FileOutput doesn't use ticker_interval.  The flush_interval defaults to a second. See: https://hekad.readthedocs.org/en/latest/config/outputs/index.html#fileoutput\n. Closed via https://github.com/mozilla-services/heka/pull/1222\n. This failure also causes Heka to hang on shutdown and it must be killed.\n. Closed via https://github.com/mozilla-services/heka/pull/1224\n. I don't think it will scale well to something like this: https://heka.shared.us-west-2.prod.mozaws.net/\n. It looks like the message you are outputting doesn't have anything set in the Payload header, you would have to extract the data from the message Fields (or change your Input/Decoder to populate the Payload).\n. The filters can run untrusted code and therefore should not be allowed to spoof other hosts, loggers, or message types.\n. However, we could lift the restriction on static sandbox filters only but I am not sure we want to fork the behavior in this way (i.e., message_matchers/downstream consumers would have to change when switching from a dynamic to static sandbox).\n. For binary data use inject_message (payload is assumed to be textual but we may want to change that assumption)\n``` lua\nrequire \"os\"\nrequire \"string\"\nrequire \"struct\"\nmsg = {}\nfunction process_message ()\n    msg.Fields = {pv = \"[ \" .. struct.pack('>L', 100) .. \" ]\"}\n    inject_message(msg)\n    return 0\nend\n```\n. closed by 0ad165b35a8d815122d2f2ab4dde27adb123c6f9\n. Actually it is any decoder that implements a WantsDecoderRunner interface but yeah some more API changes are needed.\n. Please include your log_format config and the log line that is failing to parse (BTW it is possible to create a log_format specification that is not parsable).\nThe grammar is constructed based on http://trac.nginx.org/nginx/browser/nginx/src/http/ngx_http_upstream.c#L4793.  The unit tests are here https://github.com/mozilla-services/lua_sandbox/blob/dev/src/test/lua/lpeg_clf.lua#L400\n. Thanks that is exactly what I needed. The problem is the hypen in the list of times, I will update the grammar to handle it.\n. closed by 0ad165b35a8d815122d2f2ab4dde27adb123c6f9\n. If Heka crashes like above (or is killed) the sandbox state will not be preserved since it runs on shutdown.  As far as the buffer getting zero'ed out; I would take a good look at your input data; if the timestamps are before the start of the buffer the additions will be ignored and if they are far ahead of the buffer it will advance dropping all the old data.  If the data look good please check the permissions on your sandbox_preservation directory and see if there is anything being written to it on a clean shutdown.\n. closed by 0ad165b35a8d815122d2f2ab4dde27adb123c6f9\n. I assume you mean NaN which would imply the cbuf was not initialized with any data.  I would need to see the code to provide further assistance.\n. I would have to guess the issue is data related and not code related. The next step would be for you to provide your configuration and some sample input data that reproduces the problem.\nCode review:\n- you probably want to protect against a nil name and value\n- if you are using data preservation cbufs should not be local\n- Resetting the data table is unnecessary\n  - I would actually move data outside the timer_event function and reuse the table in each iteration\n- if name is not bounded the plugin will be terminated after injecting 10 message (the max_timer_inject default)\n. Looks like there is still a bug with the LogstreamerInput checkpointing https://github.com/mozilla-services/heka/issues/740\n. These tests are working properly for me\n``` lua\n    local t = {0/0}\nassert(true == cjson.encode_invalid_numbers(\"on\"), tostring(cjson.encode_invalid_numbers(\"on\")))\nlocal ok, v = pcall(cjson.encode, t)\nassert(ok and v == \"[NaN]\", v)\n\nassert(true == cjson.encode_invalid_numbers(true), tostring(cjson.encode_invalid_numbers(true)))\nlocal ok, v = pcall(cjson.encode, t)\nassert(ok and v == \"[NaN]\", v)\n\nassert(false == cjson.encode_invalid_numbers(\"off\"), tostring(cjson.encode_invalid_numbers(\"off\")))\nlocal ok, v = pcall(cjson.encode, t)\nassert(not ok and v == \"Cannot serialise number: must not be NaN or Infinity\", v)\n\nassert(\"null\" == cjson.encode_invalid_numbers(\"null\"), cjson.encode_invalid_numbers(\"null\"))\nlocal ok, v= pcall(cjson.encode, t)\nassert(ok and v == \"[null]\", v)\n\n```\n. https://github.com/mozilla-services/heka/blob/cbuf_influx_encoder/sandbox/lua/encoders/cbuf_influx.lua#L93\nmath.nan does not exist so you are setting it to NIL (ending the array). Use 0/0\n. - ideally deleting a value that doesn't exist would be a no-op and not an error\n- CHANGES.txt should be updated\n. Not sure what the state of this is but Ben was already working on it.\nhttps://gist.github.com/bbangert/a75405fe9c805c566cf4\nTrink\nOn Sun, Feb 1, 2015 at 8:51 PM, Chance Zibolski notifications@github.com\nwrote:\n\nUse a filter to compute the deltas. Take a look here for an example:\nhttps://github.com/mozilla-services/heka/blob/dev/sandbox/lua/filters/diskstats.lua\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1301#issuecomment-72406207\n.\n. Fixed in Go 1.5 beta 2\n. The build rpath needs to be removed before packaging.\n. It is an error to have an RPATH in the libraries or executable in an RPM. So the RPATHs will be removed. What this means:\n1) Heka will only run when installed into the standard system directories\n2) Only one version of Heka can be installed on a system at a time\n3) There can only be one liblua.so (the lua_sandbox version) installed on the system.  Fortunately this will not conflict with the standard Lua RPM which installs liblua-5.1.so\n. The only fatal failures were the two dependency failures on the RPM installation.  These have been resolved and the RPATHs remain unchanged so the tarball can still be installed to whatever location is desired and function properly.  At this point in time we will not address the other rpmlint errors and warnings.\n. Closed via #1330 \n. #1317 \n. We will need to add unit tests and update the SplitterRunner documentation\n. The mailing list (https://mail.mozilla.org/listinfo/heka) is a better venue for asking support questions like this. The issue tracker is for reporting bugs and making feature requests. Please resend this inquiry to the mailing list. Thanks.\n. The latest dev branch includes the LPeg 're' module so passing in a string pattern is now possible.\n. Closed via #1332\nThe data loss is now logged and the system should shutdown cleanly.\n. Please add a note to the CHANGES.txt Bug Handling section.\n. Closed via #1327\n. Please update the CHANGES.txt\n. I have received a -1 on pulling it out and having a Heka package dependency on the sandbox.  So for now it will remain in.  The lua_sandbox package and the heka package will not be able to co-exist on a box.  We discussed statically linking the sandbox into Heka but it will not happen at this time.\n. Yes I still think pulling it out is the way to go @rafrombrc will need some more convincing.  As for HS working with the sandbox installed with Heka 0.10 the answer is not at the moment (the lua_sandbox SHA in Heka has to be updated).   BTW I was experimenting with an hs-cat it was about 6x faster on my box if that is all you want Heka for.\n. Thanks, removed.\n. The change in naming should also be noted in CHANGES.txt.  I will leave it for @rafrombrc to merge since I am unsure if he intentionally removed Type to begin with.\n. @rafrombrc we should audit the heka.* log message Types again; I know I threw in a heka.kafka default which is a probably a bad idea if they are suppose to represent local messages only.\n. Closed in https://github.com/mozilla-services/heka/issues/1251\n. I was surprised to see that read will fill a large buffer in one call (i.e. 50MB) but it appears to work.\n\nOne thing that I don't like about the design is that one always has to set buffer_size to the maximum size instead being able to grow it if needed (like the old implementation) but if @mreid-moz is happy with it then r+\n. My initial test was with Logstreamer input.  I retested with HttpListenInput for @mreid-moz and the body is split into multiple reads even on something as small as a 35KiB payload. So this will not work.\n. You can use read_message(\"raw\") and  the new decode_message (when merged https://github.com/mozilla-services/heka/issues/1344) to pull down the entire message modify it and re-inject it.  That PR https://github.com/mozilla-services/lua_sandbox/pull/72 also supports an array of fields.  The only remaining issue is that read_message(\"raw\") in most cases will return a stale message (since Heka does not re-encode a message after mutation https://github.com/mozilla-services/heka/issues/1342) but in your decoder case you should be in good shape since you are reading a protobuf input.\n. The landed PRs above address this issue.\n. In general combining circular buffers this way is a very bad idea (almost always leading to some data loss).  I would recommend updating these plugins to use the delta output, it is the reason it was introduced.\n. Removed my comments: \nSorry, I just jumped right into implementation review mode but taking a step back I think we should change the entire approach (as per https://github.com/mozilla-services/heka/pull/1365#issuecomment-76038929) which makes the other comments moot.\nSee also: https://github.com/mozilla-services/heka/pull/1076\n. The removed unit test was not added back in.\nThe use case for making delete a no-op on missing data is for our own PII scrubber in which a lot of the fields being removed are optional.  Having to pcall each one just to throw away the error is a pita.  I am just saying the final result of a delete on an existing field is the same as the result on a non-existent field (the field you don't want to be there isn't).\nAn option would be acceptable\n. For some reason I thought that already landed in dev, yes that fixes the issue.\n. That is an error in your checked out tree.  You need to fixup the submodule\nor just blow away (assuming you don't have any local changes/branches) and\nre-clone the project.\nTrink\nOn Mon, Mar 2, 2015 at 3:52 AM, syepes notifications@github.com wrote:\n\nHello,\nI am trying to build the last version (versions/0.9) of Heka on FreeBSD\nand I am getting the following error.\nNote that if a checkout the branch versions/0.8 it build correctly.\nFreeBSD 10.1-STABLE FreeBSD 10.1-STABLE r279215\n[/tmp/src/heka]# source  build.sh\n-- The C compiler identification is GNU 4.8.4\n-- Check for working C compiler: /usr/local/bin/gcc48\n-- Check for working C compiler: /usr/local/bin/gcc48 -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Found Go: /usr/local/bin/go (found suitable version \"1.4.2\", minimum required is \"1.3\")\n-- Found Git: /usr/local/bin/git (found version \"2.3.0\")\n-- GeoIP.h found. Enabling GeoIP plugin.\n-- Docker plugins enabled.\n-- sphinx-build was not found, the documentation will not be generated.\nCMake Warning (dev) at cmake/ExternalProject.cmake:202 (if):\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n  details.  Use the cmake_policy command to set the policy and suppress this\n  warning.\nQuoted keywords like \"COMMAND\" will no longer be interpreted as keywords\n  when the policy is set to NEW.  Since the policy is not set the OLD\n  behavior will be used.\nCall Stack (most recent call first):\n  cmake/ExternalProject.cmake:1524 (_ep_parse_arguments)\n  cmake/externals.cmake:19 (externalproject_add)\n  CMakeLists.txt:100 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n-- Found Hg: /usr/local/bin/hg (found version \"version 3.3\")\nfatal: reference is not a tree: 6c054de5f69fbb3b24396ed77e64b879a8b8cbd2\nUnable to checkout '6c054de5f69fbb3b24396ed77e64b879a8b8cbd2' in submodule path 'docs/source/_themes/mozilla'\nCMake Error at CMakeLists.txt:108 (message):\n  Failed to init Heka submodules\n-- Configuring incomplete, errors occurred!\nSee also \"/tmp/src/heka/build/CMakeFiles/CMakeOutput.log\".\nmake: no target to make.\nmake: stopped in /tmp/src/heka/build\nAny suggestions?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1382.\n. ElasticSearchOutput now uses buffered output which does enforce max_message_size.  You will have to update your configuration to allow for larger messages. https://hekad.readthedocs.org/en/latest/config/index.html#global-configuration-options.\n. The mailing list (https://mail.mozilla.org/listinfo/heka) is a better venue for asking questions like this. The issue tracker is for reporting bugs and making feature requests. Please resend this inquiry to the mailing list. Thanks.\n. It is automatically closed when it goes out of scope, so this is unneeded.\n. No, currently you have to return success or failure. \n\nHowever, in cases like de-duping you really don't want to count/log that as a decode failure just to drop the message and you probably don't want to pass it through to the router with a 'dupe' flag only to be ignored down stream.\n. This should really be a configuration option and not a constant since the maximum size is network/protocol dependent. With the practical maximum limit of 65507 being a good default for IPv4.\n. Thanks, r+\n. Have to make half that last change in 0.9 and the other in dev.  Closing and deleting this branch\n. The conditional check is still needed to ensure we don't restore old data when preservation is changed to false.  So it is fine as is.\n. should be against 0.9\n. https://github.com/mozilla-services/heka/issues/1329\nhttps://github.com/mozilla-services/lua_sandbox/issues/85\n. The mailing list (https://mail.mozilla.org/listinfo/heka) is a better venue for asking questions like this. The issue tracker is for reporting bugs and making feature requests. Please resend this inquiry to the mailing list. Thanks.\n. The cbuf_utlis module was rejected here: https://github.com/mozilla-services/heka/pull/1365#issuecomment-76038929\n. Please provide the full configuration and the directory listing for the configured module directory. \n. The typos were never fixed.\n. Interesting the PR in no way reflects that. \n+1\n. You most likely have a very large log line in your input file. You need to bump the size to accommodate the line or wrap the cjson.encode in a pcall, trap the error, and return -1 to non-fatally fail the processing of that message. \n. What is the largest individual string being encoded?  cjson attempts to allocate 6x its size to allow for escaping.\n. @cristi1979 You can get rid of over half that code if you use the grammar generator.\n. This is an intentional Heka design decision.  The sandbox configuration has always disabled the I/O module in everything but the Input/Output sandbox plugins.  All I/O from the other plugin types must use the sandbox API so it can be monitored.\n. I would rather see control over where the buffer lives.  i.e., If desired you you could just assign it to a RAM disk.\n. looks good r+\n. Sigh, as commented in the code. I cannot remove external access due to the global variable restoration in each and not using the environment was the less then stellar work around.  There are some ways it can be addressed in the sandbox itself but it is not worth addressing at the moment.\n. The modules and entries that are disabled in filters are done so for very specific reasons.  They should not be generically customizable.\n. The sandbox restrictions are carefully chosen for the stability of Heka and will remain predefined.\n. closed via #1506\n. I am not seeing it fail on the parse (please check your json.log), for me it is failing on the inject_message.  You cannot put arbitrary JSON into message.Fields.  It has to conform to https://hekad.readthedocs.org/en/latest/sandbox/index.html#lua-message-hash-based-field-structure.\n. @4r9h +1\n. https://github.com/mozilla-services/heka/pull/1550\n. See https://hekad.readthedocs.org/en/latest/config/outputs/kafka.html#kafka-output\nYou want to use topic_variable\n. If we want to go all-in I would just make it work like Hindsight with no path translation at all: https://github.com/trink/hindsight/blob/master/benchmarks/single.cfg#L6\nBut I was just thinking we would keep module_path and do something like this for Heka\n```\nmodule_path = \"/foo/bar;/foo/widget\" \nexpands to \"/foo/bar/?.lua;/foo/widget/?.lua and the corresponding expansion for the cpath\n```\nThis would be a non breaking change and not introduce any new configuration options.\n. The output doesn't match the format string it is missing the %u field.\nTrink\nOn Fri, May 22, 2015 at 12:58 PM, Christine Dodrill \nnotifications@github.com wrote:\n\nI am trying to get heka to parse an apache log with a nonstandard format:\n[apache]type = \"LogstreamerInput\"log_directory = \"/home/cdodrill/\"file_match = 'access.log'decoder = \"ApacheLogDecoder\"\n[ApacheLogDecoder]type = \"SandboxDecoder\"filename = \"lua_decoders/apache_access.lua\"\n[ApacheLogDecoder.config]type = \"common\"user_agent_transform = falselog_format = '%h %D %{wallclock_time}i %{cpu_user_time}i %{cpu_system_time}i %t %u \"%r\" %s %b \"%{Referer}i\" \"%{User-Agent}i\" \"%{X-Forwarded-For}i\"'\nA (stripped) log line looks like this:\n127.0.0.1 40312 49093 52003 0 [21/May/2015:16:47:54 -0700] \"POST /api/xmlrpc/client.php HTTP/1.0\" 200 466 \"-\" \"aXMLRPC\" \"8.3.56.98\"\nHeka will fail with:\n2015/05/21 17:20:09 Decoder 'apache-ApacheLogDecoder-1' error: Failed parsing:  payload: ...\nBut with no easy reason as to why, and my attempts to isolate and test the\ndependencies were unsuccessful.\nIs there something I am missing here?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1548.\n. @jotes If you are interested in taking a crack at it in Heka that is fine.\n. Not sure if you want to remove the old commented out code or make the samplesNeeded match the plugin_chansize configuration - 1 but  I am ok with getting this merged and having people put some bake time on it.\n. See: https://github.com/mozilla-services/heka/pull/1555#issuecomment-106945680\n. It is a problem whether you are building documentation or not.  This just masks the issue.\n. Yeah, but there is also no way to build the documentation from the tarball either (if you attempt to you will still fail with your issue)\n\n@rafrombrc do we want to address that or call this good enough?\n. The mailing list (https://mail.mozilla.org/listinfo/heka) is a better venue for asking questions like this. The issue tracker is for reporting bugs and making feature requests. Please direct future queries there, thanks.\nHere is the configuration I used/tested on my system.\n``` toml\n[hekad] # Heka 0.9\nmaxprocs = 4\n[RsyslogUdpInput] \ntype = \"UdpInput\" \nsplitter = \"NullSplitter\"\naddress = \":5559\" # adjust according to your syslog config\ndecoder = \"RsyslogDecoder\" \n[RsyslogDecoder] \ntype = \"SandboxDecoder\" \nscript_type = \"lua\" \nfilename = \"lua_decoders/rsyslog.lua\" \n[RsyslogDecoder.config] \n   # traditional_forward_format\n   template = '<%PRI%>%TIMESTAMP% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%' \n   tz = \"America/Los_Angeles\" \n[RstEncoder]\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"RstEncoder\"\n``\n. Duplicate of https://github.com/mozilla-services/heka/issues/1213\n. Also if you look at the actual config installed with a default apache the existing values are correct.\n. Both single and double quotes are allowed: \nhttps://hekad.readthedocs.org/en/v0.9.2/message_matcher.html#quoted-string\n. Do you want to switch the heka-mozsvc-plugins dependencies too?\n. No, I missed it... just merged it\n. https://github.com/mozilla-services/heka/pull/1555#issuecomment-106945680\n. see https://hekad.readthedocs.org/en/latest/config/inputs/logstreamer.html\noldest_duration (defaults to 720 hours)\n. The purpose of type in the nginx_access decoder is to differentiate between different format types.  The error log only has a single format so this is unnecessary. \n.Could NOT find Go: Found unsuitable version \"1.3.1\", but required is at   least \"1.4\" (found /usr/local/go/bin/go)`\n. @mhahn Sorry, I thought that was your build env and not something we setup.\n. It has been a long time since these have been passing on Windows, it is nice to see.\n```\nD:\\work\\heka\\build>ctest -R sand\nTest project D:/work/heka/build\n    Start 23: sandbox_move_modules\n1/3 Test #23: sandbox_move_modules .............   Passed    0.08 sec\n    Start 24: sandbox\n2/3 Test #24: sandbox ..........................   Passed    1.47 sec\n    Start 25: sandbox_plugins\n3/3 Test #25: sandbox_plugins ..................   Passed    3.00 sec\n100% tests passed, 0 tests failed out of 3\nTotal Test time (real) =   4.58 sec\n```\n. -1\nIn the normal non error use case having it move to build directory is desirable.  If you are seeing a lot of build script errors patches are welcome to correct those.\n. +1 after fixing up the conflict\n. Any reason you aren't using the native lib https://github.com/kengonakajima/lua-msgpack-native?  The performance would be much better.  Also, I would rather see an external dependency instead of having to maintain a copy and paste version of the much slower pure lua version of a msgpack API.\n. https://github.com/mozilla-services/lua_sandbox/issues/97\nNo it would just be a external dependency that the user would install since\nit is not sandbox specific.\nOn Thu, Sep 17, 2015 at 12:15 PM, Anton Lindstr\u00f6m notifications@github.com\nwrote:\n\n@trink https://github.com/trink Hmm, I don't think so. Maybe that's the\nbest option :-) Would that require a PR to mozilla-services/lua_sandbox?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1666#issuecomment-141193201\n.\n. It blows up on the instance_name field that was added because it is not the schema it expected (numeric only field values).\n. See: https://github.com/daurnimator/lua-systemd\n. I am not sure what your use case is or even why you would need/want to split it up between Go/Lua\n. The available options are:\n1. Truncate the message see: https://hekad.readthedocs.org/en/latest/config/splitters/index.html#common-splitter-parameters\n2. increase the max_message_size see:\nhttps://hekad.readthedocs.org/en/latest/config/index.html#global-configuration-options\n3. count the record exceeded error messages (you would have to parse the Heka log) and add them to the the total count) \n. It is not possible to use a message_matcher for a decoder.  In that case\nyou can create a sandbox decoder and do the log type identification/parsing\nthere for everything that comes out of your containers.\n\nTrink\nOn Tue, Aug 18, 2015 at 7:40 AM, Andr\u00e9 Medeiros notifications@github.com\nwrote:\n\nActually, you can do message matching with Fields, as described here\nhttp://hekad.readthedocs.org/en/v0.9.2/message_matcher.html#examples\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1680#issuecomment-132235136\n.\n. See also #1398\n. -1\nSandbox filters should never be restarted after failure.\n1) if a bad plugin is submitted it will constantly thrash (same for statically configured)\n2) it failed for a reason and the code should be corrected (and re-submitted or if it was statically configured, Heka restarted)\n. FYI: In production we monitor the termination messages and output an alert if a statically configured plugin fails (we generally ignore SandboxManager plugin failures since they are experimental code).\n. Is there a reason the code producing the error cannot be fixed?\n. make clean-heka and then make this will clean up the mocks\n. I cannot reproduce this on the dev branch using Go 1.4.1. Also the use of inject_payload is incorrect (it does not accept a table). I will fix the typo in the error message.\n\nbad argument #3 to 'inject_payload' (unsuported type)\n. Let's just fix the grammar to put it in Timestamp.\n. That grammar is specific to nginx. As for breaking compatibility it will now work as expected and it clearly hasn't been used much (so I am not that concerned)\nEdit: As for the other grammars they are not parsing directly into a Heka message schema (so this is already a one off)\n. We are talking about the nginx_error_grammar only: Pid is already correct and the Timestamp change we already discussed (that should be changed).\n. Time is optional so this is incorrect\n. I would require the grammar to be set and get rid of the else since read_next_field is deprecated.  (ideally the message model issues in the decoder would be addressed (from out of sync messages (mutation of conceptually immutable messages), to queuing and pool exhaustion)\n. As a future enhancement we may want to change the module library to allow addition of user defined prog_grammars and wildcard_grammars so custom apps and fallback grammars can be defined.\n. The warning appears to be bogus but it can be corrected.  Just out of curiosity what compiler/version are you using? \n. @tianchaijz will you try out the branch above, thanks\n. As the docs state it is the number of bytes so if you want 100KiB the setting should be\noutput_limit = 102400\n. Increase the output_limit configuration for the encoder.  The encoder is being terminated and you are hitting bug https://github.com/mozilla-services/heka/issues/1038\n. The grammar works correctly for me http://lpeg.trink.com/share/77943009734676465 what version of Heka are you using?\n. Your config/data works for me on 0.10.0 Beta 2\n2015/11/22 16:50:17 \n:Timestamp: 2015-11-22 23:48:16 +0000 UTC\n:Type: nginx.access\n:Hostname: trink-x230\n:Pid: 0\n:Uuid: f290b94e-1523-4bce-a12d-85495528a8d4\n:Logger: LogstreamerInput\n:Payload: \n:EnvVersion: \n:Severity: 7\n:Fields:\n    | name:\"upstream_response_time\" type:double value:0.001 representation:\"s\"\n    | name:\"remote_user\" type:string value:\"-\"\n    | name:\"http_x_forwarded_for\" type:string value:\"-\"\n    | name:\"http_referer\" type:string value:\"https://HOST/index.php\"\n    | name:\"body_bytes_sent\" type:double value:285 representation:\"B\"\n    | name:\"remote_addr\" type:string value:\"1.1.1.1\" representation:\"ipv4\"\n    | name:\"status\" type:double value:302\n    | name:\"http_user_agent\" type:string value:\"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0\"\n    | name:\"request\" type:string value:\"POST /includes/process.php?action=update HTTP/2.0\"\n. Did you build from source or use the prebuilt package?\n. I am not seeing that problem.  However that field can contains spaces so quotes are recommended)\n. +1 but there are a couple of issues\n1) for me mainly time (although there should be very few modifications necessary, they should be reviewed and tests added/migrated)\n2) there will be two copies (either in two repos or two branches of the lua_sandbox) unless Heka is updated to use the new sandbox.  What will most like happen is they will be copied to the master branch of the sandbox and supported maintained there and the Heka copies will be considered deprecated.\n. The PR against heka is actually fine for the moment it will be easier to migrate everything from one place.\n. It should be modularized now.  Here is an example: https://github.com/mozilla-services/lua_sandbox/blob/master/modules/ip_address.lua\n. Yes please squash them\n. @nickchappell Sorry for the delay, this request is in a state of limbo as it will need to be refactored and moved to the lua_sandbox in the very near future.  @rafrombrc thoughts? \n- merge now and migrate later?\n- turn it into a grammar module for the new sandbox and have it available when the integration with Heka is complete?\n- ??\n. The module has to be pulled out and placed in the modules directory and the decoder has to require it and implement the expected sandbox API.  The question above was for Rob to determine timeline/frame and where the work should be committed (I guess he is fine with Heka, so it should be fixed there).\n. #1920\n. Those functions are not called concurrently all sandbox access is sequential.\n. As @robison pointed out that grammar has not had support added for the ssl module so it falls back to a basic space delimited or quoted string parsing.  There are two options: alter your log_format configuration (as suggested above) or extend the grammar to include the ssl module variables.\nNote: it is possible to construct a log_format string with ambiguous output (i.e. not parse able) even with 'known' variables.\n. Heka is no longer supported see this thread https://github.com/mozilla-services/lua_sandbox/issues/209. No reason, outBytes are now passed in\n. It could get recycled before processChain receives it, so we need to increment it first\n. We should probably move away from a global stop and have the ability to stop and restart individual components i.e. add Stop() to the interface.\n. This logic can be moved back up into the plc case (it was pulled out to avoid duplication when reading from the two channels).  The pack and captures variable can go away too.\n. Nothing is actually done with the outputs array in filters, we should use it or remove it\n. yes, starting at the third byte in up to capacity of the underlying array (same deal as EncodeStreamHeader)\n. ha, yeah I was just go running them as I was testing. I will fix it.\n. Yeah, it is the file I use for testing.  It should be removed but we also need a better solution then pastebin ;)\n. The PipelinePack still needs access to the RecycleChan. So the Config cannot be removed without adding a helper interface or reference to the RecycleChan.\n. Since the packs are stored in a map until the report is completed the report generator can exhaust the pipeline pack supply if the number of Plugins exceed the config.PoolSize (hanging the system). With the ability to dynamically add plugins this is now more of an issue.\n. The same global defaults are defined is two places; once for the command line and another for DefaultGlobals.  In the case of Hekad the command line is the source of truth and in the case of the unit tests DefaultGlobals is the source of truth.\n. This prevents the inChan from being closed twice\n. Passing the last seen msgLoopCount value is exactly what we want otherwise we can create an infinite loop between two plugins by creating the messages in the timer_event function.  Check out the LoopToOtherA/LoopToOtherB when running flood with -test 1000 and -test simple (pretty cool because the termination only happens when the A->B->A cycle is happening with no external messages)\n. yes, removed\n. Actually if it failed to run we don't want in in the list\nRemoveFilterRunner should have the check so the callers of this function don't each have to test the condition\n. I reality they will probably build up a message and Inject it like statsd_input_writer instead of building a decoder, updating the message header ENUM, and adding it to the config.\n. It might be better in the example to listen on more than just the loopback interface i.e., \":5565\"\n. Not really a doc issue but how are we handling new decoders?  We don't have a reserved block (in the proto definition) for Heka use and if people start adding their own enumerations there could be conflicts in the future.\n. Still unknown?\n. Don't we support protobufstream too?\n. I corrected the default, the port is now 4352.\n. if /var/run/hekad/* is going to be the standard maybe we should change the default dashboard path to this location.\n. syscall.SIGUSR1 is not available on Windows\n. Observation/Comment only: So the plugin will process the one retained message exit and have to be restarted again for the purpose of preserving the message order within the data stream it came from, correct?.  Seems like a lot of overhead, are we guaranteeing the message order from a single data source (by it's time? by its position in the log?).  This seems like a case impacting output plugins only, when the destination cannot be reached. If the output is going to a data warehouse like Cassandra then the order doesn't matter, if it is going to some stream processor then in-order (within each source) can be beneficial.\n. Self or this is really better; too bad we are adopting a bad practice.\n. If the server is down it fails immediately.  If it is a slow response it will wait until the connection is closed.\n. We would normally want to remove the old id and create a new one but since we haven't used it and don't have any saved data I decided to re-use it, objections?\n. typo, remove paren in output string\n. I don't believe the submatch and names array can ever be a different length.  The code originally came from Ben's regex helper and I wasn't sure if he hit some condition where that wasn't true so I left it.\n. The configuration doc needs to be updated.\n. just ran gofmt against it\n. I tried nanopb but rolling our own gives us 2x the speed, allows us to use a dynamic buffer, added less than 100 lines of code, and removes another external dependency.\n. You can get rid of the configPlugin function all together\n. Can get rid of this configPlugin too, just move the init call into the if\n. I am not really concerned about the glob or map insert getting broken.  The main thing about this code is to instantiate a plugin in a running Heka (and from a unit test perspective that would be a mocked out call saying yeah it worked which doesn't buy us a lot).  Ideally we would have automated testing against a Heka instance.  Flood+ setting up and tearing down inputs, rolling log files, starting and stopping sandboxes etc... \n. A LogfileInput plugin is only spawned when a matching log file is found in the directory. i.e. if a new directory is created with no log the directory is ignored.\n. Why '='\n. Using the manager filter name for the directory and file prefix is a bit redundant.\n/heka/sbxmgrs/mymanager/mymanager-plugin1.lua\n. We use IndexByte which takes a single byte delimiter, figured if someone wanted something more complex they could just use the regexp parser.\n. The signature is still different (one takes a byte the other takes a string)\n. The lock wrappers a pair of variables that are used in a calculation in the report. It is very inexpensive since it rarely blocks.  The main reason it is there is to quiet the race warning message.\n. That is what the MultiDecoder was created for https://github.com/mozilla-services/heka/issues/307\nIssue https://github.com/mozilla-services/heka/issues/308 should have called out AMQP too.\n. This wasn't a code change, it is incorrect documentation.  I was fixing up the sample configs and realized the doc example didn't work.\n. Yeah, I started with a single issue and now this PR already includes 5.  I will make it 6 on Tuesday (at least this one will only be a couple liner (ha, and that is how I got this large PR :) )\n. The behaviour is correct but the comment should be clearer... updating.\n. I think the weirdness stems from the fact that the ProtobufDecoder is automatically created and running even if people don't use it (if we had data persistence that would change).  If the user had to explicitly create it and specify it within a decoder list it would be less weird (this addresses the awkward documentation where the decoder doesn't have to be specified for parser_type = \"message.proto\" in any of the stream parsers).  The other thing that is a little weird is if the user creates a ProtobufDecoder with a different name they will still get a default ProtobufDecoder created and have a total of 8 ProtobufDecoders running.\n. Init() already forces the message.proto decoder configuration to \"ProtobufDecoder\" regardless of what is specified  (similar weirdness to your AMQP comment, no error is generated at the moment).  Technically the user should be able to specify MyFasterProtobufDecoder if they wanted.  We should probably require the user to specify the decoder in all cases and document that token and regexp parsers pass the data in the payload and message.proto data is passed in MsgBytes (or standardize that too)\n. The cascading PR...  AMQP really needs a redesign at this point.  If we make it work like everything else we would convert it to use the stream parser and configure an input plugin per stream type otherwise we get into any combination of ContentType/StreamParser/Decoder configurations. Other ideas?\nActually we wouldn't need the stream parser since the message is in one chunk.  We could/would need to remove the protobuf message header and framing too for the decoder to work directly.\n. Yes if we keep that model you will have to specify 2 decoders in the configuration one for message.proto decoding and the other for everything else which is inconsistent with all other inputs.  What I am proposing is requiring one input plugin for each unique content type (unique by its decoding requirements)\n. Don't you want to use the decoder channel here: dRunner.InChan() <- pack\n. Six one way, half dozen the other.  I went with the accessor because it was a smaller change set than the rename and technically is should be a private member (since we just had to open it up for test)\nAs for the router, it is not exposed only access to its length is.\n. Good catch.  This was unintentional and got clobbered when the package name prefix was stripped out of the code.\n. It is probably better to verify this when parsing the config than fail when running.\n. When verifying the config we can split this once and save off the results.\n. We are starting to create a little language in the toml key (time to re-evaluate).  We have constantly been tweaking the Decoders with new configuration knobs to handle more and more use cases (this will always be an issue).  The sandbox has proven infinitely more flexible and at the current time much more performant.  If the Go Payload decoders aren't adding any value (at this point they would have to be much faster, to justify their rigidity) is it really worth spending more dev cycles on them?\n. Why is it reassigned here https://github.com/mozilla-services/heka/blob/a6852b3f876007de5253bebdc9692cb85751ebb0/pipeline/pipeline_runner.go#L632\n. This just reverts the code to what it was (remember the quick fix I put in to get an active branch working... yeah that was a bad idea)\n. +1 on pegging externals to a tag or SHA.  This will allow all versions of Heka to build with their proper configurations/components going forward.\n. Yeah the funny thing is the config setting are usually bigger than the Lua code.  And before someone asks... no lets not have the ability to embed a simple Lua transform directly in the config ;)\n. It appears that all uses of this method have been removed (in favor of Router()),  so it can be deleted.\n. The re module is LPeg, it just uses string grammars instead of language constructs. http://www.inf.puc-rio.br/~roberto/lpeg/re.html.  There are no pcre/re2 like regex's in Lua.\n. Yeah, but that proves nothing since you are mocking out the main thing.\n. It isn't that is why I had to add the sleep (the connection was being ripped down before the message was sent)\n. The stub would show it is working, in cases where it actually doesn't.  The tests are there so if the plugin is changed you can uncomment and verify that it works against a real mail server.  I can leave them there for this purpose or remove the test file.\n. Hmm, it is correct but I see the confusion. We are saving off the original headers (to original) because we are about to clobber them with the unmarshal.\n. We probably don't want to hard code the severity but those are good defaults. i.e. could be a debug or non alerting failure.\n. I know you didn't select it but we need to take a look at the message Type values we are generating.  If they are arbitrary like the one above creating message matchers will become a very painful task\n@rafrombrc \n. We do this check is several places now, it should probably be broken out into a function getFieldName.\n. The check call is redundant since the type has already been tested, just convert it (same for string)\n. Interesting empty statement syntax. but lets put it in it own scope with braces.\n. If you are going to support binary UUIDs the string would also have to be accompanied by a length since it could have embedded nulls. \n. Why is this a string?\n. On line 148 the representation should be set to \"B\" (I am trying to encourage people to use the metadata, since it can really help downstream consumers)\nResponse time could use some units too :)\n. typo do->to\n. A fixed timestamp and uuid seem problematic both for real time processing and any storage index. \n. We could get some invalid UUID into the system here, but we really don't enforce type 4.\n. The goal is once you are done testing locally you just switch the repository over to 'git'\n. There should be no risk since there is only a single writer, but I will change it for consistency.\nNote: The Go memory model will have a problem with it\nhttp://golang.org/ref/mem#tmp_9\n. Yes. \nMan I must have auto completed that.  Sad part is it didn't throw any error.\n. It is already installed to the proper location, by the decoder directory install.\n. Why is this in the docs? The syslog_decoder never landed in Heka.\n. Not a great example configuration but its removal seems unrelated to syslog parsing.\n. Remove debug output\n. Remove more debug output.\n. Hardcoded path\n. Should probably make some constants for the directory and extension and share it with the sandbox filter.\n. Should also convert  timegenerated if it is part of the message\n. This should be ../config/filters/sandbox.rst\n. It would be nice if the actual sandbox decoder implementations were here (nginx, rsyslog)\n. It would be nice if the actual sandbox filter implementations were here.  If people don't drill down into the sandbox section they may not realize we already have some aggregators etc.\n. We just output it as text.  #633 \n. Isn't the static directory 'dasher'\n. dasher\n. dasher\n. I am still not really liking this.  Users will have to put their own decoders/filters/modules in the share directory or have to hard code a fully qualified path into their config.\n. As long as upgrades don't clobber it.\n. Then +1 on having everything in one place\n. Sphinx only picks up the first so we can forgo this ugliness. The standard then becomes first multi-line comment is the documentation.\n. It really is Config here as in [MyDecoder.config].\n. Commented out when stressing testing the buffer management and apparently never reset it, doh.\n. You don't need the reportLock here since you are atomically loading the value.\n. This is now your public RollQueue method\n. Should just call the file buffered_output.go\n. unused\n. This can be removed and getQueueFilename can be called directly.\n. The encoded data is not used, did you mean to compare it to the queue output?\n. TrimSpace()\n. Will panic if <= 0\n. - The intent is to use the time passed into timer_event() \n- It prevents plugins from having to require \"os\" \n- It removes the need to make an additional time call and convert it to nano seconds before calling any functions.\n. Add a newline to this file and the one above to avoid the warning.\n. This config is incorrect,\n. Not sure I like the design difference between decoder and encoders. Freeing the input to accept more data while decoding is not really a valid argument.  If the decoder is that much slower the channel will quickly backup and block the input and I am not sure the wiggle room in the channel buys us that much.\n. \"(Decoder|Encoder|Filter|Input|Output)$\" will suffice\n. // output only\n. This should go after the atomically accessed int64's (for consistency the width is fine)\n. The todo should not be necessary; sending downstream control messages doesn't make much sense.\n. Could just go with var instead of var(). Actually TSFORMAT is unused and can be removed.\n. You were replacing the abbreviations  (b/c, w/) in other places why use them here?\n. No preservation of the original headers like on the decoder? (for multiple injects, i.e., splitting one message into many).  It can be accomplished in the encoder but the same can be said for the decoder lets make them consistent one way or the other.\n. Exactly, show me some numbers\n. no new line\n. Why aren't we using the accessor? It would have avoided the problem in the first place.\n. Add a better error message.\n. You have been doing a lot of longer line reformating are we now trying to enforce an 80 character line?\nAs per the Go docs: Go has no line length limit. Don't worry about overflowing a punched card. If a line feels too long, wrap it and indent with an extra tab. \n. No, if we do we can lose the estimate from the last minute of the day.\n. Wrong encoder\n. Missing a JSON encoder test\n. The config is identical to ESJsonEncoder, should we split it out and include it\n. Note: Windows uses the C89 formatting codes http://msdn.microsoft.com/en-us/library/fe06s4ak.aspx\n. ns will always be nil here\n. add_to_payload(idx_json, \"\\n\") will prevent the allocation and GC of a second header JSON string with the newline.\n. same note as above\n. Use number instead of int64\n. You should move this into process_message so it is cleared between calls\n. of -> or\nstrftime type time substitution (reads weird)\n. What about these? You use the shorthand syntax in the initialization but the full syntax here.\n. Still smells bad, and isn't buying much\n. Fix copy and paste error in the test below.\n. 'structur' typo\n. This map is not required (there is no name lookup so you can just iterate the Ordered array in its place)\n. Yeah, that could easily iterate over the Ordered array we should get rid of the map.\n. Thanks for the contribution.\n- Ideally the replacement would happen in parse_url.\n- The replacement should also conform to the userinfo grammar in RFC 3986 Appendix A; the expression above is far to restrictive.\n. This could be condensed into a single expression:\nstring(REGEX REPLACE \"^https?://([A-Za-z0-9$-._~!:;=]+@)?\" \"\" _path ${url})\n. Add newline\n. Still need to remove prefix_ts\n. Raw output does not handle arrays\n. This case statement and a lot of the duplicate logic can go away.  A single WriteField(first bool, b bytes.Buffer, f message.Field, raw bool) function could work for everything; writeStringField, writeRawArrayField, writeRawField could all go away\n. This still doesn't handle arrays.\n. The GetValue() call is unnecessary the value can be accessed from the values array (assuming the array has any data)\n. no semi-colon\n. parse it\nlocal anomaly_config    = anomaly.parse_config(read_config(\"anomaly_config\"))\n. singular\n. The first parameter is the payload name (the graph to apply the detection to). In this case whatever you are setting 'title' to. This allows you to specify anomaly detection of any number of cbuf data structures.  I will update the docs with this comment and add some information about when the different algorithms should be used.\nSee: https://github.com/mozilla-services/puppet-config/blob/master/shared/modules/shared/templates/hekad/sync.toml.erb for more advanced use cases.\n. Probably don't want a dependency on itself.\n. Probably want to exit non-zero\n. l.digit^1 * \".\" * l.digit^1  -- if 1.2.3 is not valid data we shouldn't allow it to match\n. could just assign the grammar result directly to msg.Fields\n. fields.time?\n. Need a DiskStats sample\n. You only want one row\n. could just assign the grammar result directly to msg.Fields\n. fields.time again\n. Lost the FilePath\n. This will work but you really want ipairs\n. We should only create the field keys once\n. Use the value returned by the iterator, probably want to call it stat instead of cbuf\n. Be more consistent with the var initialization and trailing commas\n. Since you don't use field you can just iterate over the size of time_stats.fields\n. You will want to queue the alerts here and send later\n. row\n:)\n. fyi: l.P not necessary https://hekad.readthedocs.org/en/latest/sandbox/index.html#lua-parsing-expression-grammars-lpeg\n. Just read the variable here, it gets rid of the local variable and doesn't bother loading the field unless the parse succeeds.\n. Update example with FilePath\n. wrong plugin name and file path\n. Just read the variable here, get rid of the local, and update the example message.\n. fyi: Style wise some function calls and some extra parens can be removed from this grammar (yes I realize I wrote it ;))\n. again, perform the read here\n. the Lua 5.1 idiom for array insertion is labels[#labels+1] (yeah, we still do have some inserts floating around).  This will also allow you to remove the require \"table\".\n. 'ipairs' over labels\n. This creates a global sec_per_row variable\n. This structure seems like a pre-mature optimization.  Yes arrays in theory should be more efficient but require more management in the code making it more opaque. and harder to follow.\nFor example:\n- stats[1] -- v.s., a disk_stats variable (especially since stats is only iterated once)\n- for i = 1, #time_stats.fields do -- iterating the size of stats.fields to index into the msg_fields (as opposed to just iterating msg_fields directly)\nThe above issues could be fixed without changing the structure but something like the following would be easier to follow and less prone to mistakes.\n``` lua\n-- technically you don't need to specify nil values, but:\n    -- it forces the table to be allocated to the proper size when the plugin is initialized\n    -- it documents the structure in a single place\ndisk_stats = {\n    cbuf        = nil,\n    title       = \"DiskStats\",\n    aggregation = \"none\", \n    unit        = \"\",\n    fields      = {\n        {name = \"WritesCompleted\"   , key = \"Fields[WritesCompleted]\"   , last_value = nil},\n        {name = \"ReadsCompleted\"    , key = \"Fields[ReadsCompleted]\"    , last_value = nil},\n        ...\n        }\n}\nfor i, v ipairs(disk_stats.fields) do\n    local val = read_message(v.key)\n    if type(val) ~= \"number\" then return -1 end\n    if v.last_value ~= nil then\n        local delta = val - v.last_value\n        disk_stats.cbuf:set(ts, i, delta)\n    end\n    v.last_value = val\nend\n```\n. unnecessary\n. fyi: May cause a spike with preservation = true and and delayed restart\n. Actually stats is iterated in timer_event you can still throw it in a single table and access it by key instead of index.  i.e., the title could be the key in stats.\n. The read_message keys should be generated once\n. Fix all the files missing an end of line\n. Not really like the + column thing.  We should at least only calculate the column values once (the length is not stored it is calculated each time)\n. ah, yes\n. It is just a little weird you return it, since you aren't using the return value anyway\n. title, not buf in the concat call\nalso the title and buf locals don't buy you much\n. iterate labels\n. iterate labels here too\n. For the current release, but that is the point, the schema changed.  If the user is running their own version in the future say 7 (it will actually be the internal version plus the user version so the check will test 8).  If we change the internal schema again we add 2 forcing another reset otherwise the version would remain stable at 8..\n. remove the debug print statement\n. Protect against non numeric tonumber(...)\n. This will suffice: if name ~= \"timestamp\" then\n. Will influxdb be happy with a non-numeric value?\n. If there are a large number of stats (like #1035) do we want to break it up so cjson.encode doesn't throw an error. Looking at the output plugin, it would be terminated but the output would continue to run.\n. All global data in the plugin (this implies that preservation is somehow tied to the circular buffer)\n. Add screen shots?\n. Nginx (should be capitalized)\n. /n?\n. Script type is set to the default, do we want to show it?\n. 24 minute view?\n. The rate of change comes from comparing 2-16 to 17-31 that delta is compared to the standard deviation of 32-1440\n. Timestamp will always return a number, no need to call tonumber on it.\n. This check is unnecessary.\n. Make these vars local\n. This won't work for annotated graphs\n. Unnecessary or\n. Will not account for any backfilled data (in this use case it shouldn't be an issue).\nOn a related note: If librato can take updates (incremental or full) using the cbuf delta would be less brittle.\n. Fix eol\n. The Lua idiom for nil tests are:\nlua\nif not line then  -- instead of line == nil\n--\nend\n. I would drop this library and just use t[#t+1] since you are only doing a simple insert.\n. The or case will never be reached\n. What is the reason for accepting a floating point Pid/Severity (10.7)?  It is also inconsistent with the payload decoder which only accepts an interger Severity.\n. The timestamp is already in seconds just verify it is there/numeric and multiply 1e9.\n. Accounted for in lines 58-60\n. Is there a reason a failed JSON decode shouldn't be treated as an error?\n. Should include the GELF version in EnvVersion header.\n. Not a config option\n. It was recently updated in the docs (not sure why rtd has not picked it up)\nhttps://github.com/mozilla-services/heka/blob/dev/docs/source/message/index.rst\n. Why are we keeping the float values for the Pid field?\n. Don't need this anymore\n. The type check already confirmed it wasn't nil\n. ```\nGraylog2 Extended Logging Format Decoder\n\n/work/git/heka/build/heka/src/github.com/mozilla-services/heka/docs/source/sandbox/decoder.rst:17: WARNING: Title underline too short.\nGraylog2 Extended Logging Format Decoder\n/work/git/heka/build/heka/src/github.com/mozilla-services/heka/docs/source/sandbox/decoder.rst:18: SEVERE: Problems with \"include\" directive path:\nInputError: [Errno 2] No such file or directory: '../sandbox/lua/decoders/graylog_decoder.lua'.\n```\nYou probably want to add an item to decoders/index.rst and index_noref.rst too.\n. I realize the doc generation really spews a ton of warnings but we can at least eliminate the graylog ones. I created a bug for the others #1099\n/work/git/heka/build/heka/src/github.com/mozilla-services/heka/docs/source/sandbox/decoder.rst:17: WARNING: Title underline too short.\n../sandbox/lua/decoders/graylog_extended.lua:17: WARNING: Definition list ends without a blank line; unexpected unindent.\n. make this and sub_func local\n. ts is only used here you could just set values[1] = read_message(\"Timestamp\") / 1e6\n. propagating\n. Comment is now incorrect\n. This change set is almost identical to the FileHashMismatch, we may want to factor out the common functionality.\n. typo\n. Why does the decoder have an extra level of indent?\n. The old path was correct\n/work/git/heka/build/heka/src/github.com/mozilla-services/heka/docs/source/changelog.rst:7: WARNING: Include file u'/work/git/heka/build/heka/src/github.com/mozilla-services/heka/CHANGES.txt' not found or reading it failed\n. Seems a little weird switching to an if to evaluate the last case label.  Also, the multiplier could be set to one and the special casing below could go away.\n. Technically this allows tenths of nano seconds but the result would still be correct.\n. ;) http://godoc.org/github.com/Shopify/sarama#ProducerConfig\n. Yeah, if that is true we should create a new set of message functions and update all the New_Type_Field helper functions too.\n. - Could just add this in the Type header 'heka.decode_failure' as opposed to a field.\n- Probably want to include the failure error message as a field\n. Not very useful since the failed contents are not included in the message.\n. Inconsistent, this was changed to a switch in logstreamer_input\n. I think it would be better if we always used message bytes instead of Payload for raw input.  Less GC in general and less overhead sending it to the sandbox.  If there is no decoder specified we can automatically move it to Payload.\n. Using a simple text input into a sandbox: raw ~4100ns, payload ~4500ns (so not a huge win)\n. Seems like we should audit the error messages and make sure they are consistent i.e., don't log the payload in multiple places, only add the payload to the error log if we aren't sending decode failure messages etc\n. The dRunner setup should be put in a helper function to avoid code duplication in initialization and rescan\n. use !lua_isnoneornil\n. None of this code needs to be duplicated. If you have bytes read and err was not nil and not EOF return otherwise fall through.\n. update error message and add a test case back in to exercise this path.\n. We would need the same eof handling in the other parsers too\n. If you hit eof and didn't read any bytes you don't need to fall thru and call findRecord again\n. Splitter should be lower case.\n. typo, pack\n. This is a little strange: since we currently don't write signed messages to disk.  The other thing that is weird is if you authenticate the message you end up having to decode the header a second time.\n. nm, read it as  however(comma)\n. There is no EncoderRunner\n. Probably still want to list the plugin types (6)\n. The error message will be output twice, which is just annoying.\n. I will leave it like the others for now... but noting it for https://github.com/mozilla-services/heka/issues/918\n. yeah c&p error\n. Use l.Cf/Ct instead to be consistent with the other grammars.\n. Sample is not representative of the the actual output (missing index entries).\n. Lua array counts traditionally start at 1\n. Off by 1\n. As a matter of style, you should use always the same kind of quotes (single or double) in a program, unless the string itself has quotes; then you use the other quote, or escape those quotes with backslashes.\n. Assumes the fields are numeric or convertible to a numeric bad data could crash this plugin.\n. Not sure why the list of fields is being include when they could just be iterated with read_next.  This would simplify a bunch of the following code and also reduce the message size.\n. remove\n. Blank takes up no space on the wire but I have no problem with bumping the version when we change the schema.\n. Don't need the or false\n. I assume this was added to catch an output_limit error since the grammar ensures the message is well formed.  So we are changing the behaviour from termination to dropping the data... I am good with that, any objections?\nCHANGES.txt should be updated\n. Why the removal of the space? Looking at the other error messages a colon space seems to be more of the convention.\n. +1 but we should probably move any log message cleanup to another PR so it can get merged regardless of the Type change.\n. Not used\n. Anchor the match\n. You can git rid of index_count and the index Fields in the decoder and you can replace the loop with something like this\n``` lua\nwhile true do\n    local typ, name, value, rep, count = read_next_field()\n    if not typ then break end\nif not wl or name:match(wl) then\n    if count > 1 then\n        local field_name = string.format(\"Fields[%s]\", name)\n        data[name] = {}\n        data[name][1] = value\n        for i=1, count - 1 do\n            data[name][i+1] = read_message(field_name, 0, i)\n        end\n    else\n        data[name] = value\n    end\nend\n\nend\n```\nWhen https://github.com/mozilla-services/heka/issues/1342 is fixed you will be able to pull down the entire message at once and avoid all these read calls.\n. The code above you can use now (that API has been available for a long time).  No ETA on the much nicer way yet.\n. math.MaxUint32\n. Hmm, different constructs between the two http plugins.  In this case len(record) == 0 is not needed (if it had a length the deliver flag would have also been true).\n. Sandbox encoders can mutate messages\n. I find this interface naming confusing.  Either it will encode or it won't. \n. Phrasing 'we the decoder'\n. Should validate the user provided configuration value and have a lower threshold (512 should be reasonable).\n. Don't really need this since it is only referenced in one place (could just put the value there).\n. \"our code generally copies into MsgBytes and then resizes down\" Can you point me to where you are referring, The length is meaningless when populating it with something new and we should only be dealing with capacity (like this):\nGo\n        messageLen := len(unframed)\n        if messageLen > cap(pack.MsgBytes) {\n            pack.MsgBytes = make([]byte, messageLen)\n        }\n        pack.MsgBytes = pack.MsgBytes[:messageLen]\n        copy(pack.MsgBytes, unframed)\nAlso if read_message(\"raw\") is called on a new empty message it should not return MAX_MESSAGE_SIZE of NUL bytes (which is the reason the change is included here)\n. Update the queue full unit tests to verify the roll guard invocation and reset\n. I knew I should have updated alert/annotation to use environments (see anomaly.lua).  https://github.com/mozilla-services/heka/issues/1496\nIt is too easy to leak into the global environment if you aren't careful (like thresh_bound and num_in_thresh below).\n. I was playing with some code to address 1496 but the original hack of not using environments will have to stand for now.\n. This plugin doesn't use the global preservation hack and can be written using the preferred environment method (I was just clarifying the other alert and anomaly must remain the same).\n. Not critical but a couple of M's can still be removed (here and line 101)\n. Also not critical, add an new line to the end of the files.\n. Remove debug print statement\n. Not sure why his is part of the diff since the file didn't need to be touched but ok.\n. Move these two items to the top of the struct to avoid alignment issues on 32 bit systems\n. remove\n. Not ideal hard coding it to one less than the default router channel size.\n. Remove old commented out test fragments.\n. The matcher shouldn't have to know anything about this.  The time should always be converted to a numeric value by the grammar and everything should work as it did before.\n. We don't actually have a DateTime data type.  I would just update the existing entry to:\nTimestamp (Nanoseconds since the Unix epoch or a quoted RFC3339 string that will be converted to nanoseconds e.g., '2014-02-03T14:02:03Z')\n. This helper function should go in the .y file since it is only used there\n. copy and paste error in commented out debug message 'regexp'\n. You should be able to leave VAR_TIMESTAMP in numeric_vars and get rid of this grammar (the string case is special cased and tested first)\n. Comment out this line in the example\n. If you are up to it this should be changed to read_message(\"raw\")/decode_message/iterate.  If not we will update it when the read_next_field API is removed.  https://github.com/mozilla-services/heka/issues/1602\n. In this case you want to iterate over the array of message fields. As per the decode_message documentation it returns:\nThe array based version of the message structure with the value member always being an array (even if there is only a single item). This format makes working with the output more consistent...\nThis code works correctly for me in an encoder on the dev branch.\n``` lua\nfunction process_message()\n    local msg = decode_message(read_message(\"raw\"))\n    if not msg.Fields then return 0 end\nfor i, v in ipairs(msg.Fields) do\n    add_to_payload(v.name, \" =\")\n    for m, n in ipairs(v.value) do\n        add_to_payload(\" \", n)\n    end\n    add_to_payload(\"\\n\")\nend\ninject_payload(\"txt\", \"fields\")\nreturn 0\n\nend\n``\n. I would also rename this module 'line_protocol' is too generic for an influxDB specific module.\n. left over from testing?\n. Also a type check should be used instead of an existence check. i.e.,type(json.hostname) == \"string\"`\n. if the point is to make it reversible then escaping and not substitution would be needed.\nEdit: you will also lose any NULLs so if you truly want the original use payload_keep.\n. %l ??\nHuh, I guess in their implementation it is valid.\n. extract_quote should be local too\n. Arbitrary 'waits' for stuff to finish in a multi threaded environment will eventually hit a race condition.  If we cannot just start destroying we probably need a better solution.\n. Out of bounds if the error message is less than seven characters long (you can use lua_tolstring to grab the length when retrieving errmsg or just grab it off the end of err (which is always longer than 7)\n. Abort is rc = 5\n. This and line 116 above should return abort (5)\n. The flag could be set in go_lua_write_message* to avoid re-encoding a message being passed through as-is.\n. comma instead of .. will prevent as second string from being created\n. Prefer string.format for anything over a single concatenation (your call, I would have to profile it to see if it is worth it)\nlua\nfield_out_name = string.format(\"%s_fidx_%d\", field_out_name, field_idx)\n. comment no longer matches the code\n. Fields are optional and can be missing\n. Minimally you can get just a Uuid and a Timestamp (the required fields). Everything else can be nil.\n. Fields cannot be read this way\n. Couldn't this just be an option on the rsyslog decoder instead of duplicating the code here (defaulting to false to preserve the existing behaviour).\n. add new line\n. Ideally this grammar would be made into a module so it could be re-used.  To  be used with Heka it would need to go into the heka branch of the lua_sandbox\n. Severity should be numeric\n. alnum would suffice for the first three but a common hostname grammar should be used instead (there is an open issue since we are getting quite the collection of hostname parsers) https://github.com/mozilla-services/lua_sandbox/issues/129\n. A regular capture will suffice for all RecordType and RecordClass entries e.g. l.C\"A\"\n. Yeah just change all the l.P's to l.C's and get rid of the substitution expression\n. that is fine the refactor can happen in issue 129\n. ",
    "kamilsmuga": "Why HUP signal specifically? \nHow about auto-reload when cfg file changes (with caching of old one enabled) and if it fails reload the original one?\n. ",
    "cnf": "HUP because that is pretty much a standard on unix binaries.\n. ",
    "nathanleiby": "Is this still on the roadmap? Would this be a useful OSS contribution?\nWhenever we update our config, there's a risk that it takes down Heka, so this would be useful for us!\n. cc: @mohit @burnsed\n. Build failure seems unrelated: Failed to clone repository: 'https://github.com/trink/struct.git'\n. Pushed an empty commit, and it fixed the build failure.\n. Sorry to drop this PR. Closing since no longer necessary. Thank you!\n. ",
    "iwinux": "Any updates on this issue?\n. ",
    "jjn2009": "+1\n. Any ETA yet? For file inputs what sort of effort would this take to add SIGHUP reconfigure. I need to on the fly add new logstreamer inputs without restarting hekad\n. ",
    "leslie-wang": "+1. We like Heka. However, without reload, it is hard to do dynamic log provision.\n. ",
    "Seldaek": "The host chain sounds like the best approach, it works like the X-Forwarded-For header, where every proxy can add itself. At the end you can just remove all known IPs in the chain and you know that the first unknown is where you have to stop trusting and take that as the most real external IP you'll get.\nThat said I'd much prefer proper signing support so you can just outright reject messages that aren't properly signed instead of trying to filter through the mess on output.\n. But you can't sign with the TcpOutput IIRC? That's what I meant by proper support :) Well proper support would need a UdpOutput with signing I guess, because the TCP one has its limitations here. But there is another ticket for this.\n. @benmmurphy I'd suggest using logstash_\\d to define it, if possible using the logstash version that introduced it. So if we end up having different logstash implementations it won't look like logstash_original, logstash_new, logstash_new_new, logstash_this_is_the_one, .. :) \n. Having an UDP input/output couple with signer support would be great for simple yet secure setups where it's acceptable to loose messages while the master node is down or the link is interrupted. UDP would solve issue 2 and 3 though as far as I understand. 1 is a user choice that's fairly easy to understand. Once that's in I would happily stop using the TCP input/output.\n. Just a random idea here, but would it make sense to be able to specify a failure-case payload so that if the process returns a non-zero code or times out (we should have a timeout too btw;) you can say use \"OMG ERROR\" as payload, or perhaps this should be a more structured message form. I don't know how that would look like since I'm not so familiar with heka yet, but I hope you see what I mean.\n. I don't really see why windows would need a special treatment here? Piping stuff together inside a .bat file is as easy as doing it in a .sh one as far as I can see?\n. Oh but that is great already. I'll leave the issue open because I think\nit might be a more complete approach, but your trick will get me going\nfor now thanks.\n. Regarding the open question, I would say ||int and ||int? would be decent syntax. Where int? would mean decode but if it's empty (or maybe if it's invalid) drop the field and proceed. If a plain int fails the cast I would say the message should be invalidated.\n. @adieu for now you have to use a sandbox decoder instead, and cast to int in the lua script, e.g.:\n``` toml\n[count_decoder]\ntype = \"SandboxDecoder\"\nscript_type = \"lua\"\nfilename = \"/path/to/count_decoder.lua\"\n[count_decoder.config]\ncount = \"Count\"\ntype = \"count\"\nhostname = \"...\"\nlogger = \"...\"\n```\nthen in count_decoder.lua I have this:\n``` lua\nfunction process_message ()\n    local type = read_config(\"type\")\n    local hostname = read_config(\"hostname\")\n    local logger = read_config(\"logger\")\n    local countField = read_config(\"count\")\n    local fields = {}\n    fields[countField] = tonumber(read_message(\"Payload\"))\n    inject_message({\n        Type = type,\n        Hostname = hostname,\n        Logger = logger,\n        Severity = 6,\n        Fields = fields\n    })\n    return 0\nend\nfunction timer_event(ns)\nend\n```\n. I see, sorry for missing that, but that's kinda the point, it should be linked to from the severity_map docs IMO. I'll try and send a PR for this anyway :)\n. This relates to #472\n. No more time for this tonight unfortunately but tomorrow evening I'll take another look at the tests, I think I did run make test in the repository root not in the build dir that might explain things :)\n. I pushed the fix you suggested for the docs, and tried to fix the build but still can't run tests somehow.. And I'm not sure why travis doesn't re-trigger a build.\nHere is a make test run:\n```\n$ make test\nRunning tests...\nTest project /home/seld/dev/heka/build\n    Start 1: message\n1/5 Test #1: message ..........................Failed    0.07 sec\n    Start 2: pipeline\n2/5 Test #2: pipeline .........................Failed    0.05 sec\n    Start 3: sandbox\n3/5 Test #3: sandbox ..........................Failed    0.05 sec\n    Start 4: sandbox_plugins\n4/5 Test #4: sandbox_plugins ..................Failed    0.05 sec\n    Start 5: mozsvc\n5/5 Test #5: mozsvc ...........................***Failed    0.05 sec\n0% tests passed, 5 tests failed out of 5\nTotal Test time (real) =   0.27 sec\nThe following tests FAILED:\n      1 - message (Failed)\n      2 - pipeline (Failed)\n      3 - sandbox (Failed)\n      4 - sandbox_plugins (Failed)\n      5 - mozsvc (Failed)\nErrors while running CTest\nmake: *** [test] Error 8\n```\n. I can't really use the clean format, because I need the logstash_v0 format, but it's no huge deal anyway.\n. It's still on my TODOs but I'm really swamped at the moment and it seems like your little bike-sheddy note will require quite some work potentially (given my knowledge of Go & ability to find more than 5minutes of calm and focus lately) so I haven't got to it yet.. Probably over christmas holidays I'll wrap this up.\n. Closing in favor of #682 - sorry I dropped the ball here, just too many other things going on.\n. Cool thanks\n. Thanks for wrapping this up for me!\n. @lyrixx just one comment, I don't know if calling it monolog is the best.. unless you make sure it works with a regular SocketHandler + JsonFormatter from monolog itself (call setFormatter on the handler to replace the default). I think it would do the same as the handler your created but without the need for a custom class.\n. I agree this is not perfect, but as a user I have to say it's also kinda nice to be able to quickly configure stuff without having to write lua code.\n. That's probably true, but indeed the \"lua in toml\" thing crossed my mind earlier, because it's a bit of a pain to have many files to deal with, until I move it all to puppet configs anyway. It's a dumb gut feeling thing, but I think for newcomers it's a bit nicer if they don't have to dive into lua sandboxes right away. You're probably right though. It'd just be nice to identify common lua scripts at some point and turn them into go decoders (if we can make them as performant as the lua ones, which I'd hope is possible), or at least ship some lua ones with heka, just to set common patterns and avoid forcing everyone to maintain a copy of the same stuff.\n. That's probably true, but indeed the \"lua in toml\" thing crossed my mind earlier, because it's a bit of a pain to have many files to deal with, until I move it all to puppet configs anyway. It's a dumb gut feeling thing, but I think for newcomers it's a bit nicer if they don't have to dive into lua sandboxes right away. You're probably right though. It'd just be nice to identify common lua scripts at some point and turn them into go decoders (if we can make them as performant as the lua ones, which I'd hope is possible), or at least ship some lua ones with heka, just to set common patterns and avoid forcing everyone to maintain a copy of the same stuff.\n. typo nove=>none :)\n. ",
    "jbonacci": "+1\nYes it is.\nWe need some way of balancing the use of the command line options with the more useful config file.\nI have seen this handled both ways:\n1. Config file contains everything you need\n2. Config file has most common settings with command line options for specific changes and overrides\nAs a bare minimum though, it is always nice to have the -version and -help (and -config) with the remaining info built into the config file.\n. +1\n@bbangert nice work getting this into CI\n. +1\n. Well, thanks for patching this so we can install/use on OS X. I assume somebody checked this out on OS 10.7 as well as OS 10.8...\n. Is there a corresponding Docs issue open to document this new output plug-in?\nREF: https://hekad.readthedocs.org/en/latest/configuration.html#outputs\n. Same for flood:\n$ flood\n2013/06/03 17:02:02 Error decoding config file: open flood.toml: no such file or directory\nIn this case, I do not even see a dummy flood.toml file with the RPM install\n. +1 to customized sandbox filters for internal customers.\n. @rafrombrc \nPlease add the necessary updates to the documentation!\nhttps://hekad.readthedocs.org/en/latest/developing/testing.html\nThanks!\n. +1\nWell you know I like this kind of stuff added...\n. +1 saw this in the IRC channel earlier...\nMore documentation = good\n. Why am I not seeing this here:\nhttps://github.com/mozilla-services/heka/blob/master/docs/source/installing.rst\nShould I be looking in the Dev branch?\n. Oh, yea, there it is, among other additions:\nhttps://github.com/mozilla-services/heka/blob/dev/docs/source/installing.rst\n. +1 this all looks awesome. Thanks.\n. +1 I see it listed here: https://hekad.readthedocs.org/en/latest/installing.html\n. OK, so that steps in that REF leave a lot to be desired, but you get the idea...\n. Actually, I think it also needs this:\n[StatAccumInput]\n. :100: \nPHAT\n. ",
    "thedrow": "Serf is written in Go and it fits perfectly to what you are describing.\nAn integration project could be very interesting.\n. We can create service discovery backends much like confd has and vulcand is about to have.\n. @mattcottingham I don't see a PR. Any update on this?\n. @mattcottingham Really? I haven't seen any activity on the IRC channel.\n. @rafrombrc So if share_dir and base_dir are configurable why is this issue still open?\n@mreid-moz Why is there a need to change the default?\n. Maybe a more valid approach is to create a decoder that formats the data into celery's JSON format?\n. @ecnahc515 Celery also supports json and msgpack formats.\n. Wonderful!\n. @trink I'm not sure you should replace more specific outputs with the generic HttpOutput. They make the configuration much easier.\n. ",
    "ianneub": "Possibly etcd as well.\n. @rafrombrc This is the issue that we discussed in IRC related to encoding and decoding protobuf'd messages into and out of SQS.\n. @rafrombrc Fixed the tests on this PR to setup the ProtobufEncoder with the default config settings.\n. Thanks for spotting that issue with the revised docs @rafrombrc. I've also updated the CHANGES.txt.\n. Thanks! I'll be out of town but I'll see if I can get that updated next\nweek.\nOn Thursday, June 26, 2014, Rob Miller notifications@github.com wrote:\n\nThis looks good, but it seems like with just a bit more effort we could\nsupport all of the field types and not just string. Need a\nwriteRawArrayField analog to writeStringArrayField, and to repeat the\nconversion from field.GetValue().() to field.GetValue() (w/\nthe associated length check and etc.) for the other conditions. :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/932#issuecomment-47290661.\n\n\n-- Ian Neubert\n. @rafrombrc I added the other field types to this commit. I think this will now support any of the message fields. When you have time please let me know if there are any tweaks or style changes that would improve this.\nThanks!\n. All set @rafrombrc. I've removed the reflection and also improved the tests to find an issue with the last implementation.\nI'm still a Golang newb, so please let me know if you had a more efficient idea for the for loops.\nThanks!\n. @trink I've replaced the writeRawArrayField with writeField and now use that for the Fields. I didn't replace writeRawField or writeStringField, because those are somewhat different: they actually do need the calling method to tell it the explicit string to use.\nAlso, I've updated the tests to verify the raw parameter is working. There were no tests for that before, so if you could check that it's doing what you expect it to that would be great. (I'm assuming a raw field will be written out exactly as it is). I only tested the raw field for string and bytes fields, the others didn't seem to make sense (they are already raw).\nWhat do you think of those changes?\n. Thanks @trink, I believe that I've taken care of those three things.\n. Thanks @trink. It looks like I hadn't merged from dev in awhile and the change log that I submitted with this PR incorrectly placed the change in the Features section for 0.6.0.\nDo you want me to create a new PR to fix that?\n. This fixes a regression I introduced in #932 \n. You're awesome @rafrombrc. Thanks!\n. Looks awesome! Thanks again!\n. This could relate to #65 as well.\n. Thanks @rafrombrc. It actually didn't occur to me that I could pass in the type to writeRawArrayField and use that in my switch statement. I'll give that a shot and let you know.\n. ",
    "sgerrand": "It would be great to have this. I've had to use a running heka server while developing a client library.\n. ",
    "victorpoluceno": "That would be great!\n. +1\n. ",
    "VasanthAbyeti": "Cassandra/MongoDB would be apt for this.\n. ",
    "rafaelrosafu": "I had the same issue running on Centos 6.4, which comes with glibc 2.12. I wrote down a mini guide to compile and test it on this environment https://gist.github.com/rafaelrosafu/5658550\nIt seems to work fine but I still need to do more tests.\n. ",
    "bbinet": "I'm also hit by this issue.\nAny updates?\n. For those interested, I've built a debian package from source which is compatible with debian wheezy:\nhttps://github.com/bbinet/heka/releases/download/v0.5.1/heka_0.5.1_amd64.deb\n. +1\n. That is good news, I'm very interested in this feature.\nIf I understand correctly, it means we'll be able to create a new file every day?\nSo for example, does the FileOutput plugin would be able to generate the following file structure:\n.\n|-- 2014\n|   `-- 12\n|       |-- 29.log\n|       |-- 30.log\n|       `-- 31.log\n`-- 2015\n    `-- 01\n        |-- 01.log\n        `-- 02.log\nDo you know what is the roadmap for this feature?\nI'm not proficient with golang, but I can provide feedback and help test this feature.\n. I would also like to see it in the next release, and that could be a good way for me to start with golang.\n@4r9h if you plan to work on it before the next release, please go ahead, else I'd like to have a try.\n@rafrombrc: do you have any requirements, guidance for this feature? for example what will the configuration look like? Do you want the rotation time interval to be configurable or should it be hardcoded to one day? how would you make the path to the output files configurable so that one could output a file tree as shown in my previous comment?\n. Thanks @rafrombrc and @4r9h , it sounds good.\n@4r9h: I'm happy to help and if you don't find time to work on it, please tell me as I can have a try early next week.\n. Ok, I am starting to work on this right now: I hope I could send a PR before the 0.9 release.\n. Here is the pull request: #1294.\nI would be happy to apply any changes are needed so that it can be included in the next release.\n. is this supposed to replace https://github.com/mozilla-services/heka/blob/dev/sandbox/lua/encoders/statmetric_influx.lua which was introduced in v0.7?\n. Thanks @rafrombrc for the explanations.\nAs influxdb supports receiving many metrics values from a single http request, I was also wondering if heka would support batch sending of metrics to influxdb so that we don't open a new tcp connection for every single metric flowing through heka.\n(but that might not be the right place to ask: should I move this question to the mailing list?)\n. Thanks @rafrombrc for your answer.\nFirst I thought that I would use the StatAccumInput stuff, but it does not work for me since I don't want to aggregate the data but simply batch data.\nI'm very interested by the filter you are talking about to accumulate and format data and then periodically emit an aggregate message containing the entire batch: that sounds exactly what I want to achieve. But I guess I would have to create a custom filter (such a generic \"batch\" filter don't already exist in heka?) and then also create a custom output plugin for InfluxDB because the format of the batch data would be different from the format that the current InfluxDB output plugins understand?\n. @rafrombrc sorry for hijacking this github issue: I will use the mailing list if I have some more questions.\nThanks.\n. I'm interested in this issue, because I want to use Hindsight, which requires the lua_sandbox package to be installed, but I also would like Heka to be installed so that I can use tools like heka-cat.\nFor now I think that Heka current v0.9.2 release use an outdated version of the lua_sandbox which is not compatible with Hindsight, but when v0.10 will be released, do you think Hindsight will be able to use the lua_sandbox bundled into Heka?\n. I don't really know why this issue was closed, but I think it would make sense to pull the lua_sandbox out of Heka itself as it is now a required dependency for both Heka and Hinsight.\nAnd we can imagine the lua_sandbox could have its own life cycle: it could have more frequent releases than Heka and Hindsight when lua modules are being added or updated.\nWhat do you think?\n. Yes, hs-cat would be really useful to me, thanks.\nIf possible, I still would like to be able to install both Heka and Hindsight on the same machine, so that I can easily revert back to Heka just in case.\nI think the best option would be to pull the lua_sandbox out of Heka, but as a temporary solution if it is just a matter of updating the lua_sandbox SHA to make it compatible with Hindsight, it would be great to do it before the upcoming 0.10 version is released.\n. When we are in the process to build Heka, there is already some dependencies to install like cmake, so in my opinion this is not a problem to require the users to also install the lua_sandbox as an external package before building. I also think that people who wants to build Heka could be considered as advanced users. People will manually build the lua_sandbox only if they actually need a custom build, otherwise they can use an official RPM/DEB package. \n. @rafrombrc I'd be happy to help, but I don't have any experience with CPack, Cmake.\nIn the mean time, I've seen that #1733 has been merged, which means that when Heka 0.10 is released, we should be able to install Hindsight side by side with Heka and configure it to use the lua_sandbox coming from the Heka package.\n. I've realized that #1733 has been merged in the dev branch, so it won't be available in the 0.10 version: is it possible to cherry-pick the commit to the versions/0.10 branch?\n. @nathwill I'm also interested by a systemd journal input sandbox: have you continued to work on this? Would you have anything to share?\n. Thanks for you answer @nathwill \nI will try to work on it in the near future.\n. merge conflict artifact?\n. Yes, I already thought of your proposal, but it would introduce a second configuration parameter and as I was not sure about the performance penalty, I decided to not do premature optimization and keep the more simple solution for now.\n. Right, will do.\n. But, I initially followed the same direction as the SIGHUP handling code:\nhttps://github.com/bbinet/heka/blob/fileoutput-rotation/plugins/file/file_output.go#L318\nSo, do you confirm I should change this to LogError?\n. ",
    "lukecyca": "I ran into the same problem. Here are my copy-and-pasteable instructions for building a .deb of the dev branch on Debian 7. I ran this in a docker container, picked up the .deb, and threw away the container.\napt-get install wget build-essential libprotobuf-c0-dev cmake git mercurial libgeoip-dev\ncd /usr/local\nwget https://storage.googleapis.com/golang/go1.2.2.linux-amd64.tar.gz\ntar -C /usr/local -xzf go*.tar.gz\nexport PATH=$PATH:/usr/local/go/bin\ncd \ngit clone https://github.com/mozilla-services/heka\ncd heka\nsource build.sh\nmake deb\n. ",
    "vmunix": "Example use case: read a CloudWatch metric in and then write it to a whisper file for graphite graphing.\n. Is there a reason we'd push straight to the Stackdriver API vs. just pushing to CloudWatch and pulling them into Stackdriver from there?\n. ",
    "dannycoates": "I like the fields option simply because it makes the plugin more straightforward ignoring any performance differences. For example:\n``` js\nfunction process_message()\n  local ts = read_message(\"Timestamp\")\n  local payload = read_message(\"Payload\")\n  if payload == nil then return 0 end\nfor k, v in string.gmatch(payload, \"stats.(%w+) (%d+)\") do\n    if cols[k] ~= nil then\n      data:set(ts, cols[k], v)\n    end\n  end\nreturn 0\nend\n```\nIs harder to understand than:\n``` js\nfunction process_message()\n  local ts = read_message(\"Timestamp\")\n  local rss = read_message(\"Fields[rss]\")\n  local heapTotal = read_message(\"Fields[heapTotal]\")\n  local heapUsed = read_message(\"Fields[heapUsed]\")\n  if rss == nil then return 0 end\ndata:set(ts, RSS, (rss / SCALE))\n  data:set(ts, TOTAL, (heapTotal / SCALE))\n  data:set(ts, USED, (heapUsed / SCALE))\nreturn 0\nend\n```\nI think the raw payload is best for writing to output, while fields are better for plugin use, so having it as a config option makes sense to me. Perhaps the default should be payload only to make the common(?) case of only writing to output (whisper) not duplicate data.\n. Ok, I wasn't aware graphite works that way. I'm going to close this then.\n. Awesome stuff, very informative! Thanks @trink :+1: \n. ",
    "chamaken": "Would you think about the order of ReadString()'s err == io.EOF and os.SameFile()? I think checking osSameFile() first is better whether the file was rotated because source program (syslogd, apache) may write between ReadString returned with err == io.EOF and checking os.SameFile(). That's why I made defer() which invalidate current fd.\n. Good point, I hanged up but stopped thinking, sorry.\nIt's up to you, not discad but emit (incomplete, ended without newline) new message.  As you know though, this means ``old rotated file ends without newline''. I guess it rarely happen because source program like syslogd likely flush their buffer when receiving SIGHUP or stuff like that.\n. ",
    "tlrx": "Hi Rob,\nGlad to see that this feature was expected, and thanks a lot for your code review! :) I'm not a confirmed Go programmer (but I have a good Java background) and I would love to continue working on this feature. If it's ok for you, I should have time in the coming weeks to add tests and to update the code according to your remarks.\nNote: The Bulk UDP feature of elasticsearch looks also very interesting.\n. @rafrombrc Sure, there is still work to do. Not sure that I can edit the pull request to change the base branch, but I can create a new pull request if needed.\n. Just the same pull request as #258 but based on the \"dev\" branch as asked by @rafrombrc in https://github.com/mozilla-services/heka/pull/258#issuecomment-19838144\n. Thank you for writing this documentation :)\n. Very nice! Timestamp and escaping were also on my todo list. Open source is great :)\n. Ok. Maybe outputs.go should be corrected as well? I've seen something similar for FileOutput.\n. You're definitely right, I should have seen that :/\n. ",
    "dustinrc": "Thanks for the help on IRC, @trink.  Thought I was losing my mind.  The documentation should probably indicate when features (like LoglineDecoder) have become available in which version.\n. ",
    "zined": "LGTM :)\n. seeing the same problems with gcc 4.9.1, clang 3.5.0, cmake 3.0.2:\n$ gcc --version\ngcc (GCC) 4.9.1\nCopyright (C) 2014 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n$ clang --version\nclang version 3.5.0 (tags/RELEASE_350/final)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\n```\n$ cmake --version\ncmake version 3.0.2\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n```\n. ",
    "jivid": "I want to take a stab at fixing this issue, but I can't seem to find LoglineDecoder anywhere in the repo. I see the pull request @trink made 22 days ago (https://github.com/mozilla-services/heka/pull/249) that modified logline_decoder.go in pipeline/ but I don't see that file anywhere.\n. Ahh, that works; I see all the files in there now. Thanks!\n. I've written what I think should fix this issue, but I'm having issues with running tests on this. According to the docs, I should use flood, but no flood binary got installed with make dev, so I tried running the main.go file in cmd/flood/. But that gives me this error:\nbash\nflood (dev) $ go run main.go -config=\"flood.toml\" -test=\"default\"\n2013/06/29 23:12:23 Error creating sender: dial tcp 127.0.0.1:5565: connection refused\nexit status 1\nI went over the heka docs on http://heka-docs.readthedocs.org/en/latest/, and that told me I can use make test to see if my copy of heka doesn't have errors, but when I try to run that, I see:\nbash\n/usr/local/go/src/pkg/github.com/mozilla-services/heka/sandbox/lua/lua_circular_buffer.c:9 6c: No such file or directory: ctype.h\nmake: *** [bin/hekad] Error 2\nCan anybody help with this?\n. Hey @rafrombrc, I added a test for the Kitchen time, similar to the one parsing Apache time. The only modification needed was to change the expected value based on the current date. I didn't cast the Unix timestamp as int64 since UnixNano() returns an int64 value. \nLet me know if I need to make any changes to this.\n. ",
    "pawelchcki": "Yup after second thought this way seems much clearer :smile:.I just thought for some reason that len(x) == 0 is much faster than x == \"\" when as I've just tested it seems the same. \n. Yeah I figured as much. :)\nBut as a personal habit i prefer to leave such work to the compiler. Because otherwise it makes the code less readable and compilers these days are pretty good at figuring how to optimize it.\nJust now I tested following 'snippet'\ngo\n    // var z int\n    for i := 0; i < 10000000000; i++ {\n                // z = i + 1\n        z := i + 1\n        noop2(z)\n    }\nand z:= i seems to be faster by 6 ms ;) which implies no additional overhead of stack allocation\nAnyway after looking again at the timer calc code I see that it still could use some improvement in terms of readability and variable/code reuse.\nI plan to rewrite it a little in next PR to support multiple Percentile thresholds just like latest etsy/statsd. \nBut if you prefer to move the variable declarations out of the loop I can amend this PR. \n. ",
    "michaelgibson": "+1\nVery nice!\n. Er...Ahem...Yes well you can see I don't customize the hekad configuration values much. :)\nAlso what do you think about changing the logic for the config flag to accept either a single filename or a directory name and auto load based on that instead of passing a separate flag for \"configdir\"?\nI already committed that change but can put that back in if you think it is best?\nI was trying to emulate the behaviour of Logstash. \n. To clarify this one a bit:\nWe have a need to dynamically define both the index name and type based on the value of a field in the message.\nThis is so we can take advantage of different elasticsearch templates based on the index and type names.\ni.e.\nSimilar to the logstash configuration of:\nindex => \"logstash-%{@type}-%{+YYYY.MM.dd}\"\nI am not sure how to best approach this.\nI have started down the path of adding a UseTypeField flag to the output which would overwrite the \"type_name\" with the Type field value and also interpolate it into the index name somehow.\nThoughts?\n. Here is what I have done prior to your template suggestion. \nhttps://github.com/michaelgibson/heka/compare/dev...features;issue_356\nI changed the cleanIndexName to interpolateFlag which was previously used only for the time format in the index name.\nNow it will attempt to interpolate anything within %{} to the Field value.\nexample of my config.\nindex = \"logstash-%{Program}-%{2006.01.02}\"\ntype_name = \"%{Program}\"\n. Pull Request https://github.com/mozilla-services/heka/pull/385\n. What about just a generic TCP listener that slices messages off the stream by a delimiter.\nCould default to '\\n'\n. I am able to reproduce this issue.\nLooks like the environment variables are not persisting into cmake?\nAdded to build.sh\necho GOROOT=$GOROOT\necho GOPATH=$GOPATH\necho GOBIN=$GOBIN\nand added to cmake/FindGo.cmake\nmessage(STATUS \"GOROOT=${GOROOT}\")\nmessage(STATUS \"GOPATH=${GOPATH}\")\nmessage(STATUS \"GOBIN=${GOBIN}\")\nenv | grep GO\nGOBIN=/data/github/heka/build/heka/bin\nGOROOT=/data/go/\nGOPATH=/data/github/heka/build/heka\n. build.sh \nGOROOT=/data/go/\nGOPATH=/data/github/heka/build/heka\nGOBIN=/data/github/heka/build/heka/bin\n-- GO_EXECUTABLE=/usr/bin/go\n-- GOROOT=\n-- GOPATH=\n-- GOBIN=\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:70 (MESSAGE):\n  REQUIRED_VARS (missing: GO_VERSION GO_PLATFORM GO_ARCH VERSION_VAR\n  GO_VERSION)\nCall Stack (most recent call first):\n  cmake/FindGo.cmake:36 (find_package_handle_standard_args)\n  CMakeLists.txt:16 (find_package)\n-- Configuring incomplete, errors occurred!\n. I'd like to see this as well.\nI've been formatting things at the custom Filter level in order to get the desired functionality.\n. Ok, updated with dedicated test.\n. SetDeadline apparently does not shutdown the connection immediatly after the deadline expires.\nThis means there is no benefit to extending the deadline after the request.\n. build failed but not because of anything I changed....\nHow do I tell Travis to try again? \nDo I need to commit something?\n. Nice!\nI'll see if I can mimic that.\nThanks.\n. That's actually what I am currently doing. \nI'm trying to see if I can get better performance using a lua decoder for de-compression.\nAlthough I would still need to use it in a MultiDecoder so I'm not sure if I will gain much.\nI guess I'm also trying to adhere to the encouraged practice of offloading customizations to the sandbox rather than compiling into Heka?\n. I never did get this working using Lua so still using a Go based solution.\nThis is what I came up with for compression:\nUsing this StreamAggregatorFilter\nhttps://github.com/michaelgibson/heka-stream-aggregator#streamaggregatorfilter\nIn combination with the ZlibEncoder\nhttps://github.com/michaelgibson/heka-zlib#zlibencoder\nAnd Decompression using ZlibDecoder\nhttps://github.com/michaelgibson/heka-zlib#zlibdecoder\nIn combination with the StreamSplitterDecoder\nhttps://github.com/michaelgibson/heka-stream-aggregator#streamsplitterdecoder\nI don't have anything for performance metrics for this either however. Using vs not using compression(for log aggregation) meant the difference between a slight increase in bandwith usage and completely saturating our link. That was more important at the time than how much extra overhead it was causing the process.\nThe code is functional(for me) but I'm no GoDev.\nIt's by no means optimized nor necessarily the best approach.\n. bah! \nlooks like BasicAuth is a 1.4 thing: http://golang.org/doc/go1.4#minor_library_changes\nhttp://golang.org/pkg/net/http/#Request.BasicAuth\nAny idea when Heka will be worthy of the new version?\n. Cool! \nPR for https://github.com/mozilla-services/heka/issues/1543 on it's way.\n. See PR: https://github.com/mozilla-services/heka/pull/1986\nThis adds glob_pattern as a config option which, if specified, uses Glob instead of Walk.\nMay help address your issue.\n. Looks like this is using the https://github.com/Shopify/sarama library for the Kafka client.\nI do see an option to specify the number of messages to trigger a Flush here:\nhttps://github.com/Shopify/sarama/blob/88a4afb3d18f13212477a63a78e3a57ac87830a9/config.go#L117\nIt looks like k.saramaConfig.Producer.Flush.Messages\nwould need to be added to the output:\nhttps://github.com/mozilla-services/heka/blob/be1420d226808485a557ad622bb75785cadfa43a/plugins/kafka/kafka_output.go#L291\nas well as a config option added.\n. I think....this would be as simple as adding Proxy: http.ProxyFromEnvironment,\nFrom:\nhttps://github.com/mozilla-services/heka/blob/dev/plugins/nagios/nagios_output.go#L94\nTo:\nhttps://github.com/mozilla-services/heka/blob/dev/plugins/http/http_output.go#L81\nBut haven't tested it.\n. Please look here for external community plugins.\nhttps://github.com/mozilla-services/heka/wiki/Community-Plugins#outputs\nhttps://github.com/uohzxela/heka-s3\nPlease send support requests to the mailing list instead (and close this issue). Thanks\n. Please send support requests to the mailing list instead (and close this issue). Thanks\n. @DonHarishAxe: Could you post your configs? Perhaps that would make it clearer what you are trying to do.\n. Increasing your threads probably increased the number of packs in heka's pipeline.\nWhen heka runs out of available packs, it will block until it can free up more.\nYou may need to increase either/both the \"plugin_chansize\"(default: 30) and/or \"poolsize\"(default: 100) to allow more packs to exist in the daemon.\nhttp://hekad.readthedocs.io/en/latest/config/index.html#global-configuration-options\n. That has more to do with your domain's TTL settings in DNS and the local DNS caching(nscd?) for the system on which the heka daemon is running.\nIn any case, Heka does not cache DNS in any way.\nPlease send support requests to the mailing list instead (and close this issue). Thanks\n. I'm not 100% sure what you are asking, may need some clarification.\nI'm assuming you are asking how to collect from multiple log files and how to differentiate between them once in Heka?\nAssuming you have some way of uniquely identifying each of your log streams, the logstreamer input has a config field \"differentiator\"\nWhen defined it will set the \"Logger\" header in the message.\n```\n[accesslogs]\ntype = \"LogstreamerInput\"\nlog_directory = \"/var/log/nginx\"\nfile_match = '(?P[^/]+)-access.log'\ndifferentiator = [\"nginx.\", \"Port\", \".access\"]\n[LogOutput]\nmessage_matcher = \"Logger == ''\"\nencoder = \"RstEncoder\"\n[RstEncoder]\n```\nThis allows you to distinguish between different incoming log streams.\n\n. There is a workaround for building on 1.6.\nSee: https://github.com/mozilla-services/heka/issues/1881#issuecomment-237322744\nPlease close as duplicate.\n. Looks like there is one that is included as an external plugin but it's not documented.\nIncluded in build here:\nhttps://github.com/mozilla-services/heka/blob/dev/cmake/externals.cmake#L190\nSource:\nhttps://github.com/mozilla-services/heka-mozsvc-plugins/blob/dev/statsd.go\nYou can see a listing of other external plugins here:\nhttps://github.com/mozilla-services/heka/wiki/Community-Plugins\nMost are not included in the pre-built release so you would have to add them manually:\nhttp://hekad.readthedocs.io/en/latest/installing.html#building-hekad-with-external-plugins\n. What gives travis?\nTest failures appear to be random.\nClosing and Re-opening PR to trigger rebuild. \n. Makes sense.\nDone.\n. That is weird. I wrote this a while back and must have been in a hurry.\nI'll fix it.\n. what if the request \"almost\" times out? Would there always be enough time left before for the next request comes in if we don't re-extend the deadline after a successful request?\n. I don't see a good way to implement a custom dat file that works with the vendor's libraries. It's fairly proprietary. \nThat line isn't really necessary though.\nThe rest of it is only there to satisfy the default value from \ngeoip_decoder.go line 41: DatabaseFile:   filepath.Join(Globals().ShareDir,\"GeoLiteCity.dat\"),\nBut the database file is not actually getting opened.\nIt's not a great test but might be better than nothing if we can't stub out our own database?\n. sounds good. I'll update that.\n. ",
    "radu-gheorghe": "I'd like to point out that Logstash is changing the schema in the upcoming version: https://logstash.jira.com/browse/LOGSTASH-675\nAnd this discussion: https://logstash.jira.com/browse/LOGSTASH-837\nAFAIK, all you need to play nice with Kibana is to have an ISO 8601 timestamp that goes into the field \"@timestamp\". The rest is pretty much free-formatted.\n. ",
    "benmmurphy": "yup. i'm thinking of renaming the messageformatter to logstashOldStyle or something like that as well to get rid of any confusion once logstash changes their log format.\n. ",
    "tgulacsi": "Just build issues - seems to be overkill to have a lua dev env just to\ntest my github.com/tgulacsi/heka-plugins plugins.\n2013/7/24 Mike Trinkala notifications@github.com\n\nI am not sure how optional Lua will be going forward, we are looking to\nleverage it for some additional tasks, like decoding, in the future. What\nis the particular issue you are trying to solve? Build issues? Size issues?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/pull/343#issuecomment-21515364\n.\n. Linux waterhouse 3.9-1-amd64 #1 SMP Debian 3.9.8-1 x86_64 GNU/Linux\ndebian jessie/sid (unstable)\n\ngo version devel +84917868ef43 Wed Jul 03 10:43:24 2013 -0700 linux/amd64\ngo get github.com/mozilla-services/heka/messages\ngo get github.com/mozilla-services/heka/pipeline\ngo build github.com/tgulacsi/heka-plugins\nfails with\ngithub.com/mozilla-services/heka/sandbox/lua\n.go/src/github.com/mozilla-services/heka/sandbox/lua/lua_circular_buffer.c:96c:\nNo such file or directory: ctype.h\nI understand your aversion for providing a 'nolua' tag - that suggests\nthere will be only minor downgrade in funcionality, thus you cannot\nintermingle everything with lua...\nThen what is the solution for this missint \"ctypes.h\" ?\n2013/7/25 Mike Trinkala notifications@github.com\n\nCan you send us the platform information and the output from the failed\nbuild? Thanks.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/pull/343#issuecomment-21554256\n.\n. cmake complains about everything. Doesn't find my go installed from source\nin /usr/local/go (GOROOT is set), then Protobuf (what should I install for\nit?)... Do we have a list of dependencies somewhere?\n\n2013/7/26 Mike Trinkala notifications@github.com\n\nTypically it means the generated file lua_sandbox.go was manually deleted\nand this is how cgo blows up. You could try running 'cmake ..' in the\nrelease directory to regenerate it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/pull/343#issuecomment-21590646\n.\n. Others have cons against embedding Lua, too - see\nhttps://groups.google.com/forum/m/#!topic/golang-nuts/DsgGhl9qp-I\nGThomas\n\n2013/7/26 Tam\u00e1s Gul\u00e1csi gt-dev@gthomas.homelinux.org\n\ncmake complains about everything. Doesn't find my go installed from source\nin /usr/local/go (GOROOT is set), then Protobuf (what should I install for\nit?)... Do we have a list of dependencies somewhere?\n2013/7/26 Mike Trinkala notifications@github.com\n\nTypically it means the generated file lua_sandbox.go was manually deleted\nand this is how cgo blows up. You could try running 'cmake ..' in the\nrelease directory to regenerate it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/pull/343#issuecomment-21590646\n.\n. Yes, I'm trying to circumvent it right now...\n\n\nIronic, that I moved to subpackages to not induce unneeded dependencies - to be able to import only email, and not have to download go get.\nThis was easy with plugins.json\nShould I move back to one package?\n. ",
    "augieschwer": "Yup, fixed!\n. I suggest /var/cache/hekad/seekjournals/ -- I'm not sure how standard, but this location is persistent in my Ubuntu environment, and this standard indicates it may be true for other distros.:\nhttp://www.pathname.com/fhs/pub/fhs-2.3.html#VARCACHEAPPLICATIONCACHEDATA\n. ",
    "davidbirdsong": "So does #600 fall into number 1? Do you guys foresee wanting to keep any TcpOutput functionality in the core codebase?\nI see both being useful. A forced re-connect for most of my uses will work well. A load balancer will sit in front of a bank of hekad agg/routers and an immediate reconnect will just re-route to a live one. I'm happy for the reconnect logic be outside of hekad and provided by a process restart.\nI could see this working similarly w/ UDP and a floating IP, but a load balancer is more common in my experience than the ability to float IP addresses around.\n. I hacked up externals.cmake to make some progress on my first output plugin:\nhttps://gist.github.com/davidbirdsong/7982677\nClearly any clone tags are not implemented and no sub-packages can be specified. \n...but I'm finding I have to use local_clone for more than my project. For instance, my dependency: github.com/Shopify/sarama needs \nhttps://code.google.com/p/snappy-go/ as \"snappy\" but I'm not sure how to wrangle the hg_clone function to pull in that mercurial repo and then find the go code under the subdirectory \"snappy\".\nSo far my plugin_loader.cmake is all sorts of hacky and uglified:\nsh\n[david@foulplay heka{dev}]$ cat cmake/plugin_loader.cmake \nadd_external_plugin(git https://github.com/Shopify/sarama master)\nadd_external_plugin(local_clone code.google.com/p/snappy default)\nadd_external_plugin(local_clone github.com/zebrafishlabs/toto testing) #this url is sort of pointless since all I'm after is \"toto\"\nIs there a better way?\n(sorry for being such a Cmake newb)\n. Cool, I'll keep using 'local_clone' for now. So far, so good. I'm loving this framework! \n. I'd ask on a mailing list, but I didn't see one. Where could one find a list of existing plugins? Or are you referring to this: https://github.com/mozilla-services/heka-mozsvc-plugins\n. Are you ok with the reversed naming in TlsConfig?\ngolang\n    ClientCAs              string `toml:\"server_cafile\"`\n    RootCAs                string `toml:\"client_cafile\"`\nI figured it represents the intent of the setting better this way.\n. Got it, I've changed the params to match up closer w/ crypto/tls.Config{} member names.\n. Looks great. Is anybody on this yet? I need it soon, which means I could carve out time and tackle it this week.\n. @trink I didn't catch this before, but what do you think about\ngolang\nfunc (b *BufferedOutput) QueueRecord(pack *pipeline.PipelinePack) (err error)\ninstead of\ngolang\nfunc (b *BufferedOutput) QueueRecord(record []byte) (err error)\nI'd like to count on existing parser to work, though I could see how we could change the parser and make a configurable.\n. This PR is a first pass. I just wanted to get eyes on it while I modify the tests. Noteworthy areas are how writeToNextFile() used to signal whether the plugin's Run method could succeed.\n. How's this PR, need more cleanup or good to go?\n. The short answer is no, heka does not simply send a marshaled Message message. There's a header message with length prefixing and hmac-signing...\nGiven the right use of the correct proto definitions, this isn't impossible, but as far as we can tell from a slightly incomprehensible code paste, you're not on track to read the internal heka message stream. Also, I'm pretty sure heka-py was intended for sending heka messages to heka, not receiving them.\nif you want to know how the message stream is constructed, start by reading code in this dir.\n. I just taught myself how to get a module into the sandbox. It's not tried or tested yet, but might save you some time.\nhttps://github.com/davidbirdsong/lua_sandbox/commit/95d63d31a82e60bde983bf1cbdfa51885cec6c56\n. I should point out that I've successfully built the sandbox, but not heka.\nhttps://gist.github.com/davidbirdsong/f6c2cdf5562fbc487dc7\nOn Thu, May 22, 2014 at 9:17 AM, michaelgibson notifications@github.comwrote:\n\nNice!\nI'll see if I can mimic that.\nThanks.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/issues/857#issuecomment-43910220\n.\n. woohoo. going to try this out.\n. Backwards compatible w/ 2.8?\n\nIf not, I'd not be able to build anymore. I'm running Ubuntu 14.04 and Centos 6.5. I don't plan to stay with these much longer, but not being able to build heka would be a huge bummer.\n. was just about to open a ticket for this.\nthe main thing that's messing me up is, i want to be able to require heka-shipped modules (from lua_sandbox project) and then require ours. so the thing that we'd benefit the most from is to append to the compiled-in path.\n. for the non-breaking part, what about two new config lines that, if present, override `module_directory``?\ntoml\nlua_path = \"/tmp/lua_files\" # expands to /tmp/lua_files/?.lua\nlua_cpath = \"/tmp/lua_cfiles\" # expands to /tmp/lua_cfiles/?.so\nI'm not tied to the names, but this gets closer to normal lua include path semantics.\n. We instrumented a permanent Udp endpoint that itself exposes a websocket\nthat clients can effectively 'tail -f' from. Heka pushes UDP messages to it\nand it responds to websocket requests.\nIdeally heka could expose the websocket directly as an output plugin.\nOn Wed, May 13, 2015 at 5:33 PM Rob Miller notifications@github.com wrote:\n\nNothing in particular I can think of. You probably want to look at the\nUdpOutput for inspiration.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1527#issuecomment-101865183\n.\n. @acesaro mind pointing out the area that covers the Lua float to int problem? I've been working on an encoder that converts 0.9 json style to line protocol, but I've been dead in the water on that issue for some time now.\n. i put this together a few weeks ago and have been using it production since\ninception: https://github.com/davidbirdsong/heka-prometheus\n\nOn Tue, Sep 15, 2015, 10:18 PM Mathieu Parent notifications@github.com\nwrote:\n\nI've found this:\n- https://github.com/docker-infra/heka_exporter (consuming from the\n  dashboard)\n- https://github.com/docker-infra/heka-prometheus\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1720#issuecomment-140407409\n.\n. Kafka support for anything non-jvm is a tough nut to crack. Shopify's been very diligent with this library, but it's changed a ton over the last 2 years. Keeping heka up-to-date with it takes a significant work.\n\nCan any of you offer some time and devel work to bring heka up-to-date w/ sarama? If not, can you find an example go project(consumer, producer is easy by comparison) on github that's implemented and tracked w/ sarama over the last 2 years that could provide as inspiration for a PR?\n. a field whitelist would be very useful. from my reading, the magic values\nget you close, but i didnt see a way to block all fields and only add\nexplicit without the single value option.\nOn Fri, Dec 18, 2015, 11:29 AM Rob Miller notifications@github.com wrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/mozilla-services/heka/pull/1816\nCommit Summary\n- relax outer condition\n- Changelog entry, some ts_line_protocol code cleanup (white\nFile Changes\n- M CHANGES.txt\n  https://github.com/mozilla-services/heka/pull/1816/files#diff-0 (5)\n- M sandbox/lua/modules/ts_line_protocol.lua\n  https://github.com/mozilla-services/heka/pull/1816/files#diff-1 (98)\nPatch Links:\n- https://github.com/mozilla-services/heka/pull/1816.patch\n- https://github.com/mozilla-services/heka/pull/1816.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1816.\n. done\n. done\n. removed\n. removed\n. removed, calling directly\n. I started this test uncertain of which tests to include.  In the end I settled on simply replacing what I'd removed from tcp_output_test.go and so encoding was not needed, but was forgotten and left and now removed.\n. \n",
    "bijohnvincent": "Would be great if this is available in 'FileOutput' also.\neg:\npath = \"/log/%{Date}/%{Hostname}/%{Domain}-access.log\"\n. ",
    "christianvozar": "You should not need a special input for systemd journal. I realize your request was 6 months ago but Heka now has the ProcessInput which should acomodate your needs. http://hekad.readthedocs.org/en/latest/configuration.html#processinput\nUtilize the ProcessInput with the command \"$ journalctl -b\" and this should get you what you need to inject systemd journals into Heka.\n. +1\n. Fixed with #584 \n. In this issue I am referencing the display of currently running plugins within Heka as displayed by the DashboardOutput.\n. I've given this a bit of thought on naming convention. I think it makes sense to go with heka-inject, heka-flood, etc. I know you have concerns re: tab-completion and the only real working solution to that is renaming the hekad binary just heka which would put tab-completion to the daemon by default. Ideally, a user would be using an init.d script to stop/start Heka from the command-line anyway.\nI've updated the PR with documentation and a renaming of the binary to heka-inject.\n. Your email is base64 encoded and your client isn't decoding it. It might be that your email reader needs the Content-Type set to a particular string. I.e. text/plain. I can either put in a flag to encode64 = true or we can run down why your client isn't decoding base64 emails. Testing this in Gmail and Apple Mail it works fine for me.\n. Sounds awesome.\n. Awesome.\n. This is incorporated in PR 599. \n. Have you given any thought to if you want to keep the heka binary hekad or change it to heka for auto-completion happiness?\n. I also +1 on @rafrombrc \nHowever, I do think it would be a non-trival amount of work to provide a HTTP(s) POST client for generic APIs that didn't need to be fed by a custom lua sandbox. Also, each API has particular nuances such as the Atlassian HipChat API Output I wrote requires parameters in the URL where as the Stackdriver API Custom Metrics Output I wrote requires a very specific JSON + HTTP Headers setup.\nIn light of that and this discussion I whipped together a Stackdrvier Output for custom metrics. I abstracted the API client library and put support in the library for annotations and deploy events but have not implemented them in the Heka plugin; will get to it some day or let someone else handle.\nThe output: https://github.com/bellycard/heka-stackdriver-output\nThe output has performance issues namely due to the InteroplateString() func being tied to string manipulation rather than relying on a generic interface{}. This has gotten me in trouble with the Statsd Filter (See PR https://github.com/mozilla-services/heka/issues/612 ) and now with this output. I did some crap string manipulation to ensure the output JSON to Stackdriver sends a float or int rather than a string but ideally I would just use reflection properly.\nEnjoy.\n. Why not 6 as the default? If the information is simply normal information, there is no way of entering a debug level from default normal if needed.\n. Incorporated @trink suggestions, added documentation\n. Thanks for the review @trink, missed those unnecessary lines. Fixed now.\n. Docs updated.\n. Docs updated so errors/warnings should be clear for Graylog. docs/make does not show any issues with the Graylog docs but so many warnings can make it hard to tell.\n. Totally. I guessed at what should be used here. heka.httpdata was already present. Some sort of taxonomy of types like namespace.plugin.status could be useful. heka.httpinput.error, heka.httpinput.success, etc.\n. Understood on the idiomatic Go syntax, will fix and submit.\nI can understand not wanting to break backwards compatibility but having a Url and Urls doesn't seem very DRY. Leaving the option name \"url\" but as an array would be fine; but will still require previous version users to change their TOML to reflect it being an array.\n. Fair points. I clarified and committed.\n. Agreed. I am currently running around 200 URLs over 5 Inputs with 5 sec. interval and performance has not been an issue. I will try some performance crunching to see where wheels fall off.\n. Doesn't need to be, over-sight on my part. I will submit a fix.\n. Modification made in PR 599\n. Easily done; http://hekad.readthedocs.org/en/latest/message/index.html lead me to believe envversion was deprecated.\n. ",
    "chancez": "There's also systemd-journal-gatewayd which lets you obtain data from a local, or remote systemd host. You could use the HttpInput to poll the gatewayd service, and use the application/json or another format, and set the proper decoder.\n. @rafrombrc Alternatively you can also use systemd-journal-gatewayd. In any method, you'll need a way to track your position in the logstream, so maybe the cursor/offset tracking logic can be generalized somewhere? \n. Doesn't celery use pickled python objects for representing tasks? If so, I don't know if you could make this work..\n. This was resolved in #1015.\n. Mostly looking for review right now, I'll squash everything together once that's been done.\n. Alright I made those changes, and squashed everything up. I removed joinAndPart in its own commit so it can be re-used later if we decide to implement it.\n@rafrombrc \n. I believe I addressed the things you mentioned @trink \n. Updated with the changes you mentioned, as well a change to the error handling in sandbox_filter.go. I added a new custom error type since there is actually an error which can occur when a sandbox is not terminated. Instead the plugin runner tests for that specific error type now, in order to determine if it needs to send its own terminated message.\n. Do you mean the Stopping attribute?\n. Your correct that the join code essentially takes a string which gets passed to an IRC JOIN command. Your second point makes sense, and that's how we're handling things, so I'll have to take a look at what might be happening. I tested this in a protected channel, and was able to get the bot to send some messages, but I could have overlooked something, or changed something since testing that again.\n. I've made a patch which should resolve this. Its in #1036, if you would like to try it out.\n. I think I got those changes you suggested.\n. Just one small thing: there's a few areas where you refer to different plugin types and I think in those cases it might help to make those into links. So maybe the first time you refer to the SandboxEncoder and decoder, you make those link directly to the documentation on those plugins. I think this might help so people can easily jump between your docs, and the plugin specific docs.\n. Whoops, I thought i got rid of those.\n. Sure, good idea. Forgot about tests.\n. Should probably add some tests for this if possible, if you need help on that let us know. Also, it looks like you're using goimports, which is fine but we don't typically separate the stdlib imports and external imports like goimports does. Probably best to stay consistent and not separate them.\n. Fixed by #1064 \n. ping @rafrombrc \n. Tests now pass. Now its just a matter of testing it more, and determining if the design could be changed to work better.\nStill need to replace the HTTP input's listener as well.\n. Having issues getting it to work with the HTTP listen input. But I don't think I'll have time to take another stab at it for a bit.\n. That's probably a fair statement. Another thing, is that the corresponding CPU Stats Filter, should probably split into 2 filters. One for # of processes, and the other for Load Avg.\n. It isn't necessarily without the channel parameter, but rather, since the IRC response to parting is asynchronous, the IRC server just happened to not provide an IRC channel which was parted in response to the actual PART command.\nIE: You can /part and eventually the IRC server acknowledges this, and responds with a message about you PARTing, in the normal case that response will include which server you parted from, but in this case it didn't...which is weird since you can't really just part from no channel as far as I'm aware.\n. I'm curious if this happens when the bot can't join a channel after so many attempts, and then parts. But given it never successfully joined, it parts with no channel actually joined? I'm not really sure, just a thought.\n. Hey Quintin, glad to see this! I'm not a maintainer, so @rafrombrc should be able to do this. Would be great for people trying to figure out Heka.\n. You don't need to commit your work to test anything. Just add your plugin to plugin_loader.cmake and build. You shouldn't need to commit or push for that.\n. Your InfluxOutput should have a different message_matcher which matches the different filters. So you could do something like message_matcher = \"Type == 'stats.loadavg'\"\n. Use a filter to compute the deltas. Take a look here for an example:\nhttps://github.com/mozilla-services/heka/blob/dev/sandbox/lua/filters/diskstats.lua\n. Any reason this is using uint32 and then converting to uint64 instead of just using a uint64 as the type from the beginning? I imagine the difference in types could be related to the test failures.\n. Not all people use kafka with protobuf, so that wouldn't make a lot of sense.\n. What about http://hekad.readthedocs.org/en/latest/config/splitters/index.html#common-splitter-parameters\nCould that use clarification, or is it clear how to use use_msg_bytes from that?\n. Ah yes, it would probably be useful to have examples specifically using a few various inputs/ouputs that are using protobuf decoders/encoders to help with this.\n. That seems reasonable, and useful.\n. Oh man this would be amazing!\n:+1: \n. Heka does provide this kind of feature. Using a file polling input on one of the supported proc files (like /proc/stat) in combination with the appropriate decoders/filters (like the CPU stats decoder/CPU stats filter) you can gather system metrics. Right now only basic cpu/memory/disk stats are available, though.\n. I dont really think it matters, since it goes through this super fast, and regardless we're going to be disconnecting. I'm going to move the clearing of the disconnect callback up above this though, since it could be possible to disconnect and fire a reconnect in between clearing out the backlog and calling disconnecting again.\n. Good point, I saw this too at one point and forgot about it.\nI think there is a problem here, where we should not be attempting to join irc channels, or sending messages if we're connected, so I do think this should handle something for that.\n. It shouldn't happen. One case is where we get disconnected, one's when we can't join a channel. There should never be a case where both happen.\n. I agree, in fact, I would ideally only expose the interface methods in this file, and move the rest into their own file, how do you feel about this? Ie: only keep Init, init, Run, CleanupForRestart.\n. The conf is directly passed to the InitIrcCon, and the conn var is separate because that same function can return an error.\n. You can't because a join is async. You need to try to join before attempting to send the message, so that you don't wait right up until you need to send a message, join and then return unable to send a message because you aren't in the channel yet.\n. Interesting, I dont know why gofmt hasn't fixed that.\n. Ah, I see what you mean. That could be possible I suppose. I made this a method so I could inject the timer in tests as needed, but I ended up just configuring the duration as 0. I could probably just remove this.\n. what about if it fails to match, as the if statement below does.\n. I copied that from the apache_access.lua decoder. I guess that this isn't needed in any of the decoders.\n. How do you require at least one, but no more than one?\nl.Ct((row^-1)^1)?\n. So simple.\n. I'll change that, but isnt that slower than just incrementing a counter? It has to recalculate the length everytime (unless lua keeps the length as metadata on the object).\n. IE: Could it just be labels[i] = ... since its already in a loop?\n. There's a local sec_per_row which is on line 40. This is assigning to it, right?\n. Ah, good catch. I used to be using the return, but that's changed.\n. It just makes it easier than typing stat.title and stat.buf eveywhere. I can change those.\n. Yeah, I originally had my defer foRunner.exit(pConfig) line up a bit higher, before this. But your right, this case should specifically cause a shutdown without the canExit behavior.\n. Yes, I specifically wanted to set the err return value to this. The foRunner code logs errors returned by Run(), and by returning a non nil error in the SandboxFilter, we can determine if it's sent a termination message already.\n. Good catch. Changed 106 to use conf.Channels\n. In this location, we use Conn.Join because this function only happens on connection, and we're not actually Joining, but asynchronously sending a JOIN message. output.Join is a wrapper which checks if specific channels have been marked as unjoinable. SInce we're connecting we're either connecting the first time, or we're connecting after a DC. We'll get an error back if we cannot join a channel, and the error callbacks can handle the error (ie, checking if we have no joinable channels left, ect).\n. Should this be stat_accumulator?\n. Should this be stat_accumulator?\n. Maybe add the word is before and: and is run in a protected sandbox.\n. This ref link seems to be broken for me.\n. That's interesting, I've never had a failure related to this, but I'm always giving my VM multiple cores.\n. Yeah, it already does. I'm just bind mounting a single file for a volume, but you could also bind mount a directory and pass the directory to hekad like I've done here with a single file.\n. Localhost works, but it brings you to the dashboard which asks you where you want to go (blank, guided, ect). This takes you directly to the guided dashboard which should show actual data. Although at this point there won't be any. I prefer being consistent when I refer to the URL we're going to since we'll be going to this one later in the example once we've finished.\n. Probably should go after line line 134. Otherwise if there's an error it won't be closed.\n. You might be right about that.\n. Is it possible this can be done outside the loop? Might save some CPU cycles.\n. Oh your right, i see. Yeah that should be fine.\nIn general I was thinking since we're keeping track of the queueSize on writes, we might even be able to avoid this except on plugin startup. But in general, this seems fine.\n. ",
    "nathwill": "+1 for this request; especially as journald takes over some functions of traditional syslog, a journald input would be really useful.\n. also of note here: https://www.freedesktop.org/software/systemd/man/systemd-journal-upload.html. Especially with the --save-state option, it seems like it ought to be a good tool to use in combination with HttpListenInput. I'll screw around with it and see if I can come up with a working pattern.\n. lame, systemd-journal-upload's a no-go for now it seems. it performs the entire upload as one continuous streaming POST, so an HttpListenInput in heka never chunks the messages :/. sooo cloose!\nso far, the ways i know of to get journal data at present include:\n- /run/systemd/journal/syslog as UdpInput + ForwardToSyslog=yes in journald.conf\n  - pros: individually packaged messages\n  - cons: mising a lot of context that journal provides, lossy as they're UDP and there's no history if you're not listening when a message is sent and no cursor tracking to resume where you left off.\n- journalctl + ProcessInput\n  - pros: multiple output formats, can get everything , filter by journal cli options (e.g. current boot, or specific unit, and lots of other journalctl control opts)\n  - cons: no cursor tracking, so messages can be duped if your ProcessInput restarts\nhere's an example heka config i've used for this:\n``` toml\n[hekad]\nbase_dir = '/tmp/heka'\n[Journalctl]\ntype = \"ProcessInput\"\nticker_interval = 0\ndecoder = \"JsonDecoder\"\nsplitter = \"TokenSplitter\"\n[TokenSplitter]\n[Journalctl.command.0]\nbin = \"/usr/bin/journalctl\"\nargs = [\"--boot\", \"--follow\", \"--output=json\", \"--quiet\"]\n[JsonDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/json.lua\"\n[RstEncoder]\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"RstEncoder\"\n```\nwhich results in heka messages like:\nrst\n:Timestamp: 2016-03-02 22:01:26 +0000 UTC\n:Type: json\n:Hostname: wyrd.home.nathwill.net\n:Pid: 28595\n:Uuid: bc93a564-dcbf-4e71-af0a-b79ed6f47df6\n:Logger: Journalctl\n:Payload: \n:EnvVersion: \n:Severity: 7\n:Fields:\n    | name:\"CODE_FILE\" type:string value:\"src/core/job.c\"\n    | name:\"_CMDLINE\" type:string value:\"/usr/lib/systemd/systemd --switched-root --system --deserialize 22\"\n    | name:\"CODE_FUNCTION\" type:string value:\"job_log_status_message\"\n    | name:\"SYSLOG_IDENTIFIER\" type:string value:\"systemd\"\n    | name:\"SYSLOG_FACILITY\" type:string value:\"3\"\n    | name:\"MESSAGE_ID\" type:string value:\"39f53479d3a045ac8e11786248231fbf\"\n    | name:\"PRIORITY\" type:string value:\"6\"\n    | name:\"CODE_LINE\" type:string value:\"774\"\n    | name:\"__REALTIME_TIMESTAMP\" type:string value:\"1456956000657571\"\n    | name:\"UNIT\" type:string value:\"sysstat-collect.service\"\n    | name:\"_SOURCE_REALTIME_TIMESTAMP\" type:string value:\"1456956000657378\"\n    | name:\"__MONOTONIC_TIMESTAMP\" type:string value:\"133772980231\"\n    | name:\"_PID\" type:string value:\"1\"\n    | name:\"_SELINUX_CONTEXT\" type:string value:\"system_u:system_r:init_t:s0\"\n    | name:\"_GID\" type:string value:\"0\"\n    | name:\"_COMM\" type:string value:\"systemd\"\n    | name:\"_EXE\" type:string value:\"/usr/lib/systemd/systemd\"\n    | name:\"_TRANSPORT\" type:string value:\"journal\"\n    | name:\"_MACHINE_ID\" type:string value:\"14f2d832739a4500ba270174d9a63529\"\n    | name:\"__CURSOR\" type:string value:\"s=cb5f1f6cad8a4760af865e3c4d8d6a0a;i=17ec8;b=60aba9208674459cabfe0726af1b7ef7;m=1f257db007;t=52d1800df60a3;x=1c6230d0d93e3092\"\n    | name:\"_UID\" type:string value:\"0\"\n    | name:\"_HOSTNAME\" type:string value:\"wyrd.home.nathwill.net\"\n    | name:\"_SYSTEMD_SLICE\" type:string value:\"-.slice\"\n    | name:\"_SYSTEMD_CGROUP\" type:string value:\"/\"\n    | name:\"_CAP_EFFECTIVE\" type:string value:\"3fffffffff\"\n    | name:\"RESULT\" type:string value:\"done\"\n    | name:\"_BOOT_ID\" type:string value:\"60aba9208674459cabfe0726af1b7ef7\"\n    | name:\"MESSAGE\" type:string value:\"Started system activity accounting tool.\"\nwhich isn't bad, but still not as nice as a native input that interfaced directly with the journal.\n. this'd be really nice for dumping logs into S3 as well\n. we just hit this with version 0.9.2 as well, using this config: https://gist.github.com/nathwill/6f2826bd9d30ae00e43e, and ended up with this corruption in the logstreamer seek pointer file:\njson\n{\"seek\":1071,\"file_name\":\"/srv/code_challenges/shared/log/mac_worker_3.log\",\"last_hash\":\"b0f399b045472212e4d6d10e78e1416bca1196b7\"}4\"}\n. mmmm.... nm, going to work on this as a sandbox input instead.\n. @bbinet no, unfortunately i haven't found the time to make a journal-specific sandbox input; for now something like described in https://github.com/mozilla-services/heka/issues/358#issuecomment-191462517 works well enough.\n. i tagged this for 0.11 since 0.10b is already out, so i figured the merge window on 0.10 might be closed. lmk if i should do otherwise.\n. :+1: \n. ah, egads, this pulled in way too much... need to rebase my commits on the 0.10 branch\n. ah, cool, i wasn't sure how the lua decoders were being tested, thanks for the pointer!\n. @rafrombrc alright, specs and CHANGES added, this looking better now?\n. @rafrombrc @trink ok, i expanded the mapping of the enumerated fields from the json keys to the relavent heka fields (type appropriate). the timestamp is set up to handle things like the gelf decoder: to translate from a timestamp in seconds from unix epoch to us. thoughts?\n. ok, rewrote this to reflect your recommendations, and i agree it's definitely a cleaner way to do this (though i'm still bummed about iterating over so many potential keys). holler if there's anything else you'd like to see changed :)\n. hey, rob. no worries, hope you're enjoying your time off. \ni think i understand, and i like the idea of a user-provided mapping, though i'm not super keen about supporting multiple fields mapping to heka fields; it's extra complexity that seems unlikely to be encountered in the real world from a single data source (and if it's multiple sources, just set up another input->decoder in the heka config), and... what do we do if both fields exist? staying simple and clear seems preferable. i'm also still not clear on how we can offer both keep_payload and (p|P)ayload field mapping, so for now i'll just write in keep_payload support to keep closer parity with the other decoders.\nin any case, i'll start working on rewriting this to support user-provided mappings like you described.\n. @rafrombrc ok, i think this matches what you described, excepting my stated omissions. how's this looking?\n. there's an example grammar on @trink's LPeg grammar tester that might form a nice basis for a SandboxDecoder plugin. if @rafrombrc or @trink think this is something they'd want to merge, i'd be happy to help put one together. this might be interesting for a lot of the telemetry type plugins (e.g. haproxy stats csv, lots of /proc files are tabular like this)\n. this isn't happening in 0.10 with go1.4.3 on Fedora 23, so i dunno. \n. @ChristianKniep there's nothing special about this decoder's use of the cjson module. the cjson module is used by a number of the sandbox plugins (e.g. the gelf decoder), and the module is included as part of the normal heka distribution when you install from a release package or source build.sh. If you want to use this plugin before it's merged, just dropping it into /usr/share/heka/lua_decoders on a machine with a pre-installed heka package works for me.\nunfortunately, i don't know enough about the build process to say exactly where it gets installed without doing a bunch of digging.\n. ah, ok, it comes via this and this\n. @sathieu i was just looking for this today, you rock :metal:, thanks!\n. @rafrombrc yep, looks like it works! https://gist.github.com/nathwill/c58e45c1161704f911c6.\nfwiw, those same specs @ioc32 and @ioc32 reported trouble with are failing for me as well on a clean git clone on Fedora 23: https://gist.github.com/nathwill/a768f6e206a3d4e25cad, but only when using go1.5.1; if i switch back to go1.4.3, tests pass successfully.\n. by \"it works\" of course i mean that a build from the queue_full_shutdown_fix branch is able to resolve a back-log after being restarted following a queue-full shutdown.\nspeaking of which, do you think it makes sense for queue-full shutdown to exit non-zero? hoping to avoid changing our heka service unit from Restart=on-failure to Restart=always, but can totally see the argument being made for either behavior being correct (shutdown due to \"err\", but as instructed...)\n. we ran into this as well when converting from 0.9.2 to 0.10.0, blowing up the output queue to reset let processing resume, and we haven't seen this issue again since (been running 0.10.0 for about a week)\n. hmmm\n. @rafrombrc i'm not sure why exactly, but there does seem to be something weird with the nesting... the test matches experimental results but GetFieldValue(\"nested1-field\") keeps getting nil, so those 2 tests are failing :crying_cat_face:, still digging, but if you have any pointers, i'd appreciate it.\n. awesome, thanks Rob!\n. thanks rob! hua.\n. :+1: will do\n. so, sounds like we want to escape ., and not worry about reversability since there's already an option to keep the original payload intact?\n. ah, didn't know that, thanks.\n. ah, yes, good call\n. they're uppercased since that's how they're represented in heka; if we don't store them uppercase, we have to capitalize them when assigning to the msg table, so no ops are saved either way. i favored lower-casing them when checking the json table since that's how data's typically represented in json (lowercase).\ni don't see the point of iterating over every key in the json table (like you said, this could be really slow) to check for a fixed set of known heka fields. am i missing something?\n. OK, i think i see where this is confusing.\n\nIf so, that data gets written to that message field instead of as a dynamic field.\n\nI think that's what's already happening (which is why i'd uppercased them in field_type_map); see the pack.Message.GetSeverity() and pack.Message.GetHostname() specs I added to confirm.\n. Oh, oh. For some reason this finally clicked in the middle of a baseball game... Correct me if I'm wrong, but I think the difference here is that the way you're proposing is case insensitive wrt the json keys, whereas the current implementation relies on the keys being lowercase (the most common in the real world in my experience).\nI like the idea of being case-insensitive, but the current implementation is a lot faster than any case-insensitive version would have to be.\nAs I see it, there's three possible options here for the json keys:\n- all lower case: covered by the current implementation in the for loop\n- capitalized: covered by the current implementation in the direct assignment at the end\n- camel case/other: covered by the proposed change, but slow as it requires iterating over all of the json keys\nSo, what if we added a \"downcase_fields\" boolean config option (default false), and, if true,  do the downcasing before the field_type_map mapping, so folks could opt into the slower implementation by normalizing their keys, but we otherwise still have a decent chance of doing an efficient mapping for the first two cases?\n. ok, just for kicks i added a downcase_fields implementation so we could see what that'd look like. i look forward to your comments!\n. no worries, there's no rush at all. enjoy your vacation!\nOn Mon, Aug 17, 2015 at 10:46 AM Rob Miller notifications@github.com\nwrote:\n\nIn sandbox/lua/decoders/json.lua\nhttps://github.com/mozilla-services/heka/pull/1653#discussion_r37214772:\n\n+\n-    -- map to heka native message fields\n-    local field_type_map = {\n-        string = { \"Uuid\", \"Type\", \"Logger\", \"Hostname\" },\n-        number = { \"Severity\", \"EnvVersion\", \"Pid\" }\n-    }\n  +\n-    for k, v in pairs(field_type_map) do\n-        for i, f in ipairs(v) do\n-            local low_f = f:lower()\n  +\n-            if type(json[low_f]) == k then\n-                msg[f] = json[low_f] ; json[low_f] = nil\n-            end\n-        end\n-    end\n\nSorry, been on vacation (and blissfully off-line) for the last week... and\nam currently in for 3 days before being out for another short jaunt. Yes, I\nthink the config option is a fine idea, once I get caught up on the email\nbacklog I'll circle back around and review the latest code.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1653/files#r37214772.\n. > I think we're leaking values through here.\n\nMy bad; I guess I misunderstood what happens between decodes; I assumed the initialization before the process message function was run for every decode, but it sounds like that's not the case.\n\nWe should add all of the possible fields in use to the msg initialization\n\nGotcha.\n\nand then be sure to explicitly nil out or set defaults for all static message fields that we don't get from the input JSON.\n\nIf I'm correctly interpreting the suggestion here, wouldn't it be easier to just set up the initial values for the msg table at the top of the process_message function at this point?\n. okey doke. added.\n. ok, very interesting, i'd have thought the extra cycles looping over the keys a second time would outweigh the GC/malloc penalty, but it sounds like i'm wrong there. thanks for the explanation!\n. alright, i think i found a reasonably efficient/clean way to do this, but if you know a better way, i'm all ears :)\n. if we get rid of the type config option, yeah i think that'd work fine.\n. hmmm, yeah, i suppose that's true, i was thinking if type and Type config options were set, we'd want to persist type if Type weren't present, but i guess the docs would imply that Type could end up as nil if set and not present.\n. i actually had a question about this; i copied what the gelf decoder was doing here, but i'm not super enthusiastic about either locking the user into a specific format (epoch), or trying to do a bunch of crazy date parsing. i was searching through the open issues for somewhere i might be able to help and maybe get a little more familiar with the internals, and ran into #1218, which seems to imply that there may be some support for a specific set of timestamp formats without having to parse them in every decoder. if so, maybe we could just document \"accepts mapped timestamps in these supported formats\"; skip the special case, and give users more options...\nso, do lua decoders support passing timestamps in any of these formats? (that was the closest thing i could find to match the description in the referenced docs). if so, i'd be happy to add that list to the docs referenced in #1218, get this decoder updated to support the full range of heka-supported timestamp formats (by accepting user-input on the mapping and letting it fail on inject if not in a supported format).\n. meh. i'll leave it; simpler and less potential to be surprising by being too clever that way.\n. rad, thanks. i'll take a look later this afternoon and see if i can sort that out.\n. ",
    "chrissnell": "If anyone is interested in a \"native\" (well...with cgo) Go implementation, I'm happy to help out.  I wrote a journald reader for another project with the github.com/coreos/go-system library and have been running it in production for a few months now.  I don't think it would be hard to adapt to Heka.   The one headache with of go-systemd is that newer versions of Ubuntu (and potentially others) have the journald library as part of libsystemd.  Older versions use libsystemd-journal.  So, the binaries become OS distro-specific.  Not sure if that's a problem for this project.\n. @sathieu Yes, different .so files depending on which distro.  I'm not totally sure that this was a distro packaging choice--it may be systemd version-dependent.  At any rate, it's not impossible to work around and go-systemd works well.\n. ",
    "sathieu": "I'm not sure to understand the distro-specific part. Has the \".so\" filename changed?\nI currently have to build different .deb for Debian wheezy and jessie, so it won't change for me.\n. Why not simply drop the upstart part?\nUpstart can deal with the init.d script, and upstart will not be in any upcoming big distribution (appart maybe from ChromeOS, ChromiumOS and WebOS).\n. @bbinet I find the current Debian packaging great and easy (I don't know for RPM).\nI think the way forward ould be proper integration in the distribution, I may work on this on the Debian side, but days are only 24hours long ;-)\n. Hasn't it be done already?\n. Replaces #1085 \n. @rafrombrc : I have removed the two lines from the config, and added 2 entries in CHANGES.txt.\n. I've also removed the DEBHELPER check from CMakeLists.txt, as it is in cmake/CMakeDebHelper.cmake with an explicit message (instead of silently dropping the deb target).\n. One more: I removed  debian/heka.default.in, as it was not used.\nI think I'm done with this PR.\n. %b is used instead of %O.\n. I can confirm that squeeze is affected too. The --no-close flag has been added in dpkg 1.16.5.\nFortunately squeeze-backports has 1.16.9~bpo60+1, but I'm looking for a better option.. squeeze-backports only has dpkg-dev and libdpkg-perl.\n. As a workaround, I just remove the --close from /etc/init.d/heka.\n. @loc32: Apply #1744 and see what ES is complaining about.\n. I needed this because:\n- if compiled under wheezy, it lacks the systemd services (requiring recent debhelper, which depends on dpkg-dev >= 1.17 which depends on patch >= 2.7 which depends on libc6 >=2.17 ; making the binary package depends on libc6 >=2.14)\n- if compiled under jessie, it depends on libc6 (>= 2.14)\n- I use reprepro which forbid duplicate names with different checksum\n. This is a WIP, depending on https://github.com/mozilla-services/lua_sandbox/pull/92\n. As said on IRC, I will create a generic SyslogDecoder which parses rsyslog + (postfix, sudo, openssh, dhcpd, bind, puppet, ...)\n. Closed in favor of #1649\n. This replaces #1647.\nStill WIP, I need to move part of it to lua_sandbox.\n. @rafrombrc What do you think now?\n. Replaced by #1760 on the 0.10 branch.\n. Replaced by #1670.\n. I don't know enough of Go, and get:\nheka/plugins/udp/udp_input.go:125: impossible type assertion:\n    net.UDPConn does not implement net.Conn (Close method has pointer receiver)\n. At least, now it compiles. Will test soon.\n. Verified. it now works for me.\n. This is an interim patch.\nA better integration would include\n- systemd-journal input (#358),\n- sd_notify (this one)\n- socket-activation (LISTEN_PID/LISTEN_FDS)\n- journal output\nMaybe using https://github.com/coreos/go-systemd would be a better path than rewriting everything. What do you think? (even if journal reading is not done yet: https://github.com/coreos/go-systemd/issues/70)\n. Thanks @trink, but how to do socket activation of an UdpInput (written in go) from a lua library?\n. I need to:\n-  have a journal input,\n- socket-ativate port 514 (udp and tcp), to loose less syslog messages\n- Have Type=notify, to correctly handle hekad availibility\n- (optionnaly) have watchdog\nI don't care if this is done in lua or Go, but point 2 and 3 probably need to be in Go as they are in heka core.\n. @rvzanten What is your Debian version? Works for me on wheezy and jessie (with source build.sh; make deb).\nHave you purged the \"build\" directory and retried?\n. Can you try:\n```\n!/usr/bin/env bash\nset up our environment\n. ./env.sh\nNUM_JOBS=${NUM_JOBS:-1}\nbuild heka\nmkdir -p $BUILD_DIR\ncd $BUILD_DIR\ncmake -DCMAKE_BUILD_TYPE=release ..\nmake -j $NUM_JOBS\nmake deb\nmv -v *.deb $HOME/\n```\n. @gigaroby Try #1742 with print_decode_failures=false in your decoder config (see also #1743).\n. For now, I'm using nxlog with the following config : \n```\ndefine ROOT C:\\Program Files (x86)\\nxlog\nModuledir %ROOT%\\modules\nCacheDir %ROOT%\\data\nPidfile %ROOT%\\data\\nxlog.pid\nSpoolDir %ROOT%\\data\nLogFile %ROOT%\\data\\nxlog.log\n\n    Module      xm_syslog\n\n\n    Module      xm_json\n\n\n    Module      im_internal\n\n\n    Module      im_msvistalog\n\n\n    Module      om_tcp\n    Host        heka.example.org\n    Port        5144\n    Exec        $SourceName = undef;\n    Exec        $SourceModuleType = undef;\n    Exec        to_json();\n\n\n    Path        internal, eventlog => out\n\n```\nAnd this heka input  \n```\n[winevent_tcp_input]\ntype = \"TcpInput\"\naddress = \":5144\"\nsplitter = \"TokenSplitter\"\ndecoder = \"WineventDecoder\"\nsend_decode_failures = true\nlog_decode_failures = false\n[WineventDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/json.lua\"\n[WineventDecoder.config]\ntype = \"winevent\"\nmap_fields = true\nPayload = \"Message\"\nHostname = \"Hostname\"\nSeverity\nPid = \"ProcessID\"\nTimestamp = \"EventTime\"\ntimestamp_format = \"%Y-%m-%d %H:%M:%S\"\n```\n. I'm still not sure about the field names. Comments welcome.\n. @rafrombrc Done. Thanks for your quick review.\nNB: I've not tested doc build yet, as my machine is really slow.\n. doc OK\n. I've found this:\n- https://github.com/docker-infra/heka_exporter (consuming from the dashboard)\n- https://github.com/docker-infra/heka-prometheus \n- https://github.com/davidbirdsong/heka-promethus\n... added to the Heka wiki\n. Another alternative would be ProcessInput with the typeperf or wmic command.\nRef: https://mail.mozilla.org/pipermail/heka/2015-January/000317.html\n. @trixpan  can you move the rpm dir under the packaging dir?\n. I switched to 0.10 branch, and it seems fixed.\n. Reopening, as this is still happening with 0.10\n. i'm not alone: https://mail.mozilla.org/pipermail/heka/2015-September/000772.html\nI've disabled buffering for now.\n. Any news on the heka 0.10 buffering problem?\n. @ioc32 : FYI I'm having ctests problems with docker.\n. NB: This commit has already been merged in the dev branch.\n. NB: I need this because, under systemd, all stdout/stderr is logged (and forwarded to syslog, which means I have the info twice, the printed form being subobtimal)\n. WIll do it again on v0.10 branch\n. replaced by #1742\n. NB: I need this because, under systemd, all stdout/stderr is logged (and forwarded to syslog, which means I have the info twice, the printed form being subobtimal)\n. @rafrombrc Thanks for your review. Patch updated.\n. Complement #1742 for all the non-decoder errors.\nContext: I need this because, under systemd, all stdout/stderr is logged (and forwarded to syslog, the timestamp is printed twice)\n. NB: not tested yet\n. Behavioral change: this will retry with \"Can't read HTTP response body\" and \"HTTP response didn't contain valid JSON\". This looks ok.\n. @trink What about the new patch?\n. I've choosen to only keep the programname dynamic field for now. Is it ok?\n. it now errors out when rsyslog_template is not defined or invalid.\n. @trink, I like the idea, even if I don't know how to implement it.\nI plan to add more prog_grammars, once my Heka setup has stabilized a bit (kernel/netfilter, proftpd, zorp, ctdb, heka itself, ...).\n. @trink, @rafrombrc Anything missing to merge it?\n. @trink @rafrombrc What can I do to move this PR forward?\n. this can be closed then.\n. Create a PR, we'll discuss there.\nIt would be good to have an option to allow both ways.\n. @plb Heka tests sometimes fail on travis (I can reproduce this sometimes using docker on my local machine). Run the tests on your local machine to be sure.\n. Take a look at your ES logs. There is hopefully some usefull info.\nAlternatively, you can do a network capture, if you don't have too much bandwidth.\n. See also https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=805278#20\n. heka.service.in should probably be moved directly in packaging/ as it is not Debian specific. But I've not found how yet (and configure_file is not needed for this file).\n. superseeded by #1804\n. Notes :\nSee also https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=805278#20\nheka.service.in should probably be moved directly in packaging/ as it is not Debian specific. But I've not found how yet (and configure_file is not needed for this file).\n. Applied https://gist.github.com/sathieu/6e30382759f0ee0e1b65032c26ab7d10#file-heka1825-diff\nNow I'm getting (at start):\n```\nsudo -u heka hekad -config /etc/heka/conf.d/\n2016/05/11 16:39:30 Pre-loading: [ModSecuritySplitter]\n2016/05/11 16:39:30 Pre-loading: [ModSecurityDecoder]\n2016/05/11 16:39:30 Pre-loading: [ApacheAccessNantesInput]\n2016/05/11 16:39:30 Pre-loading: [ApacheAccessNantesDecoder]\n2016/05/11 16:39:30 Pre-loading: [ApacheAccessNantesSubDecoder]\n2016/05/11 16:39:30 Pre-loading: [ApacheErrorInput]\n2016/05/11 16:39:30 Pre-loading: [ApacheErrorDecoder]\n2016/05/11 16:39:30 Pre-loading: [ModSecurityInput]\n2016/05/11 16:39:30 Pre-loading: [auth_log]\n2016/05/11 16:39:30 Pre-loading: [LinuxAuthlogDecoder]\n2016/05/11 16:39:30 Pre-loading: [syslog]\n2016/05/11 16:39:30 Pre-loading: [LinuxSyslogDecoder]\n2016/05/11 16:39:30 Pre-loading: [GeoipRemote_addrDecoder]\n2016/05/11 16:39:30 Pre-loading: [TcpOutput]\n2016/05/11 16:39:30 Pre-loading: [ProtobufDecoder]\n2016/05/11 16:39:30 Loading: [ProtobufDecoder]\n2016/05/11 16:39:30 Pre-loading: [ProtobufEncoder]\n2016/05/11 16:39:30 Loading: [ProtobufEncoder]\n2016/05/11 16:39:30 Pre-loading: [TokenSplitter]\n2016/05/11 16:39:30 Loading: [TokenSplitter]\n2016/05/11 16:39:30 Pre-loading: [HekaFramingSplitter]\n2016/05/11 16:39:30 Loading: [HekaFramingSplitter]\n2016/05/11 16:39:30 Pre-loading: [NullSplitter]\n2016/05/11 16:39:30 Loading: [NullSplitter]\n2016/05/11 16:39:30 Loading: [ModSecurityDecoder]\n2016/05/11 16:39:30 Loading: [ApacheAccessNantesSubDecoder]\n2016/05/11 16:39:30 Loading: [ApacheErrorDecoder]\n2016/05/11 16:39:30 Loading: [LinuxAuthlogDecoder]\n2016/05/11 16:39:30 Loading: [LinuxSyslogDecoder]\n2016/05/11 16:39:30 Loading: [GeoipRemote_addrDecoder]\n2016/05/11 16:39:30 Loading: [ApacheAccessNantesDecoder]\n2016/05/11 16:39:30 Loading: [ModSecuritySplitter]\n2016/05/11 16:39:30 Loading: [ApacheAccessNantesInput]\n2016/05/11 16:39:30 Loading: [ApacheErrorInput]\n2016/05/11 16:39:30 Loading: [ModSecurityInput]\n2016/05/11 16:39:30 Loading: [auth_log]\n2016/05/11 16:39:30 Loading: [syslog]\n2016/05/11 16:39:31 Loading: [TcpOutput]\n2016/05/11 16:39:31 Starting hekad...\n2016/05/11 16:39:31 Output started: TcpOutput\n2016/05/11 16:39:31 MessageRouter started.\n2016/05/11 16:39:31 Input started: ApacheErrorInput\n2016/05/11 16:39:31 Input started: ModSecurityInput\n2016/05/11 16:39:31 Input started: auth_log\n2016/05/11 16:39:31 Input started: syslog\n2016/05/11 16:39:31 Input started: ApacheAccessNantesInput\n2016/05/11 16:39:32 Input 'ApacheAccessNantesInput' error: Unable to launch decoder 'ApacheAccessNantesDecoder'\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x4a3b7c]\ngoroutine 204 [running]:\ngithub.com/mozilla-services/heka/pipeline.(deliverer).Deliver(0xc209a0fb60, 0xc2081f7900)\n    /root/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:131 +0x2c\ngithub.com/mozilla-services/heka/pipeline.(sRunner).DeliverRecord(0xc209a414a0, 0xc209a42000, 0x14e, 0x2000, 0x7fabde8604f0, 0xc209a0fb60)\n    /root/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/splitter_runner.go:286 +0x367\ngithub.com/mozilla-services/heka/plugins/logstreamer.(LogstreamInput).deliverRecords(0xc208124a00, 0x0, 0x0)\n    /root/heka/build/heka/src/github.com/mozilla-services/heka/plugins/logstreamer/logstreamer_input.go:378 +0x486\ngithub.com/mozilla-services/heka/plugins/logstreamer.(LogstreamInput).Run(0xc208124a00, 0x7fabde857208, 0xc208044000, 0x7fabde856888, 0xc208076a00, 0xc209b84060, 0x7fabde8604f0, 0xc209a0fb60, 0x7fabde860528, 0xc209a414a0)\n    /root/heka/build/heka/src/github.com/mozilla-services/heka/plugins/logstreamer/logstreamer_input.go:316 +0x204\ncreated by github.com/mozilla-services/heka/plugins/logstreamer.(*LogstreamerInput).startLogstreamInput\n    /root/heka/build/heka/src/github.com/mozilla-services/heka/plugins/logstreamer/logstreamer_input.go:196 +0x291\n[...]\n```\n. Here is the relevant part of my config:\n```\n[ApacheAccessNantesInput]\ntype = \"LogstreamerInput\"\nlog_directory = '/var/log/apache2'\nfile_match = '(?P[^/]+)/access-(?P[^.-]+)(-(?P\\d{4})(?P\\d{2})(?P\\d{2}))?.log'\npriority = [\"Year\", \"Month\", \"Day\"]\ndecoder = \"ApacheAccessNantesDecoder\"\ndifferentiator = [\"apache_access-\", \"environment_appli\", \"-\", \"servername\"]\nsend_decode_failures = true\nlog_decode_failures = false\n[ApacheAccessNantesInput.translation.Year]\nmissing = 9999\n[ApacheAccessNantesDecoder]\ntype = \"MultiDecoder\"\nsubs = ['ApacheAccessNantesSubDecoder', 'GeoipRemote_addrDecoder']\ncascade_strategy = \"all\"\n[ApacheAccessNantesSubDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/apache_access.lua\"\n[ApacheAccessNantesSubDecoder.config]\ntype = \"apache_access_nantes\"\nuser_agent_transform = true\nuser_agent_keep = true\nlog_format = '%v %h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %I %O \\\"%{Location}i\\\" \\\"%{X-Forwarded-For}i\\\" %D %p \\\"%{SSL_PROTOCOL}x\\\" \\\"%{SSL_CIPHER}x\\\"'\n```\n. Setting this helps:\nsubs = ['ApacheAccessNantesSubDecoder'] # i.e removing GeoIP\nSo this may be related to geoip (or some threshold hit only when geoip is loaded)\n. I'll submit a patch to improve error reporting, with it I have:\n2016/05/12 11:36:50 Input 'ApacheAccessNantesInput' error: Initialization failed for 'ApacheAccessNantesDecoder': Non-existent subdecoder: GeoipRemote_addrDecoder\n(then panic)\n. One more step:\nInput 'ApacheAccessNantesInput' error: Initialization failed for 'ApacheAccessNantesDecoder': Non-existent subdecoder: GeoipRemote_addrDecoder (Initialization failed for 'GeoipRemote_addrDecoder': Could not open GeoIP database: %!s(MISSING)\n(oups!)\n. Now:\nInput 'ApacheAccessNantesInput' error: Initialization failed for 'ApacheAccessNantesDecoder': Non-existent subdecoder: GeoipRemote_addrDecoder (Initialization failed for 'GeoipRemote_addrDecoder': Could not open GeoIP database '/usr/local/share/geoip/GeoLiteCity.dat': Error opening GeoIP database ([/usr/local/share/geoip/GeoLiteCity.dat]): cannot allocate memory)\n. it looks like the geoip decoder is instantiated too many times.\n. I've implemented GeoIP flags: #1935. I hope it'll help.\n. Summary:\nI've implemented:\n- Improve error reporting when Decoder initialization failed (dev=#1934, 0.10=#1932)\n- Support GeoIP flags (dev=#1935, 0.10=TODO)\nI'm now using flags=\"GEOIP_MMAP_CACHE\", and it works.\nSo, I'm closing this bug. Please note that the panic is still here, but just after the error message which points to the solution.\n. Which Debian version are you using?\n. @YiuTerran What is the output of aptitude search libc6~i --display-format \"%p %v %V\"?\n. it looks like the build mixed things about multi-arch (libc6-amd64 is only available for i386 and x32).\nAs a workaround, try to build in a container with only the amd64 arch.\n. For reference, I can reproduce the issue.\nmake deb works well, until doing apt-get install libc6-amd64:i386 and make deb again.\nThis is probably a cpack issue (or debhelper, but it seems unlikely).\n. CMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES might be the cause:\n```\ncmake --system-information  | grep LINK_DI\nCMAKE_CXX_IMPLICIT_LINK_DIRECTORIES \"/usr/lib/gcc/x86_64-linux-gnu/4.9;/usr/lib/x86_64-linux-gnu;/usr/lib;/lib/x86_64-linux-gnu;/lib\"\nCMAKE_C_IMPLICIT_LINK_DIRECTORIES \"/usr/lib/gcc/x86_64-linux-gnu/4.9;/usr/lib/x86_64-linux-gnu;/usr/lib;/lib/x86_64-linux-gnu;/lib\"\nCMAKE_PLATFORM_IMPLICIT_LINK_DIRECTORIES \"/lib;/lib32;/lib64;/usr/lib;/usr/lib32;/usr/lib64\"\n```\nI stop my investigation here, not knowing enough cmake.\n. Actually, heka is linked against a lib in libc6-amd64:\n```\nldd bin/hekad\nlinux-vdso.so.1 (0x00007ffc0122f000)\nlibluasandbox.so.0 => /heka/build/heka/bin/../lib/libluasandbox.so.0 (0x00007f1c01687000)\nlibluasb.so.0 => /heka/build/heka/bin/../lib/libluasb.so.0 (0x00007f1c0144f000)\nlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f1c01244000)\nlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f1c00f43000)\nlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f1c00d26000)\nlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1c0097c000)\n/lib64/ld-linux-x86-64.so.2 (0x000055f77117a000)\n\n```\nSo, the problem is at link time probably.\n. Could not reproduce here:\n```\n$ git show HEAD\ncommit 08b95b002fe8f784c43bcb5d4d9b940bf7bddd78\nMerge: f5e03b0 6e09299\nAuthor: Rob Miller rob@kalistra.com\nDate:   Tue Feb 9 11:13:06 2016 -0800\nMerge branch 'Nitro-pr-dockerinput-plugin' into dev\n\n```\nCan you try to remove the \"build\" directory and run . build.sh then make deb.\nAlso what is you distro and version?\n. :+1: \n. ... or build from latest 0.10 branch (See #1920)\n. The dev branch has PatternGroupingSplitter (PR #1842).\nDoc at: https://github.com/mozilla-services/heka/blob/dev/docs/source/config/splitters/pattern_grouping.rst\n(not on readthedoc yet?)\n. @shijiant Can you close this issue then?\nNext time, send your questions to the mailing-list instead. Thanks\n. Hello @nickchappell ,\nThe parser should probably go to lua_sandbox, to be available to other tools.\n. Similar to https://github.com/mozilla-services/lua_sandbox/pull/125#issuecomment-208508939?\n. it is solved on lua_sandbox HEAD, but probably not on other branches.\n. @trink Can you backport the fixes?\n@vit1251: Probably, use an older compiler or cmake, i.e use an older distribution.\n. Can this be fixed on Heka 0.10 too? (or is it already?) Thanks\n. @rafrombrc Thanks! (for,  ref, this is c12752d96a)\n. Same as #1923, for v0.10\n. This doesn't solve the panic, but I least it helps.\n. @rafrombrc @trink: Can you merge this one (and #1934)?\n. Same as #1932 , for the dev branch\n. @trink, @rafrombrc : Can you merge this one? Should I propose a PR for the 0.10 branch too?\n. Have you tried journalctl -u heka.service (or hekad.service)?\n. Have you tried with another output (i.e to console +rstdecoder)? Are the duplicate here too?\n. You have reported this to https://mail.mozilla.org/pipermail/heka/2016-June/001085.html too.\nMaybe close this one then?\n. Can you post your config here?\nTry with applying #1932 which improves reporting. I had this error with geoip eating too much memory, which is solved with #1935 and flags=\"GEOIP_MMAP_CACHE\"\n. The prebuilt packages are not built with geoip apparently. Build them yourself: http://hekad.readthedocs.io/en/v0.10.0/installing.html#from-source\nAlso, please send support request to the mailing list instead (and close this issue). Thanks\n. When memory_limit or instruction_limit  is exceeded, the plugin is terminated with a message in the log.  Unless you have can_exit =true, this will terminate heka.\nThose are thus not related to your problem.\nPlease close this issue, and ask future questions to the Mailing-list (the issue tracker is not for support request).\nThanks\n. Why auditd.service?\n. Yes, we would use the msg.Payload, but we also need msg.Fields.programname.\nI've done this to preserve all metadata. I imagine in the long term a systemd-journal Input (#358) that could be used with the Syslog decoder.\n. I probably have missed something. May I explain how I understand:\nGiven the journald \"export\" format (http://www.freedesktop.org/wiki/Software/systemd/export/), the splitting and decoding phase are tight together. The to-be-done systemd-journal input would behave like Docker input plugins and fill both Fields and dynamic Fields.\nWe should keep fields (decode_message(read_message('raw'))) and dynamic fields (fields = msg.Fields) and append more fields (fields[k] = v).\n. @rafrombrc Thanks for taking the time to explain.\nI though, that read_message(\"raw\") would contain all the fields, encoded, event when not using MsgBytes.\nThis encoder is not intended to be used with MsgBytes, the idea is to get all the fields from the original message (i.e Payload, but also Pid, ... and dynamic fields). This could be usefull if the input fills them (i.e. the hypothetic systemd-journal input).\nAdditionally, the decoder should fail if the rsyslog grammar does not match.\nWill push an updated version.\n. I get it now. Thanks.\nWhat do you think about the newer version of the patch?\n. No. Just that I don't need it.\ni'll try to improve the patch.\n. Status is a string. I should use %s here.\n. @trink. Any alternative?\nI expect having fields like _SYSTEMD_CGROUP=/user.slice/user-1000.slice/session-1.scope here.\nIf this is not possible, I will make rsyslog_template mandatory for now and we'll see later on for systemd-journal integration\n. It could, but the memory impact is not minor.\nBut the patch can be reworked to only require 'syslog_message' and 'postfix' when needed.\n. ",
    "sekka1": "Thanks @nathwill that method of sending logs to kafka is working for me.\n. ",
    "sidnei": "It doesn't mean it's a great method though. :) If you block the goroutine, or simply messages come in faster than the goroutines can finish, the process will eventually be OOM killed because too many goroutines have been created.\nSee the examples in http://golang.org/doc/effective_go.html#channels\n. That's true. Filed a bug about it: https://code.google.com/p/go/issues/detail?id=6012\nOTOH, it's easy to fix that problem for an HTTP server, you just throw a frontend proxy in front of it and limit the number of concurrent requests. With a statsd server, not so much.\nMoreover, maybe the use of goroutines in this specific piece of code is unnecessary? It seems that the work that handleMessage performs is fairly trivial and there's probably more overhead in starting and stopping a goroutine than to just handle it inline.\n. Definitely. My goal is actually to compare it with my Twisted implementation at https://github.com/sidnei/txstatsd. :)\n. ",
    "KushalP": "What is an MVP for getting this working? My particular use case is webserver logs which usually exist in files like: /var/log/nginx/*.{access,error}.log. What would a good pull request for this solve?\nAdditionally, I'm slightly worried about tackling the date filename patterns as that feels like something the writer of the toml config should own and not heka.\n. @bbangert Excellent! Let me know any way I can help you with this.\n. Sorry, I just grabbed that out of my bash history. Glad to see you're able to reproduce :smile:.\n. @trink Thanks for that.\n@rafrombrc I know Mozilla has a long history with CMake/AutoMake, but is there any likelyhood of doing OS packaging in a way that the respective package maintainers envisioned?\nI'd be happy to take part in sorting debian packaging but editing CMake configs is a bit beyond me right now.\n. Fixed in 7500d2c.\n. Closing this as it was fixed by 9af88ec1fcc52723ea4745cbae82abf95b38b511.\n. ",
    "jaredhanson": "Related to #346\n. ",
    "adieu": "I did some debugging and found that it's due to a bug in whisper-go. Already sent a pull request at https://github.com/rafrombrc/whisper-go/pull/2 . We could fix this issue by pointing at the new commit after the PR got merged.\n. I have tested the latest code and the bug has been fixed. I'll close this ticket. I did some debugging for the aggregation bug. Will open another ticket for that.\n. It's been fixed :)\n. Finally found this issue after trying to convert fields extracted from regex to numbers for 2 hours. Would love to see this feature got implemented.\n. @Seldaek Thanks a lot for your code example. I'll apply it to my heka setup tomorrow.\n. @trink Thanks. I end up making a SandboxFilter with your example since I need PayloadRegexDecoder to extract data from the log. It is working nicely.\n. ",
    "fuhao715": "you also should export  $GOOS   $GOARCH like this\u00a3\u00ba\nexport GOROOT=$HOME/go\nexport GOARCH=amd64\nexport GOOS=linux\nexport PATH=.:$PATH:$GOBIN\nAt 2013-09-05 14:54:52,kindule notifications@github.com wrote:\nHi, all!\nWe use ubuntu13.04, and we install Go lang use source code under /usr/local/go\nand I set the path like this\nexport GOROOT=/usr/local/go\n export PATH=$PATH:$GOROOT/bin\ngo version\ngo version go1.1.1 linux/amd64\nafter finished install golang, And install heka got a error\nroot@squidloganalyzer:~/heka# ./build.sh\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:97 (message):\n  Could NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH) (Required is at\n  least version \"1.1\")\nCall Stack (most recent call first):\n  /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:291 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindGo.cmake:32 (find_package_handle_standard_args)\n  CMakeLists.txt:16 (find_package)\n-- Configuring incomplete, errors occurred!\nmake: *** No targets specified and no makefile found.  Stop.\nCan anyone tell me. How can I install heka the right way?\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. ",
    "ghost": "thanks I follow your idea add \nexport GOARCH=amd64\nexport GOOS=linux\nexport GOROOT=/usr/local/go\nexport PATH=$PATH:$GOROOT/bin\nbut it gave me the same error.\n. hi, \nBut I use . build.sh the same error occur.\n```\nroot@squidloganalyzer:~/heka# . build.sh\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:97 (message):\n  Could NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH) (Required is at\n  least version \"1.1\")\nCall Stack (most recent call first):\n  /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:291 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindGo.cmake:32 (find_package_handle_standard_args)\n  CMakeLists.txt:16 (find_package)\n-- Configuring incomplete, errors occurred!\nmake: *** No targets specified and no makefile found.  Stop.\n```\n. Hi, trink.\nWe use source code to build go, beacuse under the Ubuntu13.04, apt-get install go will be under 1.1.x\u3002\nAnd we install go to the directory /usr/local/go and add it to the PATH, we also add other environment. in the .bashrc\nOur cmake version \n```\ncmake --version\ncmake version 2.8.10.1\n```\nexport GOARCH=amd64\nexport GOOS=linux\nexport GOROOT=/usr/local/go\nexport PATH=$PATH:$GOROOT/bin\n. We met this problem too, and still can't not find a good way to fix it. \n. +1\n. ",
    "mattrco": "To add another data point, I could not resolve this issue while building with the Go version 'go1.2' (hg tag).\nI updated Go to tip and it builds fine. Not great for reproducible builds(!) but it is a reasonable workaround I guess.\n. Building the deb requires a separate step, cpack: http://hekad.readthedocs.org/en/latest/installing.html#creating-packages.\nOr is that command also erroring out?\n. Great. At some point I'll see if I can reproduce the build error in a clean environment.\n. Is this ok to merge for 0.4.0?\n. We'd certainly find this useful. A PR is on its way, unless someone else has already started work.\n. @thedrow Yes I'm working on it. In fact we were just discussing the design in IRC.\n. Quick update: this branch is mostly there. SeekInFile is the main thing that needs to be updated.\nThis is because we record the number of bytes read and use this to seek in the file later. That fails for gzip files where bytes read doesn't map directly to bytes in the file.\nThere are a couple of ways around this, just figuring out the simplest one.\n. Taking a different approach (and giving the branch a more conventional name), so going to close this off. New approach is at https://github.com/mattcottingham/heka/tree/feature/gzip-logstreamer.\n. I don't think it's possible to use go run to run hekad since there are various compile-time dependencies.\nHave you built the project using source build.sh? If so there will be a hekad binary in build/heka. You can run that using your arguments above.\nMore info here: http://hekad.readthedocs.org/en/latest/installing.html#from-source\n. Oh, I see. Do you mean it's easier because you don't have to recompile or is there another reason you prefer to run it that way?\nFor what it's worth, I tried running go oracle on some source files to understand the call graph but didn't have any luck.\n. Thanks for reviewing, I've added some commits.\n. The above PR should fix this -- it was attempting to format reports for plugin categories that didn't produce any.\n. What is the output of go env?\n. Heka is much more than a log shipper though.\nI think the current tagline is good, although it doesn't say anything about the type of data (operational) that it can process (of course, that might be on purpose).\n. When you say it reprocessed the entire logstream, did it reprocess the gzipped files too?\nIf it did, I'm having trouble replicating this exactly with similar input data (empty access.log, single line in access.log.1, subsequent gzipped files in sequence), but what it's doing is still unexpected.\nIn my case, only the non gzipped file containing a single line (access.log.1) gets reprocessed on every subsequent restart.\nThe journal content is:\n{\"seek\":7708,\"file_name\":\"/var/log/nginx/access.log.2.gz\",\"last_hash\":\"763e638b2c6543029491fdd730a29a4762156a1e\"}\nMaybe (haven't read the code recently) this is down to the 500 byte line buffer being file-specific (see #972) and affecting the journal being updated to reflect access.log.1 being processed. I'll have a look and try with some other file combinations.\nI should be able to take a closer look this week.\n. Taking a look at this, committer is responsible for writing to a given file in increments of up to 10000 bytes.\nIt's fairly simple to modify this code to respect a threshold and rotate when necessary. However, since the messages are decoded into bytes at this point, this wouldn't respect message boundaries and so a message may be split across two files.\nOne option I'm considering: don't decode the messages in receiver, and pass an array of messages instead of bytes over the channel to the committer. This would delay recycling the message pack (it'd need to be done when the message is written in committer).\nAnother option is that there might be a simple way of delimiting messages in the byte stream (I'm not familiar with the format), which could then be used to prevent splits across files.\n. I've approached this in #1322. Please take a look - this works for my use case, but I'm sure there are exchange/queue configurations that I haven't anticipated.\n. (Happy to update docs for this when/if it's merged)\n. Thanks for the review @rafrombrc, I've added docs.\n. Sounds good. I'm not sure why I didn't just do that :)\n. Thanks. From the use of a lock in other ReportMsg implementations I'd assumed concurrent calls to it was the most likely race. I'll update with a RWMutex.\n. ",
    "craigmj": "I encountered the same build error, and none of the suggested fixes worked. Eventually I fixed it by symlinking my go binary (/usr/local/go/bin/go) to /usr/bin/go:\nsudo ln -s /usr/local/go/bin/go /usr/bin/go\nAs far as I could work out, the Cmake build files seemed to always decide that go was in /usr/bin/go, even if there was no such file. The build now completes successfully, but the .deb isn't created.\nI'm using go 1.3 on Linux/amd64 Ubuntu 13.10. Now trying to work that out...\n. Nope - had missed that part. Thank you so much - working great for me now!\n. ",
    "carlosvega": "\nI encountered the same build error, and none of the suggested fixes worked. Eventually I fixed it by symlinking my go binary (/usr/local/go/bin/go) to /usr/bin/go:\nsudo ln -s /usr/local/go/bin/go /usr/bin/go\n\nAlso, I installed go this way, with gmv\nhttp://www.hostingadvice.com/how-to/install-golang-on-ubuntu/\n. ",
    "bzub": "I'm in need of functionality like this also in order to replace or suppliment tcollector (http://opentsdb.net/tcollector.html).  Aside from the OpenTSDB functionality of tcollector, I think it runs and reads from external processes in a way this ProcessInput could potentially emulate.\nHere's how it works at a high level:\n- Programs (Bash, Python, executables, etc) are placed in a directory named after the interval between being executed.\n- Programs under 0/ run constantly, and output intervals are handled within the program\n- Programs under N/ directory are run every N seconds\n- The stdout of all programs is captured and processed\n- Exit codes are watched to determine if a program is failing and should be disabled\n. Making a basic ProcessInput to extend upon is probably a good idea.  I might find time to work on these plugins (fingers crossed).\nJust to think out loud, I've been impressed with the functionality that this pipe package provides in running external programs: http://godoc.org/labix.org/v2/pipe\n. Here's what I've got so far: https://github.com/bzub/heka/commits/features/ProcessInput\nWhat Works\n- Run arbitrary programs/commands at configurable intervals.\n- Specify multiple commands in order to be piped together.\n- Parse output of pipes by token or regular expression (thanks to StreamParser).\n- Final results are loaded into Message payloads\nTODO\n- Test coverage\n- Documentation\n- Smarter Message Field values\nDesign Concerns\n- If multiple commands are specified they are run in parallel.\n  - Works as expected for commands that block on stdin (ex. \"dmesg\" | \"grep\" \"-i\" \"sda\")\n  - Allow the user to configure a ProcessInput to run the commands sequentially via config bool?\n- The command configuration option is a slice of string slices per Go's exec.Command API.\n  - This could be more friendly for end users.\n  - Suggest making this just a string and ProcessInput can separate out the arguments on initialization.\nKnown Bugs\n- Doesn't handle long-running commands currently.  They will be called on run_interval on top of each other.\n  - We could interpret a run_interval of \"0\" to mean the command gets called only once.\n  - Should we try to help manage misbehaving processes?\n  - Should hekad die when commands fail or pile up?\n. I ended up changing my version of ProcessInput quite a bit.\nOne big change is that it doesn't support piping multiple commands together.  The added complexity wasn't justified for the convenience -- folks can do piping logic in a script to be run by ProcessInput.  Also, I implemented a timeout condition which can be configured to cause a plugin failure.  Users are expected to use the plugin restart parameters to manage failure/restart behaviour.\n@Seldaek I don't have code in there to send a special message on command failure, although I think it could now be feasibly implemented.  I did, however, add the ability to send stderr as messages just like stdout.\n. Now that I re-read this issue @rafrombrc mentioned piping commands together to help out Windows users.  That wasn't working well in my original version so I scrapped it for simplicity.  It shouldn't be too hard to implement in the PR code, though.\n. ",
    "rata": "I'm using heka in kubernetes and I'd like to use a configmap in different projects so they can create some local heka configurations.\nThe problem with this is that I want some basic config already there and some other config via de configmap. A configmap can be mounted in a directory, but heka on start only reads one directory.\nSo, it's not possible to do this right now. If subdirectories are also read, or several \"-config\" flags can be passed, that will work for me.\n@rafrombrc: What do you think? Is a valid use case? Or is there any other work-around that I haven't thought?\n. @rafrombrc: I'm using a hack now, that is to mount it in some other place and when the container starts, copy the file to the heka directory and then heka is started. But if I can specify two directories or the dir specified is read recursively in the directories there, it will be way more clean.\nI think there are valid use cases where several directories are needed.\n. ",
    "Meai1": "Just downloaded tdm_gcc again and installed it as 32bit+64bit, but still something very similar: _time32(0): not defined\n. I'll close this because someone else opened a new one with more detailed info: https://github.com/mozilla-services/heka/issues/443\n. I like this more: heka - piping logs from anywhere to anywhere\n. ",
    "tarasglek": "Also, it would be good to use megs/gigs over showing scientific notation of KB, eg 6.94e+6\n. ",
    "jeromer": "Thanks for your feedback. I'll do the changes tomorrow morning Europe/Paris timezone.\n. ",
    "markmelling": "This is still crashing for me.\nPulled down latest, using config of\n```\n[debug]\ntype = \"LogOutput\"\nmessage_matcher = \"TRUE\"\n[UdpInput]\naddress = \":4881\"\nmessage_matcher = \"TRUE\"\nparser_type = \"token\"\ndelimiter = \"\\n\"\n```\nThen ran nc and entered \"hello\"\nnc -u 127.0.0.1 4881\nhello\nThis caused hekad to crash.\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x28 pc=0x480f00]\ngoroutine 19 [running]:\ngithub.com/mozilla-services/heka/pipeline.networkPayloadParser(0xc2000f3e40, 0xc2000008d0, 0xc20012e6c0, 0xc20012c9a0, 0xc2000ea870, ...)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/inputs.go:240 +0x3d0\ngithub.com/mozilla-services/heka/pipeline.(UdpInput).Run(0xc200131370, 0xc2000ea870, 0xc2001371c0, 0xc200131460, 0xc2000b1c00, ...)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/inputs.go:382 +0x21d\ngithub.com/mozilla-services/heka/pipeline.(iRunner).Starter(0xc2001371c0, 0xc200131460, 0xc2000b1c00, 0xc2000b1ce0)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/inputs.go:126 +0x184\ncreated by github.com/mozilla-services/heka/pipeline.(*iRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/inputs.go:107 +0xb8\ngoroutine 1 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.Run(0xc2000b1c00)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:680 +0xed6\nmain.main()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/cmd/hekad/main.go:139 +0x915\ngoroutine 2 [syscall]:\ngoroutine 4 [syscall]:\nos/signal.loop()\n    /usr/local/go/src/pkg/os/signal/signal_unix.go:21 +0x1c\ncreated by os/signal.init\u00b71\n    /usr/local/go/src/pkg/os/signal/signal_unix.go:27 +0x2f\ngoroutine 5 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 6 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 7 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 8 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 9 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 10 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 11 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 12 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7034()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:129 +0xf2\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/decoders.go:143 +0xf9\ngoroutine 13 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(LogOutput).Run(0xc200000740, 0xc2000ea750, 0xc2000f8c00, 0xc200131460, 0xc2000b1c00, ...)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/outputs.go:82 +0x80\ngithub.com/mozilla-services/heka/pipeline.(foRunner).Starter(0xc2000f8c00, 0xc200131460, 0xc2000b1c00, 0xc200153880)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:425 +0x60a\ncreated by github.com/mozilla-services/heka/pipeline.(*foRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:389 +0x99\ngoroutine 14 [finalizer wait]:\ngoroutine 15 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7050()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:243 +0xaf\ncreated by github.com/mozilla-services/heka/pipeline.(*MatchRunner).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:279 +0x8c\ngoroutine 16 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*DiagnosticTracker).Run(0xc200124810)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:173 +0xc7\ncreated by github.com/mozilla-services/heka/pipeline.Run\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:661 +0x9fd\ngoroutine 17 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(*DiagnosticTracker).Run(0xc200124840)\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:173 +0xc7\ncreated by github.com/mozilla-services/heka/pipeline.Run\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:662 +0xa17\ngoroutine 18 [select]:\ngithub.com/mozilla-services/heka/pipeline.func\u00b7048()\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:100 +0x834\ncreated by github.com/mozilla-services/heka/pipeline.(*messageRouter).Start\n    /home/mark/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:174 +0x64\n```\n. Sorry Rob, my fault, I hadn't.\nNow when I do a build I get an error:\n./build.sh\n-- sphinx-build was not found, the documentation will not be generated.\nSubmodule 'docs/source/_themes/mozilla' () registered for path\n'docs/source/_themes/mozilla'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mark/heka/build\n[ 20%] Built target heka-mozsvc-plugins\n[ 20%] Built target whisper-go\n[ 20%] Built target go-uuid\n[ 20%] Built target gomock\n[ 20%] Built target go-simplejson\n[ 20%] Built target raw\n[ 20%] Built target amqp\n[ 20%] Built target go-notify\n[ 20%] Built target toml\n[ 20%] Built target goprotobuf\n[ 20%] Built target slices\n[ 20%] Built target xmlpath\n[ 20%] Built target g2s\n[ 20%] Built target gospec\n[ 20%] Built target sets\n[ 20%] Built target goamz\n[ 20%] Built target GoPackages\n[ 20%] Built target lua-cjson-2_1_0\n[ 20%] Built target lpeg-0_12\n[ 20%] Built target lua-5_1_5\n[ 20%] Built target heka_source\n[ 20%] Built target message_matcher_parser\n[ 20%] Built target mocks\ngithub.com/mozilla-services/heka/sandbox/lua\n/tmp/go-build394740442/\ngithub.com/mozilla-services/heka/sandbox/lua/_obj/lua_sandbox_private.o: In\nfunction require_library':\nheka/src/\ngithub.com/mozilla-services/heka/sandbox/lua/lua_sandbox_private.c:1027:\nundefined reference toluaopen_cjson_safe'\ncollect2: ld returned 1 exit status\nmake[2]: * [CMakeFiles/hekad] Error 2\nmake[1]: * [CMakeFiles/hekad.dir/all] Error 2\nOn 16 October 2013 16:49, Rob Miller notifications@github.com wrote:\n\nAre you sure you've managed to deploy the latest code from dev? You're\ncrashing at the same place, but there's now an explicit nil check right in\njust that spot, see c5b4a1chttps://github.com/mozilla-services/heka/commit/c5b4a1c959fe6ed7034ca47dd53937db8f4bb5e6.\nI was able to successfully reproduce the original problem locally and have\nverified that it's no longer happening for me, so I'm pretty sure that the\nissue has been resolved.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26430478\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n. OK, that's better, thanks.\nI can now send data over udp and see that appearing in hekad\nThis is probably something I'm doing wrong, but interestingly, although I\ncan use \"nc\" to send data, it's not quite working as expected when\nforwarding messages using rsyslog.\nSo when I forward syslog messages over tcp and have a config of:\n[TcpInput]\naddress = \":4881\"\nmessage_matcher = \"TRUE\"\nparser_type = \"token\"\ndelimiter = \"\\n\"\nThen everything is good, syslog messages appear in hekad\nBut when I switch to udp with a config of:\n[UdpInput]\naddress = \":4881\"\nmessage_matcher = \"TRUE\"\nparser_type = \"token\"\ndelimiter = \"\\n\"\nI don't see anything in hekad (having checked that they are being sent over\nudp from rsyslog). If I change the delimiter to some other character (that\nI know will be in the syslog messages), then I will see syslog messages in\nhekad. Seems strange that it would work over tcp with a \"\\n\" delimiter, but\nnot with udp.\nOn 16 October 2013 17:08, Rob Miller notifications@github.com wrote:\n\nAh, right, that's b/c there was a change (unrelated to your issue) to\nthe Lua bindings that require a 'make clean' before the next build\nattempt.\nYou can either run 'make clean' from the build folder, or you can\ndelete the build folder entirely and run \"source build.sh\" from the\nheka checkout root.\nOn Wed 16 Oct 2013 10:03:06 AM MDT, Mark Melling wrote:\n\nSorry Rob, my fault, I hadn't.\nNow when I do a build I get an error:\n./build.sh\n-- sphinx-build was not found, the documentation will not be generated.\nSubmodule 'docs/source/_themes/mozilla' () registered for path\n'docs/source/_themes/mozilla'\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mark/heka/build\n[ 20%] Built target heka-mozsvc-plugins\n[ 20%] Built target whisper-go\n[ 20%] Built target go-uuid\n[ 20%] Built target gomock\n[ 20%] Built target go-simplejson\n[ 20%] Built target raw\n[ 20%] Built target amqp\n[ 20%] Built target go-notify\n[ 20%] Built target toml\n[ 20%] Built target goprotobuf\n[ 20%] Built target slices\n[ 20%] Built target xmlpath\n[ 20%] Built target g2s\n[ 20%] Built target gospec\n[ 20%] Built target sets\n[ 20%] Built target goamz\n[ 20%] Built target GoPackages\n[ 20%] Built target lua-cjson-2_1_0\n[ 20%] Built target lpeg-0_12\n[ 20%] Built target lua-5_1_5\n[ 20%] Built target heka_source\n[ 20%] Built target message_matcher_parser\n[ 20%] Built target mocks\ngithub.com/mozilla-services/heka/sandbox/lua\n/tmp/go-build394740442/\ngithub.com/mozilla-services/heka/sandbox/lua/_obj/lua_sandbox_private.o:\nIn\nfunction require_library':\nheka/src/\ngithub.com/mozilla-services/heka/sandbox/lua/lua_sandbox_private.c:1027:\nundefined reference toluaopen_cjson_safe'\ncollect2: ld returned 1 exit status\nmake[2]: * [CMakeFiles/hekad] Error 2\nmake[1]: * [CMakeFiles/hekad.dir/all] Error 2\nOn 16 October 2013 16:49, Rob Miller notifications@github.com wrote:\n\nAre you sure you've managed to deploy the latest code from dev? You're\ncrashing at the same place, but there's now an explicit nil check\nright in\njust that spot, see\nc5b4a1c<\nhttps://github.com/mozilla-services/heka/commit/c5b4a1c959fe6ed7034ca47dd53937db8f4bb5e6\n.\nI was able to successfully reproduce the original problem locally\nand have\nverified that it's no longer happening for me, so I'm pretty sure\nthat the\nissue has been resolved.\n\u2014\nReply to this email directly or view it on\nGitHub<\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26430478>\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n\u2014\nReply to this email directly or view it on GitHub\n<\nhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26431683\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/issues/462#issuecomment-26432211\n.\n\n\nMark Melling\nSavage Minds Limited\n+447967 662 674\nmark.melling@savageminds.com\n. ",
    "nchapman": "Thanks @trink. I'll work on fixing the organization issues and try to identify any API changes. Could you provide me with a current example of a termination report?\nhere is one: http://people.mozilla.org/~mtrinkala/heka/heka_sandbox_termination.html\n. I believe all of the above mentioned missing functionality has been added. @trink, would you mind testing again to see if it looks better?\n. ",
    "sebastianschuler": "Okay, I will (re)implement the requested changes. I would like to add the possibility for the user to change the expression from default AND to OR in the config. Any idea how to implement this in TOML? Something like this but more intuitive:\nflush_interval = 1000\nflush_count = 5\nflush_operator = OR    // AND, &&, ||\n. I have already implemented the needed changes to the code, but I have no experience in writing tests. If you see value in this pull request, I will try to implement all notes. But it will take some time, about 2-3 weeks since I'm on a lot of other things right now.\n. Oh s..., I'm sorry those message are actually debug messages. Didn't know that this commit would be included immediately into the pull request.\n. You're probably right. Although this config is misleading since what it does is then effectively \"OR\", so maybe spit an error with suggestions for OR during startup?\n. Need the timer structure for the channel timer.C later, even if flushInterval is not used (to block). Could be circumvented by using a extra variable for the channel.\n. ",
    "tdavis": "I think perhaps my issue is related to this, so I'll drop a comment here instead of creating a new issue...\nI've noticed that creating messages in Go differs from Lua in terms of what types you can use. For instance:\nstuff := []string{\"a\", \"b\", \"c\"}\nf := msg.NewField(\"stuff\", stuff, \"List of stuff\")\nmsg.AddField(f)\nDoesn't really work as I'd imagine since later calling GetValueString() gives me something like []string{\"<[]string Value>\"}. Calling that same method on a field from a message created via the Lua sandbox (inject_message({..., Fields={stuff={\"a\", \"b\", c\"}}})) gives me the expected []string{\"a\", \"b\", \"c\"}.\nBy necessity my pipeline mixes filters, decoders, etc. written in both Lua and Go, so I end up having to manually split and join fields or risk it mattering which language produced the message.\n. Ah, thank you @trink.\n. Okay, so I forgot that the C types are \"namespaced\" at build time so instead of creating my own package named \"lua\" my final patch to lua_sandbox.go.in looks like this:\n``` diff\ndiff --git a/sandbox/lua/lua_sandbox.go.in b/sandbox/lua/lua_sandbox.go.in\nindex 50e8d92..6101725 100644\n--- a/sandbox/lua/lua_sandbox.go.in\n+++ b/sandbox/lua/lua_sandbox.go.in\n@@ -683,6 +684,26 @@ func CreateLuaSandbox(conf *sandbox.SandboxConfig) (sandbox.Sandbox, error) {\n    return lsb, nil\n }\n+func CreateLuaSandboxCustom(conf sandbox.SandboxConfig, tpl string) (sandbox.Sandbox, error) {\n+   lsb := new(LuaSandbox)\n+   lsb.sbConfig = conf\n+   cs := C.CString(conf.ScriptFilename)\n+   defer C.free(unsafe.Pointer(cs))\n+   ccfg := C.CString(tpl)\n+   defer C.free(unsafe.Pointer(ccfg))\n+   lsb.lsb = C.lsb_create_custom(unsafe.Pointer(lsb), cs, ccfg)\n+   if lsb.lsb == nil {\n+       return nil, fmt.Errorf(\"Sandbox creation failed\")\n+   }\n+   lsb.injectMessage = func(p, pt, pn string) int {\n+       log.Printf(\"payload_type: %s\\npayload_name: %s\\npayload: %s\\n\", pt, pn, p)\n+       return 0\n+   }\n+   lsb.config = conf.Config\n+   lsb.globals = conf.Globals\n+   return lsb, nil\n+}\n+\n func (this LuaSandbox) Init(dataFile string) error {\n    csDataFile := C.CString(dataFile)\n    csPluginType := C.CString(this.sbConfig.PluginType)\n```\nCertainly much safer than extern'ing a bunch of unsafe fields and now my package doesn't require cgo either :+1: \n. I'm perfectly happy with Lua, I just wish the Sandbox plugin APIs were a little more granular or easy to extend. For instance, SandboxFilter.Init() (or any SandboxFoo.Init(); they overlap considerably) contains the logic for how to get something satisfying Sandbox, mixed in with other initialization logic. This requires that I copy all of sandbox_filter.go into my project and register something like SandboxFilter2 instead of registering a simple factory that would produce structs satisfying Sandbox for my own script type (call it \"lua2\"). See also: SandboxManagerFilter which requires the same level of duplication because it expects the filter type to be \"SandboxFilter\" or it just gives up. (Sadly, plugin_loader imports heka's packages after mine, so I can't just register over SandboxFilter)\nI think the high reliance on Lua for the runtime exacerbates the issue for me. I want to implement as much of my filtering and so forth in Lua as possible, but the lack of any ability to customize which methods and functions are \"banned\" makes it difficult. If the interfaces were a bit more formalized and decoupled from the guts, doing what I need to do to keep using Lua would be much more convenient.\n. ",
    "whd": "I have seen this issue. I believe it only affects RHEL-based systems with rpm version higher than 4.8.0 (e.g. Fedora 19, Amazon Linux). It does not affect RHEL6 or the Services AMI, which run rpm version 4.8.0. It is a packaging bug. The heka package should not declare these system directories itself. This has been in the RPM spec for years but only in the recent versions of RPM is it enforced.\n. This is related to #550. My guess from the docs is that heka is supposed to implicitly set the decoder to ProtoBufDecoder, but doesn't. If heka did indeed set the decoder, you wouldn't have seen this error.\nIf you are required to set the decoder explicitly, it would be helpful if heka failed early in this circumstance (like it does currently for e.g. TcpInput) instead of succeeding to establish the AMQP connection but failing when a message is actually received.\n. I experience a similar issue with other inputs. For the example in the docs:\n[TcpInput]\naddress = \":5566\"\nparser_type = \"message.proto\"\ndecoder = \"ProtobufDecoder\"\nif I omit the decoder I get:\nInitialization failed for 'TcpInput': The message.proto parser must have a decoder.\nThe docs state \"A ProtobufDecoder will be automatically setup if not specified explicitly in the configuration file\", so I'm guessing this is either a bug not specific to a particular input, the docs are wrong, or my understanding is flawed.\n. We've not seen this error in the last six months, so I think it's safe to close.\n. @trink it appears to be a race condition with multiple FileOutputs. I can reproduce with the following config:\n```\n[FileOut1]\ntype = \"FileOutput\"\nmessage_matcher = \"FALSE\"\npath = \"/tmp/test/1.log\"\nformat = \"protobufstream\"\n[FileOut2]\ntype = \"FileOutput\"\nmessage_matcher = \"FALSE\"\npath = \"/tmp/test/2.log\"\nformat = \"protobufstream\"\n```\nwhile true ; do kill -HUP PID_OF_HEKA ;  done will then cause the error, which is exacerbated by the number of FileOutputs. I've also reproduced the same error on heka 0.5.1.\n. I have rebased and added an entry to CHANGES.txt.\n. This was actually both AmqpOutput and TcpOutput. TcpOutput was feeding a heka running AmqpOutput. The AmqpOutput heka was borked and would not shut down cleanly, but I didn't notice this until after circus had killed the TcpOutput heka. It seems likely the AmqpOutput heka misbehaving prevented the TcpOutput heka from exiting cleanly either.\n. :+1: \n. This still happens occasionally but I haven't been keeping track of when or how often. The process on cron is Python opening the log file in append mode.\nI'll make a concerted effort to track this down if it happens again. In theory it should be very simple to reproduce. I've also updated heka to 0.7.2 on the box. If it doesn't happen in the next week or so I'd say it's safe to close this issue.\n. Confirmed still happening. I'll work on a simple test to reproduce this.\n. It turns out the logstreamer was failing to read every other line from the file. However, I was never able to reproduce and it started working a few weeks ago (reason unknown), so I'd say it's safe to close this.\n. :+1: \n. I did a bit more poking around and found if I bypass the joined channel check (adding an || true to this line) I can get output to a protected IRC channel, but as ircMsg.IrcChannel appears to be the user-provided string the key gets prepended to every IRC message I send to the protected channel.\n. Should be fixed now.\n. Probably not, as https://github.com/golang/go/issues/7978 was fixed some time ago. Closing.\n. :+1: \n. :+1: I've confirmed the RPM builds and installs correctly on CentOS 7 (when built with CMake 3).\n. :+1:\n. :+1: \n. This has been tested pretty heavily and appears to work well :+1:. I think it's ready for code review and merging (after the merge conflict is fixed).\n. ",
    "rfk": "Just confirming that this is indeed working when I add \"decoder=ProtobufDecoder\" in the config\n. ",
    "gregglind": "https://hekad.readthedocs.org/en/latest/installing.html\nCMake 2.8.7 or greater http://www.cmake.org/cmake/resources/software.html\nGit http://git-scm.com/download\nGo 1.1 or greater (1.1.1 recommended) http://code.google.com/p/go/downloads/list\nMercurial http://mercurial.selenic.com/downloads/\nProtobuf 2.3 or greater (optional - only needed if message.proto is modified) http://code.google.com/p/protobuf/downloads/list\nSphinx (optional - used to generate the documentation) http://sphinx-doc.org/\nSo, brew install cmake go git mercurial protobuf protobuf-c  ?   Then sphinx needs to come via python (gross!)\n. ",
    "macbre": "After updating cmake/externals.cmake it now fails at mock_pluginhelper_test.go\n``\nHEAD is now at a64295d... Add cbufd grammar module\n[ 82%] Performing update step (git fetch) for 'luasandbox-0_1_0'\nHEAD is now at a64295d... Add cbufd grammar module\n[ 82%] No patch step for 'luasandbox-0_1_0'\n[ 83%] Performing configure step for 'luasandbox-0_1_0'\nNot searching for unused variables given on the command line.\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/macbre/src/heka/build/ep_base/Build/luasandbox-0_1_0\n[ 83%] Performing build step for 'luasandbox-0_1_0'\n[ 25%] Built target lua-5_1_5\n[ 51%] Built target lpeg-0_12\n[ 77%] Built target lua-cjson-2_1_0\nScanning dependencies of target luasandbox\n[ 80%] Building C object src/CMakeFiles/luasandbox.dir/lua_sandbox.c.o\n[ 83%] Building C object src/CMakeFiles/luasandbox.dir/lua_sandbox_private.c.o\n[ 87%] Building C object src/CMakeFiles/luasandbox.dir/lua_serialize.c.o\n[ 90%] Building C object src/CMakeFiles/luasandbox.dir/lua_serialize_json.c.o\n[ 93%] Building C object src/CMakeFiles/luasandbox.dir/lua_serialize_protobuf.c.o\n[ 96%] Building C object src/CMakeFiles/luasandbox.dir/lua_circular_buffer.c.o\nLinking C static library libluasandbox.a\n[ 96%] Built target luasandbox\nScanning dependencies of target test_lua_sandbox\n[100%] Building C object src/test/CMakeFiles/test_lua_sandbox.dir/test_lua_sandbox.c.o\nLinking C executable test_lua_sandbox\n../../ep_base/lib/liblua.a(loslib.c.o): In functionos_tmpname':\nloslib.c:(.text+0x48): warning: warning: tmpnam() possibly used unsafely; consider using mkstemp()\n[100%] Built target test_lua_sandbox\n[ 84%] Performing install step for 'luasandbox-0_1_0'\n[ 25%] Built target lua-5_1_5\n[ 51%] Built target lpeg-0_12\n[ 77%] Built target lua-cjson-2_1_0\n[ 96%] Built target luasandbox\n[100%] Built target test_lua_sandbox\nInstall the project...\n-- Install configuration: \"release\"\n-- Installing: /home/macbre/src/heka/build/heka/modules\n-- Installing: /home/macbre/src/heka/build/heka/modules/rfc5424.lua\n-- Installing: /home/macbre/src/heka/build/heka/modules/cbufd.lua\n-- Installing: /home/macbre/src/heka/build/heka/modules/rfc3339.lua\n-- Installing: /home/macbre/src/heka/build/heka/lib\n-- Installing: /home/macbre/src/heka/build/heka/lib/liblpeg.a\n-- Installing: /home/macbre/src/heka/build/heka/lib/libcjson.a\n-- Installing: /home/macbre/src/heka/build/heka/lib/liblua.a\n-- Installing: /home/macbre/src/heka/build/heka/include\n-- Installing: /home/macbre/src/heka/build/heka/include/lauxlib.h\n-- Installing: /home/macbre/src/heka/build/heka/include/lua.h\n-- Installing: /home/macbre/src/heka/build/heka/include/luaconf.h\n-- Installing: /home/macbre/src/heka/build/heka/include/lualib.h\n-- Installing: /home/macbre/src/heka/build/heka/lib/libluasandbox.a\n-- Installing: /home/macbre/src/heka/build/heka/include\n-- Installing: /home/macbre/src/heka/build/heka/include/lua_sandbox.h\n-- Installing: /home/macbre/src/heka/build/heka/include\n-- Up-to-date: /home/macbre/src/heka/build/heka/include/lauxlib.h\n-- Up-to-date: /home/macbre/src/heka/build/heka/include/lua.h\n-- Up-to-date: /home/macbre/src/heka/build/heka/include/luaconf.h\n-- Up-to-date: /home/macbre/src/heka/build/heka/include/lualib.h\n-- Installing: /home/macbre/src/heka/build/heka/lib/test_lua_sandbox\n[ 84%] Completed 'luasandbox-0_1_0'\n[ 84%] Built target luasandbox-0_1_0\nScanning dependencies of target heka_source\n[ 85%] Built target heka_source\nScanning dependencies of target message_matcher_parser\n[ 85%] Built target message_matcher_parser\nScanning dependencies of target mocks\n[ 86%] Built mock_pluginhelper_test.go\ncan't load package: import \".\": import relative to unknown directory\n2013/12/12 21:02:38 Loading input failed: exit status 1\n*** [heka/src/github.com/mozilla-services/heka/pipeline/mock_pluginhelper_test.go] Error code 1\nStop in /usr/home/macbre/src/heka/build.\n*** [CMakeFiles/mocks.dir/all] Error code 1\nStop in /usr/home/macbre/src/heka/build.\n*** [all] Error code 1\nStop in /usr/home/macbre/src/heka/build.\n```\n. ",
    "skurfuerst": "Hi @all,\nI am trying to build Heka on Freebsd 10.0, and I am currently running into the following error (on master):\n```\n[ 32%] No update step for 'lpeg-0_12'\n[ 35%] Performing patch step for 'lpeg-0_12'\nHmm...  Looks like a unified diff to me...\nThe text leading up to this was:\n\n|diff -Naur lpeg-0.12.orig/lptree.c lpeg-0_12/lptree.c\n|--- lpeg-0.12.orig/lptree.c    Fri Apr 12 09:31:19 2013\n|+++ lpeg-0_12/lptree.c Fri Aug  2 10:15:43 2013\n\nPatching file lptree.c using Plan A...\nHunk #1 succeeded at 1217.\nHmm...  The next patch looks like a unified diff to me...\nThe text leading up to this was:\n\n|diff -Naur lpeg-0.12.orig/CMakeLists.txt lpeg-0.12/CMakeLists.txt\n|--- lpeg-0.12.orig/CMakeLists.txt  Wed Dec 31 16:00:00 1969\n|+++ lpeg-0.12.orig/CMakeLists.txt  Fri Aug  2 09:26:42 2013\n\n(Creating file CMakeLists.txt...)\nPatching file CMakeLists.txt using Plan A...\nEmpty context always matches.\nHunk #1 succeeded at 1.\ndone\n[ 38%] Performing configure step for 'lpeg-0_12'\nNot searching for unused variables given on the command line.\n-- The C compiler identification is Clang 3.3.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Build/lpeg-0_12\n[ 41%] Performing build step for 'lpeg-0_12'\nScanning dependencies of target lpeg\n[ 25%] Building C object CMakeFiles/lpeg.dir/lpcap.c.o\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lpcap.c:9:\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lpcap.h:9:\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lptypes.h:13:9: warning: 'NDEBUG' macro redefined\ndefine NDEBUG\n    ^\n\n:1:9: note: previous definition is here\ndefine NDEBUG 1\n    ^\n\n1 warning generated.\n[ 50%] Building C object CMakeFiles/lpeg.dir/lpcode.c.o\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lpcode.c:12:\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lptypes.h:13:9: warning: 'NDEBUG' macro redefined\ndefine NDEBUG\n    ^\n\n:1:9: note: previous definition is here\ndefine NDEBUG 1\n    ^\n\n1 warning generated.\n[ 75%] Building C object CMakeFiles/lpeg.dir/lptree.c.o\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lptree.c:14:\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lptypes.h:13:9: warning: 'NDEBUG' macro redefined\ndefine NDEBUG\n    ^\n\n:1:9: note: previous definition is here\ndefine NDEBUG 1\n    ^\n\n1 warning generated.\n[100%] Building C object CMakeFiles/lpeg.dir/lpvm.c.o\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lpvm.c:13:\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lpcap.h:9:\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lpeg-0_12/lptypes.h:13:9: warning: 'NDEBUG' macro redefined\ndefine NDEBUG\n    ^\n\n:1:9: note: previous definition is here\ndefine NDEBUG 1\n    ^\n\n1 warning generated.\nLinking C static library liblpeg.a\n[100%] Built target lpeg\n[ 44%] Performing install step for 'lpeg-0_12'\n[100%] Built target lpeg\nInstall the project...\n-- Install configuration: \"release\"\n-- Installing: /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/lib/liblpeg.a\n[ 47%] Completed 'lpeg-0_12'\n[ 47%] Built target lpeg-0_12\nScanning dependencies of target lua-cjson-2_1_0\n[ 50%] Creating directories for 'lua-cjson-2_1_0'\n[ 52%] Performing download step (download, verify and extract) for 'lua-cjson-2_1_0'\n-- downloading...\n     src='http://www.kyne.com.au/~mark/software/download/lua-cjson-2.1.0.tar.gz'\n     dst='/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Download/lua-cjson-2_1_0/lua-cjson-2.1.0.tar.gz'\n     timeout='none'\n-- [download 1% complete]\n-- [download 18% complete]\n-- [download 100% complete]\n-- downloading... done\n-- verifying file...\n     file='/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Download/lua-cjson-2_1_0/lua-cjson-2.1.0.tar.gz'\n-- verifying file... done\n-- extracting...\n     src='/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Download/lua-cjson-2_1_0/lua-cjson-2.1.0.tar.gz'\n     dst='/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lua-cjson-2_1_0'\n-- extracting... [tar xfz]\n-- extracting... [analysis]\n-- extracting... [rename]\n-- extracting... [clean up]\n-- extracting... done\n[ 55%] No update step for 'lua-cjson-2_1_0'\n[ 58%] Performing patch step for 'lua-cjson-2_1_0'\nHmm...  Looks like a unified diff to me...\nThe text leading up to this was:\n\n|diff -Naur lua-cjson-2.1.0.orig/CMakeLists.txt lua-cjson-2_1_0/CMakeLists.txt\n|--- lua-cjson-2.1.0.orig/CMakeLists.txt    Wed Nov  6 19:36:00 2013\n|+++ lua-cjson-2_1_0/CMakeLists.txt Tue Nov 12 11:17:18 2013\n\nPatching file CMakeLists.txt using Plan A...\nHunk #1 succeeded at 1.\nHunk #2 succeeded at 10.\nHunk #3 succeeded at 44.\ndone\n[ 61%] Performing configure step for 'lua-cjson-2_1_0'\nNot searching for unused variables given on the command line.\n-- The C compiler identification is Clang 3.3.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Looking for isinf\n-- Looking for isinf - found\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Build/lua-cjson-2_1_0\n[ 64%] Performing build step for 'lua-cjson-2_1_0'\nScanning dependencies of target cjson\n[ 33%] Building C object CMakeFiles/cjson.dir/lua_cjson.c.o\nIn file included from /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lua-cjson-2_1_0/lua_cjson.c:47:\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lua-cjson-2_1_0/fpconv.h:15:20: warning: inline function 'fpconv_init' is not defined [-Wundefined-inline]\nextern inline void fpconv_init();\n                   ^\n/home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/Source/lua-cjson-2_1_0/lua_cjson.c:1359:5: note: used here\n    fpconv_init();\n    ^\n1 warning generated.\n[ 66%] Building C object CMakeFiles/cjson.dir/strbuf.c.o\n[100%] Building C object CMakeFiles/cjson.dir/fpconv.c.o\nLinking C static library libcjson.a\n[100%] Built target cjson\n[ 67%] Performing install step for 'lua-cjson-2_1_0'\n[100%] Built target cjson\nInstall the project...\n-- Install configuration: \"release\"\n-- Installing: /home/ry63/heka/build/ep_base/Build/lua_sandbox/ep_base/lib/libcjson.a\n[ 70%] Completed 'lua-cjson-2_1_0'\n[ 70%] Built target lua-cjson-2_1_0\nScanning dependencies of target luasandbox\n[ 73%] Building C object src/CMakeFiles/luasandbox.dir/lua_sandbox.c.o\n[ 76%] Building C object src/CMakeFiles/luasandbox.dir/lua_sandbox_private.c.o\n[ 79%] Building C object src/CMakeFiles/luasandbox.dir/lua_serialize.c.o\n/home/ry63/heka/build/ep_base/Source/lua_sandbox/src/lua_serialize.c:126:7: error: generic selections are a C11-specific feature [-Werror,-Wc11-extensions]\n  if (isnan(d)) {\n      ^\n/usr/include/math.h:118:2: note: expanded from macro 'isnan'\n        __fp_type_select(x, __inline_isnanf, __inline_isnan, __inline_isnanl)\n        ^\n/usr/include/math.h:86:39: note: expanded from macro '__fp_type_select'\ndefine __fp_type_select(x, f, d, ld) _Generic((x),                     \\\n                                  ^\n\n1 error generated.\n*** Error code 1\nStop.\nmake[5]: stopped in /var/home/ry63/heka/build/ep_base/Build/lua_sandbox\n*** Error code 1\nStop.\nmake[4]: stopped in /var/home/ry63/heka/build/ep_base/Build/lua_sandbox\n*** Error code 1\nStop.\nmake[3]: stopped in /var/home/ry63/heka/build/ep_base/Build/lua_sandbox\n*** Error code 1\nStop.\nmake[2]: stopped in /var/home/ry63/heka/build\n*** Error code 1\nStop.\nmake[1]: stopped in /var/home/ry63/heka/build\n*** Error code 1\nStop.\nmake: stopped in /var/home/ry63/heka/build\n```\nI currently don't really know what do do with this... Does anybody have any hints?\nGreets & thanks,\nSebastian\n. OK, FYI: I got it to work now, using:\n- GCC instead of Clang (that was the error above)\n- the latest master (v0.5.1 was still broken)\nGreets, Sebastian\n. :+1: :-)\n. ",
    "michaellihs": "For those of you who don't know how to change your compiler to gcc (like me), here are the commands that worked for me:\nCC=/usr/local/bin/gcc47\nexport CC\nsource build.sh\n. ",
    "mmerickel": "My expectation was that the parsed payload on the input side would not include the delimiter. It is pretty standard practice (in my experience) that when splitting things based on a delimiter, the delimiter disappears.\npython\nassert 'foo\\nbar'.split('\\n') == ['foo', 'bar']\n. From #heka about this:\n[14:27:48]  trink:   raydeo: in something like syslog the trailing linefeed it is part of the template format specification so it allows everything to work as-is\n[14:28:17]  raydeo:  I guess https://github.com/mozilla-services/heka/issues/596 is the issue\n[14:29:31]  trink:   yup, but with the new encoders the user has control over it\n[14:30:20]  raydeo:  but if I were writing a generic filter, I wouldn't expect certain messages to have a newline.. that's an encoder's decision to make about the transfer format\n[14:30:36]  raydeo:  syslog is a transport protocol, not something that belongs in the parsed payload\n[14:31:06]  trink:   template = '%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n'\n[14:31:27]  trink:   I consider that part of the message\n[14:33:19]  raydeo:  it's only part of the message because that's how heka is creating the message atm\n[14:33:25]  trink:   otherwise we would have guess if end of the template was actually a delimiter\n[14:33:45]  trink:   and couldn't use the rsyslog configuration directly\n[14:34:37]  raydeo:  when I read specs on rsyslog format they say messages are delimited by newlines, not that a newline is part of the message\n[14:35:41]  trink:   how about this one then: <%PRI%>%TIMESTAMP% %HOSTNAME% %syslogtag:1:32%%msg:::sp-if-no-1st-sp%%msg%\n[14:36:19]  trink:   anyhow *you* can do whatever works for you, that is the beauty of it\n[14:38:28]  raydeo:  seems like mixing transfer formats into your message format to me, but okay\n[14:40:59]  trink:   just trying to make it easier on the user from a config point of view. i.e., same sting in the puppet for rsyslog/heak config\n. I can confirm that @rafrombrc's original example, using the regexp parser works correctly.\ntoml\n[hello_heka_input_log]\ntype = \"LogfileInput\"\nlogfile = \"/tmp/input.log\"\nparser_type = \"regexp\"\ndelimiter = '\\n'\nAll I can say is that this was very surprising behavior to me. Considering all the other refactorings happening atm I'm not sure historical reasons is the best argument. However the goal of being able to paste configurations from other services (like the rsyslog one) into the heka is definitely noble.\n. So it sounds like the FileOutput should support a delimiter parameter (defaulting to \\n as it is now), and it should strip any duplicate delimiters from the encoded message prior to writing. Does that sound reasonable?\n. Should a heka user be created, as well, that the daemon can drop privileges to? The following as-simple-as-it-gets upstart script works fine for me right now on Ubuntu 14.04 LTS. Minor details are that I'm creating the heka user and adding it to the \"adm\" and \"syslog\" groups to allow it to parse most logfiles from /var/log.\n```\ndescription \"heka logging daemon\"\nstart on filesystem\nstop on [!2345]\nrespawn\npre-start script\n    [ -d /var/cache/heka ] || mkdir /var/cache/heka\n    chown heka:heka /var/cache/heka\nend script\nexec sudo -u heka hekad -config=/etc/heka\n```\n. I'm attempting to avoid writing certain log files to disk and thus shipping them through syslog and directly out over the network is something I need. Unfortunately I'm not a go developer yet so hopefully someone else tackles this but I may find the time. Thanks for the response.\n. The only thing preventing me from doing the dual stack is that I really would like to use heka for the full logging stack. If it's an add-on it's much less compelling from a management perspective.\n. Sorry, I'm editing this comment as part D is a generic summary of A and B.\nFrom what I'm gathering here, it is not a goal of heka to be a full stack logging replacement but rather an endpoint/aggregator/analyzer of other logging systems. This is fine but disappointing. I was not expecting this much resistance to the proposed feature. I had already pulled down and built heka's source in an attempt to start figuring it out myself but now I can see the feature is not something that is of interest to the team.\n. @rafrombrc your +1 is the first voice of support for the feature in this thread - everything else has been workarounds. :-) The issue is there are components that use syslog (postfix, sudo/auth) and in these cases it is not possible to make heka the only component in the logging stack without dumping sensitive data to disk (user email addresses in postfix for example). I must configure rsyslog to dump the messages into heka right now and if I have to continue configuring rsyslog (while very painful) then heka becomes much less compelling for me (more hops, more points of failure, more configuration). In that scenario I'd use heka only on the central aggregator, with its syslog tcp input.\nAnyway I'll see if I can make some progress on the feature. It'll be a good excuse for me to play around with go!\n. ",
    "russellballestrini": "@mmerickel yes, I agree, the delimiter should be consumed on the \"output\" side.\n. Will checkout latest and verify\n. I really wish I put my test case in this issue report...\nI tested this morning with a new test case and it didn't crash, this was a hekad built from the latest dev branch, so at the very least this issue is resolved in development.\nI'm going to try my new test case on master branch to see if the issue happens there.  I will update with details or close if issue has been resolved.\n. Closing this issue, I am no longer able to reproduce because the original test case was not documented or the issue was resolved.\n. This issue is similar to: https://github.com/mozilla-services/heka/pull/595\n. I will take a stab at the tests and the submit pull to 0.5 branch, I'm very new to Go so I might need some hand holding.  I will look into this until after work today.\n. Sorry guys, I'm stuck and I don't know how to write a new tests case for testing the default delimiter.  I ended up making the existing test case support this use case.\n. : /    I assume the test is waiting forever for a \"submit\" or \"EOL\" delimiter... \nIt worked on my box ...\nUPDATE: on further inspection, it seems my commits didn't didn't cause the Travis CI build to fail. \n. ",
    "jhawk28": "Updated commit with the recommended changes.\n. ",
    "magnusp": "Also got bit by this when decoding with nginx_access.lua. \"Fixed it\" in my case by modifying the script to expose the double value as a string.\n. I profiled the view and believe that the lag is due to a lot of events firing when the Backbone collection is merged with the parsed termination report. From what I gather, Backbone doesn't really like collections with a huge amount of models.\nLooking at #501, perhaps something like Backgrid.js would work better? This would allow for pagination and sortable tables. Another solution would be to use a raw array, as the linked BB issue suggests.\n. Yeah this was a while ago :) I think it's part I wanted to do some form of reporting/alerting and part that it would be a nice thing to have in general (boolean, append or overwrite). Didn't actually know that the DashboardOutput could be used like that, awesome!\n. Thanks for reviewing! As suggested, I changed it around so that the pidfile is specified in the configuration instead of a commandline parameter (in favor of #594). I hope I nailed everything regarding documenting the changes.. think I got the gist of it down in docs/source/config/index.rst but should probably be worded differently.\n. ",
    "levb": "This will not build until mozilla-services/heka-mozsvc-plugins#21 is merged\n. Added the SHA for https://github.com/mozilla-services/heka-mozsvc-plugins/pull/21\n. ok, stay tuned\n. k, I took care of the in-line comments, will do the other changes in the next few days\n. I updated CHANGES.txt and the documentation. Not sure what to do for the test, as none exist now, and not sure how to test the external command's execution.\n. I didn't get to implement the test for reconnecting over TCP when a persistent connection is broken, it is a little involved; if the code is not entirely obvious I can pull it out, I mostly need the UDP support which I did add the test for.\n. Rob, on a separate note, does Carbon even need to be an output, or should there be a CarbonFilter that pipes the payload to properly configured TCP/UDP (or file, even) outputs? CarbonFilter can then be richer, accept multiple input formats, maybe have a built-in cbufd aggregation?\n. Can this path be made explicitly configurable, too?  We have a specific convention where such data should be, and would be nice to just point (and yes, to run as non-root)\n. This is \n  a) on purpose - the defer fires for the error return paths, but I need to Close explicitly to force send_nsca process completion before Wait()\n  b) safe; see http://golang.org/src/pkg/os/exec/exec.go#L390\n. ",
    "bracki": "What's the plan here? An init script for each distro?\n. @mikn Feel free to enhance. Basically we would need Debian packaging for Squeeze and Wheezy and also Ubuntu flavours. \n. ",
    "nickchappell": "+1\n. +1\n. Different project and codebase/language, but Cernan from Postmates may be of interest: https://github.com/postmates/cernan\nLots of similar ideas, even down to taking in Lua scripts, though I'm sure the interface the script has to implement or conform to is different. May still be worth your time to take a look at it though.. +1\n. @davidbirdsong @dmuth @dewrich I chatted with @trink and @rafrombrc in #heka (the Mozilla one, not freenode) a few days ago about this. Updating Heka to use the newest lua_sandbox will probably take care of this. The newer versions of the sandbox have https://github.com/edenhill/librdkafka included, and librdkafka supports both Kafka 0.8 and 0.9 features (consumer groups, the new consumer API in 0.9, etc.).\nThere's a Bugzilla issue for updating Heka's lua_sanbox: https://bugzilla.mozilla.org/show_bug.cgi?id=1262555\n. What sort of messages does this emit?\n. Would it be possible to parse out the JSON data from the payload and put it into the Heka message fields so it's more accessible by other filter and output plugins (like the statgraph filter or InfluxDB or Carbon outputs, for example)?\nI'm not a maintainer and don't have merge access, just throwing out some ideas I'd find useful.\n. Pinging @trink ...\nMight be good to summarize our conversation on IRC a few days ago re: Kafka and Heka\n. Heka will soon get a newer version of the Lua sandbox that includes a C Kafka library, librdkafka: https://github.com/edenhill/librdkafka\nIt supports all of the 0.8 and 0.9 features, including the high-level balanced consumer for v0.9 brokers.\nI don't know the timing of when the newer sandbox will get rolled in though, or if it will be in a 0.11 release or be held back until 0.12.\n@rafrombrc @trink: care to comment/elaborate?\n. @trink should I close this PR and open one up against https://github.com/mozilla-services/lua_sandbox/ ?\n. @trink should the refactor to make the grammar a module wait until then as well, or should I do that now? If it should be modular-ized now, are there any examples of existing modules I should follow?\n. Would you have to write another Lua script that calls the modular-ized BIND decoder in order to actually use the grammar?\n. @trink I addressed the issues you mentioned in the inline comments, minus the hostname grammar refactor, as you mentioned that would be taken care of in #129. Do you want me to squash all of the commits?\n. @trink @rafrombrc Commits have been squashed.\n. @trink anything else needed so this can be merged?\n. @trink any updates?\n. @trink @rafrombrc Any update on getting this merged?\n. Hi @trink,\nIt should already be modular-ized. I think you mentioned earlier that this could be merged into Heka now and then added to the new sandbox at some point later.\n. Thanks!\n. @rafrombrc I made the recommended changes. On the doc issue, I removed the redundant example usage from the block comment in the Lua script, but will Sphinx pull in all of the block comments from the Lua file or just the first block comment?\n. @rafrombrc I've made the requested changes to the error message and the rst/Lua docs. Do you want me to squash all of the commits before this can be merged?\n. @rafrombrc would you like a PR against the https://github.com/mozilla-services/lua_sandbox repo to add this there as well?\n. This is superseded by #1978.\n. This is superseded by #1978.\n. Once the other things are cleaned up, could we merge this as-is with my custom hostname grammar, then modify it to use the generic hostname grammar after https://github.com/mozilla-services/lua_sandbox/issues/129 is merged?\n. Also, I started down the path of breaking the hostname into fragments so I could get the name and query domain out of the log lines as separate values (for us, having the domain be a separate searchable field is valuable).\n. How would that work with the multiple record types? A l.C\"A\" and so on for each record type?\n. Done.\n. Done.\n. Is this moot if the script has been turned into a module?\n. I didn't find any examples to follow in the Heka codebase but found one in src/test/lua/lpeg_clf.lua of the lua_sandbox: https://github.com/mozilla-services/lua_sandbox/blob/master/src/test/lua/lpeg_clf.lua\nif not fields then\n  error(\"no result\", 2)\nend\nWhat is the intended meaning of the expected return value? The number of fields that were expected?\n. ",
    "tommyvn": "If I'm remembering correctly I tested with 500 lines from an nginx log file for each append, so well above 500 bytes, and still saw the problem but I can check again tomorrow.\n. If you run thru the steps in the issue description with your dev branch do you see the same lines processed in step 5 as you did in step 3?\n. ",
    "mostlygeek": "StackDriver doesn't have the ability to link custom cloudwatch metrics to your instance. It is able to link the common ones that EC2 provides out of the box but not something special. \nStuffing straight into SD will lets us push datapoints that are either ec2 instance or service specific. So for example we can push a user-authenticated datapoint from all of our app servers, which will get aggregated globally, as well as node specific ones like storage-node-used-capacity. \n. +1 on @rafrombrc's suggestion. It'd be fine if have a way of just pushing some data to an HTTP endpoint. Most of them these days are JSON with an API key that we need to send with the request. \n. we don't need this anymore\n. ",
    "Micheletto": "We should also consider adding the ability to add event notation from Heka: http://feedback.stackdriver.com/knowledgebase/articles/260455-sending-annotation-events-to-stackdriver\n. +1\n. ",
    "bartleusink": "I've updated CHANGES.txt and added some tests\n. Thanks! I've added a note to the bug fix section of the changelog.\n. Oops! I've moved the entry to the 0.6 section.\n. Hi Rob,\nUnfortunately that doesn't work since the current code uses filepath.Join which calls filepath.Clean which in turn replaces all forward slashes with backslashes on Windows because it calls filepath.FromSlash.\n. ",
    "hynd": "I had a stab at this with https://github.com/hynd/heka-tsutils-plugins/blob/master/statsd/statsd_decoder.go (there's an accompanying Lua Filter for aggregating)\nWould that be the kind things you had in mind as a starting point?  What else would you like to see it do? (maybe the Metric/Value/Modifier/Sampling Field names should be configurable?)\n. Ah, that makes sense.\nI just have a bunch of filters that generate new messages (deduped/modified versions of ones coming from Inputs), and have to duplicate some of the top level fields (like Logger and Hostname) into dynamic Fields to avoid losing info.\n. Works good on ProcessInputs where ticker_interval = 0, though see #1412\n. Per #1418, this changes the behaviour of ProcessDirectoryInput retries.\n. Have another PR on the way that bundles this up with a couple of other tweaks.\n. Doh, fixed.  Tidied a couple of other comments i'd put in the code itself too.\n. Will reopen against versions/0.9\n. The tweak in PR #1572 (Travis is failing, though it seems unrelated?) seemed to fix it for me.  I was a little worried that it might affect other plugins like TcpInput (though that seems to work OK too)...\n. The optional/opt-in-ness was in case some already use Heka just as a proxy for compressed HTTP payloads to another upstream (happy to change the behaviour!)\n. Println adds a space between each operand anyways - they were getting two spaces after the colon, which is what made them stand out against all the other startup messages.\n. ",
    "dctrwatson": "Sounds good. I'll take care of those as well.\n. ",
    "ipmb": "I'm seeing this on other servers now. The number of CPUs was a red herring.\n. This might be a networking issue within docker and/or virtualbox. I'm closing it because I'm not sure it's valid.\n. ",
    "mreid-moz": "That makes sense about persistence through restarts. How about a \"~/.heka_cache\" dir? Then it could still run as any user.\n. @thedrow changing the default was just a suggestion - I just found it an annoyance that more steps were required after installing the package if you want to run as a non-root user.\nIf it created a heka user, created the necessary dirs, and used suid on the heka binary as @rafrombrc suggested, that would suit me fine. \nI just don't want to have to do extra manual setup to run heka as a regular user.\n. I've updated the approach here, now the code keeps track explicitly of whether we've reached the end of the stream. We also continue returning any well-formed records until EOF, so you should not need to use the GetRemainingData() function unless you have partial message bytes at the end of the stream and you want to do something with them.\n. See updated PR from the correct branch here:\nhttps://github.com/mozilla-services/heka/pull/1305\n. I removed ResetStream() - it's not necessary, as calling GetRemainingData() achieves the same thing.\n. Looks good to me. Up to you whether it's worthwhile adding a function to compute average (and avoid div by 0)\n. r+\n. CHANGES.txt updated.\n. I updated the RemoteAddr and Host fields to remove the port info, since it doesn't seem useful, and could be cumbersome for users of these fields to have to parse out the IP.\n. Why not log this fatal error anymore?\n. Consider introducing an \"avg\" utility function to save duplicate code.\n. I'm not sure if it makes sense to arbitrarily set this to \"1\", but it seemed preferable to leaving it blank.\n. ",
    "arnaudbriche": "Issue has been fixed with 0.6 release.\n. ",
    "graydon": "ES is the case I've seen. Possibly reproduction requires an ES host that isn't online at all, not responding to packets, or even not resolvable in DNS. Situation arose when an EC2 instance was stopped.\n. ",
    "carlosdp": "This seems out of scope for a logging service. The service that handles routing output in that way would ideally be service-aware and be able to add/remove nodes dynamically. Using an actual load balancer is probably the right way to go imo, and there is a plethora of great applications that serve this purpose these days.\n. go run does actually recompile though. It's just a go build that also runs it. It should be trivial to make a bash script to run ./build.sh and run hekad if it is so desired.\n. Resolved by #888, I didn't use an auto-close keyword =/\n. Alright! I added the type coercion checks. TcpInput will return an error while TcpOutput will print and error and continue on its way. Does that look good?\n. On my shell (zsh), it's actually the opposite. source works but . will not.\n. Disregard last, I got confused =P\n. We could just print a warning that KeepAlive won't be set instead and skip it? If someone is debugging, they should see it and if they don't, KeepAlive probably isn't too critical anyways.\n. ",
    "brokenjacobs": "This makes sense to me, and explains why it didn't feel right to do this parsing outside of rsyslog.lua\n. Since we are in RFC 5424 land here... Any chance on changing APP-NAME to be PRINTUSASCII instead of Alphanumeric? I have some processes with '_' characters in their names that aren't being parsed by the decoder.\n. So is the issue here that rsyslog doesn't seem to care if a log message from the default template \"%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n\" actually contains a tag meeting the specification? Because the values definitely show up in the log file as such.\n. Please see pull request #781 \n. I'll make those changes now and submit a new request. I'm not super familiar with Go so its good to know about the camel case convention.\nDo you require all pulls to be rebased?\n. Thanks! Curse you duplicated identifiers!\n. After a restart the messages have gone away. I don't have any errors on the TcpInput side which seems odd.\n. I assume by data types you mean elastic search mappings?  ipv4 -> elastic search 'ip type' for instance? I've been looking into using elastic search mapping templates for this, but doing it in the plugin would be much cleaner. Looking forward to this improvement.\n. Is there any way to use heka's field representations to emit as elastic search Mappings? Specifically I have a lot of firewall logs being stored in elastic search, and it would be great if fields matching the ipv4 representation in heka (set from the lua decoder) ended up as 'ip' type fields in elastic search instead of 'string'. \n. I will open one. I also ran into an interesting issue with cjson in the lua sandbox and null fields. I'll open one for that too.\n. The mappings are per index, so you would need to fire the mapping output every time the index rotated. (Daily in my case). I guess you could build a filter that tracked schema changes and emit mappings when that occurred?\nThere may be a way to get there by using a dynamic_template geared towards heka representations. Similar to this gist https://gist.github.com/deverton/2970285 \nAlthough the only way I see to do it would be to append heka representations to field names, unless I am missing something.\n. this is a decoder I am talking about here, using cjson.... Not on the encoder side.  The failure is in a decoder using inject_message with a lua table (Msg representation).\n. Opening a separate issue for cjson.\nhttps://github.com/mozilla-services/heka/issues/889\n. Thank you for this, this solves a huge problem for me.\n. We use a system called diamond to push these stats via statsd.\nhttps://github.com/BrightcoveOS/Diamond\nOtherwise you could use one of the input decoders Heka has now, or a seperate system like collectd.\n\nOn Apr 4, 2015, at 12:16 AM, Tianxiang Chen notifications@github.com wrote:\nLike CPU/memory/disk, etc.\nIf we use heka agent on the server, what's the recommended way to collect system metric like cpu/mem/disk?\nThanks!\n\u2014\nReply to this email directly or view it on GitHub.\n. Good catch!\n. \n",
    "davidreuss": "Also see #580 (although that's not the error you would get when trying to build heka currently).\nI'm not sure if this is the proper fix - or why the different files for setting up constants is there in the first place (they're all the same).\nBut this makes the build work properly.\n. ",
    "jrgm": "Oh, actually, interesting:\nI was running sudo strace -f -tt -p $hekapid and hekad does not have permission to read the nginx access.log:\n[pid  2263] 01:53:39.658537 open(\"/media/ephemeral0/nginx/logs/access.log\", \nO_RDONLY|O_CLOEXEC) = -1 EACCES (Permission denied)\n. I see that @ckolos setup the log rotation in https://github.com/mozilla-services/puppet-config/commit/0911a18c4719c9483b898a27994b778ced584a66#diff-e0f83fe24a2b6c181b470d681d6a6741\nPermission in https://github.com/mozilla-services/puppet-config/blob/master/fxa/modules/fxa/files/nginx-vhost-logrotate#L8 exclude hekad from reading.\nSo, I guess not a heka bug, per se.\n. I filed https://github.com/mozilla-services/puppet-config/issues/343 to fix permissions for logrotate.\nLeaving this open to ponder what hekad should do (if anything) when tripped up this way.\n. ",
    "cxtuttle": "I think may actually be an issue in the StatAccumulator.  I believe the accumulator processes all messages as if they had current time.  To fix the issue the accumulator would need to bucket by stat and some combination of timestamp and sampling rate.\n. ",
    "undying": "+1\nSame problem. We are collecting data from nginx access logs and sending this data to InfluxDB.\nWhen Heka begins to parse log file, there is a spikes on graphs, becase it sends data from current moment without considering timestamp of the message. This is the reason of incorect statistics.\nThis problem leads to impossibility of parsing old log files and building graphs by them.\n. Sorry, that's my issue, looks there is no problem (\u30b7_ _)\u30b7\n. ",
    "satyrius": "+1\n. ",
    "westurner": "IIUC, there are a few supported patterns:\nA. Source -> Syslog -> Disk -> Heka\n1. Write messages to [r]syslog (/dev/log (logger))\n2. Syslog writes messages to disk\n3. Heka: Input: read auditable disk-backed logfiles with e.g. Logstreamer [1][2]\n4. Heka: Decode\n5. Heka: Filter\n6. Heka: Output\nB. Source -> Syslog -> UDP/TCP -> Heka\n1. Write messages to to [r]syslog (/dev/log (logger))\n2. Syslog pipes messages through network stack to UDP/TCP\n3. Heka: Input: receive UDPInput/TCPInput [3][4]\n4. Heka: Decode\n5. Heka: Filter\n6. Heka: Output\nThe requested pattern is:\nC. Source -> UNIX DGRAM -> (/dev/log) -> Heka\n{?}\nThe suggested pattern is:\nD. Source -> Heka [6]\n[1] http://hekad.readthedocs.org/en/latest/config/inputs/index.html#logstreamer-input\n[2] http://hekad.readthedocs.org/en/latest/pluginconfig/logstreamer.html#logstreamerplugin\n[3] http://hekad.readthedocs.org/en/latest/config/inputs/index.html#udpinput\n[4] http://hekad.readthedocs.org/en/latest/config/inputs/index.html#tcpinput\n[5] #462 (TCPInput)\n[6] http://hekad.readthedocs.org/en/latest/config/inputs/index.html\n. Here's an upstart script: https://gist.github.com/augieschwer/6066421\nSystemd would probably also be useful.\n. Thanks!\n. Configuration modified, I am still seeing:\n2014/04/21 08:46:16 Initialization failed for 'lua_sandbox': Init() cannot open /usr/share/heka/lua_filters/sandbox.lua: No such file or directory\n. (A \"These docs are for version: x.y.z\" could be helpful)\n. Where can i find lua_filters/sandbox.lua in the source?\n. Ah, got it. So I would write in an absolute path to a custom .lua script there. Thanks again!\n. Thanks!\n. ",
    "victorcoder": "+1 upstart script would be nice to have also\n. :heart: \n. @trink yep, this is more like a quickfix, I don't know lua well but I'll try to add the check.\nBTW Travis doesn't loves me ;)\n. @trink this works like a charm, in my local tests at least, thx!\n. Also I vote to introduce this fix in 0.6 and 0.7 stable branches\n. @trink agree, I will be fixing my input JSON stream\n. ",
    "mikn": "Hello!\nWe're right now evaluating Heka and we're a debian shop. This patch is pretty much what is lacking right now, is there anything I could do to move this issue forward? :)\n. This is a great addition to the heka toolkit! Though, a couple of additions would be awesome, I think.\n1. The ability to set the series name based on fields on the received events through a simple sprintf or similar. Such as series = \"servers.%{received_from}.%{application}.%{entry_type}\"\n2. Blacklist columns. If you're able to blacklist columns that would serve no purpose in influxdb, you both save space and crowdedness in the columns list.\nThese two together would mean you could use one encoder setting definition to virtually all of the event types flowing through Heka that you actually want to end up in InfluxDB. Right now, and correct me if I'm wrong, you would have to set up different encoders depending on which filters they went through just so that you could specify which series they should end up.\n. I guess this issue can be closed now? :)\n. Ah, thank you. Very grateful for the testing. debhelper should definitely not be a hard dependency for just building the project, I have removed that now (I hope). Sorry about that!\nThe second error is very odd. But I think I have fixed it now. Seems assigning variables to command line arguments make them shell-escaped.\nIt seems the dependency on libc6 >= 2.14 is calculated by CPack, so it doesn't need to be explicitly defined, so I could remove that completely.\nI don't have tons of time for testing, so if anyone volunteers to test the upstart and systemd jobs I would be very happy as well. :)\n. Just curious if there's any thoughts, complaints or wishes regarding this PR? If there's anything more you need me to do I'd be more than happy to oblige!\n. Ah! A minimum viable config is worth a lot! I'll definitely set that up! I really like the idea of having a folder as conf instead as well, I had no idea that was possible. I would like to do something similar to how apache do it though with modules- and sites-available/ being symlinked to modules- and sites-enabled/.\nAccording to Debian standards example configuration shouldn't go into the \"live\" folder in /etc/, but rather live in /usr/share/heka/examples/ or similar.\nI'll survey how much work it would be to set up the apache-style configuration. If I'm too lazy I'll put up a /etc/heka/conf.d/ (Debian idiom) and copy the example configuration to /usr/share/heka/examples/\nI'll set the stderr-redirect up too, of course! I'm not yet using Heka, so I'm really glad you're helping out testing it properly. :)\n. Decided that the symlink thing probably was a bit of an overkill, so I created a folder for /etc/heka/conf.d/ which is the primary configuration folder for the init script. I copied your example files to /usr/share/heka/examples/ with an install (I don't think the rpm-people would mind) and during the installation I copy the example to the conf.d/ folder if there is no such directory already. If you want to add more examples you can add them to the examples/conf/ folder and if they should actually go into the default configuration you need to add them to the debian/heka.postinst.in-file as well.\nI hope that feels like an okay setup. :)\n. Oh, right. Worth mentioning that I use a parameter for start-stop-daemon called --no-close which is not supported in Ubuntu 12.04, the previous LTS. It is however supported in both debian releases squeeze and wheezy (I only tried in wheezy, but eager googling indicated that it should work in squeeze too) and the other workaround is to exec a shell, which with the recent ShellShock exploit didn't feel quite appropriate...\n. @rafrombrc Ah, good catch with that I missed moving the upstart and systemd configuration files to the same configuration format, that was rather silly. Fixed that now. You're also right that the .example is redundant, so I removed that part.\nBut the configuration file was renamed when copied for two reasons (so I'd like to keep it) first is to give it a number and thus a priority and the second is to remove .example. So, the rename looked like hekad.toml.example => 00-hekad.toml\nThe reason I add the number in the copy, and the fact that it is a named copy of the file is that it seems undesireable to have to prioritise the (hopefully) many example files between each other as they may overlap in purpose and/or in general just be contradictory to run in the same setup.\nJust now pushed a patch to address these issues. :)\n. Ah, it was not my intention to seem defensive. What I was trying to explain was the reason I thought it made sense to keep the name without \"00-\" in the examples/conf/ folder, but add those numbers during the copy step in the packaging, pre-empting another potential issue that may seem odd or so. But yes, you did initially suggest adding numbers before the name of course. :)\nI apologise if I seem like I don't appreciate your efforts, it is quite to the contrary. Keep up the good work!\n. Ah, yes. I see what you mean!\nThe reason I put the examples in /usr/share/heka/examples was because you're not really adviced to keep examples in the /etc/ folder because of some \"magic\" that debian applies to that folder. (Reference to the magic: https://www.debian.org/doc/debian-policy/ap-pkg-conffiles.html)\nBut I do right now script the copy of the file rather than let the debian package process handle it, which is less than ideal (it means that most of that magic does not apply), but that's something you may want to change in the future, I guess!\nCopying in all examples from the examples/conf/ is not a bad idea, of course. But it may make it more confusing as well if you want to show off competing heka configurations perhaps?\nIs it ok to do the install into /usr/share/heka/examples/ and do a copy from there like I've done?\nI can do the rename to add the numbers in the examples-folder, of course - if you want me to. :)\n. Ah, that is a nuisance! I copied the upstart script from the previous pull\nrequest and hoped it would work. I do not have a running Ubuntu system\nready to test on I'm afraid. I did however try both the init.d and systemd\nscripts... If someone feels like debugging the upstart script that would be\nvery appreciated! Otherwise I'll set up a Ubuntu VM in a couple of days\n(rather swamped right now) and try it out. :)\n. Ah, I'm sorry if I gave the impression that I think you should replace CMake, quite the contrary. According to my understanding CMake is an excellent choice for a platform independent build system. The problem is that the build as it is at the moment is rather messy as I interpret it. I am however no build expert at all, but Heka is the first project I've had to source a custom bash script to build rather than type something to the effect of \"mkdir build && cd build && cmake ..\"\nThe output of cmake is difficult to make sense of (what is in the build directory) before CPack actually builds the package. Though, the main confusion may only be the fact that both the built and the sources for the external dependencies end up next to the files that will actually get packaged in build/heka/ rather than just under build/.\nThe separation of concerns between CMake and CPack is rather vague in how it is configured right now as far as I can tell in summary.\nI can appreciate the sentiment of keeping the packaging for Debian or any other distros within the repository to a minimum viable product, you're a fast moving project with a small team and in general you should be ready to accept new features as something you can maintain yourselves in the long run. This is just healthy reasoning for any open source project.\nI will dig deeper into the depths of CMake and CPack and see if I can make more sense of it in relation with Heka and your requirements from them.\nIf I understand correctly, this is pretty much what you wish to have:\n1. Quick binary build with no need to rebuild all external dependencies.\n2. Provide a Good Enough(tm) packaging for as many platforms as possible with minimum effort.\nI would add:\n3. Ability to choose between dynamically linked and statically linked dependencies.\nAnd possibly:\n4. Allow for the build of source packages, which in turn can generate binary packages on build servers.\nThough, I haven't been involved enough in the project to tell if these are ambitions suited for where the project is at the moment.\nI also hope I'm not stepping on anyone's toes here...\nIn any case! Thank you again for spending time on Heka, it is quite nice indeed. I really think it is \"the next step\" in environment monitoring to have both logs and metrics flow through the same analytical pipeline.\n. Actually, I want to perform some more tests on this before you really take a look! I'll let you know when/if I want to go ahead with this. :)\n. Going to close this one. It doesn't actually do what I wanted it to do. It only generates a source upload to a repository, not a source + binary. Which is rather easy to see why, it only actually generates the source build itself. I'll see what I can come up with!\n. @jotes Hi! You mention that you were dropping support for nanoseconds, but that seems to also have meant that you didn't include support for microseconds, nor milliseconds. But neither of the libraries you mention actually support anything more detailed than seconds.\nFor us it would be nice to have support for nano/micro/milli-seconds, not for ordering logs among servers, but at least on the server itself.\nI found this go-library that implements %n with an optional offset, so you can decide the number of decimals: https://github.com/hhkbp2/go-strftime\nIn general, though, it doesn't seem very difficult to send an upstream patch to the selected strftime-library either: http://godoc.org/github.com/cactus/gostrftime - but there's always the risk of it not being accepted.\n. Oh... I'm not sure I know why that test is failing. I've tried the change, however, and it seems to work.\n. Okay, after sitting on this a while I don't think I really am able to figure out what I actually need to change. I tried just changing the EXPECT().Inject() to EXPECT().Deliver() calls in the stat_accum_input_test.go, but it seems not to be as simple as that, and I'll admit that I'm not well-versed enough in Go to read the stack trace I get when I do that...\nSo, what is going wrong here? It says \"Send on closed channel\", but browsing the test code, I cannot really identify why this would be something that happens...\n```\n$ go test -v github.com/mozilla-services/heka/pipeline\n=== RUN   TestAllSpecs\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Pre-loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Loading: [HekaFramingSplitter]\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: exiting now\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Max retries exceeded\n2015/11/10 09:18:26 Plugin 'stoppingOutput': has stopped, shutting down.\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Max retries exceeded\n2015/11/10 09:18:26 Plugin 'stoppingOutput': has stopped, exiting plugin without shutting down.\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Lost/Dropped 1 message\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': stopped\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Aborting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Aborting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Aborting\n2015/11/10 09:18:26 Plugin 'stoppingOutput': now restarting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Aborting\n2015/11/10 09:18:26 Plugin 'stoppingOutput' error: Max retries exceeded\n2015/11/10 09:18:26 Plugin 'stoppingOutput': has stopped, shutting down.\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Input 'stopping' error: Unclean Exit\n2015/11/10 09:18:26 Input 'stopping': Restarting (attempt 1/1)\n2015/11/10 09:18:26 Input 'stopping' error: Unclean Exit\n2015/11/10 09:18:26 Input 'stopping' error: Max retries exceeded\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Pre-loading: [NullSplitter]\n2015/11/10 09:18:26 Loading: [NullSplitter]\n2015/11/10 09:18:26 Decoder 'accum-FooDecoder': stopped\npanic: send on closed channel\ngoroutine 83 [running]:\ngithub.com/mozilla-services/heka/pipeline.(iRunner).getDeliverFunc.func2(0xc82027a900)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:450 +0x40\ngithub.com/mozilla-services/heka/pipeline.(iRunner).Deliver(0xc820100b40, 0xc82027a900)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:528 +0xb2\ngithub.com/mozilla-services/heka/pipeline.(StatAccumInput).Flush(0xc8200fb200)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/stat_accum_input.go:341 +0x2a6e\ngithub.com/mozilla-services/heka/pipeline.(StatAccumInput).Run(0xc8200fb200, 0x7f5e91af5bb0, 0xc820100b40, 0x7f5e93338f98, 0xc8205327c0, 0x0, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/stat_accum_input.go:159 +0x2f2\ngithub.com/mozilla-services/heka/pipeline.(iRunner).Starter(0xc820100b40, 0x7f5e93338f98, 0xc8205327c0, 0xc820532800)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:325 +0x1bb\ncreated by github.com/mozilla-services/heka/pipeline.(iRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:305 +0x88d\ngoroutine 1 [chan receive]:\ntesting.RunTests(0x81c2b0, 0x932740, 0x1, 0x1, 0xc820010c01)\n    /usr/lib/go/src/testing/testing.go:562 +0x8ad\ntesting.(*M).Run(0xc82003df08, 0xc82004fe68)\n    /usr/lib/go/src/testing/testing.go:494 +0x70\nmain.main()\n    github.com/mozilla-services/heka/pipeline/_test/_testmain.go:60 +0x116\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n    /usr/lib/go/src/runtime/asm_amd64.s:1696 +0x1\ngoroutine 5 [syscall]:\nos/signal.loop()\n    /usr/lib/go/src/os/signal/signal_unix.go:22 +0x18\ncreated by os/signal.init.1\n    /usr/lib/go/src/os/signal/signal_unix.go:28 +0x37\ngoroutine 6 [runnable]:\nsync.runtime_Semacquire(0xc82053280c)\n    /usr/lib/go/src/runtime/sema.go:43 +0x26\nsync.(WaitGroup).Wait(0xc820532800)\n    /usr/lib/go/src/sync/waitgroup.go:126 +0xb4\ngithub.com/mozilla-services/heka/pipeline.InputRunnerSpec.func3.3.4()\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners_test.go:231 +0xa18\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc820018420, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc82001a2a0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc8203559b0, 0xc82001a2a0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x53\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc8203559b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x51\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc8203559b0, 0x7d7e80, 0x1b, 0xc820018420)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x4d\ngithub.com/mozilla-services/heka/pipeline.InputRunnerSpec.func3.3()\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners_test.go:232 +0xc38\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc8200f8640, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc820544070)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc8203559b0, 0xc820544070)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x53\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc8203559b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x51\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc8203559b0, 0x7cf220, 0x1b, 0xc8200f8640)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x4d\ngithub.com/mozilla-services/heka/pipeline.InputRunnerSpec.func3()\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners_test.go:297 +0x3f7\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc8201269c0, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc820207f80)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc8203559b0, 0xc820207f80)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x53\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc8203559b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x51\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc8203559b0, 0x78f400, 0xe, 0xc8201269c0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x4d\ngithub.com/mozilla-services/heka/pipeline.InputRunnerSpec(0x7f5e93338828, 0xc8203559b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners_test.go:298 +0x4c5\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).execute.func1()\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 +0x55\ngithub.com/rafrombrc/gospec/src/gospec.recoverOnPanic(0xc8204a3c40, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 +0x56\ngithub.com/rafrombrc/gospec/src/gospec.(specRun).execute(0xc820207d50)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 +0x29\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).execute(0xc8203559b0, 0xc820207d50)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 +0x53\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).processCurrentSpec(0xc8203559b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 +0x51\ngithub.com/rafrombrc/gospec/src/gospec.(taskContext).Specify(0xc8203559b0, 0x88c240, 0x39, 0xc8204a3c40)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 +0x4d\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).execute(0xc8200864b0, 0x88c240, 0x39, 0x81c158, 0xc8203559b0, 0x650ed5)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 +0xa1\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).startNextScheduledTask(0xc8200864b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:82 +0xd0\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).startAllScheduledTasks(0xc8200864b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:55 +0x30\ngithub.com/rafrombrc/gospec/src/gospec.(Runner).Run(0xc8200864b0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:47 +0x21\ngithub.com/rafrombrc/gospec/src/gospec.runAndPrint(0xc8200864b0, 0x0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/main.go:54 +0x1c5\ngithub.com/rafrombrc/gospec/src/gospec.MainGoTest(0xc8200864b0, 0xc82008c090)\n    /home/mikn87/devel/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/main.go:39 +0x21\ngithub.com/mozilla-services/heka/pipeline.TestAllSpecs(0xc82008c090)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/all_specs_test.go:52 +0x28d\ntesting.tRunner(0xc82008c090, 0x932740)\n    /usr/lib/go/src/testing/testing.go:456 +0x98\ncreated by testing.RunTests\n    /usr/lib/go/src/testing/testing.go:561 +0x86d\ngoroutine 53 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(MatchRunner).run(0xc820241a00, 0x3e8)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:307 +0xe7\ncreated by github.com/mozilla-services/heka/pipeline.(MatchRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:360 +0x3f\ngoroutine 13 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(MatchRunner).run(0xc820241400, 0x3e8)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:307 +0xe7\ncreated by github.com/mozilla-services/heka/pipeline.(MatchRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:360 +0x3f\ngoroutine 50 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(MatchRunner).run(0xc820241700, 0x3e8)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:307 +0xe7\ncreated by github.com/mozilla-services/heka/pipeline.(MatchRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/router.go:360 +0x3f\ngoroutine 57 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown.func1(0xc820491360)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:92 +0x9a\ncreated by github.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:93 +0x35\ngoroutine 46 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown.func1(0xc8204917c0)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:92 +0x9a\ncreated by github.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:93 +0x35\ngoroutine 48 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown.func1(0xc820096000)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:92 +0x9a\ncreated by github.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:93 +0x35\ngoroutine 66 [chan send (nil chan)]:\ngithub.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown.func1(0xc820096f00)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:92 +0x9a\ncreated by github.com/mozilla-services/heka/pipeline.(GlobalConfigStruct).ShutDown\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:93 +0x35\ngoroutine 82 [runnable]:\nsyscall.Syscall(0x1, 0x1, 0xc820241500, 0x38, 0x38, 0x38, 0x0)\n    /usr/lib/go/src/syscall/asm_linux_amd64.s:18 +0x5\nsyscall.write(0x1, 0xc820241500, 0x38, 0x80, 0x2, 0x0, 0x0)\n    /usr/lib/go/src/syscall/zsyscall_linux_amd64.go:1064 +0x5f\nsyscall.Write(0x1, 0xc820241500, 0x38, 0x80, 0xb, 0x0, 0x0)\n    /usr/lib/go/src/syscall/syscall_unix.go:176 +0x4d\nos.(File).write(0xc82002e010, 0xc820241500, 0x38, 0x80, 0x0, 0x0, 0x0)\n    /usr/lib/go/src/os/file_unix.go:232 +0xaa\nos.(File).Write(0xc82002e010, 0xc820241500, 0x38, 0x80, 0x939a80, 0x0, 0x0)\n    /usr/lib/go/src/os/file.go:139 +0x8a\nlog.(Logger).Output(0xc820086280, 0x2, 0xc8200b0240, 0x23, 0x0, 0x0)\n    /usr/lib/go/src/log/log.go:166 +0x37a\nlog.(Logger).Printf(0xc820086280, 0x7bf460, 0x10, 0xc82011ae78, 0x2, 0x2)\n    /usr/lib/go/src/log/log.go:173 +0x7e\ngithub.com/mozilla-services/heka/pipeline.(dRunner).LogMessage(0xc8200f6120, 0x78ce10, 0x7)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:694 +0x16a\ngithub.com/mozilla-services/heka/pipeline.(dRunner).start(0xc8200f6120, 0x7f5e93339820, 0xc8201ad2c0, 0xc8201ad360)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:655 +0x339\ncreated by github.com/mozilla-services/heka/pipeline.(*dRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:620 +0x1a9\ngoroutine 85 [runnable]:\ngithub.com/mozilla-services/heka/pipeline.(dRunner).start(0xc8200f6120, 0x7f5e93339820, 0xc8201ad2c0, 0xc8201e7170)\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:629 +0x86\ncreated by github.com/mozilla-services/heka/pipeline.(dRunner).Start\n    /home/mikn87/devel/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:620 +0x1a9\nexit status 2\nFAIL    github.com/mozilla-services/heka/pipeline   0.324s\n```\n. ",
    "nroe": "build.sh script is ok . \n\"go run\" can help me better learning heka : )\n. bingo, i can change source code and needn't recompile.\n\"go oracle \" is a good idea. i will try later.  thanks.\n. ",
    "secretmike": "Loggly would be another great addition. It's got a mix of human readable logging plus metrics from json logs. It also uses parts of the URL path to specify tags so would be a good test case for the general output concept.\n. ",
    "asenchi": ":metal: thanks!\n. Related to this, it appears this causes issues with environment variables as well:\n2014-11-06_17:25:10.27896 2014/11/06 17:25:10 Initialization failed for 'logsearch': Unable to parse ElasticSearch server URL [http://logsearch-data.%ENV[ENV]:9200]: parse http://logsearch-data.%ENV[ENV]:9200: hexadecimal escape in host\n. Ah, I'm not. Sorry I must've missed that release.\n. FYI, the panic was spewing into logs over and over again, not sure if that changes your speculation. I'll upgrade and see if I can reproduce the behavior to test.\n. ",
    "pruthig": "Tried but same error\npruthi@pruthi-linux:~/heka$ env | grep GO\npruthi@pruthi-linux:~/heka$ export GO_PLATFORM=linux\npruthi@pruthi-linux:~/heka$ export GOBIN=/home/pruthi/heka/build/heka/bin\npruthi@pruthi-linux:~/heka$ export GOARCH=i386\npruthi@pruthi-linux:~/heka$ export GOROOT=/usr/local/go\npruthi@pruthi-linux:~/heka$ export GOOS=linux\npruthi@pruthi-linux:~/heka$ export GO_ARCH=i386\npruthi@pruthi-linux:~/heka$ export GOPATH=/home/pruthi/heka/build/heka\npruthi@pruthi-linux:~/heka$ export GOPLATFORM=linux\npruthi@pruthi-linux:~/heka$ export GO_VERSION=1.2\npruthi@pruthi-linux:~/heka$ export PATH=\"/usr/local/go/bin:$PATH\"\npruthi@pruthi-linux:~/heka$ echo $PATH\n/usr/local/go/bin:/usr/lib/lightdm/lightdm:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\npruthi@pruthi-linux:~/heka$ source build.sh\nCMake Error at /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:91 (MESSAGE):\n  Could NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH) (Required is at\n  least version \"1.2\")\nCall Stack (most recent call first):\n  /usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:252 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindGo.cmake:32 (find_package_handle_standard_args)\n  CMakeLists.txt:23 (find_package)\n-- Configuring incomplete, errors occurred!\nmake: *** No targets specified and no makefile found.  Stop.\n. Hi,\nDone what you suggested but still have problem, see comment on github.\nThanks,\n-Gaurav\nOn 17 May 2014 23:57, \"Rob Miller\" notifications@github.com wrote:\n\nLooks like the actual go binary isn't on your PATH. It will probably\nwork if you run:\n$ export PATH=\"/usr/local/go/bin:$PATH\"\nand then try again.\n-r\nOn Sat 17 May 2014 10:40:52 AM PDT, Gaurav Pruthi wrote:\n\nGetting err while installing heka\npruthi@pruthi-linux:~/heka$ source build.sh\nCMake Error at\n/usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:91\n(MESSAGE):\nCould NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH) (Required\nis at\nleast version \"1.2\")\nCall Stack (most recent call first):\n/usr/share/cmake-2.8/Modules/FindPackageHandleStandardArgs.cmake:252\n(_FPHSA_FAILURE_MESSAGE)\ncmake/FindGo.cmake:32 (find_package_handle_standard_args)\nCMakeLists.txt:23 (find_package)\n-- Configuring incomplete, errors occurred!\nmake: *** No targets specified and no makefile found. Stop.\nFollowing are the env variables:\npruthi@pruthi-linux:~/heka/build$ env | grep GO\nGO_PLATFORM=linux\nGOBIN=/home/pruthi/heka/build/heka/bin\nGOARCH=i386\nGOROOT=/usr/local/go\nGOOS=linux\nGO_ARCH=i386\nGOPATH=/home/pruthi/heka/build/heka\nGOPLATFORM=linux\nGO_VERSION=1.2\n\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/849.\n\n\nReply to this email directly or view it on GitHubhttps://github.com/mozilla-services/heka/issues/849#issuecomment-43418040\n.\n. \n",
    "jacktasia": "FWIW, I had this exact same issue on Ubuntu 14.04 once go 1.3 became a requirement for heka (after successfully compiling back when it was 1.2). After upgrading my go to 1.4 and being able to successfully run go version go env and running/compiling go programs I was still getting \nCould NOT find Go (missing: GO_VERSION GO_PLATFORM GO_ARCH)\nwhen running source build.sh\nFinally I tried rm -rf build/ and then source build.sh and it worked. This seems obvious in hindsight but took way too long to click for me because of the non-traditional way heka is built. Hopefully this will save someone else the time. \n. ",
    "alexzorin": "Is there a particular motivation for the type restrictions in Heka's implementation (https://github.com/mozilla-services/heka/blob/0c64368a0e328e6d02d0f919ffab6e7fe26aa279/pipeline/stat_accum_input.go#L155-167) ?\nEtsy'd statsd will gladly accept any Number value. It is not possible to use StatsAccumInput if we are sending float values in gauges for example, which seems totally reasonable to me.\n. I can't speak to the correctness of my workaround, only to say that it has worked for me for the past few months, with the use case of substituting etsy/statsd\nWould appreciate someone who knows what they're doing to take a look.\n. ",
    "rishid": "Any plans on getting this merged?\n. Yep, I agree did not see that one.\n. ",
    "freeformz": "shell\n\u27ad lua -v\nLua 5.1.5  Copyright (C) 1994-2012 Lua.org, PUC-Rio\n. lua and the like were installed via homebrew\n. ",
    "wolfias": "thanks for your help.\nlet me give some explanation for the listening codes.\nyou see, aggregator_output send messages to 10.63.208.42, and the python code listening on 10.63.208.42\nlog_buf = tcpCliSock.recv(BUFSIZ),   #this line receive heka message which couldn't be used directly\nlog_data = Message()\nlog_data.ParseFromString(log_buf)  #ParseFromString is in google.protobuf, not a heka-py function\nso the log_buf is not in protobuf format, how could i parse it? \ni'm reading the heka/client, but i'm not familiar with golang.\ncould you give me some example for parsing tcpoutput messages? \nthank you.\n. to ranfrombrc,\nparsing is ok now, thank you very much.\n. ",
    "jjurado": "Is there a way to increase the payload field size? \n. ",
    "AdeMiller": "Yes. You can set the max_message_size property in the global [hekad] section of your configuration.\n\nNew in version 0.9.\nmax_message_size (uint32):\nThe maximum size (in bytes) of message can be sent during processing. Defaults to 64KiB.\n. I'm seeing something similar for the Kafka input. The net result is high CPU and memory usage and Heka becoming wedged.\n\n2015/11/09 15:28:03 Input 'kafka_input_de1_ie_canary_015' error: kafka server: In the middle of a leadership election, t\nhere is currently no leader for this partition and hence it is unavailable for writes.\n2015/11/09 15:28:03 Input 'kafka_input_ne1_ie_zenoss_015' error: kafka server: In the middle of a leadership election, t\nhere is currently no leader for this partition and hence it is unavailable for writes.\n2015/11/09 15:28:03 Input 'kafka_input_ca2_analytics_replication_011' error: kafka server: In the middle of a leadership\n election, there is currently no leader for this partition and hence it is unavailable for writes.\n2015/11/09 15:28:11 Diagnostics: 63 packs have been idle more than 120 seconds.\n2015/11/09 15:28:11 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_platform_nlog_00: 1\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ateam_appfog_00: 4\n2015/11/09 15:28:11 Diagnostics:    topic_stats_short: 63\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_04: 7\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_07: 4\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_05: 4\n2015/11/09 15:28:11 Diagnostics:    topic_stats_long: 63\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_02: 11\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_00: 7\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_01: 12\n2015/11/09 15:28:11 Diagnostics:    location_stats_short: 63\n2015/11/09 15:28:11 Diagnostics:    kafka_output: 32\n2015/11/09 15:28:11 Diagnostics:    location_stats_long: 63\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_03: 9\n2015/11/09 15:28:11 Diagnostics:    elasticsearch_output_ie_zenoss_06: 4\n2015/11/09 15:28:11\n. ",
    "erewh0n": "Thanks, I skimmed other issues but didn't note #865.  I'm glad to hear that the message formatters are getting abstracted, it makes a lot of sense.\nI will try to contribute where possible once I have a chance to review the new plugins (particularly on the ES side).  I'm not sure how I personally feel about having to maintain Lua scripts, but I can see why they make sense in a logging framework.\nClosing.\n. ",
    "lyrixx": "ok, thanks.\n. Thanks ;)\n. This repository is not maintained anymore.\nI'm closing this issue.\nThanks again for your work.. You can use http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html to fix your issue\n. @trink Thanks for this review. I'm fixing my decoder. BTW, how do you see logs when something goes wrong with the decoder? \n\nThe Timestamp value is invalid (so the current time is always used)\n\nDid you see that by experience or ... ?\n. @Seldaek I my code, and everything works fine with default socket handler + json encoder.\n@trink I updated the monolog.lua script: https://gist.github.com/lyrixx/0b174188560922eecae2 ; So can I open a PR to include it into this repository ?\n. Unfortunately this repo is deprecated. There is no need to keep this issue open.\nAnyway, thanks you for heka, it's really a good tool.\nI'm a bit sad it has been deprecated without a new shiny tool.. Actually, there is no need for that.\nIt's better to use the hostname option of the LogStreamer Input.\n. Thanks, fixed.\n. ",
    "trixpan": "+1 to @lyrixx comment. \nThis is an issue easily solved in ES if you use the correct technique (Indices templates as described). \n@rafrombrc Issue is candidate for closure.\n. @rafrombrc this should be closed.\nFixed on 34e576f0ef8294f56ecaac2b1cceea2eda5d07a5\n. @rafrombrc is this still going on? Happy to try to figure out why.\n. @trink is this still going on? Happy to try to figure out why.\n. @blalor have you had any luck?\n. @ildus . From the trail it seems to be fixed, should we close this ticket?\n. @ildus \nI don't think it is.\nSeems to me you are hitting golang's default behaviour/expectation around a stable HEAD... \nIf you look at commit ad5b214aa95ad571c3f93cc783e262f5647a9058 you will see that @rafrombrc removed the very same links to googlecode back in July under the the v.0.10 tag. However, the github master tree still points to stable code (i.e. 0.9.2)\nAs the link above explains, golang get always will retrieve the default version of the git repository and as such you are instructing your compiler to get an old version of Heka (Looking back, perhaps commit ad5b214aa95ad571c3f93cc783e262f5647a9058 should have been done against the master, not only dev...)\nIn any case... Although - to the best of my knowledge - golang doesn't have a native solution around this, there are some workarounds proposed around this.  the simpler of them is using an intermediary service like gopkg.in ...\nCan you try to run and let us know if it still happens?\n$ go get gopkg.in/mozilla-services/heka.v0.10.0b1\nCheers\n. @rafrombrc: \nI've gone through that pain myself and being an user more than a developer it should be reasonably easy to flag some of the pains around building / installing the packages on Debian/Ubuntu and RHEL/Centos systems. I am happy to update the documentation to address some of the build/install things that explode right on you face when you build/install heka into a vanilla system. \nI am biased (I like the idea of chrooting hekad), but meanwhile, perhaps static linking is the way to move forward(backwards?) until an updated build is figured out? \nAs it is now we already missing some of the advantages of  dynamic linking (i.e. if people rely on source build.sh they already end with an old version of the sandbox anyhow, the RPM/DEB packages may end up with library conflicts) \n. this ticket should be closed. version 0.11 gets it from github.\nhttps://github.com/mozilla-services/heka/commit/ad5b214aa95ad571c3f93cc783e262f5647a9058\n. +1. Will submit an updated version of the docs soon.\n. @muhammad-emam, @rustamk \nKafka output has some issues around redundancy in case of dead clusters. I stepped back from using it once I was unable to get the hekad processes to survive a disconnect from the cluster without losing messages. \n. Hmmmm... not sure what is going on in here.\nI've just tried to reproduce and could not.\nSystem is a vanilla CentOS.\n```\n/usr/bin/hekad\n2015/09/20 21:08:27 Error reading config: Error opening config file: open /etc/hekad.toml: no such file or directory\n```\n```\ntouch heka.toml\n/usr/bin/hekad -config=heka.toml\n2015/09/20 21:09:24 Pre-loading: [ProtobufDecoder]\n2015/09/20 21:09:24 Loading: [ProtobufDecoder]\n2015/09/20 21:09:24 Pre-loading: [ProtobufEncoder]\n2015/09/20 21:09:24 Loading: [ProtobufEncoder]\n2015/09/20 21:09:24 Pre-loading: [TokenSplitter]\n2015/09/20 21:09:24 Loading: [TokenSplitter]\n2015/09/20 21:09:24 Pre-loading: [HekaFramingSplitter]\n2015/09/20 21:09:24 Loading: [HekaFramingSplitter]\n2015/09/20 21:09:24 Pre-loading: [NullSplitter]\n2015/09/20 21:09:24 Loading: [NullSplitter]\n2015/09/20 21:09:24 Starting hekad...\n2015/09/20 21:09:24 MessageRouter started.\n^C2015/09/20 21:09:25 Shutdown initiated.\n2015/09/20 21:09:25 Waiting for decoders shutdown\n2015/09/20 21:09:25 Decoders shutdown complete\n2015/09/20 21:09:25 Shutdown complete.\n```\n. @rafrombrc \nCool. I could not reproduce the issue, even with an empty file (note the touch heka.toml step taken above) but happy to assume it happens.\nCheers\n. Apologies if I don't share the excitement, I just get concerned about any move that make Heka look like LogStash... :-)\nIn any case, let me try to be constructive and ask/suggest:\nWouldn't ProcessInput achieve the same result without pushing extra functionality into the core application? \n```\n[on_space]\ntype = \"TokenSplitter\"\ndelimiter = \" \"\n[DemoProcessInput]\ntype = \"ProcessInput\"\nticker_interval = 5\nsplitter = \"on_space\"\nstdout = true\nstderr = false\n[DemoProcessInput.command.0]\nbin = \"/bin/cat\"\nargs = [\"/proc/diskstats\"]\n\n```\nChange the space for a comma and voil\u00e0...\nIf starting /bin/cat every 5 seconds is an issue, then you can simply set ticker_interval = 5 to 0 and run a long running process (you would have to put something together). \nI am currently using this with OpenTSDB's tcollector and works like a charm. Could join the grace of the existing tcollectors with the power of Heka's data pipeline without having to change much...\n. Yes. It seems the dependencies are indeed part of \"INCLUDE_MOZSVC\". Will submit a PR to move them to within the appropriate loop.\n. @rafrombrc \nany comments on this PR?\nCheers\n. @xrl  \nI run the dev version. The issue is not related to buffering per-se but to the way Sarama is called without this resulting on a cursor update.\nNote how Kafka's output \nhttps://github.com/mozilla-services/heka/blob/dev/plugins/kafka/kafka_output.go#L375\ndiffers from \nhttps://github.com/mozilla-services/heka/blob/dev/plugins/http/http_output.go#L117\nAlso note changing the code to update the cursor without modifying the publisher type, will lead to message losses in case of the scenarios described in #1750 \n. @timurb great work. Looking forward for this to be merged. :+1: \n. @trink @rafrombrc is the unbuffered message loss issues identified as part of #1750 and #1749 worth this PR ?\n. This is a duplicate of\nhttps://github.com/mozilla-services/heka/issues/1749\n. ",
    "vincentzhwg": "No, It's my pleasure. : ) thank you\n. done. : )\n. ",
    "donjohnson": "+1\n. +1\n. +1 \nAnother use case for this would be reconfiguration of a sandbox filter. In my case I'm configuring a sandbox filter by importing a separate lua module that returns a fairly complex configuration table (I was unable to feed it in using toml inline tables or as json in the toml) full of business rules. If these rules need to be updated, it'd be nice to only restart the analysis filter without having to disrupt inputs or any other unrelated components.\n. ",
    "didip": "@trink Thanks for the feedback! I have updated the pull request can you take a look at it again?\n. @trink Thank you for the feedback as I am not familiar with cmake's regex.\nI have updated the PR per your suggestion.\n. @rafrombrc I may need a little guidance on how to update CHANGES.txt. \nShould I put this under 0.9.0 or 0.8.1 bug handling?\n. Please close this PR as it is replaced with: https://github.com/mozilla-services/heka/pull/1173\n. ",
    "tjsoftworks": "SURE ENOUGH! \nThe file is empty. How did you know? Is the HASH that obvious? You must have seen this before.\nThanx\nTerry\nOn Jun 18, 2014, at 21:53, Mike Trinkala wrote:\n\nd41d8cd98f00b204e9800998ecf8427e is the md5sum of an empty file; your download is failing.\n\u2014\nReply to this email directly or view it on GitHub.\n. Confirmed. Firewall blocking download. Closing the issue.\n. \n",
    "validname": "Have you thought of adding percentiles of plugin's processing times? \nIndeed, average processing time is not so useful as an answer on question \"what time (e.g.) 85% of messages was processed?\". And if you count average time on the infinite period, it becomes more useless...\n. Done.\nHope I didn't some weird mistakes in description, because I'm not native English speaker.\n. I've done it. See #1119 \n. Btw, why just don't use inodes to detect new files (and use hash only to find rotated file in the logstream)?\n. Thanks for the patch! I'm going to test it in my testing environment.\n. @rafrombrc If Go supported preprocessing directives, would you use that specifics (e.g. 'if define _LINUX'...)?\n. @rafrombrc Now, I realize the reasons. Thank you for explanation!\n. I've seen the code (pipeline/stream_parser.go and logstreamer/logstreamer_input.go). Perhaps, better not to drop message buffer in all cases, but leave to decide what to do to input plugins. And in the Logstreamer either drop messages and all consequent part of it or pass only first cutted part to decoder. That give the opportunity parse significant data from message which often placed in a header.\n. I've made pull request #1137 \nDo you agree with that changes? \nDefault behaviour is not changed, just some memory is consumed to return big message.\n. My pull request was merged into 'dev' branch. Shall I close this issue?\n. That's very strange, that file output test is not passed.\nI test it manually it two different environment, it was passed both times.\nWhat I run:\ncd ./build\nGOPATH=`pwd`/heka /usr/bin/go test -ldflags=\"-s\" github.com/mozilla-services/heka/plugins/file\n. @rafrombrc I'm so sorry, I hadn't checked verb 'cut' for whether it's irregular one. English is no so weird, no, it's my fault. \n@rafrombrc , @trink Would be 'decode_truncated_messages' an appropriate name for the config option? Yes, it's too long, but I think it wouldn't be often used.\n. I've done all changes in documentation and renamed variables. I'm going to squash all my commits to only one to not mess common commit log.\nBut now I'm in doubts. Is 'decode' a proper word to describe what Logstreamer does packing messages? Using decoder is not a mandatory thing. Maybe change 'decode' to 'parse'? \nOn the one hand, Logstreamer doesn't parse records to messages, actually. But on the other hand, user doesn't know it.\n. @trink Thank you!\n. Sorry for big delay.\nI've done it at last.\n. Well, I've thought a lot about saving original UUID into a message field. I was trying to avoid this, because at the end of their way, our messages is storing in ElasticSearch with 3 UUID: original hash, one from decoder and one from ES. Too much uniqueness. ^_^\nBut I'll follow your recommendation. Thank you for your answer, Mike.\nBefore I close this issue:\ncould you tell me what causes message duplicates in your environment? (Maybe it would be better to ask this question in the maillist?)\n. Thank you for the comment!\nI've changed this lines.\nAs I'd changed CHANGES.txt, there is a merge conflict. Please, send a comment once you merge pull request #1186 and I'll resolve conflict.\n. I've added both descriptions. Please check it (especially grammar).\n. It's seems that we have erroneous loop in this case: \n- on new file function SeekInFile() returns file descriptor (aka \"hash match\") because of stored seekPosition=0,\n- hence, FileHashMismatch() returns false (\"hash match\") further,\n- and NewerFileAvailable() returns false, too.\n  And this lasts forever.\n. I've tried to look deeper at code, but it's too complicated for me. Too scary to touch hashing. So I just wrote a 'special-case-fix'. It works for me.\n``` patch\n--- dir1/logstreamer/reader.go  2014-10-14 10:28:30.000000000 +0700\n+++ dir2/logstreamer/reader.go  2014-10-14 10:37:44.000000000 +0700\n@@ -178,10 +178,14 @@\n        return \"\", false\n    }\n\nprior_was_empty := false\n    // 1. If our size is greater than the file at this filename, we're not the\n    // same file\n    if currentInfo.Size() > fInfo.Size() {\n        ok = true\n} else if (currentInfo.Size() < fInfo.Size() && currentInfo.Size()==0 )  {\nprior_was_empty = true\n\nok = true\n    } else if l.FileHashMismatch() {\n        // Our file-hash didn't verify, not the same file\n        ok = true\n@@ -234,6 +238,11 @@\n        return l.logfiles[fileIndex+1].FileName, true\n    }\n\n\n// So filename did't change, but currently opened file is empty and existing file with our filename is bigger. Maybe is it a newer one?\n\nif prior_was_empty == true {\nreturn l.position.Filename, true\n}\n+\n    return\n }\n\n```\n. @markabey Did you try to view positions in journal files? I'll say banality, but maybe heka just couldn't read your files as fast as you expect.\n. (I'm not the core developer or active contributor, just a user of heka, who relying on LogStreamer accuracy.)\nYes, I think it's safe (in that sense that messages won't  be lost).\nBut it might cause massive message duplicates in case of heka crash and re-reading log files from too old position.\nI think once you decrease rate of saving position, then it would be better move number of '500 records' to LogStreamer option, so everyone could decide for yourself what number to use. And, maybe it would be more better to have two options effecting on save rate: how often in a number of records and how often in a time period.\n. ",
    "itomsawyer": "thanks for your replies and I will follow  the advise of @rafrombrc in the future.:-)\n. ",
    "intjonathan": "Here's an example of the syslog decoder configuration I needed to deal with this: https://gist.github.com/intjonathan/a28f27022854cbb78866\nThe short version is that it uses MultiDecoder to apply a fallback rsyslog parser with a template that has no %syslogtag% field.\n. Now thats what I call service!\n. This bit me when trying to write a decoder that builds multiple grammars to match against. \n. +1 We had to use an envsubst wrapper to preprocess files during launch to work around this limitation. Wasn't fun.\n. :clap: \n. +1 would love this, it's a common pattern for securing ES.\n. Corrected.\n. We see similar patterns though we haven't gone to the work of identifying it as DashboardOutput. We had to put cgroup limits on our aggregation instances, which helped but is a bandaid. End up with restarts at these peaks.\n\n. Unfortunately I haven't been able to capture the memprof for you, instead I attacked this from a couple angles: upgrading to 0.9.2, and rewriting an accumulating filter -> http message flow into a native Lua output. It seems to have alleviated the oom killer restarts. Sorry that doesn't help you move forward on the issue at hand here. :/\n. You can set buffering => false now on TcpOutput.\n. Were I to attempt this myself, is there anything about writing it that would likely trip me up? I know heka is generally designed to care whether outputs are keeping up, but that doesn't really have meaning for this output.\n. Count me interested, that looks rad!\n. +1 we would LOVE to have this.\n. This is in 0.10.0b0 on Centos 7.\n. :yellow_heart: \n. :tophat: \n. :heart_eyes:\n. We got the impression you wouldn't necessarily have a body to close if Do() returned an error. The documentation is somewhat ambiguous.\n\nWhen err is nil, resp always contains a non-nil resp.Body.\n. \"again call SplitStream again\" ?\n. You probably meant 'SplitterRunners'.\n. Did you mean 'Sandbox Outputs'?\n. \n",
    "ioc32": "I have a similar use case, in which I would like to use one single AMQPOutput config section to process all my hadoop logs.\nAll these logs have their Logger field (hdp-hbase, hdp-spark, hdp-zk, hdp-yarn...) set by a downstream LogstreamerInput and eventually end up at the \"heka aggregators\". These post them to RabbitMQ for further processing by the \"heka consumers\", which pick the raw events and do all the massaging, then submitting the results to Elasticsearch.\nAtm I am using jinja2 templates to manually send each log to the right queue, but it'd be way more convenient and flexible to just interpolate fields as the AMQPOutput routing_key, for example.\n. Hi rafrombrc, trink et al.\nI am aware of the very bad timing of the issue I opened, considering the end of the year celebrations and all, but still I'd like to ask: did this issue slip through the cracks?\nJust checking, by no means pushing... :-)\n. Hey @4r9h \nThanks for working on this one and coming up with the PR. I was actually looking forward to have a short discussion with the lead devs regarding how they wanted it fixed.\nAs you've pointed out, the fix is simple so maybe I should have worked it out on my own without the discussion or come up with a PR to trigger it ;-)\n. FWIW I've built heka with these changes and they work fine. Please see this gist [1] with the heka config and two sample messages from the queue.\nThe resulting binary passed all the tests.\n[1] https://gist.github.com/ioc32/9e64a9948333fd9b126f\n. upstream_response_time [1] uses nginx_upstream_times [2], which again uses double, defined in [3] to only use \".\" as the decimal separator.\nI've had a similar issue in the past with some timestamps which used ',' for the time_sec_frac, as opposed to '.'.\nChanging the separator in the double grammar from l.P\".\" to l.S\".,\" should work, but I am not sure this is the proper fix.\nLet's see what @trink and @rafrombrc want to do about it.\n[1] https://github.com/mozilla-services/lua_sandbox/blob/dev/modules/common_log_format.lua#L113\n[2] https://github.com/mozilla-services/lua_sandbox/blob/dev/modules/common_log_format.lua#L50\n[3] https://github.com/mozilla-services/lua_sandbox/blob/dev/modules/common_log_format.lua#L28\n. +1\nI had the same issue when I first started setting it all up. Eventually, I\ndecided to move on and provide dedicated users with slightly more\npermissions than strictly necessary.\nOur setup could use an iteration now, so I am also interested in learning\nhow to get heka to use readonly users.\nOn Feb 6, 2015 9:31 AM, \"Alex Sanami\" notifications@github.com wrote:\n\nI'm trying to set up an AMQP input to an exchange that I only have read\npermissions to. It seems the default behaviour is to declare an exchange,\nwhich requires readwrite permissions. It is possible to simply bind to the\nexchange, if so, its not obvious how to do this from the documentation.\nThis is my current configuration:\n'''\n[AMQPInput]\nurl = \"amqp://user:password@amqp.company.com/VHOST\"\nexchange = \"MyExchange\"\nexchange_type = \"topic\"\nexchange_durability = true\nexchange_auto_delete = false\nrouting_key = \"#\"\nqueue = \"test\"\ndecoder = \"json_decoder\"\n''''\nThanks\nAlex\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1314.\n. Cannot comment on the code (go is not my strong suit) but the feature is very nice. It allows us to exert tighter control over the data pipeline.\n\nLooking forward to see progress here! I can give a hand too if you need the docs updated.\n. I still see a few of these 400s. I'll build versions/0.10 again and see if the issue still persists.\n. This has been discussed \"extensively\" in the past. rafrombrc has provided valuable input in many threads and issues, for example:\n- https://github.com/mozilla-services/heka/issues/1379\n- https://mail.mozilla.org/pipermail/heka/2015-May/000586.html\n- https://mail.mozilla.org/pipermail/heka/2014-November/000235.html\nI'd start having a look at your custom RedisInput.\n. Here's the ctest results:\nThe following tests FAILED:\n     10 - plugins/http (Failed)\n     11 - plugins/irc (Failed)\n     16 - plugins/process (Failed)\n     20 - plugins/udp (Failed)\n     26 - mozsvc (Failed)\nThe full log is here [1].\nCould you have a look at it or at least confirm all tests are passing in your environment?\n[1] https://gist.github.com/ioc32/1e08b656db9fabda2bba\n. Thanks for the hints @trink!\nI've issued the make clean-heka and make commands after a failed ctest (in an environment started from scratch). Then, I run ctest again but it keeps failing [1].\nI'm slightly confused now, I'll try and build a release package, see if that fails too.\n[1] https://gist.github.com/ioc32/2e1348786f57337551fd\n. Well, I am starting to believe this is a classic PEBKAC problem. ctest won't pass even trying to build against master.\nWill have to find another environment to build and test, in case this one's not working anymore.\n. FWIW I used the Apache Error log to learn LPEG. Here's the result, which seemed to work with the log samples I had around back then:\nhttps://github.com/ioc32/heka-lua-decoders/blob/master/apache-error.lua\nIt's nothing like the full-fledged grammars the heka team has put together for nginx formats, but I'd be happy to refine the lua code there and submit a PR.\n. @guluglu we've been using heka to send various logs to elasticsearch (since the 0.7 days) without much trouble. Certainly without the hekad process dying while at it.\nWould you share the heka version you're running as well as the relevant config snippets?\n. Ugh! Thanks!\n. Not at all. PR updated!\n. ",
    "erkiesken": "I have a concrete use case for this now.\nWith the changed InfluxDB line protocol the DB name, timestamp precision and retention policy must be set as a querystring param now (ie /write?db=foo&precision=s&rp=default).\nI currently have two different timestamp precisions coming from my filters and have to resort to 2 different output plugins to handle this, ie:\n```\n[influx-line-output-seconds]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type == 'heka.sandbox.influx.line' && Fields[precision] == 's'\"\nencoder = \"PayloadEncoder\"\naddress = \"%ENV[HEKA_INFLUXDB_URL]&precision=s\"\nusername = \"%ENV[HEKA_INFLUXDB_USER]\"\npassword = \"%ENV[HEKA_INFLUXDB_PWD]\"\nhttp_timeout = 30\n[influx-line-output-milliseconds]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type == 'heka.sandbox.influx.line' && Fields[precision] == 'ms'\"\nencoder = \"PayloadEncoder\"\naddress = \"%ENV[HEKA_INFLUXDB_URL]&precision=ms\"\nusername = \"%ENV[HEKA_INFLUXDB_USER]\"\npassword = \"%ENV[HEKA_INFLUXDB_PWD]\"\nhttp_timeout = 30\n```\nSo it would be welcome if HttpOutput plugin would either let me use dynamic %Fields[precision] replacement value in address line. In my case that would look something like this:\naddress = \"%ENV[HEKA_INFLUXDB_URL]?db=%Fields[db]&precision=%Fields[precision]\"\nOr alternatively it could also work if HttpOutput takes specially named Field values from messages, a la dynamic field http_querystring_precision and http_header_x_my_header and use these and their values in the HTTP requests accordingly.\n. As workaround I've used this before to not hard-code fixed paths into configs:\nlog_directory = \"%ENV[PWD]\"\nThis of course expects a PWD env variable to exist and point to current working folder.\n. ",
    "nemosupremo": "Sorry I thought I had messed up this function, apologies to those who will get messages in their inbox. I'm new to cmake, but this worked for me, but I'm not sure if its \"clean\". I'll submit a PR if this is good way to do it.\n```\nfunction(add_external_plugin vcs url tag)\n    parse_url(${url})\n    if  (\"${tag}\" STREQUAL \":local\")\n       local_clone(${url})\n    else()\n        if (\"${vcs}\" STREQUAL \"git\")\n           git_clone(${url} ${tag})\n        elseif(\"${vcs}\" STREQUAL \"hg\")\n           hg_clone(${url} ${tag})\n        elseif(\"${vcs}\" STREQUAL \"svn\")\n           svn_clone(${url} ${tag})\n        else()\n           message(FATAL_ERROR \"Unknown version control system ${vcs}\")\n        endif()\n    endif()\nset(ignore_root \"no\")\nforeach(_subpath ${ARGN})\n    if (\"${_subpath}\" STREQUAL \"__ignore_root\")\n        set(ignore_root \"yes\")\n    else()\n        set(_packages ${_packages} \"${path}/${_subpath}\")\n    endif()\nendforeach()\n\nif (\"${ignore_root}\" STREQUAL \"no\")\n    set(_packages ${path})\nendif()\nset(PLUGIN_LOADER ${PLUGIN_LOADER} ${_packages} PARENT_SCOPE)\n\nendfunction(add_external_plugin)\n```\n. Fixed with f216909460854703c90de5000f956800d69c9ab6\n. Alright note added. \n. ",
    "syepes": "+1\n. I have just experienced this issue, this would be a great addition.\n+1\n. First of all I am no GO programer.\nI have made this change and build Heka with it but when it creates the ES mappings, the @timestamp is a string and not a date.\nThe other strange thing is that the actual time stored in the @timestamp field is the current insert time and not the one from the XML (timeCreated)\nCan anyone with more experience give me a hand.\nBelow is a simplified example:\n```\nHeka Config\n[xml_files]\n  type = \"LogstreamerInput\"\n  parser_type = \"token\"\n  decoder = \"XmlDecoder\"\n  log_directory = \"/tmp\"\n  file_match = '(?P\\d+).xml'\n  priority = [\"seq\"]\n[XmlDecoder]\n  type = \"PayloadXmlDecoder\"\n  timestamp_layout = \"2006-01-02 15:04:05.000\"\n  timestamp_location = \"UTC\"\n[XmlDecoder.xpath_map]\n  timeCreated = \"/event/timeCreated\"\n  title = \"/event/title\"\n[XmlDecoder.message_fields]\n  Type = \"xml\"\n  Timestamp = \"%timeCreated%\"\n  timeCreated|date = \"%timeCreated%\"\n  Payload = \"%title%\"\n[es_encoder]\n  type = \"ESLogstashV0Encoder\"\n  index = \"logstash-%{Type}_%{2006.01.02}\"\n  type_name = \"%{Type}\"\n  es_index_from_timestamp = true\n  timestamp = \"2006-01-02 15:04:05.000\"\n[ElasticSearchOutput]\n  message_matcher = \"Type == 'xml'\"\n  encoder = \"es_encoder\"\n  server = \"http://localhost:9200\"\n  flush_interval = 2\n```\n```\nExample XML\necho -e 'Test message2014-09-23 09:31:46.730 UTC\\n' >/tmp/1.xml\n```\nDev branch:\n``` diff\n--- plugins/elasticsearch/encoders.go.orig      2014-10-24 08:50:09.478177338 +0200\n+++ plugins/elasticsearch/encoders.go   2014-10-24 08:36:21.981343936 +0200\n@@ -326,6 +326,7 @@\n        coord          *ElasticSearchCoordinates\n        // Field names to include in ElasticSearch document for \"clean\" format.\n        fields         []string\n+       timestampFormat string\n        useMessageType bool\n }\n@@ -339,6 +340,8 @@\n        UseMessageType bool toml:\"use_message_type\"\n        // Field names to include in ElasticSearch document.\n        Fields []string\n+       // Timestamp format. Defaults to \"2006-01-02T15:04:05.000Z\"\n+       Timestamp string\n        // When formating the Index use the Timestamp from the Message instead of\n        // time of processing. Defaults to false.\n        ESIndexFromTimestamp bool toml:\"es_index_from_timestamp\"\n@@ -352,6 +355,7 @@\n        config := &ESLogstashV0EncoderConfig{\n                Index:                \"logstash-%{2006.01.02}\",\n                TypeName:             \"message\",\n+               Timestamp:            \"2006-01-02T15:04:05.000Z\",\n                UseMessageType:       false,\n                ESIndexFromTimestamp: false,\n                Id:                   \"\",\n@@ -377,6 +381,7 @@\n        conf := config.(*ESLogstashV0EncoderConfig)\n        e.rawBytesFields = conf.RawBytesFields\n        e.fields = conf.Fields\n+       e.timestampFormat = conf.Timestamp\n        e.useMessageType = conf.UseMessageType\n        e.coord = &ElasticSearchCoordinates{\n                Index:                conf.Index,\n@@ -401,7 +406,7 @@\n                        writeStringField(first, &buf, @uuid, m.GetUuidString())\n                case \"timestamp\":\n                        t := time.Unix(0, m.GetTimestamp()).UTC()\n-                       writeStringField(first, &buf, @timestamp, t.Format(\"2006-01-02T15:04:05.000Z\"))\n+                       writeStringField(first, &buf, @timestamp, t.Format(e.timestampFormat))\n                case \"type\":\n                        if e.useMessageType || len(e.coord.Type) < 1 {\n                                writeStringField(first, &buf, @type, m.GetType())\n```\nThanks in advance for any help\n. Yes, your correct it an error on my side.\nThe build is working:\ngit clone -b versions/0.9 https://github.com/mozilla-services/heka.git\ncd heka\nbash\nexport CC=/usr/local/bin/gcc48\nsource build.sh\ncpack\n. +1\n. ",
    "perajovic": "+1\n. ",
    "sperlic": "+1\n. ",
    "djamelfel": "I am also very interested by this feature.\nDo you think it's will be possible to see it in the next release ?\n. You're right thanks, and about HttpOutput ?\n. Thanks for your quick answers\n. I found what was my problem. It's because in my file config i wrote \ntoml\nmessage_matcher = \"Type == 'heka.sandbox-output'\"\ninstead of\ntoml\nmessage_matcher = \"Type == 'heka.sandbox.output'\"\n. The mistake was in plugin_loader.cmake, I insert a git attribute\n. I try to run one more time the same configuration as previously and I'm surprise to see it's working.\n. I did new test and I found the same problem with a negative value.\nMy config is as following:\n``` lua\nrequire \"circular_buffer\"\nlocal cbufs = { }\nfunction init_cbuf(name)\n    cb = circular_buffer.new(60, 1, 1)\n    cb:set_header(1, name, 'count', 'none')\n    cbufs[name] = cb\nreturn cb\n\nend\nfunction process_message()\n    local ts = read_message('Timestamp')\n    local name = read_message('Fields[name]')\n    local value = read_message('Fields[value]')\nlocal cb = cbufs[name]\nif not cb then cb = init_cbuf(name) end\ncb:set(ts, 1, value)\n\nreturn 0\n\nend\nfunction timer_event(ns)\n    for key, cb in pairs(cbufs) do\n    local value = cb:compute('max', 1)\n    local data = {\n        Timestamp = ns,\n        Fields = {\n            value = value,\n            name  = key\n        }\n    }\n    inject_message(data)\n    data = { }\n    end\nend\n```\n. ",
    "jotes": "@rafrombrc I'll try.... is there any eta for the next release?\n. @bbinet I'll look into this today/tomorrow and i let you know if i can't make it or i'll take something else.\n@rafrombrc it would be awesome if you could give some ideas about final features for this implementation/issue?\n. Sounds okay. I'll try to hack something asap (probably during the weekend).\nOn Thu, Jan 22, 2015 at 10:33 PM, Rob Miller notifications@github.com\nwrote:\n\nHere are my thoughts on how this should work:\n- We support a limited number of time intervals: hourly, every 4\n  hours, every 12 hours, or daily. I'm not married to that exact set, but\n  what's important is that an interval goes evenly into 24 hours.\n- If you use rotation, regardless of when you start Heka, the files\n  will be named relative to midnight of the day. So if you pick hourly, file\n  rotation happens on the hour, you don't end up rotating files at 42 minutes\n  after every hour, or something like that. If you pick every 4 hours, it\n  happens at midnight, 4am, 8am, 12pm, 4pm, 8pm, etc.\n- If Heka starts and a file already exists for the current interval,\n  the existing file should be appended to.\n- If rotation is in use, then the output file name can support Go's\n  time.Format http://golang.org/pkg/time/#Time.Format syntax to embed\n  timestamps in the filename. This should allow folks to do nested folders\n  just by putting a path separator in the filename. So to achieve what\n  @bbinet https://github.com/bbinet asked for above, you'd use a\n  setting of path = \"2006/01/02.log\".\nI think this is useful enough w/o being so heavy as to force us to add a\nnew dependency for parsing cron format, etc. Does this all make sense?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/976#issuecomment-71103356\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n.  Hi @rafrombrc @bbinet, sorry for a sudden change of heart but i'll probably won't be able to make this feature before release in next week. I'll gladly take any task with longer eta.\nI hope i didn't generated too much problems for you.\n. @rafrombrc i'll look into this issue if you don't mind and nobody already works on this.\n. @whd looks like it's caused by '/part' without channel parameter. It looks strange... Could you describe more or less how to reproduce this issue?\n. which was parted in response to the actual PART command.\nYes, you're right. That's what i meant in my fuzzy way.\nwhich is weird since you can't really just part from no channel as far as I'm aware.\nYeah, it violates RFC in my opinion. I can mock this but it would be good to test it against real scenario.\n. Hi @rafrombrc, I've created PR with my solution... could you review this (and merge if possible?).\nThanks in advance.\n. @rafrombrc thx! i'll update PR later.\n. @rafrombrc I've rebased and added item in CHANGES. Could you take a look?\n. @rafrombrc Sure! Could you look if description is okay? In a meantime I'll create PR for 0.8.0\n. @rafrombrc :  \"even if it means a little bit of code duplication.\"\nI had the same thought, but as a newbie in Golang i had three solutions in mind:\na) goto with label (which was tempting but a little unorthodox)\nb) strange duplication in code which could look bad\nc) current option\nI was aware that it isn't super optimal in normal circumstances. Just for curiosity, did you consider option a)?\nI'll add more tests and then implement notes. THX for your support.\n. @rafrombrc I've redesigned code of the loop, could you review it again? I think after implementation of tests in TCPOutput i'll be ready (should be today or tomorrow) to final review. I've tested some of scenarios locally (especially blocking) and improved it by adding Gosched() and handling of heka shutdown.\n. @ecnahc515 thx, i've rebased my PR with current dev.\n. @rafrombrc I've updated PR according to your notes. Could look at this again? Thanks in advance :)\n. @simonpasquier I would add an option for this variant as it's backward incompatible, that's one thing. Also, i would describe this behaviour to clear out confusion described by you (and by some other people, i presume).\n. I'll take a look after i'll finish fixes to PR connected to TCPOutput.\nUnfortunately i have freetime only during the weekends, its pretty hard to\ndo anything after work hours. However, its fun and pleasure doing something\nfor heka.\nOn Thu, Dec 4, 2014 at 10:51 PM, Rob Miller notifications@github.com\nwrote:\n\nA user has reported that using the Heka client code to send data to a Heka\ninstance over TCP is losing messages when Heka restarts. The following is\nsample code that is said to cause the issue. To reproduce, point this at a\nrunning TcpInput and start it, then restart Heka while it's running, verify\nthat at least one message was alleged to have been delivered but actually\ndoesn't make it through the Heka pipeline.\npackage main\nimport (\n    \"fmt\"\n    \"github.com/mozilla-services/heka/client\"\n)\nconst hekaAddr = \"127.0.0.1:3242\"\nfunc main() {\n    message_bytes := []byte {97}\nsender, err := client.NewNetworkSender(\"tcp\", hekaAddr)\nif err != nil {\n    fmt.Println(\"Could not connect to\", hekaAddr)\n    return\n}\nfmt.Println(\"Connected\")\nfmt.Println(\"Shut down heka and press enter\")\nfmt.Scanln()\nvar i int\nfor i = 0; i < 100; i++ {\n    err = sender.SendMessage(message_bytes)\n    if err != nil {\n        break\n    }\n}\nfmt.Println(\"sent\", i, \"messages\")\n// Prints \"sent 1 messages\"\n}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1213.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @rafrombrc Sorry for delay. I can easily reproduce this issue.\nFrom initial investigation it looks like problem occurs because heka implements SendMessage() as basically wrapper for connection.write from net package.\nAssuming we want to make SendMessage (name is a pretty misleading in current case) more reliable and verbose there are some scenarios to consider:\n- SendMessage should check how many bytes were written and if their size isn't equal to sizee of passed bytes, return error. We can also pass the result of conn.write to user (which would be a redundant but gives him a possibility to implement his own reliability layer).\n- SendMessage blocks (with timeout) until message is fully sent, however it would require some additional mechanisms to support reliability of messages.\nProbably there are more but nothing comes to my mind (i probably need some sleep). I'm hoping for your guidance again and thanks for your time :)\n. Zhaakhi - definitely good idea.\nOn Thu, May 21, 2015 at 3:44 PM, zhaakhi notifications@github.com wrote:\n\nThe main problem as I see it is that it's not clear what the \"default\" way\nto push messages to Heka should be, for clients and for Heka to Heka\ntopologies.\nTcpInput seems to be the intended way, since it defaults to\nProtobufDecoder and is what test tools like heka-inject and heka-flood use,\nbut I think most people expect a stream processing system to provide\nreliable delivery by default.\nFor my use TcpInput/TcpOutput works acceptably, though the caveats should\nbe documented.\nMaybe a good start would be to document that if you need reliable\nat-least-once delivery you should currently use something like AMQP or\nKafka. HttpInput/HttpOutput could also be an alternative, but HttpOutput\ndoesn't currently batch messages according to the doc.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1213#issuecomment-104281811\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. hi @ioc32,\nI've checked code of Heka's Rabbitmq client to be sure and it looks like a bug in heka code.\nIts relatively easy to fix in readable manner. I'll post PR soon.\n. It looks like hekad requires to specify more than 0 parameters. I'll post PR which for this, it will probably land in 0.9.0 as it changes default behaviour of hekad.\n. @rafrombrc  Also, maybe we should to put some kind of a sample file in /etc/hekad.toml?\n. Okay, i'll add it later (I mean today, i hope so)\nOn Wed, Jan 7, 2015 at 1:09 AM, Rob Miller notifications@github.com wrote:\n\nSorry for the delay, but the holidays are over and I'm getting through the\nbacklog now. This is a simple fix, looks great, just needs an entry in the\nbug fixes section of CHANGES.txt for 0.9 and I'll be happy to merge.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1250#issuecomment-68958124\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Could you post your config file?\nOn Mon, Jan 12, 2015 at 10:01 AM, Nicolas Lamirault \nnotifications@github.com wrote:\n\nSame behavior for me. (#1231\nhttps://github.com/mozilla-services/heka/issues/1231)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1265#issuecomment-69542608\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Maybe it would be good to split repository a little and add some \"contrib\" submodule with things like this? I mean, i'm not sure if support for heroku should be a part of official maintenance.\n@carlanton i did a little review because github didn't display me Rob's comment. I hope you don't mind.\n. Okay, so i'll stick to the previous version of changes.\nOn Thu, Jan 15, 2015 at 11:59 PM, Rob Miller notifications@github.com\nwrote:\n\nSorry to take so long to look at this. Finally getting to it now.\nUnfortunately, I think this isn't the right approach. I don't think\nthere's much win to having two different loggers for each type of\nPluginRunner. The only thing it buys us is the ability to inject type into\nthe logger itself, but that's easily enough just written into the output\ntext in the various LogError / LogMessage methods.\nI'd have LogError just use the global logger, as it's already doing. At\nstartup we can create an alternate logger for stdout and store it in the\nGlobalConfigStruct, and every runner's LogMessage method can use that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1274#issuecomment-70179174\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @liukun no, you can resolve conflicts by merging or rebasing and after that you should push changes with --force. It will update PR accordingly.\n. Looks like its duplicate of #850.\n. @rafrombrc Okay, could you look now?\n. Okay, i didn't know about \"-h\" :D i just assumed it must be done\n\"manually\". I'll load fix in next couple of hours.\nOn Wed, Jan 21, 2015 at 7:32 PM, Rob Miller notifications@github.com\nwrote:\n\nOoops, my bad, I misread the diff, now I see that you removed the \"no\nflags\" check, this does resolve #1239\nhttps://github.com/mozilla-services/heka/issues/1239. But the other\npart of my comment still stands; we get -help for free, so we just need\nto remove the entire if flag.NFlag() clause.\nAlso, please add a bug fix note to CHANGES.txt. :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1283#issuecomment-70893711\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @rafrombrc okay, i've updated  pull request.\n. Hi, I've read through the code as you asked on IRC.\nMy initial thoughts are (sorry for some oversimplifications):\n- Decoders are displayed in reports because their runners are available in \"global\" slice called allDecoders.\n- DecoderRunners register themselves in this slice and because of that reporting module can generate reports for all decoders.\n- Runners implement mostly asynchronous communication via channels.\n- However, deliverer reimplements part of runner functionality to provide \"synchronous\" feature for running decoders. That's 2 different places for \"the same\" thing. I think deliverer makes it a little more hidden.\n- One of the first thoughts ws to provide new base interface (called e.g. DecoderBaseRunner) for runners and move current DecoderRunner into DecoderRunnerAsync and create second implementation called DecoderRunnerSync. However, i think it is overengineering.\n- Second idea was to remove current code related to syncdecode from deliverer and add simple waitgroup and later pass it  (together with pack via new struct) to the decoder and just .wait() until runner calls .done(). I think thats the better option to implement because leaves runners almost untouched (and the reporting layers) and don't change interfaces of existing decoders.\n@rafrombrc what do you think about these options?\n. Good point, i didn't even consider situation when there's only 1 CPU\nbecause i thought its pretty rare in 2015 (and its not an irony from my\nside).\nAnyway, even if we don't want to split code in terms of goroutines, there's\nstill one confusing thing - AllDecoders is a slice of DecoderRunner(s). We\ncould (correct me if i'm wrong):\n- change type of allDecoders elements from DecoderRunner to Decoder and\n  refactor rest of the code to reflect this change\n- remove some of parts from DecoderRunner interface (or create just\n  separate interface for async runners) and create two implementations - sync\n  and async. It would separate concerns in my opinion.\nOn Mon, Mar 16, 2015 at 9:05 PM, Rob Miller notifications@github.com\nwrote:\n\nUnfortunately I don't think this is the right approach. The entire point\nof the synchronous_decode option is to avoid using multiple goroutines, and\npaying the associated syncronization and scheduling costs. Usually async\ndecode is a better choice, but sometimes when MAXPROCS=1 synchronous\ndecoding will be more efficient because goroutines can't be run in parallel\nso the cost of context switching doesn't provide any gains.\nInstead I think the InputRunner should notice when a synchronous decoder\nis in use, and it should add the instantiated decoder to the global set of\ndecoders so reporting will happen as it does with the async decoders.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1312#issuecomment-81907775\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Re: MAXPROCS: Okay, I stand corrected. You're obviously right :) Every day i discover something new.\nAbout ticket: I can add separate slice, but don't you think it will be DRY violation? If its not i'll start working on adding new slice for syncDecoders.\n.  Not identical, but in terms of concerns there's two places to where we\n\"run\"/execute decoder. I'm outsider and i don't want to argue here - i'm\nafter quick marathon with this code and i probably have some different\nhabits.\nAnyway i fully trust your judgement here.\nIf separate slice of decoders is final option here i'll start working on\nthe patch. Can i assume that similar thing should be done in #1313?\nOn Mon, Mar 16, 2015 at 11:08 PM, Rob Miller notifications@github.com\nwrote:\n\nDRY? Not sure how storing two related-but-not-identical data points in two\ndifferent struct attributes can be construed as repeating ourselves.\nAnyway, DRY is a useful concept, but hardly an inviolable law. There are\noften times when repeating a small amount of code is a better idea than\njumping through awkward hoops (or incurring unnecessary performance\npenalties) to avoid doing so.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1312#issuecomment-81962999\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Hm, strange - tests somehow passed locally on my machine. I must forgot to\nrecompile (?). I'll fix this later, thx for notice!\nOn Mon, Feb 16, 2015 at 3:34 AM, Chance Zibolski notifications@github.com\nwrote:\n\nAny reason this is using uint32 and then converting to uint64 instead of\njust using a uint64 as the type from the beginning? I imagine the\ndifference in types could be related to the test failures.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1339#issuecomment-74452718\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @rafrombrc thx... sorry for such delays - i have some issues in my private life and just caused me to stop coding.\n. Hi, I've started working on this for my personal project. I'm not sure if it will be merged to heka because of some important design changes (i've had to remove grouping of packs in one messages) but i'll try to provide PR ASAP.\n. @rafrombrc Hi, could you review?\n. Good point. I'll update PR to reflect your comment.\n. @trink hi, i've updated PR. Could you review it again? \n. @trink updated PR again. Added new test for validation of configuration provided by user.\n. Sure :)\nOn Thu, Mar 12, 2015 at 5:44 PM, Rob Miller notifications@github.com\nwrote:\n\nThis looks awesome, and I'd love to get it into 0.9.1... would you be\namenable to closing this PR and opening an identical one against the\nversions/0.9 branch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1403#issuecomment-78522560\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @rafrombrc new PR for release 0.9.1 :)\n. Ah, sorry it looks like fault... :O Looks i've just confused/misread issue\nnumbers, i'm not sure why.\nI'll prepare patch for UDP data size against 0.9, sorry for the confusion :(\nOn Fri, Mar 27, 2015 at 1:53 AM, Rob Miller notifications@github.com\nwrote:\n\nClosed #1435 https://github.com/mozilla-services/heka/pull/1435.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1435#event-266489679.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Hi, i can start working on this. Can we treat strings without delimiters as a incomplete data? [similarily to handling data in streams]  We would  pass them without delimiter appended at the end?\n. I'm pasting directions provided by @rafrombrc on #heka channel:\n<RaFromBRC> the idea would be to end up with a `graphite.lua` module that lives here: https://github.com/mozilla-services/heka/tree/dev/sandbox/lua/modules\n<RaFromBRC> mainly i want to be able to pass in a bucket name, an interval (measured in seconds), and either a single value (for counts) or an array of values (for timers)\n<RaFromBRC> the return value would be a string containing all of the graphite formatted lines for that stat\n. @rafrombrc & @trink  could you review?\n. Yup, that's from my local machine. I'll try to post more info if it will\nhappen again.\nOn Tue, Apr 14, 2015 at 1:58 AM, Rob Miller notifications@github.com\nwrote:\n\nAre you seeing this on your local machine? We've been seeing it\nintermittently on Travis for a long time (see #1041\nhttps://github.com/mozilla-services/heka/issues/1041) but in all of\nthat time I've never been able to see it happen on my local machine. If\nyou're actually seeing it happen directly then maybe we'll have a better\nchance of tracking it down.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1477#issuecomment-92541071\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Closing because of https://github.com/mozilla-services/heka/pull/1480\n. @echampet i would use tmpfs, it will give you some performance gain and don't require refactoring of buffering layer in heka (however, i'm not up to date with development).\n@intjonathan it would cause storing messages in internal memory of Heka's process, am i right?\n. @rafrombrc I've updated PR with docs. Any language fixes/improvements are welcome (as always).\n. @trink ok, updated. \n. Hi everyone,\nAfter some code reading i think there's a possibility that ESOutput sets\noutputbuffer to nil:\nhttps://github.com/mozilla-services/heka/blob/dev/plugins/elasticsearch/elasticsearch.go#L228\nNewBufferedOutput can return nil when it's impossible to create queue\nfolder or if creation of splitter fails. That could cause situation when\nPointer to buffedouputput and value of sentMessageCount would be nil .\nAlso, i think that QueueIsFull error is possible only when outputbuffer\ntries to call queuerecord(), so if err == QueueIsFull should be false everytime.\nOn Thu, Apr 30, 2015 at 11:22 PM, Curt Micol notifications@github.com\nwrote:\n\nFYI, the panic was spewing into logs over and over again, not sure if that\nchanges your speculation. I'll upgrade and see if I can reproduce the\nbehavior to test.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1507#issuecomment-97972437\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @trink i can take this ticket if you agree.\n. @trink @rafrombrc It looks like Jehiah's implementation doesn't support all switches from strftime.org which can be a little misleading for users.\nAfter some research, i've found two (imho better) alternative libraries:\n- http://godoc.org/github.com/cactus/gostrftime\n- http://godoc.org/github.com/fastly/go-utils/strftime\n. I've chosen fastly/go-utils because they've implemented most of the modifiers of strftime(8).\nHowever, it looks like that strftime can't support fractional seconds like time.Format() does.\nSome of the tests for ElasticSearchOutput require to support miliseconds but that's currently impossible via strftime (however, it looks like there's %f switch in python implementation which is rendered to microseconds).\nHowever we can implement our own extension of strftime, like Joey suggested here: https://joeyh.name/blog/entry/strftime_fractional_seconds/\nThat would give us possibility to specify %3.3S for miliseconds and etc.\n@rafrombrc @trink I think we should revert changes from previous PR because it doesn't make a sense to release it until i'm done with this issue. Could you also give me some advice ? \nAlso i've changed my github username into @jotes because its relatively easier to write and spell out. Sorry for the initial confusion :-)\n. @eldondev yeah, you're right. I've discussed that matter some time ago with @rafrombrc on irc and final decision was to drop support for nanoseconds. I've updated PR in meantime and its probably finished, it requires only final review from @rafrombrc or @trink .\n. I think it would require PR from author and some tests (i mean unit tests).\nOn Tue, May 12, 2015 at 1:09 PM, jingbli notifications@github.com wrote:\n\ni see https://github.com/brandonbell/heka-hdfs ,so i want know the plugin\ncan merge in heka?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1524.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. @intjonathan Did you considered using TcpInput plugin or HttpListenInput plugin which are already included in heka?\n. Do you want to work on this issue or can i try dig into it?\n. Okay, will do.\nOn Tue, May 26, 2015 at 10:42 PM, Rob Miller notifications@github.com\nwrote:\n\nOverall this looks good, just need to add ESLogstashV0Encoder to the list\nof changes in the changelog, and update the docs for PayloadEncoder and\nESLogstashV0Encoder to reflect use of the strftime format.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1550#issuecomment-105659042\n.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. I think we should create separate ticket to investigate why NUM_JOBS has race condition.\nThx for your work. \n. but it's done when EOF happens so i think (correct me if i'm wrong) its good moment.\n.  Did you tried to parametrize this module via config? Looks like there's missing toml: tag\n. Looks like misspelled variable.\n. Looks like there's missing LogError.\n. Personally I think it would be good to create identificators like InvalidMsgCountError to easier handling later.\n. It would be super to add some docstring with information about purpose of this callback(?).\n. I would change that into LogError\n. Generally, your code looks good for me (as an independent reviewer). But, i was thinking about different approach to the problem caused by performance concerns (however, its only my suggestion). Did you thought about using timer to create separate go function which would be called periodically (in periods defined by user) instead of calling it for every write?\nAlso it would be crucial to create some tests for this new feature.\nHowever, thats just my thoughts @rafrombrc is more important. Anyway, good job and keep on hacking :)\n. could you look again?\n. Yeah, i'm relatively new to lua and alert.lua was my entrypoint. I've spotted 3 different conventions for modules and i wasn't sure which standard here. ;-) Can i start working on #1496 after i'll clean up this PR?\n. Okay, then i'll update  PR (with docstrings)  later\nOn Thu, Apr 23, 2015 at 4:10 PM, Mike Trinkala notifications@github.com\nwrote:\n\nIn sandbox/lua/modules/graphite.lua\nhttps://github.com/mozilla-services/heka/pull/1492#discussion_r28965410:\n\n@@ -0,0 +1,135 @@\n+-- This Source Code Form is subject to the terms of the Mozilla Public\n+-- License, v. 2.0. If a copy of the MPL was not distributed with this\n+-- file, You can obtain one at http://mozilla.org/MPL/2.0/.\n+\n+--[[\n+Module contains utilities for Graphite\n+--]]\n+-- Imports\n+require \"string\"\n+require \"math\"\n+require \"table\"\n+\n+local M = {}\n+\n\nI was playing with some code to address 1496 but the original hack of not\nusing environments will have to stand for now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1492/files#r28965410.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. Could you check again?\n. Sure, will do.\nOn Mon, Apr 27, 2015 at 3:53 PM, Mike Trinkala notifications@github.com\nwrote:\n\nIn sandbox/lua/testsupport/graphite.lua\nhttps://github.com/mozilla-services/heka/pull/1492#discussion_r29147392:\n\n\nlocal num_stats = 0\n  +\nfor bucket, count in pairs(status_codes) do\nnum_stats = num_stats + 1\nend\n  +\nadd_to_payload(graphite.multi_counts_rates(status_codes, ticker_interval, now_sec))\nadd_to_payload(graphite.multi_timeseries_metrics(request_times, ticker_interval, percent_thresh, now_sec))\n  +\nfor bucket, times in pairs(request_times) do\nnum_stats = num_stats + 1\nend\n  +\nadd_to_payload(string.format(\"stats.statsd.numStats %d %d\\n\", num_stats, now_sec))\ninject_payload(\"txt\", \"statmetric\")\n  +end\n\n\nAlso not critical, add an new line to the end of the files.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1492/files#r29147392.\n\n\njarek@reijutsu:~$ fortune\nYou have Egyptian flu: you're going to be a mummy.\njarek@reijutsu:~$ fortune\nYou now have Asian Flu.\n. I was sure that i've puhed updated version of docs. I'll fix this now.\n. :+1: it was definetely missing piece - i couldn't decide if its really required in api.\n. ",
    "Dieterbe": ":thumbsup: \n. same problem with cmake 2.8.12.2\nhttps://gist.github.com/Dieterbe/37a55e6ea6ae90a85d9f\n. bad news :( the current versions/0.8 branch fails to build for the same reason :(\ni'm at commit 337d435920e37b0291755f6b94159da380fefdfd\n. oops, i didn't realize that commit hadn't been merged.\ni manually applied the changes and seems to fix the build for me.\n. also, there's another part in the getting started guide i found unclear:\n\nIf you\u2019d like to generate additional graphs using other statistics, this can be done by including additional SandboxFilter sections using the same stat_graph.lua source code (i.e. filename = \u201clua_filters/stat_graph.lua\u201d).\n\nWhat do we really mean here with \"other statistics\" ?\nEarlier we say:\n\nOther stats can be added to this graph by adjusting the stats and stat_labels values for our existing stat_graph filter config\n\nso how the former case different from the latter?\nmaybe different types of stats? but isn't that what message_matcher is for?\n. @rafrombrc aha. the using other statistics in there threw me off.\nHow about rephrasing that to simply: \n\nYou can create multiple graphs by including additional SandboxFilter sections using the same stat_graph.lua source code \n\nWether the stats shown on an extra graph are all different from earlier used stats, or there is an overlap (some of the stats shown in graph A are also on graph B), that shouldn't matter right?\n. hmm my \"latest\" code seems awfully out of date.\nshould i use dev branch?\ni was just following http://hekad.readthedocs.org/en/v0.10.0b0/installing.html#from-source\n. dev builds fine, btw.\n. whoops, sorry & thanks.\n. how about:\n\"When a RetryMessageError is returned, ProcessMessage will be called with the same PipelinePack - allowing the plugin to try again - until a different result is returned.\nThe delay between subsequent retries will increase via an exponential back-off, though it won't keep increasing once it reaches one second.\"\nWe should also probably add some details around what's the starting delay and what exponent it uses (does it double each time?)\nAnd the current wording implies the exponential back-off also kicks in after a few attempts, would be nice to clarify that as well.\n. looks good.  but i'ld still append a \"(see next section)\" to it.\n. ah yes, can you just amend it a bit or something. going through the whole PR-review cycle for this seems a bit overkill.\n. ",
    "majj": "MINGWBASEDIR=e:\\MinGW\nGNU gdb (GDB) 7.4\nGNU ld (GNU Binutils) 2.22\nGNU windres (GNU Binutils) 2.22\nGNU dlltool (GNU Binutils) 2.22\nGNU Make 3.82\ndefine __MINGW32_VERSION           3.20\ndefine __W32API_VERSION 3.17\n. @trink grateful for your help. \nI remove MinGW (gcc-4.6.2) from the PATH and now it's OK. it's using gcc (tdm64-2) 4.8.1.\nI added TDM-GCC-64 (gcc 4.8.1) as the first one in the PATH but don't know why cmake picked up gcc 4.6.2.\n. MinGW is OK for avoid the '_get_output_format' issue.\n. It's success on 2nd Aug. but now fails. \n. Thank you @trink ! It's OK now.  In China we can't access code.google.com, so I copy gogoprotobuf and go-uuid manually. Last time I use gogoprotobuf-01c00a401bdf but this time gogoprotobuf-559fad465cf6, and ErrWrongType was removed from proto/decode.go. \n. happen again\uff1a\nCMake Error at cmake/externals.cmake:10 (message):\n  patch not found\nCall Stack (most recent call first):\n  CMakeLists.txt:109 (include)\n. ",
    "justenwalker": "Seems like this is still a problem on the latest dev branch. Any chance of this being fixed?\nI think there is an underlying limitation in Golang that it does not pass FILE_SHARE_DELETE access mode... This make any log rolling method fail to rename files being watched by Heka logstreamer.\nSee:\n- CR 8203043\n- SO Question\nFWIW - elastic beats has this: https://github.com/elastic/beats/blob/master/filebeat/input/file_windows.go\n. I was able to use github.com/elastic/beats to overcome this issue. \nhttps://github.com/justenwalker/heka/commit/7eb06d8bd698c3a51d784cb127ffc4718c8114b3\n. It's been several months - are there any pending issues preventing windows builds on Go 1.6?\n. @klauspost wrote:\n\nI cannot resolve this catch 22. Prior to this, I could rename sh.exe, and it would get through the build script eventually. This doesn't work anymore.\n\nThis problem still exists currently.  I was able to work around it by:\n- Removing C:\\git\\usr\\bin from the Windows PATH environment variables\n- Creating a new directory like C:\\tools and placing patch.exe there.\n- Adding C:\\tools to the windows PATH\nIt is not ideal, but it at least allowed me to build something. YMMV\n\nDoes anybody have an unofficial win64 binary? ;)\n\nFor those interested,  this is what I ended up with: USE AT YOUR OWN RISK\nheka_dev-windows-amd64.zip - Built against 9c6ae0a447cdcf192c1ad220248ca989490531fe with go1.6 \n. ",
    "mzcc1990": "Can you have some articles about how heka realize the input and output?\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\n\u53d1\u4ef6\u4eba: \"Alex Sanami\";notifications@github.com;\n\u53d1\u9001\u65f6\u95f4: 2014\u5e748\u670813\u65e5(\u661f\u671f\u4e09) \u665a\u4e0a8:32\n\u6536\u4ef6\u4eba: \"mozilla-services/heka\"heka@noreply.github.com; \n\u4e3b\u9898: [heka] Distrubuting heka  (#1026)\nAre there any plans or ideas about how Heka could be distributed across multiple cluster hosts, in a similar way to how Apache Storm scales its load.  \nWhat would be the recommended way to run multiple applications through Heka?  Is it common practice to spawn multiple heka daemons as a way to scale this out. \n\u2014\nReply to this email directly or view it on GitHub.\n. ",
    "jdoranster": "Since I reported this, I'm confused how no one else has seen a similar problem aggregating statsd input (port 8125) and delivering that to the carbon app on 2003.    Is a possible work around to run carbon listening via TCP on 2003 and have hekad use tcp instead of udp since it's all on the same server anyway? \n. I was having some trouble getting carbon to use tcp with various changes to carbon.conf so I reverted to udp and I haven't seen any heka errors for awhile.   Nothing definitive of course since it seems to be dependent on incoming load on the statsd port that heka is reading.\n. ",
    "JulienBlancher": "Thanks for your reply, I made some code in lua. I paste it here if it can help.\nIt's only compatible with Monolog. We can have it compatible with every json but we have to find a way in lua to do \"recursive\" while.\nHere is the code:  \n```\n--[[\nParses a monolog json line, and add each key as a field with its value for elasticsearch.\nConfig:\n    none\nExample Heka Configuration\n.. code-block:: ini\n[DynamicDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"/var/www/heka/sandbox/lua/decoders/dynamic_fields.lua\"\n\nExample Heka Message\n:Uuid: 5241d061-d807-413f-aebe-0cb3e40639f1\n:Timestamp: 2014-09-01T15:08:11.000Z\n:Type: logfile\n:Logger: BusinessLog\n:Severity: 7\n:Payload: \n:EnvVersion: \n:Pid: 0\n:Hostname: lmde-dev\n:type: Evaluation\n:Fields:\n    | :eval: 5\n    | :parent: Nico\n    | :sitter: Julien\n    | :timezone: Europe/Paris\n    | :Message: coucou\n    | :timezone_type: 3\n    | :channel: businesslog\n    | :extratest: a little extra\n    | :Level: 200\n    | :Level_Name: INFO\n    | :date: 2014-09-01 17:08:11\n--]]\n-- Include json utilities\njson = require(\"cjson\")\n-- Define data structure to send to elasticsearch\nlocal msg = {\n    Timestamp   = nil,\n    Type        = msg_type,\n    Hostname    = nil,\n    Payload     = nil,\n    Pid         = nil,\n    Severity    = nil,\n    Fields      = nil\n}\nfunction process_message ()\nlocal fields = {}\n\n-- Get the log line\nlocal log = read_message(\"Payload\")\n\n-- Unserialize the json\nlocal log_array = cjson.decode(log)\n\n-- Add the basic fields\nfields.Message = log_array[\"message\"]\nfields.Level = log_array[\"level\"]\nfields.Level_Name = log_array[\"level_name\"]\nfields.channel = log_array[\"channel\"]\n\n-- Add context fields\nfor key,value in pairs(log_array[\"context\"])\ndo\n        fields[key] = value\nend\n\n-- Add datetime fields\nfor key,value in pairs(log_array[\"datetime\"])\ndo\n        fields[key] = value\nend\n\n-- Add extra fields\nfor key,value in pairs(log_array[\"extra\"])\ndo\n        fields[key] = value\nend\n\nmsg.Fields = fields\ninject_message(msg)\nreturn 0\n\nend\n```\n. ",
    "JustinJudd": "You are right, I am using goimports, sorry about that. \nI was looking for test cases for loading the config file, but didn't see where those might be. Where do you recommend I put tests for this functionality?\n. I believe the function definition must be made first, so that the recursive call can be made.\n. Would you prefer this in multidecoder.go as subs are MutliDecoder specific?\n. ",
    "klauspost": "Of course, you are welcome to do that!\nI thought it might make a good standard plugin, since it is as generic as it gets, and it can be used for all kinds of formatting, and it doesn't require any programming to set up, and can be modified without restarting.\nMy main \"user\" is the sysadmin, who want to set something up without any programming. With this you can format mails, Text messages, even dump HTML (which could be linked by other output), etc. That could lower the threshold for users who doesn't know lua.\n. > I'd like to see the case where ticker_interval is 0 special cased so the output doesn't spin up a separate goroutine, but instead just synchronously sends each message like the original implementation.\nMakes sense. I will add that as a separate case.\n\nAlso, while maybe you handled the ticker_interval value yourself so you could handle the 0 case correctly, that value is intended to be used w/ the channel returned by the OutputRunner's Ticker() method.\n\nNot sure if I follow you there. I copied the name (and hopefully the meaning) from the IRC Output plugin, but maybe I am missing something.\n. I have implemented the points above, only I am still not using the OutputRunner.Ticker()\nThe reason for that is that (I think) it does something slightly different than what I see as the optimal implementation. \nInstead of being a steady ticker, it always sends the first mail as soon as it arrives, and only limits the rates when more messages than that arrives. That way you can safely set the interval to one minute, and not worry about a notification waiting 59 seconds before being sent. That gives quickest response time for \"emergencies\", and you still doesn't risk floods.\nShould I rename the parameter - could that help avoid confusion?\nOther than that your suggestions were great, and cleaned up the code a lot and avoids a lot of garbage generation.\n. I would love a Windows binary.\nI am unable to build, since\n1) Git requires  sh.exe, otherwise I get:\n```\nfatal: 'submodule' appears to be a git command, but we were not\nable to execute it. Maybe git-submodule is broken?\nCMake Error at C:/gopath/src/github.com/mozilla-services/heka/build/ep_base/tmp/go-notify/go-notify-gitclone.cmake:37 (message):\n  Failed to init submodules in:\n  'C:/gopath/src/github.com/mozilla-services/heka/build/heka/src/github.com/rafrombrc/go-notify'\nCMakeFiles\\go-notify.dir\\build.make:87: recipe for target 'ep_base/Stamp/go-notify/go-notify-download' failed\nmingw32-make[2]:  [ep_base/Stamp/go-notify/go-notify-download] Error 1\nCMakeFiles\\Makefile2:1197: recipe for target 'CMakeFiles/go-notify.dir/all' failed\nmingw32-make[1]:  [CMakeFiles/go-notify.dir/all] Error 2\nMakefile:159: recipe for target 'all' failed\nmingw32-make: *** [all] Error 2\n```\n2) If I have sh.exe in my path I get:\n```\nCMake Error at C:/Program Files (x86)/CMake/share/cmake-3.4/Modules/CMakeMinGWFindMake.cmake:22 (message):\n  sh.exe was found in your PATH, here:\nC:/git/usr/bin/sh.exe\nFor MinGW make to work correctly sh.exe must NOT be in your path.\nRun cmake from a shell that does not have sh.exe in your PATH.\nIf you want to use a UNIX shell, then use MSYS Makefiles.\nCall Stack (most recent call first):\n  CMakeLists.txt:5 (project)\nCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\n-- Configuring incomplete, errors occurred!\nCMakeFiles\\lua_hyperloglog.dir\\build.make:104: recipe for target 'ep_base/Stamp/lua_hyperloglog/lua_hyperloglog-configure' failed\nmingw32-make[5]:  [ep_base/Stamp/lua_hyperloglog/lua_hyperloglog-configure] Error 1\nCMakeFiles\\Makefile2:1147: recipe for target 'CMakeFiles/lua_hyperloglog.dir/all' failed\nmingw32-make[4]:  [CMakeFiles/lua_hyperloglog.dir/all] Error 2\nMakefile:159: recipe for target 'all' failed\nmingw32-make[3]:  [all] Error 2\nCMakeFiles\\lua_sandbox.dir\\build.make:111: recipe for target 'ep_base/Stamp/lua_sandbox/lua_sandbox-build' failed\nmingw32-make[2]:  [ep_base/Stamp/lua_sandbox/lua_sandbox-build] Error 2\nCMakeFiles\\Makefile2:1605: recipe for target 'CMakeFiles/lua_sandbox.dir/all' failed\nmingw32-make[1]:  [CMakeFiles/lua_sandbox.dir/all] Error 2\nMakefile:159: recipe for target 'all' failed\nmingw32-make:  [all] Error 2\n```\nI cannot resolve this catch 22. Prior to this, I could rename sh.exe, and it would get through the build script eventually. This doesn't work anymore.\nDoes anybody have an unofficial win64 binary? ;)\n. > Heka supports Linux, OSX, and Windows, separator should vary across platforms. \nReturn+Newline was used in the original implementation, so I didn't want to change that.\nstrings.Join seems like a very good way of reducing garbage!\n. Sounds like a good idea!\n. ",
    "obfuscurity": ":heart:\n. ",
    "robbiev": "Is this no longer happening?\n. Sure I understand. Allow me to just dump my use case here as a +1\nCurrently I'm using heka with docker and the Logstreamer input. Each time I start a container I generate a heka config file for that container and then restart heka. This is where the need for SIGHUP comes from - it would be nice to not have to restart heka - currently I send SIGTERM and then let supervisord revive it. I assume that SIGHUP would allow for a quicker reload (with very minimal downtime in theory) and thus result in a better user experience at the receiving end when viewing logs as they come in (a shorter or hardly noticeable interruption time because of heka restart).\nI know DockerLogInput exists but IIRC it is possible to miss logs when heka is not active (e.g. when restarting heka, after a crash or while booting the machine) and also at the very start when a container boots I think there is a potential time window in which container logs could go missing. I don't want to miss any logs if I can help it so I came up with the workaround I explained.\n. Oh, I indeed missed that. That works!\nsh\nmkdir -p externals\nln -s /my/project `pwd`/externals\nsource build.sh\nIt would still be useful to be able to use heka as \"go-gettable\" library because while I now no longer have to push my code to be able to see if it builds I still need my heka abstraction to be able to run my tests. Maybe part of the code gen could be done in advance (as with go generate). Heka doesn't need to be fully buildable, just the go part.\n. Yes that's the info I was looking for - thanks!\n. ",
    "DanyC97": "I see this as being closed, has this been implemented on any version recent version?\n. i am on 0.10.b1 and i still get this issue, what am i missing?\n. @s00 have you found a way ?\n. ",
    "martin82": "Looks good to me! :-)\n. ",
    "steverweber": "can this be merged soon... i do understand that it's not working on upstart yet... but if @mikn is busy the current state is better than nothing.  \nA new issue could be opened for upstart init, perhaps somone else has time to fix it.\n. that helped but still no luck:\ndashboard shows: \nLoadAvgFilter decoder filter: processed ~100\nInfluxOutput-InfluxEncoder: processed ~100\nbut InfluxOutput:  processed blank ?\ncurrent config:\n```\n[DashboardOutput]\nticker_interval = 1\n[loadavg]\ntype = \"FilePollingInput\"\nticker_interval = 1\nfile_path = \"/proc/loadavg\"\ndecoder = \"LoadAvgDecoder\"\n[LoadAvgDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_loadavg.lua\"\n[LoadAvgFilter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'stats.loadavg'\"\nfilename = \"lua_filters/loadavg.lua\"\nticker_interval = 1\npreserve_data = true\n[InfluxEncoder]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx.lua\"\n[InfluxEncoder.config]\nseries = \"heka.%{Logger}\"\nskip_fields = \"Pid EnvVersion\"\n[InfluxOutput]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type == 'stats.loadavg'\"\nencoder = \"InfluxEncoder\"\nticker_interval = 1\naddress = \"{{ vars.influxdb.address }}\"\nusername = \"{{ vars.influxdb.username }}\"\npassword = \"{{ vars.influxdb.password }}\"\n. would it be possible to send this loadavg data to the CarbonOutput plugin with the items already available in heka?  I could work with that because the influxdb server has an interface for Carbon.\n.\n[loadavg]\ntype = \"FilePollingInput\"\nticker_interval = 1\nfile_path = \"/proc/loadavg\"\ndecoder = \"LoadAvgDecoder\"\n[LoadAvgDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_loadavg.lua\"\n[LoadAvgFilter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'stats.loadavg'\"\nfilename = \"lua_filters/loadavg.lua\"\nticker_interval = 1\npreserve_data = true\n    [LoadAvgFilter.config]\n    rows = 1440\n    sec_per_row = 1\n[CarbonOutput]\ntype = \"CarbonOutput\"\nmessage_matcher = \"Type == 'heka.statmetric'\"\nmessage_matcher = \"Type == 'stats.loadavg'\"\naddress = \"{{ vars.CarbonOutput.address }}\"\nprotocol = \"udp\"\n```\n. @rafrombrc im much to new to the project to attempt creating the encoder.\nI created a new issue that is more direct to the core issue.\nhttps://github.com/mozilla-services/heka/issues/1272\nthanks for helping.  I hope soon i'll beable to kick collectd to the curb : )\n. example cbuf code:\nhttps://github.com/mozilla-services/heka/blob/b96179e44fb5094efece5b87ac14ccf7629d576d/sandbox/lua/encoders/cbuf_librato.lua\ninfluxdb api:\nhttp://influxdb.com/docs/v0.7/api/reading_and_writing_data.html\n. Congradulations on being so awesome!\nI look forward to using this cbuf thingy :)\nI would love an example of how to serialize some of the LoadAvgDecoder and mem data into somthing that can be fired off to influxdb.\nPerhaps this would be good to add to the getting started doc.  I think most people dont have a statsd install running... a pure heka solution will be \"gold\".\n. is this a typeo?\nfilename = \"lua_encoders/influxdb.lua\"\nhttp://hekad.readthedocs.org/en/v0.8.2/config/encoders/index.html#schema-influxdb-encoder\ni do however see a : lua_encoders/schema_influx.lua\n. still stuck... it seems to be kinda working but im not seeing data in:\nhttp://monitor.salt.math.xxx.ca:8083/\n```\n[hekad]\nmaxprocs = 2\n[DashboardOutput]\nticker_interval = 1\n[loadavg]\ntype = \"FilePollingInput\"\nticker_interval = 1\nfile_path = \"/proc/loadavg\"\ndecoder = \"LoadAvgDecoder\"\n[LoadAvgDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_loadavg.lua\"\n[LoadAvgFilter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'stats.loadavg'\"\nfilename = \"lua_filters/loadavg.lua\"\nticker_interval = 1\npreserve_data = true\n    [LoadAvgFilter.config]\n    rows = 1440\n    sec_per_row = 1\n[meminfo]\ntype = \"FilePollingInput\"\nticker_interval = 1\nfile_path = \"/proc/meminfo\"\ndecoder = \"MemInfoDecoder\"\n[MemInfoDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_memstats.lua\"\n[MemInfoFilter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'heka.memstat'\"\nfilename = \"lua_filters/heka_memstat.lua\"\nticker_interval = 1\npreserve_data = true\n    [MemInfoFilter.config]\n    rows = 1440\n    sec_per_row = 1\n[influxdb]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx.lua\"\n    [influxdb.config]\n    series = \"heka.%{Logger}\"\nskip_fields = \"Pid EnvVersion\"\n[HttpOutput]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type == 'stats.loadavg' || Type == 'stats.memstats'\"\nencoder = \"influxdb\"\nticker_interval = 1\naddress = \"http://monitor.salt.math.xxx.ca:8086/db/influxdb/series\"\nusername = \"?\"\npassword = \"?\"\n```\nno errors in logs...\n2015/01/22 14:40:26 Pre-loading: [loadavg]\n2015/01/22 14:40:26 Pre-loading: [LoadAvgFilter]\n2015/01/22 14:40:26 Pre-loading: [influxdb]\n2015/01/22 14:40:26 Pre-loading: [meminfo]\n2015/01/22 14:40:26 Pre-loading: [DashboardOutput]\n2015/01/22 14:40:26 Pre-loading: [MemInfoDecoder]\n2015/01/22 14:40:26 Pre-loading: [HttpOutput]\n2015/01/22 14:40:26 Pre-loading: [LoadAvgDecoder]\n2015/01/22 14:40:26 Pre-loading: [MemInfoFilter]\n2015/01/22 14:40:26 Pre-loading: [ProtobufDecoder]\n2015/01/22 14:40:26 Pre-loading: [ProtobufEncoder]\n2015/01/22 14:40:26 Loading: [MemInfoDecoder]\n2015/01/22 14:40:26 Loading: [LoadAvgDecoder]\n2015/01/22 14:40:26 Loading: [ProtobufDecoder]\n2015/01/22 14:40:26 Loading: [influxdb]\n2015/01/22 14:40:26 Loading: [ProtobufEncoder]\n2015/01/22 14:40:26 Loading: [loadavg]\n2015/01/22 14:40:26 Loading: [meminfo]\n2015/01/22 14:40:26 Loading: [LoadAvgFilter]\n2015/01/22 14:40:26 Loading: [MemInfoFilter]\n2015/01/22 14:40:26 Loading: [DashboardOutput]\n2015/01/22 14:40:26 Loading: [HttpOutput]\n2015/01/22 14:40:26 Starting hekad...\n2015/01/22 14:40:26 Output started:  DashboardOutput\n2015/01/22 14:40:26 Output started:  HttpOutput\n2015/01/22 14:40:26 Filter started:  LoadAvgFilter\n2015/01/22 14:40:26 Filter started:  MemInfoFilter\n2015/01/22 14:40:26 MessageRouter started.\n2015/01/22 14:40:26 Input started: loadavg\n2015/01/22 14:40:26 Input started: meminfo\nIf nothing jumps out as wrong at (you) thats fine... I'll just have to giveup for now.\nthanks.\n. setting the series... seems to help... but dont see data yet :(\n[influxdb]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx.lua\"\n    [influxdb.config]\n    series = \"heka.%{Hostname}.%{Type}\"\n. ah never mind my issue was i was using Grafana and it goes its select like:\nselect mean(value) from \"heka.monitor.salt.math.uwaterloo.ca.stats.loadavg\" where time > now() - 6h group by time(30s) order asc\nI dont have a 'value' but a select start works fine :)\nselect * from \"heka.monitor.salt.math.uwaterloo.ca.stats.loadavg\" where time > now() - 6h group by time(30s) order asc\nAll is well thanks!\n. the code is kinda ugly... this PR is more food for thought.  feel free to close.\n. Yap that's fair.  I'll work on something useful tomorrow (that /proc/stat\nplugin)\nOn Feb 4, 2015 9:30 PM, \"Rob Miller\" notifications@github.com wrote:\n\nClosing due to lack of interest... ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/pull/1286#issuecomment-72983572\n.\n. thanks!\nBen's solution also needs work... Is he activaly working on it? \n@bbangert \n. @bbangert Honestly I havent took the time to grok your code yet... at first glance some of the docs could use cleanup and the 'for 100 loop' should be redone.  Im on a system with 64cpus in the list I would not be supprised if others have servers with > 100.\n\nAlso it would be nice if some configurations could be added to ignore data that is not needed.\nPersonally im only interested in the global cpu stat delta I dont need to proccess the other cpu[0-63].\n. @bbangert  Your code does look good tho :)  do you have any plans on improving it?\n. @bbangert interesting usecase.. Since this code is not a priority of yours I hope you dont mind if I fork it and hopfully get it merged into heka.\n. i'll likly add a few tweaks... \n. this is depericated by https://github.com/mozilla-services/heka/pull/1320\n. let me know what else needs fixing.\nO also It would be nice if I could use somthing more like regex for the whitelist_pattern... any ideas?... i want to pass it in as a string.. im not sure if lpeg has any support for stuff like that.\n. while I still have time to hack on this.\nfeedback please.\n. thanks @trink\n. @bbangert forgot to include you in this PR. Perhaps you have some feedback and could preform some testing.\n. I got some time.. @trink ill make them changes and test\nThanks!\n. I never tested require \"re\" yet because i'm using the older binary build of heka.\n. this merge should be fine to merge after the whitelist_regex uses 're'\nhowever some feedback would be nice on the other above items.\nthanks.\n. see new PR.\n. opps, ill fix that up now.\n. Bla, sorry @trink , watching movie, /not all there/.  I can fix tomorrow or please feel free to fork and fix\n. thanks for fixing that up.\n. +1\njust opened a issue then found this PR.\nhttps://github.com/mozilla-services/heka/issues/1573\nJust missing the output decoder.\n. logstash has a syslog input... thats kinda what I'm looking to support from heka,\nhttps://www.elastic.co/guide/en/logstash/current/plugins-inputs-syslog.html\n. correct me if im wrong having an UdpOutput formatter that does one of the simple syslog styles should be straight forward?\nI'm looking to have heka parse log files then send them to this logstash plugin using the syslog format. (its how the IT staff seem to be collecting logs)\nhttps://www.elastic.co/guide/en/logstash/current/plugins-inputs-syslog.html\n@cristi1979 perhaps you have an example Encoder.\nThanks.\n. +1\n. missing help for new line encoder:\nhttp://hekad.readthedocs.org/en/v0.10.0b0/config/encoders/schema_influx_write.html#config-schema-influx-write-encoder\n. probably doing something silly wrong, but I cant seem to get this to work:\ninstalled influxdb 9.1 using default settings.\nopened: http://127.0.0.1:8083\ncreated database : woot\ninstalled heka \nwget https://github.com/mozilla-services/heka/releases/download/v0.10.0b0/heka_0.10.0b0_amd64.deb\nsudo pkg -i heka_0.10.0b0_amd64.deb\nvi /etc/heka/conf.d/01-test.toml\n```\n[DashboardOutput]\nticker_interval = 1\n[LoadAvgPoller]\ntype = \"FilePollingInput\"\nticker_interval = 5\nfile_path = \"/proc/loadavg\"\ndecoder = \"LinuxStatsDecoder\"\n[LoadAvgDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_loadavg.lua\"\n[LinuxStatsDecoder]\ntype = \"MultiDecoder\"\nsubs = [\"LoadAvgDecoder\", \"AddStaticFields\"]\ncascade_strategy = \"all\"\nlog_sub_errors = false\n[AddStaticFields]\ntype = \"ScribbleDecoder\"\n        [AddStaticFields.message_fields]\n        Environment = \"dev\"\n[InfluxdbLineEncoder]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx_line.lua\"\n    [InfluxdbLineEncoder.config]\n    skip_fields = \"**all_base** FilePath NumProcesses Environment TickerInterval\"\n    tag_fields = \"Hostname Environment\"\n    timestamp_precision= \"s\"\n\n[InfluxdbOutput]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type =~ /stats.*/\"\nencoder = \"InfluxdbLineEncoder\"\nusername = \"admin\"\npassword = \"admin\"\naddress = \"http://127.0.0.1:8086/write?db=woot&precision=s\"\n```\nrestarted heka service...\nwatched heka dashboard ... http://127.0.0.1:4352\nlooks to be doing stuff.\n InfluxdbOutput-InfluxdbLineEncoder processed 74...\nreturned to http://127.0.0.1:8083 | influxdb  and tried to find the data...  but with no luck...\nthoughts?\n. thanks for the debug hint @rafrombrc.  Perhaps @acesaro has a hint because I'm using basically the same config that was found in the example for schema_influx_line.lua\n.\nOnce i get this to work I can confirm this issue is resolved and can be closed :)\n. I check LogOutput with a PayloadEncoder and only get blank lines... so the problem is likely not the InfluxdbLineEncoder or the example... just me\nThanks for confirming my config looks fine!\n. @rafrombrc  yes that was what i tried first before just using the PayloadEncoder.\nfull config\n```\n[hekad]\nmaxprocs = 2\n[TcpInput]\naddress = \"127.0.0.1:5565\"\n[StatAccumInput]\n[DashboardOutput]\nticker_interval = 1\n[LoadAvgPoller]\ntype = \"FilePollingInput\"\nticker_interval = 5\nfile_path = \"/proc/loadavg\"\ndecoder = \"LinuxStatsDecoder\"\n[LoadAvgDecoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/linux_loadavg.lua\"\n[LinuxStatsDecoder]\ntype = \"MultiDecoder\"\nsubs = [\"LoadAvgDecoder\", \"AddStaticFields\"]\ncascade_strategy = \"all\"\nlog_sub_errors = false\n[AddStaticFields]\ntype = \"ScribbleDecoder\"\n        [AddStaticFields.message_fields]\n        Environment = \"dev\"\n[InfluxdbLineEncoder]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx_line.lua\"\n    [InfluxdbLineEncoder.config]\n    skip_fields = \"**all_base** FilePath NumProcesses Environment TickerInterval\"\n    tag_fields = \"Hostname Environment\"\n    timestamp_precision= \"s\"\n\n[LogOutput]\nmessage_matcher = \"Type =~ /stats.*/\"\nencoder = \"InfluxdbLineEncoder\"\n[InfluxdbOutput]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type =~ /stats.*/\"\nencoder = \"InfluxdbLineEncoder\"\nusername = \"admin\"\npassword = \"admin\"\naddress = \"http://127.0.0.1:8086/write?db=woot&precision=s\"\n```\nheka log.\n2015/07/16 20:41:34 Pre-loading: [LinuxStatsDecoder]\n2015/07/16 20:41:34 Pre-loading: [AddStaticFields]\n2015/07/16 20:41:34 Pre-loading: [InfluxdbLineEncoder]\n2015/07/16 20:41:34 Pre-loading: [LogOutput]\n2015/07/16 20:41:34 Pre-loading: [DashboardOutput]\n2015/07/16 20:41:34 Pre-loading: [LoadAvgPoller]\n2015/07/16 20:41:34 Pre-loading: [LoadAvgDecoder]\n2015/07/16 20:41:34 Pre-loading: [TokenSplitter]\n2015/07/16 20:41:34 Loading: [TokenSplitter]\n2015/07/16 20:41:34 Pre-loading: [HekaFramingSplitter]\n2015/07/16 20:41:34 Loading: [HekaFramingSplitter]\n2015/07/16 20:41:34 Pre-loading: [NullSplitter]\n2015/07/16 20:41:34 Loading: [NullSplitter]\n2015/07/16 20:41:34 Pre-loading: [ProtobufDecoder]\n2015/07/16 20:41:34 Loading: [ProtobufDecoder]\n2015/07/16 20:41:34 Pre-loading: [ProtobufEncoder]\n2015/07/16 20:41:34 Loading: [ProtobufEncoder]\n2015/07/16 20:41:34 Loading: [AddStaticFields]\n2015/07/16 20:41:34 Loading: [LoadAvgDecoder]\n2015/07/16 20:41:34 Loading: [LinuxStatsDecoder]\n2015/07/16 20:41:34 Loading: [InfluxdbLineEncoder]\n2015/07/16 20:41:34 Loading: [LoadAvgPoller]\n2015/07/16 20:41:34 Loading: [LogOutput]\n2015/07/16 20:41:34 Loading: [DashboardOutput]\n2015/07/16 20:41:34 Starting hekad...\n2015/07/16 20:41:34 Output started: LogOutput\n2015/07/16 20:41:34 Output started: DashboardOutput\n2015/07/16 20:41:34 MessageRouter started.\n2015/07/16 20:41:34 Input started: LoadAvgPoller\n2015/07/16 20:41:39 \n2015/07/16 20:41:44 \n2015/07/16 20:41:49 \n2015/07/16 20:41:54 \n2015/07/16 20:41:59 \n2015/07/16 20:42:04 \n2015/07/16 20:42:09 \n2015/07/16 20:42:14 \n2015/07/16 20:42:19 \n2015/07/16 20:42:24 \n2015/07/16 20:42:29 \n2015/07/16 20:42:34 \n2015/07/16 20:42:39\nso this is normal? good :)  i was just kinda expecting to see some data.\n. ok thanks... this sounds like a new issue.\nbut it does sound like the influxdb 9.0 item can be resolved. cheers!\n. I'll try to find time this weekend.  But if the fix worked for you @rafrombrc  then I'll likely just wait for the releases. https://github.com/mozilla-services/heka/releases\n. +1 another beta. Would like to help test this!\n. I like this... while i made that ugly for loop i was thinking most of this junk should be done at the sandbox message API.  will this api feature be added in the next release?\n. want to use regex... but yap ill ^cpu it.\n. i think i had it heare as a note because the previous developer was saving or passing the Payload on...\nI think ill drop it.\n. i added the index entries (index_count, index) ... but thses will be removed i hope.\n. ",
    "CpuID": "+1 - this will be good to have merged :) I ended up manually taking the bits and pieces from this PR to get my system working, /etc/default/heka, /etc/init.d/heka, the addgroup/adduser calls etc etc\n. ",
    "BartVB": "Would be great if this could be merged + new PR for upstart, most of the improvements have been ready/usable since october. It's a shame not to use them.\n. ",
    "davecap": "+1\n. ",
    "carlanton": "Cool! I'll give it a shot then :-)\n. Hm.. Don't know why travis fails.. Works fine locally. Timing issues?\n. It looks removing the quotes solved the problem! I also renamed the package to just docker and changed the CMake option to INCLUDE_DOCKER_PLUGINS.\nIf you have time it would be awesome with a review! I'm not that experienced with Go and could use some pointers :-)\n. Thank you for the review! I'll try to improve the code tomorrow (UTC+1 over here).\nI suspect that mocking the docker client is a bit over my head, but I'll give it a shot! Thanks again!\n. I think I'm done for the day. ~~Unfortunately I couldn't get the plugin to work again, but it doesn't work with my original plugin (carlanton/heka-docker) either.. Very strange. I'll research this more.~~ Update: I started my docker containers in interactive mode * facepalm *. Everything works as expected!\nTo solve the problem with log messages / \"assertions\" in the attacher, I introduced a new channel for sending errors back to the event loop in the plugin. Currently it is just a string wrapped in a struct, but it is probably better to just send errors. What do you think about this?\nThe other solution I thought of was to send the ir.LogError function to the attacher, but then we could just log errors and not, for example, exit the Run function...\n. I've discovered a new problem: If the Docker daemon is restarted, Heka doesn't receive any more messages from the attacher. I would guess that there is a way to get notified of this from the Docker API, but I couldn't find anything except the Ping method.\nOne way to solve this could be to ping docker every 5 second or so, and if we don't get an answer try to re-init the client. Or maybe just return from the Run function?\n. Wow! This is awesome! Thank you for all your help and effort :-)\n. @rafrombrc Cool! The splitter thing sounds really useful.. In the meantime I'll try to write a filter :-)\nHeka is awesome!\n. By rejecting messages I meant sending a 500 response code if the body validation/parsing fail, and I don't think that is possible from a decoder. Of course we could just drop the message in Heka and just log that the paring failed. I'll try to give HttpListenInput some love instead!\n. Any news on the spliter plugin btw?\n. @4r9h Thank you for the review! :+1: I agree with you (and Rob) that this shouldn't be in Heka core. I've made some modifications to the HttpListenInput plugin in a local branch and once we have splitter plugins (see #1241) all parsing can be done in a sandbox decoder instad!\n. I think this could be pretty easy to implement. fsouza/go-dockerclient implemented a TLS support in december. I'll give it a try this weekend!\n. This problem is now solved in Heka 0.9 with the new config parameter cert_path:\nhttps://hekad.readthedocs.org/en/v0.9.0/config/inputs/docker_log.html\n. That's true, but I guess it's a pretty common use case? Maybe some documentation about it would be sufficient...\n. Sorry for my rudeness, I know all these features are very new!\nI started from the examples on http://hekad.readthedocs.org/en/latest/config/inputs/kafka.html and http://hekad.readthedocs.org/en/latest/config/outputs/kafka.html. I realize now that the input example is from before the splitters.\n. Maybe something like #1350?\n. @salekseev I haven't had time to finish all of this, but I did some benchmarks between this version and another version in Go: https://github.com/carlanton/heka/commit/930b2d8a73411ab4973833e37a24bbd219d26c3a\nThe Go version was much faster (~5-6x), not Docker specific and uses https://github.com/fluent/fluent-logger-golang/ to decode messages.\nOne limitation is that it requires the \"record\" object to be a map[string]string. It's not a problem for the format that Docker uses, but maybe we need to be more flexible for the general case?\n. @trink Hmm, I don't think so. Maybe that's the best option :-) Would that require a PR to mozilla-services/lua_sandbox?\n. @4r9h You don't need the tag thing for \"simple\" names.\n. ",
    "markabey": "It isn't essential for @type to match the index _type, but it is the default for the Logstash V0 schema. It would make a migration seamless. It may be best as a separate config parameter.\nSee docs here: http://logstash.net/docs/1.1.13/outputs/elasticsearch_http#setting_index_type\nindex_type\n- Value type is string\n- Default value is \"%{@type}\"\n  The index type to write events to. Generally you should try to write only similar events to the same 'type'. String expansion '%{foo}' works here.\n. This also affects me when using a program that runs mv on the tracked log files to a different directory and then deletes them after being copied. New ones are then made at the original path.\nHeka keeps hold of the old deleted file handle (in a different directory) if the new log is not in place when the old one is deleted, so when viewed in lsof, it shows a bunch of logs marked as \"(deleted)\". This means logs get silently dropped from tracking.\nThe only workaround seems to be having a cron job that looks for heka holding deleted file handles and restarting it. I've not tried recompiling it with the patch above.\nUsing heka 0.8\n. And the old log file doesn't have to be empty for this to occur. At least if I mv the tracked log file to a new directory, and then delete it, heka loses track of that log stream until it is restarted.\n. Possibly related to #1199 ?\n. @trink - was there a reason these repos had to be deleted? I think it might also affect https://github.com/trink/struct.git\nIt means that any older versions of software can't be built again. If they could be mirrored or reinstated somewhere (in a separate account if it clutters yours) then at least heka can still be built. https://github.com/mozilla-services/lua_sandbox/issues/209 has the repo. You can build it by pointing to the repo that @composit uploaded:\n```patch\n---------------------------- cmake/externals.cmake ----------------------------\nindex d882376..5140e52 100644\n@@ -8,28 +8,28 @@ get_filename_component(GIT_PATH ${GIT_EXECUTABLE} PATH)\n find_program(PATCH_EXECUTABLE patch HINTS \"${GIT_PATH}\" \"${GIT_PATH}/../bin\")\n if (NOT PATCH_EXECUTABLE)\n    message(FATAL_ERROR \"patch not found\")\n endif()\nset_property(DIRECTORY PROPERTY EP_BASE \"${CMAKE_BINARY_DIR}/ep_base\")\nif(INCLUDE_SANDBOX)\n     set(PLUGIN_LOADER ${PLUGIN_LOADER} \"github.com/mozilla-services/heka/sandbox/plugins\")\n     set(SANDBOX_PACKAGE \"lua_sandbox\")\n     set(SANDBOX_ARGS -DCMAKE_BUILD_TYPE=${CMAKE_BUILD_TYPE} -DCMAKE_INSTALL_PREFIX=${PROJECT_PATH} -DLUA_JIT=off --no-warn-unused-cli)\n     externalproject_add(\n         ${SANDBOX_PACKAGE}\n-        GIT_REPOSITORY https://github.com/mozilla-services/lua_sandbox.git\n-        GIT_TAG 7abcb7c661c13c970fb9e928e428551671244911\n+        GIT_REPOSITORY https://github.com/composit/lua_sandbox.git\n+        GIT_TAG dd0f11dcc07a289bb236d4f255dc5e9caa2c4784\n         CMAKE_ARGS ${SANDBOX_ARGS}\n         INSTALL_DIR ${PROJECT_PATH}\n     )\n endif()\nif (\"$ENV{GOPATH}\" STREQUAL \"\")\n    message(FATAL_ERROR \"No GOPATH environment variable has been set. $ENV{GOPATH}\")\n endif()\nadd_custom_target(GoPackages ALL)\nfunction(parse_url url)\n     string(REGEX REPLACE \".*/\" \"\" _name ${url})\n```. ",
    "placeybordeaux": "For more context HttpInput currently outputs a message of the type heka.httpinput.error\n. ",
    "arctica": "I am actually quite surprised to learn that messages just get lost. Unfortunately a showstopper for my project even though I wanted to switch from Logstash to Heka because of the great Lua scripting support and built in anomaly detection. Bummer!\nRerouting a message to another output would be a step in the right direction but ultimately there needs to be some mechanism that can suspend an output and if the queues are full also the input.\nAs a last resort I would be fine with automatically shutting down Heka if an output fails so I can investigate, fix and restart without losing data. Is there a way to achieve this?\n. ",
    "matthughes": "+1000 I'm dying to leave logstashforwarder/logstash as well but dropping logs because of a temporary blip in service is not acceptable in my situation.\n. ",
    "txchen": "+10000 this is really important feature.\n. @brokenjacobs thanks! I will take a look.\nActually I am wondering can heka itself provide this kind of feature? Then we only need one agent for system metric collecting, log shipping, and custom metric collecting, that sounds clean, right?\n. ",
    "bigbo": "I encountered the same problem,the oldest_duration is not work.\nit had fix : #1437\n. @trink  thx.\n. ",
    "gdm85": "I found the typo, \"unixgram\" vs \"unixdgram\"\n. For the records, this example is kind of useful:\nhttp://pad.yohdah.com/303/heka-unixgram-test\nBut I can't get heka to always/correctly see syslog input lines.\nI am reopening since it would be nice to have a working example for this heka feature.\n. @trink for the official documentation I guess a PR is needed?\n. I understand. Somehow I thought that was a pre-defined decoder.\n. @rafrombrc the provided example should cover http://tools.ietf.org/html/rfc5424#section-6\nHowever, if you say that there is already a decoder covering RFC5424 (that I implemented in a poor way, I have to admit), then I'll withdraw the PR :)\n. @steverweber the idea here was to provide a basic working example, then others can improve on top of it. Although didn't receive any feedback yet...\n. @cristifalcas yes, I agree with you - having heka take care of /dev/log would probably be a too big leap\n. @steverweber this PR is about an example in official release explaining how to have heka receive syslog on a listening UNIX socket e.g. /dev/log (although I agree with the reserves mentioned by @cristi1979 , that it will never be as good as a proper dedicated daemon), so it's about harvesting logs without having a syslog daemon running. Seems like what you're looking for is related to output, not input.\n. It's almost 1 year this PR has been open, is it going to be merged or..?\n. @rafrombrc ah, no problem! I was keeping around the fork only for this PR, but glad to hear there is a better way to achieve this. I see it as an important feature when comparing to alternatives like fluentd etc\n. ",
    "sourcegraphbot": "\n\n\u00a0package kafka  \n\n\n\u00a0type kafka.KafkaInput  struct\n\n\n\u00a0(*kafka.KafkaInput).CleanupForRestart()\n\n\n\u00a0type kafka.KafkaInputConfig  struct\n\n\n\u00a0(kafka.KafkaInputConfig).Addrs  []string\n\n\n\u00a0(kafka.KafkaInputConfig).BackgroundRefreshFrequency  uint32\n\n\n\u00a0(kafka.KafkaInputConfig).Decoder  string\n\n\n\u00a0(kafka.KafkaInputConfig).DefaultFetchSize  int32\n\n\n\u00a0(kafka.KafkaInputConfig).DialTimeout  uint32\n\n\n\u00a0(kafka.KafkaInputConfig).EventBufferSize  int\nShowing only the first ten definitions. View the rest on \u2731 sourcegraph.com\n\nView the full smart diff on \u2731 sourcegraph.com\nSettings\n. \n\n\u00a0package kafka  \n\n\n\u00a0type kafka.KafkaInput  struct\n\n\n\u00a0(*kafka.KafkaInput).CleanupForRestart()\n\n\n\u00a0type kafka.KafkaInputConfig  struct\n\n\n\u00a0(kafka.KafkaInputConfig).Addrs  []string\n\n\n\u00a0(kafka.KafkaInputConfig).BackgroundRefreshFrequency  uint32\n\n\n\u00a0(kafka.KafkaInputConfig).Decoder  string\n\n\n\u00a0(kafka.KafkaInputConfig).DefaultFetchSize  int32\n\n\n\u00a0(kafka.KafkaInputConfig).DialTimeout  uint32\n\n\n\u00a0(kafka.KafkaInputConfig).EventBufferSize  int\nShowing only the first ten definitions. View the rest on \u2731 sourcegraph.com\n\nView the full smart diff on \u2731 sourcegraph.com\nSettings\n. ",
    "liyichao": "Thanks for the prompt reply. \nYes, I am using plugin_loader.cmake. I thought I only need to add the plugin repository and heka will automatically go get the dependencies.\nI now add add_external_plugin(git https://github.com/garyburd/redigo.git master) to the plugin_loader.cmake, but the build system reports this error:\nno buildable Go source files in /Users/liyichao/code/heka/build/heka/src/github.com/garyburd/redigo\nIt is because go files are in directory redis/, is there a way to specify this? I try to add the following line in the plugin_loader.cmake:\nadd_external_plugin(git https://github.com/garyburd/redigo/redis master)\nThe build failed because:\nfatal: repository 'https://github.com/garyburd/redigo/redis/' not found\nIs there anything in heka build system that supports this senario? or I have to create a separate repository that consists of the subdirectory of redigo.\n. Thanks for the link. I now successfully build heka.\n. Finally, I find that in docker the build.sh fails to clone my git repository because it does not have the private key I have outside docker.\n. Sorry, I think the problem may due to docker .\nI tcpdump and find that, carbon container has sent FIN to peer, and the socket enters CLOSE_WAIT, but the peer ignore the FIN and  keep sending packet to the carbon container.\n. > 2014/12/31 11:37:00 Decoder 'nginx_access_logs-nginx_access_decoder' error: Failed parsing: 121.35.221.226 [31/Dec/2014:11:37:00 +0800] \"GET /question/27311384/answer/36171927?group_id=533585763103674368 HTTP/1.1\" 499 0 \"http://www.zhihu.com/\" \"Mozilla/5.0 (iPad; CPU OS 8_1_2 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12B440 Safari/600.1.4\" 0.622 0.401, - \"121.35.221.226\" -\n\nlog_format = '$remote_addr [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\" $request_time $upstream_response_time \"$proxy_add_x_forwarded_for\" $upstream_http_x_cache_status'\n\nI think it is 0.401, that produces a problem.\n. more log:\n\n2014/12/31 10:41:17 Decoder 'nginx_access_logs-nginx_access_decoder' error: Failed parsing: 117.29.97.75 [31/Dec/2014:10:41:16 +0800] \"GET /node/AnswerCommentListV2?params=%7B%22answer_id%22%3A%229205619%22%7D HTTP/1.1\" 499 0 \"http://www.zhihu.com/question/26774592\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36\" 127.966 29.886, 1.018, 29.907, 29.657, 29.637, - \"117.29.97.75\" -\n. I encounter this when I aggregate stats like p95, mean in my application, and want to use heka to further aggregate my stats.\n\nThe message UdpInput receives is the same as heka.statmetric message that heka agents sent to a central heka server:\n\ntest.count 2 1420438130\ntest.count_ps 0.200000 1420438130\ntest.lower 1.000000 1420438130\ntest.upper 2.000000 1420438130\ntest.sum 3.000000 1420438130\ntest.mean 1.500000 1420438130\ntest.mean_95 1.500000 1420438130\ntest.upper_95 2.000000 1420438130\n\nInstead of using protobuf, I use plaintext protocol like graphite's for simplificity, but the metrics are p95, max, min which do not fit in statsd' type system, so I can't use statsdInput.\nThe solution I choose is to use UdpInput, and write a lua filter to aggregate them. Of course, I can use \"\\n\" as the delimiter, but that produces a message pack for every stat, which is not straightforward.\n. Yes, we are still using heka, it occurs when heka is restarted. We rarely restart heka, but it seems there is a high probability that this recurs whenever we restart heka. I use SIGTERM when restarting heka.\n. 1. Yes, we remove the extra \"}\", then it starts ok. We use heka to parse nginx logs in a directory, when there is a problem, only two of them have an extra \"}\", as to whether they are in correct localtion, because I did not care about that then, so I do not know.\n2. Our heka config is different from when this happened, I will try to recover what the original config file was. There are configuration for our two lua plugins.\n\n[hekad]\nmaxprocs = 5\n[StatsdInput]\naddress = \"$port\"\nmax_msg_size = 4096\n[StatAccumInput]\nticker_interval = 10\npercent_threshold = 95\n[nginx_access_logs]\ntype = \"LogstreamerInput\"\nparser_type = \"token\"\ndecoder = \"nginx_access_decoder\"\nlog_directory = \"$directory\"\nfile_match = '(?P.*)\\.access\\.log'\ndifferentiator = [\"DomainName\"]\n[nginx_access_decoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/nginx_access.lua\"\n[nginx_access_decoder.config]\nlog_format = '$format'\ntype = \"nginx.access\"\n[nginx_error_logs]\ntype = \"LogstreamerInput\"\nparser_type = \"token\"\ndecoder = \"nginx_error_decoder\"\nlog_directory = \"$directory\"\nfile_match = '(?P.*)\\.error\\.log'\ndifferentiator = [\"DomainName\"]\n[nginx_error_decoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/nginx_error.lua\"\n[nginx_error_decoder.config]\ntype = \"nginx.error\"\ntz = \"Asia/Shanghai\"\n[ESLogstashV0Encoder]\nindex = \"webnginx-log-%{2006.01.02}\"\nes_index_from_timestamp = true\ntype_name = \"%{Type}\"\n[ElasticSearchOutput]\nmessage_matcher = \"Type == 'nginx.access' || Type == 'nginx.error'\"\nencoder = \"ESLogstashV0Encoder\"\nserver = \"$server\"\nflush_count = 5000\nhttp_timeout = 5000\n[nginx_access_counter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'nginx.access'\"\nticker_interval = 10\nfilename = \"$filename\"\n[nginx_error_counter]\ntype = \"SandboxFilter\"\nmessage_matcher = \"Type == 'nginx.error'\"\nticker_interval = 10\nfilename = \"/usr/share/heka/lua_extend/nginx_error_counter.lua\"\n[CarbonOutput]\nmessage_matcher = \"Fields[payload_type] == 'stats'\"\naddress = \"$address\"\nprotocol = \"tcp\"\ntcp_keep_alive = true\n\n\n0.8.3, wheezy 7.6, heka is built from source.\n. Sorry. It seems my problem.\n. Sorry, it is result of unclean build, I resolve this by deleting the build directory and build again.\n. I use go 1.5. It panics before process_message is called, near local prefix. It is ok in v0.9.2.\n. \n",
    "simonpasquier": "@arrrrgh Sure I'll try to add a configuration option for this although be aware that I'm brand new to Go and Heka. Where do you want me to describe the behaviour: documentation of HttpListenInput, Git commit message, code comment?\n. I can reproduce the issue with a simpler configuration (eg no piped commands):\n``` toml\n[ProcessInput]\n    stdout = true\n    stderr = false\n    splitter = \"TokenSplitter\"\n    ticker_interval = 3\n[ProcessInput.command.0]\n    bin = \"/bin/df\"\n[PayloadEncoder]\n    append_newlines = true\n[LogOutput]\n    encoder = \"PayloadEncoder\"\n    message_matcher = \"TRUE\"\n```\nOutput with Heka 0.10:\n2015/07/30 09:06:41 Filesystem               1K-blocks    Used Available Use% Mounted on\nAnd it stops there, no other log after ticker_interval has elapsed.\nOutput with Heka 0.9.2:\n```\n2015/07/30 09:13:05 Filesystem               1K-blocks    Used Available Use% Mounted on\n2015/07/30 09:13:05 /dev/mapper/os-root       15350768 4019252  10528720  28% /\n2015/07/30 09:13:05 none                             4       0         4   0% /sys/fs/cgroup\n2015/07/30 09:13:05 udev                       1271676       4   1271672   1% /dev\n2015/07/30 09:13:05 tmpfs                       256608     612    255996   1% /run\n2015/07/30 09:13:05 none                          5120      44      5076   1% /run/lock\n2015/07/30 09:13:05 none                       1283020   60868   1222152   5% /run/shm\n2015/07/30 09:13:05 none                        102400       0    102400   0% /run/user\n2015/07/30 09:13:05 /dev/vda3                   200672   39188    151264  21% /boot\n```\n. When I stop Heka, it spits out goroutine traces like this:\n```\ngoroutine 130 [chan receive, 1 minutes]:\ngithub.com/mozilla-services/heka/plugins/process.func\u00b7008(0xc2080bcf80)\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/process/process_input.go:267 +0x417\ngithub.com/mozilla-services/heka/pipeline.(sRunner).DeliverRecord(0xc20805b4a0, 0xc20a73a020, 0x16, 0x1fe0, 0x7f37ba063f18, 0xc208090ab0)\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/splitter_runner.go:279 +0x2e1\ngithub.com/mozilla-services/heka/pipeline.(sRunner).SplitStream(0xc20805b4a0, 0x7f37ba074ff8, 0xc20bea27e0, 0x7f37ba063f18, 0xc208090ab0, 0x0, 0x0)\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/splitter_runner.go:359 +0x476\ngithub.com/mozilla-services/heka/plugins/process.(ProcessInput).ParseOutput(0xc20800c2a0, 0x7f37ba074ff8, 0xc20bea27e0, 0x7f37ba063f18, 0xc208090ab0, 0x7f37ba063f50, 0xc20805b4a0)\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/process/process_input.go:389 +0x62\ncreated by github.com/mozilla-services/heka/plugins/process.(ProcessInput).runOnce\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/process/process_input.go:362 +0x38e\ngoroutine 133 [chan receive, 1 minutes]:\nos/exec.(Cmd).Wait(0xc20b4163c0, 0x0, 0x0)\n        /usr/local/go/src/os/exec/exec.go:369 +0x30b\ngithub.com/mozilla-services/heka/plugins/process.func\u00b7001()\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/process/process_chain.go:66 +0x32\ncreated by github.com/mozilla-services/heka/plugins/process.(ManagedCmd).Wait\n        /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/process/process_chain.go:67 +0xb6\n```\n. @rafrombrc, it works for me with versions/0.10 (at least with the simple configuration that I provided to reproduce the issue).\n. @looatgb this might be related to https://github.com/mozilla-services/heka/issues/1841. How many files are processed by the AcmeMainLogInput plugin?\n. I'm having similar issues with 400 errors from Elasticsearch then the queue filling up and never resuming. I'll investigate and try to find a way to reproduce it.\n. @hobofan the issue with the output queue getting full has gone away for me with the the latest versions/0.10 branch. AFAICT this it is related to https://github.com/mozilla-services/heka/issues/1627 and fixed by the PR https://github.com/mozilla-services/heka/pull/1682.\nI can't tell yet if the 400 errors still exist though.\n. Could it be related with #1620?\n. AFAICT yes. With 0.10, ProcessInput doesn't work properly with TokenSplitter (and maybe other splitters too). Only the first output of the command may be passed to the decoder and then the Heka pipeline but that's all.\n. I've sent a PR to the cactus project and it's been accepted. Using the '%f' formatter will print the microseconds fraction of the timestamp.\n. @zstyblik while trying to fix #1757 I get the same error message (\"AMQP scheme must be either 'amqp://' or 'amqps://'\"). AFAICT Heka hasn't changed the version of the AMQP library for a while so I suspect a code change in Heka v0.10 that breaks the init phase for the AMQP plugins when the pipeline runner wants to respawn a stopped plugin.\n. I've dug further into this and confirmed that something is broken in the Heka framework wrt to the plugin restarts.\n@rafrombrc I suspect that https://github.com/mozilla-services/heka/pull/1538 is the culprit (or maybe some subsequent commit). Affer this change, the Config() method of pluginMaker always returns the default ConfigStruct and not the ConfigStruct generated from the TOML. I hacked a dirty fix in my local repository [0] and I verified that the configuration is now correctly set when the plugin is respawned. I'd appreciate your feedback on whether my explanation makes sense and what would be the proper way to fix this. Thanks!\n[0] https://github.com/simonpasquier/heka/commit/00143dce3f912b715eff53f31b872f2f042786c6\n. IIUC the Run() method should return something other than nil [1] to trigger a restart of the plugin. Otherwise the pipeline runner triggers a shutdown of Heka.\n[1] https://github.com/mozilla-services/heka/blob/1af899d0b975893bbf19d3bcb8c2e6653a7c9252/plugins/amqp/amqp_input.go#L206\n. This doesn't fix completely the issue because now I see the same problem as reported by the issue #1756.\n2015/10/14 08:57:22 Input 'openstack_info_amqp' error: Channel closed while reading from queue lma_notifications.info\n2015/10/14 08:57:22 Input 'openstack_info_amqp': Restarting (attempt 1/-1)\n2015/10/14 08:57:22 Input 'openstack_info_amqp' error: AMQP scheme must be either 'amqp://' or 'amqps://'\n2015/10/14 08:57:23 Input 'openstack_info_amqp': Restarting (attempt 2/-1)\n2015/10/14 08:57:23 Input 'openstack_info_amqp' error: AMQP scheme must be either 'amqp://' or 'amqps://'\n2015/10/14 08:57:25 Input 'openstack_info_amqp': Restarting (attempt 3/-1)\n2015/10/14 08:57:25 Input 'openstack_info_amqp' error: AMQP scheme must be either 'amqp://' or 'amqps://'\n2015/10/14 08:57:27 Input 'openstack_info_amqp': Restarting (attempt 4/-1)\n2015/10/14 08:57:27 Input 'openstack_info_amqp' error: AMQP scheme must be either 'amqp://' or 'amqps://'\n. You'd need to configure buffering for your HTTP output plugin to avoid this issue. Please have a look at the buffering documentation.\n. I forgot to update the documentation. I'll send an updated PR soon.\n. For the record, it is more appropriate to use the Heka mailing list or the #heka IRC channel.\nAs for testing, I would advise that you use the RstEncoder encoder instead of PayloadEncoder and you should see the messages injected by the ProcStats plugin. Here a sample output that I tested quickly:\n2016/03/08 14:37:43 \n:Timestamp: 2016-03-08 13:37:43 +0000 UTC\n:Type: stats.procstat\n:Hostname: simon-trusty\n:Pid: 0\n:Uuid: 35da0116-fd74-4423-ad43-523de6482380\n:Logger: ProcStats\n:Payload: \n:EnvVersion: \n:Severity: 7\n:Fields:\n    | name:\"intr\" type:double value:[2.470343249e+09,50,195826,0,0,0,0,0,0,0,47,0,0,17616,0,0,193085,105085,112168,0,2.330477e+06,2.364686341e+09,1.073452e+06,146,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n    | name:\"procs_blocked\" type:double value:1\n    | name:\"softirq\" type:double value:[8.0686994e+07,19,3.2565104e+07,88522,4.822638e+06,994119,0,171904,3.0934711e+07,0,1.1109977e+07]\n    | name:\"btime\" type:double value:1.457251235e+09\n    | name:\"procs_running\" type:double value:2\n    | name:\"ctxt\" type:double value:4.70790817e+09\n    | name:\"cpu0\" type:double value:[263231,4612,3.889563e+06,8.385486e+06,6612,0,11452,0,0,0]\n    | name:\"cpu\" type:double value:[920220,14333,4.295187e+06,2.613972e+07,27892,0,32557,0,0,0]\n    | name:\"cpu1\" type:double value:[656988,9721,405623,1.7754234e+07,21279,0,21104,0,0,0]\n    | name:\"processes\" type:double value:211224\n. If you increase the output_limit parameter [1] to some insane value (eg 1MiB), you'll get the encoded message. IIUC the InfluxDB encoder doesn't work well with fields holding array values.\n<snip>\nintr_vidx_1_vidx_2_vidx_3_vidx_4_vidx_5_vidx_6_vidx_7_vidx_8_vidx_9_vidx_10_vidx_11_vidx_12_vidx_13_vidx_14_vidx_15_vidx_16_vidx_17_vidx_18_vidx_19_vidx_20_vidx_21_vidx_22_vidx_23_vidx_24_vidx_25_vidx_26_vidx_27_vidx_28_vidx_29_vid\nx_30_vidx_31_vidx_32_vidx_33_vidx_34_vidx_35_vidx_36_vidx_37_vidx_38_vidx_39_vidx_40_vidx_41_vidx_42_vidx_43_vidx_44_vidx_45_vidx_46_vidx_47_vidx_48_vidx_49_vidx_50_vidx_51_vidx_52_vidx_53_vidx_54_vidx_55_vidx_56_vidx_57_vidx_58_vi\ndx_59_vidx_60_vidx_61_vidx_62_vidx_63_vidx_64_vidx_65_vidx_66_vidx_67_vidx_68_vidx_69_vidx_70_vidx_71_vidx_72_vidx_73_vidx_74_vidx_75_vidx_76_vidx_77_vidx_78_vidx_79_vidx_80_vidx_81_vidx_82_vidx_83_vidx_84_vidx_85_vidx_86_vidx_87_v\nidx_88_vidx_89_vidx_90_vidx_91_vidx_92_vidx_93_vidx_94_vidx_95_vidx_96_vidx_97_vidx_98_vidx_99_vidx_100_vidx_101_vidx_102_vidx_103_vidx_104_vidx_105_vidx_106_vidx_107_vidx_108_vidx_109_vidx_110_vidx_111_vidx_112_vidx_113_vidx_114_v\nidx_115_vidx_116_vidx_117_vidx_118_vidx_119_vidx_120_vidx_121_vidx_122_vidx_123_vidx_124_vidx_125_vidx_126_vidx_127_vidx_128_vidx_129_vidx_130_vidx_131_vidx_132,Hostname=simon-trusty value=0.000000 1457450505\nintr_vidx_1_vidx_2_vidx_3_vidx_4_vidx_5_vidx_6_vidx_7_vidx_8_vidx_9_vidx_10_vidx_11_vidx_12_vidx_13_vidx_14_vidx_15_vidx_16_vidx_17_vidx_18_vidx_19_vidx_20_vidx_21_vidx_22_vidx_23_vidx_24_vidx_25_vidx_26_vidx_27_vidx_28_vidx_29_vid\nx_30_vidx_31_vidx_32_vidx_33_vidx_34_vidx_35_vidx_36_vidx_37_vidx_38_vidx_39_vidx_40_vidx_41_vidx_42_vidx_43_vidx_44_vidx_45_vidx_46_vidx_47_vidx_48_vidx_49_vidx_50_vidx_51_vidx_52_vidx_53_vidx_54_vidx_55_vidx_56_vidx_57_vidx_58_vi\ndx_59_vidx_60_vidx_61_vidx_62_vidx_63_vidx_64_vidx_65_vidx_66_vidx_67_vidx_68_vidx_69_vidx_70_vidx_71_vidx_72_vidx_73_vidx_74_vidx_75_vidx_76_vidx_77_vidx_78_vidx_79_vidx_80_vidx_81_vidx_82_vidx_83_vidx_84_vidx_85_vidx_86_vidx_87_v\nidx_88_vidx_89_vidx_90_vidx_91_vidx_92_vidx_93_vidx_94_vidx_95_vidx_96_vidx_97_vidx_98_vidx_99_vidx_100_vidx_101_vidx_102_vidx_103_vidx_104_vidx_105_vidx_106_vidx_107_vidx_108_vidx_109_vidx_110_vidx_111_vidx_112_vidx_113_vidx_114_v\nidx_115_vidx_116_vidx_117_vidx_118_vidx_119_vidx_120_vidx_121_vidx_122_vidx_123_vidx_124_vidx_125_vidx_126_vidx_127_vidx_128_vidx_129_vidx_130_vidx_131_vidx_132_vidx_133_vidx_134_vidx_135_vidx_136_vidx_137_vidx_138_vidx_139_vidx_14\n0_vidx_141_vidx_142_vidx_143_vidx_144_vidx_145_vidx_146_vidx_147_vidx_148_vidx_149_vidx_150,Hostname=simon-trusty value=0.000000 1457450505\nintr_vidx_1_vidx_2_vidx_3_vidx_4_vidx_5_vidx_6_vidx_7_vidx_8_vidx_9_vidx_10_vidx_11_vidx_12_vidx_13_vidx_14_vidx_15_vidx_16_vidx_17_vidx_18_vidx_19_vidx_20_vidx_21_vidx_22_vidx_23_vidx_24_vidx_25_vidx_26_vidx_27_vidx_28_vidx_29_vid\nx_30_vidx_31_vidx_32_vidx_33_vidx_34_vidx_35_vidx_36_vidx_37_vidx_38_vidx_39_vidx_40_vidx_41_vidx_42_vidx_43_vidx_44_vidx_45_vidx_46_vidx_47_vidx_48_vidx_49_vidx_50_vidx_51_vidx_52_vidx_53_vidx_54_vidx_55_vidx_56_vidx_57_vidx_58_vi\ndx_59_vidx_60_vidx_61_vidx_62_vidx_63_vidx_64_vidx_65_vidx_66_vidx_67_vidx_68_vidx_69_vidx_70_vidx_71_vidx_72_vidx_73_vidx_74_vidx_75_vidx_76_vidx_77_vidx_78_vidx_79_vidx_80_vidx_81_vidx_82_vidx_83_vidx_84_vidx_85_vidx_86_vidx_87_v\nidx_88_vidx_89_vidx_90_vidx_91_vidx_92_vidx_93_vidx_94_vidx_95_vidx_96_vidx_97_vidx_98_vidx_99_vidx_100_vidx_101_vidx_102_vidx_103_vidx_104_vidx_105_vidx_106_vidx_107_vidx_108_vidx_109_vidx_110_vidx_111_vidx_112_vidx_113_vidx_114_v\nidx_115_vidx_116_vidx_117_vidx_118_vidx_119_vidx_120_vidx_121_vidx_122_vidx_123_vidx_124_vidx_125_vidx_126_vidx_127_vidx_128_vidx_129_vidx_130_vidx_131_vidx_132_vidx_133_vidx_134_vidx_135_vidx_136_vidx_137_vidx_138_vidx_139_vidx_14\n0_vidx_141_vidx_142_vidx_143_vidx_144_vidx_145_vidx_146_vidx_147_vidx_148_vidx_149_vidx_150_vidx_151_vidx_152_vidx_153_vidx_154_vidx_155_vidx_156_vidx_157_vidx_158_vidx_159_vidx_160_vidx_161_vidx_162_vidx_163_vidx_164_vidx_165_vidx\n_166_vidx_167_vidx_168_vidx_169_vidx_170_vidx_171_vidx_172_vidx_173_vidx_174_vidx_175_vidx_176_vidx_177_vidx_178_vidx_179_vidx_180_vidx_181_vidx_182_vidx_183_vidx_184_vidx_185_vidx_186_vidx_187_vidx_188_vidx_189_vidx_190_vidx_191_v\nidx_192_vidx_193_vidx_194_vidx_195_vidx_196_vidx_197_vidx_198_vidx_199_vidx_200_vidx_201_vidx_202_vidx_203_vidx_204_vidx_205_vidx_206_vidx_207_vidx_208_vidx_209_vidx_210_vidx_211_vidx_212_vidx_213_vidx_214_vidx_215_vidx_216_vidx_21\n7_vidx_218_vidx_219_vidx_220_vidx_221_vidx_222_vidx_223_vidx_224_vidx_225_vidx_226_vidx_227_vidx_228_vidx_229_vidx_230_vidx_231_vidx_232_vidx_233_vidx_234_vidx_235_vidx_236_vidx_237_vidx_238_vidx_239_vidx_240_vidx_241_vidx_242_vidx\n_243_vidx_244_vidx_245_vidx_246_vidx_247_vidx_248_vidx_249_vidx_250_vidx_251_vidx_252_vidx_253_vidx_254_vidx_255_vidx_256_vidx_257_vidx_258_vidx_259_vidx_260_vidx_261_vidx_262_vidx_263_vidx_264_vidx_265_vidx_266_vidx_267_vidx_268_v\nidx_269_vidx_270_vidx_271_vidx_272_vidx_273_vidx_274_vidx_275_vidx_276_vidx_277_vidx_278_vidx_279_vidx_280_vidx_281_vidx_282_vidx_283_vidx_284_vidx_285_vidx_286_vidx_287_vidx_288_vidx_289_vidx_290_vidx_291_vidx_292_vidx_293_vidx_29\n4_vidx_295_vidx_296_vidx_297_vidx_298_vidx_299_vidx_300_vidx_301_vidx_302_vidx_303_vidx_304_vidx_305_vidx_306_vidx_307_vidx_308_vidx_309_vidx_310_vidx_311_vidx_312_vidx_313_vidx_314_vidx_315_vidx_316_vidx_317_vidx_318_vidx_319_vidx\n_320_vidx_321_vidx_322_vidx_323_vidx_324_vidx_325_vidx_326_vidx_327_vidx_328_vidx_329_vidx_330_vidx_331_vidx_332_vidx_333_vidx_334_vidx_335_vidx_336_vidx_337_vidx_338_vidx_339_vidx_340_vidx_341_vidx_342_vidx_343_vidx_344_vidx_345_v\nidx_346,Hostname=simon-trusty value=0.000000 1457450505\n<snip>\n[1] http://hekad.readthedocs.org/en/v0.10.0/config/common_sandbox_parameter.html\n. IMO the issue is valid but should be renamed, something like \"InfluxDB line encoder doesn't work with CPU stats because it contains array fields\".\n. This might be related to https://github.com/mozilla-services/heka/issues/1714. Which versions of Heka and Kafka are you using? And how many partitions are there for the \"heka-logs\" topic?\n. For some reason, hekad can't send data to ES (probably a network issue) so I would recommend to look into the ES logs too or use tcpdump on tcp port 9200. Since you configure the 'block' policy for the buffering, the Heka pipeline gets stuck when the output plugin has filled the local buffer. Hence the idle packs errors.\nNext time, please use the mailing list (heka@mozilla.org) for such request.\n. We use a similar configuration with buffering in our environment and everything works fine for us. What's in your checkpoint.txt file for the Elasticsearch output cache directory?\n. If you're just asking questions, please don't use GitHub issues but the Heka mailing list.\nIf you catch exceptions in your decoder, you can inject the exception message in the Heka pipeline and send it wherever you want:\nlocal ok, msg = pcall(inject_message, ...)\nif not ok then\n  inject_payload(\"txt\", \"error\", msg)\nend\n. Kirill, can you close this issue and ask the question on the mailing list please?\n. I'll do more tests but it didn't seem to be an issue during my initial testing.\n. So I tested again. Indeed the Stop() method will close the AMQP channel and the Run() method will return an error but the plugin runner detects that Heka is shutting down and it doesn't try to respawn the plugin [0].\n[0] https://github.com/mozilla-services/heka/blob/versions/0.10/pipeline/plugin_runners.go#L328\n. ",
    "khenderick": "After compiling from git, I can confirm that the output_limit allows me to get rid of the second issue, and thus allows me to transfer larger amounts of data to influxdb.\nHowever, since the ticket got closed, can I assume there won't be any official support for transferring a large amount of accumulated stats to influxdb? Since the shipped lua_encoders/statmetric_influx.lua encoder will still fail because of cjson's max_size.\n. I'll create a pull-request with my changes to work around the cjson encode limitation.\n. Indeed, I somehow must have been confused during testing (probably only changed the output_limit after I changed the encoder's code). The output_limit with the default encoder does work. Sorry for the superfluous pull request.\n. ",
    "jestan": "I can reproduce this issue (in heka 0.10.b1) with rolling empty files with same name.\nI am just wondering why  @validname 's patch is not merged.\n. This bug is a show stopper for me. @validname fix works properly.\nFor time being, I have to use a forked version of heka https://github.com/jestan/heka/commit/c9366b1dd7b18d865dbdb5479445972d251a68d5\n. @rafrombrc Thanks for fix the issue.\n. ",
    "thomasalrin": "Hi, When I'm using heka(0.8.1) in truty(ubuntu14.04), output file updated with full content of input file everytime.  When i use the same toml file in centos7, output file updated only if the input file has a single line. I'm using heka 0.8.1. My hekad.toml file is\n[LogIn]\ntype = \"LogstreamerInput\"\nlog_directory = \"/root/directory1/\"\nfile_match = 'input_file'\n[counter_file]\ntype = \"FileOutput\"\nmessage_matcher = \"TRUE\"\npath = \"/root/directory1/output_file\"\nperm = \"666\"\nflush_count = 100\nflush_operator = \"OR\"\nencoder = \"PayloadEncoder\"\n[PayloadEncoder]\n. trusty:\n        output file updated with full content of input file everytime.\neg : input_file\n                line1\n      START HEKAD\n           output file\n                line1\n           Edit : input_file\n                line1\n                line2\n          output_file\n                line1\n                line1  --> Repeated the full content of input file\n                line2\ncentos:\n        output file updated only if the input file has single line.\neg : input_file\n                line1\n                line2\n      START HEKAD\n        output_file\n                        --> Empty(no content)\nEdit : input_file\n            line1\n    output_file\n            line1\n. ",
    "blalor": "Running into this, too.  There really should be first-class client support.\n. Cool!  I\u2019ll check that out.\n. I never tried it.  My Heka project never got off the ground.\n. ",
    "ildus": "@blalor By now, you can use this library: https://github.com/ildus/golog/tree/master/heka_emitter\n. @trixpan I'm not sure that is still related, but now I get this error:\n\u279c  ~  go get github.com/mozilla-services/heka/message\npackage code.google.com/p/gogoprotobuf/proto: unable to detect version control system for code.google.com/ path\n\u279c  ~  go get github.com/mozilla-services/heka/client \npackage code.google.com/p/gogoprotobuf/proto: unable to detect version control system for code.google.com/ path\n. ",
    "starchou": "I have same problem when I use command go test -run=TestInit in the path = heka/sandbox/lua\u3002\n. @rafrombrc thanks! it works now ! I hack lua_sandbox.go set the cgo CFLAGS and cgo LDFLAGS\n. ",
    "zhaakhi": "The main problem as I see it is that it's not clear what the \"default\" way to push messages to Heka should be, for clients and for Heka to Heka topologies.\nTcpInput seems to be the intended way, since it defaults to ProtobufDecoder and is what test tools like heka-inject and heka-flood use, but I think most people expect a stream processing system to provide reliable delivery by default.\nFor my use TcpInput/TcpOutput works acceptably, though the caveats should be documented.\nMaybe a good start would be to document that if you need reliable at-least-once delivery you should currently use something like AMQP or Kafka. HttpInput/HttpOutput could also be an alternative, but HttpOutput doesn't currently batch messages according to the doc.\n. This could be related to #1213\n. ",
    "dcarley": "Thanks for the comments. I'm definitely interested in hearing from real users and people more familiar with this code than I am.\nOne thing I'm not clear about is the meaning of the comment \"read as many as we can\" on L353-354. I thought it might refer to filling a read buffer as much as it can and then taking subsequent passes until EOF.\nBut from testing it seems to consume the log file (even a very large one) in a single pass up to EOF, in which case we'll always get a save and there's no increased risk of duplicated messages by removing this.\n. Thanks, Rob. Makes perfect sense. I've rebased accordingly.\nCan this be included in a 0.8.1 release?\n. Superseded by #1219.\n. @rafrombrc Did you have a release date in mind for 0.8.1? Could it be soon?\n. Is this a bug? Is there anything I can do to help debug it?\n. No problem about the delay.\nThat makes sense. I'll try it out and re-open the issue if it's still a problem. Thanks for creating the issue to change the default.\n. ",
    "SamPenrose": "Yeah, I should have mentioned that the solution was data dependent. I can see a couple paths forward:\n1) Choose a range of existing configurations (such as the mozaws.net instance above, which BTW I don't have access to), and play with layouts that seem to work acceptably for all of them, knowing that significantly different configs will result in probably ugly results.\n2) Do a more-modest cleanup of the existing page, accepting some of the structural issues (especially vertical scrolling and lack of visual continuity between in/ouputs and de/encoders.\n3) Pursue a less data-scale-dependent approach. I've proposed exploring Sankey Diagrams around l.144 of https://etherpad.mozilla.org/heka-dashboard-proposals ; there may be others.\n. ",
    "hmrm": "Can't reproduce this, appears to have been a temporary issue\n. ",
    "nlamirault": "I try heka with my Dockerfile [1]. I install heka using the deb archive.\n[1] : https://github.com/nlamirault/vision/blob/develop/hekad/Dockerfile\n. any idea ?\n. Same behavior for me. (#1231)\n. ",
    "vino-tha": "Hi ioc32,\nCurrently i am using heka-0.8.3 with Rabbitmq-3.5.6-1, Even after setting the persistent=true in Heka, Rabbitmq does not persist the messages, When i tried with python code to send persistent messages, rabbitmq is able to persist them.\nCould you please confirm if this issue is fixed in Heka-0.8.3\nThanks,\nVino\n. ",
    "gcarothers": "Also there is no hekad.toml in the package.\n. ",
    "wyndhblb": "no worries, it's little custom .lua to make cbuf for loglevels counts ... \n```\n_PRESERVATION_VERSION = read_config(\"preservation_version\") or 0\n_PRESERVATION_VERSION = _PRESERVATION_VERSION + 1 -- force a revision update due to internal changes\nrequire \"circular_buffer\"\nrequire \"string\"\nlocal alert         = require \"alert\"\nlocal annotation    = require \"annotation\"\nlocal anomaly       = require \"anomaly\"\nlocal title             = read_config(\"title\") or \"LogLevel\"\nlocal rows              = read_config(\"rows\") or 1440\nlocal sec_per_row       = read_config(\"sec_per_row\") or 60\nlocal use_field         = read_config(\"use_field\") or \"Severity\"\nlocal anomaly_config    = anomaly.parse_config(read_config(\"anomaly_config\"))\nannotation.set_prune(title, rows * sec_per_row * 1e9)\nstatus = circular_buffer.new(rows, 6, sec_per_row)\nstatus:set_header(1, \"DEBUG\")\nstatus:set_header(2, \"INFO\")\nstatus:set_header(3, \"WARN\")\nstatus:set_header(4, \"ERROR\")\nstatus:set_header(5, \"CRITICAL\")\nlocal LOG_LEVEL_UNKNOWN = status:set_header(6, \"UNKNOWN\")\nfunction process_message ()\n    local ts = read_message(\"Timestamp\")\n    local col = read_message(use_field)\nif col == \"DEBUG\" or tonumber(col) == 7 then\n    status:add(ts, 1, 1)\nelseif col == \"INFO\" or tonumber(col) == 6  then\n    status:add(ts, 2, 1)\nelseif col == \"WARN\" or tonumber(col) == 4  then\n    status:add(ts, 3, 1)\nelseif col == \"WARNING\" or tonumber(col) == 4  then\n    status:add(ts, 3, 1)\nelseif col == \"ERROR\" or tonumber(col) == 3  then\n    status:add(ts, 4, 1)\nelseif col == \"FATAL\" or tonumber(col) == 2  then\n    status:add(ts, 5, 1)\nelseif col == \"CRITICAL\" or tonumber(col) == 2  then\n    status:add(ts, 5, 1)\nelse\n    status:add(ts, LOG_LEVEL_UNKNOWN, 1)\nend\n\nreturn 0\n\nend\nfunction timer_event(ns)\n    if anomaly_config then\n        if not alert.throttled(ns) then\n            local msg, annos = anomaly.detect(ns, title, status, anomaly_config)\n            if msg then\n                annotation.concat(title, annos)\n                alert.send(ns, msg)\n            end\n        end\n        inject_payload(\"cbuf\", title, annotation.prune(title, ns), status)\n    else\n        inject_payload(\"cbuf\", title, status)\n    end\nend\n```\nThe toml is this little snipit\n```\n[log-level-info-1sec]\ntype = \"SandboxFilter\"\nfilename = \"lua_filters/log_level_info.lua\"\nticker_interval = 1\npreserve_data = true\nmessage_matcher = \"Type == 'mylogger-service'\"\n[log-level-info-1sec.config]\ntitle = \"Loglevel\"\nuse_field = \"Severity\"\nsec_per_row = 1\nrows = 1440\npreservation_version = 1\n\n```\n. After a little more digging and playing with configs a bit more, i've tracked it down to the part of the config that seems to cause the cbufs to get 'purged/nulled\" out ... And i \"guess\" that it might be because one of the CBUFs is empty.  see below\n```\n[logger-syslog-decoder]\ntype = \"PayloadRegexDecoder\"\nmatch_regex = '^(<\\d+>)?(?P[A-Z][a-z]+\\s+\\d+\\s\\d+:\\d+:\\d+) (?P\\S+) (?P\\S+): (?P.*)'\ntimestamp_layout = 'Oct 23 00:34:49'\ntimestamp_location = 'UTC'\n[logger-syslog-decoder.severity_map]\nDEBUG = 7\nINFO = 6\nWARNING = 4\nWARN = 4\nERROR = 3\nCRITICAL = 2\nFATAL = 2\n\n[logger-syslog-decoder.message_fields]\nType = \"mylogger-service\"\nHostname = \"%Hostname%\"\nLogger = \"%Logger%\"\nlog_message = \"%log_message%\"\n\n[logger-stats-acum]\ntype = \"StatAccumInput\"\nticker_interval = 10\ndelete_idle_stats = true\nmessage_type = \"mylogger-service.stats\"\n[logger-stats]\ntype = \"StatFilter\"\nstat_accum_name = \"mylogger-stats-acum\"\nmessage_matcher = \"Type == 'mylogger-service'\"\n[logger-stats.Metric.messagecount]\ntype = \"Counter\"\nname = \"loggerstat.%Hostname%\"\nvalue = \"1\"\n\n[logger-stats.Metric.all-count]\ntype = \"Counter\"\nname = \"loggerstat.all\"\nvalue = \"1\"\n\n[logger-stats-graph10]\ntype = \"SandboxFilter\"\nfilename = \"lua_filters/stat_graph.lua\"\nticker_interval = 10\npreserve_data = true\nmessage_matcher = \"Type == 'mylogger-service.stats'\"\n[logger-stats-graph10.config]\n  title = \"Total Messages 10sec\"\n  rows = 1440\n  stat_aggregation = \"none\"\n  stat_unit = \"count\"\n  sec_per_row = 10\n  stats = \"stats.counters.loggerstat.all.count\"\n  stat_labels = \"Messages\"\n```\nIt turns out that the logger-stats-graph10 is empty (for reasons i'm still trying to figure out, the stat does get emitted to the graphite instance).  however It seems that when the 1440 row count is \"up\" any other CBUFs get \"wiped / nulled out\" from dashboard/data/ directory ONLY if i include the logger-stats-graph10 cbuf in the config.\nThis is running the current 0.8.1 version of things\n. And there's more ... i noticed today (this may be a different bug, but related to preserve failing) when for some reason my AWS's elastic search nodes did not respond. The elasticsearch cluster is/was just fine, but it being AWS god knows what happens to networks sometimes, but the ES did get back to responding within a few seconds. Heka never recovered...  just repeated this over and over for hours, until i noticed it borked .. \n```\n2015/01/04 00:00:54 Plugin 'logger-es-output' error: HTTP request failed: Post http://elastic-host.mysub.com:9200/_bulk: EOF\n2015/01/04 00:01:45 Plugin 'logger-es-output' error: HTTP response error status: 504 Gateway Time-out\n2015/01/04 00:02:35 Plugin 'logger-es-output' error: HTTP response error status: 504 Gateway Time-out\n2015/01/04 00:03:24 Diagnostics: 2 packs have been idle more than 120 seconds.\n2015/01/04 00:03:24 Diagnostics: (inject) Plugin names and quantities found on idle packs:\n2015/01/04 00:03:24 Diagnostics:        dashboard: 2\n2015/01/04 00:03:24\n2015/01/04 00:03:24 Diagnostics: 98 packs have been idle more than 120 seconds.\n2015/01/04 00:03:24 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/01/04 00:03:24 Diagnostics:        log-level-info-60sec: 98\n015/01/04 00:05:24 Diagnostics: 98 packs have been idle more than 120 seconds.\n2015/01/04 00:05:24 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/01/04 00:05:24 Diagnostics:        log-level-info-600sec: 98\n2015/01/04 00:10:54 Diagnostics: 98 packs have been idle more than 120 seconds.\n2015/01/04 00:10:54 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/01/04 00:10:54 Diagnostics:        log-level-info-60sec: 98\n....\n015/01/04 02:50:24 Diagnostics: 98 packs have been idle more than 120 seconds.\n2015/01/04 02:50:24 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/01/04 02:50:24 Diagnostics:        log-level-info-60sec: 98\n2015/01/04 02:50:24\n2015/01/04 02:50:53 Shutdown initiated.\n2015/01/04 02:50:53 Stop message sent to input 'logger-tcp'\n2015/01/04 02:50:53 Stop message sent to input 'logger-udp'\n2015/01/04 02:50:53 Stop message sent to input 'logger-stats-acum'\n2015/01/04 02:50:53 Input 'udp': stopped\npanic: runtime error: send on closed channel\ngoroutine 37 [running]:\nruntime.panic(0x9c21e0, 0xf5579e)\n        /usr/local/go/src/pkg/runtime/panic.c:279 +0xf5\ngithub.com/mozilla-services/heka/pipeline.(StatAccumInput).DropStat(0xc2080058c0, 0xc20a9b2660, 0x2f, 0xc20a1df488, 0x1, 0xa38530, 0x0, 0x3f800000, 0x0)\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/pipeline/stat_accum_input.go:188 +0x93\ngithub.com/mozilla-services/heka/plugins/statsd.(StatFilter).Run(0xc208132840, 0x7f629a7e9350, 0xc208059e00, 0x7f629a7e9df0, 0xc20803e140, 0x0, 0x0)\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/plugins/statsd/stat_filter.go:121 +0x7d3\ngithub.com/mozilla-services/heka/pipeline.(foRunner).getWrapperAndRun(0xc208059e00, 0xc20812a160, 0xc20812eab0, 0x0, 0x0)\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:523 +0x14e\ngithub.com/mozilla-services/heka/pipeline.(foRunner).Starter(0xc208059e00, 0x7f629a7e9df0, 0xc20803e140, 0xc20803e1c0)\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:636 +0x1d9\ncreated by github.com/mozilla-services/heka/pipeline.(*foRunner).Start\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:496 +0x96\ngoroutine 16 [semacquire]:\nsync.runtime_Semacquire(0xc216611310)\n        /usr/local/go/src/pkg/runtime/sema.goc:199 +0x30\nsync.(*WaitGroup).Wait(0xc20803e240)\n        /usr/local/go/src/pkg/sync/waitgroup.go:129 +0x14b\ngithub.com/mozilla-services/heka/pipeline.Run(0xc20803e140)\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/pipeline/pipeline_runner.go:287 +0x190e\nmain.main()\n        /opt/heka/src/build/heka/src/github.com/mozilla-services/heka/cmd/hekad/main.go:188 +0xf64\n...LOT MORE of that kind of stuff....\n```\nthen i restarted the hekad process ... and my preserved data was \"gone\" .. \n. not a real \"go\" expert, but doing \"length\" in a few languages can be a performance issue.  if there is not penalty in Go, no, there is no need to keep a separate count.\n. okey-dokey\n. ",
    "hauleth": ":+1: \nI have noticed it when I was trying to build Docker image based on BusyBox. It will be nice to have it as optional dep or linked statically.\n. @rafrombrc I suggest using Docker for build testing ;)\n. ",
    "aaronfeng": "This broken build is not related to my commit (pretty sure).  Removed my commit and same test still fails.\n. @rafrombrc For sure.  I will add it soon.\n. Doing a proper PR on 0.9 branch.\n. @rafrombrc thanks.  fixed.\n. ",
    "liukun": "@rafrombrc I'd like to change, and is checking Python implementations. Seems not too trivial though, for example, in Python:\nfrom email.header import Header\nprint Header(\"XXXXXXX - 5 personnes vous ont nomm\u00e9 guide 1234235\u54c8\u54c8\", 'utf-8', 76, 'Subject')\ngives\n=?utf-8?q?XXXXXXX_-_5_personnes_vous_ont_nomm=C3=A9_guide_123423?=\n =?utf-8?b?NeWTiOWTiA==?=\nwhich uses the shortest encoding and limits line length. I'll try my best to cover these when I have time.\nBTW, there seems no built-in quoted-printable encoder in Go? Is it OK to import a new package from GitHub?\n. Err, do I need create new pull request to get rid of merge conflicts?\n. @4r9h thank you! will try that tomorrow.\n. Ready to merge.\n. Seems submodule URL been changed. You can reset it by:\ncd docs/source/_themes/mozilla\ngit remote remove origin\ngit remote add origin https://github.com/rafrombrc/mozilla-sphinx-theme.git\ngit fetch\ngit reset --hard origin/master\ncd ../../../../\ngit submodule update --init\n. Thanks! Met the same \"XXX redeclared\" error today and just found the solution here.\n. ",
    "highlyunavailable": "Solid comments, thanks.\nYeah, you're right about the goroutine for flushing not being needed for buffered operations. One other thing I thought of - if I drop the committer goroutine to be a dumb memory buffered sender and only use it when not buffering to disk, then move the \"processed message count\" increment operation to the Run routine, I don't need to worry about making count atomic because it won't cross goroutine boundaries then.\n. @rafrombrc Have you gotten a chance to review this? I screwed up the merge a little trying to get rid of the conflicts but I've cleaned up the channel stuff a bit so it only tries to use an outputter channel if we're not in buffered mode.\n. No problem. Fair enough, I'm not fussed about any changes you made. Thanks a bunch!\n. I'm only ever writing to count from the Run goroutine, despite the fact that committer reads the pointed-to int. Does it still need to be atomic? I've got no problem adding that, I just didn't think it needed to be atomic with 1 writer and multiple readers.\n. I guess so, according to https://golang.org/ref/mem. Good to know.\n. ",
    "kitcambridge": "I'm not sure, but I think the connection isn't reused because o.request() doesn't consume the response body for 200-class responses. Could you try modifying o.request like so, please?\ngo\n// Line 140, plugins/http/http_output.go.\nif resp, err = o.client.Do(req); err != nil {\n    // ...\n}\ndefer resp.Body.Close()\ndefer io.Copy(ioutil.Discard, resp.Body) // Always consume the response body.\nif resp.StatusCode >= 400 {\n    // ...\n}\nreturn\n. ",
    "cxwshawn": "Hi, kitcambridge, thanks for you answer, but I have tried this way, it didn't work .\n. all right~, got the solution, reference https://github.com/golang/go/issues/6785\n. ",
    "hobofan": "I just noticed that trying to delete a field that doesn't exist will cause a panic. Will fix that tomorrow.\n. The panic is fixed and now responds with an appropriate error instead.\n. I also see some 400s in my log files:\n2015/08/24 07:12:42 Plugin 'ElasticSearchOutput' error: HTTP response error status: 400 Bad Request\n2015/08/24 08:11:18 Plugin 'ElasticSearchOutput' error: HTTP response error status: 400 Bad Request\nAfter a few of those the output seems to go into some kind of failure state and won't deliver any more messages, causing my output_queue to fill up.\nI am running 0.10.0b0\n. fixed\n. ",
    "acesaro": "Thanks for the notes!  I'll take these back and update accordingly to then update the PR.\n. I've already started working on this and was going to start another issue about building out support for the InfluxDB 0.9.0 API changes.  @rafrombrc, I've started a new issue for tracking the InfluxDB 0.9.0 API update support that I will link to this one.  All of the new functionality for InfluxDB 0.9.0+ (including support for tags) I have posted in: https://github.com/mozilla-services/heka/issues/1387\n. Thanks for the feedback!  I'm going to start on a new encoder called \"Schema InfluxDB Write Encoder,\" named for the new \"write\" API that InfluxDB 0.9.0 exposes.  All of your input makes sense and I will run with it for implementation.\nIn your spirit of keeping the configuration and functionality of the encoders simple, I'm also going to make multi_series the default behavior, since the \"batching\" of data sent to InfluxDB is highly encouraged to improve the performance of the ingestion.  Instead of the Heka fields being sent as additional attributes like they were in \"Schema InfluxDB Encoder\" without multi_series = true, they will just become tags of all points in this new encoder.\nIn addition, the config option series will be renamed to name_prefix for more logical mapping to the name attribute of the JSON object we are generating and sending to the API.  This makes sense to me as a \"prefix\" because it is pre-pended to each field that we are sending as a data point to InfluxDB.  The single field value will remain as the only field sent for each point in the array within the JSON object sent to the API.\nWe should also consider functionality in the future to spool and batch more metrics together which would result in a smaller amount of larger calls to the InfluxDB write API (which is documented to improve performance).  I think that is out of scope for this initial work, though.\n. I've submitted the new Schema InfluxDB Write Encoder, which implements the initial functionality to send metrics to an InfluxDB 0.9.0+ instance via the write API.  I have tested this successfully with the Heka configuration embedded in the Lua script, sending the load average values polled from /proc/loadavg into separate series in InfluxDB.  This was tested against a local instance of InfluxDB 0.9.0, git commit 11a908f.\nHere's a sample API call from my testing:\njson\n{\n    \"database\": \"mydb\",\n    \"points\": [\n        {\n            \"fields\": {\n                \"value\": 0.01\n            },\n            \"name\": \"my_host.stats.loadavg.5MinAvg\"\n        },\n        {\n            \"fields\": {\n                \"value\": 0.05\n            },\n            \"name\": \"my_host.stats.loadavg.15MinAvg\"\n        },\n        {\n            \"fields\": {\n                \"value\": 0\n            },\n            \"name\": \"my_host.stats.loadavg.1MinAvg\"\n        }\n    ],\n    \"precision\": \"s\",\n    \"retentionPolicy\": \"\",\n    \"tags\": {\n        \"Environment\": \"DEV\",\n        \"Hostname\": \"my_host\"\n    },\n    \"timestamp\": 1426449970\n}\nLet me know your thoughts and how we go about testing this further.  If this is the only functionality that we want to add in this issue, I can open separate issues to update StatMetric InfluxDB Encoder and add InfluxDB Query Decoder.\n. @inthecloud247, are you referring to some functionality that I mentioned above: \"We should also consider functionality in the future to spool and batch more metrics together which would result in a smaller amount of larger calls to the InfluxDB write API (which is documented to improve performance). I think that is out of scope for this initial work, though.\"?  I think it's outside the scope of the encoder and better suited as a filter, but maybe @rafrombrc can chime in here about that.\n. I've submitted a new commit that I believe clears up all the comments that you made.\nOne item that I would like some help with, as I could not figure it out quickly, is support for large integers to not be converted to scientific notation.  Currently, the \"u\" and \"n\" timestamp_precision values do not work properly when sent to InfluxDB as they are converted to scientific notation when the JSON encoding is done.  Does anyone have any suggestions on how to do this in Lua?  From my research it might take the inclusion of a non-packaged module to achieve.  If not, we can resort to excluding those values from the support of the plugin.  Thanks!\n. Updates have been made per the comments in your code review and above.  Thanks!\n. Crap, not sure what I did here but I somehow included other commits that I was just trying to fetch and merge from upstream into my branch.\n. Ok, it looks like I got it fixed.  Let me know if you see any more issues.  Thanks!\n. PR submitted for this updated functionality: https://github.com/mozilla-services/heka/pull/1574\n. I've made some comments in that PR and closed it.  A better approach is going to be taken that encouraged code reuse, so I'll comment here again when that is available in a PR for testing and eventually merged into the project.\n. I finally got around to an initial implementation for this in PR https://github.com/mozilla-services/heka/pull/1595.  This also includes a new Schema Carbon Line Encoder which uses very similar functionality and allows us to integrate with Graphite in a similar manner.  Feedback on that PR is welcome.\n. I found the issue, super small typo, which was part of the cleanup work in ce0f07498dbe30676c5260e196ef9f450c5b1774.  I've submitted https://github.com/mozilla-services/heka/pull/1622 with a tested fix (using the exact config example in the ts_line_procotol.lua comments section).\n. While testing, I've also found that only base_fields are included in tags, no matter what is defined in tag_fields, which I believe I remember explicitly testing successfully.  I'm looking into this now to figure it out.\n. Fix for the missing tag fields that I just mentioned added to https://github.com/mozilla-services/heka/pull/1622.  That bug was actually part of my original PR, so the testing I mentioned must have occurred before converting from read_next_field() to decode_message(read_message(\"raw\")).\n. I started thinking about this a bit more and this encoder is very similar to functionality that I've been wanting to build for Graphite formatted metrics to send to Carbon.  I've actually already implemented it by adding a boolean config variable carbon_line_format and some if statements to just format the same data differently (and skip tag generation steps that aren't relevant).\nWhat are some thoughts about shifting the focus of this encoder to a common InfluxDB/Carbon line protocol encoder rather than copying most of the script and trimming it down for Carbon only?  Perhaps move some of the common functionality int a lua module instead and then have the two encoders reference common functions with that and be trimmed down to the essentials within them?\n. Cool, thanks for the feedback.  I'm going to go ahead and close this PR and come up with a new one.  I did take a look at the graphite module that you mentioned above (and at Monitorama) and I think I'm going to need a little bit of guidance on how it could be utilized.  I'll also take a closer look at the gist that you created that shows it in use.  We may want to rename the module to be something more generic since it will be used for different encoders eventually.\n. With the JSON protocol, yes it (heka dev branch) does.  That is eventually being deprecated by the InfluxDB project, though, which is the reason that we need to transition to the line protocol which I am working on implementing.\n. That Dockerfile points to someone's fork of the mozilla-services/heka git repository, so I don't know what it would be merged with.  Here's the documentation on the Heka Schema InfluxDB Write Encoder:  https://hekad.readthedocs.org/en/latest/config/encoders/schema_influx_write.html\n. Ya, as mentioned in that referenced issue, they have made changes to the API since this was committed to dev.  This was a moving target and we are now working on supporting the line protocol that InfluxDB 0.9.0 was released with.  Your PR looks good for the JSON protocol change required, so thanks for doing that.  Once the line protocol support is implemented, we'll get rid of this JSON protocol anyway, so I wouldn't stick with this for long.\n. It's documented here: https://hekad.readthedocs.org/en/latest/message/index.html\n. I've made some major improvements based on your excellent suggestions.  :)  Please give it another review when you have a chance.  I'll add some commentary around some of the key areas that have been updated.  Moving the configuration of the line_protocol functions to that module was a great improvement, so thanks for that and pointing out some of the other inefficiencies.  I'm learning a lot!\n. Ok, I got the iteration of the fields in a message moved to using decode_message(read_message(\"raw\")) after testing with a build of hekad from the latest dev branch commit and updated this PR.  I believe I've also addressed the other outstanding requests.  Please git it another review at your earliest convenience and let me know of anything else you find.  Thanks!\n. Closing this PR in lieu of another that will be merged into the versions.0.10 branch.\n. I personally just handle this in the deployment of the Elasticsearch instances themselves.  This, IMO, is where this really should be done, rather than the component sending data to Elasticsearch.  The json file can actually be deployed in the <elasticsearch install path/config/templates directory, as described in the documentation here: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html#config\n. I see that you just recently merged this into dev, so thanks!  I had this completed last night (along with good progress on the rest of the notes in your review), so I'll remove the changes outside of this file that I had pending to commit.  It works great, by the way. :)\n. Spelling: \"process\" instead of \"proccess\"; there are a couple others like this in the file too.\n. @davidbirdsong It's pretty ugly, but the workaround for lua converting 0.0 to 0 is here and then followed up on line 334.  It basically just converts it to a string and then removes the quotes when it's concatenating values when injecting a new message with payload composed of one or more series that are newline separated.\n. It doesn't add much. I'll migrate the if check to the msg_interpolate module and add it to this PR, updating references to field_interp to point to msg_interpolate.interpolate_from_msg instead.\n. Hmm maybe I am missing what you're pointing out.  Logger is part of both of these tables (base_fields_list and base_fields_tag_list).\n. Ya, this is really weird what I've done here, the same action is taken in both if conditions.  I like your idea and will make updates accordingly.  Thanks.\n. I actually cleaned this up and moved the functionality into the points_tags_tables function, since it really only needs to loop through base_fields_tag_list (the whole point of that table even existing).  I kept the for loop inside a conditional that checks if we are working with carbon_format so it will never run for Carbon but with always run for InfluxDB if there is configuration in place to add tags.  It then only matches if any of the previously checked conditions is true.  There is a little code duplication, but net result is improved, IMO.\n(updates to the PR will be made later after this comment is posted).\n. I added this config option and associated functionality since I'm primarily using this encoder to take the output of data collection scripts, splitting messages from it by newline, parsing them with the Payload Regex Decoder and then parsing them to Carbon lines with this encoder.  This allows the encoder to work the original way (by sending multiple fields as separate Carbon lines) or the way I just described, which identifies a single field in the message that contains the value (e.g. \"5.0\") and adds it to the Carbon line.\n. Let me know if you think a value like false would be better suited than nil for a default.  Right now the code checks the value of these config options and works accordingly, so nil is working fine, but I just wanted to get your opinion based on past experience.\n. I decided to change the name of this field based on InfluxDB vocabulary in their documentation.\n. I broke this out into its own function since the substitution was made in more than two different spots in the code.\n. Removed the separate function for this and moved it back into this function, simplifying the implementation based on what it actually needed to achieve (looping through \"taggable\" base fields, as defined by the base_fields_tag_list table.\n. This was changed from using exclude_name to the config value that replaced it (and makes it more flexible), source_value_field, which indicates which field in the message to pull the value from.\n. Thanks @trink.  I am happy to do so if you have any examples of how I would replace read_next_field with the functionality you described.  In the meantime, I'll parse through the existing lua code to see if I can find any use of that.\n. Well, I tried to implement it similar to below, but read_message(\"raw\") is returning nil in the place that read_next_field was used.  Any suggestions?\nlua\n    local decoded_message = decode_message(read_message(\"raw\"))\n    for field, value in pairs(decoded_message) do\n...\n        if config[\"carbon_format\"] then\n            field = field:gsub(\"[^%w]\", \"_\")\n        end\n...\n. How about 'ts_line_protocol' since it's being used for different time series databases (and could be expanded to others that implement a 'line protocol' in the future)?\n. Hmm, I'm still getting a null result from read_message(\"raw\"), resulting in this error message:\n2015/07/04 00:15:45 Plugin 'LogOutput' error: Error encoding message: FATAL: process_message() \n.../local/heka/share/heka/lua_modules/line_protocol.lua:135:bad argument #0 to 'decode_message'\n(must have one string argument)\nI'm using the 0.9.2 release for testing these modules/encoders.  Has this functionality been updated since that release?  I can try with the dev branch and report back with my findings.\n. ",
    "inthecloud247": "curious to know your thoughts here, but should this support message batching? or is that outside of the scope of an encoder.\n. what do you think about adding message batching functionality also?\n. ",
    "abdullin": "Is this the reason why you didn't provide Windows build for 0.9.0? Can Hekad be considered as stable for Windows if used without lua plugins?\n. @rafrombrc starting to roll-out heka 0.9 on Windows today. We are using it as collector daemons on Windows Azure instances. So far, it works good.\n. FYI, rolled heka to a production Windows Azure cluster at skuvault.com last night. This deployment uses 0.9.0 with a golang plugin instead of the lua version we used before. \nNo issues discovered so far.\n. Thanks a lot for the answer!\n. AFAIK, we notice this error when Heka comes back from the restart. Happened ~3 times in ~1 week.\n. We investigated that. It could be caused by a bug in our own configuration. Because of the cron-job gone wild, heka was restarting every minute for an hour. Repeat that multiple times a day, and there is a decent chance of getting ACK on \"Remove\" but then failure on the checkpoint.\nFixing that config issue should reduce the probability of that ever happening again to a negligible amount.\n. I'm sorry. We never encountered this issue since then. \nHowever, could some other part of the code (e.g. badly written plugin) terminate the runtime without requesting a proper shutdown? Physical machine shutdown, if it happens at unlucky point in time, could probably cause that as well.\n. Could high disk IO increase the chance of hitting this issue?\n. +1, very interested in that feature.\n. ",
    "johnjelinek": "@trink: Go 1.5 is released. Will we begin to see a windows binary in releases going forward now?\n. :+1: \n. ",
    "zyro": "OK, made some progress by switching my LogstreamerInput's parse_type to regexp with a pattern that looks like (?P<Timestamp>[0-9]{4}-[0-9]{2}-[0-9]{2}) to match the beginning of a log message.\nProblem now is the Message match group only seems to contain the first line, the rest seem to disappear...\n. ",
    "logicalparadox": "So this clearly is nowhere near compatible with the new Splitter approach. I will take the delivered fields feature back to drawing board but expect a new PR shortly for the exposed dialer functionality. \n. ",
    "bobek": ":heart: \nAwesome, thank you for the fix. I have tested it under the same circumstances as was reported and didn't face the problem.\n. ",
    "cristifalcas": "@afajl any plans on updating this PR? We would love to be able to rename the fields for elasticsearch\n. I just want to say that our experience with heka parsing of /dev/log was not very good.\nWe built a lua script instead of using regex, but we quickly learned that messages can take ANY form, there is no standard forced on what reached /dev/log. We concluded that we don't want to reimplement all the logic that makes rsyslog/syslog-ng good log parsers in lua.\nSo we have a local rsyslog server that takes care of /dev/log and forwards all messages in a rsyslog standard format to a local heka server, where we do the processing. Heka also parses various log files that are not sent via /dev/log.\n. I've put here what we did: https://gist.github.com/cristi1979/b0a8b6e6d312096f9515\nThe bulk logic was inspired by the rsyslog plugin.\n. Hi @trink, can you help me with some pointers on that? I will start writing a parser for glassfish logs and maybe I can do better with that one.\nMoving this discussion out of this PR -> https://github.com/mozilla-services/heka/wiki/Creating-a-GlassFish-parsing-module\n. It seems that the error is thrown from go itself and can't be fixed from heka.\nI was wondering if a new parameter will be accepted for unixgram. Something like this one from rsyslog: \nSysSock.Unlink  (available since 7.3.9) if turned on (default), the system socket is unlinked and re-created when opened and also unlinked when finally closed. Note that this setting has no effect when running under systemd control (because systemd handles the socket).\nhttp://www.rsyslog.com/doc/v8-stable/configuration/modules/imuxsock.html\nWhich can remove the socket if it exists before listening on it (before https://github.com/mozilla-services/heka/blob/dev/plugins/udp/udp_input.go#L70)\n. Any thoughts on this? If we make a pull request with the above changes, it will be taken into consideration?\n. Sorry, I was blind when reading \n. ",
    "afajl": "Sorry for the delay. I've added some tests and docs.\n. ",
    "jaredgisin": "What is the timing on getting this fix out into 0.9.1 ? I'm hitting this issue.\n. ",
    "Meeral": "Seeing this problem on 0.10.b1 as well\n. ",
    "sbward": "@rafrombrc I ran into this issue today on 0.10.b1, please re-open the issue :v: \n. ",
    "nightlyone": "Thx for clearing this up. I remembered the embedded Lua interpreter only be used for fast prototyping and ad-hoc scripting of plugins. I agree with your other use cases in mind my suggestions makes no sense.\n. ",
    "hejinlong": "Thank you rafrombrc. \nI use Heka  to collect nginx access log for Opentsdb.  Heka is a real-time, and Opentsdb is the second, I suspect it is because the problem of time difference. Opentsdb is too busy dealing with the past. \nBTW,  Encoder source code is following:\n```\npackage plugins\nimport (\n    \"bytes\"\n    \"encoding/base64\"\n    \"fmt\"\n    \"github.com/mozilla-services/heka/message\"\n    \"github.com/mozilla-services/heka/pipeline\"\n    \"strconv\"\n    \"strings\"\n    \"time\"\n)\n// TrendnginxEncoder generates a restructured text rendering of a Heka message,\n// useful for debugging.\ntype TrendnginxEncoder struct {\n    typeNames []string\n}\nfunc (re *TrendnginxEncoder) Init(config interface{}) (err error) {\n    if err != nil {\n        continue\n    } \n    re.typeNames = make([]string, len(message.Field_ValueType_name))\n    for i, typeName := range message.Field_ValueType_name {\n        re.typeNames[i] = strings.ToLower(typeName)\n    }\n    return\n}\nfunc (re TrendnginxEncoder) writeAttr(buf bytes.Buffer, name, value string) {\n    buf.WriteString(fmt.Sprintf(\"%s\", value))\n}\nfunc (re TrendnginxEncoder) writeField(buf bytes.Buffer, name, typeName, repr string,\n    values []string) {\n    ....\n}\nfunc (re TrendnginxEncoder) Encode(pack pipeline.PipelinePack) (output []byte, err error) {\n    // Writing out the message attributes is easy.\n    buf := new(bytes.Buffer)\n    //timestamp := time.Unix(0, pack.Message.GetTimestamp()).UTC()\n    //re.writeAttr(buf, \"Timestamp\", timestamp.String())\n    t := time.Now().Unix()\n    buf.WriteString(fmt.Sprintf(\"{\\\"metric\\\":\\\"trend_nginx\\\", \\\"timestamp\\\":%d\", t))\n    // Writing out the dynamic message fields is a bit of a PITA.\n    fields := pack.Message.GetFields()\n    if len(fields) > 0 {\n        v1 := \"\";\n        for _, field := range fields {\n            pack.Recycle()\n            ...\n        }\n        buf.WriteString(fmt.Sprintf(\",\\\"value\\\":%s\",v1))\n    }\n    buf.WriteString(\"}\")\n    return buf.Bytes(), nil\n}\nfunc init() {\n    pipeline.RegisterPlugin(\"TrendnginxEncoder\", func() interface{} {\n        return new(TrendnginxEncoder)\n    })\n}\n```\n. Thank you very much, rafrombrc !\nIt is still a problem.\nHeka is too busy, output datas GT 2000 at the same time,  \nHow  can  Heka  http out  many datas per second? \n2015/03/10 14:42:33 {\"metric\":\"trend_nginx\", \"timestamp\":1425969753,\"tags\":{\"remote_addr\":\"202.106.185.37\",\"status\":404},\"value\":0}\n2015/03/10 14:42:33 {\"metric\":\"trend_nginx\", \"timestamp\":1425969753,\"tags\":{\"remote_addr\":\"202.106.185.37\",\"status\":404},\"value\":0}\n2015/03/10 14:42:33 {\"metric\":\"trend_nginx\", \"timestamp\":1425969753,\"tags\":{\"remote_addr\":\"202.106.185.37\",\"status\":404},\"value\":0}\n2015/03/10 14:42:33 Plugin 'HttpOutput' error: HTTP Error code returned: 500 500 Internal Server Error - {\"error\":{\"code\":500,\"message\":\"Should never be here\",\"trace\":\"java.lang.RuntimeException: Should never be here\\n\\tat net.opentsdb.uid.UniqueId.getOrCreateId(UniqueId.java:602) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.core.Tags.resolveAllInternal(Tags.java:386) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.core.Tags.resolveOrCreateAll(Tags.java:373) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.core.IncomingDataPoints.rowKeyTemplate(IncomingDataPoints.java:135) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.core.TSDB.addPointInternal(TSDB.java:640) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.core.TSDB.addPoint(TSDB.java:549) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.tsd.PutDataPointRpc.execute(PutDataPointRpc.java:143) ~[tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.tsd.RpcHandler.handleHttpQuery(RpcHandler.java:255) [tsdb-2.0.1.jar:]\\n\\tat net.opentsdb.tsd.RpcHandler.messageReceived(RpcHandler.java:163) [tsdb-2.0.1.jar:]\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.SimpleChannelHandler.messageReceived(SimpleChannelHandler.java:142) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88) [netty-3.9.4.Final.jar:na]\\n\\tat net.opentsdb.tsd.ConnectionManager.handleUpstream(ConnectionManager.java:87) [tsdb-2.0.1.jar:]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [netty-3.9.4.Final.jar:na]\\n\\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [netty-3.9.4.Final.jar:na]\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Th\n2015/03/10 14:42:33 {\"metric\":\"trend_nginx\", \"timestamp\":1425969753,\"tags\":{\"remote_addr\":\"202.106.185.37\",\"status\":404},\"value\":0}\n2015/03/10 14:45:03 Diagnostics: 100 packs have been idle more than 120 seconds.\n2015/03/10 14:45:03 Diagnostics: (input) Plugin names and quantities found on idle packs:\n2015/03/10 14:45:03 Diagnostics:    LogOutput: 100\n2015/03/10 14:45:03 Diagnostics:    HttpOutput: 51\n2015/03/10 14:45:03\n. ",
    "lubko": "superseded by #1387 \n. ",
    "mattbostock": "Superseded by https://github.com/mozilla-services/heka/pull/1396. I rebased against versions/0.9 and updated CHANGES.txt. \n. What happens currently; could a failure to decode a message cause the plugin channel to fill up?\n. ",
    "kipras": "Any progress on this? This would be really helpful, because dropping messages after decoding is necessary sometimes.\n. ",
    "pasha-kuznetsov": "+1\n. ",
    "jens": "Sounds good\n. ",
    "Digicast": "Sent from my Virgin Mobile phone\nMike Trinkala notifications@github.com wrote:\n\nMerged #1402.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/mozilla-services/heka/pull/1402#event-249346259\n. \n",
    "sanga": "It appears that we have just hit this bug in production. Yesterday we had a box hard lock up (I'm still not entirely sure what happened - kernel problem or hypervisor problem maybe) and after bringing the machine back up, heka hits the error message described above when starting and immediately shuts down again\n. Would be awesome to get this fixed. This is a definite pain point for us\n. +1 this bit me for a moment too\n. ",
    "RobCherry": "Fixed.  Sorry about that, I had previously read the comment about cbuf_utils in #1365 but I had included it so I could pull in 2250129 without conflicts.\n. ",
    "mattupstate": "@rafrombrc I've experienced this memory issue as well. I'm curious what would cause the router to get \"backed up\"?\n. No, haven't nailed it down. We just recently noticed that a long running hekad instance was consuming as much as 2GB of memory at times. The best I can provide is the config at the time and some log output.\n```\ngoroutine 241684 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(PipelineConfig).allReportsData(0xc20804ca00, 0x0, 0x0, 0x0, 0x0)\n    /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/report.go:198 +0x176\ngithub.com/mozilla-services/heka/pipeline.(PipelineConfig).AllReportsMsg(0xc20804ca00)\n    /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/report.go:238 +0x27\ncreated by github.com/mozilla-services/heka/plugins/dasher.(*DashboardOutput).Run\n    /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/plugins/dasher/dashboard_output.go:292 +0x1a91\ngoroutine 241704 [chan receive]:\ngithub.com/mozilla-services/heka/pipeline.(PipelineConfig).reports(0xc20804ca00, 0xc226e38e40)\n    /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/report.go:81 +0xa3\ncreated by github.com/mozilla-services/heka/pipeline.(PipelineConfig).allReportsData\n    /home/admin/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/report.go:194 +0xf2\n```\n```\n[hekad]\nmaxprocs = 2\n[dashboard]\ntype = \"DashboardOutput\"\nticker_interval = 1\n[app_nginx_access_log_input]\ntype = \"LogstreamerInput\"\nlog_directory = \"/opt/app/log\"\nfile_match = 'nginx.access.log'\ndecoder = \"api_nginx_access_log_decoder\"\n[app_nginx_access_log_decoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/nginx_access.lua\"\n[app_nginx_access_log_decoder.config]\ntype = \"nginx.access\"\nuser_agent_transform = true\nuser_agent_conditional = true\nlog_format = '$remote_addr - $remote_user [$time_local] \"$request\" $status $body_bytes_sent $request_time $upstream_response_time \"$http_referer\" \"$http_user_agent\"'\n\n[app_nginx_nginx_status]\ntype = \"SandboxFilter\"\nfilename = \"lua_filters/http_status.lua\"\nticker_interval = 1\npreserve_data = true\nmessage_matcher = \"Type == 'nginx.access'\"\n    [app_nginx_nginx_status.config]\n    sec_per_row = 1\n    rows = 1800\n    perservation_version = 0\n\n[app_nginx_error_log_input]\ntype = \"LogstreamerInput\"\nlog_directory = \"/opt/app/log\"\nfile_match = 'nginx.error.log'\ndecoder = \"api_nginx_error_log_decoder\"\n[app_nginx_error_log_decoder]\ntype = \"SandboxDecoder\"\nfilename = \"lua_decoders/nginx_error.lua\"\n```\n. ",
    "erichandsmith": "We are seeing the same issue. I enabled memory profiling with and without DashBoardOutput. The results can be found here: https://gist.github.com/erichandsmith/8a66441e929438912893\n. I'm a super big idiot. Should have looked at the DashboardOutput code first. Just needed to set the working_directory path differently in each config. Sorry.\n. ",
    "robison": "I've been seeing some fairly large memory utilization on a pair of hosts that I'm using as Heka message routers; at first, I thought that the issue might be related to the DashboardOutput. After disabling it, I'm still seeing the same thing on both hosts - ~250MB/hour increase in memory use, at about ~3000 msgs/sec per host. Turned on memory profiling on one of them, will include some results here later today.\n. I am wholeheartedly interested in this, and would be willing to do a fair bit of testing of this feature in an already deployed environment.\n. Use the configuration directory option?\n. This discussion is most likely better suited for the mailing list, as it really isn't an issue with the software, per se.\n. Use ProcessDirectoryInput?\n. Any Heka messages that are created from decoded inputs can be sent via any output. I feel as though your issue may be one of misunderstanding how the Heka message router works. You have the option of setting up message matchers on your output plugin configuration to route Heka messages towards indexes. e.g.:\n```\n[ElasticSearchOutputDailyIndices]\ntype = 'ElasticSearchOutput'\nconnect_timeout = 5000\nencoder = 'ESJsonEncoderDailyIndices'\nflush_count = 10000\nflush_interval = 60000\nhttp_disable_keepalives = true\nmessage_matcher = '(Type != \"nginx\" && Type != \"httpd\" && Type != \"stats\")'\nserver = 'http://localhost:9200'\nuse_buffering = true\n[ElasticSearchOutputDailyIndices.buffering]\ncursor_update_count = 1\nfull_action = 'drop'\nmax_buffer_size = 8589934592\nmax_file_size = 67108864\n[ElasticSearchOutputDailyIndices.retries]\ndelay = '250ms'\nmax_delay = '8s'\nmax_retries = -1\n[ESJsonEncoderDailyIndices]\ntype = 'ESJsonEncoder'\nes_index_from_timestamp = true\nfields = [ 'DynamicFields', 'Hostname', 'Logger', 'Payload', 'Pid', 'Severity', 'Timestamp' ]\nid = '%{UUID}'\nindex = 'heka-%{colo}-%{Type}-%{%Y.%m.%d}'\nraw_bytes_fields = [ 'audit_details', 'audit_log_trailer', 'request_headers', 'response_headers' ]\ntimestamp = '%Y-%m-%dT%H:%M:%S.%f'\ntype_name = '%{Type}'\n[ESJsonEncoderDailyIndices.field_mappings]\nSeverity = 'level'\nTimestamp = '@timestamp'\nType = '_type'\nUuid = '_id'\n```\nTo answer your questions, there is no option at present to specify two configuration files, nor is there an option to have one TOML configuration file call in another TOML configuration file.\n. $ssl_protocol/$ssl_cipher aren't defined fields per the common_log_format[0] module (which is used here by nginx_access.lua) to decode the field. It simply sees a single field without space or quote delimiting, and interprets it as a single field. Here's an example; you may have some success by space-delimiting the two fields you want:\nhttp://lpeg.trink.com/share/11180597858328222060\n[0]: https://github.com/mozilla-services/lua_sandbox_extensions/blob/master/lpeg/modules/lpeg/common_log_format.lua\nAlso, as an aside, you may have more success asking these sorts of questions on the mailing list or IRC channel, rather than opening an issue. Please lemme know if this doesn't at least put you on the right track. Thanks!\n. [SomeFilter]\ntype = 'SandboxFilter'\nmemory_limit = 33554432\n. As of right now, this is not on the roadmap.\n. Please read the LogstreamerInput docs - parser_type is not a valid config setting for LogstreamerInput.\nAdditionally, this project has been deprecated for 18 months.. This is a deprecated project.. ",
    "philbooth": "Fwiw, I also ran into this issue and tried to get past it by using install_name_tool to update the lib path to my local build:\nsh\ninstall_name_tool -change \\\n    /Users/rob/golang/heka/build/ep_base/Build/lua_sandbox/src/libluasandbox.dylib \\\n    /Users/pbooth/code/heka/build/ep_base/Build/lua_sandbox/src/libluasandbox.dylib \\\n    /usr/share/heka/lua_modules/circular_buffer.so\nThat changed the error to complain about _luaL_register being not found:\n2015/12/24 16:50:18 Error making runner for FxaContentSignupMetricsCategorized: Initialization failed for 'FxaContentSignupMetricsCategorized': Init() error loading module 'circular_buffer' from file '/usr/share/heka/lua_modules/circular_buffer.so':\n    dlopen(/usr/share/heka/lua_modules/circular_buffer.so, 2): Symbol not found: _luaL_register\n  Referenced from: /usr/share/heka/lua_modules/circular_buffer.\nThis is my config (I started off with the example from the docs, then modified it to add the filter that I'm testing):\n``` toml\n[hekad]\nmaxprocs = 4\n[LogstreamerInput]\nlog_directory = \"/Users/pbooth/code/heka\"\nfile_match = 'pb.log'\n[PayloadEncoder]\nappend_newlines = false\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"PayloadEncoder\"\n[FxaContentSignupMetricsCategorized]\ntype = \"SandboxFilter\"\nscript_type = \"lua\"\nfilename = \"/Users/pbooth/code/heka/fxa_content_signup_metrics_categorized.lua\"\nticker_interval = 60\npreserve_data = true\nmessage_matcher = \"TRUE\"\n[FxaContentSignupMetricsCategorized.config.services]\nmarketplace = \"0a42ac8a73be8762\"\nhello = \"a8b39c2b1cab722e\"\npocket = \"749818d3f2e7857f\"\n\n```\nThe filter is fxa_content_signup_metrics_categorized.lua from this PR.\nThis is the directory listing for my module directory:\ntotal 552\n-rw-r--r--  1 pbooth  staff   3.0K 21 Feb  2015 alert.lua\n-rw-r--r--  1 pbooth  staff   4.5K 21 Feb  2015 annotation.lua\n-rw-r--r--  1 pbooth  staff    16K 21 Feb  2015 anomaly.lua\n-rw-r--r--  1 pbooth  staff    19K 13 Mar  2015 bloom_filter.so\n-rw-r--r--  1 pbooth  staff   1.4K 13 Mar  2015 cbufd.lua\n-rw-r--r--  1 pbooth  staff    29K 24 Dec 16:47 circular_buffer.so\n-rw-r--r--  1 pbooth  staff    29K 13 Mar  2015 cjson.so\n-rw-r--r--  1 pbooth  staff    15K 13 Mar  2015 common_log_format.lua\n-rw-r--r--  1 pbooth  staff   9.5K 13 Mar  2015 date_time.lua\n-rw-r--r--  1 pbooth  staff   3.9K 21 Feb  2015 elasticsearch.lua\n-rw-r--r--  1 pbooth  staff    19K 13 Mar  2015 hyperloglog.so\n-rw-r--r--  1 pbooth  staff   1.1K 13 Mar  2015 ip_address.lua\n-rw-r--r--  1 pbooth  staff    50K 13 Mar  2015 lpeg.so\n-rw-r--r--  1 pbooth  staff   3.1K 13 Mar  2015 mysql.lua\n-rw-r--r--  1 pbooth  staff   6.1K 13 Mar  2015 re.lua\n-rw-r--r--  1 pbooth  staff   1.8K 13 Mar  2015 sfl4j.lua\n-rw-r--r--  1 pbooth  staff    18K 13 Mar  2015 struct.so\n-rw-r--r--  1 pbooth  staff   8.6K 13 Mar  2015 syslog.lua\n-rw-r--r--  1 pbooth  staff   875B 13 Mar  2015 util.lua\nAny ideas what's up?\n. ",
    "nomadium": "Thanks for the answer. I just wanted to comment, the longest line in those files had 32528 bytes. In any case, thanks for the pcall recommendation, I'll try that.\n. That example would be the largest, 32528 bytes.\n. I set a high output_limit value (200K) and after taking into account your pcall recommendation, my custom encoder seems to be working. Thanks for the help.\n. ",
    "chzyer": "move to: https://github.com/mozilla-services/heka/pull/1447\n. It's landing in the 0.9.2 release now! @rafrombrc\n. ",
    "hoffoo": "@rafrombrc very cool, makes sense. sorry for spamming here!\n. @rafrombrc okay i hope the wording is okay, not very good at writing documentation.\nplease let me know if there are other issues!\n. I really don't know why that test is failing, they aren't locally for me. Ill figure it out and fix it\n. This is pretty bad, the reason is though that LogstreamLocationFromFile will not return an error if journal is missing. Maybe there is a better way to do this?\n. ",
    "rajivek": "Seems current fix in logstreamer_rotate_fix` branch is working. But when we doing a long run its not properly reading the logs (i.e missing some data too). \nI have used only one single log stream for testing but when we running for around 20 - 30 min heka gives \"too many files open error\". Currently its rotating in every 5 min.  \nhttp://pastebin.ca/2963610 - toml file\nhttp://pastebin.ca/2963598 - sh script for simulate the log rotation.\nsame script increasing the number of lines per log\n. I have tested this issue with many files, I did not find any issues after your commit. Any way need to check with long run. \n. I have tested this.. fix.. I haven't see any issue with this fix. Please merge this with the release.\nThank you for the help\n. ",
    "guluglu": "This question is still exist in version 0.11.0\n. I also found this question in v0.10.0.   increase flush_interval  in elasticsearchoutput can aviod it or decrease error(it depend to flush_interval). but i dont know reason\n. #1783\n. If increase flush_interval to large enough, no http error and no missing logs in Elasticsearch\n. the larger flush_interval is ,  less http error appear\n. Like start_position(string, one of [\"beginning\", \"end\"]) setting in Logstash\n. @rafrombrc  Hi, I test it, but it doesn't work when file is small than 500 bytes i.e. it  still read from begin though I set initial_tail as true\n. I use dozens machine installed heka to send data to elasticsearch,many heka process was killed, I trace process and found it point out \"killed by SIGPIPE\"\n. ",
    "bmarini": "Thanks for the detailed reply! This is extremely helpful.\n. ",
    "hamshiva": "I missed this part of the patch:\n@@ -333,7 +334,7 @@ func (o *FileOutput) committer(or OutputRunner, errChan chan error) {\n                        }\n                case rotateTime := <-o.rotateChan:\n                        o.file.Close()\n-                       o.path = rotateTime.Format(o.Path)\n+                       o.path = strftime.Format(o.Path, rotateTime)\n                        if err = o.openFile(); err != nil {\n. ",
    "briankhsieh": "I am modifying the testing cases. Will commit a fix after it passes all testing cases. \n. Will reopen a new pull request. \n. I am working on this feature for my current project. \nI will contribute my implementation back. \n. Merged, pull request #1523 \n. syscall.WaitStatus works for all platforms, to my best knowledge. \nThere is the description, sys() is a platform dependent exit information.\nhttps://golang.org/pkg/os/#ProcessState.Sys\nExec implementation for windows also indicates it implements all interfaces. \nhttps://golang.org/src/os/exec_windows.go\n. It might come from the exist status implementation in v0.10. I will check. \n. While fixing this bug, I was not able to reproduce the original issue that the earlier commit was trying to solve.\nLet me run more testing cases and will confirm the situation. \n. ",
    "k2xl": "I see you check if response is > 304.\nWhich responses does ES return that are 500 when posting data?\nI think i have seen ES return 502 for when elastic search is still starting up - and I imagine we would want to continue trying messages that see that response. \n. ",
    "timurb": "any other way to read from local unix socket?\n. @simonpasquier does it say that TokenSplitter works incorrectly?\nProbably, I haven't checked that with disabling splitter (as it has little sense for long-running input).\nOn the other hand I think I have not disabled splitter when I used a single-run version of ticker and all was ok (I'm not completely sure in this).\n. @rafrombrc Looks fine to me in this build.\n. Hi, @Madhu1512 \nYou could use FilePollingInput for that with PayloadRegexDecoder.\nDo you think this would work for you?\n. Hi @ChristianKniep \nI think you need to use some kind of Splitter in addition to Decoder. Not quite sure right away which one as I'm quite new to Heka like you are.\nYou better ask such questions in mailing list, github is only for bugreports: https://mail.mozilla.org/listinfo/heka\n. My case (described here: https://mail.mozilla.org/pipermail/heka/2015-October/000855.html) although a bit different looks to be resolved too with this patch.\nI've been running patched Heka for 5 days by now and the queue haven't got overfilled on disk since that time. On other boxes which are running 0.10.0b1 the files on disk grow large as usual.\n. I've just checked this with HTTPOutput buffering enabled in both stock Heka 0.10.0b1 and the queue_full_shutdown_fix merged in it and I'm afraid it is not get fixed.\nIn original Heka 0.10.0b1 I see the following messages on stop (see my configs at the bottom of the comment):\n2015-11-17_18:51:15.16143 2015/11/17 18:51:15 Decoders shutdown complete\n2015-11-17_18:51:15.16147 2015/11/17 18:51:15 Stop message sent to output 'DashboardOutput'\n2015-11-17_18:51:15.16151 2015/11/17 18:51:15 Stop message sent to output 'opentsdb'\n2015-11-17_18:51:15.16156 2015/11/17 18:51:15 Stop message sent to output 'relay_output'\n2015-11-17_18:51:15.16160 2015/11/17 18:51:15 Plugin 'DashboardOutput': stopped\n2015-11-17_18:51:15.50541 2015/11/17 18:51:15 Plugin 'relay_output': stopped\n2015-11-17_18:51:15.61692 2015/11/17 18:51:15 Plugin 'opentsdb' error: updating buffer cursor: can't parse queue cursor '': invalid checkpoint format\nAfter than Heka freezes and I'm only able to kill it with kill -9. When I start it up after that the queue logs are not removed from disk.\nIf I set cursor_update_count not to 1 but to 10 I no longer see the error nor do I see it in patched version but the freeze is still there in both cases and the queue logs are not removed from disk.\nHere is my config:\n```\n[OpentsdbEncoder]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/opentsdb_batch.lua\"\n[OpentsdbEncoder.config]\n  ts_from_message = true\n  add_hostname_if_missing = true\n  type_as_prefix = false\n  tag_prefix = \"tags.\"\n  tag_list = \"env adx\"\n  skip_fields = \"Timestamp timestamp Metric metric Value value ExitStatus\"\n[opentsdb]\ntype = \"HttpOutput\"\nmessage_matcher = \"(Type != 'heka.all-report' && Type != 'heka.sandbox-output' && Type != 'heka.sandbox-terminated') && (TRUE || Type == 'heka.memstat')\"\naddress = \"http://opentsd.mydomain.com:4242/api/put\"\nencoder = \"OpentsdbEncoder\"\nuse_buffering = false # or true. This param and the buffering section was added for testing\n    [opentsdb.buffering]\n    max_file_size = 268435456 # 256Mb\n    max_buffer_size = 107374182400 # 100Gb\n    full_action = \"block\"\n    cursor_update_count = 10\n[relay_output]\ntype = \"TcpOutput\"\naddress = \"10.20.30.40:9876\"\nmessage_matcher = \"(Type != 'heka.all-report' && Type != 'heka.sandbox-output' && Type != 'heka.sandbox-terminated') && (TRUE)\"\n```\nNevertheless I think my a bit different case to which I've referred above (https://github.com/mozilla-services/heka/issues/1738#issuecomment-156370202) still gets fixed with the patch.\n. Please ignore my first comment (https://github.com/mozilla-services/heka/issues/1738#issuecomment-156370202) above, it is not relevant to this issue and relates to #1790.\n. The code is updated according to your feedback.\nI've just realized that to avoid breaking @hynd's setup (whose original plugin I've taken to rework) I have to rename the lua file so it is now called opentsdb_batch.lua.\nBy the way should I put any credits for his implementation (from which I think only docs and flush function is left in place) and how to do that?\nI've updated the docs but have never worked with RST so not very confident all is fine there, please let me know if it looks correct.\n. Should I rewrite the code using read_next_field() in favor of read_raw() or read_raw() should be ok too?\nI'm referring to this discussion: https://mail.mozilla.org/pipermail/heka/2015-October/000831.html\nWe were discussing the decoders and I wonder if anything from here is relevant for encoders too.\n. Ok, I'll leave read_message(\"raw\") here then.\nRegarding tonumber -- if performance is fine let's leave this as is then. Checking for not skip_fields[name] first will add one more layer of ifs which I believe makes the code harder to read.\n. Ok, created pull request of what we have now\n. Hi,\nI usually see panics (don't know if they match the one provided above) when\nthere is some error loading some Lua plugin.\nDo you see any error above the panic?\nRegards,\nTimur\nOn Tue, Mar 29, 2016 at 11:56 AM, yqguodataman notifications@github.com\nwrote:\n\n+1\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1881#issuecomment-202784581\n. It was only in docs but really the default is true.\nI'll update this and all other points shortly and update the pull request.\n. This was left from @hynd's implementation. I'll remove the reference.\n. @sathieu If you know fields names you can read them like read_message(\"Fields[myfield]\").\nIf you don't know field names there is a read_next_field() to iterate over all fields. See http://hekad.readthedocs.org/en/v0.9.2/sandbox/index.html for that.\n\nDon't use read_message('raw') in decoder (as docs for v0.10.0b1 suggest) -- it is safe to use in filters and encoders but in decoders it sometimes works incorrectly, I've just hit this issue.\n. ",
    "phobologic": ":) Of course I submit when it seems like github is having issues... \nfatal: unable to access 'https://github.com/AdRoll/goamz/': Operation timed out after 300009 milliseconds with 0 out of 0 bytes received\n. Thanks - updated.\n. ",
    "ramukima": "Confirmed, works with log_directory being absolute path.\n. ",
    "jbarreneche": "Hi!\nI also run into this issue, but in our case we need a multiline string for a sandbox filter config. The main issue is readability (ie. avoiding a long oneliner string). \nAny plan for updating the TOML parser?\n. ",
    "avinson": "the issue can be worked around with the following code:\nlua\nraw_message = read_message(\"Payload\")\nlocal m = string.sub(raw_message, 2)\nlocal ok, json = pcall(cjson.decode,m)\nhowever, I have other JSON blobs that fail not on just the first character so something else may be going on\n. ",
    "samos123": "I'm running heka 0.10 but seems I'm still hitting the issue. Any way to workaround this and fix a running environment?\nLogs:\nbroken pipe\n2016/06/28 07:46:30 Plugin 'aggregator_tcpoutput' error: can't send record: writing to 192.168.10.2:5565: write tcp 192.168.10.2:5565:\n broken pipe\n2016/06/28 08:29:03 Plugin 'influxdb_output' error: updating buffer cursor: can't parse queue cursor '': invalid checkpoint format\n2016/06/28 08:29:05 http: panic serving 127.0.0.1:46271: send on closed channel\ngoroutine 274 [running]:\nnet/http.func\u00b7011()\n        /usr/lib/go/src/net/http/server.go:1130 +0xbb\ngithub.com/mozilla-services/heka/pipeline.func\u00b7009(0xc2082ccd00)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:456 +0x4a\ngithub.com/mozilla-services/heka/pipeline.(*iRunner).Deliver(0xc208043cc0, 0xc2082ccd00)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:534 +0xc2\ngithub.com/mozilla-services/heka/pipeline.(*sRunner).DeliverRecord(0xc20e427ad0, 0xc20c4a7000, 0xf8a, 0x1000, 0x0, 0x0)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/splitter_runner.go:284 +0x337\ngithub.com/mozilla-services/heka/pipeline.(*sRunner).SplitStreamNullSplitterToEOF(0xc20e427ad0, 0x7f5ced35d2d8, 0xc20e6ca900, 0x0, 0x0, 0x0, 0x0)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/splitter_runner.go:410 +0x50a\ngithub.com/mozilla-services/heka/plugins/http.(*HttpListenInput).RequestHandler(0xc20800d860, 0x7f5ced35d2a0, 0xc20da3c0a0, 0xc20d00a0d0)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/plugins/http/http_listen_input.go:201 +0x43c\ngithub.com/mozilla-services/heka/plugins/http.*HttpListenInput.RequestHandler\u00b7fm(0x7f5ced35d2a0, 0xc20da3c0a0, 0xc20d00a0d0)\n        /home/elemoine/src/go-workspace/src/github.com/elemoine/heka/build/heka/src/github.com/mozilla-services/heka/plugins/http/http_listen_input.go:217 +0x45\nnet/http.HandlerFunc.ServeHTTP(0xc2080b0b20, 0x7f5ced35d2a0, 0xc20da3c0a0, 0xc20d00a0d0)\n        /usr/lib/go/src/net/http/server.go:1265 +0x41\ngithub.com/mozilla-services/heka/plugins/http.func\u00b7007(0x7f5ced35d2a0, 0xc20da3c0a0, 0xc20d00a0d0)\n:\n. ",
    "eldondev": "I am new to heka, but I thought I would chime in here with an outside opinion. A highly modifiable switch for formatting fractional seconds seems a bit excessive, and might overcomplicate the matter. It seems reasonable to me to use %f for microseconds (like python), and %N for nanoseconds (this is what my date does, I don't know why Joey glosses over it) . It seems pretty unlikely that one would need much more resolution than that, and it supplies simplicity at the cost of perfect adherence to c's strftime implementation. I would recommend maybe making a pull request to one (or both) of the two projects you mentioned to support it. I may work on that as well if I can hack it out.\n. ",
    "nerdatmath": "@rafrombrc: Do you want me to incorporate your suggestions and submit another pull request?\n. ",
    "tianlin": "hekad.toml\n[dat_splitter]\ntype = \"RegexSplitter\"\ndelimiter = '\\n(zzz)'\ndelimiter_eol = false\nkeep_truncated = true\ndeliver_incomplete_final = true\n[acc_logs]\ntype = 'LogstreamerInput'\nlog_directory = \"/wls/applogs/rtlog\"\nfile_match = '(?P.*).acc'\ndifferentiator = [\"Path\", \".acc\"]\nsplitter = \"dat_splitter\"\n[PayloadEncoder]\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"PayloadEncoder\"\n/wls/applogs/rtlog/test.acc\nHello world\nzzz\nEvent 2\nzzz\nEvent 3\nEvent 4\nWhen it output Event4, it doesnot update SeekPosition.You can restart hekad, and you will found it output Event3\\nEvent4 again.So I modify it like this at my local heka source:        } else if err == io.EOF && lsi.sRunner.IncompleteFinal() && lsi.sRunner.NeedsDeliver() {\nrecord = lsi.sRunner.GetRemainingData()\nif len(record) > 0 {\n-    lsi.stream.FlushBuffer(len(record))\n    si.sRunner.DeliverRecord(record, lsi.deliverer)\n-    lsi.stream.SavePosition()\n    lsi.countRecord()\n  }\n. Please run mingw32-make package at C:\\Users\\popov_a\\Source\\Repos\\misc\\heka\\build\n. @runner-mei , I use cmake-3.3.0-rc3, the problem is still there. At last I  fix it by this:\n1.modify src/CMakeLists.txt:35 to\ninstall(DIRECTORY \"${EP_BASE}/bin/\"  DESTINATION lib COMPONENT PATTERN \"*.dll\")\n2.modify build/ep_base/Source/lua_sandbox/cmake/externals.cmake:66 to \nINSTALL_ARGS \"install\"\n. Thanks,the fix is ok.\n. I don't think so, when I read text from LogstreamerInput, I want heka to truncate record for me, but when it truncate record by MAX_RECORD_SIZE, it will put the text to Message.Payload, this will cause queue_buffer can't send this truncated record.\n. Or can you teach me how to handle it?\n. I do it with nssm(https://nssm.cc/) \ncall nssm.exe install hekad hekad\ncall nssm.exe set hekad Application \"%BIN_DIR%\\hekad.exe\"\ncall nssm.exe set hekad AppDirectory \"%BIN_DIR%\"\ncall nssm.exe set hekad AppParameters -config=\\\"%CONF_DIR%\\hekad.toml\\\"\ncall nssm.exe set hekad AppStopMethodSkip 0\ncall nssm.exe set hekad AppStopMethodConsole 10000\ncall nssm.exe set hekad AppStopMethodWindow 1500\ncall nssm.exe set hekad AppStopMethodThreads 10000\ncall nssm.exe set hekad AppStdout \"%BASE_DIR%\\log\\hekad.stdout\"\ncall nssm.exe set hekad AppStderr \"%BASE_DIR%\\log\\hekad.stderr\"\ncall nssm.exe start hekad\n. ",
    "williamsandrew": "FWIW, my company has a patch in our custom build of Heka to only write the checkpoint file out every X milliseconds in the Kafka input. If this is what Mozilla had in mind I could submit a PR.\nhttps://github.com/mozilla-services/heka/compare/dev...intoximeters:reduce-kafka-snapshot-load\n. Updated my Gist to add a timestamp to the stderr output to better explain the issue. Here is a sample of the output I get from hekad.\n```\n[root@localhost heka_testing]# hekad -config heka-bug.toml \n2015/09/03 15:30:12 Pre-loading: [TestShutdown]\n2015/09/03 15:30:12 Pre-loading: [ProtobufDecoder]\n2015/09/03 15:30:12 Loading: [ProtobufDecoder]\n2015/09/03 15:30:12 Pre-loading: [ProtobufEncoder]\n2015/09/03 15:30:12 Loading: [ProtobufEncoder]\n2015/09/03 15:30:12 Pre-loading: [TokenSplitter]\n2015/09/03 15:30:12 Loading: [TokenSplitter]\n2015/09/03 15:30:12 Pre-loading: [HekaFramingSplitter]\n2015/09/03 15:30:12 Loading: [HekaFramingSplitter]\n2015/09/03 15:30:12 Pre-loading: [NullSplitter]\n2015/09/03 15:30:12 Loading: [NullSplitter]\n2015/09/03 15:30:12 Loading: [TestShutdown]\n2015/09/03 15:30:12 Starting hekad...\n2015/09/03 15:30:12 MessageRouter started.\n2015/09/03 15:30:12 Input started: TestShutdown\nAbout to return: 1441312212\nAbout to return: 1441312214\nAbout to return: 1441312216\nAbout to return: 1441312218\nAbout to return: 1441312220\n^C2015/09/03 15:30:22 Shutdown initiated.\n2015/09/03 15:30:22 Stop message sent to input 'TestShutdown'\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\nAbout to return: 1441312222\n\n```\n. That should be fine. I'll open a new pull request as I don't think Github will let me change the branch to merge into once the pull request has been created.\nFor clarification on point 3, did you mean do something like this?\n```\nok := true\nfor ok {\n    // Do some things\n    select { // block until stop or poll interval\n    case _, ok = <-s.stopChan:\n    case <-ticker:\n    }\n\n```\n. Created #1709 with the suggested changes.\n. LGTM\n. That sounds great. Sorry for the noise as I hadn't seen that TCP re-establishment issue. It looks like that should handle our use case.\n. ",
    "rustamk": "We are seeing the same issue in our testing. \nJust a hope that fix will contain a check on kafka to see when it comes back to life to resume sending data.\n. Is there any update on this issue by any chance?\n. ",
    "JozoVilcek": "We just got bit by the connection leak. Quite an old issue and I see more for kafka support. Is this going to get some care any time soon? Or should we abandon using heka with kafka? What are the long term plans?\n. +1\nRather week support for kafka is a major complication for me to largely adopt heka. Brokers can be down from time to time but consumers / producers for replicated topics should not be broken by this.\n. ",
    "jakobkylberg": "We've been hit by the connection leak in our environment too. Just as JozoVilcek in the comment above I would like to know what the plans are regarding this issue? It would be very much appreciated if this issue could get some attention.\n. ",
    "yqguodataman": "+1\n. +1\n. ",
    "freb": "I will re-open the issue once I have more time to investigate.\n. ",
    "cruatta": "Wouldn't this be a use case for something like Kafka or another MQ? You could have Heka write output to Kafka and have your custom clients consume the Kafka topics. This way your clients can bypass ES. \n. ",
    "mathpl": "I've made an output that will forward every messages sent to it to any client that connects to a tcp socket it opens. If there are no clients all messages are discarded. You can have a look at it: https://github.com/mathpl/heka-plugins/blob/master/message_tap/messagetap_output.go\nI'll be happy to clean it up as a PR if there are interested parties.\n. ",
    "fernandezvara": "I hope it meets the documentation style. Thanks for accepting the request.\n. ",
    "jetpks": "The .git directory is not packaged in the tarball release, so there is literally no way for . build.sh to ever run unless this is patched in.\n. Ahhhhh @rafrombrc that makes a lot of sense. I got the tarball from here. I ran into this building rpms where source traditionally comes in a tarball. I'll just patch this out at build time. Thanks for the info! :)\n. ",
    "xrl": "Bump with the changelog addition.\n. I'm going to do some work on this today.\n. Cool, sounds fair to me. I'll work on that and get back to you.\n. I'll work on it first thing tomorrow and push up the changes. Thanks!\n. Yep! Consider it closed :sunglasses: \n. Ehh, I don't think this is quite right it's bringing in changes from dev.\n. Closing for a PR against version/0.10\n. What version of heka are you running? 0.10b has some queueing bugs which I believe are in the process of being sorted out.\n. @tino I have an open PR to address these problems: https://github.com/mozilla-services/heka/pull/1924. Please try it out and let me know what you think.\n. Thanks for the feedback! I think graylog is a popular format, it's included as one of the default log drivers in the docker daemon (which is why I wrote this) and it's a default plugin in logstash.\nThere's already a graylog decoder in heka core but it's not broadly useful because it can't do the standard decompression you'll see coming out of graylog connections.\nIf it's any consolation, the included dependency is a very simple one and does not itself include other dependencies. It makes heavy use of the go stdlib.\n. This is failing because the travis account has not had travis env set --public DOCKER_REPO_SLUG mozilla-services/heka. Once this DOCKER_REPO_SLUG is set it will be available to all travis builds -- and it would just make sense to kick Travis to build again.\n. How do you grab fields by name in go code?\n. I think it's worth it because people's config's won't break when the load this config. Also, how will people choose to send all Fields (the current default) without this behavior?\n. ",
    "Elemecca": "@uohzxela over at the Sarama issue, you said:\n\nWe do have 2 partitions, the problem lies with the fact that KafkaOutput plugin (Sarama producer) is producing to either one of the partitions randomly (the default setting for the partitioner is Random), and we are only consuming from one partition. I changed the Kafka config to have only one partition and it works now. Closing this issue since the problem lies with our Kafka config, not with Sarama. \n\nShouldn't this issue also be closed, since you've fixed your problem?\n. ",
    "uohzxela": "Yes, thanks for reminding me. I have forgotten about this issue. Closing this now.\n. ",
    "matschaffer": "I've been seeing this as well with https://gist.github.com/matschaffer/55864a96702f72904087 as a configuration.\nOn my Mac, corecapture basically never works. Just hangs before even one message.\ncorecapture.rb seems to work fairly consistently on 0.9.2 but seems to stop pretty readily on 0.10.0.\ncorecapture2 works a few times but has a tendency to stop.\n(tested on 0.9.2 OS X binary release, and 0.10.0-62e828 built from source)\n. Just tried with this patch and behavior appears unchanged. I re-ran \ndiff\ndiff --git a/pipeline/splitter_runner.go b/pipeline/splitter_runner.go\nindex 58c05ec..047dba7 100644\n--- a/pipeline/splitter_runner.go\n+++ b/pipeline/splitter_runner.go\n@@ -330,7 +330,7 @@ func (sr *sRunner) SplitStream(r io.Reader, del Deliverer) error {\n                                err = nil\n                        }\n                }\n-               if len(record) == 0 {\n+               if len(record) == 0 && !sr.needData {\n                        if sr.incompleteFinal && err == io.EOF {\n                                record = sr.GetRemainingData()\n                                if len(record) > 0 {\nFor good measure I threw some fmt.Println in there. Doesn't look like len(record) == 0 && !sr.needData ever evaluates to true with any of the tests. With the ruby script I get one message containing \"a\\u000ab\\u000a\" according to the ESJsonEncoder, then no more messages.\nNeither of the bash-based scripts yield any messages.\n. I can confirm #1572 works for all 3 of my scripts in the gist above. No signs of stopping yet.\nmake test from this branch passes on my mac as well.\n. Left https://gist.github.com/matschaffer/55864a96702f72904087 running over the weekend with this patch and it's still processing records this morning. :+1: \n. ",
    "ziegeer": "You are correct, statsd also does not delete idle stats by default.  I asked them for a change as well because that also has caused problems here in the past, and they said they are interested but are waiting for a 1.0 release before they change a default config value like that.\n. ",
    "jpgnz": "+1\n. ",
    "LordFPL": "+1\n. ",
    "mjseid": "+1\n. @steverweber I can confirm this works for 0.9.0 (I've been using successfully for about a week now with a minor tweak to batch up writes).  Here's the relevant parts from my config, other than the db creds looks like the only real difference is I'm specifying retention policy in my write command.\n```\n[influxdb-encoder-cf-np]\ntype = \"SandboxEncoder\"\nfilename = \"lua_encoders/schema_influx_line.lua\"\n[influxdb-encoder-cf-np.config]\nskip_fields = \"**all_base** Env IP Index Job Metric\"\ntag_fields = \"Job Index IP\"\ntimestamp_precision= \"s\"\n\n[influxdb-output-cf-np]\ntype = \"HttpOutput\"\nmessage_matcher = \"Type == 'CFCollector' &&  Fields[Env] == 'cf-np' \"\naddress = \"http://influxdb:8086/write?db=cf_np&rp=default&precision=s\"\nusername = \"root\"\npassword = \"root\"\nencoder = \"influxdb-encoder-cf-np\"\n```\nAlso as suggested you can use these settings to send everything from Heka to the log file to potentially get more info....\n```\n[PayloadEncoder]\nappend_newlines = false\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"PayloadEncoder\"\n```\n. @rafrombrc \nI tried the new config you posted and I still get an error:\n unknown config setting for 'tcp:5565': signer\nI was able to get it working by looking at the configuration here:\nhttps://gist.github.com/bbinet/d1d2125cce2710b5c7e1\n```\n[tcp:5565]\ntype = \"TcpInput\"\nsplitter = \"acl_splitter\"\ndecoder = \"ProtobufDecoder\"\naddress = \":5565\"\n[acl_splitter]\ntype = \"HekaFramingSplitter\"\n        [acl_splitter.signer.CloudOps_0]\n        hmac_key = \"key\"\n```\nAm I setting it up correctly?  Also since the sbmgr command seems to have gotten renamed to heka-sbmgr should that get updated in the docs too?  Thanks\n. ",
    "mattrobenolt": "@rafrombrc So I'm not 100% sure what the intentions of this Dockerfile are, but I explicitly removed the last RUN which calls build.sh. So since this container isn't running build.sh, /heka/build never gets created.\nI modified this container to suit the needs of a developer wanting to contribute to heka and get into a working environment which I can edit and build on my own inside the container. This is similar IMO to how you contribute to docker itself. They provide a dev environment inside a container in which you can compile and run tests, etc, on docker.\nSo maybe we can move the call for build.sh into https://github.com/mozilla-services/heka/blob/dev/docker/Dockerfile ?\n. Totally reasonable. If you're ok with this change, I'll update the rest to fit.\n. I apologize, got a little backlogged. Let me button this up today. :)\n. @rafrombrc Updated to add docs/changelog.\n. Our fork is just master + a few of my PRs applied that we need in production.\n. So there is one problem here, is the - character needs to be escaped, since that's being used as the escape character above on L394.\nThis will prevent backwards compatibility though if we escape -. So unsure how to proceed best without changing the topic names on people.\n. ",
    "fydot": "Aoh~\n. ",
    "haf": "As a newbie: does heka (https://registry.hub.docker.com/u/ianneub/heka/tags/manage/) latest support 0.9 of influxdb or not?\n. Thanks.\nWhen is the dev branch stable?\n. For the linked docker, heka cannot push to influxdb.\nDoes this docker container contain enough of the dev branch? https://github.com/disqus/docker-heka\nCould you give me a link to the JSON protocol configuration?\n. It points to @mattrobenolt's fork which you have in your unmerged PRs https://github.com/mozilla-services/heka/pull/1569 -- it seems to be the latest. At least when sorting on updated-date on docker. Do you have an official one?\n. @rafrombrc Sweet! =)\n. Those steps don't work. Have you tested them?\n\u279c  heka git:(dev) make install\nmake: *** No rule to make target `install'.  Stop.\n\u279c  heka git:(dev) cd build\n\u279c  build git:(dev) make install\n/bin/sh: line 0: cd: /Users/haf/dev/logibit/ops/heka/build: No such file or directory\nmake[2]: *** [CMakeFiles/gostrftime.dir/depend] Error 1\nmake[1]: *** [CMakeFiles/gostrftime.dir/all] Error 2\nmake: *** [all] Error 2\n\u279c  build git:(dev) ls\nCMakeCache.txt          CPackConfig.cmake       CTestTestfile.cmake     Makefile                cmake_install.cmake     heka\nCMakeFiles              CPackSourceConfig.cmake DartConfiguration.tcl   Testing                 ep_base                 plugin_loader.go\n. Fixed through PR.\n. Ok, thanks. However, isn't this a feature request?\n. Would be good to get some input on the difference between a string in a field and a payload, too.\n. And env_version and why it's being set to 0.2, 0.8 and 1 in the sources.\n. So if I read the docs correctly:\n- payload: the cherry on top of the pie, the reason for logging to exist \u2013 the LogLine message!\n- field.string_value: I have a dictionary of 'context' to send along, one key-value is string, I put that here.\nRight?\nAnd about representation: so this is the unit \u2013 some APIs, like NewRelic use this unit for scaling and transforming/normalising the data \u2013 is that the same for Heka? Can Heka unify two different values, e.g. KiB/s and B/s into B/s? Should it be KiB s-1 or KiB/s? Can Heka understand counters/events versus rates versus rollups/aggregates/reservoir samples?\n. When it comes to semantic logging without losing the structure of the data; these are good reads:\n- https://github.com/influxdb/influxdb/issues/2102\n- http://metrics20.org/spec/\n. Yup:\n```\nbrew info lua\nlua: stable 5.2.3 (bottled)\nPowerful, lightweight programming language\nhttp://www.lua.org/\n/usr/local/Cellar/lua/5.1.5 (15 files, 276K)\n  Poured from bottle\n/usr/local/Cellar/lua/5.2.3_1 (13 files, 308K)\n  Poured from bottle\n/usr/local/Cellar/lua/5.2.3_2 (77 files, 1.0M) *\n  Poured from bottle\nFrom: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/lua.rb\n==> Options\n--universal\n    Build a universal binary\n--with-completion\n    Enables advanced readline support\n--without-luarocks\n    Don't build with Luarocks support embedded\n--without-sigaction\n    Revert to ANSI signal instead of improved POSIX sigaction\n==> Caveats\nPlease be aware due to the way Luarocks is designed any binaries installed\nvia Luarocks-5.2 AND 5.1 will overwrite each other in /usr/local/bin.\nThis is, for now, unavoidable. If this is troublesome for you, you can build\nrocks with the --tree= command to a special, non-conflicting location and\nthen add that to your $PATH.\nIf you have existing Rocks trees in $HOME, you will need to migrate them to the new\nlocation manually. You will only have to do this once.\n```\nLatest OS X 10.4.4, clang:\nclang --version\nApple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\nTarget: x86_64-apple-darwin14.4.0\nThread model: posix\nheka: master. Could build a few commits ago.\n. ",
    "jkpol": "Thanks,get it.\n. ",
    "aensidhe": "Ok, I had to install some go packages, old versions of some of them:\n```\n$ go get code.google.com/p/gogoprotobuf/proto\n$ go get code.google.com/p/gogoprotobuf/gogoproto\n$ go get code.google.com/p/gogoprotobuf/protoc-gen-gogo\n$ go get github.com/rafrombrc/gomock/mockgen\n$ go get github.com/mozilla-services/heka/cmd/hekad\ngithub.com/mozilla-services/heka/message\nC:\\Users\\popov_a\\Gopath\\src\\github.com\\mozilla-services\\heka\\message\\message_matcher.go:21: undefined: tree\n```\nIt seems that this is an error, because mingw32-make package fails with the same message:\n```\nC:\\Users\\popov_a\\Source\\Repos\\misc\\heka\\build>mingw32-make package\n[  4%] Built target gogoprotobuf\n[  9%] Built target raw\n[ 13%] Built target slices\n[ 18%] Built target sets\n[ 22%] Built target go-ircevent\n[ 26%] Built target gospec\n[ 30%] Built target go-simplejson\n[ 35%] Built target goamz\n[ 40%] Built target g2s\n[ 45%] Built target xmlpath\n[ 50%] Built target go-notify\n[ 54%] Built target go-uuid\n[ 59%] Built target toml\n[ 63%] Built target snappy-go\n[ 67%] Built target amqp\n[ 71%] Built target sarama\n[ 75%] Built target whisper-go\n[ 80%] Built target gomock\n[ 80%] Built target GoPackages\n[ 84%] Built target lua_sandbox\nInstall dll's for the mock generation and unit tests\n[ 85%] Built target heka_source\n[ 85%] Built target message_matcher_parser\n[100%] Built target mocks\ngithub.com/mozilla-services/heka/message\n..........\\Gopath\\src\\github.com\\mozilla-services\\heka\\message\\message_matcher.go:21: undefined: tree\nCMakeFiles\\hekad.dir\\build.make:56: recipe for target 'CMakeFiles/hekad' failed\nmingw32-make.EXE[2]:  [CMakeFiles/hekad] Error 2\nCMakeFiles\\Makefile2:1530: recipe for target 'CMakeFiles/hekad.dir/all' failed\nmingw32-make.EXE[1]:  [CMakeFiles/hekad.dir/all] Error 2\nMakefile:159: recipe for target 'all' failed\nmingw32-make.EXE: *** [all] Error 2\n```\n. I'm sorry for non-answering. We discarded windows builds of heka. Our logs on windows are parsed now by heka in docker container. So, I close issue now.\nThanks for help to anyone.\n. ",
    "runner-mei": "update your cmake to newest.\n. i try it on win32, it already crash with lua_sandbox. don't run it on win32\n. ",
    "hblanks": "@haf  By any chance do you have an existing installation of lua or luajit on your machine? Also, what version of OS X, clang, and heka are you operating on?\n. OK! This went away after running brew unlink lua luajit and wiping out the build directory. Sorry for the mis-filing. I thought I'd already checked this, but had not.\n. ",
    "dkolli": "Thank you Rob for taking time to review these decoders and provide your feedback. I'll make all suggested code and documentation changes asap.\n. I just committed all changes to the decoders and the docs. I hope i didn't miss any that we discussed above. \n. There are some fields like substatus are unique to the IIS logs. Looks like status field is a match. There are other fields like remote ip etc. aren't matching on names at this point. I'll take a look at field by field and try to map the field names with nginx format. I'm assuming this what you are expecting by matching this with nginx/apache, correct?\n. No, it can be local. Will change it.\n. ",
    "damz": "Not a very big annoyance, but a bug nonetheless, especially if you want to be included in distributions down the road. What about leaving this issue open and looking into fixing it?\n. ",
    "looatgb": "More info for debugging\nSorted ASC by filename\n| Filename | Found? | Last Modified |\n| --- | --- | --- |\n| ACME-01-NYC_Main.log-20150419 | Yes | 2015-06-24 17:12 |\n| ACME-01-NYC_Main.log-20150426 | Yes | 2015-06-24 17:11 |\n| ACME-01-NYC_Main.log-20150503 | No | 2015-05-10 03:46 |\n| ACME-01-NYC_Main.log-20150510 | Yes | 2015-06-24 17:13 |\n| ACME-01-NYC_Main.log-20150517 | No | 2015-05-24 03:16 |\n| ACME-01-NYC_Main.log-20150524 | Yes | 2015-06-25 16:21 |\n| ACME-01-NYC_Main.log-20150531 | Yes | 2015-06-25 16:21 |\nIt seems like all \"Error\" logs (which are \"invisible\" have the last modification date in 2015-04 or 2015-05). I was thinking this might somehow be related to last Leap second, but it happened on June 30th (and files from 2015-06-24 were found, so its probably not related), however a clear pattern emerges - files last modified approximately a month ago (or earlier) are not found.\n. Thanks. I don't know how I could have missed that. I looked at that doc many times, yet somehow this parameter never made it into my brain.\n. ",
    "Tripleray": "@rafrombrc, Works for me as well with v0.10.0b0.\n. ",
    "aleksmm": "confirm here.  Centos 6.7, after few minutes after launch hekad starts to use 5-10% of CPU and then  more when idle. Hard to debug because its multi-threaded (?)\ndead simple configuration:\n[DashboardOutput]\nticker_interval = 30\n[Auditd]\ntype = \"LogstreamerInput\"\nlog_directory = \"/var/log/audit\"\nfile_match = 'audit.log'\n[ProtobufEncoder]\n[AuditKafkaOutput]\ntype = \"KafkaOutput\"\nmessage_matcher = \"Logger == 'Auditd'\"\ntopic = \"audit\"\naddrs = [\"localhost:9092\"]\nencoder = \"ProtobufEncoder\"\n. in my config in #1626 I have single log file in logstreamer config to listen to and still have same issue. If this matters.\n. ",
    "AlexTsr": "I can confirm that the issue exists and that the above fix solves it for 0.10.0b1\n. ",
    "i19": "The configuration should be like this\n[StatAccumInput]\npercent_threshold = [90, 95, 99]\ndelete_idle_stats = false\nticker_interval = 10\n. The docs has been updated, please review . \n. That's a pity. \nCould you show me the testing result , or testing case, or testing method ?\nI didn't find the document about running the testing in the doc . \n. I checked the testing case . The error is caused by the default value i set for the percentValue list .\nI removed it !\nAnd in the dev branch, when calculate the percent value, it has an logic that would judge if count of the timing data is bigger than 1, but if i have multi percent_threshold settings, then it would be unreasonable, and it would be conflict to handle that case. So i remove it too.\n. ",
    "tclh123": ":beers: \n. I need this feature too. related #1482\ncc @rafrombrc \n. @guluglu should be fixed in #1838\n. According to SeekInFile\uff0cmaybe I should deal with gzipped file in SetToTail too.\n. ",
    "jemc": "@trink - We are sending these logs through an ESJsonEncoder with index = \"%{Type}-%{20060102}\" to an ElasticSearchOutput, but Elasticsearch complains when index names have the . character.  For us, changing the message Type fromnginx.error to nginx-error is necessary.\nThis patch has a minimal solution to an actual problem, is consistent with the nginx_access decoder, and doesn't impact current behavior for any users.\n. Thanks, @rafrombrc!\n. ",
    "mwildehahn": "I understood what the error was, but it seems like go 1.3.1 is being installed here:\nhttps://github.com/mozilla-services/heka/blob/dev/Dockerfile#L24\n. ",
    "wolkykim": "We're also getting a similar error. From the caller function, ' NextRecord()', it seems getFileFromId() returns non-nil where there's no more next files to process. It tries to open \"0.log\". This happens occasionally. And I'm looking for the root cause of this.\nnextReadFile, nextFileId, err := br.getFileFromId(br.readId + 1)\n                    if err != nil {\n                            return fmt.Errorf(\"can't lookup subsequent file: %s\", err)\nAny updates on this plz?\n. Thanks. I'll try it and let you know the result.\n. No this fix doesn't work. It worked for a while but it crashed again with same error looking for 0.log file.\n. ",
    "relistan": "I implemented this in #1843. It throws out a lot of the logspout code which wasn't buying much now that we have splitters and just feeds the pipes right to the splitter. It supports multiple splitter goroutines (1 per pipe), so it should perform pretty well.\n. I'm totally open to naming it something else, that's not an issue for me at all. I didn't want to call it StacktraceSplitter because it's actually good for most multi-line things as far as I can tell. It works a lot like, but not exactly like, multi-line grouping in other logging frameworks.\nAs you say the only other option for this is the RegexSplitter but:\n1. It can only match on the beginning or end of a line which is not good enough in many cases to catch something that should be grouped multi-line.\n2. It usually misses the first line in a stacktrace, the one with the error message,\n3. It doesn't work easily with lots of stacktraces, e.g. stacktraces that get emitted without a leading space on each line (or some other leading pattern), because:\n4. It has a limitation of a single capture group, which I assume is to simplify the code and for performance reasons: https://github.com/mozilla-services/heka/blob/dev/pipeline/splitters.go#L125 . It would be more usable for this purpose without that limitation. I was not enthusiastic about changing the behavior of that splitter.\nDo you have specific reservations that I might be able to address?\n. Thanks Rob, sounds reasonable. I could take a stab at a combined splitter if that's your preference. It's easy enough to have some configuration options that support everything the RegexSplitter does now (and rename this splitter to RegexSplitter). From a separation of concerns standpoint this separate splitter seemed like the right thing to do, but maybe from a user confusion standpoint of view combining them is better. I probably won't have time until early next week to work on it, though.\nBTW saw your presentation on Heka at the first GopherCon, which is how we ended up running it at New Relic for the last year and a half or so. :+1: \n. Thanks for the explanation. But, I've been through this code a bunch and I guess I'm just totally not understanding this. I don't see how you can capture full stacktraces that are intermixed with other logs. I did say \"line\" but I'm referring to getting a stacktrace into a single record, while not also messing up all the other logging that isn't stacktraces.\nIf you wouldn't mind, please take a look at the tests I wrote for the MultilineSplitter. What can I feed to the RegexSplitter that will capture all of that? And also work for non-stacktrace lines like in the tests? Are you guys doing this? If so, what expression are you using? Googling many times before, and again now, was unsuccessful in finding anyone doing this publicly.\nAs far as I can tell, the code is only going to capture the leftmost match using FindSubmatchIndex and there is no for loop. And it always returns a record unless it didn't match at all, in which case it also returns 0 bytes read. Doesn't that mean I can only ever match the first match?\nI'm good with regexes, but maybe I'm just missing something that makes this all just work.\nHere's a Golang Playground demonstrating some attempts to make a match work with FindSubMatchIndex against one of the test cases.\n. Ah, that explains the disconnect! Glad we're on the same page. Thanks!\n. Any further thoughts on this, @rafrombrc ?\n. Excellent. I don't currently have the time allocated to update the other two splitters, but I'd be willing to do it on a future pass and will plan to allocate that time in the next project. As for naming... this name really comes from Logstash and the way it handles multiline stuff. So I think it probably makes sense to people from that arena. But it should not just make sense to some people, it needs to not confuse anyone! How about something like PatternGroupingSplitter?\nI'll update the docs and fix the above commented sections and push, probably early next week.\n. Code is updated, will work on docs.\n. @rafrombrc docs are added, ready for review.\n. @rafrombrc I know you don't have much time for this these days (been there with my own open source projects). Would very much appreciate a review if you can get the time.\n. I should have pointed out that there is one other piece of new functionality here: this supports additional fields being pulled from container labels. These were added in Docker 1.6 and are the best practice way to store metadata about containers. The config directive is called fields_from_labels and it works basically identically to fields_from_env but using labels as the source.\n. Thanks, Rob. Yeah I had the same thought about the logging position. Right now you just miss whatever logs went by while Heka wasn't listening. Being able to capture them all would be ideal. Is there any existing state mechanism for Heka plugins (i.e. between Heka restarts)?  I think I'll not tackle that right now, but will think it over because I think it's important. I have another project going at the moment, but when it's complete I'll be back to this. Likely Monday, but maybe tomorrow if I'm quick. ;)\n. @rafrombrc Ok, this is ready for re-review. I took into account all your suggestions, and I went through all the logging to see where and when messages were being generated. I removed some calls and added a couple of others where it was falling short. I added the changes to CHANGES.txt as requested as well.\n. @rafrombrc thanks!\n. :shipit: \n. This is a nice addition in functionality, it will be good to get this into merge-able state!\n. @gjtempleton :) I'm just a contributor not a committer, though, so @rafrombrc or @trink will have the final call on merge-ability. :)\n. Good catch, thanks!\n. @nickchappell can you summarize? :+1: \n. I can't quite understand what Travis is failing on here.\n. Clearly something stale in my build environment as I'm now unable to reproduce this issue from dev. Closing because it does seem to be working indeed, and I am not keen on sorting out what Travis is unhappy about. Sorry 'bout that.\n. Hmm, just hit this again. Something is weird in the build chain:\n[100%] Built target mocks\nheka/src/github.com/mozilla-services/heka/plugins/kafka/kafka_input.go:28:2: cannot find package \"github.com/Shopify/sarama\" in any of:\n    /usr/local/go/src/github.com/Shopify/sarama (from $GOROOT)\n    /home/kmatthias/heka/build/heka/src/github.com/Shopify/sarama (from $GOPATH)\nmake[2]: *** [CMakeFiles/hekad] Error 1\nmake[1]: *** [CMakeFiles/hekad.dir/all] Error 2\nmake: *** [all] Error 2\nThere appears to be some kind of ordering issue that occasionally rears its head.\n. \ud83d\udc4d \n. Great, hadn't seen anything using that so used a pattern I've used a lot. I'll use the RetryHelper instead.\n. Yeah, good call. It looks like when I refactored things to delegate Run to the AttachManager it now logs this twice. Will fix.\n. Sure.\n. Well, the reason that comment is there is that RemoveEventListener does (sometimes) also close it. Experience showed that it always closed the channel. But in double checking this I just discovered that it only closes it if there are no more event listeners, otherwise it appears to leave it open from what I can tell. I actually need to check that here so we don't leak open channels.\n. You are correct about how it works. This should be returning the error and then breaking out of the loop in handleDockerEvents though. That would cause the plugin to exit, which is what I think we want it to do here.\n. That's reasonable. There was no error logging before, so hey, it's better than that. I'll take a pass through it and consider where it might be duplicated.\n. Hmm, yeah that's rather nasty. I think this is actually a bug in the go-dockerclient because it should either do one or the other but not sometimes one and sometimes the other. I'll work around it here, but I'll open a bug there.\n. Done.\n. Doesn't look to me like this needs to be a method on AttachManager\n. This should use the same connect code as the other two plugins here. There's a method in attach_manager.go that does what you want.\n. As far as I can tell this is only ever read from, but nothing puts values into it.\n. Where would error this happen?\n. This message looks wrong and I'm not sure if this condition is reachable?\n. I think all the logic from here down doesn't apply. And, I think you'll actually end up with some pretty weird error messages here if you fall into this block with an error value set at line 119, for example.\n. Are the %#v or %v values really not good enough here?\n. I feel like the filename should probably be something like stats_handling.go or something along those lines since it sounds like a type name, but then the file doesn't have a type by that name.\n. this looks like a complete copy-paste of the code in AttachManager. The better solution is to not make that a type method and you can just use it here.\n. This all should really be factored into something that can be shared between the plugins here rather than all copy-pasted.\n. Is this still true?\n. Yeah, there's getting to be enough shared code here that some of this should be just another file with those shared methods in it. When that gets unwieldy then it could eventually be broken out more.\n. I was suggesting that you can probably let fmt do some of this for you. e.g. https://play.golang.org/p/wD-cGK-n2g\n. Yeah, agreed. I called it out in the description of the PR, but it's easy enough to make it configurable.\n. Hmm, good catch. I think that's a leftover from some code that was in the RegexSplitter, which has this restriction. The reason I don't allow you to configure whether or not you can keep the delimiter is that it will always keep all the intervening delimiters\u2013it doesn't make sense otherwise. But I suppose it might still make sense to configure whether or not you keep the last one. I'll just remove the restriction.\n. ",
    "salekseev": "@carlanton this looks great. My only concern is that you have this specifically written for docker_fluentd.lua while most fluent messages are the same (1: string tag 2: long? time 3: object record, from https://github.com/fluent/fluentd/blob/master/lib/fluent/plugin/in_forward.rb). It would be great to have a generic fluent format decoder as other projects use this as well. For example https://github.com/saltstack/salt/blob/develop/salt/log/handlers/fluent_mod.py.\n. @carlanton I'm loving https://github.com/carlanton/heka/commit/930b2d8a73411ab4973833e37a24bbd219d26c3a implementation. But a generic fluent/messagepack decoder in Lua is also very useful as people could use brokers (like Kafka) to connect fluentd and hekad for example where they wouldn't talk directly to each other. Great work. :+1:\n. ",
    "arnimarj": "I wondered about that as well. My expectation was that the whitelist field should ignore it.\n. ",
    "rvzanten": "Iam using jessie.  And it removes the entire container where it builds after the build fails or succeeds so i guess if i try again it will start with a clean environment.\nWell maybe the the difference is that i call a custom build.sh. Though, it doesn't differ that much from the original heka build.sh.\nThis build.sh looks like this:\n```\n!/usr/bin/env bash\nset up our environment\n. ./env.sh\nNUM_JOBS=${NUM_JOBS:-1}\nbuild heka\nmkdir -p $BUILD_DIR\ncd $BUILD_DIR\ncmake -DCMAKE_BUILD_TYPE=release ..\nmake -j $NUM_JOBS deb\nmv -v *.deb $HOME/\n```\nLet me know what you think.\n. Also, when i try to build 0.10.0b1 manually on a local enviorment using the same custom build script defined above; it also doesnt work and triggers the same error i get when doing it via Docker.\nI am gonna try to build it with the original build.sh now.\nEdit: So yes, then it builds alright. But why does it not when adding \"deb\" as an extra parameter to make? This used to work in v0.9.0. And should be supported in v0.10.0.\n. Yes that works. Thank you very much @sathieu .\n. ",
    "s00": "@DanyC97 I did not.\nI was trying Trink's first solution, but keep_truncated=true produces multiple number of records from big one, which is not what i'm expecting. May be i do something wrong.\nAnother solutions are not appropriate to me.\nIf you will find a way, please share your method with me:)\nGood luck!\n. ",
    "andremedeiros": "Actually, you can do message matching with Fields, as described here\n. @rafrombrc this is actually good news. We've just released 1.6, which has some CRC32 performance increases.\nShall I get this branch up to date with Sarama 1.6? I wouldn't mind taking the \"burden\" of keeping that up to date either.\n. Interesting.\nI've already rebased this branch with the latest dev and Sarama 1.6. I actually noticed some improvements on performance. \nI'll touch base with the maintainers to see if we can make those public again. In the meantime, should I send you PRs for this?\n. I'll figure something out! :heart: \n. ",
    "gigaroby": "Yes, but I would need to match directly after the Input, before the message is dispatched to a Decoder and, as far as I know, there is no way to do that...\n. ",
    "ChristianKniep": "It might expect the message to be protobuf, but that's not quite clear to me.\n. Just to close the issue with the HelloTCP config:\n```\ncat /etc/heka/hekad.toml\n[hekad]\nmaxprocs = 2\n[TcpInput]\naddress = \":5514\"\nsplitter = \"newline_splitter\"\ndecoder = \"newline_decoder\"\n[newline_splitter]\ntype = \"TokenSplitter\"\ndelimiter = '\\n'\n[newline_decoder]\ntype = \"ScribbleDecoder\"\n    [better_decoder.message_fields]\n    Type = \"MyType\"\n[PayloadEncoder]\nappend_newlines = true\n[LogOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"PayloadEncoder\"\n```\n. I cloned and built your branch, but I am still missing the cjson.(lua|so) files at the right place.\nCould you add this piece to the documentation?\n. @Kenuat Have you found a workaround how to flush stats (CPU/Memory) to carbon?\n. @Kenuat Any progress on this front or do you still use diamond? :)\n. Thanks Rob,\nI will give it a try and report back.\nThanks a bunch for this extended answer, I appreciate it.\nCheers from Berlin\nChristian\n\nOn 31 Aug 2015, at 21:00, Rob Miller notifications@github.com wrote:\nIn sandbox/lua/decoders/json.lua https://github.com/mozilla-services/heka/pull/1687#discussion_r38346522:\n\n\nif F == \"Timestamp\" then\nmsg[F] = json[f] * 1e9\nelse\nmsg[F] = json[f]\nend\njson[f] = nil\nend\nend\nend\n  +\n-- flatten and assign remaining fields to heka fields\nlocal flat = {}\nutil.table_to_fields(json, flat, nil)\nmsg.Fields = flat\n  +\nif not pcall(inject_message, msg) then return -1 end\n  Another place we can return an error message.\n\n\n\u2014\nReply to this email directly or view it on GitHub https://github.com/mozilla-services/heka/pull/1687/files#r38346522.\n. \n",
    "drsnyder": "Hi Rob, Yes, I discovered that output today. There was a termination report indicating that aggregate_gateway_stats.lua failed with an error.\nI also discovered the can_exit setting. I'm testing setting it to false in the NginxStatsAggregator block above to see if that fixes it. It's hasn't failed since I made that change so I don't know if that will fix it or not.\nSorry :blush:, this may have been better suited to the mailing list.\n. Well we decided to try this (setting can_exit to false) and allow upstart to restart heka when this happens. It's not ideal but until restart support is added to the sandbox plugins I'm not sure what else we can do. Do you have any suggestions?\n. Done! See #1692. \nThanks!\n. > Sandbox filters should never be restarted after failure.\nCould this be a configuration option? Something like can_restart that defaults to false.\nIn our case the plugin is not experimental. We are using it to gather stats in production. We would like to add this feature to make the monitoring more robust. The concern regarding constant thrashing is valid. In that case someone who uses this feature should use caution. \n. > Is there a reason the code producing the error cannot be fixed?\nIt can be fixed. To me it's a bug to have monitoring stop completely with no means to restart it other than stopping and restarting heka.\n. ",
    "jverhoeven": "I have exactly the same issue and am mostly surprised by how so many log frameworks (also syslog, fluentd) neglect the millisecond range.\n %L, %3 and %6 seem all unusable due to the choice of this library: http://godoc.org/github.com/cactus/gostrftime\nCan we move this forward by getting millisecond into the cactus implementation or does the Heka team support moving to another lib as suggested in the bottom post in https://github.com/mozilla-services/heka/issues/1508?\n. ",
    "kevpie": "I will try to move forward with this.\n. ",
    "aodj": "Is there any chance of getting in a release soon? This is a really handy feature to have for those of us forwarding to ElasticSearch.\n. ",
    "d-shi": "+1\nWe are also using sandbox filters for production, not experimental code. It is definitely not acceptable for them to just exit. The errors are few and far between, maybe one every couple weeks, out of parsing millions of log lines per day, so it is unlikely we will run into the thrashing problem.\n. ",
    "penhauer-xiao": "My default.josn as below:\n{\n  \"template\": \"logstash-default_\",\n  \"settings\" : {\n    \"number_of_shards\" : 2,\n    \"number_of_replicas\" : 0,\n    \"index\" : {\n      \"query\" : { \"default_field\" : \"message\" },\n      \"store\" : { \"compress\" : { \"stored\" : true, \"tv\": true } }\n    }\n  },\n  \"mappings\": {\n    \"default\": {\n      \"all\": { \"enabled\": false },\n      \"_source\": { \"compress\": true },\n      \"dynamic_templates\": [\n        {\n          \"string_template\" : {\n            \"match\" : \"\",\n            \"mapping\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n            \"match_mapping_type\" : \"string\"\n          }\n        }\n      ],\n      \"properties\" : {\n        \"@fields\": { \"type\": \"object\", \"dynamic\": true, \"path\": \"full\" },\n        \"@timestamp\" : { \"type\" : \"date\", \"index\" : \"not_analyzed\" },\n        \"type\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n        \"level\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n        \"host\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n        \"path\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n        \"message\" : { \"type\" : \"string\", \"index\" : \"analyzed\" },\n        \"apath\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n        \"bpath\" : { \"type\" : \"string\", \"index\" : \"analyzed\" },\n        \"ua\" : {\n          \"properties\" : {\n            \"name\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"os\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"os_name\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"device\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" }\n          }\n        },\n        \"pos\" : {\n          \"properties\" : {\n            \"pkg\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"file\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"func\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" },\n            \"line\" : { \"type\" : \"long\", \"index\" : \"not_analyzed\" }\n          }\n        }\n      }\n    }\n  }\n}\n. Sorry, I like get through heka to create es-template\n. oh, I see, thanks\n. rafrombrc  said the output plugin \"owns\" the pack, so recycling should happen in the output, but output plugin is elasticsearch\uff0c mean the elasticsearch plugin never being freed up ? But the official plugin elasticsearch that means it has a bug?\n. from client to service by heka\nOn Fri, Sep 23, 2016 at 4:01 PM, SeeWei1986 notifications@github.com\nwrote:\n\nClosed #1987 https://github.com/mozilla-services/heka/issues/1987.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/mozilla-services/heka/issues/1987#event-800028026,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF_Tucqg_QAQFFquLzmRvrcVdaFzaCx4ks5qs4dmgaJpZM4KErCV\n.\n. \n",
    "ericx10ng": "try to use pprof to see what your goroutines are doing.\n. ",
    "Kenuat": "@ChristianKniep The working examples are in:\nlua_encoders/schema_influx_line.lua\nlua_encoders/schema_carbon_line.lua\nfiles. Howerver influx encoder script doesn't return *MinAvg values for me. I'm going to switch to the diamond agent for collecting the data from hosts, since it's looks much more mature atm.\n. ",
    "0xdabbad00": "Any update on this now that we have the 0.10.0 release?\n. ",
    "torbiak": "@rafrombrc: What is the cgo issue in question?\n. @wangfeiping: https://github.com/mozilla-services/heka/issues/1818#issue-123720748\nThe easiest workaround is to put patch in your PATH.\n. ",
    "johnewing1": "+1 \nHigh level consumer support would be really useful\n. ",
    "elemoine": "There is no such thing as \"high level consumer\" in Kafka 0.9.0.\n. Good catch. Looks good to me :)\n. This is the sort of local patch we're going to use for now: https://github.com/mozilla-services/heka/compare/v0.10.0...elemoine:ratelimit (based on the v0.10.0 tag).\n. Yeah, I needed a quick fix. I'll see if I can come up with a proper patch. \n. You need to provide a test case demonstrating the issue. We use LogstreamerInput plugins and rely on Heka's journaling functionality, and we've had no issues with it. So the problem is probably related to the specific configuration you use.\n. > Might be good to summarize our conversation on IRC a few days ago re: Kafka and Heka\nYes, please do :)\n. ",
    "peternga": "Is it possible to include a configuration parameter to specify tag fields to skip? I've made a change that uses skip_fields to affect tag_fields but I think a cleaner way to approach this is to simply have another parameter that only affects tag_fields.\n. ",
    "resilva87": "Got the same error when running ctest. \nGolang is go1.5.2 linux/amd64\nMy complete output:\n```\nTest project /home/renato/Workspace/heka/build\n      Start  1: cmd/hekad\n 1/26 Test  #1: cmd/hekad ........................   Passed    2.34 sec\n      Start  2: message\n 2/26 Test  #2: message ..........................   Passed    1.08 sec\n      Start  3: pipeline\n 3/26 Test  #3: pipeline .........................   Passed    2.86 sec\n      Start  4: plugins\n 4/26 Test  #4: plugins ..........................   Passed    1.92 sec\n      Start  5: plugins/amqp\n 5/26 Test  #5: plugins/amqp .....................   Passed    1.86 sec\n      Start  6: plugins/dasher\n 6/26 Test  #6: plugins/dasher ...................   Passed    1.77 sec\n      Start  7: plugins/elasticsearch\n 7/26 Test  #7: plugins/elasticsearch ............   Passed    1.67 sec\n      Start  8: plugins/file\n 8/26 Test  #8: plugins/file .....................   Passed    2.29 sec\n      Start  9: plugins/graphite\n 9/26 Test  #9: plugins/graphite .................   Passed    1.95 sec\n      Start 10: plugins/http\n10/26 Test #10: plugins/http .....................***Failed    1.91 sec\ngithub.com/mozilla-services/heka/plugins/http.HttpOutputSpec\n  - An HttpOutput\n    - that is started\n      - honors http timeout interval [FAIL]\n*** Expected: is \n         got: \u201cfalse\u201d\n    github.com/mozilla-services/heka/plugins/http.HttpOutputSpec.func2.2.8()\n        at /home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka/plugins/http/http_output_test.go:239\n29 specs, 1 failures\n--- FAIL: TestAllSpecs (0.07s)\nFAIL\nFAIL    github.com/mozilla-services/heka/plugins/http   0.075s\n  Start 11: plugins/irc\n\n11/26 Test #11: plugins/irc ......................Failed    1.72 sec\n2016/01/27 00:38:55 Missing EXPECT() for invoked function: pipelinemock.MockOutputRunner.LogError([Dropped message. OutQueue is full.])\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gomock/gomock/controller.go:116 (0x5a4031)\n    (Controller).Call: stack := debug.Stack()\n/home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipelinemock/mock_outputrunner.go:96 (0x737da6)\n    (MockOutputRunner).LogError: _m.ctrl.Call(_m, \"LogError\", _param0)\n/home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka/plugins/irc/irc_output.go:187 (0x4863d4)\n    (*IrcOutput).Run: output.runner.LogError(ErrOutQueueFull)\n/home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka/plugins/irc/irc_output_test.go:89 (0x48b503)\n    IrcOutputSpec.func1.2.1.1: err := ircOutput.Run(outTestHelper.MockOutputRunner, outTestHelper.MockHelper)\n/usr/local/go/src/runtime/asm_amd64.s:1721 (0x45f311)\nFAIL    github.com/mozilla-services/heka/plugins/irc    0.004s\n  Start 12: plugins/kafka\n\n12/26 Test #12: plugins/kafka ....................   Passed    1.84 sec\n      Start 13: plugins/logstreamer\n13/26 Test #13: plugins/logstreamer ..............   Passed    1.65 sec\n      Start 14: plugins/nagios\n14/26 Test #14: plugins/nagios ...................   Passed    1.55 sec\n      Start 15: plugins/payload\n15/26 Test #15: plugins/payload ..................   Passed    1.47 sec\n      Start 16: plugins/process\n16/26 Test #16: plugins/process ..................   Passed    2.56 sec\n      Start 17: plugins/smtp\n17/26 Test #17: plugins/smtp .....................   Passed    1.68 sec\n      Start 18: plugins/statsd\n18/26 Test #18: plugins/statsd ...................   Passed    1.69 sec\n      Start 19: plugins/tcp\n19/26 Test #19: plugins/tcp ......................   Passed    1.77 sec\n      Start 20: plugins/udp\n20/26 Test #20: plugins/udp ......................   Passed    1.69 sec\n      Start 21: logstreamer\n21/26 Test #21: logstreamer ......................   Passed    0.85 sec\n      Start 22: client\n22/26 Test #22: client ...........................   Passed    0.72 sec\n      Start 23: sandbox_move_modules\n23/26 Test #23: sandbox_move_modules .............   Passed    0.01 sec\n      Start 24: sandbox\n24/26 Test #24: sandbox ..........................   Passed    1.18 sec\n      Start 25: sandbox_plugins\n25/26 Test #25: sandbox_plugins ..................   Passed    2.60 sec\n      Start 26: mozsvc\n26/26 Test #26: mozsvc ...........................Failed    2.75 sec\n2016/01/27 00:39:19 missing call(s) to testsupport.MockAWSService.Query(is anything, is anything, is anything)\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gomock/gomock/controller.go:164 (0x657d9d)\n    (Controller).Finish: string(debug.Stack()))\n/home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka-mozsvc-plugins/cloudwatch_test.go:235 (0x48e4b3)\n    CloudwatchInputSpec: }\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 (0x60f715)\n    (Runner).execute.func1: c.Specify(name, func() { closure(c) })\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/recover.go:33 (0x60aa66)\n    recoverOnPanic: f()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/specification.go:39 (0x60e859)\n    (specRun).execute: exception := recoverOnPanic(spec.closure)\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:90 (0x6064f3)\n    (taskContext).execute: spec.execute()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:67 (0x606271)\n    (taskContext).processCurrentSpec: c.execute(spec)\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/context.go:54 (0x60611d)\n    (taskContext).Specify: c.processCurrentSpec()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:103 (0x60d951)\n    (Runner).execute: c.Specify(name, func() { closure(c) })\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:82 (0x60d740)\n    (Runner).startNextScheduledTask: r.results <- r.execute(task.name, task.closure, task.context)\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:55 (0x60d5b0)\n    (Runner).startAllScheduledTasks: r.startNextScheduledTask()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/runner.go:47 (0x60d551)\n    (Runner).Run: r.startAllScheduledTasks()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/main.go:54 (0x607905)\n    runAndPrint: runner.Run()\n/home/renato/Workspace/heka/build/heka/src/github.com/rafrombrc/gospec/src/gospec/main.go:39 (0x6076f1)\n    MainGoTest: results := runAndPrint(runner)\n/home/renato/Workspace/heka/build/heka/src/github.com/mozilla-services/heka-mozsvc-plugins/all_specs_test.go:44 (0x48d877)\n    TestAllSpecs: gospec.MainGoTest(r, t)\n/usr/local/go/src/testing/testing.go:456 (0x47f0d8)\n/usr/local/go/src/runtime/asm_amd64.s:1721 (0x45f2b1)\n2016/01/27 00:39:19 aborting test due to missing call(s)\nFAIL    github.com/mozilla-services/heka-mozsvc-plugins 0.009s\n88% tests passed, 3 tests failed out of 26\nTotal Test time (real) =  45.38 sec\nThe following tests FAILED:\n     10 - plugins/http (Failed)\n     11 - plugins/irc (Failed)\n     26 - mozsvc (Failed)\nErrors while running CTest\n```\n. ",
    "TylerTemp": "Same here.\nDebian 3.2.81-1 x86_64\ngo version go1.6 linux/amd64\nheka source code at git commit e8799385d16915a80b69061d05542da6342e58e4; Fri Jun 10 20:15:12 2016 -0700\n. ",
    "frison": "I haven't the faintest clue why Travis failed on this guy. All I know is that it fixed issues we were seeing on our windows machines. I'll dig into it soon, any guidance would be appreciated.\n. ",
    "ZeusF1": "Bug still present, I was fixing it by increasing buffer_size\nError from logs, after start\nruns currently only apache log sending to elastic.\n2015/11/17 10:07:14 Plugin 'ElasticSearchOutput' error: can't deliver matched message: Queue is full\n(10-15 more same msgs)\nversion 10.0b1\n. Related to /lua_modules/common_log_format.lua\nmodifyed file was not in rpm package\nWas fixed in\nhttps://github.com/mozilla-services/lua_sandbox/commit/ecfd265d6f17fb9cc09de3a3c47c8a85520ab794\nClosed.\n. ",
    "feelobot": ":+1: \n. ",
    "gootik": ":+1:\n. ",
    "aafrooze": "This would be great actually :+1: \n. ",
    "klocekPL": ":+1:\n. ",
    "rafaelbarreto87": ":+1: \n. ",
    "huhongbo": "+1 \n. ",
    "vlastv": "I can fix grammar, but then need to correct all decoders apache and nginx.\nDo it?\n. Grammar is described in a different repository, respectively, will be broken backward compatibility\n. If the grammar should not be parsed into the heka schema then the PR is complete.\nOtherwise, I have to change the grammar:\n- time -> Timestamp\n- pid -> Pid\nChange the grammar?\n. The only thing that I could not overcome, is that she is not sent to the last record until the new last.\n. ",
    "zstyblik": "Just to make sure, AMQP output is defined as:\n```\n[AMQPOutput]\nmessage_matcher = \"TRUE\"\nencoder = \"json_payload\"\nurl = \"amqp://user:password@rmq.example.com/%2fvhost\"\nexchange = \"amq.topic\"\nexchange_type = \"topic\"\nexchange_durability = true\nexchange_auto_delete = false\nrouting_key = \"my.key\"\ncontent_type = \"application/json\"\nuse_framing = false\n[AMQPOutput.retries]\nmax_retries = -1\n```\n. @simonpasquier it might be possible that plugin restart is somewhat wrong. I'm sorry, but I'm new to heka and I've only quickly read through AMQP plugin in heka. I've seen nothing that would suggest URL is being manipulated with. However, it's not impossible. I suspect IP address is somehow used instead of URL, but I currently have no means to verify that(yes, I have tried to avoid compilation of heka as much as possible :) ). I'm sort of glad to see I'm not alone who's having problems.\n. facepalm I see. It still might be worth to add something for clarity ... for people like me. However, if you think it's not necessary, I agree with this issue being closed. Obviously, error was on my end.\nThank you.\nZ.\n. I see. I'm fairly sure I've tried that, however it only prolonged the time before the inevitable. I have added the following and it's still going after 6M processed messages.\noutput_limit = 10240000          \nmemory_limit = 10240000\nSo I guess you're right and with #1763 clarified and my eyes open, I guess I wasn't adding enough before.\nOne more thing, though. Please, can you explain to me why no error message has been printed? Is that expected?\nThanks,\nZ.\n. ",
    "pierce300": "+1\n. ",
    "samsieber": "+1 This bug is a showstopper for us where I work.\n. I think you might be looking for the share_dir config setting. From https://hekad.readthedocs.org/en/v0.10.0b2/config/index.html:\n\nshare_dir (string):\nRoot path of Heka\u2019s \u201cshare directory\u201d, where Heka will expect to find certain resources it needs to consume. The hekad process should have read- only access to this directory. Defaults to /usr/share/heka (or c:\\usr\\share\\heka on Windows).\n\nMy guess is the root issue is the the default isn't being set correctly / repsected? Since your running it on Windows you must be running an old version or compiled it yourself - I haven't tried compiling it on windows yet, so I'm not sure how to be of help.\nIssue #1704 is about Windows binaries. I'm keeping my fingers crossed.\n. ",
    "tianchaijz": "I got this error in CentOS 6.5 too.\n. @trink my gcc version info:\nUsing built-in specs.\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-languages=c,c++,objc,obj-c++,java,fortran,ada --enable-java-awt=gtk --disable-dssi --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-1.5.0.0/jre --enable-libgcj-multifile --enable-java-maintainer-mode --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --disable-libjava-multilib --with-ppl --with-cloog --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)\n. That works, thanks @trink !\n. I also encounter this problem, by printing the message which leads ES respond 500, I found some messages were truncated (or were overwritten) as showing:\n...\n{\"index\":{\"_index\":\"heka-foo-2015.11.16\",\"_type\":\"bar\"}}\n{\"json\":\"this json is ok\"}\non\":\"this json is truncated\"}\n...\n...\nfoo-2015.11.16\",\"_type\":\"bar\"}}\n...\nAfter inspecting heka's source code, I suspect it is the ProcessMessage function of ES plugin(slice append) which is not thread safe that leads this bug.\n. I'll open a PR which fixes this bug.\n. Hi Rob,\nThanks for your suggestions, I'll follow them. I haven't done any throughput comparisons, the monitor of ES shows that the indexing rate is double however, since there are no damaged bulks sent to ES.\n. Hi guluglu,\nAre your messages encoded in UTF-8? If not, they will be dropped by ES, it can be found in ES's log.\n. ",
    "pib": "I don't understand how this patch could have caused tests to not pass. io.ReadFull does the same thing as Reader.Read except it makes sure it fills the buffer.\n. Unfortunately,  on my local machine even more tests fail :D \nI'm guessing the Heka developers don't test on OSX too often?\nI can try it in a Linux VM later and see if it completes there.\nMy only guess as to why this would legitimately fail is that it may not return an EOF in some situations when a regular Read call would. But even then, the next read should result in an EOF being returned.\n. @rafrombrc Sure, I can do a PR against 0.10 instead. I've already made a release on the OwnLocal fork with the change and pointed our build system at that so the fix is available for us to use until we switch to 0.10 later.\n. ",
    "bob3000": "+1\n. ",
    "waqark3389": "@rafrombrc Any workaround to this? I would have to define around 500 file outputs if i define them in the config. would be nice if I can add %hostname%.log which would make things easier.\nEDIT: I wrote a sandbox output in lua to use read_message function to get the hostname. Can you see any problems with this, Will the output only be called on message? \n. ",
    "smalenfant": "I'm currently evaluating Heka. That's been a problem for me as well as I want to write file depending on some fields in the log line. Something supported in other log aggregation tools. \n. ",
    "yunkai": "Sorry, I misunderstand the unit of Message.Timestamp.\n. ",
    "dmvk": "Are there any news on this issue? Producers definitely should keep working when new partition leader gets elected on another broker after the crash...\n. ",
    "dmuth": "I too am having this problem.  I have a 7 node Kafka cluster with a replication factor of 3, yet if I shut down a single Kafka instance, I get this error.  This behavior happens whether I have one, a few, or all of the IPs of machines in my Kafka cluster listed in the configuration.\nThat's not the sort of behavior I expect when interfacing with a messaging system that has multiple replicas.\nShould we maybe file a bug with the auth of Sarama as well?\n-- Doug\n. I found a workaround.\nI ended up installing Apache Nifi (https://nifi.apache.org/) and I set up a web service to listen on it which then forwards to Kafka.  One thing Nifi does not do is read logfiles, which Heka is actually pretty good at.\nSo, to summarize, here is the workflow:\nLogfile(s) on disk -> Heka -> Webservice provided by Nifi -> Kafka\n(If anyone has questions about Nifi, feel free to hit me up)\nI feel a little frustrated about this whole thing, because Heka works fine in all other aspects.  But having this misbehaving library is really affecting the usefulness and overall quality of Heka. :-(\n. Oh, do I feel stupid!  This is my first time using Heka and I can't believe I made that typo.  Thanks for catching it. :-)\n. ",
    "dewrich": "I just hit this as well, seems like a big problem with resiliency.  Any other \"hacks\" to get around this is appreciated.\n. ",
    "bobrik": "Whoa, there is another bug tracker for heka.\n. ",
    "rohit01": "We are also facing this issue. Any luck?\n. ",
    "gjuric": "The latest, 0.10.0 Beta 2\n. Prebuilt package, 64bit.deb. I will double check the config tomorrow.\n. Sorry, my bad, I had \"$http_x_forwarded_for\" in the nginx.log so heka was processing a \"-\" instead of \"0.01\".\n. ",
    "tuziben": "Thanks  a lot.\nI got it done. It is great to use heka.\n. I got it down by cmake changed to cmake3.0\n. ",
    "logrusorgru": "I do not need to integrate them. All I need is the only one package.\n. Currently my plugin_name.lua is the independent file. I want put it into .deb-package. I build the Heka manually. There is a way to add (embed) Go-plugin to the .deb-pakage. What about lua plugins? Is it not a feature request?\n. Thanks a lot.\nYou should to add a CONTRIBUTING file (or CONTRIBUTING.md) as described here. And put your previous message into it.\n. ",
    "ei-grad": "Imho it would be better if Elasticsearch JSON Encoder would translate nested keys into nested JSON objects.\n. ",
    "roddik": "Any updates on this? This issue renders heka unusable for any new setup, that logs with nested json, and uses up-to-date elasticsearch. Logstash has a dedot filter for this, and adding a new option as suggested by sbward is easy.\n. ",
    "nfnty": "I implemented ESJsonEncoder field_name_substitute for this: https://github.com/nfnty/heka/commit/7540517254371c8d17eda2020226481affcf6aec\nI may send a PR later.\n. ",
    "cleslie": "Solved the problem - externals.cmake was not picking up my git path correctly due to a faulty local install of git. Sorry for the hassle.\n. ",
    "wangfeiping": "happen again\uff1a\nWho can tell me how to solve this problem?\nThanks a lot!\n. @torbiak \nThanks! I'm building Heka on Centos 6.6, I'll try it right now.\n. https://github.com/mozilla-services/heka/issues/1954\nThanks again!\n. https://github.com/mozilla-services/heka/blob/dev/docs/source/config/outputs/kafka.rst\nIs this can help?\nmax_buffered_bytes (uint32)\nThe threshold number of bytes buffered before triggering a flush to the broker. Default is 1.\n. @michaelgibson Ok, I see, thanks!\n. @michaelgibson  Thanks!\n. ",
    "SeeWei1985": "yum install patch is ok\n. try to use \"GODEBUG=cgocheck=0   hekad -config  xx.toml \"\n. [xxx@ip-172-31-12-225 tomls]$ /usr/local/bin/hekad -config  hekad.toml \n2016/07/29 07:52:53 Pre-loading: [LogOutput]\n2016/07/29 07:52:53 Pre-loading: [LogstreamerInput]\n2016/07/29 07:52:53 Pre-loading: [es_payload]\n2016/07/29 07:52:53 Pre-loading: [NullSplitter]\n2016/07/29 07:52:53 Loading: [NullSplitter]\n2016/07/29 07:52:53 Pre-loading: [ProtobufDecoder]\n2016/07/29 07:52:53 Loading: [ProtobufDecoder]\n2016/07/29 07:52:53 Pre-loading: [ProtobufEncoder]\n2016/07/29 07:52:53 Loading: [ProtobufEncoder]\n2016/07/29 07:52:53 Pre-loading: [TokenSplitter]\n2016/07/29 07:52:53 Loading: [TokenSplitter]\n2016/07/29 07:52:53 Pre-loading: [PatternGroupingSplitter]\n2016/07/29 07:52:53 Loading: [PatternGroupingSplitter]\n2016/07/29 07:52:53 Pre-loading: [HekaFramingSplitter]\n2016/07/29 07:52:53 Loading: [HekaFramingSplitter]\n2016/07/29 07:52:53 Loading: [es_payload]\n2016/07/29 07:52:53 Loading: [LogstreamerInput]\n2016/07/29 07:52:53 Loading: [LogOutput]\n2016/07/29 07:52:53 Starting hekad...\nHere, that is the output message when I execute your codes in my linux environment. It presents the same results like yours.\nThus, When I add prefix  \"GODEBUG=cgocheck=0 \", it works well.\n2016/07/29 07:56:05 Pre-loading: [LogstreamerInput]\n2016/07/29 07:56:05 Pre-loading: [es_payload]\n2016/07/29 07:56:05 Pre-loading: [LogOutput]\n2016/07/29 07:56:05 Pre-loading: [HekaFramingSplitter]\n2016/07/29 07:56:05 Loading: [HekaFramingSplitter]\n2016/07/29 07:56:05 Pre-loading: [NullSplitter]\n2016/07/29 07:56:05 Loading: [NullSplitter]\n2016/07/29 07:56:05 Pre-loading: [ProtobufDecoder]\n2016/07/29 07:56:05 Loading: [ProtobufDecoder]\n2016/07/29 07:56:05 Pre-loading: [ProtobufEncoder]\n2016/07/29 07:56:05 Loading: [ProtobufEncoder]\n2016/07/29 07:56:05 Pre-loading: [TokenSplitter]\n2016/07/29 07:56:05 Loading: [TokenSplitter]\n2016/07/29 07:56:05 Pre-loading: [PatternGroupingSplitter]\n2016/07/29 07:56:05 Loading: [PatternGroupingSplitter]\n2016/07/29 07:56:05 Loading: [es_payload]\n2016/07/29 07:56:05 Loading: [LogstreamerInput]\n2016/07/29 07:56:05 Loading: [LogOutput]\n2016/07/29 07:56:05 Starting hekad...\n2016/07/29 07:56:05 Output started: LogOutput\n2016/07/29 07:56:05 MessageRouter started.\n2016/07/29 07:56:05 Input started: LogstreamerInput\n2016/07/29 07:56:05 {\"index\":{\"_index\":\"LogstreamerInput-2016.07.29\",\"_type\":\"logfile-xxxx\"}}\nThe same issue about \"GODEBUG=cgocheck=0\" can be found https://github.com/mozilla-services/heka/issues/1881. \nThat can give some suggestions to you and you can explore the solutions for window environment.\n. The checks can be disabled by setting the environment variable GODEBUG=cgocheck=0, but note that the vast majority of code identified by the checks is subtly incompatible with garbage collection in one way or another.  https://golang.org/doc/go1.6\n\" GODEBUG=cgocheck=0 \"\nit is a environment variable. \n. I think it  has nothing to do with the godebug command.... since it has not been installed in my linux. \nIn your window cmd shell and try like this as follow.\n\nset GODEBUG=cgocheck=0\nhekad -config xx.toml\n. :-)\n. memory_limit (uint):\nThe number of bytes the sandbox is allowed to consume before being terminated (default 8MiB).\n\ninstruction_limit (uint):\nThe number of instructions the sandbox is allowed to execute during the process_message/timer_event functions before being terminated (default 1M).\noutput_limit (uint):\nThe number of bytes the sandbox output buffer can hold before being terminated (default 63KiB). Warning: messages exceeding 64KiB will generate an error and be discarded by the standard output plugins (File, TCP, UDP) since they exceed the maximum message size.\n1) what is the meaning of instructions? Would you give some examples to explain this concept of instructions\n2) output_limit  is the limit of size for one message, is right?\n. ",
    "randomchance": "I would also really like to give this a shot in our lab, but the lack of a windows release makes it really difficult...\nUnfortunately having \"An internet connection to fetch sub modules\" as a build requirement means I can't even build it since we run in disconnected environment. \n. ",
    "YiuTerran": "@sathieu The Release Version is Linux Mint 17.2 Rafaela, based on Ubuntu 14.04 LTS.\n. @sathieu , the output is:\nbash\nlibc6                                              2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6:i386                                         2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6-amd64:i386                                   2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6-dev                                          2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6-dev:i386                                     2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6-dev-amd64:i386                               2.19-0ubuntu6. 2.19-0ubuntu6.\nlibc6-i386                                         2.19-0ubuntu6. 2.19-0ubuntu6.\n. OK, i will try. Thanks for your help.\n. Maybe you are right. I reopen this issue.\nFor now i just build and scp the build/heka/ folder to the server, it works fine. So, i prefer there is a bug somewhere.\n. I should post this issue to CMake.\nAnd to fix this problem, just remove libc-amd64.\n. +1\n. OK, i got the reason. git path is not right.\n. ",
    "psychonaut": "It looks like it should be:\nindex                   = 'heka-%{%Y.%m.%d}'\n. happen very frequently for me in 0.10.0. \n. heka 0.10.0 and kafka 0.9.0.1\nTopic:heka-logs PartitionCount:2    ReplicationFactor:2 Configs:\n    Topic: heka-logs    Partition: 0    Leader: 1   Replicas: 1,3   Isr: 3,1\n    Topic: heka-logs    Partition: 1    Leader: 3   Replicas: 3,2   Isr: 3,2\n. How soon is soon?\n. ",
    "liuyangc3": "fixed it\nadd auth info, and gitlab url endwith .git\nadd_external_plugin(git http://Mycount@gitlab.xxx.com/heka-redis-plugin.git master)\nthis works fine for me \n. thanks\n. and if I use go 1.6 after run hekad:\npanic: runtime error: cgo argument has Go pointer to Go pointer [recovered]\n    panic: runtime error: cgo argument has Go pointer to Go pointer\n. ",
    "nitram509": "+1\nThis feature would help us a lot.\n. ",
    "bestdeveloperever": "+1\n. ",
    "waswrongassembled": "+1\n. ",
    "dnltsk": "+1\n. ",
    "nikhilRP": "+1\n. ",
    "gjtempleton": "The perils of making changes and a PR at the end of a long, long work day...\nSorry, really should have picked up on that. Fixed now.\n. It marshals the docker.Stats object into a string for the payload, as an example:\n2016/02/18 17:33:27\n:Timestamp: 2016-02-18 17:33:27.059935284 +0000 UTC\n:Type: DockerStats\n:Hostname: fcf6fc08102e\n:Pid: 0\n:Uuid: aa725ccd-df02-49d8-810d-b4f4873fdf87\n:Logger: elasticsearch\n:Payload: {\"read\":\"2016-02-18T17:33:27.059935284Z\",\"network\":{},\"networks\":{\"eth0{\"rx_bytes\":1734,\"tx_packets\":7,\"rx_packets\":21,\"tx_bytes\":578}},\"memory_stats\":{\"stats\":{\"cache\":188416,\"mapped_file\":126976,\"total_inactive_file\":32768,\"pgpgout\":10663,\"rss\":210354176,\"total_mapped_file\":126976,\"pgpgin\":32427,\"total_rss\":210354176,\"total_rss_huge\":121634816,\"total_inactive_anon\":4096,\"rss_huge\":121634816,\"hierarchical_memory_limit\":9223372036854771712,\"total_pgfault\":34164,\"total_active_file\":110592,\"active_anon\":210395136,\"total_active_anon\":210395136,\"total_pgpgout\":10663,\"total_cache\":188416,\"inactive_anon\":4096,\"active_file\":110592,\"pgfault\":34164,\"inactive_file\":32768,\"total_pgpgin\":32427,\"hierarchical_memsw_limit\":9223372036854771712},\"max_usage\":210608128,\"usage\":210542592,\"limit\":2099945472},\"blkio_stats\":{\"io_service_bytes_recursive\":[{\"major\":8,\"op\":\"Read\"},{\"major\":8,\"op\":\"Write\"},{\"major\":8,\"op\":\"Sync\"},{\"major\":8,\"op\":\"Async\"},{\"major\":8,\"op\":\"Total\"}],\"io_serviced_recursive\":[{\"major\":8,\"op\":\"Read\"},{\"major\":8,\"op\":\"Write\"},{\"major\":8,\"op\":\"Sync\"},{\"major\":8,\"op\":\"Async\"},{\"major\":8,\"op\":\"Total\"}]},\"cpu_stats\":{\"cpu_usage\":{\"percpu_usage\":[5318061134],\"usage_in_usermode\":5200000000,\"total_usage\":5318061134,\"usage_in_kernelmode\":170000000},\"system_cpu_usage\":83391410000000,\"throttling_data\":{}},\"precpu_stats\":{\"cpu_usage\":{\"percpu_usage\":[5314170242],\"usage_in_usermode\":5200000000,\"total_usage\":5314170242,\"usage_in_kernelmode\":170000000},\"system_cpu_usage\":83390420000000,\"throttling_data\":{}}}\n:EnvVersion:\n:Severity: 7\n:Fields:\n    | name:\"ContainerID\" type:string value:\"58bca12c2449\"\n    | name:\"ContainerName\" type:string value:\"elasticsearch\"\n    | name:\"ContainerImage\" type:string value:\"elasticsearch\"\n. Certainly seems like a sensible suggestion to me (don't know why I didn't think of that before). I'll get on it this evening or tomorrow morning.\n. The below is no longer true, feedback has said the handling of this should be moved to a sandbox decoder\nSorry, took longer than I expected.\nThe new format is:\n2016/02/23 09:16:28\n:Timestamp: 2016-02-23 09:16:28.606246174 +0000 UTC\n:Type: DockerStats\n:Hostname: 7c7009eb6318\n:Pid: 0\n:Uuid: 3462c670-aedf-4710-a019-8c3cb8b128ad\n:Logger: heka\n:Payload: {\"read\":\"2016-02-23T09:16:28.606246174Z\",\"network\":{},\"networks\":{\"eth0\":{\"rx_bytes\":1518,\"tx_packets\":8,\"rx_packets\":19,\"tx_bytes\":648}},\"memory_stats\":{\"stats\":{\"cache\":4886528,\"total_inactive_file\":4853760,\"pgpgout\":1561,\"rss\":12627968,\"pgpgin\":5326,\"total_rss\":12627968,\"total_rss_huge\":2097152,\"rss_huge\":2097152,\"hierarchical_memory_limit\":9223372036854771712,\"total_pgfault\":4733,\"active_anon\":12652544,\"total_active_anon\":12652544,\"total_pgpgout\":1561,\"total_cache\":4886528,\"pgfault\":4733,\"inactive_file\":4853760,\"total_pgpgin\":5326,\"hierarchical_memsw_limit\":9223372036854771712},\"max_usage\":17559552,\"usage\":17559552,\"limit\":2099945472},\"blkio_stats\":{\"io_service_bytes_recursive\":[{\"major\":8,\"op\":\"Read\"},{\"major\":8,\"op\":\"Write\",\"value\":61440},{\"major\":8,\"op\":\"Sync\"},{\"major\":8,\"op\":\"Async\",\"value\":61440},{\"major\":8,\"op\":\"Total\",\"value\":61440}],\"io_serviced_recursive\":[{\"major\":8,\"op\":\"Read\"},{\"major\":8,\"op\":\"Write\",\"value\":15},{\"major\":8,\"op\":\"Sync\"},{\"major\":8,\"op\":\"Async\",\"value\":15},{\"major\":8,\"op\":\"Total\",\"value\":15}]},\"cpu_stats\":{\"cpu_usage\":{\"percpu_usage\":[124057941],\"usage_in_usermode\":40000000,\"total_usage\":124057941,\"usage_in_kernelmode\":10000000},\"system_cpu_usage\":29821210000000,\"throttling_data\":{}},\"precpu_stats\":{\"cpu_usage\":{\"percpu_usage\":[116431547],\"usage_in_usermode\":40000000,\"total_usage\":116431547,\"usage_in_kernelmode\":10000000},\"system_cpu_usage\":29820210000000,\"throttling_data\":{}}}\n:EnvVersion:\n:Severity: 7\n:Fields:\n    | name:\"stat-Read-sec\" type:integer value:63591815788\n    | name:\"stat-Read-nsec\" type:integer value:606246174\n    | name:\"stat-Read-loc-name\" type:string value:\"UTC\"\n    | name:\"stat-Read-loc-cacheStart\" type:integer value:0\n    | name:\"stat-Read-loc-cacheEnd\" type:integer value:0\n    | name:\"stat-Read-loc-cacheZone\" type:string value:\"invalid\"\n    | name:\"stat-Network-RxDropped\" type:integer value:0\n    | name:\"stat-Network-RxBytes\" type:integer value:0\n    | name:\"stat-Network-RxErrors\" type:integer value:0\n    | name:\"stat-Network-TxPackets\" type:integer value:0\n    | name:\"stat-Network-TxDropped\" type:integer value:0\n    | name:\"stat-Network-RxPackets\" type:integer value:0\n    | name:\"stat-Network-TxErrors\" type:integer value:0\n    | name:\"stat-Network-TxBytes\" type:integer value:0\n    | name:\"stat-Networks[eth0]-RxDropped\" type:integer value:0\n    | name:\"stat-Networks[eth0]-RxBytes\" type:integer value:1366\n    | name:\"stat-Networks[eth0]-RxErrors\" type:integer value:0\n    | name:\"stat-Networks[eth0]-TxPackets\" type:integer value:7\n    | name:\"stat-Networks[eth0]-TxDropped\" type:integer value:0\n    | name:\"stat-Networks[eth0]-RxPackets\" type:integer value:17\n    | name:\"stat-Networks[eth0]-TxErrors\" type:integer value:0\n    | name:\"stat-Networks[eth0]-TxBytes\" type:integer value:578\n    | name:\"stat-MemoryStats-Stats-TotalPgmafault\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Cache\" type:integer value:4886528\n    | name:\"stat-MemoryStats-Stats-MappedFile\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-TotalInactiveFile\" type:integer value:4853760\n    | name:\"stat-MemoryStats-Stats-Pgpgout\" type:integer value:1561\n    | name:\"stat-MemoryStats-Stats-Rss\" type:integer value:12627968\n    | name:\"stat-MemoryStats-Stats-TotalMappedFile\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Writeback\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Unevictable\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Pgpgin\" type:integer value:5326\n    | name:\"stat-MemoryStats-Stats-TotalUnevictable\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Pgmajfault\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-TotalRss\" type:integer value:12627968\n    | name:\"stat-MemoryStats-Stats-TotalRssHuge\" type:integer value:2097152\n    | name:\"stat-MemoryStats-Stats-TotalWriteback\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-TotalInactiveAnon\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-RssHuge\" type:integer value:2097152\n    | name:\"stat-MemoryStats-Stats-HierarchicalMemoryLimit\" type:integer value:9223372036854771712\n    | name:\"stat-MemoryStats-Stats-TotalPgfault\" type:integer value:4733\n    | name:\"stat-MemoryStats-Stats-TotalActiveFile\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-ActiveAnon\" type:integer value:12652544\n    | name:\"stat-MemoryStats-Stats-TotalActiveAnon\" type:integer value:12652544\n    | name:\"stat-MemoryStats-Stats-TotalPgpgout\" type:integer value:1561\n    | name:\"stat-MemoryStats-Stats-TotalCache\" type:integer value:4886528\n    | name:\"stat-MemoryStats-Stats-InactiveAnon\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-ActiveFile\" type:integer value:0\n    | name:\"stat-MemoryStats-Stats-Pgfault\" type:integer value:4733\n    | name:\"stat-MemoryStats-Stats-InactiveFile\" type:integer value:4853760\n    | name:\"stat-MemoryStats-Stats-TotalPgpgin\" type:integer value:5326\n    | name:\"stat-MemoryStats-Stats-HierarchicalMemswLimit\" type:integer value:9223372036854771712\n    | name:\"stat-MemoryStats-Stats-Swap\" type:integer value:0\n    | name:\"stat-MemoryStats-MaxUsage\" type:integer value:17559552\n    | name:\"stat-MemoryStats-Usage\" type:integer value:17559552\n    | name:\"stat-MemoryStats-Failcnt\" type:integer value:0\n    | name:\"stat-MemoryStats-Limit\" type:integer value:2099945472\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[0]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[0]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[0]-Op\" type:string value:\"Read\"\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[0]-Value\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[1]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[1]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[1]-Op\" type:string value:\"Write\"\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[1]-Value\" type:integer value:61440\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[2]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[2]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[2]-Op\" type:string value:\"Sync\"\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[2]-Value\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[3]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[3]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[3]-Op\" type:string value:\"Async\"\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[3]-Value\" type:integer value:61440\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[4]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[4]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[4]-Op\" type:string value:\"Total\"\n    | name:\"stat-BlkioStats-IOServiceBytesRecursive[4]-Value\" type:integer value:61440\n    | name:\"stat-BlkioStats-IOServicedRecursive[0]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServicedRecursive[0]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[0]-Op\" type:string value:\"Read\"\n    | name:\"stat-BlkioStats-IOServicedRecursive[0]-Value\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[1]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServicedRecursive[1]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[1]-Op\" type:string value:\"Write\"\n    | name:\"stat-BlkioStats-IOServicedRecursive[1]-Value\" type:integer value:15\n    | name:\"stat-BlkioStats-IOServicedRecursive[2]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServicedRecursive[2]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[2]-Op\" type:string value:\"Sync\"\n    | name:\"stat-BlkioStats-IOServicedRecursive[2]-Value\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[3]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServicedRecursive[3]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[3]-Op\" type:string value:\"Async\"\n    | name:\"stat-BlkioStats-IOServicedRecursive[3]-Value\" type:integer value:15\n    | name:\"stat-BlkioStats-IOServicedRecursive[4]-Major\" type:integer value:8\n    | name:\"stat-BlkioStats-IOServicedRecursive[4]-Minor\" type:integer value:0\n    | name:\"stat-BlkioStats-IOServicedRecursive[4]-Op\" type:string value:\"Total\"\n    | name:\"stat-BlkioStats-IOServicedRecursive[4]-Value\" type:integer value:15\n    | name:\"stat-CPUStats-CPUUsage-PercpuUsage[0]\" type:integer value:124057941\n    | name:\"stat-CPUStats-CPUUsage-UsageInUsermode\" type:integer value:40000000\n    | name:\"stat-CPUStats-CPUUsage-TotalUsage\" type:integer value:124057941\n    | name:\"stat-CPUStats-CPUUsage-UsageInKernelmode\" type:integer value:10000000\n    | name:\"stat-CPUStats-SystemCPUUsage\" type:integer value:29821210000000\n    | name:\"stat-CPUStats-ThrottlingData-Periods\" type:integer value:0\n    | name:\"stat-CPUStats-ThrottlingData-ThrottledPeriods\" type:integer value:0\n    | name:\"stat-CPUStats-ThrottlingData-ThrottledTime\" type:integer value:0\n    | name:\"stat-PreCPUStats-CPUUsage-PercpuUsage[0]\" type:integer value:116431547\n    | name:\"stat-PreCPUStats-CPUUsage-UsageInUsermode\" type:integer value:40000000\n    | name:\"stat-PreCPUStats-CPUUsage-TotalUsage\" type:integer value:116431547\n    | name:\"stat-PreCPUStats-CPUUsage-UsageInKernelmode\" type:integer value:10000000\n    | name:\"stat-PreCPUStats-SystemCPUUsage\" type:integer value:29820210000000\n    | name:\"stat-PreCPUStats-ThrottlingData-Periods\" type:integer value:0\n    | name:\"stat-PreCPUStats-ThrottlingData-ThrottledPeriods\" type:integer value:0\n    | name:\"stat-PreCPUStats-ThrottlingData-ThrottledTime\" type:integer value:0\n    | name:\"ContainerImage\" type:string value:\"mozilla/heka\"\n    | name:\"ContainerID\" type:string value:\"7c7009eb6318\"\n    | name:\"ContainerName\" type:string value:\"heka\"\nHowever: the majority of the fields at root in a docker.Stats struct are uint64s which Messages don't support as a field type. I'm not sure how to approach this. My code currently casts them to int64s if their value is low enough, and just skips them if their value is too large to be cast to int64 as I'm not sure I see a huge amount of value in adding them as string fields.\n. Nice to know I've not been barking up completely the wrong tree.\nThanks for all the feedback, much appreciated. Will take it on board and try to commit back in the next 24 hours with fixes to all of the issues.\n. Think I've now handled all of the issues raised (let me know if I've missed anything.) Thanks for all the feedback.\n. This is something I've been thinking about implementing myself for both DockerLogs and DockerStats, borrowing from the message_matcher= syntax and allowing for wildcards.\nForeseeing allowing specification by container_matcher=\"ContainerLabel[foo] == 'bar'\", ...\"ContainerName == '*goldwasser'\", ...\"ContainerImage != 'nginx*'\". \nIs this something others would be interested in, and if so, does this seem a sensible way to proceed? I would think those three criteria would match every use case I can think of using current best practice?\n. @Oloremo Sorry, life rather got in the way of this. \nI wrote a quick and very dirty hack to get what I wanted out of this at work and only getting back around to writing something general enough I'd be happy enough to contribute back now. Probably halfway there with the logic.\n@fpytloun If you're only wanting to send specific containers' output to specific decoders I think you could achieve what you want using fields_from_labels and using these fields with the message_matcher for each decoder.\n. Yep, exactly what I was trying to do. Realised at the time. Apparently my push to fix it failed, pushed now.\n. Agree completely. I wasn't 100% sure how best to achieve it. \nWould your thought be to create a new file for shared functionality methods between the different plugins?\n. I couldn't think of a simpler way of preserving the types of the fields of the struct when adding them to the fields whilst giving them a predictable naming convention.\n. No problem. Would rather remove work and do it right. Thanks for the feedback.\n. I think this should trigger if there's an error on reading from the attachErrors channel indicating an issue with the channel rather than an attachError correctly being put onto the channel.\n. ",
    "relud": "I need to be able to preserve payload on all logs, so that I can extract it later for creating aggregated versions of the original logs.\n. ",
    "Madhu1512": "I am also trying to parse url query string without body but no luck with HTTP GET. Looking for help on this \n+1\n. I will resubmit the PR after fixing the build issues.\n. ",
    "brightyang": "Change CMake version to 3.0.2\uff0cbuild passed\n. ",
    "joell": "I just got bitten by the same issue.  Is this by design (for reasons not yet apparent to us), or was it simply a mistake that slipped into a commit somewhere?\n. ",
    "fire": "As someone who's not on the development team but would like to try this, can you update this to master?\n. ",
    "McStork": "@simonpasquier Thanks for your answer\nI tried the RstEncoder in LogOutput, works well.\nThen I tried InfluxdbLineEncoder in LogOutput and it seems to explain why InfluxDB does not receive any data:\n2016/03/08 14:49:20 Output started: LogOutput\n2016/03/08 14:49:20 MessageRouter started.\n2016/03/08 14:49:20 Input started: ProcStats\n2016/03/08 14:49:21 Plugin 'LogOutput' error: Error encoding message: FATAL: process_message() not enough memory\n2016/03/08 14:49:22 Plugin 'LogOutput' error: Error encoding message: FATAL: process_message() not enough memory\n2016/03/08 14:49:23 Plugin 'LogOutput' error: Error encoding message: FATAL: process_message() not enough memory\n2016/03/08 14:49:24 Plugin 'LogOutput' error: Error encoding message: FATAL: process_message() not enough memory\n. It is resolved by putting \"intr\" in skip_fields of InfluxdbLineEncoder.config.\nSo IMO there could be two issues:\n- InfluxdbOutput does not output any message if something goes wrong. It would be nice to have a error: Error encoding message: FATAL: process_message() not enough memory\n- Maybe there should be enough memory? But intr is quite long so it might not be a bad idea to not output it for some plugins.\n. :+1: \n. ",
    "hy05190134": "panic: runtime error: cgo argument has Go pointer to Go pointer\ngoroutine 13 [running]:\npanic(0xb01ae0, 0xc820130bc0)\n    /usr/local/go/src/runtime/panic.go:464 +0x3e6\ngithub.com/mozilla-services/heka/sandbox/lua._cgoCheckPointer0(0x9dedc0, 0xc820126a40, 0x0, 0x0, 0x0, 0x7fb5cc000900)\n    github.com/mozilla-services/heka/sandbox/lua/_obj/_cgo_gotypes.go:65 +0x4d\ngithub.com/mozilla-services/heka/sandbox/lua.CreateLuaSandbox(0xc8200c7600, 0x0, 0x0, 0x0, 0x0)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/sandbox/lua/lua_sandbox.go:697 +0x92a\ngithub.com/mozilla-services/heka/sandbox/plugins.(_SandboxDecoder).SetDecoderRunner(0xc820078790, 0x7fb5ddd3c708, 0xc82011d440)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/sandbox/plugins/sandbox_decoder.go:152 +0x120\ngithub.com/mozilla-services/heka/pipeline.(_dRunner).Start(0xc82011d440, 0x7fb5ddd3cfe0, 0xc820128000, 0xc8201280a0)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:624 +0x160\ngithub.com/mozilla-services/heka/pipeline.(_PipelineConfig).DecoderRunner(0xc820128000, 0xc820130160, 0xb, 0xc8201324c0, 0x1a, 0x7fb5ddd3c708, 0xc82011d440, 0x1)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/config.go:324 +0x358\ngithub.com/mozilla-services/heka/pipeline.(_iRunner).getDeliverFunc(0xc820105040, 0xc820130768, 0x1, 0xc820050000, 0x0, 0x0, 0x0, 0x0)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:452 +0x545\ngithub.com/mozilla-services/heka/pipeline.(_iRunner).NewDeliverer(0xc820105040, 0xc820130768, 0x1, 0x0, 0x0)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:514 +0x51\ngithub.com/mozilla-services/heka/plugins/logstreamer.(_LogstreamerInput).startLogstreamInput(0xc820078840, 0xc820013e80, 0x1, 0x7fb5ddd3caf0, 0xc820105040, 0x7fb5ddd3cfe0, 0xc820128000)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/plugins/logstreamer/logstreamer_input.go:196 +0x8e\ngithub.com/mozilla-services/heka/plugins/logstreamer.(_LogstreamerInput).Run(0xc820078840, 0x7fb5ddd3caf0, 0xc820105040, 0x7fb5ddd3cfe0, 0xc820128000, 0x0, 0x0)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/plugins/logstreamer/logstreamer_input.go:216 +0x132\ngithub.com/mozilla-services/heka/pipeline.(_iRunner).Starter(0xc820105040, 0x7fb5ddd3cfe0, 0xc820128000, 0xc82012818c)\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:325 +0x1b7\ncreated by github.com/mozilla-services/heka/pipeline.(*iRunner).Start\n    /home/vagrant/workspace/heka/build/heka/src/github.com/mozilla-services/heka/pipeline/plugin_runners.go:305 +0x88d\n. I also encounter  the problem\n. ",
    "ru-lai": "I encountered the problem as well.  Only when I build from source.\n. ",
    "affinity226": "+1\n. post toml configuration file : \n[test_input]\ntype = \"LogstreamerInput\"\nlog_directory = \"/home/jiwoncha/LOG\"\nfile_match = 'log(?P\\d+).txt'\ndecoder = \"test_decoder\"\npriority = [\"idx\"]\nrescan_interval = \"10s\"\n[test_decoder]\ntype = \"PayloadRegexDecoder\"\nmatch_regex = '^(?P\\S+)'\ntimestamp_layout = \"2006/01/02 15:05:05\"\n[test_decoder.message_fields]\nType = \"TestLogFile\"\nPayload = \"Message\"\n[test_encoder]\nType = \"PayloadEncoder\"\nAppend_newlines = true\n[RstEncoder]\n[test_output]\ntype = \"FileOutput\"\nmessage_matcher = \"Logger == 'test_input'\"\npath = \"/home/jiwoncha/out.txt\"\nencoder=\"test_encoder\"\nperm = \"666\"\nflush_count = 100\nflush_operator = \"OR\"\n. ",
    "ahjdzx": "+1\n. ",
    "cweaver1321": "+1\n. @baniuyao Have you found a way to set the batch size? I am having a similar issue with the performance of the kafka producer. It appears to be sending 1 message no matter how fast the throughput is. It has become a serious bottleneck. I am hoping you have found a solution. \n. ",
    "hbrkanwei": "+1\n. ",
    "miah": "FYI, to work around the issue until its resolved in hekad (though that seems unlikely: https://mail.mozilla.org/pipermail/heka/2016-May/001059.html)\nStart hekad with the GODEBUG variable exported in its environment:\nexport GODEBUG=cgocheck=0\nSee: https://golang.org/doc/go1.6#cgo for more details.\n. ",
    "bruth": "I experienced the same issue.\n. ",
    "jpuigm": "Any updates on this end?\n. ",
    "spkane": "\nThe issue is SIP in El Capitan.\nThe installer installs to what is now considered a protected directory /usr/share.\nYou can work around this by installing to a none root mount point like this:\n\nsudo installer -pkg /tmp/heka-0_10_0-darwin-amd64.pkg -target /Volumes/usb_stick/\n- You can then move the files around as you see fit.\n- The fix is likely to create the installer so that it is installing into /usr/local or better yet /opt.\n. ",
    "ericpai": "I can't pass the unit test because the program is hanging when mockOutputRunner calls UpdateCursor(), but it works well under production environment. Is there any issue of the gomock libraries? \n. @trixpan Thanks for your reply. Your solution is to use SyncProducer instead which may impact the performance. Is there any solution with AsyncProducer? \n. ",
    "frankyaorenjie": "buffer bytes parameters will make effect according to the message size. Batch size is according to the number of messages and they are different parameters in Kafka manbook as well\n. ",
    "jamiegwatkin": "Another example where install fails when using Ansible -\n```\nASK: [heka | Install heka] *********\n deb=/tmp/heka_0.10.0_amd64.deb\nfailed: [localhost] => {\"failed\": true}\nmsg: A later version is already installed\nFATAL: all hosts have already failed -- aborting\n```\nubuntu@ip-172-50-2-xx:/opt/ansible$ hekad --version\n0.10.0b1\n. ",
    "maxstepanov": "There is already a #1867 pull request. But it introduces additional configuration of keeping track of remapping which makes things even more complex. I just don't want fields to be lost as it happens now.\n. ",
    "cbroglie": "Thanks, I had the same problem and installing cmake 3.0 worked for me (brew install homebrew/versions/cmake30, pretty sure this is specific to OSX)\n. @sathieu Tried that, didn't work:\n```\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$ source build.sh\n-- GeoIP.h was not found, GeoIP functionality will not be included in this build.\n-- Docker plugins enabled.\n-- sphinx-build was not found, the documentation will not be generated.\nCMake Warning (dev) at cmake/ExternalProject.cmake:202 (if):\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n  details.  Use the cmake_policy command to set the policy and suppress this\n  warning.\nQuoted keywords like \"COMMAND\" will no longer be interpreted as keywords\n  when the policy is set to NEW.  Since the policy is not set the OLD\n  behavior will be used.\nCall Stack (most recent call first):\n  cmake/ExternalProject.cmake:1524 (_ep_parse_arguments)\n  cmake/externals.cmake:19 (externalproject_add)\n  CMakeLists.txt:105 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\nCMake Warning (dev) at cmake/ExternalProject.cmake:701 (if):\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n  details.  Use the cmake_policy command to set the policy and suppress this\n  warning.\nQuoted keywords like \"TEST\" will no longer be interpreted as keywords when\n  the policy is set to NEW.  Since the policy is not set the OLD behavior\n  will be used.\nCall Stack (most recent call first):\n  cmake/ExternalProject.cmake:1432 (_ep_get_build_command)\n  cmake/ExternalProject.cmake:1562 (_ep_add_build_command)\n  cmake/externals.cmake:19 (externalproject_add)\n  CMakeLists.txt:105 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\nCMake Warning (dev) at cmake/ExternalProject.cmake:701 (if):\n  Policy CMP0064 is not set: Support new TEST if() operator.  Run \"cmake\n  --help-policy CMP0064\" for policy details.  Use the cmake_policy command to\n  set the policy and suppress this warning.\nTEST will be interpreted as an operator when the policy is set to NEW.\n  Since the policy is not set the OLD behavior will be used.\nCall Stack (most recent call first):\n  cmake/ExternalProject.cmake:1432 (_ep_get_build_command)\n  cmake/ExternalProject.cmake:1562 (_ep_add_build_command)\n  cmake/externals.cmake:19 (externalproject_add)\n  CMakeLists.txt:105 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\nCMake Warning (dev) at cmake/ExternalProject.cmake:701 (if):\n  Policy CMP0064 is not set: Support new TEST if() operator.  Run \"cmake\n  --help-policy CMP0064\" for policy details.  Use the cmake_policy command to\n  set the policy and suppress this warning.\nTEST will be interpreted as an operator when the policy is set to NEW.\n  Since the policy is not set the OLD behavior will be used.\nCall Stack (most recent call first):\n  cmake/ExternalProject.cmake:1458 (_ep_get_build_command)\n  cmake/ExternalProject.cmake:1563 (_ep_add_install_command)\n  cmake/externals.cmake:19 (externalproject_add)\n  CMakeLists.txt:105 (include)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /Users/cbroglie/Workspace/go/src/github.com/mozilla-services/heka/build\n[  0%] Performing build step for 'lua_sandbox'\n[  1%] Performing update step for 'lua_sax'\n[  2%] Performing configure step for 'lua_sax'\nNot searching for unused variables given on the command line.\nCMake Error at /usr/local/Cellar/cmake/3.5.2/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:148 (message):\n  Could NOT find Lua (missing: LUA_LIBRARIES)\nCall Stack (most recent call first):\n  /usr/local/Cellar/cmake/3.5.2/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:388 (_FPHSA_FAILURE_MESSAGE)\n  cmake/FindLua.cmake:126 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)\n  CMakeLists.txt:56 (find_package)\n-- Configuring incomplete, errors occurred!\nSee also \"/Users/cbroglie/Workspace/go/src/github.com/mozilla-services/heka/build/ep_base/Build/lua_sandbox/ep_base/Build/lua_sax/CMakeFiles/CMakeOutput.log\".\nmake[5]:  [ep_base/Stamp/lua_sax/lua_sax-configure] Error 1\nmake[4]:  [CMakeFiles/lua_sax.dir/all] Error 2\nmake[3]:  [all] Error 2\nmake[2]:  [ep_base/Stamp/lua_sandbox/lua_sandbox-build] Error 2\nmake[1]:  [CMakeFiles/lua_sandbox.dir/all] Error 2\nmake:  [all] Error 2\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$\n```\nCurrent branch:\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$ git branch\n  dev\n* versions/0.10\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$\n```\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$ git log -1\ncommit c12752d96a80f1caf44f1ca590db8a8af6720cc2\nAuthor: Mike Trinkala trink@acm.org\nDate:   Thu Apr 28 10:35:34 2016 -0700\nPull in the lua_sandbox cmake 3.5 fix\n\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$\n```\ncmake version:\n```\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$ cmake --version\ncmake version 3.5.2\nCMake suite maintained and supported by Kitware (kitware.com/cmake).\n~/Workspace/go/src/github.com/mozilla-services/heka/build (versions/0.10)$\n```\nBuilding with cmake 3.0.2 succeeds.\n. ",
    "shijiant": "@sathieu  thanks\ni found  the RegexSplitter is works for this  situation\n. ",
    "jsh2134": "can you add more details? maybe a traceback?\n. ",
    "awl": "Did you ever resolve this? I'm running into the exact same problem trying to load a filter I wrote.\n. ",
    "binman-docker": "No, unfortunately. We decided to move to Logstash since Heka is end-of-life and Logstash has a lot more built-in filters (including what I was trying to do with this custom lua filter) anyway.\n. ",
    "deric": ":+1: \n. ",
    "vit1251": "@sathieu and this similar task is resolve?\n. @sathieu could you suggest working version ready to build? Now I was trying 0.10.0 and 0.9.2 with topic error on 0.10 with lua_sandbox and 0.9.2 does not found lua.h but I previously install lua5.3-dev\n. > Probably, use an older compiler or cmake, i.e use an older distribution.\n@sathieu No, I use modern version of Ubuntu 16.04 release from 21.04.2016 (release 7 day ago) and version of cmake is 3.5.1, lua is 5.3, gcc is 5.3.1.\n@trink Your #1920 resolve the problem. Thanx.\n. Yep. I know about CMake processing,\nbut no words about this in documentation\nand it may confuse.\n. ",
    "bigkraig": ":+1: although why make this configurable? is the data useful if its still compressed?\n. @rafrombrc @trink Whats the likelihood of this being included? I am evaluating using heka to funnel scollector metrics, which are posted in gzip.\n. ",
    "lairdnote": "I checked network . network is fine . the cache data has send to ES . I check ES data. find cache data into .\nSo .I thins maybe ES output plugin can't Recycle cache. I change hekad version to 0.9.2 so all things is good\n. this is my checkpoint.txt file.  \nroot@heka:/var/cache/hekad/output_queue/ElasticSearchOutput# ll\ntotal 204812\ndrwxr--r-- 2 root root      4096 May 20 05:47 ./\ndrwxr--r-- 3 root root      4096 May 20 04:30 ../\n-rw-r--r-- 1 root root 209715062 May 20 05:16 3.log\n-rw-r--r-- 1 root root         0 May 20 05:23 4.log\n-rw-r--r-- 1 root root         0 May 20 05:47 5.log\n-rw-r--r-- 1 root root        11 May 20 05:17 checkpoint.txt\nroot@heka:/var/cache/hekad/output_queue/ElasticSearchOutput# cat checkpoint.txt\n3 189562626\nthanks for your help \n. ",
    "ZhangYet": "That helps a lot. Thank you!\n. Sorry for my bad habit. And this solves my problem. Thanks a lot\n. ",
    "Oloremo": "Im totally ok with this example.\n. @gjtempleton Any updates? Just wondering...\n. ",
    "fpytloun": "Hello, this feature seems to be a must have to me :-)\nOtherwise it's not possible to pair decoders with specific containers.\n. @gjtempleton Thank you, that probably may work for my use-case. So I will have to use DockerInput with MultiDecoder and message_matcher for each decoder?\n. ",
    "jxstanford": "@rafrombrc no problem.  really excited about working with Heka.  all comments addressed...\n. ",
    "DonHarishAxe": "Nevermind that was due to use_buffering set to true by default.\n. How come? I need to use the common config file with all the other config files. So i must import the common file and all the config files into the same directory. But that will load all of the plugins and clash unnecessarily. Or else i must make copies of common file and put it in a seperate directory with each config file correspondingly. That will not serve its purpose. Btw thanks for replying. Hope you prove me wrong and come up with a solution.\n. To be simple, I am actually asking if there is an option in Heka to specify two configuration files together instead of specifying it in a directory. Or if there is any provision in toml format itself to import another toml file.\n. ProcessDirectoryInput can dump into a single output plugin and into the same index and type in my case as far as I know. But as i have already told, I need it to be dumped onto different indices. Moreover I do not know why you brought this into this discussion. Because ProcessDirectoryInput just takes the ProcessInput block of the config files. So I feel this isn't related to my problem.\n. ",
    "adamchainz": "The link on the Github repo description also needs updating.\n. Also it looks like heka-docs.readthedocs.io has gone down, any idea about that?\n. ",
    "vpriyada": "Thanks for the response SeeWei1986\nI am getting this error 'GODEBUG' is not recognized as an internal or external command,\noperable program or batch file.\nShould I install GODEBUG?\nI have only GO installed in my machine and have created a environement variable GOPATH and its value as C:\\Go\\bin and also include path of Go in PATH variable.\n. Hi,\nCould you tell me how to set this environment variable GODEBUG=cgocheck=0\nI had Go installed in my windows not GODEBUG. So I did a git pull from  \"go get github.com/mailgun/godebug\"\nNow godebug is present in my windows machine.\nAfter that if I give the command GODEBUG=cgocheck=0 -config=hekad.toml I get the following output:\n**C:\\etc>GODEBUG=cgocheck=0 hekad -config=hekad.toml\ngodebug is a tool for debugging Go programs.\nUsage:\ngodebug command [arguments]\nThe commands are:\nbuild     compile a debug-ready Go program\nrun       compile, run, and debug a Go program\ntest      compile, run, and debug Go package tests\nUse \"godebug help [command]\" for more information about a command.**\nI did not get the output like yours.\nCould you help me in this if you are aware of what I am missing.\nThanks.\n. Hi SeeWei1986,\nIt just worked :-)\nThanks for your timely help and immediate response.\nYou just made my day :-):-):-) It is working fine now after setting the environment variable.\n. ",
    "wwalker": "I've narrowed the problem down to this:    $ssl_protocol/$ssl_cipher\nthat is straight from the nginx.conf as it should be, but build_grammar doesn't work right.\nIf I change it to $ssl_protocol  it works, including the data that should be in the ssl_cipher in $ssl_protocol.\n. I asked on both the mailing list and IRC.  I only reported it here when I was sure it was a bug.  The documentation says to paste the log_format directly from the nginx.conf.  I did that and it doesn't work; therefore it is a bug that should either be fixed, or being a documented known bug.  I had already worked around it both with PayloadRegexDecoder and with modifying the log_format in SandboxDecoder/nginx_access.lua (losing a bit of data granularity).\n. ",
    "rmrf": "OK, Thanks. \n. ",
    "sampointer": "If this is anything like fluentd, which uses msgpack, then small messages can blow ElasticSearch request limits if they pack very efficiently. In fluentd the message size limits are enforced after serialization to the transport format, which when unrolled at the other end can balloon to into huge HTTP POSTs, which are rejected. This is especially prevalent with JSON and logging information, which msgpack compresses very well.\nThe fluentd ecosystem is a tire fire, so don't be tempted to jump ship in that direction :smiley: . ",
    "tosh-iba": "Oops!\n. ",
    "gorsuch": "LOL\n. ",
    "cablehead": "Just came across this as I'm trying to get read_message(\"raw\") to work. It's returning nil for me, inside a lua sandbox encoder plugin as well...\nAny advice would be appreciated!\n. ",
    "iandioch": "The comment here doesn't make much sense anymore\n. "
}
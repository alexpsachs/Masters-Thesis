{
    "maciejkula": "Solved in https://github.com/lyst/lightfm/issues/22 and deployed to pypi as 1.4. pip install lightfm==1.4 should get the new version.\n. Thanks for opening the issue; could you have a look if https://github.com/lyst/lightfm/pull/24 solves this for you?\n. Can you try a glob pattern that works for your installation?\n. I updated #24, can you try again?\n. Great, let's see if it works with the other issue.\n. I pushed the updated code to pypi as version 1.5. Thanks for your help!\n. Could it be because you're trying to link to the Anaconda version of python? This should not be related to llvm at all.\nCan you try using a non-Anaconda python installation?\n. You can also try #24 and see if that solves your issue.\n. Are you using the anaconda python distribution? I can only suppose that could be part of the problem.\n. I see. Could you have a look at https://github.com/lyst/lightfm/pull/26?\nIt might fix the problem, according to http://stackoverflow.com/questions/9840207/how-to-use-avx-pclmulqdq-on-mac-os-x-lion/19342603#19342603\n. I got hold of a mac and will try to reproduce your problem.\n. It is a problem with the Anaconda distribution. I have updated #26 to automatically drop -march=native whenever we know we're dealing with anaconda. This should finally fix it!\n. Perfect, thanks for your help!\n. You are right, that will be the case for movies up to 18 (as there are 18 genres).\nNo, this is not intended. I'll have a think about whether this is worth fixing.\n. Looks good, thanks. Could you rebase on top of newest master (I merged #26) and I'll merge?\n. Yes, looks good, thanks!\n. Apparently that is a clang-only flag, it should not be given to gcc. Can you tell me what OSX version you are on and which Python distribution you are using?\n. Can you paste the output of which python please?\n. I managed to reproduce your issue, it arises because you are building against the default OSX Python distribution. Installing Python from brew, macports, or the anaconda distribution should solve this problem while I look for a better solution.\n. Can you have a look if https://github.com/lyst/lightfm/pull/30 fixes the issue for you?\n. Yes, this manifests the same as https://github.com/lyst/lightfm/issues/25 (though the root cause is different). I'll have a think.\n. I updated #30 to remove the -march=native flag. Can you confirm that it works for you?\n. I tried many things, but I think the soundest course of action here is to not support the default OSX Python installation. I'll update the readme to that effect.\n. Default OSX Python is not supported.\n. Yes, that sounds great! What were your results?\nI think the notebook will need a bit of clean-up (for instance, I'd like to have utility functions that do all the data download/directory creation automatically; I'd also like the code to be close to PEP8 compliant), but it sounds like a useful addition!\n. If you'd like to include the example in this repo, I think it should be entirely self contained (that is, no cloning of other repositories) and if possible have very minimal dependencies. Kaggle login details are a problem, is there no other way of getting the data?\nIf this all sounds too painful I would be happy to include a link to your notebook in the readme?\n. Sure, that sounds like the best plan for the moment. Where would you like me to link to?\n. Made a PR in #34 \n. Interestingly, this does not seem to materially change the results of the learning schedules example --- the accuracy curves look pretty much the same. Is there something I'm missing @jongwook ?\n. Can you link to the dockerfile please? I'll pull it and try to reproduce.\n. This is probably due to an old version of gcc, can you try with a newer version?\n. You can now use the Dockerfile from the lightfm repo to set up your dependencies.\n. This is due to the fact that rand_r is part of glibc which you are obviously not using.\nIt may work for you as-is if you use gcc for compilation. Alternatively, I have opened https://github.com/lyst/lightfm/pull/42 which includes its own rand_r implementation. I obviously cannot use MSVC's impl as that would only compile on Windows!\nHaving said that, you may run into more problems still with openmp support and so on; I cannot support Windows myself but would be happy to accept changes that make it work on Windows.\n. It would be great if you could check out  #42 and see if it solves your problem.\n. Thanks, please let me know if you succeed in using it.\n. Does it work fine?\n. Fantastic, I'll merge my changes and cut a new release.\n. Thanks!\n. The current implementation supports float feature weights.\n. Yes, you need to do this for logistic loss.\nFor WARP loss you only need positives, as the negatives are sampled from other users' interactions. So in that sense this is not necessary. I guess the interpretation here is that only values above some X are actual positives.\n. @xiaop1987 are you happy about this answer? Or can I expand more?\n. Yes, WARP loss is for implicit data. So this step basically drops anything less than 4 (but you could equally well leave everything in).\nSure, should I drop you an email at list.gutj@gmail.com?\n. Done, let me know if you didn't get it!\n. Thanks to @VaibhavBehl for the motivation\n. On it.\n. I changed the API slightly in https://github.com/lyst/lightfm/pull/61, please have a play and tell me what you think. Accepting a sparse matrix makes it much harder to attach the wrong weights to your interactions.\n. This adds an additional check that everything works as expected: https://github.com/lyst/lightfm/pull/62\nClearly random weights don't make that much difference!\n. I guess you'll know if it's useful when you have a task that requires it. Thanks for your help.\nResolved by #61.\n. You can git rebase -i upstream/master to get rid of the unnecessary commits. Or just apply your change again, it looks small enough.\n. Thanks!\n. Thanks!\n. LightFM itself simply computes the model. How you serve it in production is a different matter, and one that will be specific to each application. For example, you could precompute recommendations for all users and save them somewhere for later retrieval.\nI'm not very familiar with predictionIO so will not be able to comment there.\n. I've opened https://github.com/lyst/lightfm/pull/64 which adds a bunch of optimized evaluation functions, please have a look and see if they help.\nIn general, evaluating learning-to-rank metrics is expensive because you have to compute the full set of recommendations for every user. If you still find it prohibitively slow I would advise choosing a smaller test set by randomly sampling from your users.\n@seanlaw you might find this useful too.\n. I'd just use the ranking metric score on the validation set. Especially for WARP watching validation loss (as in neural networks) is not the best approach because the algorithms selects more difficult examples the better it gets, so your raw loss values would be all over the place.\nI'd also suggest random search or hyperopt for hyperparams search, but that's up to you :)\n. Thanks!\n. Great, thanks!\n. Thanks for the report --- can you post what your version of scipy is?\n. You should upgrade to a newer version of scipy, >= 0.17.0. I will change the requirements file to reflect this.\n. Can you have a look at https://github.com/lyst/lightfm/pull/90 and see if that solves your problem?\n. Merged, thanks for the report.\n. That sounds like a great idea, I'd love to see contributions!\n. Sorry! I agree that many applications will have an ANN model to accompany this. However, this is probably best accomplished with an external package.\nAs to guidance: you're probably right, we could add an example ipython notebook that shows how to do this. Would you like to have a stab at a first draft?\n. Sure, I'd love an example! One thing that might be useful as well is some discussion on the pros and cons of using something like Annoy or NMSlib.. Great, thanks! Would be great to include this in the examples and integrate in the documentation. Would you mind submitting a PR and we can go from there?\nIt might even be worth having a lightfm.contrib.ann module and actually build these functions in. I have historically been wary of including heavy(ish) external dependencies, but if we keep them optional it should be fine.. Yep, the CI test is unrelated, I can re-run it.\nCould you add a small test that tests the common cases for the recall function?\nAfter that I'll be very happy to merge, it looks great. By the way, you should create a contributors file and add yourself there!\n. Thanks for the changes, I think it looks good now (except for the failing test).\nBefore merging, could you rebase on top of newest master and squash the commits? I can do that if you prefer.\n. Looks great. As you probably noticed I surreptitiously added some PEP8 checks as well :)\n. So close, one more lint failure :) (The failing test is an accident.)\n. This is what the Circle flake8 finds:\n```\nflake8 --config .flake8rc\n./lightfm/evaluation.py:81:84: W291 trailing whitespace\n./build/lib.linux-x86_64-2.7/lightfm/evaluation.py:81:84: W291 trailing whitespace\nflake8 --config .flake8rc returned exit code 1\n``\n. Great, thanks!\n. A L2 penalty of 0.05 is extremely strong, probably leading all parameters to be zero. Can you try1e-05`?\n. Great, no worries!\n. You need working C compiler to install LightFM.\nThe easiest way to get this (on Windows) is to use Miniconda as your Python environment. Alternatively, you could install the Microsoft Visual C++ compiler.\n. Use pip to install it.\nSo:\n1. Install miniconda.\n2. Run pip install lightfm.\n. You can use the https://github.com/lyst/lightfm/blob/master/appveyor.yml file for reference.\n. It looks like you still need the Microsoft C compiler. There are some pointers here: https://wiki.python.org/moin/WindowsCompilers\nIn general I strongly recommend developing on Linux.\n. LightFM is designed to be able to deal with this problem.\nYou do this via user features: as long as each user is described by a set of features and these features are shared between the training and the test set, you will be able to make predictions in the test set.\nSuppose you describe each user by their unique id and their country of origin. While training, you estimate parameters for the id feature and the country feature. At test time, you can use the country feature to provide prediction even for a user whose id isn't part of the testing set. (You couldn't do this if you didn't have the country feature, because there would be no way of estimating the representation for a new user.)\nThis is exactly equivalent to the StackExchange example, but instead of using item features (in this case, question tags) you would use user features (by passing them to the fit and predict methods).\n. Does my answer help?\n. Thanks!\nYou'll need to run python setup.py cythonize and commit the .c files before CI can build the new version.\n. The code looks good. Could you explain why this would be more useful than just predict? My preference would be to expand predict to make it more flexible (if necessary), rather than adding another method to the API.\n. I see, computing representations only once is an advantage.\nWhat do you think about the following?\n1. Accept sparse matrices as well as current arguments (for backwards compatibility)\n2. Accept an flag which would cause all needed user and item representations to be precomputed before computing all the scores. This flag could also be useful for predict_ranks.\n. You can have a look here: https://github.com/lyst/lightfm/blob/master/lightfm/_lightfm_fast.pyx.template#L756\nLightFM uses a dialect of Python which compiles to C --- the actual code lives in the file I linked to.\n. Did you use the Dockerfile included in the lightfm repo? Your image needs\nto have blas/lapack installed.\nOn 15 Jul 2016 01:41, \"Sean M. Law, Ph.D.\" notifications@github.com wrote:\n\ndocker-compose build lightfm\nProduces this on my Mac OS X:\nFile \"/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py\", line 966, in add_subpackage caller_level = 2)\nFile \"/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py\", line 935, in get_subpackage caller_level = caller_level + 1)\nFile \"/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py\", line 872, in _get_configuration_from_setup_py\nconfig = setup_module.configuration(*args)\nFile \"scipy/linalg/setup.py\", line 20, in configuration\nraise NotFoundError('no lapack/blas resources found')\nnumpy.distutils.system_info.NotFoundError: no lapack/blas resources found\n\nCan't roll back scipy; was not uninstalled\nCleaning up...\nCommand /usr/bin/python -c \"import setuptools, tokenize;file='/tmp/pip-build-XnyZzK/scipy/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-cBi7ka-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip-build-XnyZzK/scipy\nStoring debug log for failure in /root/.pip/pip.log\nERROR: Service 'lightfm' failed to build: The command '/bin/sh -c pip\ninstall .' returned a non-zero code: 1\nI'm new to Docker but see that it is SciPy complaining about missing\nlapack/blas\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/101, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA6gLuEe02hYRKTpBT3qvE2uwwM5Wks5qVtc4gaJpZM4JM-rc\n.\n. OK, do you think you could make a PR with the fix?\n\nOn 15 Jul 2016 12:55, \"Sean M. Law, Ph.D.\" notifications@github.com wrote:\n\nYeah, I just used the plain default image from Docker and then built\nlightfm using the provided Dockerfile. I added this line to the Dockerfile:\nRUN apt-get install -y libopenblas-dev gfortran\nAnd the install succeeded. Should this be added to the repo? Otherwise,\nthe Docs won't reflect something that runs properly.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/101#issuecomment-232922244, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA0MGZlYHOq5f4OlO5z7_TOfRR3_cks5qV2c0gaJpZM4JM-rc\n.\n. Thanks! Could you change libopenblas-dev to libblas-dev liblapack-dev\ngfortran?\n\nNumpy used to have problems with OpenBLAS.\nOn 15 Jul 2016 3:11 pm, \"Sean M. Law, Ph.D.\" notifications@github.com\nwrote:\n\nAdded an 'apt-get' line in the Dockerfile so that scipy installs\nsuccessfully\nissues: fixes #101 https://github.com/lyst/lightfm/issues/101\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/lyst/lightfm/pull/102\nCommit Summary\n- Fixed issue #101 install blas, required for scipy\nFile Changes\n- M Dockerfile\n  https://github.com/lyst/lightfm/pull/102/files#diff-0 (1)\nPatch Links:\n- https://github.com/lyst/lightfm/pull/102.patch\n- https://github.com/lyst/lightfm/pull/102.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/pull/102, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAxcA62F74uYWkLwMgQ243AwTd_h9ks5qV4cEgaJpZM4JNYhr\n.\n. Great, thanks!\n. Have you built the extension? Can you try running it from a different\ndirectory?\n\nOn 15 Jul 2016 16:30, \"Sean M. Law, Ph.D.\" notifications@github.com wrote:\n\nFollowing the issue #101 https://github.com/lyst/lightfm/issues/101, I\ntried running the test as per the Docs:\ndocker-compose run lightfm py.test -x tests/\nAnd then I ran it from the Docker shell (see below). In both cases, the\ntest fails to find lightfm_fast\nroot@002ca3eaa9c1:/home/lightfm# py.test -x tests/\n================================================== test session starts ==================================================\nplatform linux2 -- Python 2.7.9, pytest-2.9.2, py-1.4.31, pluggy-0.3.1\nrootdir: /home/lightfm, inifile:\ncollecting 0 items / 1 errors\n======================================================== ERRORS =========================================================\n______ ERROR collecting tests/test_api.py _______\ntests/test_api.py:7: in \nfrom lightfm import LightFM\nlightfm/init.py:1: in \nfrom .lightfm import LightFM\nlightfm/lightfm.py:9: in \nfrom .lightfm_fast import (CSRMatrix, FastLightFM,\nE   ImportError: No module named lightfm_fast\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n================================================ 1 error in 0.30 seconds ================================================\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/103, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA-6VZbMegYU0Yr4YZLJwcmv2TV7Zks5qV5logaJpZM4JNc_-\n.\n. Off the top of my head:\n1. Cython does not need to be in the container.\n2. You do not need to run cythonize.\n3. When the container is built lightfm is already installed.\n4. When you run tests they fail because pytest picks up the source code\n   from the directory you are running the tests in instead of the installed\n   package. You must move the tests somewhere else or run them or run pip\n   install -e .\n\nOn 16 Jul 2016 12:59 pm, \"Sean M. Law, Ph.D.\" notifications@github.com\nwrote:\n\nNo, I had not built the extension and noticed that Cython is not in the\nDockerfile as well.\nThen, I did the following:\npip uninstall lightfm\ncd /home/lightfm\napt-get install -y cython\npython setup.py cythonize\nls lightfm\nThis shows that the extension is present (the lightfm_fast.pyx file). And\nthen I install lightfm and ran the test again:\npip install .\npy.test -x tests/\nThis results in the same error as my original post.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/103#issuecomment-233125151, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA8k-w2u3XdZjP3d4HJLnAEj5AlT9ks5qWLl-gaJpZM4JNc_-\n.\n. I just verified that after adding the libblas fixes lightfm is correctly\ninstalled in the docker container.\n\nYou can check this by deleting the lightfm folder from the mounted\nrepository folder. This will prevent python from picking up the local files\n(which do not have the extension built) in the import process, and tests\nwill complete.\nOn 16 Jul 2016 1:13 pm, \"Maciej Kula\" noocrat@gmail.com wrote:\n\nOff the top of my head:\n1. Cython does not need to be in the container.\n2. You do not need to run cythonize.\n3. When the container is built lightfm is already installed.\n4. When you run tests they fail because pytest picks up the source code\n   from the directory you are running the tests in instead of the installed\n   package. You must move the tests somewhere else or run them or run pip\n   install -e .\nOn 16 Jul 2016 12:59 pm, \"Sean M. Law, Ph.D.\" notifications@github.com\nwrote:\n\nNo, I had not built the extension and noticed that Cython is not in the\nDockerfile as well.\nThen, I did the following:\npip uninstall lightfm\ncd /home/lightfm\napt-get install -y cython\npython setup.py cythonize\nls lightfm\nThis shows that the extension is present (the lightfm_fast.pyx file). And\nthen I install lightfm and ran the test again:\npip install .\npy.test -x tests/\nThis results in the same error as my original post.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/103#issuecomment-233125151, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA8k-w2u3XdZjP3d4HJLnAEj5AlT9ks5qWLl-gaJpZM4JNc_-\n.\n. Thanks for the changes! Would it be possible to introduce the required\ninterface changes into the LightFM class without using the sklearn mixins?\n\n\nThat would feel like a cleaner solution to me, and we can enforce it in\ntests.\nOn 2 Sep 2016 3:21 am, \"Florian Wilhelm\" notifications@github.com wrote:\nIn order to fix issue #106 https://github.com/lyst/lightfm/issues/106\nthis PR adds a BaseEstimator Scikit-Learn Mixin if Scikit-Learn is\navailable. This adds get_params and set_params which is used by many meta\nestimators like grid_search.RandomizedSearchCV.\nIf this PR gets accepted I would also extend the documentation and provide\nfurther changes to allow the usage of grid_search.RandomizedSearchCV for\nhyper-parameter optimization for LightFM models.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/lyst/lightfm/pull/107\nCommit Summary\n- Added get/set_params\n- Some cleanups and easier usage\n- Fixed unit test\nFile Changes\n- M lightfm/init.py\n  https://github.com/lyst/lightfm/pull/107/files#diff-0 (5)\n- A lightfm/compat_sklearn.py\n  https://github.com/lyst/lightfm/pull/107/files#diff-1 (13)\n- M lightfm/lightfm.py\n  https://github.com/lyst/lightfm/pull/107/files#diff-2 (19)\n- M tests/test_movielens.py\n  https://github.com/lyst/lightfm/pull/107/files#diff-3 (4)\n- A tests/test_sklearn_api.py\n  https://github.com/lyst/lightfm/pull/107/files#diff-4 (6)\nPatch Links:\n- https://github.com/lyst/lightfm/pull/107.patch\n- https://github.com/lyst/lightfm/pull/107.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/pull/107, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA_jzU4zJAwB-PFLUaWlrKuvu8Ifxks5ql_idgaJpZM4JzkXY\n.\n. I think I'd be happiest with option (1). It may be some duplication but I think it keeps things simple.\nBoth other options introduce dependencies which I can't control, and, especially with (3), inheritance from a class I don't control. I agree that LightFM should conform to the sklearn interface, but I think inheritance is a very heavy-handed approach to that, especially when we just need a couple of methods.\n. Looks great with just one minor comment!\n. I think this is perfectly sufficient for a test --- thanks for doing this!\n. Cross-validation (as implemented in sklearn) isn't a great fit for collaborative filtering problems, as explained in Florian's comment above.\nYou can approximate it by doing several independent train-test splits and using those in your hyperparameter search.. You need to pass user_features to the predict call as well (or auc_score as\nis the case here).\nOn 19 Sep 2016 11:38, \"Andre\" notifications@github.com wrote:\n\nI'm trying to fit a very basic instance of LightFM\nmodel = LightFM(loss='warp')\nmodel = model.fit(df, user_features=user_features)\nWhere it should be noted that df.shape = (222113, 2269), and\nuser_features.shape = (222113, 24). The model object is able to be fitted,\nbut when I subsequently try to call the auc_score method, I get the\nfollowing error.\nTraceback (most recent call last):\nFile \"main.py\", line 35, in \nmodel = engine.fit(train, test, user_features=user_feat)\nFile \"/Users/andre/Playphone/Recommendations/lightfm-engine/engine.py\",\nline 23, in fit\ntrain_auc = auc_score(model, train, num_threads=self.NUM_THREADS).mean()\nFile \"/Users/andre/miniconda3/envs/py2/lib/python2.7/site-\npackages/lightfm/evaluation.py\", line 118, in auc_score\nnum_threads=num_threads)\nFile \"/Users/andre/miniconda3/envs/py2/lib/python2.7/site-packages/lightfm/lightfm.py\",\nline 649, in predict_rank\nitem_features)\nFile \"/Users/andre/miniconda3/envs/py2/lib/python2.7/site-packages/lightfm/lightfm.py\",\nline 235, in _construct_feature_matrices\nassert self.user_embeddings.shape[0] >= user_features.shape[1]\nAssertionError\nSo I thought, maybe the shapes of the two sparse matrices are different\nbut as it turns out:\nmodel.user_embeddings.shape[0] = 24\nuser_features.shape[1] = 24\nNot really sure what's wrong, because the boolean value of the assert\nstatement should evaluate to True in this case.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/108, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA8HjUXzmsvUdiUCnLdp0s4jxFhg6ks5qrtbBgaJpZM4KA0rt\n.\n. This is because your data is a double (np.float64), and it expects a float.\n\nArguably there should be a piece of code that does the casting automatically if needed. I'd be happy to accept a PR to that effect.\n. Correct, this is the right approach. Think of your items as your users and you can use the model as-is.\n. This method is to be used when you're only evaluating a couple of interactions for every user, as is most common in evaluating models. It is quadratic in the number of nonzero interactions per user, so if you want to evaluate more items than that I suggest you use predict instead.\nI updated the docstring to reflect this in https://github.com/lyst/lightfm/pull/111\n. Looks great. There's something wrong with Travis but it seems unrelated.\n. Thanks! Yes, it looks like Travis has changed.\n. Thanks for the report. I'll attempt to reproduce and find a fix.\n. The fix will be pushed to PyPI in the next release of LightFM.\n. Fix included in the 1.10 release, available on PyPI.. Multi-class applications aren't terribly common in recommender systems, but I'm sure you could emulate this via suitable one-vs-rest wrappers.\n. The values returned by predict do not have a defined range. If you'd like for them to be bounded by 0 and 1 I would suggest post-processing them, for example via the sigmoid function.\n. You might want to look into unsupervised dimensionality reduction techniques, like PCA/LDA or autoencoders.. I think what's happening is that the sigmoid function is simply not applied to the outputs.\nArguably it should be, but I'm hesitant to make this fix as it would change the public API in a way that's not backwards-compatible.. What are the values in the feature matrices? Have you tried normalizing them (for example to be between 0 and 1)?. Just for my understanding: what does your indexing items_features[0,items_features[0].nonzero()[1]] accomplish?\nAre you expecting these matrices to be dense?. Cool, I understand.\nCan you try reducing the learning rate and/or reducing the scale of the nonzero items even further?. You could try turning off regularization as well to try to narrow the problem down.. You're also using a lot of parallelism: this may cause problems if a lot of your users or items have the same features.\nLet me know what you find!. Any luck?. Can you try the newest version (1.12)? It has numerical stability improvements which may resolve your problem.. Hmm. I may have to have a look at your code and data. Can you email me at lightfm@zoho.com?. It would be best if you could reproduce the problem using synthetic data (or a subset of your data that you don't mind sharing).. Is this still a problem? I'd really like to help if it is!. No, this shouldn't be the case. My first suspicion was that I don't zero the representation buffers, but they are zeroed.. If you can construct a minimal test case that manifests this problem, I would be happy to have a look and solve this.. No worries, glad to see you found a solution.. I'm afraid the Movielens dataset does not incorporate user features.\nYou can use item features, however: http://lyst.github.io/lightfm/docs/datasets.html#lightfm.datasets.movielens.fetch_movielens. I'd be open to adding this additional data to the helper functions if it's there in the original data. Please submit a PR and we can have a look.. You can pickle it like any sklearn model: https://github.com/lyst/lightfm/blob/master/tests/test_movielens.py#L437. But the advice on pickling stands: https://github.com/lyst/lightfm/blob/master/tests/test_movielens.py#L476. This assertion is meant to prevent you from passing interaction matrices of incompatible shapes into fit and predict. In this case, I think your test matrix has a larger first dimension than your train matrix, and so there are some users in the test set that were not in the original train set.\nCan you check that this is not the case?\n. Arguably the library should be more informative about what's wrong. I'll look into providing a better error message.. Can you:\n1. Check where it's looking for the data (the zip_path above)?\n2. Check that the file at that path is a valid zip file?\n3. Try deleting that file and its directory and run the command again.\nMy first suspicion is that there might have been a problem with the download and you have a partially completed file cached in the data directory.. Any luck?. ?. Are you saying that the code doesn't work? Could you post the trace for the error you encountered?. Can you point out the difference between the two? I don't quite see it!. Are you use? Python uses what's called implicit string concatenation; these quotes are simply dropped in the final string. Your changed code is syntactically identical.. This is not a bug in the code. Your download was corrupted for some reason. Delete the downloaded file and the code should work again. The advice above (wrt changing paths) is incorrect.. This problem is solved in the newest release, 1.14.. It's possible that the logistic and BPR losses rely more on item popularity (encoded in item biases) than WARP, and so the head and the tail of the list will have a large overlap between users. Can you check if the  recommendations start changing beyond the first 10 or 20?. I confirm there is an issue with the BPR loss. I will be pushing a fix this week and will keep you posted.\nIn the meantime, please use the WARP loss.. Out now, v1.11.. How did you install LightFM? What directory are you running this code from?. No. But doing so is one explanation for import problems.. If your cloned lightfm code is in your import path but you installed into site-packages, you will have problems.. Any luck?. Closing because of the other issues you opened.. You need to install Python development headers: sudo apt-get install python-dev. Thank you for all the issues. Have you tried pip install lightfm within your Conda virtualenv?. I'll go ahead and submit official packages to the conda repositories in the next couple of days.. Have you got an up-to-date version of pip? It might be helpful if you post the entire pip output.. It looks like you're trying to use the MSVC compiler and failing: error: command 'cl.exe' failed: No such file or directory.\nI don't have access to a Windows machine so I'm afraid I will be of limited help in debugging this.. I will look at building and distributing binary wheels for LightFM. In the meantime, could you try installing the Microsoft Visual C++ compiler?. It looks like there is something wrong with your miniconda installation. Try installing scipy from conda?. Are you sure? It doesn't look like you have any linear algebra libraries\nwhich makes me doubt you even have numpy?\n. Correction: you seem to have numpy but without atlas/blas/mkl, which is\nunusual. Try conda install scipy.\nOn 16 Jun 2017 8:53 pm, \"Maciej Kula\" noocrat@gmail.com wrote:\n\nAre you sure? It doesn't look like you have any linear algebra libraries\nwhich makes me doubt you even have numpy?\nOn 16 Jun 2017 8:00 pm, \"Rakshith Vasudev\" notifications@github.com\nwrote:\nIt has already been installed. Conda did all the pre req installation when\nI wanted to install Sklearn.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/139#issuecomment-309108480, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA9KoObBSQpAbH5f8EHNkUFGCZUtUks5sEtDjgaJpZM4LUg-m\n.\n. You can also try downloading the binary wheel from here (assuming you use Python 3.6), and installing it by running pip install lightfm-1.13-cp36-cp36m-win_amd64.whl.. Hey @Mohammed-Maamari, have you Googled it? Do those solutions not work for you?. Can you check if it's a valid zip file?. This sort of error appears when you have cloned into LightFM, installed it into your virtualenv or site-packages, then try to run tests from a path that has the cloned code in its import path.\n\nTry installing it from the cloned repo by running pip install -e .. Any luck?. BPR always chooses one negative example per positive example.\nWARP also attempts to find one negative example per positive example, but sampled negative examples that are ranked lower than the positive examples are discarded. The algorithm then tries again up to max_sampled times; if that fails, it just moves on to the next positive without performing a gradient step.. Great, thanks for the fix. I need to set it up in a way that makes it impossible for me to mess up again.. The build is failing (I think) due to my sloppiness with float indices. I'll fix that separately.. My suspicion is that in this case your learning rate is too large and the\nmodel diverges. Can you check the actual predicted scores?\nOn 20 Jan 2017 14:09, \"Eric Scott\" notifications@github.com wrote:\n\nI'm running parameter optimization on a LightFM implicit factorization\nmodel using \"warp\" loss. When I run np.mean(precision_at_k(...)) on the\ntest data, it is virtually always the same result, 0.316850870848, out to\n12 digits. Is this expected for some reason? If not, any ideas how to\nfigure out what might be wrong?\nHere is sample output showing the param combinations and the output:\n2017-01-20 21:10:02,363 INFO main: Starting training iteration 1,\nparams:\n{\n\"alpha\": 0.0041233076328981919,\n\"epochs\": 45,\n\"learning_rate\": 0.50174314850490254,\n\"no_components\": 184\n}\n2017-01-20 21:10:02,363 INFO lightfm_uma: Training model...\n2017-01-20 21:25:20,518 INFO lightfm_uma: Finished precision@6 =\n0.316850870848\n2017-01-20 21:25:21,453 INFO main: Starting training iteration 2,\nparams:\n{\n\"alpha\": 0.0064873564172718886,\n\"epochs\": 63,\n\"learning_rate\": 0.50406151543722921,\n\"no_components\": 180\n}\n2017-01-20 21:25:21,453 INFO lightfm_uma: Training model...\n2017-01-20 21:44:36,565 INFO lightfm_uma: Finished precision@6 =\n0.316850870848\n2017-01-20 21:44:37,495 INFO main: Starting training iteration 3,\nparams:\n{\n\"alpha\": 0.020473212242717205,\n\"epochs\": 62,\n\"learning_rate\": 0.74135691825946459,\n\"no_components\": 156\n}\n2017-01-20 21:44:37,496 INFO lightfm_uma: Training model...\n2017-01-20 22:00:45,661 INFO lightfm_uma: Finished precision@6 =\n0.316850870848\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA57zLjrU7uDILZIBSZbaNkw5hNWWks5rUTC0gaJpZM4LpzXz\n.\n. Actually, I think this is a better explanation: you have high regularization parameters, and everything is regularized away to zero. All the predictions are zero too, and the resulting precision values are nonsensical.\n\nThis would change that to be less misleading: https://github.com/lyst/lightfm/pull/149. This is an artifact of how rank is computed when there are ties. When everythig is tied (the 0.0 zero case), all positive examples have rank one. The PR I linked to above (https://github.com/lyst/lightfm/pull/149) fixes this and would make your precision 0.0. Which I guess isn't the right solution either, but at least it indicates that your model has a problem.. All SGD algorithms are sensitive to hyperparameters. NaNs are most often the result of excessive learning rates. Arguably I should warn when this happens, it can be easy to miss.\nCan I suggest you pull the branch I linked above and use it for your optimization? It will have the advantage of giving you terrible precision scores when you algorithm has actually diverged or regularized itself to zero.. Lower than 0.1 is probably a good idea. However, this also depends on data, and on the scale of the features you are using.. I would do the following:\n\nStart with a known faulty configuration.\nStart eliminating other confounding factors, like number of epochs or regularization.\nStop when the problem disappears.\n\nThat should give us the culprit. To help, I made the model throw an exception when it diverges: https://github.com/lyst/lightfm/pull/151. Thank you for all the work digging into this!\nSome assorted thoughts:\n1. Whether the fitting diverges or not can be non-deterministic. You may\nwant to fix the random seed and disable parallelism to get repeatable runs.\n2. You can insert any Python code into the Cython files. You will have to\ndisable OpenMP to do that, though. Some print statements may show you the\nevolution of parameter values as you're fitting the model.\nIn general I agree that your parameter values look sensible and the\nalgorithm should work with these values.\nOn 23 Jan 2017 11:25 am, \"Eric Scott\" notifications@github.com wrote:\n\nSeveral new points:\n\nThe dev version doesn't diverge on several of the cases where 1.11\n   does diverge. So the dev version has some improvements that avoid some of\n   the divergences.\nI found one combination where the model still diverges. It diverges\n   on item_embeddings going +/-inf. I don't see an obvious cause of that, but\n   you guys are the experts.\n\nHere's the param combo that still causes divergence:\n{\"alpha\": 0.07356951575008387, \"epochs\": 1, \"learning_rate\":\n0.054121552105164714, \"no_components\": 48}\nThese all look reasonable to me. Am I missing something? And this combo\ncontinues to diverge until I reduce the learning rate to 0.0005. That's an\nextremely small rate, isn't it?\nAnd here's the session showing the +/- inf:\n(Pdb) l\n320 for parameter in (self.item_embeddings,\n321 self.item_biases,\n322 self.user_embeddings,\n323 self.user_biases):\n324 if not np.all(np.isfinite(parameter)):\n325 -> raise ValueError(\"Not all estimated parameters are finite, \"\n326 \"your model may have diverged. Try decreasing \"\n327 \"the learning rate.\")\n328\n329 def fit(self, interactions,\n330 user_features=None, item_features=None,\n(Pdb) parameter.shape\n(2001, 48)\n(Pdb) self.item_embeddings.shape\n(2001, 48)\n(Pdb) np.isnan(parameter).sum()\n0\n(Pdb) (parameter==np.inf).sum()\n46\n(Pdb) (parameter==-np.inf).sum()\n72\n(Pdb) (parameter==0).sum()\n95930\nThe +/- inf happens in the first call to fit_warp(). I can't seem to debug\nthis further since the code is all .pyx -- unless there's magic to do that.\n(I've never debugged at the pyx level.)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148#issuecomment-274590352, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAzhbZ_BECkmEjMHDX2msFeuyWQEvks5rVP6OgaJpZM4LpzXz\n.\n. Yes. All the threads write to a shared memory buffer, so results will very\nfrom run to run if you have more than one thread.\n\nOn 23 Jan 2017 12:13 pm, \"Eric Scott\" notifications@github.com wrote:\n\nI already fix the random seed, so I was hoping it is deterministic. Does\nparallelism introduce non-determinism also?\nI'll try the good, old debugging by print and see what I find...\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148#issuecomment-274603410, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA1nrwLusD68-BAhuSj4IzbCCFz3zks5rVQn2gaJpZM4LpzXz\n.\n. I think you're onto the disadvantages of the way I implemented lazy regularization here. It inflates the values of certain features, then divides them by a global regularization value. With high regularization rates this can run away to infinity. \n\nIn other projects I have found a better way of doing this which should be more stable numerically, but requires a lot more memory. I'll need to think about that.\nIn the meantime, can you try reducing the alpha values and see if the problem goes away? (Plus, there is a slight bug in the implementation as well. I'll fix that.)\nAgain, thanks a lot for looking into this!. That would be great, I could then go away and hammer at it. A Dropbox link\nmaybe? If you'd like to send it privately you can contact me via my Twitter\naccount @Maciej_Kula or the LightFM gitter channel.\nOn 24 Jan 2017 09:16, \"Eric Scott\" notifications@github.com wrote:\n\nAnd BTW, I have a non-proprietary set of data and code to reproduce the\nproblem. Unfortunately the training data is 46MB compressed. If we can\nfigure how to pass it along I can share it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148#issuecomment-274871524, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAxEm6XlzaCCMkrs3WC1p5_G1yeU6ks5rVjG3gaJpZM4LpzXz\n.\n. I added a tentative fix for the runaway growth of regularization values in https://github.com/lyst/lightfm/pull/152\n\nThis forces a full regularization cycle mid-epoch if necessary, and I think this will resolve your problem.. This is expected - at high regularisation rates full regularisation passes\nare triggered repeatedly during an epoch, holding up all the threads until\nthe work is finished. I can probably optimize this a little more.\nThe result of this change should be that:\n1. There should not be any performance implications with low regularisation\nrates.\n2. When regularisation rates are large, the algorithm will be slow but\nnumerically stable (as opposed to fast but producing nonsense).\nOn 25 Jan 2017 09:43, \"Eric Scott\" notifications@github.com wrote:\nIt's a good news, bad news thing:\n\nGood: The #152 https://github.com/lyst/lightfm/pull/152 PR does seem\n   to have fixed the divergence problem.\nBad: It also seems to have crippled performance.\nGood: Crippling performance seems to happen mostly when model starts\n   to diverge, or gradient goes to 0. Small performance hit w/o openmp.\n\nWith openmp and 16 threads, the divergent param combo is barely faster than\nno openmp and 1 thread. Here's 16 threads:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py\n2017-01-25 16:30:17,882 INFO main: Training model...\nEpoch 0\n2017-01-25 16:36:27,548 INFO main: Finished training model\nTakes 6min, 10sec. Here's 1 thread, no openmp, but on my laptop, not AWS:\n$ ./lightfm_optimize_prob.py\n/Users/escott/projects/lightfm/lightfm/_lightfm_fast.py:9: UserWarning:\nLightFM was compiled without OpenMP support. Only a single thread will be\nused.\nwarnings.warn('LightFM was compiled without OpenMP support. '\n2017-01-25 10:17:55,189 INFO main: Training model...\nEpoch 0\n2017-01-25 10:24:02,712 INFO main: Finished training model\nTakes 6min 7sec. Here's 1 thread, with openmp on AWS:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py -threads 1\n2017-01-25 16:45:17,436 INFO main: Training model...\nEpoch 0\n2017-01-25 16:52:12,090 INFO main: Finished training model\nTakes 6min, 55sec.\nNow let's look at a non-diverging, gradient doesn't go to zero, param\ncombo. One thread, no openmp:\n$ ./lightfm_optimize_prob.py\n/Users/escott/projects/lightfm/lightfm/_lightfm_fast.py:9: UserWarning:\nLightFM was compiled without OpenMP support. Only a single thread will be\nused.\nwarnings.warn('LightFM was compiled without OpenMP support. '\n2017-01-25 10:50:20,861 INFO main: Training model...\n...\nEpoch 19\n2017-01-25 11:01:26,286 INFO main: Finished training model\nTakes 11min 6sec. This used to take 10min, 30sec before #152\nhttps://github.com/lyst/lightfm/pull/152.\nNow 16 threads and openmp on AWS:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py\n2017-01-25 16:59:48,265 INFO main: Training model...\n...\nEpoch 19\n2017-01-25 17:03:57,042 INFO main: Finished training model\nTakes 4min 9sec. This isn't significantly slower than it used to be, that I\ncan tell.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148#issuecomment-275178621, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA7WyuqZluNBGQCBBuckof5Rjjfr9ks5rV4mxgaJpZM4LpzXz\n.\n. I fixed an embarrassing bug with the regularisation, so those results may\nunfortunately need re-computing. I don't believe the change will be large,\nbut it may be worth checking.\nOn 25 Jan 2017 11:16, \"Eric Scott\" notifications@github.com wrote:\n\nOne other question then about #152\nhttps://github.com/lyst/lightfm/pull/152... Are the results I have from\nparam optimization on version 1.11 still valid for master+PR152?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/148#issuecomment-275204452, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAzTmx9zx1uJcCRuX8jUVsrcFDAvYks5rV5-agaJpZM4LpzXz\n.\n. I think this is fixed in 1.12 --- thanks for your help!. Yes, you can. LightFM accepts standard scipy.sparse matrices as input.. 1. Train and tests sets have to have the same dimensionality. In your example you have fewer users (30k vs 89k) in the test set, and so user 0 in the test set may not map to user 0 in the training set and so on.\n2. In the binary case, you can interpret AUC as the probability that a randomly chosen positive example will be ranked higher than a randomly chosen negative example. Precision at 3 tells you what proportion of the top 3 results are positives. In your case where you have far more than 3 items, it's perfectly possible for positives to be ranked correctly overall (high AUC), but not make it to the top 3 (say, putting your positive at position 4 will give you 0.99 AUC but 0 precision@3). The results you are getting look sensible to me.\n3. The prediction scores generally do not have an interpretation. They are simply a means of ranking the items.\n\nHope this helps!. 1. That's because the model estimates a latent representation for user 0. If you tried to use that representation for any user other than 0 the results may not be very good. \n2. This depends on the training method. In general, negative examples are sampled from the set of items where no interaction was observed. You can read the BPR and WARP papers (linked from docs) for more details.\n3. Biases encode mostly item popularity. It looks like in your data popularity is extremely important? In general I'd treat a result like this with extreme suspicion. Your estimation or evaluation may well be faulty.. Closing this for now. Let me know if you have any more questions!. @kyowill LightFM does not predict movie ratings. It is, by and large, an implicit feedback model.. For every non-zero (user, item) pair in test, it predicts the position of that item in that user's recommendation list (with smaller numbers being closer to the top of the ranking).\nSo if you have a non-zero entry in test at (10, 150), the corresponding entry of the result will give you an integer expressing the position of item 150 in user 10's recommendation list.. Yes. The (ordered) list of all items.. It gives you the position of item X in a list where you ordered all items by how strongly they are recommended (so yes, 42 is 'goodness', and smaller numbers are better, because they are closer to the head of the list). Of course, if you are actually serving recommendations, you should use the predict method (which will give you a score where higher is better).\nRecommendations are a ranking problem, not a classification problem, so things like thresholds don't really make sense. The idea is that you have, say, 4 slots to fill in your interface, and you sort all products by their recommendation score and pick the top 4; you never set a threshold.\nReading the references is a good start. The BPR paper is a classic. There is also this but I can't say anything about how good it is.\n. Looks good, thanks for the fix. Could you maybe add a regression test?. I'm going to merge this and add tests in a later PR.. The current documentation says:\n```\n        \"\"\"\n        Fit the model.\n    Arguments\n    ---------\n\n    interactions: np.float32 coo_matrix of shape [n_users, n_items]\n         the matrix containing\n         user-item interactions. Will be converted to\n         numpy.float32 dtype if it is not of that type.\n    user_features: np.float32 csr_matrix of shape [n_users, n_user_features], optional\n         Each row contains that user's weights over features.\n    item_features: np.float32 csr_matrix of shape [n_items, n_item_features], optional\n         Each row contains that item's weights over features.\n\n```\nThis is relatively close to what you suggest --- how would you improve it? I'm happy to have a look at PRs.. This blog post has a good section on how to build the feature matrices: http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/\nIs this something you'd like to see documented more explicitly?. I will absolutely accept PRs on this :). Thanks for the suggestions. I think the docs are much better now!. Awesome. Could you submit a patch that does this automatically? Say, tries 3 times before giving up if the file is invalid?. It already does, via keyword arguments to the fit function.. I'm afraid that's not in the works. I think you will find LightFM on a big machine is already fast enough for most use cases.. That is true. But I have no plans to support this.. The model operates in terms of features: so it knows about 1246 features, the linear combinations of those can form any arbitrary number of items.\nWhen you pass in the item_features, your item index indexes that matrix, then looks up the features in that row (so one or more of the 1246) in the embeddings matrices.. Addressed in https://github.com/lyst/lightfm/pull/167. No, that is not correct.\nIf you supply a feature matrix, the i-th column of that matrix has a latent representation in the i-th row of the embeddings.\nIf you do not supply a feature matrix, an (num_items x num_items) identity feature matrix is generated for you.\nIf you'd like to obtain the behaviour your describe, you need to supply a feature matrix of the appropriate structure.. The latent representation for the user/item represented by the i-th row of your feature matrix is given by the product of that row and the user/item embedding matrix.\nSo if you'd like to get the latent representation for user i, you'd take the i-th row (1 x num_components) of the user feature matrix, then multiply it with the (num_users x num_components) user_embedding matrix to obtain a (1 x num_components) latent representation for that user.\nIs this what you're looking for?. That's right!\nNo worries, happy I could help.. All arguments with default values are optional, yes.\nI'm not sure what you mean by 'fitting directly'? User and item features can have different cardinality.. This is not how this model works. There is no assumption that the user features and item features are the same.. Well, you'll always need interactions. Otherwise what do you fit your model on?. I think the backend can be switched in seamlessly without changing the external API when the feature is added.\nBefore I add precompute in the C code I'll probably want to refactor it to not use the same buffer for latent vectors and biases.. Thanks!. This looks great, thanks a lot.\nI keep to (and enforce in the lints) a 100-column line width, but these changes look good anyway, except in those places where they fail the linter. Please have a look at the CircleCI build; you should also be able to run the linter locally by running flake8 --config .flake8rc.\nYou could, if you like, add yourself to the contributors file as well.\nCan you explain what you mean about validity of tests?. Not entirely sure what you mean by your comments re: tests. But the changes look good, thanks!. You can combine the explicit and implicit feedback data in the model by giving higher weights to interactions that have explicit likes. This is somewhat crude, but should work. I don't have a firm opinion on how to treat feature likes.\nA more principled approach to implicit + explicit data is something like this paper. I have a related implementation here via the truncated regression loss. In your case, you'd treat the implicit likes as the truncation variable and explicit likes as the truncated ratings.. Copying my answer from #225: \nIf your hybrid model performs worse than a pure CF model, the following may be helpful.\n1. Make sure your features are appropriately scaled.\n2. If your features are continuous, consider discretization.\n3. Make sure you don't drop per-item or per-user features. Again, have a look at the notes section.\n4. Consider inverse-frequency weighting of your features.. In the feature matrix, give low values to common features, 1.0 / count_of_occurrence for example.. No, I am suggesting this for categorical features. Instead of inserting a 1, insert the weight.\nI imagine you could, depending on what task is important to you.. 1. Even if it is one-hot encoding, you can always insert some other value (0.5, 0.001) instead of the 1.\n2. I mean that you should use values other than 1 in the item/user feature matrices. If you look at scipy sparse matrices you will see that they have a data array that holds the values for the non-zero entries.\nFor a general overview of inverse frequency weighting, you could read about the TF-IDF technique.. You can have a look here for example: http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/. I've added splitting utilities in the master branch: https://github.com/lyst/lightfm/blob/master/lightfm/cross_validation.py#L20\n@dofine sorry for the late response.\n1. This is simply the nature of implicit feedback recommender systems. The model are trained and tuned to recommend those items with which a user does not interact in the training set but does interact with in the test set (likely, the recommendation score is lower than known (that is, present in training) items, but should still be high).\n2. You could do this. Ultimately, you want your train/test split to reflect the conditions your production system is going to face.\n3.  If features change you will probably want to have multiple rows per user (representing views over that user over time).. Bear in mind that if you don't pass features in, the model assumes one indicator feature per user/item. If you do pass them in, the model takes features as provided: so if your user features do not contain per-user features your resulting model may well be less expressive.. You could make it rectangular with 14015-14016 columns by horizontally stacking your features matrix with an identity matrix.\nRMSE isn't really the right metric for evaluating recommender systems. You should use ranking metrics like AUC, MRR, or nDCG.. Great!\nThe model should perform well with any reasonably sized dataset, easily scaling to 20-40 million interactions.. You can supply arrays of user and item ids that reflect all the user-item pairs you have in your dataset. If you are using COO matrices, these will be the row and col arrays.\nYou may want to have a look at the documentation or docstrings. You simply pass a new training matrix into  partial_fit, and model training will resume on the new data from the state earlier training stopped.. There was some discussion about this here.\nThe gist of it is that you should preallocate dimensions for however many items/users you expect in the future. You can then pass in matrices which have data on users you had no data on before.\nThe matrices must always have the same dimensions, since users and items are identified by their row and column indices.. 1. The sample weight affects the size of the gradient step taken for a given sample during fitting.\n2. This is an implicit feedback model (for BPR and WARP losses). 1s are positive, 0s are negative. You should not be using ratings.\n3. The sigmoid function is not applied during prediction. Only the ranking and not the actual scale of the scores matters.. You can make it context aware by using suitably constructed item or user features. For example, you can add (quantised and one-hot-encoded) time of day to your item features in addition to any other features your item have.. Yes, that's correct.. This is a tiny dataset, try running for more (say, 2000) epochs. Then you should see the result you expect.\nRegarding your other question: I designed the library to allow using different matrices at train and predict times.. Sorry, what's the second question?. 1. This way of splitting the matrix is not going to work. The usual sklearn tools split row-wise: this will of course not work here, where the entire point is taking some interactions from every row and putting them in the test set. Have a look at this blog post series to see an example of how to do the split.\n2. Make sure your train and test matrices have the same dimensions. Having more users or items in the test set is nonsensical, as you would not have estimated representations for them in the training phase.\n3. A sparse matrix representation is the right thing to do. I would do an implicit version with 1 entries for positive interactions are zeros everywhere else, and use the BPR or WARP losses.. Handled in https://github.com/lyst/lightfm/pull/187. Can you check the scores that are returned? There was a related issue where if the same score was returned for every item precision/recall scores were nonsensical.. 1. You must not use 'null' values (I'm guessing you mean NaN?). This makes me suspect that all of your scores are NaNs, hence the strange result.\n2. Features must be normalized, as in any other SGD model.\n. No worries. More checks in https://github.com/lyst/lightfm/pull/188. 1. LightFM is a stochastic gradient descent model, and so the model parameters will vary slightly from run to run, unless you fix the random seed\n2. If you're seeing large variations, chances are something is wrong with your model.\nTo select the right values, I would define an evaluation function and run a hyperparameter search, for example using sklearn's ParameterSampler.. Yes, this is correct. This is a helper function for computing various ranking-related evaluation scores where you need to know the position (in the full ranked list for any given user) of the test interactions.. 1. Yes, LightFM works best with sparse feature vectors. You could have a look at the Stack Exchange example, which uses sparse bag-of-words features.\n2. Correct.\nI would also recommend experimenting with indicator features: a feature that's 1 only for one item/user.. Do you have per-user binary features? Do you have any features that lots of users have in common?. You could try excluding features, either individually or in groups, and measure the performance changes?. Do you have per-user features, that is, is an identity matrix part of your user_features?\nIt's no a prior worrying for scores to be negative. The values themselves have no interpretation.. This is what I mean by 'per-user binary features': it if often useful for each user to have a feature that's one for them and zero for everyone else. This is in fact what happens if you do not pass any features in.. Yes, I think that's worth trying.. Glad I could help. It would also be great you'd like to contribute some of that to the documentation or the examples!. You should use the Dataset functionality included in the latest version of LightM.. Closing for now; might use conda-forge instead.. It's probably because your import paths are wrong. You are trying to import from /home/finlay/lightfm/lightfm/ (which you shouldn't have if you installed via pip and not cloned the repo) but the compiled extensions got installed somewhere else (in your virtual env or site packages). \nIf you want to run tests, pip install -e . is the right way. If not, delete the lightfm directory and everything should work.. LightFM works with Python 2.7 too; I am convinced this was some misconfiguration of your Python installation/Python paths.. No worries. I'm glad it worked out.. The features you are looking for are cold fold-in. Unfortunately they are not supported at the moment, but I am thinking of adding them as they are incredibly useful in many settings. For the moment what you suggest in (1) is the best you can do.. Not at the moment. I am working on a possible v2 which will feature fold-in, but I don't expect for this to be available soon.. Getting OpenMP working on an OSX machine is a challenge.\nI would recommend running LightFM in Docker.. This is the expected behaviour on Windows: to get multithreading to work, run it in Docker or use Linux.\n(Alternatively, you could try compiling with with an OpenMP-enabled C compiler. I don't know how to do that, so I can't help there.). User ids are row indices into the features matrix: the features matrix you pass in has to have at least as many rows as you have users. You can have a look at the docstring.. Python indexing is 0-based. By including a user id of 1, you are trying to index into the second row of the features matrix. But, as you say, you have only one row.. From your first example it looks like whatever you pass into predict has\nonly one row?\nOn 12 Jul 2017 08:50, \"kyowill\" notifications@github.com wrote:\nWhat I mean a user id of 1 is a index in some set, and\n'user_features_concat[1]' is one of the row in features matrix which\ncorresponding to the user id of 1. User id of 1 is the second user. feature\nmatrix has many rows.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/196#issuecomment-314684452, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAz2Ttb1rW776QGEISKzlh36Bm4IHks5sNHrFgaJpZM4OTy-Y\n.\n. That's because user ids are indices into the feature matrices. In this case you are trying to index the second row of a matrix with only one row.\nJust pass the entire matrix and it will work.. I think aggregation should be fine. You may also want to explore encoding your hierarchy in item features: for example, top level hierarchies may be columns 0 to 1000, sub-level 1000 to 10000 and so on.. It's just feature engineering, really. You'll have features which denote categories/sub-categories etc. Because they are one-hot encoded it ends up looking a little that way.. If there are 241 distinct tags for item features, how is the column dimension of item_features 6918? Shouldn't it be 241? What is happening is that you most likely fit embeddings only for 241 features, but then supply a features matrix with far more features later.. @chrisbangun I opened a PR to improve the documentation on this point: https://github.com/lyst/lightfm/pull/206\nLet me know if that answers your question --- if not, I'll try to expand until it's clear.. Great! Let me know if you have any further questions.. Yes. Good spot.. NaNs are absolutely not allowed. I already check most possible inputs for NaNs and generate appropriate errors, but apparently not this bit yet.\nI'll add that check in a PR later today.\nMissing interactions should be represented as zeros in your sparse interaction matrix.. Also, LightFM is primarily an implicit feedback algorithm: it doesn't use ratings. If you think ratings are important, you could consider using them as weights.. Correct; take care to remember that these are weights, not ratings, and the choice of their magnitude will affect convergence.. Checks merged in https://github.com/lyst/lightfm/pull/200. This could be for many reasons. Two leading ones:\n1. If you supply explicit item/user features it only uses the features you provide: that is, a per-user and per-item feature is no longer used. If you'd like to retain your behaviour you need to concatenate your feature matrices with an identity matrix.\n2. You may need to do a hyperparameter search.. If you don't provide any features, LightFM uses an identity matrix. This means that every user and every item get a single features that is unique to them.\nIf you do supply features, LightFM just takes them as given. This may mean a reduction in accuracy if your features are less expressive. If you stack things, you should do it column-wise: your output will be of shape (num_items, num_items + num_metadata_features).\nThis is currently under-documented and is a frequent point of confusion. I will try to expand the docs soon.. Sounds good.. Added some more explanation to the docs: https://github.com/lyst/lightfm/pull/206\nAny suggestions on how to make it clearer?. Yes, that would be useful. I'll make that change.. predict_rank will return a (num_users x num_items) matrix, where nonzero (i, j) entries contain ranks of item j for user i. You specify which these should be by passing in a test_interactions matrix.\nIn general returning a dense matrix of this form is infeasible: for many problems, it would require a ludicrous amount of memory. If that's what you'd like, I'd recommend just iteratively calling predict for every user.. The scores themselves have no meaning in isolation; they are only meaningful because they define a ranking over items for a given user. The scale they take depends on the loss you specify, the learning rate, the regularization parameters, and the data itself.\nI recommend keeping an eye on the MRR/AUC scores of your model, and comparing them with what a random or a popularity model would achieve.. You can use implicit feedback (1s/0s) for the WARP and BPR losses, and explicit feedback (1s and -1s) for the logistic loss.\nWhich one is better depends on your application. I would define an objective function that works for you, and evaluate both. For example, you may want to balance ranking positive items highly (good) and ranking known negative items highly (bad), and select model hyperparameters based on that.. If you're model's performance is constant regardless of what you do that suggests there is something wrong with the model.\nAre the predictions NaNs? Are they all zeros?. Coordinates (that is, which user interacted with which item) are definitely key.. You should definitely scale your input data. Just experiment with getting the right value; between 0 and 1 is a good start.\nThis is not as much about overfitting as about making sure the training doesn't diverge: inputs that are large will likely lead to poor fit or outright numerical failures.. The three metrics you mention measure different things, so it's perfectly natural for a successful model to have different AUC, precision, MRR, and recall scores. I would look at their Wikipedia definitions to get a feel for how to interpret them; for what it's worth, your models' scores look fine!. You can simply use Python's pickle once you've fitted your model:\n```python\nimport pickle\nmodel = LightFM()\nmodel.fit(data)\nwith open('savefile.pickle', 'wb') as fle:\n    pickle.dump(model, fle, protocol=pickle.HIGHEST_PROTOCOL)\n``. The scores in LightFM define the ranking: the higher the scores is for a given item, the more likely it is to be recommended. To get the top items, you should score all items and then retrieve the ones whose scores are the highest (this holds even when the scores are negative). TheXrin your terminology is exactly what is output by thepredict` method.\nIn the binary case (clicked/not clicked), the AUC score has a nice interpretation: it expresses the probability that a randomly chosen positive item (an item the user clicked) will be ranked higher than a randomly chosen negative item (an item the user did not click). Thus, an AUC of 1.0 means that the resulting ranking is perfect: no negative item is ranked higher than any positive item. Yes, they are treated as binary.\nOn 1 Aug 2017 9:51 pm, \"Osman Ilyas\" notifications@github.com wrote:\n\nSo are all the ratings treated as binary for the AUC score in LightFM? I\nam using implicit feedback so there are counts of each interaction the user\nmade with the items.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/208#issuecomment-319493133, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA7puAVOm1b2DiKpqjpfBNLK0BsVBks5sT4_ZgaJpZM4OoRUo\n.\n. Please have a look at my paper for the full formulation. You may also find my recent addition to the documentation helpful.. Closing this for now. Let me know if you have any more questions.. You can just instantiate the model and replace the existing attributes. It might be a bit fiddly.\n\nThis is probably not a very common use case, so I don't think I'll be adding any functionality to support it.. Can you have a look at the updated documentation for user and item features?\nThe gist of it is that user and item ids are simply indices into rows of their feature matrices: if you supply feature matrices that have features, you are using features, and the user ids are only a means of telling the model which row of the features matrix it should use.. 1. It is usually better to provide it, because then the model will be more expressive: it will be able to express preference for every individual user. But sometimes this is counterproductive (data is too sparse) or impossible: you want to always predict for new users for whom you have no historical data (cold start).\n2. Given the same features, predictions will be the same. It doesn't matter which row it's in: all that matters are the features in the row. If you use the identity matrix, this is of course impossible: every row is different.\nHope this helps!. You're close. Remember, user and item ids are indices into their respective feature matrices. In this case, your item matrix has three rows, so you'd need to do:\npredict(0, item_ids=[0, 1, 2], item_features=[11's features, 12's features, 13's features], user_features=[D.features]). Can you try changing the Dockerfile to run pip install -e . instead?. Can you have a look if https://github.com/lyst/lightfm/pull/212 solves the problem?. One of the dataset tests is highly memory intensive. You are simply running out of memory.\nWhat is the movielens issue?. @HarlanH I've updated the instructions for launching a notebook server.. Great!. Either should work well. Normally, a hyperparameter search will tell you which one performs better for your problem.. I think making hyperparameter optimization for LightFM is a great idea!\nI would love to have an example that goes through the process of hyperparameter search, maybe using a couple of different approaches and libraries. However, because there are different ways of optimizing hyperparameters, I think it may be best to keep the specifics separate from the core of the library, and let the users figure out how they want to do it.\nWould you be open to converting this to an example?. Great! Maybe a Jupyter notebook we could include along with the existing\nexamples?\nOn 28 Aug 2017 14:11, \"Harsh Gupta\" notifications@github.com wrote:\n\nSure! I'll put it in the doc.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/pull/214#issuecomment-325336407, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA4oiuqO-Jg2SoUN3xP66YhxnQJb_ks5scq5ngaJpZM4PD9i3\n.\n. Hey @harshgupta7 --- I'd still love an example!\n\nI'm going to close this for now, but let me know if you'd like a hand with the example.. The most common practice is to make sure that the training and the test set are disjoint: no interactions should be shared between the two.\nThat way your evaluation gives you a better measure of the model's generalization performance: its accuracy on data it has not seen during training.. @iozzyi https://github.com/lyst/lightfm/pull/222. Out in the newest version.. These are all good questions. My responses below.\n\nYou do not need to make the contiguous and starting at 0; the model will work find. However, it will allocate parameters for indices in-between that you don't use, which will make your model larger (and potentially more noisy).\nYes, you can, subject to the same caveat as above.\nUser ids here are indices into the feature matrices. So if your user id is 100, your feature matrix will need to have at least 101 rows; but if you make the user id 0, you can have a feature matrix with a single row.\nWith the values you provided, n_users = max([1, 1, 20, 20, 100]) + 1 = 101.\n\nUsers and items are symmetrical in this model, so this applies equally. You may find my recent changes to the documentation helpful here.. 1. That's correct.\n2. That is also correct.\nAgain, the ids are just indices into rows of the feature matrices.\nHope this helps!. A dataset of this size should be easily runnable on a laptop in a couple of\nhours.\nWould you mind giving some details on the hyperparameters you used, and\nalso the details of your cluster? LightFM is single-machine only, so I\nwould be curious to see how you mesh it with distributed computing. In\nparticular, are you sure that you are spending time in LightFM rather than\nshuffling data across the cluster?\nOn 15 Sep 2017 09:03, \"FassyGit\" notifications@github.com wrote:\nHi, thank you for composing such a useful model to us.\nI am recently working on this model, and I tried several datasets on\nFoursquare. Some of them worked well, but when using the large dataset, the\nmodel just kept running and gave no response. I have run it for almost five\ndays on a cluster.\nThe information of the dataset I used is as follows:\nThis dataset includes long-term (about 18 months from April 2012 to\nSeptember 2013) global-scale check-in data collected from Foursquare. It\ncontains 33,278,683 checkins by 266,909 users on 3,680,126 venues (in 415\ncities in 77 countries). Those 415 cities are the most checked 415 cities\nby Foursquare users in the world, each of which contains at least 10K\ncheck-ins. Please see the references for more details about data collection\nand processing.\nCan anyone help me on that?\nWhat is the size of your runnable dataset\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/217, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA73xn2XUHR4M_cRfa7DSKEWG0vwoks5sii9pgaJpZM4PYn0O\n.\n. Thanks for the code (by the way, you can use triple backticks ``` to delimit code blocks).\n\nSo you are running it on a single machine. That's good, I presume there is nothing like Spark that would make things unnecessarily slow.\nHow many cores does your cluster machine have? 4 cores looks low; you should set NUM_THREADS to the number of available cores\nIn LightFM, evaluation is much more expensive than training (because in evaluation a score has to be computed for every item, for every user). I see you are computing the AUC score on the entire training set: I would expect that this takes the majority of the time. Did you ever get to the end of fitting? I would suggest you time just fitting first, and then experiment with smaller evaluation sets.. Great, I'm happy it helped.\n\nAs for the evaluation: when computing the accuracy metrics, LightFM will score all items for all users that are present (have at least one interaction) in your test set. So the time it takes to do an evaluation is proportional to the number of users in the test set. If it's taking too long, try limiting the number of users that have interactions in the test set.\nOf course, this may reduce the value of your evaluation. However, beyond a certain number evaluating on more users has little benefit.. It's not possible to predict for users (or, more accurately using user features) that are not present in the training set.\nIt could be done using fold-in, but that's not currently implemented.. See answers above: this is currently not possible.. The problem lies in the fact that your matrices aren't really sparse:\n```\nIn [20]: train\nOut[20]: \n<3x5 sparse matrix of type ''\n        with 15 stored elements in COOrdinate format>\nIn [21]: test\nOut[21]: \n<3x5 sparse matrix of type ''\n        with 15 stored elements in COOrdinate format>\n```\nThis violates the (very reasonable) assumptions made by the metrics code, and nonsense results.\nYou can actually make them sparse by calling train.eliminate_zeros(). Can you try this on a real dataset and see what happens?. Thanks!. It looks like the hyperparameters which are appropriate for the model without user features aren't appropriate for the model with user features.\n\nAre your user features all binary? If not, do they have large (in absolute value) values? If so, you need to scale them to lie between 0 and 1, or in a similar small range.\nHave you tried a lower learning rate, or performed any hyperparameter searches?\n\nIf your features are very dense (lots of users share the same feature), it may be beneficial to reduce the amount of parallelism you use (although this is unlikely to be the problem).. (Aside: you can use triple backticks (```) to format code blocks.)\n\n\nHyperparameter search is one of the most important concepts to master in machine learning. There are lots of good examples and guides on the internet (like the  sklearn one). I highly recommend you look at those. For LightFM, the principle will be the same: you will run your model on the same training data with different hyperparameters, and look at the accuracy; then select that set of hyperparameters that gives best accuracy.\n\nI suggested the learning rate specifically because I think it has the best chance of helping you. Please try it.\nAre you sure that your matrix concatenation works? Not doing the concatenation correctly would produce results very similar to the ones you have.\n\nThe side information you have is fairly sparse, so I wouldn't really expect it to help, but the model should be no worse than with no side information. Tweaking the learning rate is I think key.. This looks fine, as far as I can tell. Try lowering the learning rate!. Without having the data I may not be able to help more.\nOne possibility is that you do not actually stand to gain from your user features: in actual fact, they seem very coarse and maybe not that helpful. In LightFM, user latent representations are unweighted sums of the latent representations of their features: here, you are probably simply averaging something informative (a specific user's latent feature) with very uninformative features (gender and age), and the resulting model is worse.. It's possible that it might help, but even so I think the features would not carry much information.. @gqoew . It's naturally possible to use all these datasets: you simply need to parse them into suitable scipy.sparse matrices.\nThe easiest way of getting them would be with the help of the Spotlight package. After installing it, you could simply run the following to obtain the sparse matrix representations of those datasets:\n```\nfrom spotlight.datasets import movielens\ndata = movielens.get_movielens_dataset(variant='1M').tocoo()\n```\nAll the Movielens datasets are available (('1M', '10M', '20M')). They are not used in the canned datasets in LightFM, but there is no reason why you couldn't parse them into suitable feature matrices and use them in your model (have a look at the docs).. 1. Is the input data the same?\n2. Are you using multithreaded fitting? If so, results are not deterministic.\n3. Can you try comparing runs on the same machine?\nDo the recommendations differ in a material way?. Is it possible that you use different versions of Python or numpy on the\ndifferent machines, or different C compilers?\nOn 31 Oct 2017 19:23, \"drraghuram\" notifications@github.com wrote:\n\n\nYes, the input data is the same.\nNo, we are not using multithreaded fitting.\nRepeated runs on the same machine gives the same reults.\n\nI am not sure what you mean by a material way, but the order in which the\nresults appear does change. For example, a recommendation appearing as the\n4th recommendation in one machine appears as the 6th recommendation in a\ndifferent machine.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/225#issuecomment-340733619, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA8PrmlIb06B_-s2M2JVv1hNdCQdJks5sxwMugaJpZM4QMiNk\n.\n. Interesting. Could you try initializing a np.random.RandomState with the same seed on the different machines and getting a sequence of random values? They should all be the same. If that holds, then we know the bug is in LightFM.. Hmm, there are two other possibilities.\n1. Are you passing COO matrices to fit? If they are not COO, they are converted to COO, and that conversion may result in different order of elements, which affects the result. Similarly, if you are passing COO matrices, can you confirm that the order of entries is the same?\n2. Are the CPU architectures of your machines the same? By default, LightFM is compiled with --ffast-math, which means that float operations are reordered and vectorized. You could test this by recompiling the dynamic objects without --ffast-math.\n\nOut of curiosity, why is it important that the result be the same across machines? Usually, fitting is done once on a single machine, and only prediction is distributed.. 1. CSR input is fine, it will be transparently converted (but you'll use less memory if you start with the right format).\n2. The row number of the interaction matrix is the user id.. It's not really implicit: the i-th row of the interaction matrix (the i-th user) will use the features stored in the i-th row of the feature matrix. Have a look at the Notes section in the documentation.. If your hybrid model performs worse than a pure CF model, the following may be helpful.\n1. Make sure your features are appropriately scaled.\n2. If your features are continuous, consider discretization.\n3. Make sure you don't drop per-item or per-user features. Again, have a look at the notes section.\n4. Consider inverse-frequency weighting of your features.. Item id 14 will look up the 14th row of the item feature matrix and so on. The order doesn't have anything to do with it.. The main reason is that this would require preallocating a 70k x 90k result matrix, which takes around 24 gigabytes of RAM. This is probably not a sensible thing to do by default.. In your case, I would probably distribute the predictions using multiprocessing.. Because they define user-item pairs.\nOn Thu, 12 Apr 2018, 18:43 Ben Duffy, notifications@github.com wrote:\n\nFair enough.\nThough, I'm still confused as to why do they have to be equal sizes though?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/226#issuecomment-380888013, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA1dG1xPGTP2jtorgs30GH4-wDwonks5tn5IzgaJpZM4QQlwL\n.\n. Once this is merged I'll make sure we can push this to the documentation pages as well.. Thanks. Is this for building debian packages?. Hey, glad you found LightFM helpful! A couple of thoughts:\n\n\nI think this could be useful! Would you mind writing an example (say, a Jupyter notebook)? I don't really want to include pandas as a dependency of this library.\nThe metrics are already here! Have a look at the docs. (RMSE is not a good metric for evaluating recommendations, so it is not included.)\nI added something recently here. It's not on PyPI yet, but I will push it soon.. Update: the train/test split is out in the new version.. Not quite: the interactions within item features or within user features are not modelled, only between them.. Factorization Machines, but they are light and not the whole deal. To be honest I didn't think about the naming too much.. Let me clarify: you have particular users who have no interactions, or you have no interactions for any users?. The easiest way to accomplish what you want is to simple program that behaviour into your system. ML-based systems rely on learning from data; if you don't have any data, you are using the wrong tool.\n\nHaving said that, there are of course ways of encoding your prior expectations into the model. In your case, you could construct a synthetic interaction matrix that expresses the behaviours you expect: pink-liking people interacting with pink things and so on. Simply add 1s to your matrix where you would expect an interaction to be, and train your model. You could then enrich the matrix with real interaction data as it comes in.\n(You are getting random recommendations in your example because the model does not do any training on an empty interaction matrix. The initialisation is random, so you are in fact getting completely random recommendations.). Yes, this is correct (and very well expressed, too). If you expect to gather data soon, encoding some priors may be a reasonable solution.. Sure. You can just read your interaction data from disk in batches, construct sparse matrices, and fit them to subsequent fit_partial calls. Just make sure that all the matrices have the same number of rows (users) and columns (items).. No, each minibatch be transformed into an interaction matrix with the same dimensions. This doesn't mean that each minibatch has to have the same number of interactions, or interactions from the same users.\nExample: minibatch A has interactions [(user 1, item 1), (user 1, item 2)], minibatch B has interactions [(user 2, item 3), (user 2, item 4)]. These are fine as long you transform each into sparse matrices of shape (2, 4) before passing them into LightFM (the same is true for Spotlight).. You should create a sparse matrix with 1s for those (user, item) pairs where an interaction happened. Leave everything else at 0.. No, this would go in the user/item feature matrices. Please have a look at the documentation.. What call are you making to the predict function? What is the shape of its arguments?. Ah, I see. The error message is actually quite misleading, see fix here https://github.com/lyst/lightfm/pull/236. The gist is that you need to pass in the item features to the evaluation method (the features are not stored in the model).. Thanks for the bug report!. 1. Sure, you could do that. To construct the features matrix, you need to treat the views of one user over time as new users, giving the features new values where they have changed, and keeping them constant where they haven't. For example, if you had 100 users, and wanted 5 time-views of each,  you'd end up with a feature matrix with 500 rows.\n2. LightFM doesn't take ratings: it's an implicit feedback model (which is what you want; models with ratings is something you almost never want).. Have a look at this paper. The bottom line is: data on which items users choose to rate is much more important than the rating given. Hence, implicit recommender systems.. You can weigh those differently when fitting the model (using the sample_weight argument to the fit methods).. LightFM has (1) user features and (2) item features, these are assumed to be constant over all interactions of that user and that item.\nIf you want to have contextual features (time-varying, or specific to a given interaction instance), you could integrate them in either user or item features. This way you would have more than one features row for a given user or item. For example, instead of having one row for item 10: {'id': 10, 'category': 'foo'} you could have two rows: {'id': 10, 'category': 'foo', 'time-of-week': 'friday'} and {'id': 10, 'category': 'foo', 'time-of-week': 'tuesday'}. With a little ingenuity in feature engineering you can make the model very flexible. . You mean you want to have emeddings conditional on some other variable? I think you'll have to have separate models.. 1. You need to pass in the feature matrix; these are not stored in the model object. This may make the model a bit harder to use, but allows you to change the feature matrices.\n2. The implementation complexity, both in training and in prediction, is linear in the number of nonzero features. I suggest you use the get_embedding methods which give you the embeddings given a fixed feature matrix. Once you have them, making predictions is just a matter of a vector-matrix product between the user row and the item rows.\n3. Yes, this is possible. The function accepts pairs of (user_id, item_id). For example, passing [1, 1, 2, 2] for user_ids and [1, 2, 1, 2] will give predictions for [(user_1, item_1), (user_1, item_2), (user_2, item_1), (user_2, item_2)].. If your question isn't specific to LightFM you may have better luck asking on a more general forum like StackOverflow.. No problem.\n\nThis is somewhat difficult to answer, as it also depends on how uniform the distribution is (you may be able to do some thing with a sparse dataset if it has denser regions). I'd say that something 10 times sparser than Movielens should still be manageable; 100 times sparse may be a challenge.\nYou can use cosine similarity between the embeddings. This article has an example of how to do it with LightFM.\nThe most robust answer is that you periodically recompute the model from scratch to handle all three. For just new interactions, you can run additional fitting iterations on the same model with the new data. (You can persist the model by pickling it.) Adding new users/items (this is called fold-in) is somewhat tricky, and not explicitly supported. For models that naturally take in new information (and new users), you should have a look at sequence-based models.. One thing you could do is rank items by a weighed average of the user-item recommendation score and the item-item similarity score.\n\nIf you do that, you may want to make sure to normalize the user-item recommendation scores appropriately, for example by transforming them into percentiles. User-item scores out of the LightFM model do not have a guaranteed range or scale (as that is irrelevant for ranking items for any particular user).. Could you ask this question on StackOverflow and post a link here so that I could respond? That way more users can benefit from the response.. This is correct. You can also use the get_<user/item>_representations methods to get the latent representations: http://lyst.github.io/lightfm/docs/lightfm.html#lightfm.LightFM.get_item_representations.. Yes, that would work.\nI would suggest that having 0 norms is a symptom of a problem with your model. Do you have any user/item vectors you don't use? Does your feature matrix have rows with all zeros? Is your regularization too large?. You are using private functions, so you are just asking for trouble :)\nThe raw data (as opposed to the data returned from the public functions) has timestamps in the data array. So the numbers are the timestamps of the interactions (as you say).\nIn practice during algorithm fitting it doesn't matter what number is there, as long as it's positive.\n  . Happy to help, and feel free to use whatever functions you like :). I changed your wording from 'excess' to 'access', which is probably closer to what you mean.\nWhen using a fitted model to predict, you will get the score. You need to keep track of everything else yourself.. Yes, thanks! I don't think the rebase was entirely successful but I can probably work with this.. Actually, sorry, this is a bit too much effort to fix up. Could you please rebase cleanly and start a new PR?\nMaybe the easiest solution is to simply start from newest master and add and commit the jupyter files.. You can try using item similarity to do that present support for your recommendations. For example, you could use cosine similarity between the embedding of the item you are recommending and items that the user has interacted with in the past to find items from the user's history that are most similar to the recommendation you are making.. 1. It could be --- I don't really have enough information to say. What is the purpose of your system?\n2. It's your training data, so not sure I can post it :) What sort of trouble do you have with building the feature matrices?. The features matrices of the Movielens and Stackoverflow datasets are available through the dataset fetching functions, as per the documentation. Have a look at those and see if that helps.. Done!. How did you install LightFM? Can you post the inputs you are using and the functions you are calling?. data['items'] and data['users'] are dense numpy matrices? Or sparse matrices? If sparse, in what format?. Can you experiment and try to narrow down with what arguments, or in what conditions, does the problem happen?. What shape is lines?. OK, so you have a 100 users/items in your dataset? Is it a dataset you can share so that I could reproduce the problem?. Can you please make a gist or a github repo with the full code that I can run?. Your code doesn't actually work from line 34 onwards: https://github.com/phenric/reco/blob/master/recommander.py#L34. I know what the issue is. You're passing a ludicrously large user_id into the predict function. Am I right?. https://github.com/lyst/lightfm/pull/256\nFor future reference, LightFM expects that user and item ids be contiguous and start at zero. This means that if you have 10 users the largest possible user index you should be passing in to predict is 9.. Yes, it was your error, but the library should never segfault. So thank you for the report!. In general, it's difficult to say what a 'good' value of a metric is. This\ndepends hugely on the dataset you have. The best way of figuring out what a\ngood model is for your data is to try lots of different models and compare\nthem.\nLightFM implements implicit feedback algorithms: passing in rating values\nhas no effect on the model. Fitting the same model repeatedly will give\nslightly different results each time due to randomness inherent in the\nfitting process.. Well, there is also an explicit binary feedback algorithm that is quite bad and you shouldn't use. It's here for backward compatibility but I may well remove it.\nTake my earlier answer to say that LightFM does not use ratings to fit its models.. Looks good. Can you please remove the .DS_store files?. These are all very good comments, but I feel quite bad about asking for so many changes!\n@lesterlitch how would you prefer to handle this?. I like the changes, thanks for making them.\nIf I were in a nitpicky mood, I'd say that:\n- the structure (headings/subheadings and so on) could still be a bit better,\n- a lot of the lines are too long and wrap, we could make them shorter.\nAny other comments @rth @benfred?\nBut overall, this is a great example! I particularly like the inner product trick, it's nice to see it implemented.. I think this is a great example. I'm going to merge. Thanks all for your help!. 1. One interesting characteristic of WARP is that it is effectively playing against itself: it is preferentially using examples that it scores highly as negative examples. Up to a certain point, this allows it to express user preferences better; beyond that point, it is self-defeating, as it is selecting the examples that it should recommend and treating them as negatives. The extent to which that happens is governed by the max_sampled hyperparameter. I suspect that this is what happening in your case. Instead of switching to BPR, can you try decreasing that hyperparameter? Otherwise, I don't see any obvious problems with switching losses. It's interesting that you find different results with adagrad and adadelta, I haven't observed that.\n\n\nsample_weight is the right way of using this in the LightFM model. If you are optimizing for some weighted objective function, changing the weights here will probably help in achieving your goal.\n\n\nI'm open to a submodule along the lines of lightfm.contrib which would contain high-quality, but not strictly speaking supported functionality (say, with optional dependencies). I'd be happy to discuss your additions (but I have to warn I can be quite picky!). While I personally am not a huge fan of pandas, I know it's quite popular, so maybe input from pandas dataframes would be a good first step?. @RAbraham I use plain Python objects and numpy arrays.\n\n\nPandas is a useful tool, but I prefer not using it.. @artdgn thanks for this, I will have a look. Which parts do you think would be useful for most users?\nI also noticed you have a helper for building interaction matrices. I've been working on something similar to include in LightFM itself, and would love to have your comments (https://github.com/lyst/lightfm/pull/281)!. How do you deal with categorical features in pandas? Load it in as strings, change type to categorical, then one-hot encode?. I think doing the name -> idx mappings correctly and consistently is the tricky thing, because we're dealing with two objects: features and interactions (as you say). As far as constructing sparse matrices goes, I'm pretty sure my approach is about as efficient as you can get without dropping down to compiled languages (much more efficient that scipy's DOK and LIL matrices).\nCan you point me to a place where you do the dataframe -> sparse matrices translation?. You can use the sample_weights argument to weigh the importance of individual interactions.. Yes. However, the values should be normalized, and must not be large. In practice you may find that discretization works better.. This is, in fact, the primary purpose of LightFM. In the literature, this is referred to as implicit feedback.. You will note that all the builds failed, so my guess would be: no, they should not be implemented this way.\nMore seriously, have you ran any evaluation of your proposed changes? What are the results?. Thanks for the explanation.\nI think changing the numerator is OK. Changing the way we increment sampled I am less sure about: it introduces a nasty corner case for users who have a lot of positive interactions.\nMore generally, do you think there are tangible benefits to making these changes? Unless we can demonstrate benefits, there doesn't seem to be a clear case for them.. Closing due to inactivity.. 1. Try it and let me know!\n2. Yes, that's a decent option and one that I've used in the past.. Hope this helped. Let me know if you have any further questions!. What do you mean by volatility?. What changes have you experimented with already? Try varying your hyperparameters (especially the loss functions).. I would recommend defining a metric that captures your goal (different recommendations for different users), then running a random search over hyperparameters to see if there are any hyperparameters that affect it. One suggestion I have is that 1000 epochs is too many, by an order of magnitude.\n(This assumes that you've already checked your data and there is nothing fishy there.). How did you install LightFM?. Can you try installing using pip install instead?. Thanks for the detailed question!\nConstructing explicit hierarchies of preference is actually a question I'm quite often asked. Unfortunately, you are correct in thinking that this would required fairly complicated changes to the Cython code in LightFM: while changing sample_weights is an approximation to this, it does not attempt to preserve the rankings you have. (And, as you say, may be of limited effectiveness: it's possible that changing relative weights even more to weigh top movies even more heavily will work, but I am not fully convinced.)\nI think that capturing the relationships you have will require a substantially different model that will be hard to express in LightFM. Instead, I would recommend something more flexible, like Spotlight or wyrm (my new deep learning library, with an example of building a recommender here). I would be happy to provide pointers in both cases.. Hope this helped. Let me know if you have any further questions!. Thank you!. Gremlins. Thanks for the fix!. Hint: when reporting issues, use proper code formatting (have a look at the guide).. Hint number 2: if you have a snippet of code that reproduces your problem, try to make it runnable. In your case, I had to add imports etc etc.. This happens because of dtype conversions inside LightFM (using scipy's astype operations). The order of the entries changes, but the matrix remains unchanged.\nHaving said this, I don't like that astype changes the original matrix: this should not be happening. I will be making a pull request moving away from the astype call.\n. https://github.com/lyst/lightfm/pull/279. No, there is no upper or lower bound. If you're ensembling with tree-based methods, you should be fine; otherwise, you may want to employ some sort of scaler.. That may work, but I would double check that the sigmoid doesn't saturate too much.. No problem.. LightFM accepts sparse matrices as input. If you can convert an RDD into a sparse matrix, you should have no problem using LightFM.\nI do not myself use Spark so I will not be able to offer help.. (OOM sadly happens every now and then. I can always re-run it.)\nI like the idea, and thank you for the contribution! I am keen to have something like this in the package, but I would suggest a slightly different API. Maybe something like this:\n```\noriginal = (...some model...)\nnew = original.resize(new_num_user_features, new_num_item_features)\n````\nAs in your suggestion, new would be a new, wholly independent object. I am also (rather strongly) opposed to having any type of feature names in the model; I'd rather leave this sort of plumbing to the user. Is there a use case that your suggestion supports where the names are important?\nFinally, I think it would be good to carry over the rest of the state of the model, like the accumulated gradients, and all the hyperparameters. The get_params and set_params method will be useful for the latter.\nTo sum up: I look forward to having this feature, but I am keen to get it at least somewhat right, so it may take us a couple of iterations to get there.\n. I see the merit in making things easier for the user. I see a fair number of questions about how to construct features, and having to map IDs people use in their systems to contiguous number ranges also generates a fair amount of confusion. I think it may be time to tackle this more head-on; I'm going to write a quick prototype, share it soon, and maybe then we can collaborate on that?. This is not to say I don't like this approach, we will definitely get this done too :) But there is a meatier bit about data processing that you correctly suggest should be handled, and I think once we get that done this will become a fair bit easier to do in a nice way.. I had a go at adding some functionality to making managin user/item ids and feature names easier: https://github.com/lyst/lightfm/pull/281.\nPlease have a look: I think with that we can offload the responsibility for maintaining consistent features to the Dataset class, and change the resize function to not accept feature names, but rather the desired dimensions of the new model (as returned by the model_dimensions method).\nThis doesn't allow dropping features (only adding new ones works) --- but I think I'm OK with that.. (Also, I think resize returning a new object is a good idea. Thanks for implementing that.). Hey @dakl do you want to collaborate on the design for the data builders?. The WARP and BPR losses are implicit feedback. Logistic is explicit.\nExplicit feedback algorithms perform very poorly. I would suggest not using them.. There is a hint in the error message! What steps did you take to try to fix it?. ValueError: Corrupted Movielens download. Check your internet connection and try again.\nThis suggests that when the package tried downloading the dataset, the download failed. It should recover automatically, otherwise find the file (in the lightfm_data directory in your user directory), delete it, and try again.\nAlso, when reporting an issue, please use proper code formatting (have a look at the guide).. No worries. Did you manage to get it working?. Done. I have never used it myself.. Might be your error, nothing to do with LightFM:\nhttps://github.com/pypa/pip/issues/4251\nGitHub issue formatting tips, for formatting code blocks:\nhttps://guides.github.com/features/mastering-markdown/\nOn Thu, 5 Apr 2018, 21:26 snufflemate, notifications@github.com wrote:\n\nHow can I get around this error?\nC:\\Users\\Username\\Documents\\Code>pip install lightfm\nCollecting lightfm\nUsing cached lightfm-1.14.tar.gz\nRequirement already up-to-date: numpy in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from lightfm)\nRequirement already up-to-date: scipy>=0.17.0 in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from lightfm)\nRequirement already up-to-date: requests in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from lightfm)\nRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from requests->lightfm)\nRequirement already up-to-date: urllib3<1.23,>=1.21.1 in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from requests->lightfm)\nRequirement already up-to-date: certifi>=2017.4.17 in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from requests->lightfm)\nRequirement already up-to-date: idna<2.7,>=2.5 in\nc:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\n(from requests->lightfm)\nInstalling collected packages: lightfm\nRunning setup.py install for lightfm ... error\nException:\nTraceback (most recent call last):\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\compat_\ninit_.py\", line 73, in console_to_str\nreturn s.decode(sys.stdout.encoding)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 142:\ninvalid start byte\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\commands\\install.py\",\nline 342, in run\nprefix=options.prefix_path,\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\req\\req_set.py\",\nline 784, in install\nkwargs\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\req\\req_install.py\",\nline 878, in install\nspinner=spinner,\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils_\ninit.py\", line 676, in call_subprocess\nline = console_to_str(proc.stdout.readline())\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\compat\ninit_.py\", line 75, in console_to_str\nreturn s.decode('utf_8')\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x94 in position 142:\ninvalid start byte\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\commands\\install.py\",\nline 385, in run\nrequirement_set.cleanup_files()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\req\\req_set.py\",\nline 729, in cleanup_files\nreq.remove_temporary_source()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\req\\req_install.py\",\nline 977, in remove_temporary_source\nrmtree(self.source_dir)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 49, in wrapped_f\nreturn Retrying(dargs, dkw).call(f, *args, kw)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 212, in call\nraise attempt.get()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 247, in get\nsix.reraise(self.value[0], self.value[1], self.value[2])\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\six.py\",\nline 686, in reraise\nraise value\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 200, in call\nattempt = Attempt(fn(args, kwargs), attempt_number, False)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils_\ninit_.py\", line 102, in rmtree\nonerror=rmtree_errorhandler)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\shutil.py\",\nline 494, in rmtree\nreturn _rmtree_unsafe(path, onerror)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\shutil.py\",\nline 393, in\nrmtree_unsafe onerror(os.rmdir, path, sys.exc_info()) File\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils_init.py\",\nline 114, in rmtree_errorhandler\nfunc(path)\nPermissionError: [WinError 32] Der Prozess kann nicht auf die Datei\nzugreifen, da sie von einem anderen Prozess verwendet wird:\n'C:\\Users\\username\\AppData\\Local\\Temp\\pip-build-sr9azq0l\\lightfm'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\basecommand.py\",\nline 215, in main\nstatus = self.run(options, args)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\commands\\install.py\",\nline 385, in run\nrequirement_set.cleanup_files()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils\\build.py\",\nline 38, in exit\nself.cleanup()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils\\build.py\",\nline 42, in cleanup\nrmtree(self.name)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 49, in wrapped_f\nreturn Retrying(dargs, dkw).call(f, *args, kw)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 212, in call\nraise attempt.get()\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 247, in get\nsix.reraise(self.value[0], self.value[1], self.value[2])\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\six.py\",\nline 686, in reraise\nraise value\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip_vendor\\retrying.py\",\nline 200, in call\nattempt = Attempt(fn(args, kwargs), attempt_number, False)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils_\ninit_.py\", line 102, in rmtree\nonerror=rmtree_errorhandler)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\shutil.py\",\nline 494, in rmtree\nreturn _rmtree_unsafe(path, onerror)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\shutil.py\",\nline 384, in _rmtree_unsafe\n_rmtree_unsafe(fullname, onerror)\nFile\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\shutil.py\",\nline 393, in\nrmtree_unsafe onerror(os.rmdir, path, sys.exc_info()) File\n\"c:\\users\\username\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pip\\utils_init.py\",\nline 114, in rmtree_errorhandler\nfunc(path)\nPermissionError: [WinError 32] Der Prozess kann nicht auf die Datei\nzugreifen, da sie von einem anderen Prozess verwendet wird:\n'C:\\Users\\username\\AppData\\Local\\Temp\\pip-build-sr9azq0l\\lightfm'\nlast line: \"process can't be used because it is used by another process\"\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/282, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA1qAvTDB4a6Dt5m8zAw2wakekGMfks5tlnAAgaJpZM4TJBmO\n.\n. How did you measure this? It sounds unlikely: the same code is used for both paths. My suspicion is that if you used something like top, your prediction job doesn't actually spend enough time in the multithreaded routines for an eyeball test like this to work.. I think that's probably what is happening. I'm going to close this for now, but please let me know if you hit any further problems.. You can recommend items to new users as long as you have metadata features for that user that were also present in the training set. You do this by constructing appropriate feature matrices.\n\nIf you are introducing new features, you will have to retrain the model to create embeddings for them.. Hope this helped. Let me know if you have any further questions!. I would suggest periodic (say, daily) retraining of the model. . I am working on other models that react to new information in real time, but they are not ready yet.. @vghiya06 Yes. You can use them in Rust or Go.\nI'll be working on Python bindings but I don't have a timeframe on that.. Sounds great, thanks a lot!. Are they different when you use one thread only?\nAre your train and test sets unchanged between runs, including the matrices being in the same order?. Exactly: you will only get perfectly reproducible results with 1 thread. If we use parallel fitting, every run will have slightly different results due to a multitude of factors that are outside your control. The number of threads may or may not make the results closer.. Thank you for the report! How do you propose to check for the intersection?. Cool, this sounds fine. Could we make this an exception-throwing check that is disableable via a flag?. This sounds roughly right, I think what you suggest would work. You can use interaction weights to express confidence.\nIf you'd like more detailed information please drop me an email at lightfm@zoho.com and I'd be happy to talk more about your specific application.. Could you add a way of passing this flag through the evaluation functions that call predict_rank?. This is mostly to maintain a way of getting the old behaviour back. In principle, you're right that it should never be necessary.. This looks great, thanks for contributing!. How does pip install lightfm not work?\n(When asking questions, try to provide as much detail as you can. It will let me help you more quickly.). Follow this guide or try installing from Conda.. Your code is buggy. It looks like you may have more luck splitting your lines on tabs rather than hyphens.. Hey @ravijoe, I'll be happy to help you with LightFM issues. So far, it looks like your problems are more of a general programming nature: how to install packages, how to read CSV files. For questions like these, sites like StackOverflow may be more useful.. It looks like you have (at least) two problems with your code.\n\nThe CSV file you're reading is tab-, not comma-separated. The Python csv package has options to deal with that.\nThe dates are \\-separated, not --separated.\n\nThis is quite clear from the error message you posted: the line from the file that is printed clearly shows both things.. @ravijoe, it's OK to ask for help when you get stuck --- but asking people to do your work for you is not.\nI would recommend finding a contractor to help you with your project, and paying them.. I may not be able to give you a satisfactory answer without more information.\n\nCan you run this without item features first?\nWhat how many epochs are you running? What are your other hyperparameters?\n\nFinally, this is an unusually small dataset.. Could you please post a gist that reproduces your issue (so that I can run it), describing what you should happen and what is happening.\nFitting models on datasets with 10 users is not the use case the package was designed and tested for; it's not clear what's even supposed to happen.. Great! Sorry I didn't get to this earlier.. The model estimates the embeddings for user and item features (via matrix factorization), then uses their elementwise sum as representations for whole users and items. Those summed representations are then combined via inner products to yield user-item scores.. :+1: . 1. When adding features you may need to change hyperparameters: have you performed a hyperparameter search after adding features?\n2. Are the features high quality? Are any of them not normalized? If there are features that are shared between many users or items, performing tf-idf weighting on the features matrix may be helpful.\nGiven that you seem to have relatively high accuracy without features, what are you trying to achieve by adding them?. Can you try transforming your features so that the feature weights in every row sum to 1?. For ranking losses the interaction matrix should have zeros and ones.\nOn Fri, 11 May 2018, 09:19 rajat1602, notifications@github.com wrote:\n\nAlright! will try this too.\nDo I need to normalize my interaction matrix as Interaction matrix\ncontains ranking from 0 to 5? and Feature Matrices are binary.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/297#issuecomment-388295762, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA7JAcYe_473XNG8Q57Kc4cDC_l9-ks5txUmmgaJpZM4T55BK\n.\n. Great!\n\nYou will most likely want to pass the entire matrix: user/item indices index into its rows.. Sure, if you want to predict for items 0..19, you just need the first 20 rows of the matrix. But what if you want to predict for items [0, 14000, 45000]?. (This issue may be helpful https://github.com/lyst/lightfm/issues/298). It should help if the new items are described by the features you used in training.\nYou will need to construct a new feature matrix containing the new items. After that, you should be able to get reasonable predictions for them.\n(You will still want to occasionally retrain the model.) . Create the new feature matrix and give the items ids from 0 to 29 is the best option. \nYes, you are exactly correct: they are simply indices, and you need to retrain when new features come in.. Many things could be wrong, but most of the time it's either (1) data errors or (2) bad hyperparameters.\nWhat are your hyperparameters and what is the shape and density of your data?. Your hyperparameters look OK, maybe you could try a higher learning rate and larger no_components?. That does look better. In general, you always want to do a hyperparameter search. Have a look here for some sample code.. Hey @dawenl, it's fine as long as you cite me :)\nThe easiest way of accomplishing this (if you're using adagrad) is to run the following:\n```python\nmodel = ... # some model object that's been estimated before\nmodel.item_embedding_gradients = np.finfo(np.float32).max\nmodel.item_bias_gradients = np.finfo(np.float32).max\nContinue fitting: the item parameters will now be more or less frozen.\n```\nThis will set the accumulated squared gradients in the adagrad optimizer to their maximum value. Since the effective learning rate is eta / squared_grad.sqrt(), this will make the learning rate very small. The parameters will still update, but the changes will be infinitesimal.\nWill this do?. No, the gradients are unchanged, so it should end up roughly 0: https://github.com/lyst/lightfm/blob/master/lightfm/_lightfm_fast.pyx.template#L381. Yep, that's exactly right.. Try it and let me know: I would be afraid of non-normal floats causing performance/numerical havoc, but I don't know for sure.. By strong regularization you mean evaluation on a set of users who have been completely excluded from training? This is not straightforward, but is still eminently doable.\nYou'll need to do the following:\n1. Call fit with an interaction matrix that's oversized: say, it has 10,000 rows, but you only have training data for the first 8,000 customers (that is, all the entries in the last 2,000 rows are empty).\n2. Freeze the gradients using the method we discussed above. Additionally, set the regularization rate for items to 0 (otherwise they'll still be regularized even though the learning rate is zero).\n3. If you're using regularization, you may have to reinitialize the last 2,000 rows of embeddings to make sure they haven't been regularized to zero.\n4. Call fit_partial with a 10,000 row matrix: but this time, there will be no data in the first 8,000 rows - only data in the last 2,000 rows.. Did you freeze before the epoch=n run?. OK. So what I think should be happening is this:\n\nYou run fit on your training data.\nYou freeze the model.\nYou run additional fit_partial runs to do user fold in.\n\nIs that what you did?. Once you freeze the gradients the model is shot on the items side unless you unfreeze them, it will not train any more. So you should have gotten much better results after epochs=n.. How did you unfreeze them?. Hmm, in which case the results should be the same. Did you reset regularization as well?\nTo help more I'd need a reproduction example, I suspect. Maybe try the Movielens dataset and see if the results are similar?. The code looks solid at first glance.. Awesome! That would be great. I can then dig in.. That would be easiest.. Can you please format your issue using Github Markdown? The code is hard to read.. Can you please format your issue using Github Markdown? The code is hard to read.. The sample weights basically scale the learning rate up (above 1) or down (below 1) for a particular sample. Obviously, it doesn't make sense for this to be less than zero; similarly, I think very large values will make the model diverge.. That sounds sensible!. Hi @OktayGardener, thanks for this!\nLet me know what you think of the data builder: I want to make sure it's useful!. This is the desired behaviour (have a look at the docs). The idea is that, by default, a feature is allocated for every item and every user.\n(Please have a look at this guide to learn to format code in issues correctly.). Sounds good!. Have you tried normalizing after the weighted summation?\nOn Sun, 17 Jun 2018, 16:06 Joakim Rishaug, notifications@github.com wrote:\n\nI think this has to do with the addition of multiple normalized vectors\nonto each other. This doesn't seem to gel well with stable results. Don't\nreally have the time to look for a solution now, but it would be cool if\nanyone else dropping by this PR looked into it :)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/pull/309#issuecomment-397885008, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA72-Giv4i6LvwbLA1D1vQdUoUODrks5t9nB0gaJpZM4UTeDX\n.\n. @reditya your approach makes sense! The main reason why something like this isn't in LightFM is that it's somewhat tricky to make sure we cover this case as well as the more common case where we just want uniform negative sampling (and where we don't confuse novice users!).. @christinedekock an alternative approach is to train two WARP models, one for positive and one for negative interactions, and ensemble them together.. You should pass in an instance of numpy.random.RandomState. I will accept a PR that takes integers and transforms them into a RandomState as well, if you care to write one.\n\nDo you have duplicate entries in your interactions matrix? Try converting it to CSR and back to COO.. In the hybrid case, hard to say. AUC of over 0.9 is quite normal, but these results look very high.. I think precision and MRR are more reliable metrics than AUC: if those look OK, I think you'll be fine.. My pleasure. Let me know if you have any further questions!. Yes, that makes sense. I would accept the fix if you'd like to make a PR!\nThere aren't really any benchmarks in the same way you'd find benchmarks for imagenet, for example. I think it would be a worthwhile project to set up a repo that does benchmarking of the sort.. Merged the fix.. That's because explicit feedback recommenders are not very useful: https://github.com/maciejkula/explicit-vs-implicit.\nOnly use them if you are really sure what you're doing. In most cases they produce substandard results.. I would recommend not using an explicit feeback approach: most likely, it will perform worse than an implicit feedback approach.\nYou may also be interested in the discussion in https://github.com/lyst/lightfm/issues/310.. The predict method accepts a user id and a list of item ids: you should filter the item ids to only contain ids from the group you'd like to score, then compute predictions for the group of users you are interested in.. In your example, you need to pass two arrays of equal length such that corresponding entries form user-item pairs. For example, if you want to predict for items 10, 20 and users 1, 2, you'd pass [1, 1, 2, 2] and [10, 20, 10, 20]. In general, it'd be easier to make two predict calls: predict(1, [10, 20]) and predict(2, [10, 20]).. You can average or otherwise combine the individual predictions and return that averaged score for both users?. That sounds good enough :). The easiest way is probably to use the get_item_representations and get_user_representations methods.\nThey return matrices of embeddings and arrays of biases, allowing you to perform fast batch multiplications via standard numpy dot operations. Combined with pre-allocating the receiver (using the out argument in dot), this should give you pretty decent speed for predictions themselves.\nFor sorting, np.argpartition may help for efficient partial sort up to 100, followed by a full argsort on the top 100.\nTogether, and parallelized using joblib or similar, this should take you close enough to C speed. If that is still too slow, you may have to distribute this across a cluster.. This is correct, the metric isn't normalised.. 1. You can use fit_partial as you suggest.\n2. How do you measure this? You will only see full utilization in top/htop if you run a long-running fitting process: 10 epochs on Movielens 100K runs too quickly for top to show the utilization. Can you run it with something like 1000 epochs and see?. Can you post the code you are using? Did you install LightFM from conda or from pip?. I'm afraid I cannot reproduce your problem. Can you run the following and post the output?\necho |cpp -fopenmp -dM |grep -i open. Do you use a lot of regularization?\nOther than this, nothing comes to mind. Can you post a gist of code that reproduces the problem on the built-in Movielens dataset?. That looks fine. Are you using a lot of regularisation?\nOn Fri, 22 Jun 2018, 03:03 aldll, notifications@github.com wrote:\n\nOK\nthe output\ndefine _OPENMP 201107\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/317#issuecomment-399297859, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA2B6m7gjUzRG1QOjQj0ata_2HFfQks5t_FBogaJpZM4UupiX\n.\n. It looks like you didn't.\n\nI'm afraid I can't help more unless I can reproduce.. By regularization I mean the L2 regularization imposed by the item/user_alpha hyperparameter.\nThe algorithm forces full regularization passes whenever the accumulated regularization is quite large. It's possible you're experiencing this, although it shouldn't be dependent on the feature sizes.\nAt this stage you must give me a short example on synthetic data that reproduces this. Otherwise we're not going to get anywhere.. There are 8 interactions in your data. You are not going to get parallelism on data that small. I can confirm that increasing the number of interactions to 100000 via\nn = 100000\n    interaction = sp.coo_matrix((np.ones(n), (np.arange(n), np.ones(n))))\nmakes the parallelism kick in.. When using parallelism the results are not deterministic, and different than just using a single thread. Have a look at the Hogwild paper for an idea about the reasons.. This is the expected result. By default, LightFM adds a feature per every user and item. You can disable that in the constructor.. (Well, you should get 160000 x 1600300 or something like that. Are your feature names the same as some of your user ids?). What data are you passing as user_features and item_features? It should just be a list (or other iterable) of the names of user/item features.\n(So if you have 300 user features it should be 300 long).. You need to pass an iterable of tuples of (id, [list of features for that id]) into build_features. It looks like at the moment you're passing the same features for every user?. Are you passing features for the second user? Is the resulting matrix an identity matrix? Can you post a short gist that reproduces this?\nIt may be useful to print some of the elements in your iterator and make sure that they are what you think they are. Is x[list_columns_user] an iterable of things?. (If you think the docs are unclear on this point please make a PR with improvements.). One more pointer: if you are using generators, you can only iterate over a generator once: subsequent iterations will yield zero elements. Maybe you are creating one csv reader, using it for fit, then trying to use it again for build_features?. If all users have the same number of features the value on the diagonal will be the same for all of them.. What is the type of model? Are you passing the right arguments to recall_at_k?. Is the model accurate? What's the MRR?\nLooking at your code, what is item?. There is nothing obviously wrong, but the model should not (and will not) normally give the same recommendations to everyone.\nWhat are the predicted values? Does it consistently predict 0.0? Have you tried other losses and hyperparameters?. A couple of suggestions:\n\nTrain without user features.\nTrain without item features.\nAre your users really different in the training data?\nMaybe some items are just vastly popular.\nExclude items users have already seen from the returned recommendations.. Have you tried running this in a regular Python interpreter in your\nterminal?\n\nOn Sat, 30 Jun 2018, 09:30 balancy, notifications@github.com wrote:\n\nI have the code like this:\nimporting the libraries\nimport numpy as np\nfrom lightfm.datasets import fetch_movielens\nfrom lightfm import LightFM as lf\nfetch data and format it\ndata = fetch_movielens(min_rating=5.0)\nprint training and testing data\nprint(repr(data[\"train\"]))\nprint(repr(data[\"test\"]))\ncreating and fitting the model\nmodel = lf(loss=\"warp\")\nmodel.fit(data[\"train\"], epochs=30, num_threads=2)\nAnd I always receive an error \"Kernel appear to have died\" on the line\n\"model.fit\"\nSystem is Ubuntu, IDLE is jupyter notebook.\nI tried to reduce the number of epochs to 1 and the number of threads to\n1, but it still doesn't work.\nWhat can be a problem?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/321, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAyTnmqvlv_7uXTIrLhi8TPFYrO6Fks5uBzc_gaJpZM4U95Q4\n.\n. Did you install it from pip or conda? If from conda, uninstall and install\nfrom pip.\n\nOn Sat, 30 Jun 2018, 20:09 balancy, notifications@github.com wrote:\n\nYes, I tried. In Spyder it gives the same error with the kernel, in\nterminal it gives an error like 'Invalid instruction. Dump of memory is\nmade'\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/321#issuecomment-401557282, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA4hJrbimwKnNs2OHod41Dq9wRMGbks5uB77jgaJpZM4U95Q4\n.\n. It looks like your CPU does not support some of the instructions that are\nused on the machines where the conda wheels are compiled (are you using a\nvery old CPU?).\n\nYou'll have to have a working C compiler locally and install from pip.\nOn Sun, 1 Jul 2018, 09:25 balancy, notifications@github.com wrote:\n\nI tried to install it via pip at the beginning. It gives me an error:\nCollecting lightfm\n  Using cached https://files.pythonhosted.org/packages/e9/8e/5485ac5a8616abe1c673d1e033e2f232b4319ab95424b42499fabff2257f/lightfm-1.15.tar.gz\nRequirement already satisfied: numpy in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (1.14.5)\nRequirement already satisfied: scipy>=0.17.0 in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (1.1.0)\nRequirement already satisfied: requests in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (2.18.4)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (3.0.4)\nRequirement already satisfied: idna<2.7,>=2.5 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (2.6)\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (1.22)\nRequirement already satisfied: certifi>=2017.4.17 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (2018.4.16)\nBuilding wheels for collected packages: lightfm\n  Running setup.py bdist_wheel for lightfm ... error\n  Complete output from command /home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-fotd7sfp --python-tag cp36:\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib.linux-x86_64-3.6\n  creating build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/data.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/_lightfm_fast.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/lightfm.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/evaluation.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/init.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/cross_validation.py -> build/lib.linux-x86_64-3.6/lightfm\n  creating build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/_common.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/init.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/stackexchange.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/movielens.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/_lightfm_fast_no_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/_lightfm_fast_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n  running build_ext\n  building 'lightfm._lightfm_fast_openmp' extension\n  creating build/temp.linux-x86_64-3.6\n  creating build/temp.linux-x86_64-3.6/lightfm\n  x86_64-conda_cos6-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -Wstrict-prototypes -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fPIC -I/home/pavel/anaconda3/include/python3.6m -c lightfm/_lightfm_fast_openmp.c -o build/temp.linux-x86_64-3.6/lightfm/_lightfm_fast_openmp.o -ffast-math -fopenmp\n  unable to execute 'x86_64-conda_cos6-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-conda_cos6-linux-gnu-gcc' failed with exit status 1\n\nFailed building wheel for lightfm\n  Running setup.py clean for lightfm\nFailed to build lightfm\nInstalling collected packages: lightfm\n  Running setup.py install for lightfm ... error\n    Complete output from command /home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-record-fzunoz2d/install-record.txt --single-version-externally-managed --compile:\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.6\n    creating build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/data.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/_lightfm_fast.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/lightfm.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/evaluation.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/init.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/cross_validation.py -> build/lib.linux-x86_64-3.6/lightfm\n    creating build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/_common.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/init.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/stackexchange.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/movielens.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/_lightfm_fast_no_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/_lightfm_fast_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n    running build_ext\n    building 'lightfm._lightfm_fast_openmp' extension\n    creating build/temp.linux-x86_64-3.6\n    creating build/temp.linux-x86_64-3.6/lightfm\n    x86_64-conda_cos6-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -Wstrict-prototypes -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fPIC -I/home/pavel/anaconda3/include/python3.6m -c lightfm/_lightfm_fast_openmp.c -o build/temp.linux-x86_64-3.6/lightfm/_lightfm_fast_openmp.o -ffast-math -fopenmp\n    unable to execute 'x86_64-conda_cos6-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-conda_cos6-linux-gnu-gcc' failed with exit status 1\n----------------------------------------\n\nCommand \"/home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-record-fzunoz2d/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-jlt987mm/lightfm/\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/321#issuecomment-401589436, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA4QKzbGiwtI7VZISCqv_6s2mTR9Uks5uCHlngaJpZM4U95Q4\n.\n. The Intel Core designation covers quite a lot of different CPUs with\ndifferent architectures. In this case I suspect you have a CPU that's older\nthan the CPUs used to build the wheels (maybe it doesn't support AVX\ninstructions?).\n\nThe solution is to have a working C compiler and install from source, which\nis what installing from pip does.\nOn Sun, 1 Jul 2018, 12:10 balancy, notifications@github.com wrote:\n\nI am using Intel i7. Normally, pip install works for a lot of libraries\nlike keras, numpy, matplotlib, etc. It's just lightfm which doesn't want to\nbe installed on my machine.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/321#issuecomment-401597059, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA01gJ1-Ih6As3Q_h6R9EYtIYoncrks5uCKASgaJpZM4U95Q4\n.\n. You can't add new users or new items (or new user features or new item features) to an already estimated model.\n\nIf you expect new users, you should train the original model allowing for them (even if they have no interactions yet).. You have to retrain if you have a new user or item features.\nYou can make predictions for new items and users that use only the features that have been used in training. Simply construct the appropriate feature matrices.\nHave a look at https://stackoverflow.com/questions/46924119/lightfm-handling-user-and-item-cold-start?rq=1 and https://github.com/lyst/lightfm/issues/210 for some more pointers.. Thanks for the detailed problem description!\nThe running time of the algorithm is linear in the number of nonzero item\nand user features: if the average number of nonzero features per intem\nincreases from 1 (no item features) to 10, you can expect a (somewhat less\nthan) tenfold increase in training time. This in not because of an\ninefficient implementation: it's just that more vector multiplications are\nnow required.\nIt looks like your observations are consistent with this expectation, so\nall looks normal. To reduce training time I would recommend using\nmultithreading. You will need a Linux machine for this, because clang does\nnot support openmp.\nOn Mon, 2 Jul 2018, 15:07 Si Wharton, notifications@github.com wrote:\n\nI've spent the last couple of weeks working to improve our recommender,\nwhich uses your library under the hood, with the main goal being to include\nuser features at training and prediction time.\nMy first step was to upgrade to LightFM 1.15 (previously we were on 1.09),\nso as to make use of LightFM.Dataset to replace our previously\nhand-rolled dataset generation.\nWe have ~8,000,000 users and ~5,500 items. Users have 13 features, and\nitems have ~200 features.\nPlugging this data into LightFM.Dataset gives me these datasets to use\n\ninteractions (7954774, 5501)\nweights (7954774, 5501)\nitem_features (5501, 5705) (with identity matrix)\nuser_features (7954774, 13) (without identity matrix)\n\nIf I build and train a model in the following way, it takes approximately\n15 minutes on my MacBook (a 2015 2.9 GHz Intel Core i5 MacBook Pro with 8Gb\nof RAM)\nmodel = lightfm.LightFM(no_components=32, loss='warp')\nmodel.fit(\n    interactions,\n    sample_weight=weights,\n    epochs=20,\n    num_threads=4,\n    verbose=True\n)\nHowever, when including the item_features and user_features, it takes\napproximately 30 minutes to do one epoch, let alone the full 20 (again,\non my MacBook).\nmodel = lightfm.LightFM(no_components=32, loss='warp')\nmodel.fit(\n    interactions,\n    sample_weight=weights,\n    item_features=item_features,\n    user_features=user_features,\n    epochs=20,\n    num_threads=4,\n    verbose=True\n)\nThis issue was flagged when I first deployed the above code to production.\nOur trainer runs on an AWS c5.2xlarge EC2 instance, and after it had been\nrunning for 4 hours we decided to kill it and revert the change, as it was\ngoing to impact some other overnight tasks.\nIs this increase in training time to be expected, given these hyperparams\nand datasets, or is likely that I've made a mistake in my understanding of\nhow LightFM should be used?\nOne final thing to note: I'm not a data scientist - I've taken ownership\nof this recommender from a colleague who has since left the company. I\nunderstand the Python side of things, but not the nuts and bolts of what\nyour library is actually doing.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/323, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA6-0-qgv9KOaD3_knKgkDkd950PAks5uChsCgaJpZM4U_SDW\n.\n. My pleasure. If you install it in Docker you can train it with parallelism even on a mac.. I'm OK with this as an option (triggered by an optional argument), although I don't think it really matters --- the main thing is that the metric allows you to reliably compare two models.\n\n. Many people ask a question on Gitter while at the same time asking a question on Github. Which is what you have done, and that is why I responded to you here but not on Gitter.. 1. This is correct.\n2. Yes, you can also do that.\nOne additional way of speeding up requests is by using the get_item/get_user_representations methods. Those will give you the latent vectors (and biases) given features, and you can just do item_bias + dot(user_repr, item_repr) from there.. No. After fitting with the full set, the model uses only the features for\nthe user (and items) you are predicting for.\nHave you read the LightFM paper? The model works by estimating latent\nvectors and bias terms for features; the representation for an item is then\na weighted sum of its feature vectors. The methods I mention give you these\nsums given an estimated model and feature matrices. You can then computer\nthe scores efficiently using the latent vectors (taking their inner\nproduct) and adding item biases.\nOn Thu, 5 Jul 2018, 12:19 Si Wharton, notifications@github.com wrote:\n\n\nYes, you can also do that.\n\nWould using a mostly-empty set of user_features such as this have an\nimpact on the quality of the prediction - vs using a 'full fat' updated set?\nOne additional way of speeding up requests is by using the\nget_item/get_user_representations\nhttp://lyst.github.io/lightfm/docs/lightfm.html#lightfm.LightFM.get_user_representations\nmethods. Those will give you the latent vectors (and biases) given\nfeatures, and you can just do item_bias + dot(user_repr, item_repr) from\nthere.\nI'll be honest, I don't fully follow this suggestion I'm afraid! Would it\nbe possible to go into more detail?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/325#issuecomment-402676500, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA4WnTXYZkVRNVn9392SaHarPVUuZks5uDegtgaJpZM4VA4q8\n.\n. To evaluation functions (as per this line)?\n\nIt sets the score of interactions that are in the train set to a very low value, to simulate the fact that in production systems it's common to avoid recommending items that the user has already consumed.\nWhat do you mean when you say that 'you get different evaluation results'? Different from what? Different in what case?. It simply sets the scores of items present in the training set to MIN_FLOAT.\nIt absolutely makes sense that you would get (very) different results.. It looks like Sphinx doesn't like the formatting. Would you like to open a small PR to fix this?. Great, thank you!\nOn Mon, 9 Jul 2018, 11:31 Noel Kippers, notifications@github.com wrote:\n\nI'll find some time for it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/327#issuecomment-403434503, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA7KwD1eZi-qOVYDye7dxgrfG5Hutks5uEzDrgaJpZM4VHTJl\n.\n. This is now resolved. Thanks for your help!. I think it's a good idea to transparently instantiate a RandomState. I'd be happy to accept a PR for this (I think there are a couple of places where this would be an improvement).. To be clear, I meant your suggestion (1).\n\nOn Mon, 9 Jul 2018, 21:49 Noel Kippers, notifications@github.com wrote:\n\nI can't say I completely agree with the transparency argument but I'm\nhappy to make a PR for the second suggestions of making a copy of the\nRandomState.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/328#issuecomment-403615691, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCAyA3g8OHdL_-CLxyBmrvIL9mk_99ks5uE8HtgaJpZM4VIUTW\n.\n. @tianpunchh weights are not always one and zero: if you supply weights during dataset building, they can will be whatever you supply. Additionally, when fitting a model, you always need to supply the interactions, and also sometimes weights (but you can never just use weights).. I think thinking in terms of explicit rating is a red herring here, and may be confusing rather than aiding understanding.\n\nThe weights are not ratings: they are indicators about how important a given interaction is. Interactions with higher weights will have a larger effect on the model. This is exactly equivalent to the sample_weight argument in most sklearn models.\nYes, if you are providing weights you must always provide interactions too. You can never just give weights.. Yes, sample_weight gets the weight matrix. Let's say your user can both click on a product and purchase it: you may want to give a higher weight to the purchase interaction than to the click interaction. You could use explicit scores similarly: a score of 5 should get a higher weight than a score of 3. (A score of 1, on the other hand, might indicate that the user did not like the item, and you should not include the interaction at all in your training.) Keeping everything equally weighted is, as you suggest, a reasonable default.. 1. Interactions for the WARP and BPR losses are binary only. The values have no effect.\n2. You cannot swap them.\nIn the implementation, the presence of absence of an entry in the interaction matrix determines whether a gradient descent step is taken to updated the model to encode a preference. The weight determines the magnitude of that step.. Resolved. Thanks!. That's correct. You need to transform the ids using the Dataset mappings. You can get them from the mapping method.. Have you had a look at the docs and the example?\nIn particular, have a look at the docs for building features to understand what arguments you need to pass in to allow multiple features for each item: http://lyst.github.io/lightfm/docs/lightfm.data.html#lightfm.data.Dataset.build_item_features. To create mappings, you simply pass all possible features in sequentially.. Have you read the documentation? What have you tried that didn't work? If you give me a couple of examples I'll be able to improve the documentation for future users.. This doesn't look right! Should we try to figure out constructing the interaction matrix first? Do you want to walk me through what you are trying to do and what you expect to happen?\n(Can you also try using Markdown formatting for code blocks? It makes everything much easier to read.). I think I prefer to do it here.. This makes sense. How do you build a dataset from it?. I mean, how you create the sparse interaction matrix using the Dataset class?. Please use Markdown code blocks.\nThis looks OK; does the resulting matrix have the shape and density you expect?. Have a look at the Markdown guide to understand how to do code blocks.\nI'm sorry, I am not quite sure what your question is.. OK. You need to pass an iterable of (entity_id, [features of that entiity]) tuples.\nAssuming (for example) your features are in the Tags column, you could something akin to this:\npython\nfeatures = [(x['entity_id'], x['Tags'].split(', ')) for x in dataframe.iterrrows()]\ndataset.build_item_features(features)\nRemember you have to pass the features names to fit before running this so that feature name mappings are constructed first.. Does it work? If not, can you try what I suggested?. You have to combine all features for a given entity into a single list.. Hint: you single backticks to provide inline code formatting.\nYour understanding of features and the embedding process is correct. You can provide latent embeddings instead of binary features if you like; I don't really have an expectation if this will help or hinder the model. Try and let me know! And yes, negative feature weights are perfectly fine.. Looks great, thanks!. Thank you!. I appreciate the effort you have taken to make this work with tqdm being optional. Still, I'm not sure I want to add such interactivity-focused pieces of code. Is there a way you could convince me?. Thanks for being patient :)\nThat's fair enough. Let's merge!. How about using the predict method?. The way you exclude training interactions when using predict is by not passing the item ids of the training items into it.. That sounds like a good idea. I'll try to do that.\nThe vast majority of user interactions are user questions, of varying degrees of sophistication. Have you found any actual issues that you think are worth addressing? I can tag them appropriately.. This is a very unusual setting. The only way this will work better than random is if you use user features. If those are reasonably predictive, you should be able to get an OK model.\nHave you tried something simpler that tries to predict the items as a function of user features, like a random forest?. The most common reasons for model divergence are:\n\nLearning rate which is too high.\nFeature weights which are too large.\n\nCan you check those things first?. What values have you tried? Have you tried without user/item features?. Is your test set nonempty?. Can you post the line of code you use to obtain the AUC values?. I don't know the immediate reason why you are getting the result.\nYour dataset is extremely small, much smaller than anything that can be reasonably used in a real application. For this reason you may be hitting an edge case that I have not seen before. If you figure out what the problem is, let me know and we can make sure that LightFM handles your use case too.. Have you had a look at https://github.com/lyst/lightfm/issues/330?. What do you mean when you say it doesn't work?. Is that feature in set((users['age'][i[0]-1], users['gender'][i[0]-1]) for i in ratings.values)?. But is it in set((users['age'][i[0]-1], users['gender'][i[0]-1]) for i in ratings.values)? It looks to me like that set contains tuples of (age, gender) rather than age and gender separately.. Just make them into a single flat collection.. I would not recommend doing that. In hindsight, a service with some models in memory and responding to requests is a much simpler solution.. @ahayamb you can get the get_user/item_representations methods to dump user and item embeddings and biases as numpy arrays. Once you have those the score for (user i, item j) is given by\npython\nscore = np.dot(user_embeddings[i], item_embeddings[j]) + item_biases[j]\nSince these are plain numpy arrays, you can keep them on disk and memmap them into the memory of each of your individual processes.. You should filter them, preferably before obtaining scores.. Only pass those ids that match the filter into the prediction functions!. User/item indices are indices into rows of the user/item features matrices. Construct a row in the features matrix that corresponds to the user/item you want (at some index X), then use id X to refer to that row.\nIs there a reason why you ignore the code formatting I specifically ask for in the issue template?. LightFM doesn't really use ratings. Have a look here to see why I think ratings are a bad idea.. It's called no_components in the code.\nIncreasing it will always increase runtime; after some point this brings diminishing accuracy returns, and may even worsen accuracy.. That's correct!\nOn Wed, 25 Jul 2018, 11:59 Nghi D. Q. Bui, notifications@github.com wrote:\n\nMy guess, the algorithm doesn't care about the threshold, as long as there\nis interaction, says > 0, means relevant, it is correct?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/348#issuecomment-407716254, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA3xC4hkdw3fe9Ltncm0nnNkV__yfks5uKE90gaJpZM4Vecn2\n.\n. I think this is an accurate description.\n\nThe purpose of this is as follows:\n1. A good recommender model will definitely recommend items the user has already seen.\n2. The test set does not contain the items the user has already seen, so if they are ranked highly, this will reduce the measured accuracy on the test set.\n3. To eliminate this effect, we can exclude the already-seen items from the evaluation.. This is correct, but please use the get_item/user_representation methods to get the representation matrices.. For batch predictions like these, I would suggest getting the embeddings out (get_item_representations, get_user_representations) and performing the prediction using those.\nIf you would not mind something slightly slower, I would call predict for every user separately, perform the sorting and storing of top items, then move on to the next user. Setting aside the storage of results, you should be able to perform this in constant memory.. @RothNRK sure, but let's first make sure that predict works in constant memory. Can you post a snippet here that reproduces the unbounded memory growth problem on the Movielens dataset? I'll then run it on my end and see if I can find the culprit.. Any luck? Are you still experiencing problems?. The model uses randomness internally, so the results will be different with every run unless you fix the random seed. The constructor of the LightFM class allows you to set the random state.. I'm uncomfortable with calling this a tunable hyperparameter. With the right (other) hyperparameters and enough data, the choice of random seed should have a small effect on the final accuracy. In fact, the setting is mostly there for testing purposes: in applications, you would not hold it fixed and let it very from fitting to fitting.. 1. It's the dot product plus item bias.\n2. Yes. Construct an item feature matrix that includes the new items as a column, then pass it to predict. User and item ids are just row indices into their corresponding feature matrices, so as long as you point at the right row with the right features you can predict for an item/user with any combination of features whose embeddings were estimated during training.. This is absolutely correct. You can simplify your code by converting feat_idxs to a sparse matrix and then getting out the representation by calling get_item_representations (this is precisely the use case this method is for). For example:\n```python\ndef cold_start_similar_items(feat_idxs, item_feat_mtx, model, N=10:\n    feat_mat = scipy.sparse.coo_matrix((np.ones_like(feat_idxs),\n                                       (feat_idxs, np.zeros_like(feat_idxs))))\n    repr, bias = model.item_embeddings(feat_mat)\n    scores = item_representation.dot(repr[0].T)\n# snip\n\n```\n(Treat the above as pseudocode.)\nSlight fix to your code: you should not be using biases, these have no use for similarity computation.. @hodaraad as @eggie5 suggests, you can dump user/item representations (these methods) into numpy arrays, then perform the user-item dot products (+ item biases) on the Ruby side as desired.. @hodaraad you should use the methods I liked to above.. For this sort of real-time updating I would recommend trying another package of mine. I wrote a blog post on how to use it.. It looks like you're using the 0 index in the code above but the 1 index in your predict call.. You need to use the same index in your code and in the predict call.. This passes, using your code from above, fixing variables u1 -> user1 and i1 to item1:\n```python\n    import numpy as np\n    from lightfm import LightFM, datasets\n    data = datasets.fetch_movielens()\nmodel = LightFM(learning_rate=0.05, loss=\"bpr\", random_state=SEED)\nmodel.fit_partial(data['train'], epochs=10)\n\nuser_biases = model.get_user_representations()[0]\nuser_embeddings = model.get_user_representations()[1]\nitem_biases = model.get_item_representations()[0]\nitem_embeddings = model.get_item_representations()[1]\nuser1 = user_embeddings[1]\nitem1 = item_embeddings[1]\nscore = user1.dot(item1.T)\nub1 = user_biases[1]\nib1 = item_biases[1]\n\nresult = score + ub1 + ib1\npredict = model.predict(1, [1])\n\nassert np.allclose(result, predict[0])\n\n``. Negative scores in themselves do not indicate that there's anything wrong with the model: think of the scores are just arbitrary numbers that enable sorting.\n. I would recommend the normalization that's used by default by theDataset` class.. That's right. You may still be able to get reasonable predictions if you are using user features.. That sounds about right!. Again, @DoronGi's answer is exactly correct.. You should use method 1. This is what the built-in Dataset class does.\nThe feature matrices are sparse, so you don't have to worry about matrix\ndimensions.\nOn Wed, Sep 5, 2018, 22:38 astrung notifications@github.com wrote:\n\nWell, does anyone has other ideals ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/370#issuecomment-418970516, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACSCA-ESgbxLqI0_qjxzpj9KyUCxLR6yks5uYLS4gaJpZM4Wa8jf\n.\n. 1. Yes. Every row can have different feature weights. Think of it as a sparse matrix.\n2. The continuous value is basically the feature weight.. @DoronGi's answer is exactly right. Thanks!. Pass [ItemA, [providerX, Tag1]] and [itemB, [providerZ, Tag1, Tag2, Tag3]] rows into the feature matrix construction method.. @hugos0910 @raf-1 the elements of the collection you are passing as the item_features argument to fit_partial are precisely the features that will be created. If you pass tuples, your features will be tuples.\n\nPass a flat list of your tags instead.. You are trying to create an [Asian, Female] feature (and so on). To create the feature name -> feature index mapping, feature names must be hashable, as they are turned into keys of a dictionary. Lists are not hashable because they are mutable.\nI strongly suspect you do not want to  create cross features, so you should be passing a list of the form [Asian, Female, Hispanic, Male]to fit.. This is a great answer. Thanks, @DoronGi!. The Dataset class does this automatically. Have a look at the class docstring.. This is not strictly correct, as it doesn't save and restore model hyperparameters.\nBased on the principle that pickling is dangerous and inefficient, it may be useful to have something like this. Would you be willing to work on this a little more to whip it into shape?. Looks good, but I have two more comments.\nFirstly, what do you think about making load a class method? I think it might be clearer to not need to instantiate a model class first, and then completely override it. This way, instead of\npython\nmodel = LightFM()\nmodel.load(path)\nwe would have\npython\nmodel = LightFM.load(path)\nSecondly, can I ask for one more test please, along the following lines:\n1. Fit a model.\n2. Evaluate it.\n3. Save and load it.\n4. Evaluate again. The metrics should be exactly the same.. Thanks for your patience. I poked Circle and will have another look.. Thanks, @DoronGi!. Have you tried the weights argument to fit?. Did that help?. Hey all, sorry about the spam. I've blocked Mr Khan from posting any further issues in this repository.. @hugos0910 that's a great explanation, thank you!. This is exactly correct. Thanks again!. This is correct. Thanks for your suggestions, @DoronGi!. I am afraid not.. Are you using sparse matrices for your features? Would sparse matrices help?\nDo the matrices themselves take a lot of memory, or is memory mostly used by various intermediate objects constructed as you are preparing your matrices?. For a back-of-the-envelope calculation, consider that a float matrix with 10 features and 25 million rows takes 1 gigabyte of memory.. With LightFM, you can stream interactions data, but you always have to load the feature matrices into memory. You could experiment with using memory-mapped numpy arrays, but my recommendations would be to rent a bigger server.. You can use memmapped arrays even when you are modifying them.. Sparse arrays are composed of constituent numpy arrays. You can memory map those.. Thanks for letting me know!. Fixes #390 . 1. This is partially, but not entirely correct.\n2. The position does not matter. Your features should look similar to the following:\n[\n    (item1, 'price:1', 'accept_credit_cards:False', 'smoking_allowed:True', 'category:bar'),\n]\n4. Start with something that easy and quick to fit, like 32 or 64 components and 10 epochs, and go from there. You will quickly get an intuition for what works and what does not; follow this up with a fuller hyperparameter search before deploying to production.\nHope this helps!. This is because by default, LightFM allocates a separate feature for every user and item: think of these as embeddings/vectors that encode each user and item, and how they related to each other. This is where most of the model's expressive power comes from. \nTo see this, you can disable the user_identity_features and item_identity_features options in the constructor for the Dataset class and compare the accuracy you get out of the model.. This is a common characteristic of recommender systems.\nI think your initial intuition of weighting your observations is on the right track: you can think of it as applying inverse propensity weights.\n(Slight nitpick with terminology: this is implicit feedback data, but not really implicit ratings. I think it's best not to think in terms of ratings at all.). Fixed in #401. Thanks @DPGrev!. Thanks!. The user feature matrix is just a matrix object: you can change it however you like. When you pass the changed value into fitting or prediction routines the change will be reflected.. Thanks!. Thanks!. Can you prepare a snippet that I can run that reliably reproduces the issue?. Closing unless we can reproduce.. Thanks @DPGrev!. You have to map your ids to a contiguous range starting at zero. The Dataset class helps with this.. Though the matrix dimensions stay the same (as they must), the non-zero elements are split between the test and train matrices. Count the number of nonzero elements in each to confirm this.. It's worth pointing out that you don't need the interactions to serve the model. Just the model itself plus the mapping dictionaries.\nYou only need the interactions in training.. You can always set the embedding and bias attributes on the model instance to your pre-trained embeddings.. You can set the embedding fields on your model instance to whatever you desire.\nThis is circumventing the public API of the model, but will allow you to accomplish what you are trying to do.. This is a little unusual, but nothing I would worry about as long as you can find a solution through other settings.. Could you restate your question for me? I am not quite sure what you are asking.. This feels like a question more about the motivation and advantages of various information retrieval metrics than LightFM. Others will be more competent here than I will. Perhaps try CrossValidated?. This is why ordinarily you would not deserialize on every request. It is more common to deserialize once at server start up, then keep it in memory to serve multiple requests.\nI am not familiar enough with the specific of Google App Engine to say what the equivalent there is.. I would suggest creating three features, corresponding to all possible values of your input. This way the model can learn three sets of parameters: what it means for the feature to be there, not to be there, and be unknown.. That's actually quite a nice result. I am not sure whether this is guaranteed in a ranking model. However, if you were to treat the model as a probabilistic model, estimating the probabilities that a given item will be watched by a given user via maximum likelihood, you would hope that this global property would hold.. It's hard to say without knowing what your features are, but this does not look unusual. Features allow you to generalize better beyond the training set when your data is sparse.\nI'm not sure I get your second question!. Thanks! Would you care to make a PR? This should be an efficient way of doing it.. Thanks!. Do I understand this correctly when you say that you have pairs of (tweet author, tweet)?\nIf this is the case, then I think the best you can do is say things like \"people from country X write tweets that contain words V, Y, and Z\". Is this a relationship you are interested in?. It does look like some of your features are too granular and help you overfit on the training set. Dropping rare features or adding regularization might help. 1. That depends. Running on old data as well as new data will give you as model that is relatively backward-looking, as it pays more attention to old data. Running only on new data will make it adapt faster to new data, but potentially at some cost to model stability. Once you have your system up and running, I would recommend A/B testing this.\n2. When training? Sure, that sounds very sensible. That way you lose less progress if your machine goes down etc.\n3. Yes! One thing that LightFM does not support, though, is adding new features. There are ways around this, but there is nothing out-of-the box.\n4. Up to you, this may be helpful to understand how things work.\n5. The general assumption is that there is only one interaction per user/item pair. You can customize this to your liking by manually constructing your interactions matrix. . One sketch of what you might want to do is this:\n1. Consume new interactions from your queue.\n2. Once you have some fairly chunky number of new data, construct a dataset containing new data only, and call fit_partial on the model.\n3. Re-generate your recommendations.\n4. Run a separate job that trains a new model from scratch in a batch fashion. You can use this model to add new features/users/items.\n5. Periodically swap in the new batch model to be updated from your queue.. One of the reasons of the get_item/user_embeddings method is to allow fast prediction via BLAS, as you do in your example.\nThere are tricky balancing issues here:\n1. Materializing the embedding matrices is slow.\n2. Materialized matrices are large.\n3. There is an operating point where predict is faster than doing the above.\n4. What does the signature of the hypothetical fast_predict function look like? Does it compute recommendations for all users? A subset of users? All products? Subset of products? How much memory does it take to store all predictions for all users?\nFor that reason I have so far been reluctant to add this to the library. If you're interested, we could add a guide for fast predictions to the documentation?. That's some great debugging! It may be useful to add this cause as another suggestion in the error message.. In fact, the Dataset class does exactly that: always adds a diagonal matrix to the feature matrices.. Great answers, @DoronGi and @SimonCW.\n@Ivanclj you could try one of the following things:\n1. Use city names instead of coordinates.\n2. Compute geohashes (tuning the granularity) from your location data.. It works OK with continuous variables. The bigger issue is that, in your case, it doesn't really make sense to treat lat/lon as continuous variables: do you really believe that a certain item should be recommended in proportion to how far north or south of the equator it is?. Should this be /opt/local/bin/gcc-mp-[0-9]? and the next line /usr/local/bin/gcc-[0.9.[0-9]? That's because gcc-mp is the macports version and installs into /opt/local/, and the Homebrew version goes into /usr/local/ and does not have the mp infix?\n. Could you use getnnz(axis=1) here? (If I understand correctly, you're counting the positives per user, and this would make that more obvious than doing the sum.)\n. Why np.divide instead of \\?\n. Could you add an empty line before Returns and Parameters (above) please?\n. Can you add a rudimentary grid search test just for completeness?\n. I'll have a think about what's useful.. Could we leave this change out? I prefer not aligning things.. Can you have a look at the CircleCI build results and address the lint violations?. You can use Python's truthiness rules to simply write if print_output. Will not work in Python 3: let's use range instead.. I am not sure what is going here but it almost certainly shouldn't be going on :). Can we use the get_item_representation methods here?. Could you call model.get_item_representations(movielens['item_features']) instead of the multiplication (as per the docs). The effect will be the same, but it's probably a more straightforward experience for the user.. This is pretty great, nice to have it there.. Did you actually write the HTML?\nYou should use Markdown instead.. Should be: dependencies. This is, in fact, the loss of the current example. So not what you want at all!. Do we need to convert to bool?. Can we not just check n_intersections = bool_intersections.getnnz()?. Can we reformulate this to better reflect the fact that this affects training as well? Something like the following:\nIf True, user and item vectors will be rescaled to unit length during training and prediction.. Can we call this compute_norm? I think it better reflects what this is actually doing.. Thanks!. I think the argument name should remain random_state, but the code should be changed to do the following:\n1. If a RandomState instance is passed, behave exactly as before.\n2. If something else is passed, construct a random state from it, treating it as a seed.\nThe reason is backward compatibility:\n1. Uses in the wild will rely on the argument name.\n2. Uses in the wild will rely on the function working with a RandomState instance.\nAn (slightly overcomplicated) example of this is here.. Do we need this file?. Do we need this file?. Excepts => Accepts. Since this is an evaluation routine, there is no training involved. Why do you say 'trained'?. Is there a compelling reason to change this file?. Ditto.. Got it, thanks.. When the test fails, this is never executed, and is never cleaned up. Could you use pytest fixtures for setup/teardown?. ",
    "saihttam": "This does not fix the problem completely regretfully as it results in the problem addressed issue #25. There are several gcc-related binaries in that folder. For me this includes:\n['/opt/local/bin/gcc-ar-mp-4.8', '/opt/local/bin/gcc-ar-mp-4.9', '/opt/local/bin/gcc-mp-4.8', '/opt/local/bin/gcc-mp-4.9', '/opt/local/bin/gcc-nm-mp-4.8', '/opt/local/bin/gcc-nm-mp-4.9', '/opt/local/bin/gcc-ranlib-mp-4.8', '/opt/local/bin/gcc-ranlib-mp-4.9']\nBut you don't want the -ar- 's and the -ranlib-'s...\nSo I guess the setup must be more selective....\n. Maybe something like this? (That solves the compile issue for me)\n    patterns = ['/opt/local/bin/gcc-[0-9].[0-9]',\n                '/opt/local/bin/gcc-mp-[0-9].[0-9]']\nAnd I guess you would want to treat /usr/local/bin/ similarly...\n. Tried\npython setup.py test\n(30 passed in 128.18 seconds)\nand the movie lens example and both work fine. \nThanks!\n. ",
    "WheresWardy": ":+1:\n. ",
    "oddskool": "Ok, #24 fixed part of the problem, however I had to modify the CXX flags to get it to compile:\n-                                          '-march=native',\n+                                          '-march=corei7',\nOtherwise gcc complains about unknown asm instructions (e.g. no such instruction: vmovsd 16(%rdi), %xmm0).\nIt's a bit odd since native architecture should be detected as corei7 I presume.\n. Yes I do. \nTrying to compile with the brew python (2.7.9) in the path instead of the Anaconda one still triggers the \"no such instruction\" error.\nSo it seems that the error may be more related to something else (cpu architecture detection maybe?).\nThe gcc command with brew python:\nbash\ngcc-4.9 -fno-strict-aliasing -fno-common -dynamic -I/usr/local/include -I/usr/local/opt/sqlite/include -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c lightfm/lightfm_fast.c -o build/temp.macosx-10.9-x86_64-2.7/lightfm/lightfm_fast.o -fopenmp -march=native -ffast-math\nOutput of uname -a:\nDarwin 14.4.0 Darwin Kernel Version 14.4.0: Thu May 28 11:35:04 PDT 2015; root:xnu-2782.30.5~1/RELEASE_X86_64 x86_64\nThis is a MacBookAir under Yosemite.\nAt this point I managed to get past the issue and I believe it is likely related to a library or configuration you use, so maybe a sensible thing to do is to keep this thread for other users...\n. Indeed, gcc's native arch seems to be x86_64:\nbash\n$ /usr/local/bin/gcc-4.9 -dumpmachine    \nx86_64-apple-darwin13.4.0\n. with #26 same \"no such instruction\" error :(\n. Yes it works on my setup :+1: \n. ",
    "jongwook": "Rebased ^_^!\n. @maciejkula wasn't aware of the -mp- difference; does it look okay now? thanks.\n. @hammadkhann This happened before in #358, and I've also submitted only one PR. Don't slam mentions to people you see in the contributors page, it is inappropriate. btw, you need to better describe your situation  and write proper question.. ",
    "dustinstansbury": "Yosemite 10.10.3\nPython 2.7.6\n. That definitely fixed the error related to -Wshorten-64-to-32 flag, thank you. \nHowever, I'm still getting some assembly errors. It's apparently related to this: http://stackoverflow.com/a/10894096.  \nSeems like a possible fix is to replace the flag -march=native with  -mno-avx, or, to add the flags -Wa,-q\n. Removing -march=native seems to have addressed the assembly errors. Will let you know if I run into anything else. Thank you for your time!\n. Hmm, looks like removing the native architecture flag compiles for the wrong architecture:\nImportError: dlopen(/Users/dustinstansbury/.virtualenvs/test/lib/python2.7/site-packages/lightfm/lightfm_fast.so, 2): no suitable image found.  \nDid find: /Users/dustinstansbury/.virtualenvs/test/lib/python2.7/site-packages/lightfm/lightfm_fast.so: mach-o, but wrong architecture\n. ",
    "RajeshThevar": "Hi, What should be the details at function define_extensions(file_ext) ? I am getting error around this function when trying to setup.py install \nNo argument is passed when calling define_extensions() at line 167, End of file. . ",
    "tdeboissiere": "I finished 68.\nI will refactor the code to PEP8 and might add some figures as well. All directories should normally be created when cloning the repository. I can automatize the data downloading with wget. But you still need to have a Kaggle ID and password.\n. The competition organisers will release the data for research at some point but it is not yet clear when. In the meantime, there is no other option.\nFor now it might be better to simply include a link. I will refactor the notebook when the data is public.\n. You can use the github link I gave in my first post. I will update the notebook when the data becomes public.\n. ",
    "jakubLangr": "https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook\n2015-11-12 18:10 GMT+00:00 maciejkula notifications@github.com:\n\nCan you link to the dockerfile please? I'll pull it and try to reproduce.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/lyst/lightfm/issues/39#issuecomment-156187088.\n\n\nJakub Langr\nMobile (UK): +44 7511 624 004\nMobile (CZ): +420 606 754 679\nMobile (US): +1 650 338 8807\nSee my blog at jakublangr.com http://goo.gl/Zbq4t3!\n. ",
    "lenguyenthedat": "Duplicated: https://github.com/lyst/lightfm/issues/29\nDefault OS X's Python is definitely not a good idea to be used for development work :smiley:\n. ",
    "gglanzani": "@maciejkula #42 compiles on my machine. Will keep you posted if I incur into openmp issues\n. Hi Maciej, I'm expecting to have some time on Wednesday afternoon. I promise to let you know by then (like, really promise!)\n. @maciejkula Contrary to my promise, I tried it a little earlier (now). It works (i.e. in the example notebook I arrived up until the AUC score, so I guess that's enough to test the openmp).\n. ",
    "hiyijian": "Thanks. It gets more clear after reading codes\n. ",
    "springcoil": "Thanks man. You can probably merge that now. \n. Your interaction matrix needs to be positive and negative. The value 4.0 is arbitrary you could pick 3.0 for example. \n. Is this correct @maciejkula ?\n. ",
    "xiaop1987": "@maciejkula surely I'm happy, thanks a lot :).\nI take this step as making explicit feedback training data, but when I reading the example/movielens/readme.md which said that warp loss is used in implicit feedback, did I miss anything?\nbtw: I'm implementing warp loss in spark as  recommending algorithm, but the  result is not so good, would you mind if I posted some problems about warp loss(wsabie)  by email?\n. @maciejkula  yes, that would be very appreciated!\n. ",
    "seanlaw": "+1\n. Tried this with the implicit feedback section of the MovieLens example:\n\nmodel = LightFM(learning_rate=0.05, loss='bpr')\nweight = train.copy()\nweight.data = np.random.rand(weight.getnnz())\nmodel.fit(train, epochs=10, sample_weight=weight)\ntrain_precision = precision_at_k(model,\n                                 train,\n                                 10)\ntest_precision = precision_at_k(model,\n                                test,\n                                10)\ntrain_auc = full_auc(model, train)\ntest_auc = full_auc(model, test)\n\nprint('Precision: %s, %s' % (train_precision, test_precision))\nprint('AUC: %s, %s' % (train_auc, test_auc))\n\nI might be doing something wrong but I'm trying to pass random weights and I expected the results to be different when calculating the AUC. The results were the same as the original MovieLens example without weights passed.\n. Awesome! I wonder if it makes a difference if we did an inverse decay where older ratings are weighted higher than recent ones. Just curious. Let me know if you want me to test anything else.\n. @maciejkula I was already subscribed for notifications to this issue :)\n. Yeah, I just used the plain default image from Docker and then built lightfm using the provided Dockerfile. I added this line to the Dockerfile:\nRUN apt-get install -y libopenblas-dev gfortran\nAnd the install succeeded (though, with a lot of warnings). Should this line be added to the repo? Otherwise, the Docs won't reflect something that runs properly.\n. No, I had not built the extension and noticed that Cython is not in the Dockerfile as well.\nThen, I did the following:\npip uninstall lightfm\ncd /home/lightfm\napt-get install -y cython\npython setup.py cythonize\nls lightfm\nThis shows that the extension is present (the lightfm_fast.pyx file). And then I install lightfm and ran the test again:\npip install .\npy.test -x tests/\nThis results in the same error as my original post. As per the document:\nIf you\u2019d like to use the multi-threading capabilities of LightFM on these platforms, you should try using it via Docker as described in the next secion.\nMy goal is to be able to get multi-threading working on my Macbook Pro via Docker. \n. ",
    "VaibhavBehl": "+1 would be a very good feature addition\n. ",
    "musicformellons": "Ok thanks. Actually:\n1) I found your article http://ceur-ws.org/Vol-1448/paper4.pdf which gives some clues regarding production setup.\n2) From what I've seen it seems predictionio has 'hybrid' planned but not yet there.\n. Ouch, you closed it after having me in the dark for 5 months... a small comment, please!!\n. ",
    "bejjani": "Great, thanks!\nHow are the hyperparameters tuned (step size, regularization)?\nI am thinking of using the same rank estimation approach as in WARP when computing the loss on the validation set to let it scale (specially when doing grid search + CV)\n. ",
    "ricardoatsouza": "Thanks for your reply, and sorry for my late reply.\nScipy version is 0.13.0b1\nI found out by executing:\nimport scipy\nscipy.__version__\n. I wasn't aware that my SciPy version was too old.\nI updated it and, indeed, that was the problem.\nThank you!\n. ",
    "paoloRais": "Yes it does, thanks!\n. The unsuccessful test does not appear to be related to the code I added...\n. Sorry, I was fixing the type casting thing that was causing the failing tests in Python 2.7 and I didn't see your message.\nCleaned up now.\n. Right, corrected now.\n. My lint is catching only a problem on line 69 in test_evaluation.py...\n. The CircleCl fail seems unrelated.\n. I wrote this code to be able to compute the scores for on a (no_users, no_items) matrix, with a syntax as close as possible as predict_ranks.\nThe advantage over using predict is that the user representation is computed only once for each user in the matrix, so it should be faster for large matrices than predict_lightfm (which re-computes both the user and item representations for each user-item pair).\nI agree that is also possible to extend predict to obtain the same effect (using predict_scores instead of predict_lightfm), I will look into it if you think it is a better solution. \n. Sounds good. I should have some time to look into that in the next days.\n. The appveyor fail is really strange, I wonder if it is due to the testing using Windows, since on Linux I can't replicate it.\n. I think you can obtain what you want by switching the role of the users and items, that is by giving to the model :\n- your users as the model's items input,\n- you items as the model's users input.\nIn this way you would get the best users for each item and can use the code as is.\n. Right, done.\n. Right again, more readable (and in any case they are both numpy arrays so the result is the same).\n. ",
    "lesterlitch": "I did a rough example using annoy for item-to-item filtering and an example of maximum inner product search with ball tree. If the approach is sound I could tidy this up for an example?\nhttps://github.com/lesterlitch/misc/blob/master/Light%2Bfm%2Bannoy%2Band%2Bproduct%2Bsearch%2Bexample.ipynb. Ok I've tidied up and done an example of using NSMlib and Annoy for item and member recommendations. I also did a speed / scaling chart. I haven't worked out how to do a bulk predict yet in order to calculate avg. P@K, but there is a qualitative example. \nhttps://github.com/lesterlitch/misc/blob/master/Light%2Bfm%2Bannoy%2Band%2Bproduct%2Bsearch%2Bexample.ipynb\n. Thanks for the response!\nI have yes, using the learn function - i.e. normalize(raw_member_features_matrix, axis=1, norm='l1')\nHere are the first rows from the item and user feature csr matrices:\n```python\nitems_features[0,items_features[0].nonzero()[1]].todense()\nmatrix([[ 0.2189845 ,  0.18301879,  0.29823944,  0.19250721,  0.62113589,\n          0.28761694,  0.4387733 ,  0.15228976,  0.32908452]], dtype=float32)\nmembers_features[0,members_features[0].nonzero()[1]].todense()\nmatrix([[ 0.01500955,  0.00687691,  0.00488463,  0.03807613,  0.01714612,\n          0.06524359,  0.01370857,  0.0203032 ,  0.0091073 ,  0.01899276,\n          0.0170573 ,  0.03180252,  0.03951597,  0.03765749,  0.02067481,\n          0.00863998,  0.03003284,  0.010614  ,  0.01699004,  0.02135187,\n          0.02568188,  0.02606232,  0.01938645,  0.06161183,  0.0126634 ,\n          0.01294042,  0.00720311,  0.030777  ,  0.01884086,  0.01178526,\n          0.05592889,  0.02763181,  0.00907691,  0.01116292,  0.01343661,\n          0.01717991,  0.01464464,  0.00726902,  0.01353738,  0.00541887,\n          0.01728139,  0.01083446,  0.04138919,  0.01978991,  0.05642271,\n          0.00835726]], dtype=float32)\n```\n. That's right, What I'm trying to do with that is go from items_features (a csr matrix of shape n_items, n_features) to a dense matrix of the values in the first row of that matrix, just to show the non-zero values are normalized between 0 and 1. \nitems_features[0] gives:\n<1x2790 sparse matrix of type ''\n    with 9 stored elements in Compressed Sparse Row format>. Cool, will try those and come back to you. Cheers.. Unfortunately not. I tried reducing the scale further (0-0.1), reducing the learning rate (several values), removing regularisation and running with only 4 threads. Strangely I can get a result with either only user or only item embeddings but not both. I'm not sure if this is a factor, but I have many more users than items (around 10x).. In the new version I get:\nValueError: Not all estimated parameters are finite, your model may have diverged. Try decreasing the learning rate.\nLearning rate is 0.001 and have tried down to 0.00001. Have normalized features between 0-1, but also tried 0-0.1 and 0-0.01\nMy datasets look like:\nitems_features\n(<39267x2790 sparse matrix of type ''\n    with 335801 stored elements in Compressed Sparse Row format>,\nmembers_features\n <305803x2790 sparse matrix of type ''\n    with 14772846 stored elements in Compressed Sparse Row format>,\ninteraction_matrix\n <305803x39267 sparse matrix of type ''\n    with 3767965 stored elements in COOrdinate format>). Just had a chance to revisit this. \nWhen I recreate my matrices with random floats and ints, same value scale and same sparseness / shapes, I didn't encounter the same problem. \nAfter investigation I discovered a bunch of empty rows in my member / item features. It seems the model can handle a few, but in my case there were 700 or so, and that was enough to push parameters to infinity. \nIs this expected behavior?. Hi, sorry I was so slow on this. I've done a bunch more testing and found that when using very sparse factors for users and items, the learning rate needs to be very small to prevent divergence. This was an issue previously, possibly because of the numerical stability issues you mentioned? Anyway, after upgrading and retesting I can get the model to fit by adjusting the learning rate. Thanks!. Changes haven't come through to this branch - recreating pull request.. I've updated that in the branch and rebased. All look ok?. That's done.. No problem to refactor, update variable names etc. Will have to write a method to calculate AUC using the indexes, but shouldn't be too bad and should be done to demonstrate performance as noted. Sorry have been slow to respond but a lot on at the moment. Cheers.\n. I've attempted to incorporate all the feedback so far. I've added a precision at k comparison with the ANN libraries, amended the variable names etc. and added a bit more explicit commentary. I've also tidied up the timing function to make it a single loop. Cheers.. That's great to know about NMS, I will have a look at index building times without that option. And yes this notebook was inspired by that post :). Yes much better.. I can only see this used with 1d arrays? Looks like a useful function.. Done.. ",
    "LongbinChen": "believe it or not, using numpy's matrix multiplication is fast enough for most of the application. with a 256 vector, search from 1 M doc only cost 100ms. . ",
    "hammadkhann": "Right now I am trying to use annoy using item embedding and I am confused about how apply my filters on the result and also how to incorporate user embedding while giving recommendations ??? @lesterlitch  @maciejkula \n. Kindly make it for python please.. Hey I have a list of tags in a dataframe column against each entity and same goes with categories how do I feed it into the build_item_features function kindly help I am confused?. Can we talk some where else?? Linkedin or somewhere?. Ok then lets start with user interaction matrix the idea was to recommend user the brands or entities of particular category in which the user is searching to make interaction matrix I use the user activity data in which if he views some entity details and he have interacted with it similarly if user have wishlist some entity so he likes it. So the above user_interaction dataframe have been created using these two measures.. Is this the right way to make interaction matrix?\n. we have our data on elastic search I did a query for rest call which was responsible for viewing entity details so it gives us the entity id and user id who has viewed it, and wishlist data was queried by user profile data.. right now I am doing this using entity id and user id from the dataframe I have built I have also attached the snapshot above of named user_interaction dataframe\n(interactions, weights) = dataset.build_interactions(((x['user_id'], x['entity_id'])\nfor index,x in user_interaction.iterrows()))\n. I am fitting my data like this: \nis it right to take entity ids from entity_data and not from user_interaction? because not all the entities id will be present in the user_interaction data ?\ndataset = Dataset()\ndataset.fit((i['user_id'] for index,i in user_interaction.iterrows()),\n            (i['entity_id'] for index,i in entity_data.iterrows()),\n            item_features = entity_final[feat]\n            )\n. yes it has same shape and density I expected.. I am sorry let me see it\n. dataset = Dataset()\ndataset.fit((i['user_id'] for index,i in user_interaction.iterrows()), (i['entity_id'] for index,i in entity_data.iterrows()), item_features = entity_final[feat] )\nThats how I am fitting my data. Anyways now tell me how can I make my item_feature_matrix? I have attached the dataframe screenshot above name of the dataframe is entity_final.\n. Below is the code through which I am making item feature dataset right now\nentity_features=list(entity_final.columns)\n```\ndeleting entity id from entity features list.\ndel entity_features[1]\n\nbuilding item features dataset\nitem_features = dataset.build_item_features(((x['entity_id'], feat) \n                              for index,x in entity_final.iterrows()))\n```\nprint(item_features)\n. Yes I know that thats why I have passed feature names at the time of fitting the dataset. It is compiling but not giving me good results and also I am not sure what is happening underneath with these features thanks for your help let me try your thing.. If I have to use multiple features of an entity like with tags I also use entity rank and rating as a feature so how will I pass it?\n. Ok thankyou Its working great now :). and how can I do it exactly?. Ok thanks :+1: . Right now I am trying to use annoy using item embeddings and I am confused about how apply my filters on the result and also how to incorporate user embeddings while giving recommendations ???. Do I have to retrain my lightfm model on full feed data everytime? because I have around 70gb data rightnow which I gather from elastic search and then transform it for my model and it takes any solution for this problem I dont know how to use it in production . @RothNRK No I know about I was asking about how reduce training time or only train my model on the new data not on full feed does that work?. Great it means I have to train my model once one full data offline and then use fit_partial on the new production data? . Do I have to load user_features and item_features to pass them in predict method while getting predictions in production server?? I think my server memory will run out because of this? How do you guys manage this problem?? @maciejkula @RothNRK . @nocedan  Rightnow I am filtering my item_ids before prediction based on users country,city and in which category he is currently is and then I pass those item ids to the predict func this filtering part takes time when done on runtime and also I have to load items_data in to memory and then apply filters do you have any solution to that?. @nocedan I have not deployed it yet but on my local  machine my call is taking around 580ms. @nocedan Another filtering process I am doing i missed before is on items tags similarity in which I have to compare  list of tags of each item_id against the list of tags of item_id passed in a post call means user is on that item right now then find their intersection if they are 80% similar then I pass those item ids to the predict function Can I skip that and pass Item id to predict func lets say my user is seeing details of Mcdonalds to my model recommends the user similar items like burgerking,kfc ??. @nocedan I have optimised it rightnow it works fast i will tell you the exact time after measuring but as the data grows it will increase so I am worried about the scalability issue in future.\n. @nocedan Yes i have seen the paper and documentation. Anyways can you guide me about online training of my lightFM model in production how can I do it do I have to use fit partial or what?. Before finding similar items I want to filter my items based on some criteria like city,country,category\nwhat should be the optimal way to do this?? in terms of memory and speed @fischjer4 @maciejkula  @trepca . @fischjer4 from where I will find the indices from item data??\n. @fischjer4 I am confused about filters you defined. Like I just define my filters as [city = \"Karachi\", country=\"Pakistan\"]??. @maciejkula @fischjer4   what do you guys  prefer filtering item representation space and using cosine similarity to get similar items and pass those item_ids to predict function or I filter items using item data and then pass filtered item_ids to the predict function on real time prediction? I am talking in terms of speed and memory usage or is there any other optimal way to filter items on real time and get recommendations?. @fischjer4 thanks. @fischjer4  I am getting only two items in filtered_item_feat_idxs when used filter as [\"Karachi\",\"Pakistan\"] although there are many items containing these filters :/. @fischjer4  got it but I am getting zero results in indices_that_meet.. @fischjer4  I fixed that issue kindly tell me can I do and operations between the filters means only those indices will come which have combination of \"Karachi\" and \"Pakistan\".??. For batch predictions like these, I would suggest getting the embeddings out (get_item_representations, get_user_representations) and performing the prediction using those.  @maciejkula how can we perform this?. @hugos0910 on average 15 interactions per user and I am checking precision@10:\n\n[0.00571085698902607, 0.005477807950228453, 0.005436719860881567, 0.005406337324529886, 0.005396706517785788, 0.005383670795708895, 0.005372782703489065, 0.00536548113450408, 0.005364592187106609, 0.005356848705559969, 0.005356738343834877, 0.00535407941788435, 0.005347725003957748, 0.005346707068383694, 0.005345961544662714, 0.005345502402633429, 0.005343026481568813, 0.005341863725334406, 0.005341744981706142, 0.005335849244147539, 0.005333098582923412, 0.0053287651389837265, 0.005327129736542702, 0.005323854275047779, 0.005318386945873499, 0.005316123832017183, 0.00531594455242157, 0.005314420443028212, 0.005314376670867205, 0.005310103762894869, 0.005308026447892189, 0.005303124897181988, 0.005301858298480511, 0.005297009367495775, 0.005296340212225914, 0.005296127870678902, 0.005295416805893183, 0.005287356674671173, 0.005285672843456268, 0.005285257939249277, 0.005281541962176561, 0.005278915166854858, 0.005277582444250584, 0.005274641327559948, 0.0052709272131323814, 0.0052699134685099125, 0.005269142333418131, 0.005268954671919346, 0.005266934633255005, 0.005266845691949129]\n\nI am getting these score of top 50 recommendations for a user they are very similar to each other and recommendations are not very good I am also making user and item bias term to 0 what am I doing wrong? @maciejkula @RothNRK . can you give a code example? @hugos0910 . @olddaos  and then just stack that with  my original matrix?. features = [(x[\"entity_id\"], x[\"tags\"].split(\", \")) for (index, x) in entity_data.iterrows()]\n            item_features = dataset.build_item_features(features)\n            eye = sp.eye(item_features.shape[0], item_features.shape[0]).tocsr()\n            item_features_concat = sp.hstack((eye, item_features))\n            item_features = item_features_concat.tocsr().astype(np.float32)\ngetting error \n'numpy.ndarray' object has no attribute 'tocsr'\n@hugos0910  @olddaos  is this the right way?. @maciejkula  thanks . ",
    "josemazo": "Yep, you are completely right! A million thanks!\n. ",
    "PhanDuc": "With Miniconda i had error : \nError: Package missing in current win-64 channels:\n  - lightfm-1.9.tar.gz\n. Hi maciejkula, sorry for the late response\nNow this is my error when i installed with Miniconda : \n```\nC:\\Users\\palev>pip install -e F:\\Miniconda3\\envs\\lightfm\\Lib\\lightfm-master\\\nObtaining file:///F:/Miniconda3/envs/lightfm/Lib/lightfm-master\nRequirement already satisfied (use --upgrade to upgrade): numpy in f:\\miniconda3\\envs\\lightfm\\lib\\site-packages (from lightfm==1.9)\nRequirement already satisfied (use --upgrade to upgrade): scipy>=0.17.0 in f:\\miniconda3\\envs\\lightfm\\lib\\site-packages (from lightfm==1.9)\nRequirement already satisfied (use --upgrade to upgrade): requests in f:\\miniconda3\\envs\\lightfm\\lib\\site-packages (from lightfm==1.9)\nInstalling collected packages: lightfm\n  Running setup.py develop for lightfm\n    Complete output from command F:\\Miniconda3\\envs\\lightfm\\python.exe -c \"import setuptools, tokenize;file='F:\\Miniconda3\\envs\\lightfm\\Lib\\lightfm-master\\setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" develop --no-deps:\n    Compiling without OpenMP support.\n    running develop\n    running egg_info\n    writing lightfm.egg-info\\PKG-INFO\n    writing dependency_links to lightfm.egg-info\\dependency_links.txt\n    writing top-level names to lightfm.egg-info\\top_level.txt\n    writing requirements to lightfm.egg-info\\requires.txt\n    warning: manifest_maker: standard file '-c' not found\nreading manifest file 'lightfm.egg-info\\SOURCES.txt'\nwriting manifest file 'lightfm.egg-info\\SOURCES.txt'\nrunning build_ext\nbuilding 'lightfm._lightfm_fast_no_openmp' extension\nerror: Unable to find vcvarsall.bat\n\n----------------------------------------\n\nCommand \"F:\\Miniconda3\\envs\\lightfm\\python.exe -c \"import setuptools, tokenize;file='F:\\Miniconda3\\envs\\lightfm\\Lib\\lightfm-master\\setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" develop --no-deps\" failed with error code 1 in F:\\Miniconda3\\envs\\lightfm\\Lib\\lightfm-master\\\n```\n. ",
    "DeoLeung": "Hi @maciejkula ,\nMay I ask your help about the features, as you mentioned we can use ['user_id', 'country'] as features.\nsupposed we have 1000 user and 100 country, how to construct the features will be better?\n1. should I normalize them into 1? like user_id == 0.001, country = 0.01\n2. for small feature, like country, expand it to multiple features, like ['country a', 'country b' ...] and mark the related one to 1, similar to the stackoverflow tags.\n3. or is there any instruction on how to weight the features? e.g if I define userid from [0, 1000), country [0, 100), will user_id be over weighted 10 times?\nthank you\n. Thank you for the reply, I will then just train two models~\n. maybe remove the extra parameter docstring copied from predict as well\nhttps://github.com/lyst/lightfm/blob/master/lightfm/lightfm.py#L622\n. see the comment below \u2193\u2193\u2193\n. ",
    "ajohannsdottir": "Thank you Maciej, exactly what I was looking for. \nI only looked at the file itself,and as you know it isn't readable. \nBest regards,\nAgnes\n. ",
    "FlorianWilhelm": "My first patch was actually to directly implement get_params and set set_params in the LightFM class. Then I looked up the implementation of BaseEstimator and realized that this is so much smarter since they use introspection in order to collect the necessary attributes of a class that need to bet set and get by the simple convention that __init__ parameters must be equally named to the actual attribute (that's why I renamed rng to random_state). This way the DRY principle is satisfied and adding new attributes to the class is no longer a source of error since there is no need to add them to an explicitly implemented get_params and set_params method. A second way would be to copy over the code of Scikit-learn but this would lead to more than 100 additional lines in the LightFM class and an extra file holding the fallback implementation of inspect from sklearn.externals.funcsigs that is needed in case signature is not available in Python. So this option would pollute LightFM with a lot of lines of code that actually do not belong in there and also contradict the DRY principle.\nSo in total we have three possible solutions:\n1. Naive and explicit implementation of set_params and get_params\n2. Smart implementation based on the code of Scikit-Learn\n3. Mixing in the code of Scikit-Learn (like currently done)\nThe advantage of option 3 also is that people using isinstance and issubclass will get the expected results since SKLearnLightFM really is a BaseEstimator.\nI still believe that option 3 is a valid way but I can also change it to option 1 if you want to. I would rather refrain from choosing option 2 since this would introduce a lot of code that needs to be maintained from Scikit-Learn into LightFM. \n. ... but it runs on my machine! Nay, just kidding, but there was an IOError, could you just restart the Travis tests? I think that was a temporary issue.\n. Well... I am not sure you gonna like this. I got the RandomizedSearchCV working but actually it is no CV anymore since I am using train_index == test_index. This is necessary due to the fact that with pure collaborative filtering you can't really predict for users and items that are yet unknown. So the training/test split for LightFM is actually accomplished by removing entries from the user-item interaction matrix while Scikit-Learn generates smaller train and test matrices by removing rows. This is a major difference that cannot be dealt with except of rewriting the cross validation generator the way I did. To overcome this I would like to provide a modified RandomizedSearchCV that complies with the requirements of LightFM in another PR. To accomplish this I would copy over all parts I can reuse from sklearn.cross_validation if this is alright or is Scikit-Learn as dependency a cleaner solution?\n. ",
    "thisisjl": "Hi @FlorianWilhelm, @maciejkula, as this issue is old, this might have been implemented and I have not seen it. I've been looking for a way to evaluate hyper-parameters of the LightFM model and also the models in the Implicit library. \nIn the same way as you mentioned above, I would like to use sklearn's GridSearchCV or a similar method in combination with a cross validation procedure that creates a training and a testing interaction matrices (such as the random_train_test_split function).\nIs there any example I could use to understand how to accomplish this? If not, how do you deal with hyper-parameter tuning?\nI checked the test_sklearn_cv, but I do not really understand how I could use it for this purpose.\nThank you. ",
    "thisisandreeeee": "Thanks for the quick reply @maciejkula, all is well now.\nI have encountered a separate problem though. My user_features matrix is a scipy CSR matrix of type np.float64. If I'm not wrong this is being cast to a C double instead of a flt - and it seems that it is required:\n/Users/me/miniconda3/envs/py2/lib/python2.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n  warnings.warn('LightFM was compiled without OpenMP support. '\nTraceback (most recent call last):\n  File \"main.py\", line 35, in <module>\n    model = engine.fit(train, test, user_features=user_features)\n  File \"/path/to/dir/lightfm-engine/engine.py\", line 19, in fit\n    train_auc = auc_score(model, train, user_features=user_features, num_threads=self.NUM_THREADS).mean()\n  File \"/Users/me/miniconda3/envs/py2/lib/python2.7/site-packages/lightfm/evaluation.py\", line 118, in auc_score\n    num_threads=num_threads)\n  File \"/Users/me/miniconda3/envs/py2/lib/python2.7/site-packages/lightfm/lightfm.py\", line 675, in predict_rank\n    CSRMatrix(user_features),\n  File \"lightfm/_lightfm_fast_no_openmp.pyx\", line 156, in lightfm._lightfm_fast_no_openmp.CSRMatrix.__init__ (lightfm/_lightfm_fast_no_openmp.c:2339)\nValueError: Buffer dtype mismatch, expected 'flt' but got 'double'\nIs there some underlying piece of logic that automatically converts the np.float64 dtype to a double? I apologize for the many questions, just trying to figure things out. Thanks buddy\n. ",
    "andyluther": "i believe predict_rank predicts the rank of every item for each user in interactions\n. ",
    "ogrisel": "@maciejkula the Mac OSX travis config seems out of date. The failure is unrelated to the change in this PR.\n. ",
    "swanandj7": "Sigmoid function was really helpful. Thanks!  . ",
    "kewlcoder": "Hi Maciej, \nI am getting the same error - \"ValueError: Not all estimated parameters are finite, your model may have diverged. Try decreasing the learning rate.\"\nI have tried all these values for learning rate - [0.05, 0.025, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001] but still giving the same error. \nAlso, it only occurs when I add the item_features. It works fine with interactions + user_features data.\nPlease help !. > This is the expected result. By default, LightFM adds a feature per every user and item. You can disable that in the constructor.\nHi Maciej,\nI couldn't find anything in the constructor that can disable the addition of a feature per every user & item. Can you please help.\nDocumentation link - [https://lyst.github.io/lightfm/docs/lightfm.html]\nclass lightfm.LightFM(no_components=10, k=5, n=10, learning_schedule=\u2019adagrad\u2019, loss=\u2019logistic\u2019, learning_rate=0.05, rho=0.95, epsilon=1e-06, item_alpha=0.0, user_alpha=0.0, max_sampled=10, random_state=None)\n. ",
    "dwy904": "Why I got all zeros in both of the embedding matrices [user and item]?. How can I calculate the original interaction back and make recommendation? do I just simply do\nnp.matmul(model_bpr.user_embeddings, model_bpr.item_embeddings.transpose()) ?. What is the difference between fit and fit_partial if the dimension of the interaction matrix needs to be the same. Use it for parameter tuning?. Not all of the values match exactly. How could that be?\n```python\nfind the best model\nmodel_bpr_ult = model_bpr[np.argmax(bpr_map_test)]\nreformulate the interaction matrix.\nuser_biases = model_bpr_ult.get_user_representations()[0]\nuser_embeddings = model_bpr_ult.get_user_representations()[1]\nitem_biases = model_bpr_ult.get_item_representations()[0]\nitem_embeddings = model_bpr_ult.get_item_representations()[1]\nuser1 = user_embeddings[0]\nitem1 = item_embeddings\nscore = np.matmul(user1, item1.transpose())\nub1 = user_biases[0]\nib1 = item_biases\ntest on user_1\nresult = score + ub1 + ib1\npredict = model_bpr_ult.predict(0, list(range(len(item1))))\nCounter(np.argsort(result.round(3)) == np.argsort(predict.round(3)))\n```\n\nCounter({False: 340, True: 13201})\n. \n",
    "dofine": "https://github.com/lyst/lightfm/blob/master/tests/test_movielens.py#L437\nFind something in the tests :). @maciejkula What if I'd modify the code and add user features? Is that possible? I see there exists user info (like sex, age, occupation) in the original data file. . @maciejkula \nMaybe not worth opening a new issue so I'll post here.  I read the examples in the stackexchange docs:\n\nThe training and test set are divided chronologically:\nthe test set contains the 10% of interactions that happened after the 90% in the training set.\n\nAnd I've also read the code. Does this mean that all user-question interactions are sorted by time, and the last 10% of them are taken as test set? The train and test interaction matrix are of the same shape. But what confuses me is that, say a user i interacts with two questions m in the train set and n in the test set. We would have train[i][m]=1 train[i][n]=0 and test[i][m]=0 test[i][n]=1. I've several qustions.\n\n\nDo these two equations conflict with each other? How can a model, fitted with training set in whihch [i][n] = 0,   produce right recommendations in the test set?\n\n\nWhile in the movielens example, can we simply use tools like sklearn provided, randomly split 20% rows of the total ratings as test, 80% as train? If true, what's the difference between these different choices and methods used when doing the split?\n\n\nIf using user_features and the feature for some users have changed over time, e.g. age, height, weight, should the user be considered the same in the interaction matrix, or a new one?\n\n\nI've read other tutorials such as Implementing your own recommender systems in Python and couldn't find an explanation. Newbie questions not related to lightfm code. Hope you wouldn't mind. :)\n. Find out that I get feature names with ['iid=hash1', 'iid=hash2', 'tag=A', 'tag=B']. The shape of item_features 6918  is equal to 241 plus number of items 6676. Does this mean I get the item_features shape right? (Identity matrix concat with tag matrix) \n. Read the docs and fixed my problems. \nGreat work and thanks! @maciejkula . @maciejkula https://github.com/lyst/lightfm/blob/master/lightfm/lightfm.py#L311\nHere the exception should be \"The item feature matrix specifies more\" instead of \"The user feature matrix specifies more\"?. If my understanding is right, the docs say it's better to provide a userid-userid identity matrix concatenated with feature matrix. So anyhow the overall user feature matrix would have a unique userid in every row? \nSay two users 1 and 2 have two same features, age and height. Given a new user with same age and height to them, what will the model predict? We feed the new age and height, in your reply, which row should the new user feature be in? Or this doesn't matter at all, any row is OK, because their features are exactly the same.. ",
    "code-mc": "Ok thanks!. ",
    "stuartcrobinson": "i think this line number might be out-dated:\nhttps://github.com/lyst/lightfm/blob/master/tests/test_movielens.py#L437. ",
    "zzzrpagliari": "silly mistake. I think you are right :)\nOn Fri, Dec 16, 2016 at 4:45 PM, maciejkula notifications@github.com\nwrote:\n\nThis assertion is meant to prevent you from passing interaction matrices\nof incompatible shapes into fit and predict. In this case, I think your\ntest matrix has a larger first dimension than your train matrix, and so\nthere are some users in the test set that were not in the original train\nset.\nCan you check that this is not the case?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/133#issuecomment-267637642, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AWJKwyejmIm5lziE8PUg6jfxCOvgwRAsks5rIsA9gaJpZM4LPQ2c\n.\n. \n",
    "anshulg8": "You need to update zip_path in lightfm/datasets/movielens.py. I was stuck in a similar issue, which I posted here.\nAll I had to do was replace this:\nzip_path = _common.get_data(data_home,\n                            ('https://github.com/maciejkula/'\n                             'lightfm_datasets/releases/'\n                             'download/v0.1.0/movielens.zip'),\n                            'movielens100k',\n                            'movielens.zip',\n                            download_if_missing)\nwith \nzip_path = _common.get_data(data_home,\n                            ('https://github.com/maciejkula/\n                            lightfm_datasets/releases/\n                            download/v0.1.0/movielens.zip'),\n                            'movielens100k',\n                            'movielens.zip',\n                            download_if_missing). The 4 extra apostrophes(') in the URL were creating problem for me.. It didn't work for me, I am using Python3.6. ",
    "ParikshitSinghTomar": "Thanks @anshulg8, for pointing url. While my problem not solved by the help of above solution. What I tried written below.\n1. Copy path and download zip file\n2. Replace below code\ndata = fetch_movielens(min_rating=4.0)\nwith \ndef _read_raw_data(path):\n       with zipfile.ZipFile(path) as datafile:\n        return (datafile.read('ml-100k/ua.base').decode().split('\\n'),\n                datafile.read('ml-100k/ua.test').decode().split('\\n'),\n                datafile.read('ml-100k/u.item').decode(errors='ignore').split('\\n'),\n                datafile.read('ml-100k/u.genre').decode(errors='ignore').split('\\n'))\ndata=_read_raw_data('movielens.zip')\nI know this is not right way but I tried it and it works perfect. . ",
    "TimonVS": "Same issue on macOS with OpenMP enabled: . The issue persists after 25 recommendations: . Thanks for the fix @maciejkula! When can I expect a new release on Pypi? :). ",
    "salman-bhai": "Oh wait does LightFM have to be downloaded in the same directory where I was running this code?. Alright let me try by putting it in the same directory! If it works out, I'll close this Issue, if not I'll get back to you anyways!. ",
    "danlou": "I believe you need to specify the channel. I got it installed with:\nconda install --channel https://conda.anaconda.org/mlgill lightfm. ",
    "jariel17": "@maciejkula I tried pip install lightfm  in a conda virtualenv but still fails the installation with the message:\n\nFailed building wheel for lightfm\nerror: option --all not recognized. I have pip 9.0.1\n\nI am using Windows 10 x64 and I have installed Visual Studio 2017 with VC++ 14.0\nHere is the entire output:\n```\nC:\\ProgramData\\Miniconda3\\Scripts>pip install lightfm\nCollecting lightfm\n  Using cached lightfm-1.12.tar.gz\nRequirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (from lightfm)\nRequirement already satisfied: scipy>=0.17.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from lightfm)\nRequirement already satisfied: requests in c:\\programdata\\miniconda3\\lib\\site-packages (from lightfm)\nBuilding wheels for collected packages: lightfm\n  Running setup.py bdist_wheel for lightfm ... error\n  Complete output from command C:\\ProgramData\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\etech-09\\AppData\\Local\\Temp\\pip-build-fg5djgu0\\lightfm\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d C:\\Users\\etech-09\\AppData\\Local\\Temp\\tmp17_26dqapip-wheel- --python-tag cp36:\n  Compiling without OpenMP support.\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build\\lib.win-amd64-3.6\n  creating build\\lib.win-amd64-3.6\\lightfm\n  copying lightfm\\evaluation.py -> build\\lib.win-amd64-3.6\\lightfm\n  copying lightfm\\lightfm.py -> build\\lib.win-amd64-3.6\\lightfm\n  copying lightfm_lightfm_fast.py -> build\\lib.win-amd64-3.6\\lightfm\n  copying lightfm__init__.py -> build\\lib.win-amd64-3.6\\lightfm\n  creating build\\lib.win-amd64-3.6\\lightfm\\datasets\n  copying lightfm\\datasets\\movielens.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets\n  copying lightfm\\datasets\\stackexchange.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets\n  copying lightfm\\datasets_common.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets\n  copying lightfm\\datasets__init__.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets\n  copying lightfm_lightfm_fast_no_openmp.c -> build\\lib.win-amd64-3.6\\lightfm\n  copying lightfm_lightfm_fast_openmp.c -> build\\lib.win-amd64-3.6\\lightfm\n  running build_ext\n  building 'lightfm._lightfm_fast_no_openmp' extension\n  creating build\\temp.win-amd64-3.6\n  creating build\\temp.win-amd64-3.6\\Release\n  creating build\\temp.win-amd64-3.6\\Release\\lightfm\n  cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\ProgramData\\Miniconda3\\include -IC:\\ProgramData\\Miniconda3\\include /Tclightfm/_lightfm_fast_no_openmp.c /Fobuild\\temp.win-amd64-3.6\\Release\\lightfm/_lightfm_fast_no_openmp.obj -ffast-math -march=native\n  error: command 'cl.exe' failed: No such file or directory\nFailed building wheel for lightfm\n  Running setup.py clean for lightfm\n  Complete output from command C:\\ProgramData\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-fg5djgu0\\lightfm\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" clean --all:\n  Compiling without OpenMP support.\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\nerror: option --all not recognized\n\nFailed cleaning build dir for lightfm\nFailed to build lightfm\nInstalling collected packages: lightfm\n  Running setup.py install for lightfm ... error\n    Complete output from command C:\\ProgramData\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-fg5djgu0\\lightfm\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\etech-09\\AppData\\Local\\Temp\\pip-20pnlace-record\\install-record.txt --single-version-externally-managed --compile:\n    Compiling without OpenMP support.\n    running install\n    running build\n    running build_py\n    running build_ext\n    building 'lightfm._lightfm_fast_no_openmp' extension\n    cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\\ProgramData\\Miniconda3\\include -IC:\\ProgramData\\Miniconda3\\include /Tclightfm/_lightfm_fast_no_openmp.c /Fobuild\\temp.win-amd64-3.6\\Release\\lightfm/_lightfm_fast_no_openmp.obj -ffast-math -march=native\n    error: command 'cl.exe' failed: No such file or directory\n    ----------------------------------------\nCommand \"C:\\ProgramData\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-fg5djgu0\\lightfm\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\user\\AppData\\Local\\Temp\\pip-20pnlace-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-fg5djgu0\\lightfm\\\n```. ",
    "MrPhilosopher": "I got it installed by running pip install lightfm using cmd.. Found Error,\nThe downloaded zip file size was invalid, found the file from path and removed it.\nran the script again and it worked.. I'll work on it.. ",
    "Reddman5": "hey @jariel17  I have the exact same issue, did you manage to find a solution?. ",
    "rakshithvasudev": "Hey @maciejkula, this is a great package. I wish I could get it installed via conda. Unfortunately, it doesn't even install via pip. Is it possible for you to submit files to anaconda? Or can somebody please help me have a workaround to install this package?\nPS: I'm using Win 10 x64.\nThis is what it says during pip install: \n```\nCollecting lightfm                                                                                                       \n  Downloading lightfm-1.13.tar.gz (247kB)                                                                                \n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 1.5MB/s                                                                \nRequirement already satisfied: numpy in c:\\users\\rakshith\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from lightfm) \nRequirement already satisfied: scipy>=0.17.0 in c:\\users\\rakshith\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from l\nightfm)                                                                                                                  \nRequirement already satisfied: requests in c:\\users\\rakshith\\miniconda3\\envs\\machinelearning\\lib\\site-packages (from lightf\nm)                                                                                                                       \nBuilding wheels for collected packages: lightfm                                                                          \n  Running setup.py bdist_wheel for lightfm ... error                                                                     \n  Complete output from command C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\python.exe -u -c \"import setuptools, token\nize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-5rdtkozp\\lightfm\\setup.py';f=getattr(tokenize, 'open',\n open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d C:\\Use\nrs\\Rakshith\\AppData\\Local\\Temp\\tmpr0gj19dopip-wheel- --python-tag cp36:                                                  \n  Compiling without OpenMP support.                                                                                      \n  running bdist_wheel                                                                                                    \n  running build                                                                                                          \n  running build_py                                                                                                       \n  creating build                                                                                                         \n  creating build\\lib.win-amd64-3.6                                                                                       \n  creating build\\lib.win-amd64-3.6\\lightfm                                                                               \n  copying lightfm\\evaluation.py -> build\\lib.win-amd64-3.6\\lightfm                                                       \n  copying lightfm\\lightfm.py -> build\\lib.win-amd64-3.6\\lightfm                                                          \n  copying lightfm_lightfm_fast.py -> build\\lib.win-amd64-3.6\\lightfm                                                    \n  copying lightfm__init__.py -> build\\lib.win-amd64-3.6\\lightfm                                                         \n  creating build\\lib.win-amd64-3.6\\lightfm\\datasets                                                                      \n  copying lightfm\\datasets\\movielens.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets                                      \n  copying lightfm\\datasets\\stackexchange.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets                                  \n  copying lightfm\\datasets_common.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets                                        \n  copying lightfm\\datasets__init__.py -> build\\lib.win-amd64-3.6\\lightfm\\datasets                                       \n  copying lightfm_lightfm_fast_no_openmp.c -> build\\lib.win-amd64-3.6\\lightfm                                           \n  copying lightfm_lightfm_fast_openmp.c -> build\\lib.win-amd64-3.6\\lightfm                                              \n  running build_ext                                                                                                      \n  building 'lightfm._lightfm_fast_no_openmp' extension                                                                   \n  error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visualstu\ndio.com/visual-cpp-build-tools                                                                                             \n\nFailed building wheel for lightfm                                                                                      \n  Running setup.py clean for lightfm                                                                                     \n  Complete output from command C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\python.exe -u -c \"import setuptools, token\nize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-5rdtkozp\\lightfm\\setup.py';f=getattr(tokenize, 'open',\n open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" clean --all:       \n  Compiling without OpenMP support.                                                                                      \n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]                                                        \n     or: -c --help [cmd1 cmd2 ...]                                                                                       \n     or: -c --help-commands                                                                                              \n     or: -c cmd --help                                                                                                     \nerror: option --all not recognized                                                                                       \n\nFailed cleaning build dir for lightfm                                                                                  \nFailed to build lightfm                                                                                                  \nInstalling collected packages: lightfm                                                                                   \n  Running setup.py install for lightfm ... error                                                                         \n    Complete output from command C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\python.exe -u -c \"import setuptools, tok\nenize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-5rdtkozp\\lightfm\\setup.py';f=getattr(tokenize, 'open\n', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\n\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-rm_q0b4a-record\\install-record.txt --single-version-externally-managed --compile: \n    Compiling without OpenMP support.                                                                                    \n    running install                                                                                                      \n    running build                                                                                                        \n    running build_py                                                                                                     \n    running build_ext                                                                                                    \n    building 'lightfm._lightfm_fast_no_openmp' extension                                                                 \n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": http://landinghub.visuals\ntudio.com/visual-cpp-build-tools                                                                                           \n----------------------------------------\n\nCommand \"C:\\Users\\Rakshith\\Miniconda3\\envs\\machinelearning\\python.exe -u -c \"import setuptools, tokenize;file='C:\\User\ns\\Rakshith\\AppData\\Local\\Temp\\pip-build-5rdtkozp\\lightfm\\setup.py';f=getattr(tokenize, 'open', open)(file);code=\nf.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\Rakshith\\AppData\\\nLocal\\Temp\\pip-rm_q0b4a-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 i\nn C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-5rdtkozp\\lightfm\\                                                         \n```. I tried after installing Microsoft Visual C++ compiler now, unfortunately, no luck! \nI had trouble installing Scipy via pip. That's why I switched to Miniconda. Now looks like that's biting me back.  Here is what it says: \n```\nCollecting lightfm\nRequirement already satisfied: requests in c:\\users\\rakshith\\miniconda3\\lib\\site-packages (from lightfm)\nCollecting scipy>=0.17.0 (from lightfm)\n  Using cached scipy-0.19.0.zip\nRequirement already satisfied: numpy in c:\\users\\rakshith\\miniconda3\\lib\\site-packages (from lightfm)\nBuilding wheels for collected packages: scipy\n  Running setup.py bdist_wheel for scipy ... error\n  Complete output from command C:\\Users\\Rakshith\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d C:\\Users\\Rakshith\\AppData\\Local\\Temp\\tmpo33j1792pip-wheel- --python-tag cp36:\n  lapack_opt_info:\n  lapack_mkl_info:\n    libraries mkl_core_dll,mkl_intel_lp64_dll,mkl_intel_thread_dll not found in ['C:/Users/Rakshith/Miniconda3\\Library\\lib']\n    NOT AVAILABLE\nopenblas_lapack_info:\n    libraries openblas not found in ['C:\\Users\\Rakshith\\Miniconda3\\lib', 'C:\\', 'C:\\Users\\Rakshith\\Miniconda3\\libs']\n    NOT AVAILABLE\natlas_3_10_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries tatlas,tatlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries tatlas,tatlas not found in C:\\\n    libraries lapack_atlas not found in C:\\\n    libraries tatlas,tatlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  \n    NOT AVAILABLE\natlas_3_10_info:\n    libraries satlas,satlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries satlas,satlas not found in C:\\\n    libraries lapack_atlas not found in C:\\\n    libraries satlas,satlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  \n    NOT AVAILABLE\natlas_threads_info:\n  Setting PTATLAS=ATLAS\n    libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries ptf77blas,ptcblas,atlas not found in C:\\\n    libraries lapack_atlas not found in C:\\\n    libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  \n    NOT AVAILABLE\natlas_info:\n    libraries f77blas,cblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n    libraries f77blas,cblas,atlas not found in C:\\\n    libraries lapack_atlas not found in C:\\\n    libraries f77blas,cblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n    libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  \n    NOT AVAILABLE\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n      Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n      Directories to search for the libraries can be specified in the\n      numpy/distutils/site.cfg file (section [atlas]) or by setting\n      the ATLAS environment variable.\n    self.calc_info()\n  lapack_info:\n    libraries lapack not found in ['C:\\Users\\Rakshith\\Miniconda3\\lib', 'C:\\', 'C:\\Users\\Rakshith\\Miniconda3\\libs']\nNOT AVAILABLE\n\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n      Lapack (http://www.netlib.org/lapack/) libraries not found.\n      Directories to search for the libraries can be specified in the\n      numpy/distutils/site.cfg file (section [lapack]) or by setting\n      the LAPACK environment variable.\n    self.calc_info()\n  lapack_src_info:\n    NOT AVAILABLE\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n      Lapack (http://www.netlib.org/lapack/) sources not found.\n      Directories to search for the sources can be specified in the\n      numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n      the LAPACK_SRC environment variable.\n    self.calc_info()\n    NOT AVAILABLE\nRunning from scipy source directory.\n  non-existing path in 'scipy\\integrate': 'quadpack.h'\n  Traceback (most recent call last):\n    File \"\", line 1, in \n    File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 416, in \n      setup_package()\n    File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 412, in setup_package\n      setup(metadata)\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\core.py\", line 132, in setup\n      config = configuration()\n    File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 336, in configuration\n      config.add_subpackage('scipy')\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1001, in add_subpackage\n      caller_level = 2)\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 970, in get_subpackage\n      caller_level = caller_level + 1)\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 907, in _get_configuration_from_setup_py\n      config = setup_module.configuration(args)\n    File \"scipy\\setup.py\", line 15, in configuration\n      config.add_subpackage('linalg')\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1001, in add_subpackage\n      caller_level = 2)\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 970, in get_subpackage\n      caller_level = caller_level + 1)\n    File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 907, in _get_configuration_from_setup_py\n      config = setup_module.configuration(args)\n    File \"scipy\\linalg\\setup.py\", line 20, in configuration\n      raise NotFoundError('no lapack/blas resources found')\n  numpy.distutils.system_info.NotFoundError: no lapack/blas resources found\n\nFailed building wheel for scipy\n  Running setup.py clean for scipy\n  Complete output from command C:\\Users\\Rakshith\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" clean --all:\nsetup.py clean is not supported, use one of the following instead:\n- `git clean -xdf` (cleans all files)\n- `git clean -Xdf` (cleans all versioned files, doesn't touch\n                    files that aren't checked into the git repo)\n\nAdd --force to your command to use it anyway if you must (unsupported).\n\nFailed cleaning build dir for scipy\nFailed to build scipy\nInstalling collected packages: scipy, lightfm\n  Running setup.py install for scipy ... error\n    Complete output from command C:\\Users\\Rakshith\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-q1mppjh0-record\\install-record.txt --single-version-externally-managed --compile:\nNote: if you need reliable uninstall behavior, then install\nwith pip instead of using `setup.py install`:\n\n  - `pip install .`       (from a git repo or downloaded source\n                           release)\n  - `pip install scipy`   (last SciPy release on PyPI)\n\n\nlapack_opt_info:\nlapack_mkl_info:\n  libraries mkl_core_dll,mkl_intel_lp64_dll,mkl_intel_thread_dll not found in ['C:/Users/Rakshith/Miniconda3\\\\Library\\\\lib']\n  NOT AVAILABLE\n\nopenblas_lapack_info:\n  libraries openblas not found in ['C:\\\\Users\\\\Rakshith\\\\Miniconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\Rakshith\\\\Miniconda3\\\\libs']\n  NOT AVAILABLE\n\natlas_3_10_threads_info:\nSetting PTATLAS=ATLAS\n  libraries tatlas,tatlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries tatlas,tatlas not found in C:\\\n  libraries lapack_atlas not found in C:\\\n  libraries tatlas,tatlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n<class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n  NOT AVAILABLE\n\natlas_3_10_info:\n  libraries satlas,satlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries satlas,satlas not found in C:\\\n  libraries lapack_atlas not found in C:\\\n  libraries satlas,satlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n<class 'numpy.distutils.system_info.atlas_3_10_info'>\n  NOT AVAILABLE\n\natlas_threads_info:\nSetting PTATLAS=ATLAS\n  libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries ptf77blas,ptcblas,atlas not found in C:\\\n  libraries lapack_atlas not found in C:\\\n  libraries ptf77blas,ptcblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n<class 'numpy.distutils.system_info.atlas_threads_info'>\n  NOT AVAILABLE\n\natlas_info:\n  libraries f77blas,cblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\lib\n  libraries f77blas,cblas,atlas not found in C:\\\n  libraries lapack_atlas not found in C:\\\n  libraries f77blas,cblas,atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n  libraries lapack_atlas not found in C:\\Users\\Rakshith\\Miniconda3\\libs\n<class 'numpy.distutils.system_info.atlas_info'>\n  NOT AVAILABLE\n\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n    Atlas (http://math-atlas.sourceforge.net/) libraries not found.\n    Directories to search for the libraries can be specified in the\n    numpy/distutils/site.cfg file (section [atlas]) or by setting\n    the ATLAS environment variable.\n  self.calc_info()\nlapack_info:\n  libraries lapack not found in ['C:\\\\Users\\\\Rakshith\\\\Miniconda3\\\\lib', 'C:\\\\', 'C:\\\\Users\\\\Rakshith\\\\Miniconda3\\\\libs']\n  NOT AVAILABLE\n\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n    Lapack (http://www.netlib.org/lapack/) libraries not found.\n    Directories to search for the libraries can be specified in the\n    numpy/distutils/site.cfg file (section [lapack]) or by setting\n    the LAPACK environment variable.\n  self.calc_info()\nlapack_src_info:\n  NOT AVAILABLE\n\nC:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\system_info.py:572: UserWarning:\n    Lapack (http://www.netlib.org/lapack/) sources not found.\n    Directories to search for the sources can be specified in the\n    numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n    the LAPACK_SRC environment variable.\n  self.calc_info()\n  NOT AVAILABLE\n\nRunning from scipy source directory.\nnon-existing path in 'scipy\\\\integrate': 'quadpack.h'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 416, in <module>\n    setup_package()\n  File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 412, in setup_package\n    setup(**metadata)\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\core.py\", line 132, in setup\n    config = configuration()\n  File \"C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py\", line 336, in configuration\n    config.add_subpackage('scipy')\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1001, in add_subpackage\n    caller_level = 2)\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 970, in get_subpackage\n    caller_level = caller_level + 1)\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 907, in _get_configuration_from_setup_py\n    config = setup_module.configuration(*args)\n  File \"scipy\\setup.py\", line 15, in configuration\n    config.add_subpackage('linalg')\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1001, in add_subpackage\n    caller_level = 2)\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 970, in get_subpackage\n    caller_level = caller_level + 1)\n  File \"C:\\Users\\Rakshith\\Miniconda3\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 907, in _get_configuration_from_setup_py\n    config = setup_module.configuration(*args)\n  File \"scipy\\linalg\\setup.py\", line 20, in configuration\n    raise NotFoundError('no lapack/blas resources found')\nnumpy.distutils.system_info.NotFoundError: no lapack/blas resources found\n\n----------------------------------------\n\nCommand \"C:\\Users\\Rakshith\\Miniconda3\\python.exe -u -c \"import setuptools, tokenize;file='C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-q1mppjh0-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in C:\\Users\\Rakshith\\AppData\\Local\\Temp\\pip-build-f1o9gyqz\\scipy\\\n```\nThanks for your response.. It has already been installed. Conda did all the pre req installation when I wanted to install Sklearn.. I uninstalled miniconda and installed anaconda. It works with pip install pip install lightfm now. I've finally installed lightfm. Thanks for your replies @maciejkula. Appreciate your help.\n. ",
    "ModMaamari": "When I tried to install lightfm by : \npip install lightfm\nand I got an error :\nerror: command 'cl.exe' failed: No such file or directory. @maciejkula\nI Googled it and found many solutions , but none of them worked for me (in Python 3.6)\nbut when I installed Anaconda , and used conda install -c conda-forge lightfm it worked smoothly .\nnow I want to install LightFM  using pip install lightfm , in (Python 3.6 environment), not in Anaconda. ",
    "scottee": "I'll look at what the predictions say and report back.  And for clarification, I'm using the adagrad learning_schedule.\nBTW, what parameter equates to regularization, alpha or learning_rate?\nIn the mean time, do you have recommendations on ranges for alphas and learning_rate?  Below is the space I'm using for parameter optimization.  Is this reasonable?\nspace = [\n    (10**-6, 10**-1, 'log-uniform'),   # alpha (user_alpha and item_alpha)\n    (20, 200),                                   # epochs\n    (10**-4, 1.0, 'log-uniform'),      # learning_rate\n    (20, 200),                                  # no_components\n]\n\nBelow are the lowest alpha and learning_rate combos where the magic precision@6 number happens.  These don't seem very high, but I've never trained with a Warp/BPR type approach before.\n(This first entry came when the space had epochs ranging from (1, 200).)\n{\"alpha\": 0.07356951575008387, \"epochs\": 1, \"learning_rate\": 0.054121552105164714, \"no_components\": 48, \"precision@6\": 0.316850870848},\n{\"alpha\": 0.00684464334830365, \"epochs\": 187, \"learning_rate\": 0.005631464052454842, \"no_components\": 154, \"precision@6\": 0.316850870848},\n{\"alpha\": 0.001769098654581489, \"epochs\": 70, \"learning_rate\": 0.06384023381248471, \"no_components\": 87, \"precision@6\": 0.316850870848},\n{'alpha': 0.0039032887003676189, 'epochs': 84, 'learning_rate': 0.065409138131584846, 'no_components': 157, 'precision@6': 0.316850870848},\n. Attached is a python session that looks at the predictions when the weird precision@6 does and doesn't happen.  It show several things:\n\nWhen the precision@6 goes weird, all predictions are either NaN or 0.  How can predictions being NaN be correct?\nWhen the precision@6 goes weird, the precision@6 values are often above 1.0.  How can this be right?\nWhen predictions are all NaN or 0, how can there be any precision@6 value but 0?\nWhen precsion@6 is sane, all predictions are > 0 and <= 1.0.\n\nSo it really seems like several problems, possibly with a common root cause (predictions going to NaN?).  Thoughts?\nlightfm_problem.txt.zip\n. Where do the NaN predictions come from, though?. Yes, agreed.  But the learning rates are not excessive in all cases where this happens (at least in my experience).  In your experience, what should the range on learning rate be?. I'm just doing simple IMF, no user or item features.  Matrix cells are either 1 or 0.\nCan you point me to any place in the code to start debugging?  Since we have a case where it happens on epoch 0, we have a chance of seeing the NaNs happen.\nI'll also try resetting the learning_rate range and optimizing again...\nAnd BTW, thanks for all the help!. Just FYI... Running optimizations constrained to 0.1 max learning_rate.  There have been 4 weird precision@6 numbers over 115 optimization combinations.   Seems to make a case for the model divergence theory.\nStill debugging where the NaN predictions are coming from.. Several new points:\n\nThe dev version doesn't diverge on several of the cases where 1.11 does diverge.  So the dev version has some improvements that avoid some of the divergences.\nI found one combination where the model still diverges.  It diverges on item_embeddings going +/-inf.  I don't see an obvious cause of that, but you guys are the experts.\n\nHere's the param combo that still causes divergence:\n{\"alpha\": 0.07356951575008387, \"epochs\": 1, \"learning_rate\": 0.054121552105164714, \"no_components\": 48}\nThese all look reasonable to me.  Am I missing something?  And this combo continues to diverge until I reduce the learning rate to 0.0005.  That's an extremely small rate, isn't it?\nAnd here's the session showing the +/- inf:\n(Pdb) l\n320             for parameter in (self.item_embeddings,\n321                               self.item_biases,\n322                               self.user_embeddings,\n323                               self.user_biases):\n324                 if not np.all(np.isfinite(parameter)):\n325  ->                 raise ValueError(\"Not all estimated parameters are finite, \"\n326                                      \"your model may have diverged. Try decreasing \"\n327                                      \"the learning rate.\")\n328\n329         def fit(self, interactions,\n330                 user_features=None, item_features=None,\n(Pdb) parameter.shape\n(2001, 48)\n(Pdb) self.item_embeddings.shape\n(2001, 48)\n(Pdb) np.isnan(parameter).sum()\n0\n(Pdb) (parameter==np.inf).sum()\n46\n(Pdb) (parameter==-np.inf).sum()\n72\n(Pdb) (parameter==0).sum()\n95930\nThe +/- inf happens in the first call to fit_warp().  I can't seem to debug this further since the code is all .pyx -- unless there's magic to do that.  (I've never debugged at the pyx level.)\n. I already fix the random seed, so I was hoping it is deterministic.  Does parallelism introduce non-determinism also?\nI'll try the good, old debugging by print and see what I find.... I may have found an issue, or at least a symptom.  Please tell me if I'm chasing a red herring here.\nIn file _lifhtfm_fast.pyx.template around line 440-ish there is the regularization on feature+component:\n        features[feature, component] *= (1.0 + alpha * local_learning_rate)\n\nSince during an epoch any feature+component cell can be visited many times, this regularization compounds on itself, instead of adding a little noise.  (For example, one cell was regularized 19K times in my log file.) I've not implemented SGD algorithms before, but it seems fishy that the code regularizes a single cell multiple times during an epoch, and especially with multiplication.   Below are some print statements that illustrate the problem.  These were printed during one epoch:\nWARN: update_features() reg on features[989, 0] is BIG 1001730.187500, old val 997782.437500, alpha 0.073570, l_rate 0.053779\nWARN: update_features() reg on features[989, 0] is BIG 1005693.562500, old val 1001730.187500, alpha 0.073570, l_rate 0.053779\nWARN: update_features() reg on features[989, 0] is BIG 1009672.625000, old val 1005693.562500, alpha 0.073570, l_rate 0.053779\n... 18852 lines deleted with updates on [989, 0]\n. Another point about the regularization...  I'm seeing lots of cases where the gradient has gone to 0, but then regularization compounds on a value.  That doesn't seem right either.. I already played with alpha.  If I divide alpha by 100 (0.0007) it works.  If I divide learning_rate by 100 (0.0005) it works.  If I divide each by 10 it still diverges.\n. And BTW, I have a non-proprietary set of data and code to reproduce the problem.  Unfortunately the training data is 46MB compressed.  If we can figure how to pass it along I can share it.. It's a good news, bad news thing:\n\nGood: The #152 PR does seem to have fixed the divergence problem.\nBad: It also seems to have crippled performance.\nGood: Crippling performance seems to happen mostly when model starts to diverge, or gradient goes to 0.\n\nWith openmp and 16 threads, the divergent param combo is barely faster than no openmp and 1 thread.  Here's 16 threads:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py\n2017-01-25 16:30:17,882 INFO __main__: Training model...\nEpoch 0\n2017-01-25 16:36:27,548 INFO __main__: Finished training model\nTakes 6min, 10sec.  Here's 1 thread, no openmp, but on my laptop, not AWS:\n$ ./lightfm_optimize_prob.py\n/Users/escott/projects/lightfm/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n  warnings.warn('LightFM was compiled without OpenMP support. '\n2017-01-25 10:17:55,189 INFO __main__: Training model...\nEpoch 0\n2017-01-25 10:24:02,712 INFO __main__: Finished training model\nTakes 6min 7sec.  Here's 1 thread, with openmp on AWS:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py  -threads 1\n2017-01-25 16:45:17,436 INFO __main__: Training model...\nEpoch 0\n2017-01-25 16:52:12,090 INFO __main__: Finished training model\nTakes 6min, 55sec.\nNow let's look at a non-diverging, gradient doesn't go to zero, param combo.  One thread, no openmp:\n$ ./lightfm_optimize_prob.py\n/Users/escott/projects/lightfm/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n  warnings.warn('LightFM was compiled without OpenMP support. '\n2017-01-25 10:50:20,861 INFO __main__: Training model...\n...\nEpoch 19\n2017-01-25 11:01:26,286 INFO __main__: Finished training model\nTakes 11min 6sec.  This used to take 10min, 30sec before #152.\nNow 16 threads and openmp on AWS:\n[hadoop@ip-10-13-181-155 ~]$ ./lightfm_optimize_prob.py\n2017-01-25 16:59:48,265 INFO __main__: Training model...\n...\nEpoch 19\n2017-01-25 17:03:57,042 INFO __main__: Finished training model\nTakes 4min 9sec.  This isn't significantly slower than it used to be, that I can tell.\n. Given that, a couple more questions about #152...  \n\nAre the results I have from param optimization on version 1.11 still valid for master+PR152?\nIs master+PR152 validated enough that I can use it in production?. I think the exception should also contain any contextual information it can, such as which epoch failed, what is the current learning-rate, and anything else that can help the caller resolve the divergence and which is readily available.. \n",
    "drraghuram": "@maciejkula Is there any example code that shows how to load our own datasets into lightfm? \nThanks. 1. Yes, the input data is the same.\n2. No, we are not using multithreaded fitting.\n3. Repeated runs on the same machine gives the same results. \nI am not sure what you mean by a material way, but the order in which the results appear does change. For example, a recommendation appearing as the 4th recommendation in one machine appears as the 6th recommendation in a different machine.\n. All of us are using Python 2.7, with the same versions of numpy. The C compiler is the same across machines.\nnumpy - 1.13.3\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) \n. Hey, sorry for the late reply. Was unwell. We have already tried first. We got the same sequence of random values. Will try it again today to double check.. ",
    "harshgupta7": "http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/. Formatting causing build failure, will fix!. Sure! I'll put it in the doc.. Sounds good..will do!. Yeah. Will fix.. ",
    "msh-tf": "Thanks for the reply!\n\n\nI still don't get why train and test set have to be of the same sizes; why does user 0 from test have to match user 0 from train?\n\n\nThe input training data is basically a user by item matrix where every interaction is represented by 1 and no interaction is represented by 0. What is a positive/negative example in this case when a predicted var is not explicitly input?\n\n\nAlso if I set the item baises to 0 as your example model.item_biases *= 0.0 the test auc score decreases to 0.5006. \nWhy is that?\n\n\nThanks again.\n. ",
    "kyowill": "why the prediction score is negative? movie ratings shouldn't be (1,5)? I still don't understand  . Yes, I know. My 'user_id' is i(user_id = 1, only one user).My 'feature_matrix' is 'user_features_concat[1]'(only one vector). But lightfm still reports the exception.In my opinion, it may be a bug:). What I mean a user id of 1 is a index in some set, and 'user_features_concat[1]' is one of the row in features matrix which corresponding to the user id of 1. User id of 1 is the second user. feature matrix has many rows.. Yes, I only passed one row of features matrix. When user id of 0, I passed features matrix[0] in predict function, when user id of 1, I passed features matrix[1] in predict function. User id of 0 is OK, but user id of 1 is not. I couldn't find reason, so checked lightfm source code. when user id of 1, the 'n_user' is added up to 2, which bigger than number of feature vector('feature matrix[i]' is just one vector)  . thanks a lot:). thanks a lot. ",
    "samshipengs": "@maciejkula A follow-up question, I trained a model on user purchase history. 1 for an user has purchased this item, 0 otherwise. I would like to make item suggestion to some users.\nJust wondering what is the result of model.predict_rank(test)? Is it the recommended item index(or ids) for the test set?\nThanks. @maciejkula Thanks for the quick reply. \nI see, what is the recommendation list for the user? Is it just the list of items we used in training (also testing as they require same number of rows)? . @maciejkula By ordered you mean the same order as how the items were presented in training or? \nSo far this is what I understood, for example I have total 1000 items, and I got 42 from predict_rank for (10, 150), then this tells me it's at top 42 position to recommend to this user. So basically the number 42 shows the goodness or 'likelihood' the item 150 to be recommended to this user. And if I were to determine whether I recommend this product to this user or not, I can set a threshold, e.g. if the position is smaller than top 10% ~ 0.1*1000 = 100 then I recommend this item, otherwise not, does this sound right?\nI'm new to recommendation system, I only got to know a bit about collaborative filtering through Andrew Ng's coursera class. What do you suggest if I would like to know about the topic and build one at work, reading up the paper listed in the documentation references or just a text book?\nMany thanks.. ",
    "joxer": "I agree with @thadwoodman, having a better documentation also for testing would be great! I'm having hard time figuring out out to do a dataset for training.. ",
    "tfzxyinhao": "+1\nin real world,maybe have more than one feature for every user\nis there has other tutorial for this ?. Sorry for my incorrect description, I mean that support for distribute,\nfit on multiple machine and then merge to one model. In some scence, train data distribute on more than one machine. @tugakariem maybe you need to read docments first\nand make attention on example code\nexamples includes three demos show how to use lightfm\nvideo https://www.youtube.com/watch?v=9gBC9R-msAk\nnotebook https://github.com/tdeboissiere/Kaggle/blob/master/Ponpare/ponpare_lightfm.ipynb\ntutorial http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/\n. I think you need to use fit_partial train for more times and show your changing of accuracy  by matplotlib\nand then adjust the params\nthere is tutorial http://lyst.github.io/lightfm/docs/examples/warp_loss.html. ",
    "thadwoodman": "@maciejkula Thanks! I eventually stumbled onto that after diving into the code a bit more. \nI was thinking that it might be helpful to mention it briefly in the documentation (\"for requisite data formats, take a look at the inline documentation on lightfm.py\") or to go through an example or converting a .csv or some other standard data file into something that is ready to be processed. That definitely would have saved me quite a bit of time.\n. ",
    "bcongdon": "Ah, ok. I see. Thanks for the clarification!. ",
    "blancyin": "Thank you for your anwser, but i still did not understand.\nI have trained a lightfm model, and i pickle.load this model. \nUser feature matrix and item feature matrix  is userd.\nI want to achieve the  feature and item latent, i don't konw the how to explain model.item_embeddings.\nThen load the model and print the item_embeddings:\nitem_embeddings=model.item_embeddings\nprint item_embeddings.shape\nthe shape of item_embeddings.shape ,user_embeddings.shape,item_biases.shape,user_biases.shape\n(1412757, 10)\n(27001044, 10)\n(1412757,)\n(27001044,)\ni think: \n10 is my no_components number (it is equal indeed)\n1412757= m items + k item features (it is equal indeed)\n27001044 = m' user + k' user features (it is equal indeed)\nfor k in item_embeddings:\n   file.write(str(k))\nand i print the model.item_embeddings, i found the latent, each latent is 10 dimension.\nCan you tell me how can i find the correspondence between latent and item ?. I am sorry for my poor english.\nI know what your mean.\nI want to calculate the item feature like your paper :\n'bond' is same to '007'.\nSo i just use the i-th row in item_embeddings, i is the index of bond.\nThank you vere much\n. ",
    "pencoa": "In the document\nfit(interactions, user_features=None, item_features=None, sample_weight=None, epochs=1, num_threads=1, verbose=False)\nIf interactions is a must to input, the others are optional?. In some cases, the features from users and items can be selected to be the same. \ne.g, users have features a1, a2, a3\nitems have features a1, a2, a3\nIf user_1 and item_1 both have feature a1, then item_1 is recommended to user_1.. if All arguments with default values are optional \nI will have a try to modify lightfm/lightfm.py lightFM class fit function\nset interactions = None . soga. Thanks for your reply.. ",
    "qwhex": "Made the requested changes. \nWhen I tried to follow the method you outlined in the readme to run the tests (python setup.py test) it didn't work. I included the output + the pip freeze output. I could make it work by calling .venv36/bin/py.test -xv tests. Of course my venv was active the whole time.\nAbout the evaluation: the items in the test and train dataset differ by the interactions, but the features remain the same for both, right? My main concern is that measurements only evaluate the prediction of interactions and not the prediction of features.\n. ",
    "JasonTam": "You can also try encoding the explicit interactions as user features. In that sense, the final user representation will be characterized by the sum of a user-specific embedding and the embeddings associated with liking various movies.. Your implicit interactions would just be a boolean of whether a user has interacted with an item or not. Then you can use your explicit data to fill in sample_weight (https://github.com/lyst/lightfm/blob/master/lightfm/lightfm.py#L377) . Ex) you might give twice as much weighting to an interaction if a user has explicitly liked an item. The actual values you choose for sample_weight is then a hyper-parameter.\ndisclaimer: I haven't found sample_weight to be useful in my own uses lol. dunno about this CI failure though lol. features arg doesn't match docstring. ditto. ",
    "laszbalo": "@JasonTam Thanks for your answer. I was considering the same thing, but I was worried that by doing so I will add a lot of bias to the recommender. In other words, the most telling latent features to predict whether a user has viewed an item, are the explicit feedback features. The lack of independence among user features and the interaction matrix is not a problem in practice? Am I missing something here?. @maciejkula Thanks for the info.\n\ngiving higher weights to interactions that have explicit likes\n\nLet me get this straight. I have to do this outside of the realm of LightFM, right? Like in the way I outlined above. In the case of one implicit and one explicit interaction matrices, I have to train two separate models, get a prediction from each, and take the weighted average of the predictions, where higher weight goes to the prediction obtained from the model trained with the explicit data.. ",
    "arita37": "I have this dataset :\n   Nuser(100,000)  >> Mitem (100)\nFor some reasons, Convergence + AUC is better\nwithout item feature, user feature.\niteam and user are category and item incorporate rating from users.....\nAny idea why pure CF is better than Hybrid ?\n. Thanks !\n1) 2) and 3) have been checked.\nStrangely, pure CF converged fast (after 4 epochs) and AUC on test is quite high\nAlso tested om additionnal test data, AUC is high (>.85).\nwhat do you mean by\ninverse-frequency weighting of your features ?\nOn Dec 19, 2017, at 23:33, Maciej Kula notifications@github.com wrote:\n. ok, for continouous feature.\nBut, here, most are categories.\nIs there any way to check the item embedding \u201cquality\u201d (outside of total AUC) ?\nOn Dec 20, 2017, at 0:39, Maciej Kula notifications@github.com wrote:\nIn the feature matrix, give low values to common features, 1.0 / count_of_occurrence for example.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. 1) Isn\u2019t Item feature input be as one hot encoding ?\n2) you mean \n        EmbeddingWi x 1/freqi makes inderweight most frequent category and\nmakes diversify \nWhat is the dimension of item_embedding ?. Ok,noted.\nAfter testing, my view is \nif Nusr  >>  Nitem then\n pure CF has higher AUC....\nAs you mentionned, embedding may not improve when the Nitem is low....\n(similar items...)\nOn Dec 22, 2017, at 5:44, Maciej Kula notifications@github.com wrote:\nEven if it is one-hot encoding, you can always insert some other value (0.5, 0.001) instead of the 1.\nI mean that you should use values other than 1 in the item/user feature matrices. If you look at scipy sparse matrices you will see that they have a data array that holds the values for the non-zero entries.\nFor a general overview of inverse frequency weighting, you could read about the TF-IDF technique.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. In that topic,\n1) is CSR input fine ? (or only COO)\n2) how LFM recognize the order of userid ibetween interaction matrix and user_id ?. When creating user feature and item feature,\nwe need to be careful on the \u201cimplicit Id\u201d.\nI have a training set, when using user feature and item feature (price,...), I have\nboth lower results on test and train sets.\nThanks.\nOn Dec 13, 2017, at 23:21, Maciej Kula notifications@github.com wrote:\nCSR input is fine, it will be transparently converted (but you'll use less memory if you start with the right format).\nThe row number of the interaction matrix is the user id.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Am investigatin the input mapping :\nInput interaction matrix:\nuserid itemid\n7          14\n5           11\nInput item matrix:\n11   ....\n14. ....\nIs this one correct/incorrect ?\n(it seems we need to follow the interaction matrix order  for user/item ....)\nThanks !\nOn Dec 13, 2017, at 23:32, Maciej Kula notifications@github.com wrote:\nIf your hybrid model performs worse than a pure CF model, make sure:\nYour features are appropriately scaled.\nIf your features are continuous, consider discretization.\nYou don't drop per-item or per-user features. Again, have a look at the notes section.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ok, am checking it.\nOn another item, after reading your paper,\n1) why a full FM model is less convenient\nthat the dot product of item-user ?\n(the model used in light).\n2) Is there a way to add contextual feature,\nie attached to the pair (user, item) and not necessary to one of them ?\nOn Dec 14, 2017, at 8:26, Maciej Kula notifications@github.com wrote:\nItem id 14 will look up the 14th row of the item feature matrix and so on.\n\u2015\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Suppose we have\n1 million of users and 1 million items\non text file \nEach mini-batch does not contain the same\nnumber of user, item.\nIs it fine ?. Hello,\nThanks,\nBut, how the sparse matrix is filled ?\nwith float for numerical values\nwith int=1 for category one hot encoding ?\nShould we do the one hot index encoding ?\nThanks, regards\nKevin. Can we add context features as numerical,\nlike the age of persons (whatever stats...)\nin the same sparse matrix ?\nie combine category with no category.\nKind regards.\nOn Dec 1, 2017, at 23:07, Maciej Kula notifications@github.com wrote:\nYou should create a sparse matrix with 1s for those (user, item) pairs where an interaction happened. Leave everything else at 0.\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hello,\n1)\nthanks, this is similar to online fitting with partial fit.\n2)\nFor ranking problem, is implicit feedback better tham sorting the rating ?\nThis is not clear , in which way it can be better.\nThanks.\nOn Dec 3, 2017, at 22:08, Maciej Kula notifications@github.com wrote:\nSure, you could do that. To construct the features matrix, you need to treat the views of one user over time as new users, giving the features new values where they have changed, and keeping them constant where they haven't. For example, if you had 100 users, and wanted 5 time-views of each, you'd end up with a feature matrix with 500 rows.\nLightFM doesn't take ratings: it's an implicit feedback model (which is what you want; models with ratings is something you almost never want).\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hello,\nIt makes sense for rating survey.\nBut, what about when we transpose\nNb of Clicks and Nb of Conversion into a rating system.\nIsn\u2019t those 2 actions different by nature/level ?\nThanks.\nOn Dec 4, 2017, at 0:23, Maciej Kula notifications@github.com wrote:\nHave a look at this paper. The bottom line is: data on which items users choose to rate is much more important than the rating given. Hence, implicit recommender systems.\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. For time dependant features,\nhow the model recognize the 500 \u2018virtual users\u2019 into 100 users ?\nIs user only recognize but his user feature ?\nIf 2 users having same feature, is it considered as same user ?\nThanks\n. ",
    "datascienceit": "+1. ",
    "mayank1806": "Thanks @maciejkula .. Does it mean that my ideal user feature matrix should be a square one? My current user feature matrix has around 14000 users and some 15-16 features. Is there any way I can calculate RMSE of my predictions in lightFM?\n. Thanks @maciejkula Even after horizontally stacking my feature matrix with an identity matrix, I am not getting good AUC for test set. Though I am getting a great AUC of 0.92 with the train set. Now, I have three doubts:\n1) My model has ratings spread between 1 and 5. Is this a problem? Should my model have binary ratings only?\n2) Does the score generated by your predict module can be normalized to get the rating between 1 to 5? \n3) What kind of train and test does this model expect?\n. There was a problem with my train and test set. Now I am getting perfect results. AUC of 0.94 for train and 0.82 for test. Thanks for building such a wonderful capability. \nI just wanted to know that whether this model is scalable or not. Will this model perform better with very large dataset? Thanks in advance. Hi! I have around 2500 users and 970 movies. I want to store all the recommendation scores in an array. I am using model.predict command to do that but getting error: \nassert len(user_ids) == len(item_ids)\nAssertionError\nI can understand that the length of my arrays are not same but what is the way out? How can I get recommendation scores for all the user-movie combination for which I have non-zero interaction in my dataset.. Thanks @maciejkula \nWhat I felt unique about this model that it supports partial_fit. This can have a very good business implication as it will take less time to train the model if the train set is changed slightly. Can you give some good reference to understand this functionality?. What I can understand is: Suppose I have a model of 2000 users and 1000 movies. A new user joins and thus I have 2001 rows now in user-item interaction as well as user features matrix. Should I pass that one row only (matrix of 1X1000) to the partial_fit which will be zero since he is a new user or should I append that row with my original matrix and then pass it to the model. Same doubt with User feature matrix. \nMy understanding says that I should pass that row appended with original matrix. It will be very kind of you if you can clear this confusion.. Hi @maciejkula .. Some questions about the model:\n1) How are you using the sample weight in calculating score for user-item pair? My interaction data is in the form of some kind of counts like duration of viewing,visit etc. I have assigned a score to each interaction based on some maths on those numbers. I just wanted to know that how does the model takes into consideration the weight of each interactions?\n2) Suppose there is a user who has watched two movies M1 and M2. I have scores for those interactions. May be 2 and 1 over a scale of 5. Will this model consider 1 as negative interaction and 2 as positive?\n3) Since you are using sigmoid function to calculate the scores, it should come between 0 and 1 but we are getting huge scores. How are we getting those scores?\nSorry for long questions but I am using this model for sometimes and really need to know these before I conclude on some results. Thanks in advance\n. ",
    "chrisbangun": "I see. So if I understand correctly, I have to build several matrix. one for item features, one for user features (if I have any) and the other one for user - item interactions matrix. Previously, I thought that the context is incorporated in one matrix, the user - item interaction matrix. So instead of have one value, we have a vector or list of values inside the user-item matrix.. hi @maciejkula \nthank you very much for your response.\nAlso, thanks for the reference. I'll redo my experiment as advised.. I have similar problems to this. I've checked your code and found this from https://github.com/lyst/lightfm/blob/a37d0c2/lightfm/lightfm.py#L261:\nassert self.item_embeddings.shape[0] >= item_features.shape[1]\nWhat is the intuition behind this? Why must the number of item features be less than the number items?. ",
    "piero10": "maciejkula, thanks, I'll try. What about second question?. maciejkula, I'm sorry, you already answered it. \n\nI designed the library to allow using different matrices at train and predict times.\n\nThanks for your work!\n. ",
    "davenovelli": "Turns out it was the NaNs and the not normalized features. AUC and P@k are now both lower after adding user features, but I haven't done a grid search over hyperparameters yet and assume that should help tun things.\nIs it worthwhile adding a check for NaN's and or normalized values, and at least writing that out in verbose mode?\nThanks so much for the quick responses!\n . ",
    "jwheatp": "Thank you for your fast answer !. Thanks for your answer.. Thanks.\nI built item feature vectors and user feature vectors. \nMy item feature vectors increased well my precision score, but my user feature vector decrease slightly my precision score (both considering my test set). I did many checks and my matrix is good, but I can't figure out why my precision score decreases. \nI know you don't have my code / data, but maybe you would have some clues about it ? Is there something special / harder with user vectors ?\nNote: I normalized my matrix. Yes, they are binary user features, and some features are shared by a lot\nof users, such as the age bucket or the city they live in.\nLe ven. 16 juin 2017 18:03, maciejkula notifications@github.com a \u00e9crit :\n\nDo you have per-user binary features? Do you have any features that lots\nof users have in common?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/191#issuecomment-309065806, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHnRgkLRrAPikQpJ-zi3xVTtisqHIOVKks5sEqc5gaJpZM4N75-G\n.\n. This is strange, because whatever the user feature group I try, It always decrease the accuracy.\n\nAlso, this is what I get for the user_biases matrix : \n[-4.84616089 -4.32035065 -4.42363214 -4.59546661 -4.87165165 -4.79935265\n -4.94092464 -5.11230135 -4.56947231 -4.36375046 -4.21067286 -4.38716269\n -4.1725626  -4.3375802  -4.24554253 -4.46307468 -4.42445707 -4.73932028\n -4.65806532 -4.51775551 -4.33859062 -4.4521699  -4.07685661 -4.85057259\n -4.41702795 -4.46412659 -4.5712266  -5.0045948  -4.38546419 -4.05183697\n -4.37861013 -4.53687239 -4.5924015  -4.55746889 -4.3572669  -4.5783453\n -4.63864565 -4.74321175 -4.44993877 -4.66760731 -4.82987118 -4.93526697\n -5.1922636  -4.5340414  -4.94212103 -4.23927116 -4.57736731 -4.67142248\n -4.49732637 -4.4367733  -5.08003712 -4.75161123 -2.26396394]\nand for the predicted scores (using the predict method once) (top ten recommendations)\nscores\n-1.688872\n-1.835690\n-1.859145\n-1.872488\n-1.919604\n-1.953401\n-2.013271\n-2.017010\n-2.022043\n-2.069141\nIs it not strange that these values are negative ?\nIf I set the user_feature = None,  I get this for the scores \n1.106682\n1.056287\n1.000357\n0.997396\n0.994831\n0.847245\n0.827781\n0.782809\n0.765976\n0.760468\n. > Do you have per-user features, that is, is an identity matrix part of\nyour user_features?\nNo, should it be the case ? Maybe I misunderstood something. Could you\nplease give more details / an example?\nThanks in advance\nLe dim. 18 juin 2017 17:55, maciejkula notifications@github.com a \u00e9crit :\n\nDo you have per-user features, that is, is an identity matrix part of your\nuser_features?\nIt's no a prior worrying for scores to be negative. The values\nthemselves have no interpretation.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/191#issuecomment-309285948, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHnRglcX_Fvgwzrj-HIWFVvpHrmA-cKOks5sFUiLgaJpZM4N75-G\n.\n. Thanks for your answer. Do you mean I should make an identity matrix first\nand concatenate it with other custom features user vectors ? Like the\nschema I did above but with an identity matrix concatenated on left?\n\nLe dim. 18 juin 2017 18:05, maciejkula notifications@github.com a \u00e9crit :\n\nThis is what I mean by 'per-user binary features': it if often useful for\neach user to have a feature that's one for them and zero for everyone else.\nThis is in fact what happens if you do not pass any features in.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/191#issuecomment-309286517, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHnRgo286_E875aHiUvnnwNX0Vi93JW7ks5sFUqwgaJpZM4N75-G\n.\n. Thanks a lot, it worked. I think I will do a blogpost or some PRs to explain some details of knowledge I missed during my work. I'll keep you informed ;). \n",
    "MatthiasKirsch": "Hi,\nmaybe I have the same Problem as jwheatp. \nMy user_features sparse matrix looks like this: \n<483407x5 sparse matrix of type '<class 'numpy.float64'>' with 966814 stored elements in C00rdinate format>\nEdit: I converted the sparse matrix from C00rdinate to CSR by applying .tocsr() after importing the user_feature matrix. This also doesn't make a change on the bad result.\nThe rows are ordered like the users, so that each row implies another user. In the columns I have something like 'Gender_M', 'Gender_W', 'Age_group_young', 'Age_group_middle', 'Age_group_old'. So my matrix is binary with for example 1 in 'Gender_M' and 'Age_group_old' if the user is male and old. The other columns are 0 in this case. \n@maciejkula you answered the following: \n\nThis is what I mean by 'per-user binary features': it if often useful for each user to have a feature that's one for them and zero for everyone else. This is in fact what happens if you do not pass any features in.\n\nand jwheatp answered:\n\nThanks for your answer. Do you mean I should make an identity matrix first and concatenate it with other custom features user vectors ? Like the schema I did above but with an identity matrix concatenated on left?\n\nDoes this mean I should do the following?:\n\nThanks in advance!! :)\n@jwheatp have you already created a blogpost or PRs to explain your workaround? :)\nThank you very much!\nEDIT: I found here https://github.com/lyst/lightfm/blob/master/lightfm/lightfm.py#L124 the following:\n\nNote: when supplying feature matrices, an implicit identity feature matrix will no longer be used. This may result in a less expressive model: because no per-user features are estiamated, the model may underfit. To combat this, include per-user (per-item) features (that is, an identity matrix) as part of the feature matrix you supply\n\nBut I don't know if and how this should do the trick :(\nAdditional information:\nTrain is: \n<483407x65430 sparse matrix of type '<class 'numpy.int64'>' with 3621261 stored elements in C00rdinate format>\nTest is:\n<483407x65430 sparse matrix of type '<class 'numpy.int64'>' with 3621261 stored elements in C00rdinate format>\n. 1.    My user features are all binary yes. They look very similar to the image I posted above. So I did one hot encoding for \u201cmy own user features\u201d and concatenated it with an identity matrix via hstack. So for example if a user_1 is male and young, he has a 1 in the columns gender_male and age_young and of cause in the identity matrix for himself.\n2.  I tried it tonight with these features: \nNUM_THREADS = 10\nNUM_COMPONENTS = 30 (before: 10)\nNUM_EPOCHS = 30 (before: 20)\nITEM_ALPHA = 1e-8 (do I need this when using item_features?)\nUSER_ALPHA = ITEM_ALPHA (do I need this when using user_features?)\nSo I left the learning rate of in this scenario. I also added item_features into the process and the result looks like this: (the blue line is again without side information) \n\nAnd I did not do any hyperparameter searches yet. I only saw the one here http://lyst.github.io/lightfm/docs/examples/learning_schedules.html?highlight=hyperparameter which explains how to handle the EPOCHS I think. And in #189 you commented \u201cTo select the right values, I would define an evaluation function and run a hyperparameter search, for example using sklearn's ParameterSampler.\u201c, but I don\u2019t really know yet how to do that. \n3.  Ok I will also try to test it with reduced number of THREADS in my parameters.\nSome extra information: My data is some sparse purchase data (5000000 transactions) and I have about 500000 users with attributes gender and age. There is no missing data in my user_features.\n. Ok thanks!I will have a look on how to perform hyperparameter search for LightFM to get the learning_rate that gives best accuray.\nThis is how I produced the user_features matrix: (data is my complete dataframe and customers is a list of unique sorted UserIDs  customers = list(np.sort(data.UserID.unique()))\ndef create_user_features(customers, data):\n    data = data[[\"UserID\", \"Gender\", \"Age_Group\"]].drop_duplicates().reset_index(drop=True)\n    sorterIndex = dict(zip(customers, range(len(customers)))) \n    data['User_Rank'] = data['UserID'].map(sorterIndex) \n    data = data.sort_values(by='User_Rank')\n    data = data[['Gender', 'Age_Group']]\n    data_as_dicts = [dict(r.iteritems()) for _, r in data.iterrows()]\n    vectorizer = DictVectorizer()\n    user_feat = vectorizer.fit_transform(data_as_dicts)\n    user_iden = identity(len(customers), format='csr')\n    user_features = sparse.hstack([user_iden, user_feat])\n    return user_features\n. Hi :)\nSo I found a little tutorial http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/ on how to search for the perfect hyperparameters and tried to apply it on my own application.\nFirst of all I had to optimize the hyperparameters for the model without adding any user- or item features.\n```\nOptimization:\n1. without user- & item_features\ndef objective(params):\n    learning_rate, no_components = params\n    model = LightFM(\n        loss = 'bpr',\n        random_state=2016,\n        learning_rate=learning_rate,\n        no_components=no_components\n    )\n    model.fit(train_sketch, epochs=30, num_threads=4, verbose=True)\n    patks = evaluation.precision_at_k(model,test_sketch, train_interactions=None, k=20, num_threads=6)\n    mapatk = np.mean(patks)\n    out = -mapatk # Make negative because we want to minimize objective\n    if np.abs(out + 1) < 0.01 or out < -1.0: # Handle some weird numerical shit going on\n        return 0.0\n    else:\n        return out\nspace = [(10**-4, 1.0, 'log-uniform'), # learning rate\n         (10, 50) # no_components\n         ]\nres_fm = forest_minimize(objective, space, n_calls=250, random_state=0, verbose=True)\nprint('Maximum p@k found: {:6.5f}'.format(-res_fm.fun))\nprint('Optimal parameters:')\nparams = ['learning_rate', 'no_components']\nfor (p, x_) in zip(params, res_fm.x):\n    print('{}: {}'.format(p, x_))\n```\nI tried to insert just the learning_rate into space but this did not work for any reason. This is why I also added the parameters for no_components into space. \nThe results are as follows:\nMaximum p@k found: 0.00524\nOptimal parameters:\nLearning_rate: 0.030491116769588475\nNo_components: 50\nAfter this step I had to optimize the model again, but this time with user/item features:\n```\n2. with User/Item Features\ndef objective_wsieinfo(params):\n    learning_rate, no_components = params\n    model = LightFM(\n        loss='bpr',\n        random_state=2016,\n        learning_rate=learning_rate,\n        no_components=no_components\n    )\n    model.fit(train_sketch, epochs=30, item_features=item_features_concat, user_features=user_features_concat , num_threads=6, verbose=True)\n    patk = lightfm.evaluation.precision_at_k(model, test_sketch, item_features=item_features_concat ,user_features=user_features_concat, train_interactions=None, k=20, num_threads=6)\n    mapatk = np.mean(patk)\n    # Make negative because we want to minimize objective\n    out = -mapatk\n    # Weird shit going on\n    if np.abs(out + 1) < 0.01 or out < -1.0:\n        return 0.0\n    else:\n        return out\nspace = [\n         (10**-4, 1.0, 'log-uniform'), # learning_rate\n         (10, 50) # no_components,\n         ]\nx0 = res_fm.x.append(1.) \nThis typecast is required\nitem_features = item_features.astype(np.float32) # I don't know why he inserted this\nuser_features = user_features.astype(np.float32)  # I don't know why he inserted this\nres_fm_itemfeat = forest_minimize(objective_wsieinfo, space, n_calls=50, x0=x0, random_state=0, verbose=True)\nprint('Maximum p@k found: {:6.5f}'.format(-res_fm_itemfeat.fun))\nprint('Optimal parameters:')\nparams = ['learning_rate', 'no_components']\nfor (p, x_) in zip(params, res_fm_itemfeat.x):\n    print('{}: {}'.format(p, x_))\nand the results:\nMaximum p@k found: 0.00214\nOptimal parameters: \nLearning_rate: 0,14359959944482836\nNo_components: 49\n```\nI think what I am doing here is terrible, because instead of giving me the advise to lower the learning rate, he wants me to increase it to 0.14??? I will try to fix this but maybe you can give me an advice :)\n. Ok thanks! Perhaps it could be usefull not to group the ages in \"young\", \"middle\" and \"old\" but their normal ages like 18, 20, 30... or perhaps to group them in a more wide spectrum (e. g. 10, 20, 30, 40, ...)?\nWould you try it?. ",
    "tkanngiesser": "@MatthiasKirsch did you make some progress on this in the meantime ? Happy to exchange some ideas. I have bits and pieces (= pipeline) from pandas data frame to correct sparse format but haven't done any real test or measurement yet.. ",
    "fmacrae": "Is it because I'm using Python 2.7?. I installed via pip initially and due to the error\n ImportError: No module named _lightfm_fast_no_openmp\nI then cloned the project and compiled it as instructed.  That is why my path changed to my home directory.\nTried reinstallation using \npip uninstall lightfm\npip install -e lightfm\nRan the test code and still get:\n~$ python LightFM.py \n/home/finlay/anaconda2/lib/python2.7/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n  warnings.warn('LightFM was compiled without OpenMP support. '\nTraceback (most recent call last):\n  File \"LightFM.py\", line 1, in \n    from lightfm import LightFM\n  File \"/home/finlay/anaconda2/lib/python2.7/site-packages/lightfm/init.py\", line 4, in \n    from .lightfm import LightFM\n  File \"/home/finlay/anaconda2/lib/python2.7/site-packages/lightfm/lightfm.py\", line 8, in \n    from ._lightfm_fast import (CSRMatrix, FastLightFM,\n  File \"/home/finlay/anaconda2/lib/python2.7/site-packages/lightfm/_lightfm_fast.py\", line 12, in \n    from ._lightfm_fast_no_openmp import *  # NOQA\nImportError: No module named _lightfm_fast_no_openmp\n. I'm on ubuntu so default is python 2\ninstalled pip3\n~$ sudo apt-get install python3-pip\nthen lightfm using pip3\n~$ sudo pip3 install lightfm\nthen the code works with python3 :)\n~$ python3 movie_rank.py \n<943x1682 sparse matrix of type ''\n    with 49906 stored elements in COOrdinate format>\n<943x1682 sparse matrix of type ''\n    with 5469 stored elements in COOrdinate format>\n. Thanks maciejkula, you are probably right but no point keeping the issue open for I am very new to python so have probably made a host of mistakes.  Saying that I've done little other than take latest ubuntu build then put annaconda on it using the default settings.  Then followed a bunch of steps to get tensorflow working along with pip installing dozens of things for Siraj's demos on Youtube. ",
    "nlassaux": "@maciejkula First thank you for your work! Cold fold-in seem to be a very valuable feature for lightfm, do you already have anything in the pipeline?. Understood, thank you for your answer \ud83d\udc4d . ",
    "dakl": "Hello! \nI have an idea on how to implement this that I wanted to check with you, if this is still on the table. \nThe idea I have (a poc implementation of) is to be able to create a new model from an old model, initialise it with random values for the item and user embeddings and biases (as ususal) and then copy the ones that we're learned in the previous model over to the new one. \nThis requires two new parameters on the model, item_feature_names and user_feature_names in order to copy over the values to the correct places in the new model. \nSomething along the lines of \n~~~python\n    @staticmethod\n    def from_model(old_model, item_feature_names, user_feature_names):\n        old_model._check_initialized()\n        new_model = LightFM(\n            no_components=old_model.no_components,\n            item_feature_names=item_feature_names,\n            user_feature_names=user_feature_names\n            )\n        new_model._initialize(\n            no_components=new_model.no_components,\n            no_user_features=len(user_feature_names),\n            no_item_features=len(item_feature_names))\n    item_feature_idx = np.in1d(old_model.item_feature_names, new_model.item_feature_names)\n\n    user_feature_idx = np.in1d(old_model.user_feature_names, new_model.user_feature_names)\n\n    new_model.item_embeddings[item_feature_idx] = old_model.item_embeddings\n    new_model.user_embeddings[user_feature_idx] = old_model.user_embeddings\n    new_model.item_biases[item_feature_idx] = old_model.item_biases\n    new_model.user_biases[user_feature_idx] = old_model.user_biases\n\n    return new_model\n\n~~~\nUsage would be old_model = LightFM(..., item_feature_names=item_feature_names,\n    user_feature_names=user_feature_names), train, save the model, and then later \n~~~python\nnew_model = LightFM.from_model(\n    old_model,\n    item_feature_names=new_item_feature_names,\n    user_feature_names=new_user_feature_names)\n~~~\nI can write this up as a proper PR of course, but wanted your input first. I'm guessing it's relevant to copy over the momentums and gradients from the old model as well. \ncheers\n. @maciejkula pinging here since it's and old thread and this might be missed otherwise :). We can update the c files if this is interesting to you.. Haha, that's my misunderstanding! Let's kill this PR with fire then :) . The build seems to have failed due to OOM. I'm not sure that's related to my changes?. I fixed the flake8 stuff and the memory thing went away. \ud83c\udf89 . Thanks for the feedback!\nI definitely agree with carrying more of the state, I just didn't want to invest too much time in the implementation before getting some feedback. \nI think feature names would be useful in the model. An example could be item features which could be words in the titles of products on an ecommerce site, for example 'cable', 'case' and 'iphone'. Let's say we want to add the feature 'charger' because the item \u00b4iphone charger\u00b4 is new in the inventory. This could then be added as the last element in the list, but in preprocessing would be far more likely to be put in the list of features in alphabetical order, resulting in the feature list 'cable', 'case', 'charger' and 'iphone'. Having the words in item_feature_names greatly simplifies the creation of a new model for the user, who won't have to care at all in which order the features sit in the model, that's all handled internally in the model. \nI understand that it's of course possible to let the user write the logic for it, but I think it's much cleaner to write in once (in the lightfm implementation) and add proper tests and have users use it and feel confident that it's correct. This kind of logic/plumbing is pretty error prone in my experience, and can require a fair bit work to get right. In the model however, I think it's much eaiser since everything is known in one place in the model object.\nThose would be my two cents for having the feature names in the model. \nRegarding the API, I'm fine with original.resize(...). original.update_features(...) could be another suggestion. I have no real opinion here, the static method was just the first thing that came to mind :)\nGreat work with LightFM btw, it's a really useful system! \ncheers.  Collaborating on that sounds great. I\u2019ll pause the development of this pull request until your prototype is further along. . @maciejkula It strikes me that the .resize(...) API might not be intuitive for the user. Having the method be an instance method to me indicates that it's supposed to modify (resize) the instance in question, but in our case it will return a new object, factory method style. I think this was why I chose the @staticmethod in my original implementation.. As you can see I couldn't stop myself from doing the rest of the implementation. \ud83d\ude1c. ",
    "ezietsman": "@dakl I'm trying to do this right now, I've basically done what you have there but when I start training it using fit_partial the accuracies effectively starts back at zero and I need a similar number of epochs than just training it from scratch. Is there some place I can have a look at how to do the cold fold-in correctly?. ",
    "chris-boson": "@dakl @ezietsman Did you get anywhere with this approach?. ",
    "rumaniac": "Hello,\nI've installed lightFM inside  python3  for Windows 10 Pc.\nI am getting the same error.\nplease help\n\n. ",
    "iozzyi": "Thanks for the tip, is there any reason why they should be organised in this way? . Sorry, I am not so clear on point 1.\n1. Is this just the way LightFM was built that it doesn't use some features when explicitly giving it an item feature matrix? What are the per-user and per-item feature? Did you mean it doesn't calculate a score separately for each user/each item for efficiency? Is this described somewhere in detail in the docs that I can read about it? Finally, if I concat an identity matrix to the feature matrix, I guess it will be in the rows otherwise the labels will be messed up. But why does that trigger LightFM to use per item features?\n2. I guess it would not be a fair comparison without finding optimal hyperparameters for both models so I'll try that next.\nThank you.. Okay, thanks. I will leave this open until you have the link to the doc (in case other people searching find the same question).. Just an example in the docs on how the identity matrix is appended to the item/user feature matrix would be nice? You wrote it here but I didn't see it in #206, '(num_items, num_items+num_features)'. Since LightFM already has code written to do this, it would be good to have a parameter option to append the identity matrix when supplying the item/user feature matrix. But otherwise clear, thanks.. Resolved then, thanks again :). So are all the ratings treated as binary for the AUC score in LightFM? I am using implicit feedback so there are counts of each interaction the user made with the items.. Hello,\nI didn't fully understand how the latent item features are incorporated into the score in LightFM.\nIn the LightFM paper it says: \np = user embedding, q = item embeddings, score = dot(q,p) + biases\nI am having this issue when trying to calculate the scores with the item feature matrix:\n    p = (N users, M items)\n    q = (M items, Z features)\nSo when I make the product of dot(p,q) I get:\n    scores = (N users, Z features)\nI am actually doing 1 user at a time by taking a single row index from the user-interaction (N,M) matrix = (1 user, M items).\nThen for a given user I have the scores as an array of (1, Z features) instead of (1, M items)\nSo now I don't have a score for each item?\nBut to calculate the AUC I believe I need a score for each item for each user? \nI have been following the example here: https://jessesw.com/Rec-System/ (There is a method def cal_mean_auc)\nThanks. Thanks. ",
    "Tullio-DH": "Alright, thanks!\nDon't you think this might be ambiguous however? Some datasets (including mine, which is a real one) do admit rating = 0. Of course I can easily change that to some arbitrary small numbers, and then replace the nans with zeros.. I see. So, would you use an input matrix of only 1s and 0s (for the presence/absence of interactions) and a separate (identically shaped) matrix of ratings to be passed to the fit function as weights?. ",
    "esandoval30": "Quick update: \nI realized that item features needs weights of each feature component rather than 1s or 0s for a item feature being present or not\nI also increased the  NUM_EPOCHS \nWhile I am still get some negative scores, the prediction scores are higher where I am expecting them to be so this is very useful to rank a set of items to recommend to a given user \nIt'd still be good to understand/document what the predict method returns (e.g range of values that could be returned)...Alternatively a normalized prediction score value between 0..1 would be useful. Thanks, Maciej. Very helpful\n. ",
    "MaxBalashov": "Unfortunately, for my case using different values for positive and negative interactions doesn't effect on auc score (and other metrics), it's constant. How should it be interpreted?. It is my embeddings and biases. It's not NaNs and zeros. But  model.user_biases looks strange.\nSorry for so large code outputs.\n```\n\nmodel.user_embeddings\narray([[0.01854292, -0.03616943,  0.02743073, ..., -0.08873698,\n        -0.0281186 ,  0.04932814],\n       [ 0.01835506, -0.02285067, -0.02461982, ..., -0.0196003 ,\n        -0.00183558, -0.01746818],\n       [-0.00016604, -0.01641887,  0.00801281, ...,  0.02026852,\n        -0.01372182, -0.01250093],\n       ..., \n       [-0.00038331, -0.02331057,  0.0014284 , ..., -0.0269407 ,\n        -0.02862924,  0.04169686],\n       [ 0.13069977,  0.01168023, -0.0036469 , ..., -0.09148387,\n        -0.0869351 , -0.00417987],\n       [-0.00157261,  0.01230439,  0.00775047, ..., -0.02291745,\n         0.00544394, -0.01943119]], dtype=float32)\nmodel.item_embeddings\narray([[0.02933569, -0.0732343 ,  0.07156094, ..., -0.09587108,\n        -0.02510785,  0.04858794],\n       [-0.02053796, -0.01279098,  0.00457765, ...,  0.01316937,\n         0.0132997 ,  0.01030643],\n       [ 0.0673373 ,  0.01364722, -0.00136843, ..., -0.00953384,\n        -0.00528875, -0.01918415],\n       ..., \n       [ 0.15962589, -0.32825601,  0.30606297, ..., -0.20719305,\n        -0.12428237,  0.2490337 ],\n       [-0.0179254 ,  0.04813654, -0.07512292, ...,  0.1126662 ,\n         0.11551356, -0.07269695],\n       [-0.316017  ,  0.2270131 , -0.19657041, ...,  0.46460572,\n         0.20413968, -0.25520191]], dtype=float32)\nmodel.user_biases\narray([-0.07031802,  0.        ,  0.        ,  0.        , -0.04835589,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        , -0.73506993,  0.        , -0.05971936,\n        0.        , -0.00876252, -0.00447214, -0.0531995 , -0.06756987,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        ...,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n       -0.00447214,  0.        , -0.01356889,  0.        ,  0.        ,\n        0.        ,  0.        , -0.02967845,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        , -0.026685  ,\n       -0.15701669,  0.        ], dtype=float32)\nmodel.item_biases\narray([3.45653705e-02,   8.24284647e-03,   1.79438330e-02,\n        -2.43308563e-02,   1.47444410e-02,   1.29504711e-03,\n        -1.56834326e-03,   1.76689308e-02,  -3.99955250e-02,\n         1.15162646e-02,  -3.82953659e-02,  -8.33974569e-04,\n         3.71466838e-02,  -1.56373773e-02,   2.79283850e-03,\n         2.29972787e-02,   0.00000000e+00,   5.43882977e-03,\n         ...,\n        -5.75311063e-03,  -1.46178296e-02,   5.30885439e-03,\n         2.16178242e-02,  -6.85118930e-03,  -8.92395154e-03,\n        -5.27057843e-03,   2.16984842e-02,   4.13220096e-03,\n        -1.97833162e-02,   3.11827380e-02,  -2.06727460e-02,\n         1.01115098e-02,  -1.69814434e-02,   0.00000000e+00,\n         2.73628443e-01,   8.90778452e-02,  -4.03039247e-01], dtype=float32)\n```\n\nI suppose that coordinates of interactions are more important than values of interactions (positive, negative). . Could you give advice about scaling input data? Should I scaling user and item features for avoiding overfitting?\nUsually matrix factorizations depends on it, as example SVD.. ",
    "leminhlong5194": "Thank you very much. My problem is more of an implementation problem. \nSupposing if I have a NEW USER D, and D is does not have any user interaction data. Now if I have trained with user A, B, C and have  interactions with item 1 to 9: A1, A2, A3, B4, B5, B6, C7, C8, C9. \nNow say D's has similar user features to A's (age, gender, job...). My goal is to recommend NEW ITEMS 11, 12, 13 with similar features to items 1, 2, 3 from user A. So this is a problem with new user being recommended new items with similar user, and item features.\nWhat item_features and user_features do I pass to the predict method if I want to predict : \npredict(D.id, item_ids=?, item_features=?, user_features=?)\nIs it correct if I make this function:\npredict(0, item_ids=[11,12,13], item_features=[11's features, 12's features, 13's features], user_features=[D.features]). Thank you very much. That explains a lot. ",
    "kurokochin": "I have questions about this problem also:\n1. How do we know new user D is similiar with A?\n2. If we want to add new user D in models, do we need to re-train our model from scratch?\nThanks! :). ",
    "HarlanH": "No joy...\n```\n$ tail Dockerfile\nRUN conda install pytest jupyter\nENV PYTHONDONTWRITEBYTECODE 1\nADD . /home/lightfm/\nWORKDIR /home/lightfm/\nRUN pip install lightfm\nRUN pip install -e .\n$ docker-compose build lightfm\nBuilding lightfm\nStep 1/10 : FROM ubuntu:16.04\n ---> ccc7a11d65b1\nStep 2/10 : RUN apt-get update\n ---> Using cache\n ---> 439ded84e99b\nStep 3/10 : RUN apt-get install -y libxml2 libxslt-dev wget bzip2 gcc\n ---> Using cache\n ---> c99fc8a179f8\nStep 4/10 : RUN echo 'export PATH=/opt/conda/bin:$PATH' > /etc/profile.d/conda.sh &&     wget --quiet https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh &&     /bin/bash ~/miniconda.sh -b -p /opt/conda &&     rm ~/miniconda.sh\n ---> Using cache\n ---> b42ccae6cb65\nStep 5/10 : ENV PATH /opt/conda/bin:$PATH\n ---> Using cache\n ---> 103267604857\nStep 6/10 : RUN conda install pytest jupyter\n ---> Using cache\n ---> ef336a45e3df\nStep 7/10 : ENV PYTHONDONTWRITEBYTECODE 1\n ---> Using cache\n ---> 3ae375262e5d\nStep 8/10 : ADD . /home/lightfm/\n ---> d1f7a1d5919d\nRemoving intermediate container fb1c103f30e3\nStep 9/10 : WORKDIR /home/lightfm/\n ---> 0334480218b3\nRemoving intermediate container 860727d68dbb\nStep 10/10 : RUN pip install -e .\n ---> Running in dc3cdf531843\nObtaining file:///home/lightfm\nCollecting numpy (from lightfm==1.13)\n  Downloading numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\nCollecting scipy>=0.17.0 (from lightfm==1.13)\n  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from lightfm==1.13)\nInstalling collected packages: numpy, scipy, lightfm\n  Running setup.py develop for lightfm\nSuccessfully installed lightfm numpy-1.13.1 scipy-0.19.1\n ---> 23b49d552147\nRemoving intermediate container dc3cdf531843\nSuccessfully built 23b49d552147\nSuccessfully tagged lightfm_lightfm:latest\n$ docker-compose run lightfm py.test -x tests/\n============================= test session starts =============================\nplatform linux -- Python 3.6.1, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\nrootdir: /home/lightfm, inifile:\ncollecting 0 items / 1 errors\n=================================== ERRORS ====================================\n___ ERROR collecting tests/test_api.py ____\nImportError while importing test module '/home/lightfm/tests/test_api.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\nlightfm/init.py:2: in \nLIGHTFM_SETUP\nE   NameError: name 'LIGHTFM_SETUP' is not defined\nDuring handling of the above exception, another exception occurred:\nlightfm/_lightfm_fast.py:3: in \n    from ._lightfm_fast_openmp import *  # NOQA\nE   ModuleNotFoundError: No module named 'lightfm._lightfm_fast_openmp'\nDuring handling of the above exception, another exception occurred:\n/Users/harlan/Development/lightfm/tests/test_api.py:7: in \n    ???\nlightfm/init.py:4: in \n    from .lightfm import LightFM\nlightfm/lightfm.py:8: in \n    from ._lightfm_fast import (CSRMatrix, FastLightFM,\nlightfm/_lightfm_fast.py:12: in \n    from ._lightfm_fast_no_openmp import *  # NOQA\nE   ModuleNotFoundError: No module named 'lightfm._lightfm_fast_no_openmp'\n------------------------------- Captured stderr -------------------------------\n/home/lightfm/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n  warnings.warn('LightFM was compiled without OpenMP support. '\n!!!!!!!!!!!!!!!!!!! Interrupted: stopping after 1 failures !!!!!!!!!!!!!!!!!!!!\n=========================== 1 error in 0.52 seconds ===========================\n```\n. #212 solves this, so closing the issue!. Well, it's better! But...\n```\n\u2714 ~/Development/lightfm [docker_instructions|\u20261]\n16:08 $ docker-compose run lightfm py.test -x lightfm/tests/\n============================= test session starts =============================\nplatform linux -- Python 3.6.1, pytest-3.2.1, py-1.4.34, pluggy-0.4.0\nrootdir: /home/lightfm, inifile:\ncollected 56 items\nlightfm/tests/test_api.py ................\nlightfm/tests/test_datasets.py .\u2718-KILL ~/Development/lightfm [docker_instructions|\u20261]\n16:09 $ docker-compose run lightfm jupyter notebook lightfm/examples/movielens/example.ipynb --ip=0.0.0.0\n[I 20:09:15.896 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\n[C 20:09:15.963 NotebookApp] Running as root is not recommended. Use --allow-root to bypass.\n\u2718-1 ~/Development/lightfm [docker_instructions|\u20261]\n```\nSo, it looks like test_datasets is maybe timing out and getting killed. And the movielens example (which does need the path change too, by the way!) is failing with another issue.\n. Oh, scroll right in my code block above -- it looks like I'm getting some sort of warnings about running as root, but then the process is existing with error code 1, and docker ps shows nothing running.\nI'm relatively new to docker, so I'm probably not being as clear as I ought to be...\n. @maciejkula Great, works like a charm, thanks!. ",
    "lucidfrontier45": "@maciejkula \nThank you for response.\nI wanna confirm if I understood it correctly.\nA. On question 3 and 4 about the predict api, the user feature matrix's row indexes correspond to the user_ids passed as the first argument. If I passed [0, 1,4,10] as user_ids, the feature matrix must at least   has 11 rows and 4 rows (0, 1, 4, 10) should have non-zero values.\nB. With paired style, same user_ids will be passed several times like [0, 1, 1, 1, 4, 4, 10] but feature matrix must not contain duplicated id and is the same as case A.\nAre A and B correct?. @maciejkula \nThank you very much!. ",
    "FassyGit": "I used the parameters as mentioned in the document.\n\nNUM_THREADS = 4\nNUM_COMPONENTS = 100\nNUM_EPOCHS = 10\nITEM_ALPHA = 1e-6\nmodel = LightFM(loss='warp',\n                item_alpha=ITEM_ALPHA,\n                no_components=NUM_COMPONENTS)\nAnd as for the cluster problem, actually I am only using one node to run the program so there should not  be distribution problem here? Also runing other datasets on the cluster the same way as I do on thi s dataset works out the result fairly fast.\n And if you would like to read a little bit more. Here is the code:\nimport codecs\nimport pandas as pd\ndoc1 = codecs.open('dataset/dataset_TIST2015/dataset_TIST2015_Checkins.txt','rU','latin-1')\ncheckins = pd.read_csv(doc1, delimiter='\\t')\ncheckins.columns=['userId', 'venueId', 'timeUTC', 'timeOffset']\ndoc2 = codecs.open('dataset/dataset_TIST2015/dataset_TIST2015_POIs.txt','rU','latin-1')\npois = pd.read_csv(doc2, delimiter='\\t')\npois.columns=['venueId', 'lat', 'lon', 'venueCat', 'countryCode']\nlen(checkins['venueId'].unique())\nlen(checkins['userId'].unique())\nlen(pois['venueCat'].unique())\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix\nuserIdencoder = LabelEncoder().fit(checkins['userId'])\ncheckins['userIdencoded'] = userIdencoder.transform(checkins['userId'])\nn_users = len(userIdencoder.classes_)\nvenueIdencoder = LabelEncoder().fit(checkins.venueId)\ncheckins['venueIdEncoded'] = venueIdencoder.transform(checkins.venueId)\nn_venues = len(venueIdencoder.classes_)\nvenueCatIdencoder = LabelEncoder().fit(pois.venueCat)\npois['venueCatIdEncoded'] = venueCatIdencoder.transform(pois.venueCat)\nn_venueCats = len(venueCatIdencoder.classes_)\n\ntags = venueCatIdencoder.classes_\ntags.astype(dtype='<U50')\nfrom sklearn.cross_validation import train_test_split\ntrain_df, test_df = train_test_split(checkins, train_size = 0.8, random_state = 42)\nimport numpy as np\ntrain = csr_matrix((np.ones(train_df.shape[0]), (train_df.userIdencoded, train_df.venueIdEncoded)), shape=(n_users,n_venues))\ntest = csr_matrix((np.ones(test_df.shape[0]), (test_df.userIdencoded, test_df.venueIdEncoded)), shape=(n_users,n_venues))\nprint(test.nnz)\nprint(train.nnz)\nprint(test.max())\nprint(train.max())\nfrom lightfm import LightFM\nfrom lightfm.evaluation import auc_score\nimport time\nNUM_THREADS = 4\nNUM_COMPONENTS = 100\nNUM_EPOCHS = 10\nITEM_ALPHA = 1e-6\nmodel = LightFM(loss='warp',\n                item_alpha=ITEM_ALPHA,\n                no_components=NUM_COMPONENTS)\nprint (\"I am beginning to model\")\nstart = time.clock()\nmodel.fit(train,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\nprint(\"model has been fitted\")\nelapse = (time.clock() - start)\nprint(\"Time used:\", elapse)\ntrain_auc = auc_score(model, train,num_threads=NUM_THREADS).mean()\nprint(\"Train_auc is %f\" %train_auc)\ntest_auc = auc_score(model, test,train_interactions=train,num_threads=NUM_THREADS).mean()\nprint(\"Test_aus is %f\" %test_auc). Thanks! It is exactly the problem you mentioned!\nI did not take care of the core allocation problem, the default CPU number for every node is 2, so no wonder it is running slow.\nAfter I fix this problem, it takes less than half an hour to fit the model.\nAnd by the way, can you be more specific on the smaller evaluation sets? Does it mean to cut the train and test matrix after fitting the model?\nMany thanks!. \n",
    "satya326": "I have trained my model on some n number of users without any user or item features. suppose i have to predict for user n+1 who has done items 3,4,5 of total 100 items(present in the training set), where will i pass this information while predicting.. ",
    "gqoew": "Amazing, eliminate_zeros() solved my mistake on the sparse matrix creation process. I also figured out pd.get_dummies() has a sparse parameter set to false by default. Will try to set it to true and see if it work as well. Thanks for the quick answer!. ",
    "bhangaboy": "@MatthiasKirsch I modified your function to generate item features. When i use those item features in model.fit\ni get this error\nIncorrect number of features in item_features\nSize of training matrix 1475 x 253\nNo of unique products 253\nsize of item features generated 253 x 285 \nTotal number of features 32. ",
    "Adnation": "So is it user features in fit model like https://lyst.github.io/lightfm/docs/lightfm.html#lightfm.LightFM.fit you are saying? If yes can you provide one example of such?. ",
    "beduffy": "Hey,\nI'm also wondering why this is so.\nWhy can't we predict scores for 500 items for 10 users?\nlen(user_ids) = 10\nlen(item_ids) = 500\nThis seems totally logical to me. However, the way around this is to use a loop where I call predict on each user. However, for my problem this will take over 10 hours for ~70k users and ~90k items.. Fair enough.\nThough, I'm still confused as to why do they have to be equal sizes though?\nThanks for the advice!. I just tested, and it seems i only get different results for each run when I use multiple threads.\nIf I want exact reproducibility, is the solution to always call fit or fit_partial with 1 thread?\nIf I want close enough reproducibility, I should ensure the different runs of the algorithm have the same number of threads.\nI assume the algorithm has the unpredictability of using and storing different parameters on different cores. Even if they use the same random seed, due to stochasticity of concurrency we get different results. . Ok thanks a lot for the quick answer!\nI hope my results are \"close enough\" both quantitatively and qualitatively from multiple runs with multiple threads. I'll try out a few things.\nClosing. ",
    "atikunov": "Yes, this is for building debian and ubuntu packages with the sbuild command. For exampe:\ndeb_dist/lightfm-1.13$ sbuild -A -s --force-orig-source -d trusty. ",
    "PabloMessina": "I see, so what does \"FM\" stand for at the end of \"lightFM\", if I may ask?. ",
    "jmmroldan": "I have no interaction for any users. I was thinking of the \"coldest start\" when the system just started and the users didn't express any preferences yet. I thought that in that situation, the features could be used to make a basic prediction (as I said, recommend purple items to users who like the purple color). \nLater, when the users have issued some scores for the items, I could retrain the model adding some interactions.. Thanks for the clarification I think that I had misunderstood how the model works. I thought that the model could learn from both interaction-data and feature-data, even if one of them were not present. \nFrom your answer, I understand that feature information on its own is not considered data since it does not express information about what items the users actually like or dislike.\nA short question just to make myself a better idea of how the features are used: if I have a positive interaction data for a user and an item, then the model will recommend items with similar features to that user or predict that users with similar features will like that item. Is this (roughly) right?. Great, I think I get it know. Thanks for the clarifications, the suggestion, and, of course, for making the library available!. ",
    "praveen2916": "This discussion is golden :). ",
    "JaimePata": "Sorry, I was wrong in my first post. I can make individual predictions. However I can't use functions to evaluate my model using a test set.\nI was trying to test precision using test_precision = precision_at_k(model, test, k=4).mean()\ntest have shape (17832, 16010).\nFull error log:\nTraceback (most recent call last):\n  File \"lightfmtest.py\", line 82, in <module>\n    print(test_precision = precision_at_k(model, test, k=4).mean())\n  File \"/usr/local/lib/python3.4/dist-packages/lightfm/evaluation.py\", line 64, in precision_at_k\n    num_threads=num_threads)\n  File \"/usr/local/lib/python3.4/dist-packages/lightfm/lightfm.py\", line 781, in predict_rank\n    item_features)\n  File \"/usr/local/lib/python3.4/dist-packages/lightfm/lightfm.py\", line 315, in _construct_feature_matrices\n    item_features.shape[1]\nValueError: The user feature matrix specifies more features than there are estimated feature embeddings: 687 vs 16010.. Oh, thanks a lot. I assumed that the item features was stored in the model, and the misleading error didn't help either haha, I was confused because I thought that the error was related to the users features, that I did not use.\nThanks, now is working as expected.. ",
    "rwu1992": "Hi, I have a follow-up question on this. How do we build the interaction matrix in this case? For example, say a user purchased item 1 at time 1 and item 2 at time 2. Is my matrix simply [1,0] at time 1 and [0,1] at time 2? Or should it be [1,1] at time 2?. ",
    "italoPontes": "Are you using Item2Vec?\nhttps://arxiv.org/abs/1603.04259. ",
    "jchen236": "Thank you for the advice. One problem I am trying to tackle is how to generate recommendations for a user that are also similar to a particular item. An idea that comes to mind is that I could generate a set of recommendations for a user, then generate a set of similar items to the target item, and take the intersection of that set, but that seems kind of hacky.\nIs there a good way to achieve this?. ",
    "RAbraham": "Absolutely. Here it is: https://stackoverflow.com/questions/48068147/recommender-systems-convert-uuids-to-32-bit-ints-for-recommender-libraries\nThanks a bunch.. Thanks for the confirmation \ud83d\udc4d \nThe above code is throwaway code to learn how to feed my custom data into lightfm and it helps me correct a lot of my incorrect assumptions about the structure of data. I won't be using any private modules in production :)\n. @artdgn Looking forward to your contributions!. @maciejkula Just curious, what do you use in place of pandas?. Thanks!. typo objective?. minor change: split to split_count. ",
    "EthanRosenthal": "I believe the following should work with LightFM. One thing to note is that the similarity function below ignores the item biases that LightFM learns, but I think it should be suitable for generating sensible recommendations.\n```python\ndef similar_items(item_id, item_features, model, N=10):\n    item_representations = item_features.dot(model.item_embeddings)\n# Cosine similarity\nscores = item_representations.dot(item_representations[item_id, :])\nitem_norms = np.linalg.norm(item_representations, axis=1)\nscores /= item_norms\n\nbest = np.argpartition(scores, -N)[-N:]\nreturn sorted(zip(best, scores[best] / item_norms[item_id]), \n              key=lambda x: -x[1])\n\n```. It looks like you're storing the data in a dense dataframe. If you avoid storing the data in dense format and solely use sparse matrices, you should use considerably less memory.\nFor example, let's say that, on average, each of your 600,000 users has interacted with 100 items. Your interactions matrix will consist of 600,000 * 100 floating point numbers which is only 240 MB for 32-bit floats.. If you re-instantiate the RandomState object each time, then random_train_test_split works fine:\npython\ntrain, test = random_train_test_split(interactions, test_percentage=0.2, random_state=np.random.RandomState(3))\ntrain_weights, test_weights = random_train_test_split(weights, test_percentage=0.2, random_state=np.random.RandomState(3)). ",
    "mstrauch": "Hi,\nthanks for your help.\nUnfortunately the function returns some nan values along with this warning:\nRuntimeWarning: invalid value encountered in divide\n[(39022, nan), (36609, nan), (39031, nan), (36614, nan), (39036, nan), (36613, nan), (39037, nan), (39039, nan), (39041, nan), (39169, nan)]\nThere seem to be some nan values in scores after dividing by item_norms. Do you have some idea?. Figured it out: We have to replace zero values with very small numbers before dividing:\nitem_norms[item_norms == 0] = 1e-10\nHere the full code again:\npython\ndef similar_items(item_id, item_features, model, N=10):\n    (item_biased, item_representations) = model.get_item_representations(features=item_features)\n    # Cosine similarity\n    scores = item_representations.dot(item_representations[item_id])\n    item_norms = numpy.linalg.norm(item_representations, axis=1)\n    item_norms[item_norms == 0] = 1e-10    \n    scores /= item_norms\n    best = numpy.argpartition(scores, -N)[-N:]\n    return sorted(zip(best, scores[best] / item_norms[item_id]),\n                  key=lambda x: -x[1])\nThank you very much for your help!. You are right. I had rows of zero values in my feature matrix. No need for replacing zero values any more! Thanks.. ",
    "novan-p-simanjuntak": "@maciejkula, thanks for your explanation. I will try it.. @maciejkula , thanks, I will try tree-based methods.\nWhat scaler do you think is good?\nI am thinking of using  sigmoid function to scale the result between 0 and 1.. @maciejkula , thank you.. ",
    "dalalkrish": "\n\nPurpose of my system is to rank the ten one-hour block in the order of highest likelihood for a contact to answer the call.\n\n\nI am asking to post the feature matrices of the example you have posted along with the package. I'm struggling to understand how to structure feature matrices with the features that describe a 'contact' and his previously answered call preferences. Does this help you to understand y problem? :) . \n\n",
    "phenric": "Thank you for your responsiveness. \nI installed LightFM with pip.\nI use scores = model.predict(user_id, np.arange(n_items), data['items'], data['users'], 2)\nwhere data['items'], data['users'] are float matrices and np.arange(n_items) is a int matrix.. Yes data['items'] and data['users'] are spares.csr_matrix  build with:\n`def csv_to_csr(file_given):\nwith open(file_given, 'r') as f:\n\n    lines = list(csv.reader(f, delimiter=\";\"))\n    lines = np.array(lines[0:], dtype='float32')\n\nreturn sparse.csr_matrix(lines, dtype='float32m')`. The shape of lines is (100, 8). Yes, that's right. My datasets are not real. I generated them for the test. So, I can give them. Do you have an email address where I can send the files ?. Here you can find the repo https://github.com/phenric/reco\n\nFeel free to add modifications\nThank you. When I comment those lines, I still have a segmentation fault. Do you have any idea about the issue ? What am I wrong ?. Thank you very much for your help !\nYou're right. When the ID is smaller, I don't have Segmentation fault anymore but Exception: Number of user feature rows does not equal the number of users. So, I supposed it was my error ?. Thank you for your help !\nIf I want to calculate the prediction for 2 users and 4 items, I have to create a first array with the two user ids and another with 4 item ids as explained in the documentation, isn't it ?\n\nan array containing the user ids for the user-item pairs for which a prediction is to be computed\n\nHere what I did for my example \nuser_id = np.asarray([], dtype=np.int32)\n    ar = sys.argv[1] \n    for x in ar:\n        user_id = np.append(user_id, x)\nAnd I do the same code to create the item_id.\nWith args as [ [ 17, 15 ], [ 266, 285, 300, 315 ] ].\nHowever, when I try to predict the model it results an AssertionError with this line assert len(user_ids) == len(item_ids) in lightfm.py line 699.\nWhere is my error ? What's my problem ?\nThanks again !. Thank you for the answer.\nBut how could I do to get the merged prediction. Only one prediction that fits for two users as a kind of a \"median\" prediction ?. Ok it seems to work ! Thank you !\nI created for loop to get the prediction for each user as you suggested. Then with another for loop, I add the score to zero numpy array. That's how I get my averaged prediction for few users.\nIs that a correct way ? Is there a more efficient way ?. Thanks again for the help I will close the issue :). ",
    "kahrkunne": "LightFM only implements implicit feedback? Then why does the documentation start with (emphasis mine:) \"LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\". ",
    "artdgn": "\n\nThanks, tried tuning that parameter as well. So far it seems that increasing this parameter a little bit increases accuracy. So I guess I'll leave that at that and will explore switching losses, or decreasing that parameter mid-training later.\n\n\nThanks for the confirmation.\n\n\nGreat. I'll create a generic LighFM-tools repo from what I'm working on, this way some of it might be useful to people even before all of it is of sufficient quality to be included in contrib to the main package. Will keep you updated.. Hi @maciejkula, Had some time to put together an open repo with the tools I'm using, with LightFM's at it's core. \nhttps://github.com/DomainGroupOSS/ml-recsys-tools/\n\n\nHopefully it might be useful to someone else and reduce the learning curve and time to results.\nNaturally since I've been working on it alone and for the specific uses of the project I'm working on, it's by no means as generic or bulletproof, and is still a work in progress. \nWould be grateful for any feedback or ideas to make it more useful. I intend to add examples (there's just one now), documentation and tests.. @maciejkula, not sure what parts would be most useful for most users, but my guess would be the most basic things that I started with: \n1) easy way to get data in (exactly the issues you're trying to solve in #281) both for interactions (and features, if they are useful)\n2) easy way to get recommendations and similarities out\n3) easy way to get evaluation results for multiple metrics and multiple test sets at once\n0) With all three of those performance quickly becomes very important for even medium sized data (because it's out of the Cython part) and I found the only feasible way is to rely heavily on numpy + batched multiprocessing (perhaps the better way is to replace both with Dask)\nRe #281 : I think I have useful and quite easily separable classes for two important bits: \n1) handling the dual state of interaction-matrices and dataframes (and there was a lot of tinkering with performance there, the bottlenecks for the matrix creation vary a lot between different dataset scales)\n2) feature engineering and aligning feature-matrices out of \"regular\" features using pandas-sklearn: this allows a user to just specify which are the numeric and categorical columns and they will all be one hot encoded and aligned to the interaction matrix.\nHowever, this adds pandas and sklearn as basic dependencies, and pandas-sklearn if you'd like to include the one-bang feature-engineering (which takes away a big part of the headache).\nI had to stay away from native python for those things, I think it becomes very slow very quickly on large datasets, and using pandas-numpy-sklearn should make a future port to dask (and ability to parallelize for cluster) much easier (or so I hear).\nI'd be happy to help in anyway you'd think best.. > How do you deal with categorical features in pandas? Load it in as strings, change type to categorical, then one-hot encode?\nI load as (or convert to) strings and one-hot encode (here)\nThe more tricky part is to one-hot encode numerical features, which is done with a custom transformer (same place). As I found that using the numerical features as is (scaled) seems to not work as well (maybe because the relationships are rarely linear? Don't know if that makes sense).\n\nthink doing the name -> idx mappings correctly and consistently is the tricky thing, because we're dealing with two objects: features and interactions (as you say). As far as constructing sparse matrices goes, I'm pretty sure my approach is about as efficient as you can get without dropping down to compiled languages (much more efficient that scipy's DOK and LIL matrices).\nCan you point me to a place where you do the dataframe -> sparse matrices translation?\n\nThe matrix construction is quick once you have the indexes, it's the conversion that I found needed to be parallelized (this is how I'm doing it for now). I'm handling the conversion using sklearn LabelEncoders (they allow to transform and inverse_transform), and I'm converting and creating the matrix in the same place. I've noticed you're mapping using a dict but are converting IDs to idx in a loop, which is where I'd guess I'd expect to have a bottleneck.\nThe alignment of the feature matrix is done on creation of the features dataframe, by using the relevant encoder - here.\n. how about:\ndef number_of_intersections(mat_a, mat_b):\n    return np.sum((mat_a.astype(bool).multiply(mat_b.astype(bool))).tocsr().data)\nseems pretty quick on my data:\n```\n\n\n\nmat\n<707181x334671 sparse matrix of type ''\n    with 11400380 stored elements in Compressed Sparse Row format>\ntest_mat\n<707181x334671 sparse matrix of type ''\n    with 180605 stored elements in Compressed Sparse Row format>\nts = time.time(); print(number_intersections(mat, test_mat)); print(time.time() - ts)\n0\n0.06684756278991699\nts = time.time(); print(number_intersections(mat, mat)); print(time.time() - ts)\n11400380\n0.1686718463897705\n```. Sure, I'll make a PR.. Added. Just out of curiosity, in what scenario would a user want to provide a train matrix to ignore, but would like to prevent that check?. Great, thanks!. nope, you're right, it's slightly faster without it.. Right again. Changes below.. \n\n\n",
    "majd1239": "\nCan you point me to a place where you do the dataframe -> sparse matrices translation?\n\nWas just reading this and thought my input could help. I recently used lightfm to train a 17GB dataset with 420M rows for 10M unique  users and 20k unique items.\nThe raw data was a pandas dataframe of user_id, item_id, score. Using the Dataset module to build the interaction matrices was really slow. Even on a sample of 1M, it took a very long time.\nMy initial approach was converting to category dtypes and then use the cat.codes as mappings, but with large dataframes even changing the dtype takes a long time. What I did was simply construct a manual mapping for indices and ids. \nEg:\n```\nunique_user_ids = df.user_id.unique()\nnum_user_ids = len(unique_user_ids)\nuser_ids_mapping =  dict( zip(unique_user_ids , range(num_user_ids) )\nbelow method is slow, what I did was split the df in chunks and applied the mapping in parallel then # concated the chunks back.\ndf.user_id = df.user_id.apply(lambda id: user_ids_mapping[id] )\n```\nAnd then do the same for item_ids\nThen to get the coo matrices:\ncoo_matrix( (df.score , (df.user_id , df.item_id)), dtype=np.float32)\nThe whole mapping, transforming and constructing the matrices took under 3 mins for the whole dataset. Keeping in mind the dataset has 420 Million rows. \nThis can be easily extended to accommodate user_features and item_features. Point is using numpy/pandas  vectorization to construct the interactions matrices is much faster than the current setup of native python looping/appending\n. Can the recall at k ever be equal to 1.0 if k is less than the total number of positive examples for each user? \nIf all the predicted top k recommended items are correct, but the total number of positive examples is different for each user, how can we know what's a good score for the recall?. ",
    "skurzhanskyi": "Oh, thanks, I'm a newbie to recommender system for now and more familiar with one-class term.\nOnce more question: If I have user/item model, can I show the most similar items for my chosen one.\nBecause I want to show similar items for an item depending on that item and user, who's on this page.\nThanks again  . Ok, I read all the issues and understood a big part of the points. But I have 1-2 questions, I'll ask it as the new issue.. Cosine similarity on item embeddings really worked well and was, actually, better than this similarity on item-user matrix. Thanks a lot!. Now top recommends do not change or change a little, when I change user. I\nwant them change\nOn Feb 20, 2018 18:39, \"Maciej Kula\" notifications@github.com wrote:\n\nWhat do you mean by volatility?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/267#issuecomment-367037811, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AQ0ltTVtZctMJMdWVYfJqJCEC6aeTlkFks5tWvUpgaJpZM4SMM8X\n.\n. I use smth basic like:\npython\nlfm = LightFM(loss='warp', random_state=0)\nlfm.fit(mtrx_train, item_features=mtrx__feat, epochs=1000)\nAnd I try to make top reccomendation for my users, but they (reccomenctions) almost do not change.\nIt's also irrelevant.\nMy mtrx_train has shape (39227, 36731), sparsity is 0.0586%.\n\nMaybe you have any thoughts . Thanks for your answer. I'll try to do it. Tuning hyperparameters helped. thanks!. ",
    "danieljl": "Superseded by #265 . Oops, I forgot to handle if there isn't any negative item. All the tests should pass now. I don't know why tests/test_movielens.py::test_logistic_precision failed on CircleCI only. I didn't even touch the code of logistic loss.\nI've tried to train using my own dataset. The precision and NDCG scores were essentially the same as those of the original implementation.\nBasically, my changes follow the original papers of WARP and WARP k-OS.\nIn the paper a random negative sample is picked from the set of negative items, while LightFM samples it from the whole items: negative_item_id = (rand_r(&random_states[{thread_num}]) % item_features.rows). It can be implemented this way as long as the positive item picked is not counted as a \"wrong\" sample, i.e. sampled is not incremented when the positive item is picked.\nThe second change I made is regarding the loss computation. In WARP paper the numerator is <number of labels/items> - 1 because there is only one true label. If there are more than one label, it should be <number of labels/items> - <number of negative items> as proposed in WARP k-OS paper.. ",
    "aferreira44": "I installed it running conda install -c conda-forge lightfm.. Thanks @maciejkula. It worked installing with pip install lightfm.. ",
    "bdqnghi": "I see, thanks \ud83d\udc4d . My guess, the algorithm doesn't care about the threshold, as long as there is interaction, says > 0, means relevant, is it correct?. Thanks :). ",
    "AkibSadmanee": "Thanks for your response.\nI cloned the repository and tried to install it through that. But got the same result \nand I am new to python, trying to learn it for data science, So I suppose I am not getting the hint from the error. . Thank you very much.\nand Sorry for that. Will have a look at the guide right away. . yes. thanks for your help. ",
    "jellchou": "Yes, I used top to measure this. In prediction stage, the CPU usage is always slightly smaller than 600%.  Maybe each prediction job doesn't need to fully occupy the cpu.  . ",
    "adrianob": "Hi @maciejkula , I'm having the same problem but I'm still at a loss about what to do.\nI'm using LightFM as a simple Collaborative filtering model for now, so I only train it with user ids and item ids. If I try to predict a new item or user that wasn't in the training set I get a The user feature matrix specifies more features than there are estimated feature embeddings error.\nMy problem is that whenever a new user rates an item I want to show recommendations immediately, so retraining the whole model which takes almost 10 seconds is too slow. Is there any better solution?. ",
    "vghiya06": "Are they ready? @maciejkula . ",
    "ravijoe": "i dunno there's this error .i am facing this issues also while importing modules directly from pycharm for example matplotlib etc .i'll also attach a screenshot\n\n. dude . why do you always have to close the thread without even properly answering it ?. i asked my question here because stackoverflow also couldn't answer the question . could you please run this code by yourself ?. ok. so the problem is only in the csv file right ? what i meant by earlier comment was that you fix the errors and try to run the code so that we can be sure there are no other errors . plus this is a fun stock market predictor tool.. you got it all wrong man . suggested edits in a code along with an explanation does not amount to asking someone to do my project . nevermind the question . \nhave a good day .. ",
    "limacher58": "Problem should be solved in gitter comment, thanks a lot. ",
    "aldll": "\nOK, after ran it, I still got the same situation.\nI have tried epochs=30, 50, 100, 1000\nhyperparameters from model.get_params():\n{'epsilon': 1e-06,\n 'item_alpha': 0.0,\n 'k': 5,\n 'learning_rate': 0.05,\n 'learning_schedule': 'adagrad',\n 'loss': 'warp',\n 'max_sampled': 10,\n 'n': 10,\n 'no_components': 10,\n 'random_state': <mtrand.RandomState at 0x7f2b7ae5ffc0>,\n 'rho': 0.95,\n 'user_alpha': 0.0}\n\nYou said 'an unusually small dataset' is that mean the number of user shouldn't too little or something else?\nThanks a lot!. OK, the link: https://github.com/aldll/rs_test\nI have tried to make it simpler and more readable.\nWhat I expected is when user A has not similar record or same hobby with the others, the prediction should give some recommend based on item's content(item features)\nbut it seems the recommender always give the unrelevant items that other users has read,\nSorry for my unclear questions before. Thank you!. I though my data about item features' dimension is too large so that the latent representation calculation makes something wrong in prediction, it that correct??\n. After some trying, tf-idf normalization has eliminated this situation. Thank you for your patiently replying my question and gave me the advise and opinion about input data!. Thanks for your reply.\nI know the value should be > 0, so I want to set each sample_weight value start with 0.01 and if user takes some actions(like, click many times etc), add up this value, but maximum is 1. Is this setting make sense? . Got it! I'm very grateful for your help. Thank you!. 1. OK I see! Thank you!\n2. I have tried epochs 100 300, now I also try 1000, but the CPU utilization still the same(only one is full utilized), so it makes me confused.. I installed LightFM from pip, pip3 install lightfm\nmy code:\nmodel = LightFM(loss='warp', no_components=100, random_state=seed)\nmodel.fit_partial(interaction,\n                      user_features=user_features,\n                      item_features=item_features,\n                      epochs=100,\n                      num_threads=4). OK\nthe output\n#define _OPENMP 201107. Is regularization some parameters in model?\nI used the code above to build the model, and the other settings are default, so I think I didn't use regularization?. I have trying to figure out the problem. Sorry for replying late,\nI want to make sure that:\nIs the regularzation means normalization?\nBecause I used normalization in my item features twice,\nAt first stage, I build a tf-idf matrix and normalizing it to unit length.\nThen I use L1 normalize to rescale tf-idf matrix so that each row's sum equals to 1.\nI try to confirm the assumption above, so I use other item features with all value is 1 to train model, it can run with 4 CPUs.\nIs the item features can't contain too many small values for algorithm of SGD to compute?\nCould you please tell me description about the question? I want to know it in more details. And what is the problem about using a lot of regularization? Thank you for reading my long questions! I appreciate it!\n-----Update message-----\nAfter some other trying, I found that simply reduce the item features makes it work with multi threads.\nmy tf-idf matrix size: 30000 * 500000\nslicing it to 30000 * 10000\nBut the dimension of item features shouldn't be too large seems strange.. OK, this is the data: https://github.com/aldll/t\nSince the data is too large, I only upload 3000 items.\nYou said \"The algorithm forces full regularization passes whenever the accumulated regularization is quite large.\"\nSorry I didn't understand this meaning. the algorithm means which part? loss function? Could you please explain more about it? Many thanks!. Got it!\nMay I ask for more details in algorithm you mention before? \nAlso, the recommend result with different num_threads is different, it there any setting wrong?\nOr the reason is the algorithm?\n. I see. I will read the paper!\nThank you for your answer and provide great information!\nI will close this issue later.. Yes, I'm using sparse matrices. I simply load input data by method in lib scipy.sparse: sp.load_npz() and observe the memory usage.\nBecause a part of my item features matrix contains dense structure,\nfor example, an item with 300 features contains sparse vector in the first 200 features, the rest are dense vector,\nso a float matrix with 300 features and 2 hundred rows takes 3 gigabyte of memory.\nOr does it have other approaches to not load all input data into memory for training?\nThank you for your reply!. Got it.\nmemory-mapped numpy arrays can only apply after the model has trained( use in prediction), Is that right?\nSo if I want to train the recommendations with very large interaction and features matrices, \nthe only way is enlarging memory?\nThank you for answering my questions!\n. Yes, but memmapped can only be used in numpy array, not in sparse array(scipy.sparse).\nand input data for model should use sparse array only. Is that right?. ",
    "simon19891101": "Hi Maciej, thanks very much for replying. The overall workflow sounds pretty clear however I was wondering what happens if we pass an user-defined use_feature/item_feature rather than 'None' to the fit function? Please feel free to leave comments if I'm fundamentally wrong in understanding the logic. Cheers. . Hi Maciej, I just figured out how it works! There was a trivial mistake made when tired to undertsand the logic. Thanks.. ",
    "rajat1602": "\nCurrently, doing the same.\nBoth user and item features are binary. 1 if the feature is available for that particular user or item.\nSo, weighting is either 0 or 1. \nNote: feature matrix has been transformed to datatye = np.float32\n\nBy adding them, I want to solve user / item cold start problem.  Is there any other way to tackle this, that may help!. Alright! will try this too.\nDo I need to normalize my interaction matrix as Interaction matrix contains ranking from 0  to 5? and Feature Matrices are binary.. Ok...I have made the few changes which you suggested. Will let you know the results.\nOne more thing, while predicting for new user and new item do I need to pass user / item feature for that particular user and item or the whole user feature and item feature plus the new user / item feature used during training?. Hi, It worked man!!....Thanks much...Getting \nAUC: 0.98 after using both User and Item Feature.\nOne more thing, while predicting for new user and new item do I need to pass user / item feature for that particular user and item or the whole user feature and item feature plus the new user / item feature used during training?. I am a bit confused. Suppose I want to predict for 20 items for 1 user, Do I need to pass only the id's of these 20 items and 1 user plus the user / item features for 1 user and 20 items. Something like predict(0,[0...19],user_features = [1X200],item_features = [20X100].\nOR, whole user / item feature matrix used during training i.e. .User Feature matrix of shape (15732,15932) .Item Feature matrix (CSR) of shape (48947,49047)???\n. Let me tell you my business problem. I am making recommendation for Market Research. So, I get almost 100-150 new items daily. So there are no interactions for these. I just only want to recommend these items in periodic fashion (say 1 hour) to the existing user in database.\n1. Do I need to make the whole new interaction matrix daily, which means suppose U1 interact with 1 item from 100 items, so i can use this interaction to recommend for different users which may have same interaction pattern from last 2 day interaction matrix for those items.\n2. Or, just build the model once with huge dataset say, past 1 year data and train the interaction / user and item features...pickle the model and just pass the ids and features.\nFor above query:\nNo, these 20 items are new items. So there are no interactions for these item in user-item matrix. and that User has suppose index of 1000 in interaction as well as user feature matrix and want to predict for these 20 items for this user. Do I need to pass the index 1000 in predict or 0 plus what will be the user / feature matrix? Do I need to append the user / feature matrix of these 20 new items in the user / feature matrix used during training ??\nAny help will be much appreciated!!. ",
    "vndkmr": "Thank you !!\nOne last question I have is, for these 30 new items do I need to append these in item features matrix used during training and then give the proper indices in predict function or just create the new feature matrix for the items from 0 to 29 (note :  sharing the same features as in training set) and pass the item ids as 0 to 29 and item_features as [0..29 X no of features]?\nAs per I know LightFM, in predict function userid and itemid are just the indices in the user and item feature matrices passed in predict function? Am i correct? If this is the case, then just I need to create the new matrix sharing the same features and run the prediction.\nIf new feature comes in, then I need to retrain the whole model.. ",
    "EricC91": "I am in the same situation as vndkmr.\nBut I have a supplementary question: \nI would like to predict recommendations for items that are in the training set but also new items.\nHow I can do this ?\nI think append features of new items to the training matrix is the best choice ? No ?\nBut there is no function to add features with your Dataset object.... Ok \nI didn't see the parameters of the Dataset Class.\nSorry for the inconvenience.. ",
    "SantGIIT": "Sorry that I forgot to mention that as of now, I'm experimenting with collaborative filtering only, i.e., I haven't specified the user and item feature matrix (but tried specifying those as identity matrix too).\nHyperparameters: learning_rate=0.01, loss='bpr', no_components=10, epoch=20.\nIn a typical example,  the number of users: 1500, the number of items: 1000, Sparsity: 1.694% and the rating matrix is binary, so couldn't understand what do you mean by the distribution of the data. \nPerformance: \n Train set size: 17411\n Test set size: 4353\n Training model WITHOUT meta-info.\n Item Embedding Matrix Dims: (1000, 10)\n User Embedding Matrix Dims: (1500, 10)\n Precision: train 0.02, test 0.01.\n AUC: train 0.54, test 0.53..  With loss='logistic', and learning_rate=0.05, no_components=30, the result is\n Precision: train 0.00, test 0.00.\n AUC: train 0.82, test 0.23. . Hi Maciej,\nThanks for the help. Have got improved performance in CF without content\ninfo.  in terms of AUC. But, now I have another difficulty figuring out why\nincorporating the item's content information is not helping in my case.\nHere is what the performance my code reported.\nLoading the data...\n Creating Interaction Matrix\n Created Size: (5000, 3)\n=====================================\n Starting likes info\n Number of users: 1686\n Number of items: 4381\n Sparsity: 0.068%\n Ending likes info\n Number of users: 1678\n Number of items: 3903\n=====================================\nTuning parameters without metadata\n=====================================\nWithout Metainfo Best score 0.535032510757 at {'num_epochs': 49,\n'no_components': 35, 'user_alpha': 4.7024178561511364e-09, 'max_sampled':\n10, 'item_alpha': 2.326188727461577e-09, 'loss': 'bpr', 'learning_rate':\n0.019314686949204716, 'learning_schedule': 'adadelta'}\n=====================================\n Training model WITHOUT meta-info.\n=====================================\n Item Embedding Matrix Dims: (3903, 35)\n User Embedding Matrix Dims: (1678, 35)\n Precision without Meta-info: train 0.12, test 0.00.\n AUC without Meta-info: train 1.00, test 0.53.\n=====================================\nTuning parameters with metadata\n=====================================\nWith Metainfo Best score 0.528344392776 at {'num_epochs': 41,\n'no_components': 45, 'user_alpha': 3.792675779791911e-09, 'max_sampled':\n11, 'item_alpha': 1.2647070730452016e-09, 'loss': 'bpr', 'learning_rate':\n0.01985679207599666, 'learning_schedule': 'adadelta'}\n=====================================\n Training model WITH meta-info.\n=====================================\nPrecision with meta-info: train 0.00, test 0.00.\nAUC with meta-info: train 0.54, test 0.50.\nItem Embedding Matrix Dims: (24, 45)\nUser Embedding Matrix Dims: (37, 45)\nI understand that data is very sparse in this example, but I have increased\nsparsity level to 5% but still no improvement.\nPlease, help.\nSantosh\nOn Tue, May 15, 2018 at 5:13 PM, Maciej Kula notifications@github.com\nwrote:\n\nThat does look better. In general, you always want to do a hyperparameter\nsearch. Have a look here\nhttps://gist.github.com/maciejkula/29aaf2db2efee5775a7f14dc387f0c0f for\nsome sample code.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/lyst/lightfm/issues/299#issuecomment-389137271, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AaAagYA4ULvpFQeU1s55SjIjh9KH5zX6ks5tyr9wgaJpZM4T9xBB\n.\n. \n",
    "dawenl": "I can try that. But wouldn't this also make the gradient itself very large? I am not sure whether learning_rate * gradient will end up being 0 or blow up. Any idea?. Oh I see, so model.item_embedding_gradients and model.item_bias_gradients are essentially just the cumulative gradient norms for the denominator of adagrad updates? If so, that I think this would work :) . In that case, wouldn't setting them to np.inf be more precise (i.e., setting the updates to exactly 0)? Or do you see it causing other problems? . It seems this does not work: For strong generalization, you would want to evaluate on a set of users who are not in training, but apparently fit_partial was expecting the same set of users from training, while fit will overwrite everything. Did I miss something? . Thanks! This seems to work (quite a hassle though). Setting to np.finfo(np.float32).max seems large enough that the item embeddings didn't change at all. . I think something is off -- I did it one way: call model.fit_partial multiple times with epoch=1, and after each time, I did all the freezing gradients method we discussed; later I did a model.fit_partial once with epoch=n, the same n which I ran multiple times, and then did the freezing gradients method. There two should be equivalent, but the results are drastically different. \nI reported in the paper with the better numbers I got so that I can sleep at night, but would like to solve it in a more principled way. . For both runs,  the model starts fresh. . Sorry, let me be more clear: \nHere when I call fit_partial, I was doing the model training. The only difference is that in the first way, I compute the strong generalization metrics after every epoch by freezing the gradients, while the second way, I only compute the strong generalization metrics after n epochs by freezing the gradients. . OK, I glossed over the details... I did unfreeze them after each epoch. And interestingly, I got much better results in the first way (n calls with epochs=1). . I created a copy of the gradients before freezing them and reassign them back after evaluation. . Here is some code snippet: \n```\nfor _ in range(epochs):\n        // train for one epoch\n        model.fit_partial(train_data_expanded_coo, epochs=1)\n    iemb_g_bkp = model.item_embedding_gradients.copy()\n    ibias_g_bkp = model.item_bias_gradients.copy()\n\n    // freeze the gradient and optimize held-out users\n    model.item_embedding_gradients = np.finfo(np.float32).max * np.ones_like(model.item_embedding_gradients)\n    model.item_bias_gradients = np.finfo(np.float32).max * np.ones_like(model.item_bias_gradients)\n\n    model.item_alpha = 0.0\n\n    model.user_embeddings[-n_vad_users:] = (\n        (model.random_state.rand(n_vad_users, model.no_components) - 0.5) /\n        model.no_components).astype(np.float32)\n\n    model.fit_partial(vad_data_expanded_coo, epochs=5)\n\n    // unfreeze the gradient\n    model.item_embedding_gradients = iemb_g_bkp\n    model.item_bias_gradients = ibias_g_bkp\n\n    model.item_alpha = alpha\n\n```. This is on MovieLens dataset (since i am writing a paper). I will try to get to a reproducible example. . ",
    "eggie5": "\nThe calculations of gradients 1, 2, 4, 5, 6 seem right to me, but isn't biases of user (b_u) subtracted by itself, as the above fig?\n\nYes, that is true, for pair-wise loss, the user biases cancel out. . Fair enough, was mostly trying to get your thoughts as I don't think you check Gitter anymore :). I have a disjoint set. If I run precision@k for w/ and w/o the train_interactions flag, I get different results. Does this make sense? \nI didn't think I would get different results, b/c I thought train_interactions would simulate the effect of a disjoint set -- the condition that I already have.. Is this a deployment question? Do you just want to run inference/predictions in another environment. If that's the case since light if is a matrix factorization model you just need to export your user and item embeddings. For example, export the embeddings to csv. Then, in ruby you can load the embeddings into memory and make recommendations using the dot product.. As you can see in: https://github.com/lyst/lightfm/blob/master/lightfm/_lightfm_fast.pyx.template#L320\nIt is indeed taking the dot product between the user and item embedding. \nitem_features and user_features are side-information which you can ignore if you want to approach this a classic matrix factorization problem. If you have side information/meta-data you can use it in the training routine (see arguments to fit function) at which point you depart from classical MF and doing a hybrid approach which he highlights in referenced paper as a remedy for cold-items. . If you use feature side-info you to get the user/item representation you need to multiply the embedding matrix by the feature matrix for user and item respectively as per this function: http://lyst.github.io/lightfm/docs/_modules/lightfm/lightfm.html#LightFM.get_item_representations\n. ",
    "NegatioN": "Sorry for the late reply :) Cool that you like it, I'll try to write some tests during next week, and hopefully results will make enough sense that we can feel comfortable merging it.. I think there are some aspects of the code that I don't fully understand, and that are not immediately compatible with normalized vectors. \nDo you have any tips/pointers for what might be causing this?\nWhen i try to run a simple movielens-test on the data, loss=logistic item_embeddings diverges, and loss=warp gets poor performance on both test an train data.\nI will continue to investigate\n```\n============================================================================================================== FAILURES ==============================================================================================================\n______________ testmovielens_normalized_accuracy_fit _______________\ndef test_movielens_normalized_accuracy_fit():\n    model = LightFM(random_state=SEED, normalize=True, loss='warp')\n    model.fit(train, epochs=20)\n\n    train_predictions = model.predict(train.row,\n                                      train.col)\n    test_predictions = model.predict(test.row,\n                                     test.col)\n\n    #assert roc_auc_score(train.data, train_predictions) > 0.84\n\n\n  assert roc_auc_score(test.data, test_predictions) > 0.76\n\nE       assert 0.63457222732392338 > 0.76\nE        +  where 0.63457222732392338 = roc_auc_score(array([ 1,  1,  1, ..., -1,  1, -1], dtype=int32), array([-0.51581025,  0.03291166, -0.27147731, ..., -0.23394495,\\n       -0.53144515, -0.6818071 ]))\nE        +    where array([ 1,  1,  1, ..., -1,  1, -1], dtype=int32) = <943x1682 sparse matrix of type ''\\n    with 9430 stored elements in COOrdinate format>.data\n```. I think this has to do with the addition of multiple normalized vectors onto each other. This doesn't seem to gel well with stable results. Don't really have the time to look for a solution now, but it would be cool if anyone else dropping by this PR looked into it :). I didn't write the correctly in my last post, this is what I meant:\nI don't think the normalization works well after the weighted summation step.\n\nAs far as I understand this should be after summation and almost the last step before calculating the loss. Code for normalizing looked like:\n```\ncdef inline flt compute_norm(flt *v_repr, int no_components) nogil:\ncdef int i\ncdef flt result\n\nfor i in range(no_components):\n    result += v_repr[i] * v_repr[i]\nreturn sqrt(result)\n\ncdef inline flt compute_prediction_from_repr(flt user_repr,\n                                             flt item_repr,\n                                             int no_components,\n                                             bint normalize) nogil:\ncdef int i\ncdef flt result\n\n# Biases\nresult = user_repr[no_components] + item_repr[no_components]\n\n# Latent factor dot product\nfor i in range(no_components):\n    result += user_repr[i] * item_repr[i]\n\nif normalize:\n    result = result / (compute_norm(user_repr, no_components) * compute_norm(item_repr, no_components))\n\nreturn result\n\n```\nHowever, I now tried normalizing at the summation-step... for science or something, and that didn't seem to produce any different results, so maybe I'm doing something fundamentally wrong somewhere.\n```\ncdef inline void compute_representation(CSRMatrix features,\n                                        flt[:, ::1] feature_embeddings,\n                                        flt[::1] feature_biases,\n                                        FastLightFM lightfm,\n                                        int row_id,\n                                        double scale,\n                                        flt *representation) nogil:\n    \"\"\"\n    Compute latent representation for row_id.\n    The last element of the representation is the bias.\n    \"\"\"\ncdef int i, j, start_index, stop_index, feature\ncdef flt feature_weight\ncdef flt norm\ncdef flt *current_emb = <flt *> malloc(lightfm.no_components+1 * sizeof(flt))\n\nstart_index = features.get_row_start(row_id)\nstop_index = features.get_row_end(row_id)\n\nfor i in range(lightfm.no_components + 1):\n    representation[i] = 0.0\n\nfor i in range(start_index, stop_index):\n    feature = features.indices[i]\n    for j in range(lightfm.no_components+1):\n        current_emb[j] = feature_embeddings[feature, j]\n    norm = 1 / compute_norm(current_emb, lightfm.no_components+1)\n    feature_weight = features.data[i] * scale * norm\n    for j in range(lightfm.no_components):\n        representation[j] += feature_embeddings[feature, j] * feature_weight\n\n    representation[lightfm.no_components] += feature_biases[feature] * feature_weight\n\nfree(current_emb)\n\n```. I'm not sure if it's smart to try handling the user/item mappings found in the default dataset implementation: https://github.com/lyst/lightfm/blob/master/lightfm/data.py#L135\nIt definitely adds friction that it's handled outside of the save/load functions, but I think they need to be integrated tighter to make it easy to include in the numpy-object.\nOther than that, both vectors and hyperparams should be taken into consideration now @maciejkula :)\nSeems like black-tests are passing on my machine, so I'm not quite sure what to make of that.. @maciejkula I've fixed the formatting errors now, would you mind taking a look? Seems like the current error is related to imports in an untouched file.. Build is now failing because (of what I can only assume), there is something wrong with the how the caching is done in CircleCI. I can fix the commit history after this issue is resolved.\nIt either fails because it has a definition of .load() defined which it think takes two inputs (as both a classmethod and staticmethod it only takes one), and if I rename the function to load_uncached to force through some way to un-cache this, I get the error-message: AttributeError: type object 'LightFM' has no attribute 'load_uncached'.\nPlease correct me if I'm wrong, but I don't think I can help fixing this.. I understand that this has been long in the making, but I believe I have addressed the changes you wanted @maciejkula ? It just needs another cache refresh. @rth ONNX is definitely better. I guess it should be possible to translate the whole model structure as well, so even if the lyst model definitions change, they are still compatible. And exporting models to different frameworks for serving or training could be interesting too.\nI do believe the numpy save/load functionality here would work with allow_pickle=False too though, as we're only really persisting floats and strings. \nI'm kinda considering this PR dead, as there is little traction, and it is more of a \"nice-to-have\" and not really important for the usage of the library. Just figured I'd contribute my local utils since no save/load functionality existed.. Should be done now. Needs another cache-clean. Also, if you do end up merging this, just squash everything. I don't think it makes sense to keep any of the history except the initial commit.. ",
    "reditya": "Hi @christinedekock,\nRecently, I also explored similar situation as yours, where in addition to positive implicit feedback data, I also have implicit negative feedback. The way I approach the problem is instead of sampling negative items at random, I modified a bit the sampling strategy to sample from implicit negative feedback. This is somewhat intuitive IMO, because if we already have the negative feedback for each users, why would we choose at random? This is even more relevant if we have sufficient number of negative feedbacks. \nComparing to random sampling, this small tweak lead to better MAP with fewer epoch. I think it helps the model to converge faster (in terms of epoch). I'm still performing extensive evaluation though, but the initial result seems promising. I use the WARP loss, haven't tried using logistic loss though.\nI'd like to say thanks as well to @maciejkula  for this great package!. ",
    "AmiraAyadi": "Converting the interaction matrix to CSR then back to COO format work! thank you! in fact, I did a \"drop_duplicates\" on my data but I forgot to specify on which column and as I had the date of purchase I found myself with duplicates without me noticing. I was sure there were no duplicates, sorry to have bothered you with that.\nI will try to find time to write the PR that takes integers and transforms them into a RandomState :)\nI have another question though, I have another question though, as I have my train and test set now. I'm getting pretty high results : \nCollaborative filtering train AUC: 0.9123019\nCollaborative filtering test AUC: 0.9124159\nand \nHybrid train AUC: 0.99997926\nHybrid test  AUC: 0.9999861\nit seems very odd to me...do you have any idea why I have these results?. in the other hand, Precision values seem normal:\nPrecision: train 0.12, test 0.11.\nShould I work with these values or do you think there is a problem to look for in my data ? . that's great then! thank you for your amazing work and your help :). Thank you very much.  I was not sure about it :). ",
    "bkj": "Also -- Are there any benchmarks for \"good\" P@K on Movielens?  I'm trying to figure out what the state-of-the-art on these benchmarks is, but can't find many resources.\n~ Ben. ",
    "LauraMitchell16": "Thanks for the response, that\u2019s a shame. \nCould you recommend any other libraries that could help implement this hybrid explicit model? From my research it seems that Spotlight and Suprise libraries only implement Explict Collaborative Filtering models. Or if not a hybrid approach, could you recommend a library for a Explict Content-Based model? . ",
    "jattenberg": "This is a good lead, thanks. i think i should be able to do some manipulation and use nmslib too. . Small followup for future generations: using nmslib with the option: space='negdotprod' does efficient approximate max inner product search, and seems to work pretty well for generating recs. I'd make a demo, but I don't think it'd me worth introducing the extra dependency.. ",
    "rayane-m": "Oh ok.\nNo, none of my features names are the same as user ids. \n. I misunderstood this line in the example : dataset.fit_partial(items=(x['ISBN'] for x in get_book_features()),\n                    item_features=(x['Book-Author'] for x in get_book_features()))\nI was trying to do the same thing, obviously the wrong way.\nby modifying using just the name, I have the right result. Thank you !\n. Sorry to re-open this, but after that I continued by building the users/items features (always following the example and the documentation):\n```\nCreat user matrix\nuser_features = dataset.build_user_features(((x['id_user'],list_columns_user) for x in user), True)\nprint(repr(user_features))\n```\nLike in the documentation : \n\ndata (iterable of the form) \u2013 (user id, [list of feature names])\n\nx[id_user] being my user id and list_columns_user being my list of features names. But when I visualize one row of the user_features, I only get 0 everywhere except in the index of that row. in other term, user_features is just the identity matrix.\nExample:\n```\nuser_features[1, :].todense()\nOut[31]: matrix([[0., 1., 0., ..., 0., 0., 0.]], dtype=float32)\n```\nIs it the excepted result ? If yes, I think I don't really understand how the user features matrix is build and how it's different from the collaborative filtering.\n. At the moment I pass the user features name in fact, that what I read in the doc.\nI tried with the actual list of features for each id, like that:\nuser_features = dataset.build_user_features(((x['id_user'], x[list_columns_user]) for x in user), True)\n(user is a  csv.DictReader like in your example)\nbut it did change a thing :( . Sorry for late answer, I no longer had access to the data.\nyou nailed it! It was the generator problem, I didn't know that we can't iterate over it plus then once.  I don't really know how to deal with this type. So I redid all the treatment with pandas (I know it better), and I think it works!\nI build a new user_features  and it is a diagonal matrix with 0.0454 everywhere on the diagonal. Is the fact that it's the same value across the diagonal is excepted? \nI didn't normalized my values, just used the parameter normalize=True while building user/item features (with build_item_features and build_user_features) may be this can lead to a mistake ?. Got it. thanks a lot, I really really appreciated your help.. ",
    "ailurus1991": "OMG I didn't pass the model.\nsorry my bad.\ndrunk.... ",
    "RaphLot": "Here all the evalution I did:\nHybrid train AUC: 0.93\nHybrid test  AUC: 0.93\nPrecision@k: train 0.01374, test 0.01130. \nreciprocal_rank: train 0.05904, test 0.04969. (I suppose this is the MRR)\nItem is a DataFrame where I stored all my items ids specific to my case. It's just to me a way to reconize the item.  . If I take the last predicted values, here what I get:\narray([ -9.96430874, -11.22552967, -12.57436848, ..., -12.96259212,\n       -12.85723877, -12.80404377])\nI just tried it with the default params and the prediction change but are still the same for everyone.\nI printed the user_embeddingsand I find them very close. Here a sample:\narray([[ 0.19810183,  0.31938615, -0.31442386, ...,  0.3452737 ,\n         0.30237758, -0.35419983],\n       [ 0.2045071 ,  0.3318007 , -0.3208398 , ...,  0.3587329 ,\n         0.30575866, -0.3844172 ],\n       [ 0.196473  ,  0.30080256, -0.28448632, ...,  0.31968027,\n         0.27496523, -0.35817078],\n       ...,\n       [ 0.19269793,  0.29828662, -0.2888104 , ...,  0.32078212,\n         0.28065202, -0.3432573 ],\n       [ 0.16556157,  0.28962615, -0.28945726, ...,  0.32600516,\n         0.27702335, -0.36565372],\n       [ 0.16648588,  0.28340086, -0.2805839 , ...,  0.31332952,\n         0.26852673, -0.3434492 ]], dtype=float32). I played with params and loss and I get now slightly diffrent recommendation for users. \nThe problem, and I think it's the fact that the user_embeddings are close who causes this, is that it's always the same 5 items or so. Example of what I get: \nuser 0 : item 2, item 3, item 5, item 8, item44\nuser 1 : item 2, item 3, item 5, item 8, item44\nuser 2 : item 3, item 2, item 5, item 8, item44\nuser 3 : item 2, item 3, item 5, item 8, item44\nuser 4 : item 5, item 2, item 3, item 8, item44\nuser 5 : item 2, item 3, item 5, item 44, item 8\n. thanks a lot for these suggestions.\nI work on that and :\n1. It's work I have different recommendations  for all users \n2.It's work I have different recommendations except for some users\n3.  In global, yes. But in some cases there are a lot of features in commun  (like  the country). I saw you mentionned tf-idf in the some previous issues, but to be honest I don't know how to do it.\n\nI will try this soon, thanks.\n\nDo you know why I can't use both user and items features ? \n. ",
    "balancy": "Yes, I tried. In Spyder it gives the same error with the kernel, in terminal it gives an error like 'Invalid instruction. Dump of memory is made'. I tried to install it via pip at the beginning. It gives me an error:\n```\nCollecting lightfm\n  Using cached https://files.pythonhosted.org/packages/e9/8e/5485ac5a8616abe1c673d1e033e2f232b4319ab95424b42499fabff2257f/lightfm-1.15.tar.gz\nRequirement already satisfied: numpy in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (1.14.5)\nRequirement already satisfied: scipy>=0.17.0 in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (1.1.0)\nRequirement already satisfied: requests in /home/pavel/anaconda3/lib/python3.6/site-packages (from lightfm) (2.18.4)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (3.0.4)\nRequirement already satisfied: idna<2.7,>=2.5 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (2.6)\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (1.22)\nRequirement already satisfied: certifi>=2017.4.17 in /home/pavel/anaconda3/lib/python3.6/site-packages (from requests->lightfm) (2018.4.16)\nBuilding wheels for collected packages: lightfm\n  Running setup.py bdist_wheel for lightfm ... error\n  Complete output from command /home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-fotd7sfp --python-tag cp36:\n  running bdist_wheel\n  running build\n  running build_py\n  creating build\n  creating build/lib.linux-x86_64-3.6\n  creating build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/data.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/_lightfm_fast.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/lightfm.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/evaluation.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/init.py -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/cross_validation.py -> build/lib.linux-x86_64-3.6/lightfm\n  creating build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/_common.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/init.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/stackexchange.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/datasets/movielens.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n  copying lightfm/_lightfm_fast_no_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n  copying lightfm/_lightfm_fast_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n  running build_ext\n  building 'lightfm._lightfm_fast_openmp' extension\n  creating build/temp.linux-x86_64-3.6\n  creating build/temp.linux-x86_64-3.6/lightfm\n  x86_64-conda_cos6-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -Wstrict-prototypes -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fPIC -I/home/pavel/anaconda3/include/python3.6m -c lightfm/_lightfm_fast_openmp.c -o build/temp.linux-x86_64-3.6/lightfm/_lightfm_fast_openmp.o -ffast-math -fopenmp\n  unable to execute 'x86_64-conda_cos6-linux-gnu-gcc': No such file or directory\n  error: command 'x86_64-conda_cos6-linux-gnu-gcc' failed with exit status 1\n\nFailed building wheel for lightfm\n  Running setup.py clean for lightfm\nFailed to build lightfm\nInstalling collected packages: lightfm\n  Running setup.py install for lightfm ... error\n    Complete output from command /home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-record-fzunoz2d/install-record.txt --single-version-externally-managed --compile:\n    running install\n    running build\n    running build_py\n    creating build\n    creating build/lib.linux-x86_64-3.6\n    creating build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/data.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/_lightfm_fast.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/lightfm.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/evaluation.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/init.py -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/cross_validation.py -> build/lib.linux-x86_64-3.6/lightfm\n    creating build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/_common.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/init.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/stackexchange.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/datasets/movielens.py -> build/lib.linux-x86_64-3.6/lightfm/datasets\n    copying lightfm/_lightfm_fast_no_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n    copying lightfm/_lightfm_fast_openmp.c -> build/lib.linux-x86_64-3.6/lightfm\n    running build_ext\n    building 'lightfm._lightfm_fast_openmp' extension\n    creating build/temp.linux-x86_64-3.6\n    creating build/temp.linux-x86_64-3.6/lightfm\n    x86_64-conda_cos6-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -Wstrict-prototypes -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -fPIC -I/home/pavel/anaconda3/include/python3.6m -c lightfm/_lightfm_fast_openmp.c -o build/temp.linux-x86_64-3.6/lightfm/_lightfm_fast_openmp.o -ffast-math -fopenmp\n    unable to execute 'x86_64-conda_cos6-linux-gnu-gcc': No such file or directory\n    error: command 'x86_64-conda_cos6-linux-gnu-gcc' failed with exit status 1\n----------------------------------------\n\nCommand \"/home/pavel/anaconda3/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-jlt987mm/lightfm/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-record-fzunoz2d/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-jlt987mm/lightfm/\n``\n. I am using Intel i7. Normally, pip install works for a lot of libraries like keras, numpy, matplotlib, etc. It's just lightfm which doesn't want to be installed on my machine.. Finally,conda install gxx_linux-64` solved my problem. Thanks.. ",
    "navy-xie": "\nDid you install it from pip or conda? If from conda, uninstall and install from pip.\n\u2026\nOn Sat, 30 Jun 2018, 20:09 balancy, @.***> wrote: Yes, I tried. In Spyder it gives the same error with the kernel, in terminal it gives an error like 'Invalid instruction. Dump of memory is made' \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub <#321 (comment)>, or mute the thread https://github.com/notifications/unsubscribe-auth/ACSCA4hJrbimwKnNs2OHod41Dq9wRMGbks5uB77jgaJpZM4U95Q4 .\n\nUsing pip to install solved my issue. Thanks a lot.. ",
    "PlissBurl": "\nYou can't add new users or new items (or new user features or new item features) to an already estimated model.\n\nOK, I thought it was the purpose of fit_partial.\n\nIf you expect new users, you should train the original model allowing for them (even if they have no interactions yet).\n\nSo I have to re-train the model for all users (new and old) and all items every time I have new user/item ?\nIs that it ? Sorry if I misunderstand.\nIt seems to be very heavy to compute all this, knowing that we want to use this recommendation also for visitor (as we have their meta data) and we have a lot of visits, re-compute all the model will take too much time. Is there an other solution ? \n. ",
    "seniath": "\nThanks for the detailed problem description!\n\nAnd thank you for the speedy reply! This is very useful information. I'll see if we can figure out a better solution to when and where our model is trained overnight.. > 2. Yes, you can also do that.\nWould using a mostly-empty set of user_features such as this have an impact on the quality of the prediction - vs using a 'full fat' updated set?\n\nOne additional way of speeding up requests is by using the get_item/get_user_representations methods. Those will give you the latent vectors (and biases) given features, and you can just do item_bias + dot(user_repr, item_repr) from there.\n\nI'll be honest, I don't fully follow this suggestion I'm afraid! Would it be possible to go into more detail?. > No. After fitting with the full set, the model uses only the features for the user (and items) you are predicting for.\nPerfect \ud83d\udc68\u200d\ud83c\udf73 \ud83d\udc8b \n\nHave you read the LightFM paper?\n\nNot yet - will take a look, thanks!. ",
    "RothNRK": "I'll find some time for it. . I can't say I completely agree with the transparency argument but I'm happy to make a PR for the second suggestions of making a copy of the RandomState. . No I didn't even bother looking through them since the majority of them are questions. I think the project would get more help if it was clear of where people can help.. In my case latency is an issue so I make predictions for each user/lot, store the top_n recommendations for each user, and serve those results from an endpoint. The endpoint is essentially a dictionary lookup. . Yeah I missed that. I wouldn't know how to address that but would be interested to see what you come up with.. From the docs it looks like you want LightFM.fit_partial.\nhttp://lyst.github.io/lightfm/docs/lightfm.html#lightfm.LightFM.fit_partial. You could train your model offline using your full set of historical data, and afterwards update your model using the LightFM.fit_partial with the new data during production.\nIn this case your model will continue grow if you add new users/items that were not included in the initial LightFM.fit.\nDoes this example help?\n```\nfrom lightfm import LightFM\nfrom lightfm.data import Dataset\nMake some fake data.\ndef fake_data(n):\n    users = np.random.choice([0., 1., 2.], (n, 1))\n    items = np.random.choice([0., 1., 2.], (n, 1))\n    weight = np.random.rand(n,1)\n    return np.concatenate((users, items, weight), axis=1)\ntrain_data = fake_data(10)\nnew_data = fake_data(3)\nUse Dataset to prep your interactions and weights.\ndataset = Dataset()\ndataset.fit(users=np.unique(train_data[:, 0]), items=np.unique(train_data[:, 1]))\ntrain_interactions, train_weights = dataset.build_interactions((i[0], i[1], i[2]) for i in train_data)\ndataset.fit_partial(users=np.unique(new_data[:, 0]), items=np.unique(new_data[:, 1]))\nnew_interactions, new_weights = dataset.build_interactions((i[0], i[1], i[2]) for i in new_data)\nFit the model using your full set of historic data.\nsolver = LightFM()\nsolver.fit(interactions=train_interactions, sample_weight=train_weights)\nIn production update your old model with new data.\nsolver.fit_partial(interactions=new_interactions, sample_weight=new_weights)\n``\n. @fischjer4 I'm sorry but I just ran my example without any error (lightfm==1.15).  If you run my example and still get an error can you try and increase the number oftrain_data = fake_data(100)` to 100? Let me know  how that works for you. . I'm happy you sorted out the issue.. You would likely pickle your trained model after training. When new data becomes available you would load the model object and then fit the model with the new data using fit_partial.. Your second suggestion is what I am doing, just for a group, and the memory requirements grow at a massive rate. How would doing this for each user change anything? \nI'll look into the first suggestion. Thank you.. @maciejkula This seems like something that should be fixed in the library itself. If you can offer some direction I'm happy to look into it myself.. Hey, I haven't been able to recreate the problem so I've been focussing on my project. I'm pretty sure this should be closed but wanted to hold off for a bit. Thanks for your patience. . Hey. Sorry about this ticket. Looks like the problem was on my side. . @hammadkhann I'd also suggest that you read Maciej's paper. It's pretty accessible and a better understanding of the library will likely help you clear up questions faster than tagging us.\nhttps://arxiv.org/abs/1507.08439. That makes sense. I'll get that in there. . ",
    "tianpunchh": "Maybe I do not get the point. I think \"interactions\" and \"weights\" are twins from the build_interactions(), interactions is just giving 1's if at the same location in weights matrix a nonzero value (e.g. a 0 to 5 rating) is provide. When you train the model, only one, either interactions or weights, not both of them, is gonna fed in, so why do you need to split both of them at the same time?. @maciejkula \nI guess I do not clarify my point well or maybe I indeed lose some important points. But thank you so much for your immediate reply.\nFrom the Dataset.build_interactions(), it codes like this\ninteractions.append(user_idx, item_idx, 1)\nweights.append(user_idx, item_idx, weight)\nSo my understanding is that \"interactions matrix\" is basically a 0/1 binary version of \"weights matrix\" (of course, only if you provide the weight otherwise the code assigns a default 1). The weight I think could be an explicit rating, like in movie case from 0 to 5 star. So for example, if a user has rated movie 2 (from the set move1, move2, move3) as star 5 and leave other 2 movies no score, interaction could be denoted as [0, 1, 0] and weight as [0, 5, 0]\nMy point is when I look at the LightFM.fit() function, the fitting target is either \"interaction matrix\" or \"weight matrix\", the former is kind of binary interaction indicator while the latter provides explicit rating. When reading your comment \"Additionally, when fitting a model, you always need to supply the interactions, and also sometimes weights (but you can never just use weights)\", I am a little confused, do you mean we can use both interactions and weights  at the same time?. @maciejkula \nI am half way to be clear.\nI guess I missed \"sample_weight\" argument previously, basically sample_weight should get the weights matrix that Dataset has built?\nLightFm.fit(interactions, user_features=None, item_features=None, sample_weight=None)\nBTW, if for a data the rating (say 0 to 5 or non-rating) or number_of_visits or whatever scores provide more than a binary yes/no interaction. I want to take advantage of this information. Shall I put them to weight matrix (directly or maybe take a square root or something). My unclear is that for an implicit recommendation, should we forget about the explicit score?. @maciejkula \nI think I finally get the point. I guess if I want to maintain for example the user rating, I should make it happen in interactions rather than weights. I think it would be wise to keep all interactions equally important for most studies, in this case I can just let fit() to take a None for sample_weight. @maciejkula \nThe last question. So you mean any explicit scoring (for example explicit 0 to 5 score) should go to the weight matrix? How about in this case if I make the weight matrix default, but instead for the interaction matrix I make these scoring explicit 0 to 5 instead of binary 0/1? Is there any disadvantage to swap these two matrices for this particular example, although the direct element-wise product of these two matrices is obviously maintained? If the weight matrix only serves as the sample weight, I do not see any difference.\nSo essentially for this question, does weight has some specialty in the algo that we need let interaction matrix as binary indicator and weight will get further processing? Sorry I am not quite familiar with for example an implicit recommender system. Hi Sir,\nI tried word embedding, it speeds up the training significantly. By using the same learning rate, to converge to the same in sample and out of sample benchmark, word embedding one takes about only 1/4 or 1/3 training epochs compared with that w/o providing features. However, the final result as long as it converges looks similar. I think lightfm indeed performs well to learn embedding itself. All these make sense to me.\nBTW, I have a question, does lightfm have an api (or we can easily get it) for this task which I think is natural? Given a user that is involved in the training (basically the model has his/her latent embedding), if we have a new item that is not seen by the model before, but we know its features just as those in the training data (for example, I have its word embedding that is trained in the same way as other items), how can I predict the affinity score of this item to this user? For an already-seen item, we can use model.predict(uid, item_id) since both are registered. For this particular problem,  we have user_latent_embedding and of course we can derive item_latent_embedding, my question is: is the \"score\" in the lightfm just the inner product between user and item embeddings? Does lightfm have some convenient way to accomplish this task?. cool, thank you. @maciejkula I tried and actually find a problem. Because the model has a default identity feature column for each item, when a new item is introduced,  the model does not have the corresponding identity latent embedding for the new item, and it will break. Do you have any idea to get around this issue? Not using the identity feature?. ",
    "foreveryang0208": "@tianpunchh  Hi,\nThanks for sharing. May I ask once you generated the item-embeddings  say with dimension [N_i * D] Where  N_i stands for the number of items, D stands for embed size, which parameter do you assign this embedding matrix to? Is it \"item_features\"? or just initialise \"self.item_embedding\" with this matrix and keep item_features as None? \nI'm a bit confused because the \"item_features\" parameter expects dimension [N_i * F_i] where F_i is the number of features. If as you said, treating [0.3, 0.5, -0.1, -0.9] as four features with weights 0.3, 0.5, -0.1 and -0.9, then does it effectively mean that D becomes 1?\nMany Thanks\n. ",
    "TariqAHassan": "@maciejkula \nThanks for being open-minded here.\n\nThere already is a verbose option. This is simply an alternative way of realizing that functionality (with the benefit, critically, of an ETA provided by tqdm).\nI recognize that if you already have a system built using LightFM, this may not be of much utility. However, as you know, there can be a fair bit of trial and error involved in developing these kinds of systems, which can be quite time consuming with large datasets. A full-fledged progress bar makes it much easier to iterate through different approaches until a suitable solution is found.. \n",
    "Ivanclj": "I tried to use the predict method. But first it does not have an argument for passing in train_interaction. Then I tried : predict(user_ids=0, item_ids=item_ids) where item_ids are remaining item_ids after removing all item_ids that have interacted with user 0 in train. However, the results did not match with the precision at K score from the evaluation module. So I was wondering what should I do here.. i have the same issue. Where i had AUC around 0.84 for CF but only 0.44 for Hybrid. Is there a way we can check how good the item features are?. > Great answers, @DoronGi and @SimonCW.\n\n@Ivanclj you could try one of the following things:\n\nUse city names instead of coordinates.\nCompute geohashes (tuning the granularity) from your location data.\n\n\nThank you all so much for answering! So does it mean Lightfm works best with only categorical variables? Does it calculate similarities (cosine or pearson) anywhere internally when doing recommendations that works well with both numerical value features and one-hot encoded features? . ",
    "loloCFC": "I kept tweaking the learning rate and feature weights but also to no avail. 0.00001 for learning rate and 1e-6 for item features and 1e-2 for user features. These are not the only values I have tried though but most are relatively close to these values. Also removing either of the features still results in nan AUC. Funny thing is that when I only include only the item features the predictions are almost the same for all users but if I include both feature matrices then the predictions vary.. do I need to have a test set? I mean I know I should but I tried to look at the stackexchange example and it was able to get an auc on the training set alone. I am trying to do the same thing but am getting nan values instead. This was the part before it had side information. food_auc = auc_score(model, data['interactions'], item_features=data['item_features'], user_features=data['user_features'], num_threads=2)\ninteractions here is assumed to be my training set. ",
    "0x5eba": "Yes, This was my attempt\n```python\ndataset = Dataset()\ndataset.fit(ratings['userid'], ratings['movieid'], user_features=set((users['age'][i[0]-1], users['gender'][i[0]-1]) for i in ratings.values), item_features=set(movies['genre']))\nuser_features = [(i[0], [users['age'][i[0]-1], users['gender'][i[0]-1]]) for i in ratings.values]\ndataset.build_item_features(user_features)\n``\nbut doesn't work.\nI think I fail to fid user_features into the dataset.. When I dodataset.build_item_features(user_features)it says that can not find the feature 'F' (from users['gender']).\nThe error is:\"Feature {} not in eature mapping. \" \"Call fit first.\".format(feature)\nValueError: Feature F not in eature mapping. Call fit first.. Yes, is insideusers['gender'][i[0]-1]. Every row ofusers['gender']contain 'M' or 'F'.. I think that's the problem.\nHow should i put both (users['age']andusers['gender']) inside thedataset.fit(user_features=)?. I found the problem. \nI was usingdataset.build_item_featuresinstead ofdataset.build_user_features`, problem solved.\nThanks for your help.. ",
    "williamcao-01": "@0x5eba Hi, i am using the same dataset but couldnt fit the user features. Tried using your code too. Experiencing the exact same problem as  Feature F not in eature mapping. Call fit first.  How did you solve it?. can someone help with this question? I am having the same problem too, what happens if the user is new. I tried build the user feature with the new ID, it gives the error Number of user feature rows does not equal the number of users @maciejkula . ",
    "ahayamb": "Thanks for the answers. \nI'm sorry for not stating my point clearer before. I've used lightfm to do prediction and it is fast enough. I have approximately 3gig model and serve it using uwsgi since some of python web frameworks doesn't support multi threaded request. If I spawn 4 uwsgi processes, it will consume approx. 12gig of RAM. I've tried to take out model's parameter and put it to postgres, but prediction time is increasing.\nIs there any strategies to overcome memory issue but still getting an acceptable prediction time? I'm sorry this is so demanding \ud83d\ude02 . > Since these are plain numpy arrays, you can keep them on disk and memmap them into the memory of each of your individual processes.\nGreat!!! Sounds interesting, I will try. Thanks for the suggestion \ud83c\udf8a \n. ",
    "2mrdaya": "Hi@maciejkula, \nThanks for your reply.\nI have just copied the code from my notebook. My problem is slightly different. Actually there is a new user 4 with user feature user_city='Z'. this user is not present in the entire dataset (not in interaction and not in the feature dataset).  What values should i pass in the given lines of codes\n at the place of \"?\" mark to recommend news. Should i need to write a function to find the user with similar feature first? \nuser_features_test = dataset.build_user_features(((x['user_id'], [x['user_city']]) for x in [{\"user_id\":\"?\",\"user_city\":\"Z\"}]))\nscores = model.predict(?, np.arange(num_items), user_features=user_features_test)`\nCan you provide a simple example with simple small dataset similar like my dataset. that can explain user/item cold star problem seperately?. ",
    "hugos0910": "Thanks for the response Maciej, I watched the talk and read the paper referenced.\nI read on the documentation that LightFM can also handle explicit data.  To do so we just need to choose a suitable loss function?  I will be using implicit feedback for the project I am working on, but just curious about it.\nAlso, if we scaled the interaction matrix for implicit feedback, will LightFM treat all non-zero values as indicators for interaction and values close to 1 for \"stronger\" interaction?\nThanks. @maciejkula  I followed the paper and used item_embeddings, user_embeddings, item_biases, and user_biases to calculate the prediction score, and was able to do so for the matrix factorization model (without using user and item metadata).  However, if I put in user and item metadata the dot product of the matrices no longer produce the score (incorrect dimensions).  Thanks in advance. . @eggie5 I understand now, thanks!. @maciejkula Thank you, is there any normalization I need to do to the user-user_feature matrix or using it one-hot-encoded is just fine?. Does that mean if user[0] has an interaction in the evaluation set but not the training set, this becomes a cold-start problem?\n. How many interactions do you have per user?  If the number of interaction is low and you set K to a high number, the precision @ K can be low.. @moggeandersson How did you use the fit/fit_partial for multiple features?  I tried:\npython\ndataset.fit_partial(items=df['name'], item_features=((tag[0], tag[1], tag[2]) for tag in df['tags']))\nThis encodes (tag[0], tag[1], tag[3]) as one feature instead of individual features. @maciejkula I have constructed my dataframe the following way:\n| name        | item           | ethnicity  | gender  | features |\n|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| Jo      | apple | Asian | Female | [Asian, Female] |\n| Juan      | banana |   Hispanic | Male | [Hispanic, Male] |\n| Juan | orange      |    Hispanic | Male | [Hispanic, Male] |\nI've constructed the dataset with:\npython\nds = Dataset()\nds.fit(users=sample_df['name'], items=sample_df['item'])\nI tried doing:\npython\nds.fit_partial(users=sample_df['name'], user_features=sample_df['features'])\nbut got an error of unhashable type: 'list'.  Appreciate the help!. @maciejkula Tried and got it to work.  I understand now, thank you!. @hammadkhann One method is to use scipy.sparse.hstack, not sure if there's a better way.. I'm late, @olddaos beat me to it!  But yeah, @hammadkhann that's the way to go. @DoronGi Thanks for the help, I just implemented your suggestion and I was able to build the item feature matrix.  However, I am still unable to get the item_feature_mapping.  item_id_mapping and item_feature_mapping are still returning the same dictionary.. I was skimming through the dictionaries, not realizing there are extra entries in item_feature_mapping, careless mistake on my part!  Thanks for the help @DoronGi , I will be closing the thread now.. Here's a good link that discusses precision & recall@k:\nhttps://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54\nFor precision, take the number of recommended items that are positive interactions in the test set and divide by the number of recommended items@k.. The max precision that can be obtained depends on the number of interactions in the test set and the k you choose.  Assume that every user in your test data has exactly 5 interactions, and you want to do a precision@10, the max precision you can obtained would be 5 (assume you hit 5 out of 5) / 10, which is 0.5.  If you change the k to 5, the max precision you can get would be 5 / 5, which is 1.0.  If each user in the test set has different number of interactions, the max precision you can obtain for each user is different.\n@maciejkula Please correct me if I'm wrong, I am still pretty new to building recommender systems.. @DoronGi For the assumed identity matrix, isn't it just a n_user x n_user one-hot-encoded- matrix, how does that differentiate the users when predicting?  . @DoronGi So from what I understand, if we train without supplyinguser_features, the user_id we give when making prediction will refer to the rows in the embedding matrix (obtained from training).  If we supply user_features when we trained the model, the user_id we give when prediction refers to the user_features matrix supplied upon prediction.  Am I on the right track?. ",
    "nocedan": "Its more common having two machines. One worker suited for training which will have more memory and another one just to serve the API. Their communications can be made with a DB.. I think it depends on your api architecture. I have implemented this kind of filtering this way having filter parameters in memory by user key. The latency was not a problem. IF it is in your case i think you shoud have some kind of cache for recommendations. Have you measured filtering time?. This seams fine in my opinion. What about the filtering time alone?. Well IF your worried you can make tests with a simulated scenario. About the predictions i dont know, have you tried the documentation?. @fischjer4 , i'm glad as well that you brought this to light. In such cases, for new users its better to treat the problem as cold start, as in the examples of the documentation. But unfortunately, after a while, when new user has recorded behavior, it seams that the SGD that you have already ran is \"lost\".. ",
    "fischjer4": "@RothNRK In the example you provided above an error is thrown ValueError: The item feature matrix specifies more features than there are estimated feature embeddings: 2 vs 3.  I stumbled upon this thread after getting this same error in my own project. Any idea how to solve this? The error is raised in the fir_partial call.. @RothNRK Whoops, a mistake on my end. Sorry for the inconvenience. \nAs a cautionary note to new users, when using model.fit_partial the users/items/features in the supplied matrices must have been present during the initial training. Meaning, if you want to add new users/items/features, you must retrain the model. Check here. ```\ndef similar_items(internal_idx, item_feats_mtx, model, N=10):\n    item_representations = item_feats_mtx.dot(model.item_embeddings)\n# Cosine similarity\nscores = item_representations.dot(item_representations[internal_idx, :])\nitem_norms = np.linalg.norm(item_representations, axis=1)\nscores /= item_norms\n\nbest = np.argpartition(scores, -N)[-N:]\nreturn sorted(zip(best, scores[best] / item_norms[internal_idx]),\n              key=lambda x: -x[1])\n\n```\nHi @hammadkhann, I found this on the internet and it works great for me. internal_idx is the item's index in the matrix which you can get from dataset.mapping()[2][<your item ID>].  Hopefully, someone with more experience will comment on what exactly is happening behind the scene because I am still a little bit confused as well. But, I wanted to post this to at least get you started.. @maciejkula Just to clarify, you're suggesting to simply replace the first line \nitem_representations = item_feats_mtx.dot(model.item_embeddings) \nwith \nitem_representations = model.get_item_representations()[1] correct?. @hammadkhann Based off of your question and answer from #343  my guess is that you would filter the item_representations to only include the representations of the items which meet your criteria.\nassume indices_that_meet is an array of item indices that meet your criteria.\nfiltered_item_reps = item_representations[indices_that_meet, ]\nand then use filtered_item_reps instead of item_representations in thesimilar_items function. @hammadkhann Maybe something like this? \n```\ndef find_filtered_item_idxs(item_feat_mtx, filtered_item_feat_idxs):\n    i_mtx = item_feat_mtx.tocoo()\n    indices_that_meet = []\n    for item_idx, feat_idx in itertools.izip(i_mtx.row, i_mtx.col):\n        if feat_idx in filtered_item_feat_idxs:\n            indices_that_meet.append(item_idx)\n    return indices_that_meet\nfilters = [ \"city_newyork\", \"city_portland\", \"country_usa\" ]\nitem_feat_to_internal = dataset.mapping()[3]\nfiltered_item_feat_idxs = [ item_feat_to_internal[filter] for filter in filters ]\nindices_that_meet = find_filtered_item_idxs(item_feat_mtx, filtered_item_feat_idxs)\n```\ni_mtx is of the form (item index, item feature index) so you can iterate through it and where the feature matches one of your filters add the item index.. @hammadkhann regarding the filters, the filters is a list of the feature labels that you used when fitting your dataset (dataset.fit(..., item_features=items_feat_labels, ...)). So if the label was simply \"Karachi\" and \"Pakistan\", then your filters = [\"Karachi\", \"Pakistan\"]. @hammadkhann You should only get two in filtered_item_feat_idxs if you only supply it two filters. What filtered_item_feat_idxs represents is the mapping from the external feature name to the internal index that that item feature is in the matrix. The matrix has no clue what \"Pakistan\" is, so you map \"Pakistan\" to its index in the matrix, then use that index to find items that meet by calling find_filtered_item_idxs \nMake sense?. @hammadkhann Are [\"Karachi\",\"Pakistan\"] the actual feature labels that you used to fit the dataset? What does filtered_item_feat_idxs contain, are they legit indices? When you built your item_feat_mtx did you pass it your item features, specifically were their items that had \"Karachi\" and \"Pakistan\" as features in your call to dataset.build_item_features(). The way to do this is to remember that \n...user and item representations are expressed in terms of representations of their features: an embedding is estimated for every feature, and these features are then summed together to arrive at representations for users and items...\nSo, one can simply add the modeled embeddings for those features together and then dot that with the item_representations.\n```\ndef cold_start_similar_items(feat_idxs, item_feat_mtx, model, N=10):\n    i_biases, item_representations = model.get_item_representations(features=item_feat_mtx)\nsummation = 0\nfor idx in feat_idxs:\n    summation += (model.item_embeddings[idx] + model.item_biases[idx])\n\nscores = item_representations.dot(summation)\nitem_norms = np.linalg.norm(item_representations, axis=1)\nitem_vec_norm = np.linalg.norm(summation)\nscores = np.squeeze(scores / item_norms / item_vec_norm)\n\nbest = np.argsort(-scores)[0 : num_res]\nreturn sorted(zip(best, scores[best]), key=lambda x: -x[1])\n\n```. @maciejkula Great! Thank's for the help!. ",
    "arundasK": "Thanks maciejkula. random seed also will be parameter that need to be fine tuned. Random seed can take any value, so how to find the value which give best recommendation ? Thanks in advance  . @maciejkula  I highly appreciate your reply. Thanks for your time and effort.. ",
    "hodaraad": "Yes, my question was about deployment and prediction only. I went over your paper (http://ceur-ws.org/Vol-1448/paper4.pdf) after posting this question and almost came into the same conclusion as what @eggie5 posted. So I should export \"self.user_embeddings\", \"self.item_embeddings\", \"self.item_biases\" and \"self.user_biases\" and use them as in equation 1 of the paper, right?\nThe only thing is I checked \"predict\" method of LightFM class to have a sense of how things should be calculated on our production side, and it seems it calls \"predict_lightfm\" method from here looked https://github.com/lyst/lightfm/blob/master/lightfm/_lightfm_fast.pyx.template. In that method,  I don't see these embedding and bias variables of the lightfm object to be used in calculating the predictions/scores for test items. Instead 4 set of other variables (\"lightfm.user_features\", \"lightfm.user_features\", \"lightfm.user_biases\", \"lightfm.item_biases\") are used to get representation of new test items. Are these the same as the earlier variables I mentioned? cause LightFM class does not seem to have any instance variables with the name \"user_features\" for instance. What am I missing?\nThanks. ",
    "letotefrank": "0 index in numpy means user1,so it's right.\nIn my consequence : result=-3.568,but predict(1,[1])=17.025.\nI think that'sshould be equal in your paper with LightFM.. user_biases = model.get_user_representations()[0]\nuser_embeddings = model.get_user_representations()[1]\nitem_biases = model.get_item_representations()[0]\nitem_embeddings = model.get_item_representations()[1]\nuser1 = user_embeddings[1]\nitem1 = item_embeddings[1]\nscore = u1.dot(i1.T)\nub1 = user_biases[1]\nib1 = item_biases[1]\nresult = score + ub1 + ib1\nresult=-3.221\uff0cpredict(1,[1])=17.025\nThe above is the result of the calculation.\nMaybe it's my expression is not clear, I calculated it according to what you said.\nIs this method of fitting described in your paper? I don't know if my understanding is wrong. Thank you very much for your reply.\n. not at all @maciejkula . ",
    "FankLi": "\nNot all of the values match exactly. How could that be?\n```python\nfind the best model\nmodel_bpr_ult = model_bpr[np.argmax(bpr_map_test)]\nreformulate the interaction matrix.\nuser_biases = model_bpr_ult.get_user_representations()[0]\nuser_embeddings = model_bpr_ult.get_user_representations()[1]\nitem_biases = model_bpr_ult.get_item_representations()[0]\nitem_embeddings = model_bpr_ult.get_item_representations()[1]\nuser1 = user_embeddings[0]\nitem1 = item_embeddings\nscore = np.matmul(user1, item1.transpose())\nub1 = user_biases[0]\nib1 = item_biases\ntest on user_1\nresult = score + ub1 + ib1\npredict = model_bpr_ult.predict(0, list(range(len(item1))))\nCounter(np.argsort(result.round(3)) == np.argsort(predict.round(3)))\n```\n\nCounter({False: 340, True: 13201})\n\n\nfor example: There are 943's user and 1682'2 item in movielens100k\uff0c  model.predict\uff08943,[1]\uff09 or model.predict\uff081,[1682]\uff09 will error. So, model.predict\uff08942,[1]\uff09 is user943 to item0. . ",
    "DoronGi": "I don't think that this is due to empty rows. Unless you specified in your call to the evaluation function preserve_rows=True, the computation will skip empty rows, and the score will we computed only for rows with interactions. See here:\nhttps://github.com/lyst/lightfm/blob/3e42e3e74ea5eb9e39be2a571a214a15f411a667/lightfm/evaluation.py#L81\nSince you have only one interaction for most users, the maximal score you can achieve is ~0.1 (assuming you use default k=10). A random choice in this case will give ~0.0084, so 0.03 is not that terrible.\nEither way, depending on your usage of the recommendation results, you can try either continue using precision_at_k with lower k (as low as 1), or try using recall_at_k or reciprocal_rank. Your model is using user and item features. The evaluation functions need these arguments. Try:\ntrain_precision = precision_at_k(model, train, k=10, user_features=uf, item_features=itemf).mean(). There is no problem to do so. You build one dataset to which you fit all your users, user-features, items and item-features by calling fit and fit_partial. Similarly you call build_user_features and build_item_features to build the feature matrices for all users and items.  Next you call build_interactions twice, once with the interactions of the first user group (1-11 months) to get the test interaction matrix, and the second time with the interactions of the second user group (12th month) to get the test matrix.. I can understand why you would want to avoid method1 (it wouldn't only slow down computation; unless you have plenty of users in each city, your model won't be able to learn significant representation for most city features) , but I don't think method2 makes any sense. As far as I understand LightFM will construct the representation for each user as a linear combination of its features representations, and you suggest to use a different weight for the user_city for each city. If you arbitrarily assign numbers to cities I can't see how that can help your model understand the user better.\nYou could however try different approaches:\n\n\nDefine few city features which you think are relevant to you problem (e.g. big_metropolis, small_village, coastal etc.) and map each city to the corresponding feature (one hot encoded).\n\n\nDefine even fewer city features (e.g. big_city) and map each city to a weight of the relevant features, such that New York would get a higher big_city weight than Seattle.\n\n\nKeep in mind that the model can only be as good as the data you provide. . @maciejkula wouldn't you at least suggest to remove cities with very few users/interaction? I found it to improve predictions for my model (trained with little data).. I found this helpful #210.\nBasically in the hybrid model each user is represented by an identity feature, together with all other relevant features for that user. The latent representation is then just the sum of all the feature's representations. For a new user you don't have an identity feature, but everything else is exactly the same.\nYou can create a new user_features sparse matrix of size (1, num_of_features) for the new user, and use it in predict. In that case use user_id=0 when calling the function to indicate that you want to use the first (and only) row in your user_features matrix.. I believe that with this library you would have to retrain, or at least continue training (fit_partial) from your previous model (provided that you pre-prepared additional self-features for the new users).\nWithout additional training all you can do is to generate recommendations based on the user features, or recommend items similar to the items the user has interacted with. . Since you don't feed your model with any real interaction data, all it can learn is the tags co-occurrences (meaning if all airplanes are also tagged as aircrafts, we can expect similarity between these tags embeddings). There is the question of why would you use such a complicate method to find this similarity, instead of simply counting the number of times each tag-pair occurs together or something of that sort.\nAnyway, I think what you suggest can potentially work, and suggest you will try the following:\n\n\nMake sure your item feature matrix doesn't include an identity part (meaning every item is described only by its features, without an identity feature). Your call when constructing the dataset should be as follows:\nlightfm.data.Dataset(item_identity_features=False)\nWithout it the model could reach perfect fit just by using the identity feature to match between all user-item pairs.\n\n\nPlay with your model item_alpha, specifically try to increase it (default is 0). This will prevent over-fitting your interaction matrix which is very sparse. I would try values between 0.1 to 0.001, but that is just a guess.\n\n\nGenerate a bigger interaction matrix. As long as you generate it randomly, I don't think there is any problem in having as many users as you like, and having each user interact with several items. This should again help with over-fitting. \n\n\nTry reducing the number of components in your model, again to avoid overfit. \n\n\nBut again, if it is only for the purpose of experimenting and education, go ahead. Otherwise I suggest you will try a different approach. When providing the model with real data, you get significant tag similarity. You may get that golden-retriever is similar to Labrador since users that were interested in one were also interested in the other. In your can you will be lucky to get that both are similar to dogs (if items tagged with Labrador are also with dogs) and extremely luck to get similarity between the breeds.. Glad to help. And thank you for this great package! So well written and documented, a real pleasure!\n. When using item features, your call to model.get_item_representations should include the feature matrix as input: get_item_representations(features=item_features), see documentation:\nParameters: features\u00a0(np.float32 csr_matrix of shape\u00a0[n_items,\u00a0n_item_features],\u00a0optional) \u2013 \nEach row contains that item\u2019s weights over features. An identity matrix will be used\nif not supplied.\nIn your code, assuming you used lightfm.data.Dataset to arrange your data, you would generate similarities based only on the identity feature of each items (since they are the first ones created as can be seen here : https://github.com/lyst/lightfm/blob/3e42e3e74ea5eb9e39be2a571a214a15f411a667/lightfm/data.py#L207\n), which is OK to do, but doesn't represent the items fully (as they were trained).. 1. I guess no. You should provide an iterable of strings. Decide if your features are ['automobile', '336x280', 'image', 'food', '250x250', 'text', '200x200',...] or ['automobile+336x280+image', 'food+250x250+text', 'food+200x200+text',...]. \n2. Your format seems OK to me if you choose the first option from 1. \n3. Your example is using one-hot form. The alternative would be to have a feature 'ad_size' and give it a value which corresponds to the ad area for example..\n. According to the documentation the call for dataset.build_item_features should have a data parameter in the form of (item id, [list of feature names]) or (item id, {feature name: feature weight}). Even if each user has only one feature, you still need to provide it in list form. Try:\ndataset.build_item_features(((x, [y]) for x, y in zip(df['item'], df['item_feature'])))\nThis should solve both your problems.\n. Are you sure these are identical dictionaries? They should start the same, both having the item_ids as first values, but in the item_feature_mapping they should be followed by your features. Check to see if they are the same size.. The template file is indeed the one you edit. There is an explanation at the bottom of the project page:\n\nDevelopment\nPull requests are welcome. To install for development:\nClone the repository: git clone git@github.com:lyst/lightfm.git\nInstall it for development using pip: cd lightfm && pip install -e .\nYou can run tests by running python setup.py test.\nLightFM uses black (version 18.6b4) to enforce code formatting.\nWhen making changes to the .pyx extension files, you'll need to run python setup.py cythonize in order to produce the extension .c files before running pip install -e ... In cases where user_features isn't provided, an identity matrix is assumed. Each user has its own identity feature in that case, and this is what differentiates between them. If you have trained your model with user_features, you need to user those in predict too.\n\nAs for your follow-up questions, users are described only by their features + bias. Since user bias have no effect on the ordering of the predictions, you should get the same predictions for users who share features. Do note that this applies only if they share all features, so if each has its own identity features the predictions would be different.. The feature matrix in this case is indeed n_user x n_user identity matrix (one-hot-encoded), but each of these n_user features has its own 1 x no_components embedding, which is trained to predict the positive items to the corresponding user.. You do not need to split items_features to train and test. Split only interactions (and interaction weight if you are using it). item_features describes all the items, and doesn't contain interaction data.. I believe LightFM let you choose between having all you features as on-hot encoded, or as continuous variables. The latter is achieved by passing a dictionary {feature name: feature weight} instead of a list of features per user. you can use it for the coordinates (after normalization), and weight 1 for one-hot encoded features, but I would be surprised if it would benefit your model. It makes more sense to try and create new features from these coordinates, according of what your model is trying to predict.. ",
    "FrancescoI": "Hi again,\nI found that I was making a mistake with the .build_item_features() method.\nNow I'm getting items similarity (by calculating the cosine distance of products in the embedding space) that are really similar between the two different flows described above.\nStill they don't exactly match, even if I'm setting the same seed during the fitting process, but I guess it has something to do with the users and products indexes (which don't match between flows).. Thanks, I'm closing it!. Thanks @maciejkula .\n. ",
    "nishal999": "Ahh yes. Now I get it. Its solved now. Thank you so much!!!. I am trying to validate my model with respect to train and test data. For example, I want to include only interactions for 11 months as my training set and want to test it on userids  for the 12th month. The problem here is, I cannot have different dimensionalities for train and test as mentioned by @maciejkula  here. I have lost direction as to how I should be proceeding further.\n. Thank you @maciejkula and @DoronGi . ",
    "astrung": "From what you recommended, it may be like as dimension reduction - replace user_city one hot encoded vector ( huge dimensions ) into another small dimension vector ? \nIf i do that, it is like as i need to implement content based filtering from scratch :(  . Well, does anyone has other ideals ?. Thank everyone for help, but i still have 2 questions:\n\n\nWhy one docs, it said that we can use feature weight for every items. But if we use different weights of a feature for every row, can it build and predict correctly ? \n\n\nHow can we handle  and fetch continuous features for lightfm model ?. @maciejkula Thank you for your responses.\n1.please correct me if my approach is correct for continuous features.\nIf we have a dataset like this:\nuser_features = [{\"user_id\":\"1\",\"age\":13},\n{\"user_id\":\"2\",\"age\":23},\n{\"user_id\":\"3\",\"age\":30}]\n\n\nShould i fetch into a model like that ? :\ndataset.fit( ..., user_features=['age'])\nuser_features = dataset.build_user_features( [('1', { 'age':13 }), ('2', {'age': 23}), ('3', {'age': 30}) ] )\n\n\nCan i use this approach for a continuous feature with very big values like money or income ? ( from 0$ to 1.000.000$ ) ?. I am still waiting for responses. For current models,  I convert all continuous features into categorial features and fetch it.. \n",
    "kj-lai": "hi, any update on this issue? I'm also wondering about how to handle such features. In my case, i have a item feature which value ranges from 0 to 10k and each row are non-zero values, which approach I should use?. The explanation is still unclear to me in finding the MAXimum value of precision and recall. How do I know how well the recommender performs with the precision and recall value if i don't know the maximum value?\nI hope there would be a section in showing this example some day in the tutorials.. @hugos0910, thank you for your explanation.. Noted, thanks for the reply. Apologies for the trouble.\nAfter reading the documentation more thoroughly, i found out my mistake. Here is the solution for my problem.\nUsers\u2019 and items\u2019 latent representations are expressed in terms of their features\u2019 representations. If no feature matrices are provided to the lightfm.LightFM.fit() or lightfm.LightFM.predict() methods, they are implicitly assumed to be identity matrices: that is, each user and item are characterised by one feature that is unique to that user (or item). In this case, LightFM reduces to a traditional collaborative filtering matrix factorization method.\nso the -1s score refers to CF whereas -226s score refers to hybrid method.. Noted, thank you for your explanation.. ",
    "dimitris93": "I'm not sure that you understood the question. Maybe I did not explain it well enough. Let's assume I have:\nusers\ntracks\n\"like\" user-track interactions in a sparse matrix \nusers' (age, gender) information in a matrix of dimension: [num_users x 2]\nLet's say that I have already trained model. Now a new user comes in the system. He is a 25 year old male. He likes 5 songs. How to recommend n=50 songs to this new user?. ",
    "raf-1": "@maciejkula i have the same issue as @hugos0910 above. appreciate if you can offer some insight?. ",
    "moggeandersson": "Thanks a lot, that gave me great insight! . ",
    "DPGrev": "Thank you, this made the issue clear.. @NegatioN It is because your branch is behind the lyst/lightfm master branch. \nIn 90904560973f3869c668f4269a9b6d58261e0f3b sklearn got updated. Which means that RandomizedSearchCV is now part of sklearn.model_selection. This is not the case for your submitted pull request since you are behind on the current master.. Hi Jayp496,\nAre you using the random_train_test_split function for splitting you train and test set?\nIf so, are you sure you are feeding in the item and user features when evaluating the model like so:\npython\nprecision_at_k(model,\n               test,\n               item_features=item_features,\n               user_features=user_features,\n               k=10)\nOtherwise LightFM will give the error  you are reporting.\n. When you run your code. Could you check your memory usage. It could be that you are running out of memory.. - Do you run it in the same pip/conda environment?\n- If you try a sample of the dataset does the same error appear?. Hi amw5g,\nI think this will answers your questions: Lightfm in production. ",
    "olddaos": "I believe something like  scipy.sparse.csr_matrix(np.eye(your_data.shape)) will create the required sparse identity matrix for you.. Yeah, like this:\n```\nNeed to hstack item_features\neye = sp.eye(item_features.shape[0], item_features.shape[0]).tocsr()\nitem_features_concat = sp.hstack((eye, item_features))\nitem_features_concat = item_features_concat.tocsr().astype(np.float32)\n```. ",
    "50stars": "\nThe Dataset class does this automatically. Have a look at the class docstring.\n\nHey,\nI am trying to use the Dataset class for multiple features (in the example it's only for a single one)\ncould you help me with an snippet example of fit and build_item_features?. ",
    "rth": "\nBased on the principle that pickling is dangerous and inefficient, it may be useful to have something like this.\n\nLoading pickled models from unknown sources should never happen in practice I think? Also note that numpy.load (used in this PR) has allow_pickle=True by default, so that is not really safer than regular pickling. The inefficient part is not true if one uses joblib.dump / load (e.g. used in scikit-learn).\nOverall, I'm not saying that is PR is not useful (thanks for working on it @NegatioN), just wanted to add more context. In the long term, ONNX export (https://github.com/onnx/onnx) might be a more production ready and language agnostic solution.. > I do believe the numpy save/load functionality here would work with allow_pickle=False too though, as we're only really persisting floats and strings.\nDefinitely.\nFor ONNX they don't support sparse arrays yet, I think, so it's more a long term solution that could be investigated in the future... https://github.com/maciejkula/spotlight has a similar implementation, as far as I understand, based on PyTorch (that supports GPU).. Approximate. the docs\nAdd a section on what dependencies are needed to run this notebook (annoy, nmslib  with a link to the corresponding websites). Import not needed, %timeit is built in in jupyter notebook. py\n_, item_embeddings  = model.get_item_representations(movielens['item_features'])\nto be consistent with the docs, adapt variable name in the following. Maybe reformulate \"item vectors\" in terms of \"item embeddings\". f is not a great name for a shape (I know it's used in the annoy readme), maybe use instead,\npy\nn_item_features, n_components = item_vectors.shape\n...\nthen the comment won't be needed.. It doesn't look like this saved index is actually used in the example. To say that the performance is good at this point, we would need to compare with the default nearest neighbours implementation (e.g pairwise_distances). \nMake a new section (with a header) starting at \"Now we'll do an Annoy example\", use links in the rst to link to the PDFs. Use a meaningful name for the index object, e.g. annoy_index. Also make it consistent with the nnms index object.. Variable names need to be meaningful and consistent with what came before. lower case function names see PEP8. \"item_vecotrs\" -> \"item_vectors\". Could try to use numpy.broadcast_to (see this SO anwser). It's not clear what's happening here. Shouldn't x be stored as the same time as the timing is computed?. You should explicitly pass nms_idx, item_vectors as inputs of this function, otherwise the nms_index that gets used is the one from several cells before not the one defined ~20 lines below.\nMaybe also generally re-factor this whole section, to have setup functions for each algorithm,  make response_times = {'annoy': [], 'nms': []..} use a for loop over method name, and compute the timing in this second inner loop.. ",
    "cyteen": "My mistake was inputing item features in Dataset in the form of tuples by tuples instead of all features in 1 long list. \nIt works now. \nThank you @DoronGi . ",
    "OktayGardener": "@hammadkhann Homeboy why are you tagging me I\u2019ve just submitted one PR \ud83d\ude02. You should explain whatever you\u2019re trying to do a bit (when I say a bit I mean a lot) more clearly and give us information about your environment. We can\u2019t really help or deduce anything from the information you\u2019ve provided. What dataset are you using? What version? Maybe provide us with a notebook of your code? Like bruh. . ",
    "bonobo": "Thank you for quick response!\nJust to be sure, the correct \"shape\" of features passed to build_item_features/build_user_features should be (I think you forgot to put features into list in your response):\n[\n    (item1, ['price:1', 'accept_credit_cards:False', 'smoking_allowed:True', 'category:bar']),\n    (item2, ['price:4', 'accept_credit_cards:True', 'smoking_allowed:False', 'category:restaurant']),\n]\nHave a nice day!. ",
    "Sammckay12": "Thanks a lot @maciejkula ! Much appreciated.. ",
    "marielen": "Hi!\nWhen I have user_feature not built by Dataset class, do I need to put together with the identity matrix?\nThanks.. ",
    "Chirag-Amin": "You don't have to stack the identity matrix as @maciejkula mentioned, if you are not interested in the latent vectors for your user_features then you can forgo this step.  \nIf you are interested in creating the latent vector without using the Dataset class then have a look at this blog: https://www.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/. Although I would suggest using the Dataset class.. @maciejkula Deviating slightly from the original question, but in your original paper, you mention using a bag-of-words representation on the CrossValidated dataset (tags + about). How were the bag-of-word representation vectors included in the model? \nI found the following example but I couldn't find the (tag + about) example. https://github.com/lyst/lightfm/blob/master/examples/stackexchange/hybrid_crossvalidated.ipynb. ",
    "g-eorge": "Thanks for the insight!. ",
    "SimonCW": "@g-eorge may I ask whether you have experimented with the inverse propensity weighting and what your experiences are?. Hi, \nif you don't need to live-update your recommendations (e.g. based on clickstream) you could pre-compute the Top-K recommended items per user on a daily basis and cache these per user recommendations. This has the additional benefit that you can easily try out different algorithms, save/cache the results to different locations and A/B test on them (Leemay Nassery talks about this in this excellent podcast: twimlai)\nI'm interested what others think about this!\nIf you want higher frequency updates:\nAfaik other alternatives are approximate nearest neighbour search Benfred's Blog or pre-computing item factors (they shouldn't change much over shorter time periods) and only update user factors. But I don't have any practical experience with these. \nMaybe this helps.. Does your item_features matrix (that you provide to the fit function) include an identity matrix of shape (number_items x number_items)? If not, it might well be that your model is less expressive than pure collaborative filtering. I would recommend to build your item features with the build_item_features method from the Dataset class (http://lyst.github.io/lightfm/docs/lightfm.data.html) where the default option is to include the item identity matrix.. Hi @Ivanclj,\n\nIs this right that for numerical values you still take each unique value and make a feature for it? And do lightfm try to do one-hot encoding for these numerical values in this case?\n\nI am pretty sure this is how it works and LightFM \"one-hot encodes\" these unique long-lat pairs. \nYou could either bin your values or come up with sensible features (country, region, tropics, subtropics, etc).. Hi @sbarbone, \ntry adding your features to the Dataset.fit() call! See the method description here dataset.fit and this example.. ",
    "petrus-jvrensburg": "Thanks @maciejkula, that should all be resolved now. I'll split these types of changes into smaller chunks in future.. Aah, that's a mistake. Thanks, let me fix that.. Yes, it's just for fixing the mismatch between lines 60 and 68. The comment talks about training 10 epochs, but the code runs 30.. Same as above: 30 vs 10 epochs. ",
    "Jayp496": "Hi DPGrev,\nI am not using the random_train_test_split because I have chronological transaction data which I don't want to break. Therefore, I build interaction matrices for training and testing separately with respect to the dates.\nBut you were right, I missed to specify item_features and user_features in the precision_at_k command! Now it works. Thank you very much!. ",
    "SanjiAp": "\nWhen you run your code. Could you check your memory usage. It could be that you are running out of memory.\n\nThis code is running on the another machine with same configuration. So if I am correct, memory error should not occur. . ",
    "amw5g": "Thanks DPGrev, it sort of does.  I'd started down that thread before, but perhaps I didn't read it as well or gave up too soon.  It sounds like fit_partial() may be the way to go.  If I understood it (and clearly I should just try it out), when an existing user has updated interactions, fit_partial() is appropriate.\nCheers!. ",
    "lampda": "I found the reason for the issue above.\nuid=0 is of integer type, then it will be converted to np.ndarray with the length of the number of total item_ids, say 733 here in my case.\nitem_ids is also converted to np.array if it is of type list or tuple, for example the item_ids=array([1,200,2883,123,99,124,...]).\nSince there is NO user_features NOR item_features, the model trained will issue model._construct_features_matrices with inputs n_user, n_items, and internally call user_features =sp.identity(n_users, dtype=CYTHON_DTYPE, format=\"csr\") and item_features = sp.identity(n_items, dtype=CYTHON_DTYPE, format=\"csr\").\nThe problem is here: n_users = user_ids.max()+1 and n_items = item_ids.max()+1 from model.predict(....) before they are passed to model._construct_features_matrices.\nWhy is this problematic? user_ids and item_ids we passed to the function model.predict() are the (array of) raw ids, while _construct_features_matrices will use internal ids converted from model.predict.\nUsers like me are NOT aware of this setting will confront the issue above. Here, I passed user_id=0 and item_ids=array([1,200,2883,123,99,124,...]) with the length of 733 and the max item_id 2883. Then mode._construct_features_matrices constructed a wrong * item_features*.\nOne need to use some kind of internal ids, rather than the raw ids. But how to do this mapping?. @maciejkula  So, why the 'model.predict' will use the max values? why not just use the length of list/array/tuple of ids?\nI suggest there is an improvement or hot fix for this:\nn_users = len(user_ids) and n_items = len(item_ids)\nInstead n_users = user_ids.max()+1 and n_items = item_ids.max()+1 in line 807 and line 808 of the lightfm.py.\nAnd there maybe other files with this logic to be modified accordingly.\n. Thanks for the point.\nbtw, during this time, I managed to figure out how to map my data with internal ids. However, I think it is better to use your idea anyway.\nsolved.. ",
    "jeydn": "Ah yes that's true. Sorry I was a little confused for a second. Everything looks fine. Thanks.\nNon-zero elements: interactions 100836, train 80668, test 20168. After checking the documentation again I think I'm running into issues of the train/test split. So that might cause the different values of train and test. . ",
    "nazmiasri95": "Thanks for the responses, yesterday I tried to study the code and found out the interactions actually can be stored with lesser size, as what @EthanRosenthal mentioned. I avoided storing the data in dense format, instead I store it in key value paired in NoSQL db server. \nI needed it to be real time/ live update recommendations, therefore I will store all the clickstream and update users' rating in the nosql, and retrieved it back when user is coming. \nThanks @SimonCW for your suggestions. I think that's a really brilliant idea ! I should consider to test using ANN (Approximate Nearest Neighbour) as well and see which one is better. . I'm not sure whether they have plan for GPU based implementation or not but I suggest that you only take top 1 million active users (for example) instead of taking all users. You might want to reduce it since you have resources limitation. \nBut if it can support GPU, for sure it will help to accelerate training a lot. :D Cheers. I think that it is normal, even for my use case, CF model outperform the hybrid.\nMy reasons are the metadata might become noise to the model and since I'm using item description as item metadata, I can see that the item description make the model confused based on my observation on the results. . ",
    "jithurjacob": "I'm also looking for something similar. I have the images of the products. I wanted to know how to add latent vectors from Auto encoders as item features.. ",
    "pigooosuke": "I tried to add those dense values. But, it failed.\nValidation score was down in my case.\nIf you use word2vec, its dimension size will be about 200-300 and values are dense, so it spends a lot of time to train. Therefore, I used word count instead of word vector. It works a little in my case.. ",
    "rahmanidashti": "I changed the lightfm code to push the pre-train user or item embeddings. To this purpose, you can change the lightfm.py and load your pre-train in embedings. I will tell the details on my github. But, I think you want to consider it as the features and I think it will be different.. https://github.com/rahmanidashti/pretrain-lightfm. > You can always set the embedding and bias attributes on the model instance to your pre-trained embeddings.\n@maciejkula So, could you please let's know how we can do it in the code? I checked the code, and I found you have just to user_feature and item_feature to pass the content information. . ",
    "mikkelam": "Hi I am using the the random_train_test_split function given from the library. Simply call it twice for interactions and weights.\nThis would be nice if the _shuffle function was reproducible, alas it is not:\n```\n\n\n\na = np.arange(10)\nrs = np.random.RandomState(3)\nrs.shuffle(a)\na\narray([5, 4, 1, 2, 9, 6, 7, 0, 3, 8])\na = np.arange(10)\narray([5, 4, 1, 2, 9, 6, 7, 0, 3, 8])\n\n\n\nok cool\n\n\n\na = np.arange(10)\nrs.shuffle(a)\na\narray([3, 8, 2, 1, 9, 4, 0, 6, 7, 5])\n\n\n\nwhat?\n```\nto fix this, change the _shuffle function like so:\n```\nimport numpy as np\nimport scipy.sparse as sp\ndef _shuffle(uids, iids, data, random_seed):\n    import copy\n    shuffle_indices = np.arange(len(uids))\ncopy.copy(random_seed).shuffle(shuffle_indices)\n\nreturn (uids[shuffle_indices],\n        iids[shuffle_indices],\n        data[shuffle_indices])\n\n```\nthen\n```\nfrom unittest import mock\nrs = np.random.RandomState(3)\nwith mock.patch('lightfm.cross_validation._shuffle', _shuffle):\n    train, test  = random_train_test_split(interactions, test_percentage=0.2, random_state=rs)\n    train_weights, test_weights  = random_train_test_split(weights, test_percentage=0.2, random_state=rs)\n\n\ntrain.row, train_weights.row\n(array([57440, 43122, 12378, ..., 60084, 16424, 10415], dtype=int32),\n array([57440, 43122, 12378, ..., 60084, 16424, 10415], dtype=int32))\n\n\nYay!\n```. @EthanRosenthal you're a smart man... ..That absolutely makes sense, thanks! Also think you for your great project good sir. ",
    "tomjwalker": "Great suggestions, thanks guys!. ",
    "SethiPawandeep": "It was just a silly mistake. Was supposed to be datasets and not dataset.\n. ",
    "mattflo": "Done! https://github.com/lyst/lightfm/pull/423. ",
    "impaktor": "There is a limit to how well any recommendation system can do, based on the data. If there's almost no interactions, especially if each user only interacted with one item, rather than several different items, then it will naturally be difficult to build a good recommender system. \nI would try to add more features that can allow the model to \"link\" what users have in common between user-item interactions and feature. E.g. geography (city, country), interest/political affiliation, twitter-verified, etc. You could also do text embedding on the twitter user's profile description text, and use that as feature.. Adding more features / \"metadata\" should help if the metadata is meaningful, i.e. actually has a pattern that helps your predictions (e.g. male/female, age, etc.). But adding more features will give you more parameters, so training will take longer, and depending on the amount  of data you have to train on, you risk overfitting on the training set, giving poor performance on the test set.\nI see LightFM looks to have regularization parameters, but these are by default 0. Maybe something to play with if you're in the overfitting-domain.. ",
    "ajhepburn": "@impaktor Yeah, I think my project is more focused around alleviating cold-start issues and proving empirically that adding metadata, external knowledge, embeddings etc can increase various evaluation scores. Am I right in thinking that regardless of data availability, it's good to have some element of collaborative filtering? I wasn't sure if the limited interactions were somehow negatively impacting the results. I've been struggling to find quality content-based approaches.. @maciejkula My data is specifically in this format: StockTwits: Message JSON Format\nWith regard to your question about what relationship I would be interested in, I've been tasked to recommend other tweets based on similar content/similar 'type' of stocks based on stock 'cashtags' or maybe content embeddings, for now I'm playing around with what information could potentially be useful. I'm already using the \n\"symbols\": [{\n    \"id\": 686,\n    \"symbol\": \"ABC\",\n    \"title\": \"Alpha Bravo Charlie, inc.\",\n    \"is_following\": false,\n    \"exchange\": \"NASDAQ\",\n    \"sector\": \"Technology\",\n    \"industry\": \"Personal Computers\",\n    \"logo_url\": \"http://logos.xignite.com/NASDAQGS/00011843.gif\",\n    \"trending\": true,\n    \"trending_score\": 16.4019,\n    \"watchlist_count\": 12370\n}],\nto recommend items in say Technology, like above or even more specifically, Personal Computers.\nFor now I've used user's location which has been identified row-by-row with spaCy's Named Entity Recognition then converted to say 'newyork' for instance. Item features are a tweet's cashtags (symbol), the sector and industry they belong to but I'm getting really bad results on my test sets.\n2019-02-28 18:53:31,849 [MainThread  ] [INFO ]  The dataset has 176 users and 92363 items with 18473 interactions in the test and 73890 interactions in the training set.\n2019-02-28 18:53:31,849 [MainThread  ] [INFO ]  Begin fitting collaborative filtering model...\n2019-02-28 18:53:34,157 [MainThread  ] [INFO ]  Collaborative Filtering training set AUC: 0.9582749\n2019-02-28 18:53:35,507 [MainThread  ] [INFO ]  Collaborative Filtering test set AUC: 0.28770456\n2019-02-28 18:53:35,507 [MainThread  ] [INFO ]  There are 92 distinct user locations, 9 distinct sectors, 215 distinct industries and 3929 distinct cashtags.\n2019-02-28 18:53:35,508 [MainThread  ] [INFO ]  Begin fitting hybrid model...\n2019-02-28 18:53:38,968 [MainThread  ] [INFO ]  Hybrid training set AUC: 0.8867248\n2019-02-28 18:53:40,068 [MainThread  ] [INFO ]  Hybrid test set AUC: 0.80986875\n2019-02-28 18:53:43,067 [MainThread  ] [INFO ]  Hybrid training set Precision@10: 0.27272728\n2019-02-28 18:53:44,150 [MainThread  ] [INFO ]  Hybrid test set Precision@10: 0.0011363636\n2019-02-28 18:53:47,148 [MainThread  ] [INFO ]  Hybrid training set Recall@10: 0.009839114903936408\n2019-02-28 18:53:48,183 [MainThread  ] [INFO ]  Hybrid test set Recall@10: 0.00015057677451455357\n2019-02-28 18:53:48,184 [MainThread  ] [INFO ]  Hybrid training set F1 Score: 0.018993023190626002\n2019-02-28 18:53:48,184 [MainThread  ] [INFO ]  Hybrid test set F1 Score: 0.0002659174721051054\n2019-02-28 18:53:51,185 [MainThread  ] [INFO ]  Hybrid training set MRR: 0.33839628\n2019-02-28 18:53:52,304 [MainThread  ] [INFO ]  Hybrid test set MRR: 0.004157745\nI've got a feeling I'm maybe overfitting with the 3929 cashtags and was thinking of just using the sector, industry tags and just making the tweet bodies into an embedding using something like word2vec/doc2vec? Maybe also using the 'trending score' as a feature weight for each industry per tweet as well?\nAny pointers would be greatly appreciated.. @maciejkula I've decided to make Doc2Vec representations of each tweet body and take out the cashtags. Is there a way to pass pre-trained embeddings to the model? \nI saw that:\nitem_embeddings: np.float32 array of shape [n_item_features, n_components]\n         Contains the estimated latent vectors for item features. The [i, j]-th\n         entry gives the value of the j-th component for the i-th item feature.\n         In the simplest case where the item feature matrix is an identity\n         matrix, the i-th row will represent the i-th item latent vector.\nBut I was a little confused. Would I just train the model as normal without passing any item_features then set the item_embeddings property to an np matrix that I make myself?\nWhat I wanted to know was, if I was just to use an embedding for each tweet body but retain industry + sector information as separate features to be passed in, would this be possible? . @nazmiasri95 I'd understand if my parameters could add noise but things like sector and industry for each item are small categories (9 and 210 respectively), I thought if anything that would help group similar hashtags together (which are my items). ie, Google and Facebook both being \"Internet Services\" or something similar. \nI get at least some results for almost purely content-based systems which shows its helpful for cold-start users but I thought hybrid would at least always complement a model given the features were not too granular.. ",
    "robertddewey": "Thank you so much for the feedback! Very helpful!. ",
    "inpefess": "A scenario which I encountered myself in a couple of projects and which several people I consulted also had is the following:\n there are not too many items (all item_embeddings can fit the memory of one server)\n prediction is done by batches of users\n* recommendations for one batch of users and all items fit into the memory of one server\nIn this scenario, one needs a function computing recommendations for a batch of users.\nI agree that this case is far from being universal, but it could be useful for some people.. ",
    "frbattid": "I was wondering if my sparse matrices were too much sparse... Nevertheless, I've tried with extremely very little shapes, and the same error arises:\n```\n\n\n\nimport scipy.sparse as sp\nimport numpy as np\nimport lightfm\nuf_row = np.array([2,4,9])\nuf_col = np.array([4,9,3])\nuf_data = np.array([1,1,1])\nif_row = np.array([0,3])\nif_col = np.array([9,7])\nif_data = np.array([1,1])\ni_row = np.array([1])\ni_col = np.array([8])\ni_data = np.array([1])\nuf_csr = sp.csr_matrix((uf_data, (uf_row, uf_col)), shape=(10, 10))\nif_csr = sp.csr_matrix((if_data, (if_row, if_col)), shape=(10, 10))\ni_csr = sp.csr_matrix((i_data, (i_row, i_col)), shape=(10, 10))\nmodel = lightfm.LightFM(loss='warp')\nmodel.fit(i_csr.tocoo(), user_features=uf_csr, item_features=if_csr)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib64/python3.6/site-packages/lightfm/lightfm.py\", line 479, in fit\n    verbose=verbose)\n  File \"/usr/lib64/python3.6/site-packages/lightfm/lightfm.py\", line 578, in fit_partial\n    self._check_finite()\n  File \"/usr/lib64/python3.6/site-packages/lightfm/lightfm.py\", line 413, in _check_finite\n    raise ValueError(\"Not all estimated parameters are finite,\"\nValueError: Not all estimated parameters are finite, your model may have diverged. Try decreasing the learning rate or normalising feature values and sample weights\n```\n\n\n\nDefinitely, I'm doing something wrong.... I think I found the problem... I did the following experiment:\n```\n\n\n\nuf_csr = sp.csr_matrix((np.array([1]),(np.array([0]), np.array([0]))),shape=(20,20))\nif_csr = sp.csr_matrix((np.array([1]),(np.array([0]), np.array([0]))),shape=(20,20))\ni_csr = sp.csr_matrix((np.array([1]),(np.array([1]), np.array([1]))),shape=(20,20))\nmodel = lightfm.LightFM(loss='warp')\nmodel.fit(i_csr.tocoo(), user_features=uf_csr, item_features=if_csr, epochs=20)\nTraceback (most recent call last):\n  ...\nValueError: Not all estimated parameters are finite, your model may have diverged. Try decreasing the learning rate or normalising feature values and sample weights\n```\n\n\n\nI.e. I had the Exception as usual. Now, if you observe the interaction matrix, it has an interaction regarding a user and an item having all their features set to 0 in the user and item feature matrices, respectively. So, let's change this, in the user features matrix, for instance:\n```\n\n\n\nuf_csr = sp.csr_matrix((np.array([1,1]),(np.array([0,1]), np.array([0,0]))),shape=(20,20))\nmodel.fit(i_csr.tocoo(), user_features=uf_csr, item_features=if_csr, epochs=20)\n\n```\n\n\n\nEt voil\u00e0!\nWe can do the same with the item features matrix:\n```\n\n\n\nuf_csr = sp.csr_matrix((np.array([1]),(np.array([0]), np.array([0]))),shape=(20,20))\nif_csr = sp.csr_matrix((np.array([1,1]),(np.array([0,1]), np.array([0,0]))),shape=(20,20))\nmodel.fit(i_csr.tocoo(), user_features=uf_csr, item_features=if_csr, epochs=20)\n\n```\n\n\n\nSo, I'll try to find a way of filtering interactions related to all-zero user and item features and I'll post it ;). Happy to help, @maciejkula . That additional suggestion in the error message sounds great!\nRegarding the solution, my first thought was, as said, to remove interactions related to these users and items. But then I thought that could affect somehow the recommendation to those users, or the recommendation of those items.\nThus, a different solution could be to expand the user and item features matrices with a diagonal matrix, in order at least such a feature (the user him/herself) is set to 1.\n0 0 0        0 0 0 1 0 0\n0 1 0   -->  0 1 0 0 1 0\n1 0 0        1 0 0 0 0 1\nI've tested it, and it does the trick ;). ",
    "benfred": "Maybe try without the {'post':2} option for NMSLib - it roughly doubles the time to create a nmslib index. It does improve the recall rate, but its still possible to get a good QPS at over a 99% recall without it.   I have some breakdowns comparing the times with and without it here: https://github.com/benfred/bens-blog-code/tree/master/approximate_als.\nAlso I think you might want to note that Annoy is a pretty good choice for calculating similar items, but for some reason it isn't a great choice for recommending items after making that transform. I benchmarked this using Annoy/NMSLib/Faiss for recommending items here: http://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/  - and its not clear that Annoy is better than just using a brute force solution for recommending items.. "
}
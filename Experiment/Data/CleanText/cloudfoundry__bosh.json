{
    "vmware-ac-gerrit": "Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nComplete our CLA Agreement for\n   individuals or\n   corporations.\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n5. Install our gerrit-cli gem:\ngem install gerrit-cli\n6. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n7. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nComplete our CLA Agreement for\n   individuals or\n   corporations.\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n5. Install our gerrit-cli gem:\ngem install gerrit-cli\n6. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n7. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n4. Install our gerrit-cli gem:\ngem install gerrit-cli\n5. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n6. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n4. Install our gerrit-cli gem:\ngem install gerrit-cli\n5. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n6. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n4. Install our gerrit-cli gem:\ngem install gerrit-cli\n5. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n6. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. Dear Cloud Foundry contributor,\n\nIf you are reading this message, it means you submitted a pull request in the\nCloud Foundry GitHub repository.\n\n\nFirst of all, thanks! We really appreciate your participation.\n\n\nRecently we made some changes in how we are verifying and reviewing open source\ncontributions like yours. In addition, we changed the way we can expose our\ninternal development in real-time. The changes are exciting, as they allow all\nour staff to collaborate seamlessly with you, which increases our mutual\nvelocity and gives the community a bigger stake in our direction.\n\n\nThe Cloud Foundry team uses Gerrit, a code review tool that originated in the\nAndroid Open Source Project. We also use GitHub as an official mirror, though\nall pull requests are accepted via Gerrit.\n\n\nFollow these steps to make a contribution to any of our open source\nrepositories:\n\n\nSign up for an account on our public Gerrit server at\n   http://reviews.cloudfoundry.org/.\nCreate and upload your public SSH key in your Gerrit account profile.\nSet your name and email:\n\ngit config --global user.name \"Firstname Lastname\"\n           git config --global user.email \"your_email@youremail.com\"\n4. Install our gerrit-cli gem:\ngem install gerrit-cli\n5. Clone the Cloud Foundry repo:\n   Note: to clone the BOSH repo, or the Documentation repo, replace\n   vcap with bosh or oss-docs\ngerrit clone ssh://reviews.cloudfoundry.org:29418/vcap\n           cd vcap\n6. Make your changes, commit, and push to gerrit:\ngit commit\n           gerrit push\n\nOnce your commits are approved by our Continuous Integration Bot (CI Bot) as\nwell as our engineering staff, return to the Gerrit interface and MERGE your\nchanges. The merge will be replicated to GitHub automatically at\nhttps://github.com/cloudfoundry. If you get feedback on your submission, we\nrecommend squashing your commit with the original change-id. See the squashing\nsection here for more details: https://help.github.com/rebase.\n\n. ",
    "PharkMillups": "Really? I need a CLA in place just to make a 5 line, non-code change to a README? \n. I took the lazy road and filed an issue for this: https://github.com/cloudfoundry/bosh/issues/2\nI understand VMware needs a CLA, but you'll probably get a lot more people filing issues for small fixes as opposed to actually fixing them with this in place. I'm not sure there's much you can do about it as CLAs tend to be all or nothing, but it's unfortunate. \nAnyways, looking forward to watching BOSH grow. Looks like a promising chunk of code. \n. ",
    "mkocher": "Can you rebase this and not add to the gemfile?\nWe've reorganized the gemfiles and gemspec today, so there may be a little shuffling when you rebase.\n. Thanks!\n. Can we get to a working gem by branching off the commit that we built 1.4.0 off of and changing the gemspec?\n. How long is this normally taking for you when it works?  I assume 19 minutes is much longer than normal?\n. Assignment returns the value assigned, so this changes the behavior if settings['env']['bosh'] is nil.\nI find checking the result of an assignment to be rediculous though, so I'd love to see this updated to be if settings['a'] && settings['a']['b']\n. Yup, that's exactly what I was suggesting.\n. Ideally we'd have a \"global package cache\" that was keyed on stemcell as well as the code SHA.  This is a large piece of work, but something that would improve the bosh user experience immensely.\n. Hmm, are you using a mixture of old and new style properties?\n. We're working on releasing a new set of gems. In the mean time, bundler is much better than rubygems at resolving a dependency tree, so if you create a Gemfile and run bundle I bet it'll work.\n. I'd love to just be able to tell the director to download a stemcell.  All the downloading and uploading drives me nuts.\n. This probably isn't a high priority at the moment for us, as ec2 light stemcells make the download-upload dance painless.  On vSphere we'll be packaging up the stemcell with the release, so it'll already be on the disk. \nSeems like it would make a good pull request though.\n. Have you tried this?  Light stemcells only work in us-east, so I'm not sure if this is the only change necessary.\n. I don't see why not.  It'd certainly be a nice touch.\n. Why not set ssh gateway when targeting a director? bosh ssh is something an operator does all the time, and the gateway is pretty much always the same.\nI think the wider goal here should be to eliminate the need for all the bosh_ssh scripts that various teams/people use.\n. Sweet!\n. The bosh core team is not maintaining the vCloud CPI at this point, it's moved over to https://github.com/vchs/bosh_vcloud_cpi, I'm going to close this but you're welcome to open an issue over there and point to this one.\nThis discussion sounds like it might also get better results on the bosh-users mailing list.\n. The intention was it only ignores uuid's of \"ignore\", is that not what's happening?\nThe use of this is for things like targeting a bosh-lite director with the warden cpi.  We tend to rebuild the vm all the time, and the manifests are distinctly for the warden cpi.\n. I see @pmenglund's point - if I switched from targeting my warden-cpi local vm to production, I could happily deploy production with the warden-cpi manifest.\nIt seems to me that a manifest should have a \"deployment guid\" that identifies any deployments of that manifest.  This would enable you to easily use the same manifest for many deployments, but prevent you from overwriting deployments when the name string happens to match.\n. At the moment all directors have a uuid, and it seems like it could be useful to be able to identify them.  We could make it a property \"enforce uuid\" and make your proposal work.\nI think switching the uuid to be fore the deployment instead of the director makes the most sense in the long run.\n. :thumbsup: \n/cc @goehmen \n. The postgres error is certainly a bosh error, not a bosh-lite problem.  To route around the problem we've turned off the resurector in newer builds of bosh-lite.  This means the cck is necessary after a reboot, which is too bad. \n@jbayer & @tsaleh - can we prioritize this as a bug to investigate?\n. @d can you expand on that thought? It seems like there's a working order (remove static IP from old instance, assign to new instance) and a non-working order (assign to new instance, remove static IP from old instance). Since update order is (possibly) up to the user, there's certainly one case where an error should be raised.\nThere's an important different between hard to implement and impossible to implement, and I may be wrong but I think you meant hard but said impossible.\n. Sorry @tammer, we're looking for @tsaleh.\n. Ping @goehmen.\n. Accidental close there, reopened.\n. Accidental close there, reopened.\n. Just to second @cppforlife, we do need to do it right or not do it at all. Otherwise we build up technical and/or product debt.\n. Just to second @cppforlife, we do need to do it right or not do it at all. Otherwise we build up technical and/or product debt.\n. :thumbsup: \n. :thumbsup: \n. Is this version of monit agpl?\n. Is this version of monit agpl?\n. ",
    "rajdeepd": "Closing this pull request. Will submit a new one\n. ",
    "kinsersh": "Created this pull request to continue the conversation started from gerrit  (http://reviews.cloudfoundry.org/#/c/13445/).\nI'm looking into Nicholas Kushmerick's comment about trying the approach they used for vcap-services-sample-release. My preliminary guess is that vcap-services-sample-release relies on the same blobstore as cf-release, so was probably simpler from that standpoint, but it does show a job in one release that is able to rely on packages in a different release. I don't know yet how it works, but that looks like it from a superficial glance.\nI get it, Martin, that you don't like like the composite blobstore approach because it creates dependencies in a separate release on blobs in the blobstore used by cf-release. Building that separate release would break if blobs were ever cleaned up in the cf-release blobstore. You don't want to break others' releases nor do you want your hands tied in not being able to clean up your own blobstore.\nI was looking to a composite blobstore approach because the actual act of cloning blobs looks really complicated (http://bit.ly/V52gBq). Syncing blobs in that fashion, in my opinion, relies on implementation details of how bosh_cli operates and would likely need to change if bosh were refactored.\nOne thought I'm having is around the way Nexus works as a maven repository. It can mirror upstream repositories such that each artifact that's needed is fetched from the upstream server and stored locally such that further requests are resolved locally. Even if the upstream repository went offline or failed to serve up that artifact, it still serves it locally. \nWe could write a blobstore server that acts in that same fashion that can mirror an upstream blobstore. We could simulate that on the client-side by, on a NotFound on the primary blobstore, fetching from the upstream blobstore, then storing it in the primary blobstore using the same object_id. I'd probably call it a MirroringBlobstoreClient. Thoughts on this last approach?\nI'll investigate the approach used with vcap-services-sample-release and see if I have any other ideas.\nThanks,\nStephen\n. Created this pull request to continue the conversation started from gerrit  (http://reviews.cloudfoundry.org/#/c/13445/).\nI'm looking into Nicholas Kushmerick's comment about trying the approach they used for vcap-services-sample-release. My preliminary guess is that vcap-services-sample-release relies on the same blobstore as cf-release, so was probably simpler from that standpoint, but it does show a job in one release that is able to rely on packages in a different release. I don't know yet how it works, but that looks like it from a superficial glance.\nI get it, Martin, that you don't like like the composite blobstore approach because it creates dependencies in a separate release on blobs in the blobstore used by cf-release. Building that separate release would break if blobs were ever cleaned up in the cf-release blobstore. You don't want to break others' releases nor do you want your hands tied in not being able to clean up your own blobstore.\nI was looking to a composite blobstore approach because the actual act of cloning blobs looks really complicated (http://bit.ly/V52gBq). Syncing blobs in that fashion, in my opinion, relies on implementation details of how bosh_cli operates and would likely need to change if bosh were refactored.\nOne thought I'm having is around the way Nexus works as a maven repository. It can mirror upstream repositories such that each artifact that's needed is fetched from the upstream server and stored locally such that further requests are resolved locally. Even if the upstream repository went offline or failed to serve up that artifact, it still serves it locally. \nWe could write a blobstore server that acts in that same fashion that can mirror an upstream blobstore. We could simulate that on the client-side by, on a NotFound on the primary blobstore, fetching from the upstream blobstore, then storing it in the primary blobstore using the same object_id. I'd probably call it a MirroringBlobstoreClient. Thoughts on this last approach?\nI'll investigate the approach used with vcap-services-sample-release and see if I have any other ideas.\nThanks,\nStephen\n. ",
    "mheath": "If we were only syncing the contents of the blobs/ folder, syncing blobs would be easy but because .final_builds/ is full of blobs, it greatly complicates the matter. If there's a tool for doing this that I've overlooked, please tell me. Right now we've cleared out .final_builds and set our releases back to version 1.\nUnfortunately for us, our approach for forking cf-release is not very ideal. It was working fine for a while until I decided to merge in cf-release changes after the 127 release came out and now I'm in merge hell resolving all the conflicts between our .final_builds that uses our custom blob store and changes coming down in cf-release. I'm going to be mildly grumpy if this is what I have to deal with every time there's a CF release.\nChaining blobstores would fix most of this for us because we're not changing  many of the jobs. Sure we would have to do some merging every release but not as much as I've done today.\n. What do you mean by \"old and new style properties\"? I'm deploying ccng as is in cf-release.\n. ",
    "nterry": "You don't need to clear out .final_builds. Just clear out final_releases to reset your version, and you wont have as much of a merge nightmare...\n. Added incorrect author email to one commit, closing.\n. ",
    "drnic": "I do love the colors in bosh :) :rainbow: \n. Who is @cf-frameworks? Can we have real human names in commits & comments please?\n. This bug appeared here https://github.com/StarkAndWayne/bosh-bootstrap/issues/57 first.\n. Bump. Currently bosh_deployer gem can still not be deployed from rubygems.\n. Please deploy a new bosh_deployer gem only as a patch of currently released gem as an immediate fix; and also fix it in HEAD.\n. Guys; it's solved when you upload a patched deployer gem (say 1.4.1 or\n1.4.0.1) with this fix for 1.4.0. It's not really \"fixed\" with a commit to\nmaster/HEAD.\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. There is a workaround - install httparty gem before installing\nbosh_deployer.\nOn Wed, Feb 13, 2013 at 10:02 PM, Matthew Kocher\nnotifications@github.comwrote:\n\nCan we get to a working gem by branching off the commit that we built\n1.4.0 off of and changing the gemspec?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/19#issuecomment-13534474.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. How does a warden CPI know what set of physical machines or VMs to use to run warden containers?\n. Which part of the manifest says which machine it is; or is it the local\nmachine?\nAre the stemcell info used? (Since its containers not guest VMs)\n\nDr Nic Williams\nhttp://drnicwilliams.com\ncell +1 (415) 860-2185\n. So I would deploy a microbosh; then I need to deploy a bosh-release with\njust a director?\nHow do I build the warden-enabled stemcell?\n\nDr Nic Williams\nhttp://drnicwilliams.com\ncell +1 (415) 860-2185\n. From the logs:\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/bosh_aws_cpi-0.7.0/lib/cloud/aws/cloud.rb:373:in `block (2 levels) in delete_stemcell'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/data.rb:92:in `block in method_missing'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/data.rb:91:in `each'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/data.rb:91:in `method_missing'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/aws-sdk-1.7.1/lib/aws/core/data.rb:116:in `method_missing'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/bosh_aws_cpi-0.7.0/lib/cloud/aws/cloud.rb:372:in `block in delete_stemcell'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/bosh_common-0.5.4/lib/common/thread_formatter.rb:46:in `with_thread_name'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/bosh_aws_cpi-0.7.0/lib/cloud/aws/cloud.rb:368:in `delete_stemcell'\n/var/vcap/packages/director/bosh/director/lib/director/jobs/delete_stemcell.rb:43:in `block (2 levels) in perform'\n/var/vcap/packages/director/bosh/director/lib/director/event_log.rb:58:in `track'\n/var/vcap/packages/director/bosh/director/lib/director/jobs/delete_stemcell.rb:42:in `block in perform'\n/var/vcap/packages/director/bosh/director/lib/director/lock_helper.rb:35:in `block in with_stemcell_lock'\n/var/vcap/packages/director/bosh/director/lib/director/lock.rb:58:in `lock'\n/var/vcap/packages/director/bosh/director/lib/director/lock_helper.rb:34:in `with_stemcell_lock'\n/var/vcap/packages/director/bosh/director/lib/director/jobs/delete_stemcell.rb:24:in `perform'\n/var/vcap/packages/director/bosh/director/lib/director/job_runner.rb:98:in `perform_job'\n/var/vcap/packages/director/bosh/director/lib/director/job_runner.rb:29:in `block in run'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/bosh_common-0.5.4/lib/common/thread_formatter.rb:46:in `with_thread_name'\n/var/vcap/packages/director/bosh/director/lib/director/job_runner.rb:29:in `run'\n/var/vcap/packages/director/bosh/director/lib/director/jobs/base_job.rb:8:in `perform'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/resque-1.15.0/lib/resque/job.rb:127:in `perform'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/resque-1.15.0/lib/resque/worker.rb:163:in `perform'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/resque-1.15.0/lib/resque/worker.rb:130:in `block in work'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/resque-1.15.0/lib/resque/worker.rb:116:in `loop'\n/var/vcap/packages/director/bosh/director/vendor/bundle/ruby/1.9.1/gems/resque-1.15.0/lib/resque/worker.rb:116:in `work'\n/var/vcap/packages/director/bosh/director/bin/worker:77:in `<main>'\n. In bosh_aws_cpi-0.7.0 this maps to this method:\n``` ruby\n    # Delete a stemcell and the accompanying snapshots\n    # @param [String] stemcell_id EC2 AMI name of the stemcell to be deleted\n    def delete_stemcell(stemcell_id)\n      with_thread_name(\"delete_stemcell(#{stemcell_id})\") do\n        snapshots = []\n        image = @ec2.images[stemcell_id]\n    image.block_device_mappings.each do |device, map|\n      id = map.snapshot_id\n      if id\n        @logger.debug(\"queuing snapshot #{id} for deletion\")\n        snapshots << id\n      end\n    end\n\n    image.deregister\n\n    snapshots.each do |id|\n      @logger.info(\"cleaning up snapshot #{id}\")\n      snapshot = @ec2.snapshots[id]\n      snapshot.delete\n    end\n  end\nend\n\n```\nThis looks fixed on HEAD.\n. Going to upgrade microbosh to HEAD and see if I can delete the stemcell.\n. JINX!\n. Ahh, I was running bundle install --without test development before creating stemcells.\n. Can't run bundler either:\n$ bundle install\nUsing rake (10.0.3) \nUsing addressable (2.3.2) \nUsing httpclient (2.2.4) \nUsing yajl-ruby (1.1.0) \nUsing agent_client (1.5.0.pre) from source at agent_client \nUsing multi_json (1.1.0) \nUsing multi_xml (0.5.2) \nUsing httparty (0.10.2) \nUsing json (1.7.6) \nUsing nokogiri (1.5.6) \nUsing uuidtools (2.1.3) \nUsing aws-sdk (1.8.0) \nUsing bcrypt-ruby (3.0.1) \nUsing bosh_common (1.5.0.pre) from source at bosh_common \nUsing builder (3.1.4) \nUsing excon (0.16.10) \nUsing formatador (0.2.4) \nUsing mime-types (1.19) \nUsing net-ssh (2.6.3) \nUsing net-scp (1.0.4) \nUsing ruby-hmac (0.4.0) \nUsing fog (1.9.0) \nUsing log4r (1.1.10) \nUsing ruby-atmos-pure (1.0.5) \nUsing blobstore_client (1.5.0.pre) from source at blobstore_client \nUsing gibberish (1.2.0) \nUsing bosh_encryption (1.5.0.pre) from source at bosh_encryption \nUsing highline (1.6.15) \nUsing crack (0.3.2) \nUsing monit_api (1.5.0.pre) from source at monit_api \nUsing daemons (1.1.9) \nUsing eventmachine (0.12.10) \nUsing json_pure (1.7.6) \nUsing rack (1.5.1) \nUsing thin (1.5.0) \nUsing nats (0.4.28) \nUsing netaddr (1.5.0) \nUsing posix-spawn (0.3.6) \nUsing sigar (0.7.2) \nUsing tilt (1.3.3) \nUsing sinatra (1.2.8) \nUsing bosh_agent (1.5.0.pre) from source at bosh_agent \nErrno::EPERM: Operation not permitted - /var/vcap/store/repos/bosh/bosh_agent/bin/bosh_agent\nAn error occurred while installing bosh_agent (1.5.0.pre), and Bundler cannot continue.\nMake sure that `gem install bosh_agent -v '1.5.0.pre'` succeeds before bundling.\n. Ahh, I was the wrong user.\n. It does seem weird to require that rspec is installed in order to run \"rake stemcell:micro\" or \"rake -T\".\n. It's blowing up in this rake task in gem.rake:\nruby\n    task :gem_with_deps => 'all:prepare_all_gems' do\n      dirname = \"#{root}/release/src/bosh/#{component}\"\n      rm_rf dirname\n      mkdir_p dirname\n      Dir.chdir dirname do\n        Bundler::Resolver.resolve(\n            Bundler.definition.send(:expand_dependencies, Bundler.definition.dependencies.select{|d| d.name == component}),\n            Bundler.definition.index\n        ).each do |spec|\n          sh \"cp /tmp/all_the_gems/#{spec.name}-*.gem .\"\n          sh \"cp /tmp/all_the_gems/pg*.gem .\" if COMPONENTS_WITH_PG.include?(component)\n        end\n      end\n    end\n. I'm now seeing a similar error but different files that can't be found #44 \n. @rkoster discovered that the following gems need to be uninstalled before running the rake task:\nhttps://github.com/StarkAndWayne/bosh-bootstrap/commit/68b919288c67ffa4d6357cab0ce0162e0bdacf63\n. Very cool seeing the beginnings of RHEL/Centos support\n. Weird; almost looks like its trying to run stemcell-copy as a ruby script instead of sh\n. I reverted to the bosh_deployer gem and it follows a different flow on my 10.04 inception VM, because I get the following in my log:\nfalling back to using dd to copy stemcell\nWhich means it takes the 2nd path in this method https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L538-L539\nWhereas in the bug being raised, using HEAD, it follows the first path (unsuccessfully) https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L533-L536\n. What changed with has_stemcell_copy(path)?\n. AARGH, bosh repo has no tags for gem releases :/\n. There is no obvious recent change around these methods via git blame. Only validate_options & initial_agent_settings seem to have been changed in 2013 (in bosh_aws_cpi's cloud.rb) \n. Yes the root user has ruby in its path.\nvcap$ ruby -v\nruby 1.9.3p125 (2012-02-16 revision 34643) [x86_64-linux]\nvcap$ sudo ruby -v\nruby 1.9.3p125 (2012-02-16 revision 34643) [x86_64-linux]\n. password-less sudo sounds like an ok requirement; I'll test HEAD soon.\n. When I re-run bosh micro deploy ... I get this error:\nStopping agent services      |                   | 0/5 00:00:00  ETA: --:--:--Error: nil value given for persistent disk id\n. Was able to delete; so deploying again.\n. On the next deploy it passed this point.\nWhy does it happen?\n. When it worked correctly, the bosh logs still had the SystemCallError: #<Errno::ETIMEDOUT: Connection timed out - connect(2)> so that is unrelated.\n. Happened again today:\n```\nStemcell info\n\nName:    micro-bosh-stemcell\nVersion: 0.8.1\nDeploy Micro BOSH\n  unpacking stemcell (00:00:12)                                               \n  uploading stemcell (00:10:37)                                               \n  creating VM from ami-a21786cb (00:00:30)                                    \nWaiting for the agent        |ooooo              | 3/11 00:18:27  ETA: 00:23:08/usr/local/lib/ruby/gems/1.9.1/gems/agent_client-0.1.1/lib/agent_client/http_client.rb:46:in rescue in request': cannot access agent (Connection refused - connect(2) (http://54.235.133.196:6868)) (Bosh::Agent::Error)\n    from /usr/local/lib/ruby/gems/1.9.1/gems/agent_client-0.1.1/lib/agent_client/http_client.rb:29:inrequest'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/agent_client-0.1.1/lib/agent_client/http_client.rb:56:in post_json'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/agent_client-0.1.1/lib/agent_client/http_client.rb:23:inhandle_method'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/agent_client-0.1.1/lib/agent_client/base.rb:17:in method_missing'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:406:inblock in wait_until_agent_ready'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:394:in wait_until_ready'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:406:inwait_until_agent_ready'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager/aws.rb:122:in wait_until_agent_ready'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:145:inblock in create'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:84:in step'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:144:increate'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:104:in block in create_deployment'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:97:inwith_lifecycle'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/deployer/instance_manager.rb:103:in create_deployment'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.0/lib/bosh/cli/commands/micro.rb:171:inperform'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/command_handler.rb:57:in run'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/runner.rb:61:inrun'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/runner.rb:18:in run'\n    from /usr/local/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/bin/bosh:16:in'\n    from /usr/local/bin/bosh:23:in load'\n    from /usr/local/bin/bosh:23:in'\n       error  deploy micro bosh\n```\n. From memory it takes longer.\n. Ferdy, is there a nice test we could do upon an OpenStack API (in\nbosh-bootstrap) to check that they've setup OpenStack correctly? I think\nthis is the 2nd or 3rd of these errors?\nNic\n. I'm cloning all repos from scratch (via\nhttps://gist.github.com/drnic/4950698); will report back.\n. Yep, problem gone now.\n. Spoke too soon.\nmkdir -p /var/vcap/store/repos/bosh/release/src/bosh/bosh_aws_registry\ncp /tmp/all_the_gems/multi_json-*.gem .\ncp /tmp/all_the_gems/pg*.gem .\ncp /tmp/all_the_gems/multi_xml-*.gem .\ncp: cannot stat `/tmp/all_the_gems/multi_xml-*.gem': No such file or directory\nrake aborted!\nCommand failed with status (1): [cp /tmp/all_the_gems/multi_xml-*.gem ....]\n/var/vcap/store/repos/bosh/rake/gem.rake:52:in `block (5 levels) in <top (required)>'\n/var/vcap/store/repos/bosh/rake/gem.rake:48:in `block (4 levels) in <top (required)>'\n/var/vcap/store/repos/bosh/rake/gem.rake:47:in `chdir'\n/var/vcap/store/repos/bosh/rake/gem.rake:47:in `block (3 levels) in <top (required)>'\nTasks: TOP => stemcell:mcf => all:build_with_deps => bosh_aws_registry:build_with_deps => bosh_aws_registry:gem_with_deps\n. Also not working for rake stemcell:micro\n. It is a debian package of 1.9.3 that comes from apt.unboxedconsulting.com\nhttps://github.com/StarkAndWayne/bosh-bootstrap/blob/master/lib/bosh-bootstrap/stages/stage_prepare_inception_vm/install_ruby#L17-L27\n. How do I figure out what is included in that ruby?\n. Stemcell building used to work from the same VM before the rake file\nrefactoring; if that helps with debugging\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. @rkoster discovered that the following gems need to be uninstalled before running the rake task:\nhttps://github.com/StarkAndWayne/bosh-bootstrap/commit/68b919288c67ffa4d6357cab0ce0162e0bdacf63\n. Before running the rake task?\nAren't the bad gems coming from bosh_deployer gem being already installed?\nOn Saturday, February 16, 2013, Ruben Koster wrote:\n\nbundle install --deployment solves the problem\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/44#issuecomment-13655347.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Could this be done within package_compiler? Or could a bosh_agent be given multiple blobstores to search for packages?\n. If we did it in bosh_agent, then we'd need to allow for 2+ blobstores. Currently it has a flat pair of options https://github.com/cloudfoundry/bosh/blob/master/bosh_agent/bin/bosh_agent#L14-L15\nruby\n  \"blobstore_options\"  => {},\n  \"blobstore_provider\" => \"simple\",\nPerhaps we change this to:\nruby\n    \"blobstores\" => [{ \"options\" => {}, \"provider\" => \"simple\" }],\nThen allow for multiple blobstores to be specified when bosh_agent is run\n. --blobstore uri might need to include the provider; since we'd want to support different types of providers. Perhaps put the provider at the front of the uri\nruby\nbosh_agent --blobstore simple:///var/vcap/micro_bosh/data/cache --blobstore s3://....\nIt would try to get the compiled package from each blobstore, before reverting to the SLOOOW process of compiling each package. \n. This same concept of an agent looking in multiple blobstores for compiled packages could be used to radically improve the zero-to-launch time of new Cloud Foundry deployments; if a public blobstore was available with pre-compiled packages, that all cf-release agents pointed to.\n. There would shades of grey of \"exactly the same stemcell\"? Aren't all the stemcells built from the same basic stemcell?\n. There is a cloud.properties.aws.region field in my micro_cloud.yml:\nyaml\ncloud:\n  plugin: aws\n  properties:\n    aws:\n      access_key_id: XXX\n      secret_access_key: YYY\n      region: us-east-1\n      ec2_endpoint: ec2.us-east-1.amazonaws.com\n      default_security_groups:\n      - microbosh-aws-us-east-1\n      default_key_name: microbosh-aws-us-east-1\n      ec2_private_key: /home/vcap/.ssh/microbosh-aws-us-east-1.pem\n. It's a bug (or user error) if core cf-release components cannot\nbe collocated.\n. Perhaps in a wardenized world, services could be colocated - but the\nservices wouldn't know about it, so technically they aren't colocated (I'm\nalmost confusing myself).\nNic\nOn Wed, Apr 3, 2013 at 11:22 AM, Dr Nic Williams drnicwilliams@gmail.comwrote:\n\nFWIW bosh-cloudfoundry adheres to Nick's recommendation - service nodes\nare on dedicated VMs; and not on the core/0 or shared VMs with other\nservice nodes.\nNic\nOn Wed, Apr 3, 2013 at 11:19 AM, kushmerick notifications@github.comwrote:\n\nfor sure services jobs don't spec their properties today.\nit makes sense in smallish CFs to colocate service-gateways, so for sure\nall foobar_gateway jobs should be upgraded.\nbut IMHO it is a bad idea to colocate service nodes, except perhaps for\ntoy smoke-test scenarios. the colocated services will step all over each\nother. not in the sense of (eg) writing to the same files [ie, the reason\nyou can't colocate jobs that use vcap_registrar]. Rather, VMs can become\nfull because service nodes assume they are the only consumers of\ndisk/memory. One can argue we should remove this assumption [though we\nthought long & hard about how to achieve lights-out operation of each\nservice and I don't think see an alternative]. in the meantime, we don't\nneed to bother to upgrade any of the foobar_node_ng jobs.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/57#issuecomment-15854118\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. FWIW bosh-cloudfoundry adheres to Nick's recommendation - service nodes are\non dedicated VMs; and not on the core/0 or shared VMs with other service\nnodes.\nNic\nOn Wed, Apr 3, 2013 at 11:19 AM, kushmerick notifications@github.comwrote:\n\nfor sure services jobs don't spec their properties today.\nit makes sense in smallish CFs to colocate service-gateways, so for sure\nall foobar_gateway jobs should be upgraded.\nbut IMHO it is a bad idea to colocate service nodes, except perhaps for\ntoy smoke-test scenarios. the colocated services will step all over each\nother. not in the sense of (eg) writing to the same files [ie, the reason\nyou can't colocate jobs that use vcap_registrar]. Rather, VMs can become\nfull because service nodes assume they are the only consumers of\ndisk/memory. One can argue we should remove this assumption [though we\nthought long & hard about how to achieve lights-out operation of each\nservice and I don't think see an alternative]. in the meantime, we don't\nneed to bother to upgrade any of the foobar_node_ng jobs.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/57#issuecomment-15854118\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Andy, I thought more on this. If its only for node.js then you can also have it installed with node.js and link it into the building of node.js package. You do this in cf-release. This is more common than asking for all bosh stemcells to include an additional system package.\nThe goal is for a bosh release to have no dependencies on the stemcell. If you need something, you include it in your bosh release.\n. If the SREs & co think its a generally useful system pkg then great; but I still think all packages needed by a bosh release should be in that bosh release. Theoretically, even gcc; but I might be pushing the friendship there with people :)\n. @andypiper if you install the package (as a bosh package, rather than a debian package) it will still be available to a CF release - it will be installed into a /var/vcap/packages/libicu folder and the DEA can be configured to include /var/vcap/packages/libicu/lib in its library path variables.\nThis is one downside of bosh packaging - whilst you get awesome separation of packages you don't get the converse - one lovely /lib folder for all libs & one lovely /bin for all executables.\n. @andypiper re the question - how does a buildpack install dependencies? - that answer is independent of this PR :)  Ultimately a buildpack would need to bundle everything it needs to run. The fewer assumptions it makes about a DEA VM and a DEA warden container the better for the future of cloudfoundry.\nOne of the challenges that a PaaS has is how to continually upgrade its system packages & services without upsetting legacy applications that assume those old packages haven't gone away. Hence we have dea_ruby18 and dea_ruby19; but also hence we've never changed what ruby is inside them.\n. The bosh-users list is the best place to have this conversation so others\ncan stalk and learn too. Super excited that you attempted your first\nbosh-related patch :)\n. Workaround:\ngem install net-ssh -v 2.2.2\ngem install net-scp -v 1.0.4\ngem install fog -v \"~> 1.9.0\"\ngem install bosh_deployer\n. Note, the reasons @fog upgraded to ~> 1.1 are:\n- https://github.com/fog/fog/pull/1562\n- https://github.com/fog/fog/issues/1561\n. Very happy for pending release of 1.5.0 gems\n. Perhaps start by shipping the 1.5.0-pre2 gems to rubygems. Currently:\n$ gem install bosh_cli --pre\nFetching: bosh_cli-1.0.rc1.gem (100%)\nSuccessfully installed bosh_cli-1.0.rc1\n. gem uninstall fog\ngem uninstall net-scp\ngem uninstall net-ssh\nThen:\ngem install net-ssh -v 2.2.1\ngem install net-scp -v 1.0.4\ngem install fog -v 1.9.0\nWork?\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Mar 7, 2013 at 9:34 PM, ajpande notifications@github.com wrote:\n\nHi\nAfter installing bosh_deployer if i run commands like \"bosh help\" or \"bosh micro deployment aws\" i get following error. \nFailed to load plugin gems/bosh_deployer-1.4.1/lib/bosh/cli/commands/micro.rb: Unable to activate fog-1.10.0, because net-scp-1.0.4 conflicts with net-scp (~> 1.1)\nFile gems/bosh_deployer-1.4.1/lib/bosh/cli/commands/micro.rb has been loaded as \nplugin but it didn't contain any commands. Make sure this plugin is updated to \nbe compatible with BOSH CLI 1.0. \nUnknown command: micro deployment aws\nPlease let me know how  i can fix this. here are my env details\nRubyGems Environment:\n- RUBYGEMS VERSION: 2.0.2\n- RUBY VERSION: 1.9.3 (2011-10-30 patchlevel 0) [x86_64-linux]\n- INSTALLATION DIRECTORY: /usr/lib/ruby/gems/1.9.1\n- RUBY EXECUTABLE: /usr/bin/ruby1.9.1\n- EXECUTABLE DIRECTORY: /usr/bin\n- RUBYGEMS PLATFORMS:\n  - ruby\n  - x86_64-linux\n- GEM PATHS:\n  - /usr/lib/ruby/gems/1.9.1\n  - /home/apande/.gem/ruby/1.9.1\n- GEM CONFIGURATION:\n  - :update_sources => true\n  - :verbose => true\n  - :backtrace => false\n  - :bulk_threshold => 1000\n- REMOTE SOURCES:\n  - https://rubygems.org/\n    Regards\n    Ajay\n    ---\n    Reply to this email directly or view it on GitHub:\n    https://github.com/cloudfoundry/bosh/issues/73\n. Martin, was his config correct?\n. xoxo to you Martin!\n. James, would these be release job scripts? Or if not, how would you like to inject the monitoring behaviour into the VMs? Or do you want notifications to 3rd party systems via director/NATS messages?\n. I have deployed the wordpress example a few days ago; but tonight its failing as above on a new deployment.\n. How do you kill a deployment when the compilation VM is hanging due to agent issues? Timeout takes forever when you know its a glitch.\n. Upgrading from 676 to 693 to see if it fixes issue.\n. ## Can you create a ticket to add this as a migration?\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 4, 2013 at 7:23 AM, Ruben Koster notifications@github.com\nwrote:\n\nFixed the problem of the failed postgres migration with:\n/var/vcap/packages/postgres/bin/psql -d bosh -U vcap -c \"REASSIGN OWNED BY postgres TO bosh\"\nNow have successfully deployed microbosh 703.\nIf the problem still persists I should manifest itself within one day.\nWill report back tomorrow.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/96#issuecomment-18912194\n. ## Perhaps it's not possible to migrate actually. So do we need to add properties to legacy micro_bosh.yml?\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 4, 2013 at 7:23 AM, Ruben Koster notifications@github.com\nwrote:\n\nFixed the problem of the failed postgres migration with:\n/var/vcap/packages/postgres/bin/psql -d bosh -U vcap -c \"REASSIGN OWNED BY postgres TO bosh\"\nNow have successfully deployed microbosh 703.\nIf the problem still persists I should manifest itself within one day.\nWill report back tomorrow.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/96#issuecomment-18912194\n. ## Perhaps a ticket/feature to test n-1 -> n upgrades?\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 4, 2013 at 7:30 AM, Martin Englund notifications@github.com\nwrote:\n\nThe regression happened as our CI system unfortunately doesn't test upgrades, just clean installs. Sorry about that, I'll make sure someone looks at fixing it.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/96#issuecomment-18912702\n. xoxo to the ci team!\n. Update: Martin has been working on #274\n. Kinda weird, as the last commit to change this line was merely refactoring it https://github.com/cloudfoundry/bosh/commit/5ed9261069895c5e9570a628355f8d1972efa23f\n. How come the integration tests are passing but the rake task for microbosh stemcell is failing? https://travis-ci.org/cloudfoundry/bosh/builds \n. Perhaps related to https://github.com/cloudfoundry/bosh/commit/ad60ba5aa28d1628c4cfcf0a207b0d6aba42ceb1 ?\n\nWhy is failing for @rkoster and myself; but I guess it doesn't fail for anyone at Pivotal or their CI in the last week? Working for anyone?\n. Looks like @danhigham found this bug too https://github.com/cloudfoundry/bosh/commit/ad60ba5aa28d1628c4cfcf0a207b0d6aba42ceb1#commitcomment-2925556\n. Duplicate of #112 \n. Ooh, is it:\n$ curl -I http://bosh-jenkins-artifacts.s3.amazonaws.com/last_successful_micro-bosh-stemcell.tgz \nHTTP/1.1 200 OK\nx-amz-id-2: 5HpVuDDX/4PfC10bGBf658MZ34x9w2wgQTMbAArlFrv5BVbH5f7ScqdVwGvqO1HS\nx-amz-request-id: 07D804842C343153\nDate: Mon, 08 Apr 2013 16:15:16 GMT\nLast-Modified: Sat, 30 Mar 2013 00:06:36 GMT\nx-amz-expiration: expiry-date=\"Tue, 30 Apr 2013 00:00:00 GMT\", rule-id=\"Clean Out Old\"\nETag: \"c54cd640cc7f39d1f5ee3401fc5e56c8-94\"\nAccept-Ranges: bytes\nContent-Type: \nContent-Length: 491369111\nServer: AmazonS3\n. It's probably for another PR, but I think you want to add region to the URL too.\n. ## Ah yep\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Apr 11, 2013 at 7:52 AM, Ferran Rodenas notifications@github.com\nwrote:\n\nNot now, the .tgz is a bare stemcell (so you need to upload it to AWS), and the _ami.tgz and the _light.tgz are actually tied to us-west-1 (or east, I don't remember exactly which one). In the future, when we publish them to other regions, the only ones that will survive should be the .tgz and *_light.tgz, and the last one will contain ami's ids for all regions, so no need to add the region to the stemcell name.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/134#issuecomment-16239470\n. Why did this never get merged by someone?!\u200b\nCan I please have commit access....\n. Will do. Thanks for creating the build-breakage ticket too. Thanks in advance if you fix the build too.\n. Does this mean we'll have 1.5.0 gems soon? :)\n. How do I use this awesomeness so I don't need to download stemcells? What bosh micro deploy XYZ command do I run?\n. Do I add a cloud_properties.image_location to my micro_bosh.yml and ignore the XYZ part above?\n. If this patch isn't desirable; I've worked around it by adding vagrant user to vcap group.\n. I would prefer that the release itself closed off /tmp if it felt that it needed to; rather than do it for all bosh releases\n. Thanks\n. Going to close this; and instead change the module to Cloud\n. Blah. This naming scheme is a mess. \"cloud\" vs \"Clouds\" everywhere. Yuck.\n. [5/29/13 3:07:31 PM] Martin Englund: Can I merge it now? or still working\non it?\n[5/29/13 3:07:34 PM] Dr Nic Williams: done\n[5/29/13 3:07:47 PM] Martin Englund: Damm!!!!!\n[5/29/13 3:07:56 PM] Martin Englund: Ferdy went ahead\n. hahahah\n. I feel like my comment has been edited by the great @frodenas for his own amusement :D \n. I know @pmenglund has a grander plan for VM authentication/authorization; but I personally would like a PR like this one to be merged now; and \"better\" solution to implemented later. The intent of this PR is better than what we have now.\n. Can we merge this? My \"a\" key is wearing out.\n. ## Agree with Sean; if we keep the generated users can we at least let them do something useful :)\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 18, 2013 at 10:07 PM, Sean Sweda notifications@github.com\nwrote:\n\nif we are going to leave the non-privileged account then please, please, please change the damn unix permissions so that it has read access to /var/vcap\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/223#issuecomment-19663311\n. I'll re-test. What was the sha1 for the fix?\n. Bump\n. Why was the PR closed?\n. Close it and ill reopen it when I get back to stemcell/micro/packaging work. Sorry that I can't create time to revisit this PR at the moment.\n. I'm feeling more loose but yet more json pure already! :dancer: \n. Wow! Only one person getting notifications.\n\nPerhaps change it to entire pivotal team so they (the only people with commit access and who are committing directly to master on Friday afternoons) can fix the build asap?\n. Oh awesome! Cloning now!\nOn Wed, Jun 5, 2013 at 9:43 AM, Matt Stine notifications@github.com wrote:\n\nHere are the beginnings of this implementation: mstine@ba5e013#L3L243https://github.com/mstine/bosh/commit/ba5e013b6390d3562ef6daddc3eda52e3863a121#L3L243\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/237#issuecomment-18990704\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. @mstine if you can create a branch in your repo (say \"swagger\"), and then\nattach your branch to this issue.\nYou can do this via the hub app.\ngit checkout -b swagger\ngit push mstine swagger\nbrew install hub\nhub pull-request -i 237\n. Mike, can you try p(\"bosh.password\") instead?\n. Notably, that the agent on the AWS AMI is the new 699 agent and it also has the issue.\n. Has anything changed in the requirements for running bosh micro deploy recently? Ports that need access? (its still over ssh isn't it?)\n. Correction, eventually it does get the registry. Very ugly logs though. Makes it look like its failing.\n. @ngtuna I haven't seen this error in a long. I wonder why you get it indeed.\n. Unfortunately I don't remember; I'm sorry.\nOn Tue, Aug 5, 2014 at 12:03 PM, ngtuna notifications@github.com wrote:\n\n@drnic How do you think openstack-registry didn't run so that I got this error?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/243#issuecomment-51243922\n. I have a feeling this worked for me the other day. I'm going to close.\n. I have a feeling this worked for me the other day. I'm going to close.\n. Ahh interesting.\n\nHmm. As I recall, I was trying to create a persistent disk of size 4096.\nBut I can reproduce the error by creating a 4Gb volume in the openstack dashboard; so I'll close this ticket.\n. This gem has been renamed for the 1.5.0.pre family of gems to bosh_cli_plugin_micro\nTry:\ngem install bosh_cli_plugin_micro \"~> 1.5.0.pre\" --source https://s3.amazonaws.com/bosh-jenkins-gems/\n. Sorry:\ngem install bosh_cli_plugin_micro -v \"~> 1.5.0.pre\" --source https://s3.amazonaws.com/bosh-jenkins-gems/\nThere are no official working docs for deploying anything to OpenStack. Confirmation came through just now (independently) that @frodenas & I will get a lot more time to get the full stack running & documented on OpenStack asap.\n. Today, the state of bosh/cf docs for openstack is summarized as:\n\n. The \"better\" way to install gems, that might solve your problem, is to\ncreate a Gemfile with the two rubygems sources, and bundle it. Then use\n\"bundle exec\" to run the commands.\nIts not impossible. Other's are running CF on openstack.\n. Perhaps the bug is related to the new http GET / response you added?\n. What we need now in the description is a screenshot of the awesomeness:\n\n. Great job @mstine on this PR!\n. Can we please merge this so we can all continue contributing to allowing people to see and explore the API of this bosh?\n. ## What does \"team's comfortable living with\" mean? As a member of the team of people who contribute to bosh, I think this is going to be a wonderful way for people to explore the API and the nest values that come back. Especially since there seems a sad predisposition to \"no docs\" and actually deleting docs within the project, it is even more critical to allow users to explore the values being passed around on their bosh.\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Jun 14, 2013 at 7:09 AM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\nWe'd like a pair to take a closer look at Swagger first and see if it's something the team's comfortable living with in the long run.\nWe'll aim to loop back to this PR once that has happened.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/252#issuecomment-19458953\n. Oh this is this [redacted] business of bosh's own bosh release not being able to be created with bosh create release. \n. I say \"[redacted this too]\" because its inconsistent with every other bosh release and work flow.\n. The correct command is bundle exec rake release:create_dev_release\n. Bump. Anyone have idea of what's happening here?\n. Bump\n. Bump\n. I'll rewrite this PR another day with the great suggestions above.\n. I put this in manual and try to not accidentally commit it into each patch :)\n. Unsure how this PR broke the integration tests for health manager.\n\nhttps://travis-ci.org/cloudfoundry/bosh/jobs/7918805#L401\nWeird.\n. I have another work around now.\ngit update-index --assume-unchanged bosh_cli/bin/bosh\nI don't want to figure out how changing bin/bosh affected the HM tests. :) Closing.\n. There are 14 other PRs to argue over.\nPersonally, I always directly load the local lib/ folder from my\nexecutables; rather than hope that rubygems loads it correctly.\n. Ok. I looked and none of the other tests were breaking. Perhaps it was a\none-time thing. If someone else wants this PR, happy to have it merged.\n. Noticed that cf gem includes a bin/cf.dev as an alternate dev CLI. https://github.com/cloudfoundry/cf/tree/master/bin\nWill look at creating a similar bin/bosh.dev PR later.\n. Bump\n. You'll need a newer director that includes this patch. But you've tested\nthat it degrades nicely (doesn't error) if you use --jobs upon an old\ndirector.\nNic\n. Heh, neat.\n. Bump\n. That's one large commit! Oh the piecemeal story that has been lost :)\n. bosh 1.6 edition - \"shit doesn't need to touch my laptop all the time!!!\"\n. I'm just transferring my notebook full of product proposals into the issues section for feedback on the proposal. Starting with little ones like this.\nIf Pivotal wants to implement any of them that'd be great. Someone else might want to and I'll help them. Ultimately I'll implement them if no one else does first. I want to move away from external tools that make living with bosh easier (like bosh-bootstrap, bosh-cloudfoundry, half of bootstrap-cf-plugin); and just make bosh easier to live with. Nay, I want to make it a delight to get started and live with!\n. Thoughts on implementation? A long running task that runs curl to get the stemcell then uploads it back into the director API?\n. Matt, it is useful to know also for each proposal which ones you think Pivotal values for its internal goals outside of the goals of bosh itself.\n. Martin, preference for large file downloads within Ruby? \nOn initial or mid-way failure, retry for a while, and then fail the deployment?\n. Martin, it wouldn't stop people from pre-uploading stemcells. It just wouldn't fail a deployment just because they haven't told their bosh about stemcells yet. New users don't know what stemcells are. Why force an extra step on them that they don't know what it's about? Later on, when they know what's what, they can start manually managing stemcells. Once they're beyond \"Level 1 Bosh Noob\" :)\n. Perhaps the director download could be first implemented as a standalone command to trigger the task; and then be added into deployment as a 2nd (albeit, 1st) task if necessary.\u00a0\nI'll create an extra ticket.\n. Created #278 as the initial feature; and this issue would be implemented second.\n. Gabi mentioned this. I think it might get awkward when you next deploy via a normal manifest. Your manifest says 3 instances but you've changed it to 5 via the director API. Which one is correct?\u00a0\nMy gut says we need to keep \"truth\" in one source. Currently that is a deployment manifest on the CLI side, which is uploaded via #deploy API call.\n. Ok, the specific implementation of this issue depends on whether the dynamic resource pool feature is done or not at the time.\n. The CLI command would still be the same to allow adjustments to job instance counts?\n. Do you want to change \"bosh vms\" to \"bosh instances\"?\n. I understand the vms vs instance(s) naming now.\nI don't have any concrete thoughts on manifest vs director DB for truth\nyet. Changing it is probably a 2.0 change, fwiw.\nI personally like the \"bosh action verb\" style of CLI. I can understand\nthat its unfamiliar to new people; and its great if more bosh-dev's are\ncaring about the initial experiences of new bosh users. If rewriting the\nCLI to be \"bosh resource verb\" fits this goal, then great. Again, probably\na 2.0 change.\n. I think this ticket is now implemented by ferdy by passing remote\nhttp://to \"bosh upload stemcell\".\nThis ticket is different from #275\n. Yes & yes\n. Selective updating would be provided for a bosh release that finished implementing properties in spec files. But for those that don't, I don't see why it should stop jobs colocating at all.\n. Bosh isn't just about cf-release though.\n. I guess its coded and its probably not worth the extra effort to uncode/fix.\n. I guess its coded and its probably not worth the extra effort to uncode/fix.\n. Not fixing this does mean that each day or week we will have broken\ndeployments (when we try to update) because different jobs will need to be\nmoved from one VM to another just because of this colocation error\nbug/warning. So cf-release is unstable for those of us who want to colocate\njobs.\n@frodenas - we'll need to keep the client abreast of this and let them know\nwe may need to regularly destroy & rebuild our CFs as the cf-release jobs\nare upgraded, I think\n. Not fixing this does mean that each day or week we will have broken\ndeployments (when we try to update) because different jobs will need to be\nmoved from one VM to another just because of this colocation error\nbug/warning. So cf-release is unstable for those of us who want to colocate\njobs.\n@frodenas - we'll need to keep the client abreast of this and let them know\nwe may need to regularly destroy & rebuild our CFs as the cf-release jobs\nare upgraded, I think\n. See the email thread today. Some jobs cannot be collocated with others. I wish it didn't generate an error; but that's how bosh works this year :)\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 2:05 AM, niuqingqing1001 notifications@github.com\nwrote:\n\nhello, drnic.\nI understood the discussion above, but I can't get the method to solve the error after a lot of attempts.\nCan you give a hand?\nThank you very much.\nerror info:\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/281#issuecomment-22931400\n. See the email thread today. Some jobs cannot be collocated with others. I wish it didn't generate an error; but that's how bosh works this year :)\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 2:05 AM, niuqingqing1001 notifications@github.com\nwrote:\n\nhello, drnic.\nI understood the discussion above, but I can't get the method to solve the error after a lot of attempts.\nCan you give a hand?\nThank you very much.\nerror info:\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/281#issuecomment-22931400\n. Heh, yeah you could rip out the properties: section from each of the spec files as a workaround :)\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 5:54 PM, niuqingqing1001 notifications@github.com\nwrote:\n\nThank you, drnic.\nI have see the thread at:\nhttp://grokbase.com/p/cloudfoundry.org/vcap-dev/136bfq5zfv/script-which-jobs-using-new-p-x-y-z-helpers-and-which-arent\nI think your point is this:\"To summarize: these two jobs(cloud_controller_ng and gorouter) can be colocated; but I think not with any other job today. Or something like that.\"\nmy cf.yml is at the back:\nyou can see cloud_controller_ng and gorouter are in the job \"api\", they are together. But my error is in the job \"core\".\nSo what can I do?\nI had another quesion. \nShould I manually modify the /jobs/*/templates files that use the old style into the new style to resolve the error?\nI had attempt to modify all the files to new style, but it didn't work, and the error still appear.\nor Should I manually add properties section to the jobs that had no this section to resolve the error?\nI had spent four days on this error, and hope you can resolve the problem with me.\nThank you.\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't.\ncf.yml\njobs:\n- name: core\n  template:\n  - syslog_aggregator\n  - nats\n  - postgres\n  - health_manager_next\n  - collector\n  - debian_nfs_server\n  - uaa\n  - login\n    instances: 1\n    resource_pool: medium\n    persistent_disk: 8192\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    properties:\n    db: databases\n- name: api\n  template:\n  - cloud_controller_ng\n  - gorouter\n    instances: 1\n    resource_pool: medium\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    - name: floating\n      static_ips:\n  - 10.141.123.245\n    properties:\n    db: databases\n- name: dea\n  template:\n  - dea_next\n    instances: 1\n    resource_pool: dea\n    networks:\n  - name: default\n    default: [dns, gateway]\n    ---\n    Reply to this email directly or view it on GitHub:\n    https://github.com/cloudfoundry/bosh/issues/281#issuecomment-22988879\n. Heh, yeah you could rip out the properties: section from each of the spec files as a workaround :)\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 5:54 PM, niuqingqing1001 notifications@github.com\nwrote:\n\nThank you, drnic.\nI have see the thread at:\nhttp://grokbase.com/p/cloudfoundry.org/vcap-dev/136bfq5zfv/script-which-jobs-using-new-p-x-y-z-helpers-and-which-arent\nI think your point is this:\"To summarize: these two jobs(cloud_controller_ng and gorouter) can be colocated; but I think not with any other job today. Or something like that.\"\nmy cf.yml is at the back:\nyou can see cloud_controller_ng and gorouter are in the job \"api\", they are together. But my error is in the job \"core\".\nSo what can I do?\nI had another quesion. \nShould I manually modify the /jobs/*/templates files that use the old style into the new style to resolve the error?\nI had attempt to modify all the files to new style, but it didn't work, and the error still appear.\nor Should I manually add properties section to the jobs that had no this section to resolve the error?\nI had spent four days on this error, and hope you can resolve the problem with me.\nThank you.\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't.\ncf.yml\njobs:\n- name: core\n  template:\n  - syslog_aggregator\n  - nats\n  - postgres\n  - health_manager_next\n  - collector\n  - debian_nfs_server\n  - uaa\n  - login\n    instances: 1\n    resource_pool: medium\n    persistent_disk: 8192\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    properties:\n    db: databases\n- name: api\n  template:\n  - cloud_controller_ng\n  - gorouter\n    instances: 1\n    resource_pool: medium\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    - name: floating\n      static_ips:\n  - 10.141.123.245\n    properties:\n    db: databases\n- name: dea\n  template:\n  - dea_next\n    instances: 1\n    resource_pool: dea\n    networks:\n  - name: default\n    default: [dns, gateway]\n    ---\n    Reply to this email directly or view it on GitHub:\n    https://github.com/cloudfoundry/bosh/issues/281#issuecomment-22988879\n. I guess the p(...) methods in the templates are based off the spec properties; I won't suggest ripping them out in future :)\n\n```\nGlad you're good to go now.\n--\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 7:57 PM, niuqingqing1001 notifications@github.com\nwrote:\n\nHi, drnic.\nThe method of ripping out the properties: section from each of the spec files doesn't work.\nBut it doesn't matter.\nMy colleague stevenbzb  also open an issue for the same problem, the link is as follows: \nhttps://github.com/cloudfoundry/bosh/issues/398\nAccording to you suggestions, we modify the /deployment/cf.yml. At last the error is resolved.\nps: We didn't modify any file in the /jobs.\nThank you very much.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/281#issuecomment-22992572\n. I guess the p(...) methods in the templates are based off the spec properties; I won't suggest ripping them out in future :)\n\n```\nGlad you're good to go now.\n--\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 7:57 PM, niuqingqing1001 notifications@github.com\nwrote:\n\nHi, drnic.\nThe method of ripping out the properties: section from each of the spec files doesn't work.\nBut it doesn't matter.\nMy colleague stevenbzb  also open an issue for the same problem, the link is as follows: \nhttps://github.com/cloudfoundry/bosh/issues/398\nAccording to you suggestions, we modify the /deployment/cf.yml. At last the error is resolved.\nps: We didn't modify any file in the /jobs.\nThank you very much.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/281#issuecomment-22992572\n. Thanks Gabi.\n. I believe this is still the case.\n. I believe this is still the case.\n. Extended suggestion - the generated ssh user has various $JOB, $INDEX,\n$PATH, $PS1 variables set for them in their .bashrc\n\nNow that I know more about the agent itself & how to develop it, I might be\nable to do this.\n. Could this still be done by the agent but for the root user?\n. I think you mean #223 - although it was subsequently reverted.\n. I think you mean #223 - although it was subsequently reverted.\n. I've never seen this \"end of file reached\" compile exit error before.\nAnyone else have ideas?\n. I've never seen this \"end of file reached\" compile exit error before.\nAnyone else have ideas?\n. I believe you can have a per-job update: section as well to override the\nglobal settings.\n. Ooh, so I can just edit .bosh_config and it becomes a target alias?\n. I DID NOT KNOW THAT!!\nI just leveled up. It feels tingly.\n. I DID NOT KNOW THAT!!\nI just leveled up. It feels tingly.\n. Yes, the alias thing was the solution. Sorry I didn't close.\u00a0\nCan I close tickets via email?\nOn Mon, Sep 30, 2013 at 12:21 PM, Mark Rushakoff notifications@github.com\nwrote:\n\n@drnic is the alias a good enough solution to close this issue? \n@mark-rushakoff / @d\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/301#issuecomment-25394969\n. Yes, the alias thing was the solution. Sorry I didn't close.\u00a0\n\nCan I close tickets via email?\nOn Mon, Sep 30, 2013 at 12:21 PM, Mark Rushakoff notifications@github.com\nwrote:\n\n@drnic is the alias a good enough solution to close this issue? \n@mark-rushakoff / @d\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/301#issuecomment-25394969\n. Bump.\n. Bump.\n. Since you've got Gemfile.lock in the bosh releases they won't be affected until they update.\n. Since you've got Gemfile.lock in the bosh releases they won't be affected until they update.\n. I'd hate to think we can't fix bad human UX design in the CLI because of Pivot's jenkins scripts :/\n. I'd hate to think we can't fix bad human UX design in the CLI because of Pivot's jenkins scripts :/\n. If there are public scripts you're referring to, please let me know and I'll create PRs for them too.\n. If there are public scripts you're referring to, please let me know and I'll create PRs for them too.\n. You're a good man, Mr Brown.\n. You're a good man, Mr Brown.\n. Yep, I'll try and get to it soon. Battling with bosh all day today. I'm not\nsure I'm winning yet :)\n. Rebased & hyphens removed & tests moved to new location/new style\n. Thanks. I can reproduce and am fixing. Sorry about that.\n. Ok, spec is fixed and pushed.\n\nAgain, sorry about that.\n. ## Perhaps: bosh set ssh gateway\u00a0\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 18, 2013 at 3:57 PM, gabis notifications@github.com wrote:\n\nI don't feel enthusiastic about this.  Shouldn't the user get a say in whether it's cached?\nWe have a story in to make this settable in .bosh_config, but we haven't settled on UI yet.  I'm not 100% happy with any of the ideas I've had so far.\n1. bosh target $ip --gateway_host xx --gateway_user xx --public_key xx\nconveys the idea that you're setting it per target, but seems 'hacky'\n2. bosh gateway .... etc\nset the gateway for the current target (or --target), without args show current gateway\nseems weird to add another verb\n3. use bosh properties\nin theory this could allow you to set a gateway per deployment (rather than per target), but currently it is not really utilized, not sure we should revive it\n4. force hard core users to set it with vi in the .bosh_config\n:-1: obvious flaws\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-19649230\n. ## Perhaps: bosh set ssh gateway\u00a0\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 18, 2013 at 3:57 PM, gabis notifications@github.com wrote:\n\nI don't feel enthusiastic about this.  Shouldn't the user get a say in whether it's cached?\nWe have a story in to make this settable in .bosh_config, but we haven't settled on UI yet.  I'm not 100% happy with any of the ideas I've had so far.\n1. bosh target $ip --gateway_host xx --gateway_user xx --public_key xx\nconveys the idea that you're setting it per target, but seems 'hacky'\n2. bosh gateway .... etc\nset the gateway for the current target (or --target), without args show current gateway\nseems weird to add another verb\n3. use bosh properties\nin theory this could allow you to set a gateway per deployment (rather than per target), but currently it is not really utilized, not sure we should revive it\n4. force hard core users to set it with vi in the .bosh_config\n:-1: obvious flaws\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-19649230\n. I'm happy with any UI. At the moment the workaround is a bash function that passes in all the flags each time.\n. I'm happy with any UI. At the moment the workaround is a bash function that passes in all the flags each time.\n. Yes\n\nOn Mon, Aug 4, 2014 at 9:42 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic would be ok with --gateway flag that would use previously saved settings?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-51149444\n. Yes\n\nOn Mon, Aug 4, 2014 at 9:42 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic would be ok with --gateway flag that would use previously saved settings?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-51149444\n. I kind of think that if you needed the gateway flags (to access any VM on a deployment or a whole director) once you probably always need them.\n. I kind of think that if you needed the gateway flags (to access any VM on a deployment or a whole director) once you probably always need them.\n. The config could go in .bosh_config next to user/pass\n\nOn Tue, Aug 5, 2014 at 7:45 AM, Matthew Kocher notifications@github.com\nwrote:\n\nWhy not set ssh gateway when targeting a director? bosh ssh is something an operator does all the time, and the gateway is pretty much always the same.\nI think the wider goal here should be to eliminate the need for all the bosh_ssh scripts that various teams/people use.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-51207333\n. The config could go in .bosh_config next to user/pass\n\nOn Tue, Aug 5, 2014 at 7:45 AM, Matthew Kocher notifications@github.com\nwrote:\n\nWhy not set ssh gateway when targeting a director? bosh ssh is something an operator does all the time, and the gateway is pretty much always the same.\nI think the wider goal here should be to eliminate the need for all the bosh_ssh scripts that various teams/people use.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-51207333\n. Cool re default gateway settings.\n\nOn Mon, Oct 5, 2015 at 1:26 PM, bosh-ci-push-pull\nnotifications@github.com wrote:\n\nWith newer stemcells (3093+) strict_host_key_checking is not necessary\nbecause CLI knows which host key is expected and trust it. You have to\nupdate your CLI, Director and stemcells for this to take the effect.\nbosh-lite stemcells are not available yet for that version, but we are\nworking on it.\nRegarding gateway: We decided to make default gateway settings a director\nconfiguration. CLI will see them and use the gateway without having user\nspecify any of the gateway options. Story for this:\nhttps://www.pivotaltracker.com/story/show/104125746\nOn Mon, Oct 5, 2015 at 8:09 AM, Alexey Zakharov notifications@github.com\nwrote:\n\nAlso it could be nice to keep default value for long settings like --strict_host_key_checking\n somewhere in the .bosh_config\nIt's very uncomfortable to type that option every time.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-145563534.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/305#issuecomment-145605183\n. ProTip: The :25555 is optional.\n\n\nInitial user/pass is admin/admin. This is deleted when you create a user:\nbosh create user\n. Hurray for Azure! \n. Are there other blobstore plugins for the director and for the CLI yet; or is this a new idea?\n\u200b\u00a0\n\u200bAfaik we currently don't have a way for a microbosh or a bosh-release bosh to include a blobstore plugin (since they are bosh releases which aren't really modifiable at runtime).\nI'm not sure what the plugin story for CLI blobstores is \u200beither.\u200b\nNor for the agent on stemcells. I think it needs a blobstore client lib too to access the blobstore?\nHmm. Anyone with ideas?\n. ## Bosh newbs are the best! Love our newbs :)\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Sat, Jun 22, 2013 at 8:02 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\n\nAre there other blobstore plugins for the director and for the CLI yet; or is this a new idea?\nProbably a new idea from a BOSH newb :)\nI see what you mean about releases. Bummer - makes interesting exploration like this a bigger commitment.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/312#issuecomment-19868239\n. Its probably not a big fix to remove this aspect of bosh_cli. I'm not sure\nwhat its for - or rather, I assume there is an attempt to say \"the minimum\nCLI probably infers a minimum bosh director & bosh agent/stemcell\". But\nit's a weird way about enforcing this restriction.\n. ## You're right - I'm trying to suggest a solution; I really care about not having to run cck before running deploy again.\n\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jun 25, 2013 at 3:58 PM, gabis notifications@github.com wrote:\n\nI'm not sure this would work.  VMs aren't created one at a time.  Typically it's up to 32 at a time.\nIt sounds like what you really want is 'if VM creation partially fails, I shouldn't have to run cck before I can deploy again'\nDoes that sound right?  \nThis seems like an ordering problem to me.  The DB should not reflect that the VMs have assignments before 'binding instance VMs' has occurred.  So if some of your VMs failed to create (network glitch, bad day on AWS, config problem, working ^C, etc), you could just run bosh deploy again, then it would roll right into binding instance VMs when the right number were available.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/315#issuecomment-20014213\n. The version number doesn't look right for \"latest\"; I think we're in the 1000s now. It may or may not help; but look for the new stemcell URL (now only one URL for microbosh and base stemcells). Sorry I can't remember the URL format off the top of my head.\n\nOn Tue, Oct 1, 2013 at 7:18 AM, James Masson notifications@github.com\nwrote:\n\nWe're running the latest stemcell releases for vsphere -\nbosh-stemcell-vsphere-992.tgz and micro-bosh-stemcell-vsphere-912.tgz\n- is this bug supposed to be fixed in these versions?\n  We're running into the same issue when changing DNS properties for an existing deployment\n  Networks\n  default\n  subnets\n  10.20.219.0/24\n  changed dns: \n- 10.20.216.1\n- 10.20.216.2\n- 10.20.219.2\n  Error 450002: Timed out sending `prepare_network_change' to 25d37b9c-ff51-4f40-b0a6-e80d044367d5 after 45 seconds\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/issues/316#issuecomment-25453261\n. Off the top of my head we'll need to rethink:\n- how we lay out the jobs on the file system to avoid name classes, say /var/vcap/jobs/cf-release/dea_next etc.\n- support a new deployment file schema to allow a job to list templates that come from multiple releases.\n\nCurrently template: can take a string for a single job template, or a list of strings for a list of job templates from a single release. So I guess we'd need it to also take a list of hashes - each hash providing a release & job template name. E.g.\nyaml\njobs:\n- name: api\n  template:\n  - name: sslproxy\n    release: sslproxy\n  - name: gorouter\n    release: cf\n- I don't think there is any affect upon the CLI - currently none of the CLI commands show which job templates are running on each job/instance.\nAnything else?\n. There would be a consequence for changing /var/vcap/jobs/NAME to /var/vcap/jobs/RELEASE/NAME - many releases assume the location of the job & its configuration files.\nPerhaps the first iteration of colocation/composite releases would be to assume/prohibit colocating job templates that have the same name?\n. Can the other engineers be asked if they can think of any other aspects to\nthis implementation? Anything missing? Any concerns or edge cases?\n. This work is now active in tracker. Is this solution above the one being used or a different approach?\n. For anyone following, looks like job structure did have to be changed https://github.com/cloudfoundry/bosh/commit/b32f5cfe9756c163960333097a13f9321ca62271\n. Is this feature working now?\nWhat is an example deployment file example where two job templates from two releases are included in the same job?\n. Attempted to work around it by renaming the job; but bosh cannot delete the previously failing job:\n```\nCreating bound missing VMs\n  common/0 (00:06:18)                                                                             \nDone                    1/1 00:06:18                                                                \nBinding instance VMs\n  redis_service_node2/0 (00:00:01)                                                                \nDone                    1/1 00:00:01                                                                \nPreparing configuration\n  binding configuration (00:00:01)                                                                \nDone                    1/1 00:00:01                                                                \nDeleting unneeded instances\n  edadffa5-0281-4e42-b77e-4e8ce35e4aeb: OpenStack API Bad Request (Invalid volume: Volume still has 1 dependent snapshots). Check task debug log for details. (00:00:54)\nError                   1/1 00:00:54                                                                \nError 100: OpenStack API Bad Request (Invalid volume: Volume still has 1 dependent snapshots). Check task debug log for details.\n``\n. Part of the work around was to runbosh cck` and delete the aberrant VM that way:\n```\nbosh cck\nPerforming cloud check...\nDirector task 90\nScanning 15 VMs\n  checking VM states (00:00:20)                                                                   \n  14 OK, 0 unresponsive, 1 missing, 0 unbound, 0 out of sync (00:00:00)                           \nDone                    2/2 00:00:20                                                                \nScanning 5 persistent disks\n  looking for inactive disks (00:00:00)                                                           \n  5 OK, 0 inactive, 0 mount-info mismatch (00:00:00)                                              \nDone                    2/2 00:00:00                                                                \nTask 90 done\nStarted     2013-06-25 06:21:40 UTC\nFinished    2013-06-25 06:22:00 UTC\nDuration    00:00:20\nScan is complete, checking if any problems found...\nFound 1 problem\nProblem 1 of 1: VM with cloud ID `edadffa5-0281-4e42-b77e-4e8ce35e4aeb' missing.\n  1. Ignore problem\n  2. Recreate VM using last known apply spec\n  3. Delete VM reference (DANGEROUS!)\nPlease choose a resolution [1 - 3]: 3\nBelow is the list of resolutions you've provided\nPlease make sure everything is fine and confirm your changes\n\nVM with cloud ID `edadffa5-0281-4e42-b77e-4e8ce35e4aeb' missing.\n     Delete VM reference (DANGEROUS!)\n\nApply resolutions? (type 'yes' to continue): yes\nApplying resolutions...\nDirector task 91\nApplying problem resolutions\n  missing_vm 56: Delete VM reference (DANGEROUS!) (00:00:00)                                      \nDone                    1/1 00:00:00                                                                \nTask 91 done\n```\n. I think that blog post needs a warning & redirection URL on it :)\nHarikrishna, sorry that that blob post isn't working for you. There have\nbeen a lot of change to the bosh universe since that blog post and not all\nof them (most?) are not yet published to make a concise blog post yet.\nI did start writing an AWS doc that would replace that blog post. Sorry I\ngot distracted trying to get CF services deployed and running.\nWhen a simple tutorial article is on http://docs.cloudfoundry.com for AWS I\nthink it'll be a good idea to update the blog post with a URL to it.\nNic\nOn Tue, Jun 25, 2013 at 10:01 AM, Harikrishna Doredla \nnotifications@github.com wrote:\n\nHi,\nI used this below URL for aws deployment.\nhttp://blog.cloudfoundry.com/2012/09/06/deploying-to-aws-using-cloud-foundry-bosh/\n1) Micro bosh instance was successfully installed on aws with help of\nbelow command.\nbosh micro deploy ami-69dd6900\n2) bosh micro status\nStemcell CID ami-69dd6900\nStemcell name ami-69dd6900\nVM CID i-5ec3b13b\nDisk CID vol-8980e4d3\nMicro BOSH CID bm-717e75df-8850-48ee-921e-a89f85793432\nDeployment /root/deployments/awstest/micro_bosh.yml\nTarget http://54.235.177.111:25555\n3) bosh status\nUpdating director data... done\nDirector\nName awstest\nURL http://54.235.177.111:25555\nVersion 0.5.2 (release:ffed4d4a bosh:21e0b0bc)\nUser admin\nUUID 77af2b18-ef36-478c-9bc7-ed1a66154277\nCPI aws\nDeployment\nnot set\n4) Now i tried to upload bosh stemcell getting error like below.\nbosh upload stemcell bosh-stemcell-aws-1.5.0.pre1.tgz\nVerifying stemcell...\nFile exists and readable OK\nManifest not found in cache, verifying tarball...\nExtract tarball FAILED\nStemcell is invalid, please fix, verify and upload again\n5) Not only this version tried with 0.6.4,0.70 for all the versions\ngetting same error.\n6) Can you please help why it's happening like this.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/321\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Gabi, that URL is only for light stemcells which currently only support us-east-1 still I think\n. Oooh can't remember now. Closing.\n. I think it's still a good idea - create the HM user when it is configured - otherwise it is complex to setup a non-admin user as the HM stops working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley notifications@github.com\nwrote:\n\n@drnic \nIs this still an issue? If not, we would like to close this since it looks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n. If a bosh/microbosh config says that it should use a specific hm user then it might as well create that user if it's missing.\n\nHaving said that, this would immediately disable the admin/admin default. So perhaps it's screwed either way.\nGreg, what is you're preference for onboarding?\nOn Mon, Apr 21, 2014 at 4:02 PM, greg oehmen notifications@github.com\nwrote:\n\nThere are a few stories related to BOSH user management in the\nbacklog/icebox.  Mainly -\nhttps://www.pivotaltracker.com/story/show/68212726- Bosh User Admin:\nRole Development.   This fits in that theme for sure.\nNic - is what you are saying \"Precreate the HM user because creating it\nexactly when you need it is too painful.\"?\nOn Mon, Apr 21, 2014 at 3:53 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nI think it's still a good idea - create the HM user when it is configured\n- otherwise it is complex to setup a non-admin user as the HM stops\n  working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley notifications@github.com\nwrote:\n\n@drnic\nIs this still an issue? If not, we would like to close this since it\nlooks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986220\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986896\n. My primary concern is that the moment a new bosh user is created say drnic/password, then now the health monitor can't function (I believe it assumes admin/admin is its credentials). And I don't think any notice is given.\u00a0\n\n\nPerhaps some documentation on health manager credentials might do the trick in the meantime.\nOn Mon, Apr 21, 2014 at 10:08 PM, rboshman notifications@github.com\nwrote:\n\n...Screwed either way right now.  Obviously, bosh user mgt is in need of\nmaturation.\nMy preference is to leave it as it is now and fix it once - the right way -\nwhen we can rather than spending cycles on it now and then cycles again in\nthe future.  That is, of course, unless compelling reasons arise for\nbumping bosh user mgt up in prioritization.\nGreg\nOn Mon, Apr 21, 2014 at 4:06 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nIf a bosh/microbosh config says that it should use a specific hm user then\nit might as well create that user if it's missing.\nHaving said that, this would immediately disable the admin/admin default.\nSo perhaps it's screwed either way.\nGreg, what is you're preference for onboarding?\nOn Mon, Apr 21, 2014 at 4:02 PM, greg oehmen notifications@github.com\nwrote:\n\nThere are a few stories related to BOSH user management in the\nbacklog/icebox. Mainly -\nhttps://www.pivotaltracker.com/story/show/68212726- Bosh User Admin:\nRole Development. This fits in that theme for sure.\nNic - is what you are saying \"Precreate the HM user because creating it\nexactly when you need it is too painful.\"?\nOn Mon, Apr 21, 2014 at 3:53 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nI think it's still a good idea - create the HM user when it is\nconfigured\n- otherwise it is complex to setup a non-admin user as the HM stops\n  working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley notifications@github.com\nwrote:\n\n@drnic\nIs this still an issue? If not, we would like to close this since it\nlooks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986220>\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986896\n\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40987161\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-41004402\n. Yes happens all the time. Requires me to ssh in, change to root and \"monit stop all\" before I can try re-deploying (with a new version of the release).\n\n\nOn Mon, May 5, 2014 at 2:40 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic This issue seems pretty stale. Are you still running into this? Can you give us some more details about how to get into this situation?\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/339#issuecomment-42244115\n. This is what it looks like after a job previously failed to run, and you try to re-deploy (to try to fix it):\n\nFailed updating job loggregator_trafficcontroller_z1: loggregator_trafficcontroller_z1/0 (canary) (00:00:21): Cannot stop job: Service Unavailable\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:428:in `rescue in process'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:417:in `process'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:265:in `process'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:250:in `process_long_running'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:177:in `block in process_in_thread'\n<internal:prelude>:10:in `synchronize'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:175:in `process_in_thread'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.2409.0/lib/bosh_agent/handler.rb:155:in `block in handle_message'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `call'\n/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:1060:in `block in spawn_threadpool'\nOnly workaround I know is to ssh in, change to root, and run monit stop all.\nUnsure how/where to fix this. Neither ruby nor go agent runs monit commands, rather talks to monit via API.\n. Ok will do. Thanks for trying to reproduce.\nOn Tue, May 6, 2014 at 2:03 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic We couldn't reproduce this by just deploying a job that failed to start (i.e. did not create a pidfile) and/or failed to stop (i.e. did not remove the pidfile). Can you give us any more information about what caused your job to fail during start/stop?\nCF Community Pair (@adamstegman & @mbhave)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/339#issuecomment-42358568\n. ## It will be interesting to start documenting crafty things to try during various wacky states that bosh can get itself in (eg how to hack director DB)\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Jul 19, 2013 at 1:18 PM, gabis notifications@github.com wrote:\n\nYeah, we didn't have that option in staging so we tried some creative things, including sending crafted nats messages to reapply the spec.  Fun things to try at home :)  You may have had a bad pidfile laying around or something that prevented something from starting?  Leaving this in tracker to see if we can reproduce the problem.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/340#issuecomment-21267452\n. Yep don't use those. They are very old and I guess don't work with latest bosh gems that you're using.\n\u200b\nEither try using old public bosh_cli (1.0.3) gem with that public stemcells; or figure out where the new pseudo-public vSphere stemcells are going.\n\n\u200b\nSorry I'm not being very helpful. It's been very confusing for everyone all year.\u00a0\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Jul 4, 2013 at 1:10 AM, Jack Wu notifications@github.com wrote:\n\njackwu@jack-ubuntu-bosh:~$ bosh public stemcells --full --all | grep micro | grep vsphere\n| micro-bosh-stemcell-vsphere-0.6.4.tgz            | https://s3.amazonaws.com/blob.cfblob.com/stemcells/micro-bosh-stemcell-vsphere-0.6.4.tgz            | vsphere, micro, stable      |\n| micro-bosh-stemcell-vsphere-0.7.0.tgz            | https://s3.amazonaws.com/blob.cfblob.com/stemcells/micro-bosh-stemcell-vsphere-0.7.0.tgz            | vsphere, micro, test        |\n| micro-bosh-stemcell-vsphere-0.8.1.tgz            | https://s3.amazonaws.com/blob.cfblob.com/stemcells/micro-bosh-stemcell-vsphere-0.8.1.tgz            | vsphere, micro, test        |\n| micro-bosh-stemcell-vsphere-1.5.0.pre1.tgz       | https://s3.amazonaws.com/blob.cfblob.com/stemcells/micro-bosh-stemcell-vsphere-1.5.0.pre1.tgz       | vsphere, micro              |\n| micro-bosh-stemcell-vsphere-1.5.0.pre2.tgz       | https://s3.amazonaws.com/blob.cfblob.com/stemcells/micro-bosh-stemcell-vsphere-1.5.0.pre2.tgz       | vsphere, micro\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/348#issuecomment-20464346\n. I've disagreed with this \"no non-vpc support\" before on the mailing list\nand I disagree with it now.\n\nMany OpenStack installations run without complex networking turned on nor\nforced upon users. And newer AWS accounts that sit above VPC do not force\nthe exposing of subnets etc (albeit they do expose some of the VPC-only\nconstraints such as # of security groups that can be assigned to a VM).\nNic\nNic\n. No. Sorry, sometimes hard to motivate myself to fix old PRs :)\nOn Wed, Sep 4, 2013 at 2:42 PM, Andreas Maier notifications@github.com\nwrote:\n\n@drnic have you had a chance to add some tests to this PR?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/351#issuecomment-23827216\n. I can no longer remember why I needed this. I'm ok with closing this. \n. ## Can you tell me what the choice is and I'll patch it.\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Jul 9, 2013 at 2:37 PM, gabis notifications@github.com wrote:\n\nNot opposed to this but not a current priority.  Added to bottom of backlog.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/354#issuecomment-20707361\n. ## Ok\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Jul 19, 2013 at 1:01 PM, gabis notifications@github.com wrote:\n\nbosh diff should be the one to stay\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/354#issuecomment-21266375\n. Use bosh_cli_plugin_redis or bosh-cloudfoundry (see their Gemfile for how to use a local bosh/bosh_cli with this patch). You'll see the initial step of \"bosh create redis\" or \"bosh create cf\" fetches bosh status. It includes a delay before printing OK.\n\nAlternately, create a small bosh CLI plugin that uses a step with a \"sleep 5\" as the body.\nThe former gives you a bonus chance to see these tools in action.\nThe latter isolates the acceptance test.\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Sun, Jul 14, 2013 at 8:04 PM, gabis notifications@github.com wrote:\n\n@drnic can you suggest a test case to validate this is working?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/355#issuecomment-20949953\n. Thanks for merging. I didn't realise it was merged and started getting concerned that the feature had always existed as I watched the delayed OK effect today. So its now easy to confirm its working - Upload a release and you'll see lots of delayed OKs:\n\nJob 'dashboard' needs 'common' package                       OK\nJob 'dashboard' needs 'ruby' package                         OK\nJob 'dashboard' needs 'dea_jvm' package                      OK\nJob 'dashboard' needs 'syslog_aggregator' package            OK\nJob 'dashboard' needs 'dashboard' package                    OK\nMonit file for 'dashboard'                                   OK\n. Duplicates that I can spot are only: bosh_registry/bin/migrate & director/bin/migrate\n$ ls */bin/*   \nagent_client/bin/agent_client                       bosh_registry/bin/migrate                           director/bin/worker\nblobstore_client/bin/blobstore_client_console       bosh_vsphere_cpi/bin/vsphere_cpi_console            health_monitor/bin/health_monitor\nbosh_agent/bin/bosh_agent                           director/bin/director                               health_monitor/bin/health_monitor_console\nbosh_aws_cpi/bin/bosh_aws_console                   director/bin/director_console                       health_monitor/bin/listener\nbosh_cli/bin/bosh                                   director/bin/director_scheduler                     package_compiler/bin/package_compiler\nbosh_openstack_cpi/bin/bosh_openstack_console       director/bin/drain_workers                          simple_blobstore_server/bin/simple_blobstore_server\nbosh_registry/bin/bosh_registry                     director/bin/migrate                                stemcell_builder/bin/build_from_spec.sh\nI assume to rename these executables will also require changes in bosh/release (I assume that's where they're being used).\nGive me a minute and I'll see if I can make a PR and let the CI system decide if its all ok.\n. Ok, Travis and Jenkins, do you thing.\n. The bosh_registry.log shows:\nI, [2013-07-10T23:55:28.007449 #22115]  INFO -- : BOSH Registry starting...\nI, [2013-07-10T23:55:28.007688 #22115]  INFO -- : HTTP server is starting on port 25888...\nE, [2013-07-10T23:55:28.375069 #22115] ERROR -- : Sinatra::NotFound\nI, [2013-07-10T23:56:00.146017 #22115]  INFO -- : BOSH Registry shutting down...\n. bosh_micro_deploy.log  shows\n```\nI, [2013-07-10T23:56:57.759856 #23752] [0x10e5ff0]  INFO -- : Loading existing deployment data from: /home/ubuntu/.microbosh/deployments/bosh-deployments.yml\nI, [2013-07-10T23:57:06.173147 #23752] [0x10e5ff0]  INFO -- : [AWS EC2 200 0.216009 0 retries] describe_addresses(:filters=>[{:name=>\"instance-id\",:values=>[\"i-d0476bb3\"]}])  \nI, [2013-07-10T23:57:06.268651 #23752] [0x10e5ff0]  INFO -- : [AWS EC2 200 0.09435 0 retries] describe_addresses(:filters=>[{:name=>\"instance-id\",:values=>[\"i-d0476bb3\"]}])  \nI, [2013-07-10T23:57:06.268896 #23752] [0x10e5ff0]  INFO -- : discovered bosh ip=54.225.99.105\nI, [2013-07-10T23:57:12.087386 #23752] [0x10e5ff0]  INFO -- : bosh_registry is ready on port 25888\n``\n. Deleting the microbosh. The above are the only local logs during the failed upgrade.\n. Hmm, can'tbosh micro delete` it either:\n```\n$ bosh micro delete \nYou are going to delete micro BOSH deployment `firstbosh'.\nTHIS IS A VERY DESTRUCTIVE OPERATION AND IT CANNOT BE UNDONE!\nAre you sure? (type 'yes' to continue): yes\nDelete micro BOSH\n  stopping agent services (00:00:01)                                                              \nUnmount disk                        |ooo                     | 1/7 00:00:30  ETA: --:--:--/usr/local/rvm/gems/ruby-1.9.3-p429/gems/agent_client-1.5.0.pre.804/lib/agent_client/base.rb:21:in method_missing': {\"message\"=>\"Timed out\", \"backtrace\"=>[\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:71:inblock in handle_message'\", \"/var/vcap/bosh/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:67:inhandle_message'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:127:in handle_message'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:120:inblock in '\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1541:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1541:inblock in compile!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:950:in []'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:950:inblock (3 levels) in route!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:966:in route_eval'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:950:inblock (2 levels) in route!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:987:in block in process_route'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:985:incatch'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:985:in process_route'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:948:inblock in route!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:947:in each'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:947:inroute!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1059:in block in dispatch!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:inblock in invoke'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:in catch'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:ininvoke'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1056:in dispatch!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:882:inblock in call!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:in block in invoke'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:incatch'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1041:in invoke'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:882:incall!'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:870:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/xss_header.rb:18:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/path_traversal.rb:16:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/json_csrf.rb:18:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/base.rb:49:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/base.rb:49:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-protection-1.5.0/lib/rack/protection/frame_options.rb:31:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/nulllogger.rb:9:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/head.rb:11:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:175:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:1949:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/builder.rb:138:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/urlmap.rb:65:in block in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/urlmap.rb:50:ineach'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/auth/basic.rb:25:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/rack-1.5.2/lib/rack/commonlogger.rb:33:in call'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/sinatra-1.4.3/lib/sinatra/base.rb:212:incall'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/connection.rb:81:in block in pre_process'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/connection.rb:79:incatch'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/connection.rb:79:in pre_process'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/connection.rb:54:inprocess'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/connection.rb:39:in receive_data'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:inrun_machine'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in run'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/backends/base.rb:63:instart'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/thin-1.5.1/lib/thin/server.rb:159:in start'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:43:instart'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent/http_handler.rb:13:in start'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent.rb:119:instart'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/lib/bosh_agent.rb:86:in run'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.776/bin/bosh_agent:102:in'\", \"/var/vcap/bosh/bin/bosh_agent:23:in load'\", \"/var/vcap/bosh/bin/bosh_agent:23:in'\"]} (Bosh::Agent::HandlerError)\n```\n. It was an issue I discovered, it didn't happen all the time (yay for the\ncloud), so I raised the issue. Do you think anything was added since then\nto ensure it doesn't happen again? Anything that prevents microclouds from\nupgrading 100% is a concern.\n. I haven't seen this again. Perhaps close it until it happens again if no one else has seen it?\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Aug 22, 2013 at 4:26 PM, Ryan Tang notifications@github.com\nwrote:\n\n@tsaleh \u2014 Can you comment on the relative priority of this story?  It appears to be in the icebox of the BOSH tracker.\n@ryantang\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/358#issuecomment-23131038\n. ## Hurray! Can someone update the readme (which has implicit TODOs in it) on usage and stemcell creation etc?\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Mon, Jul 29, 2013 at 3:09 AM, Xiang Kai notifications@github.com\nwrote:\n\nYou can merge this Pull Request by running:\n  git pull https://github.com/cloudfoundry/bosh warden-cpi\nOr you can view, comment on it, or merge it online at:\n  https://github.com/cloudfoundry/bosh/pull/369\n-- Commit Summary --\n- Init commit for warden CPI\n- Support create/delete stemcell in warden CPI\n- Add init db migration for warden CPI\n- Add Bosh agent for Warden\n- Add stemcell builder for warden\n- Add console to talk to Warden CPI\n- Merge \"Add Bosh agent for Warden\" into warden-cpi\n- Merge \"Add stemcell builder for warden\" into warden-cpi\n- Use uuid utils to generate stemcell id\n- Merge \"Use uuid utils to generate stemcell id\" into warden-cpi\n- Support create/delete vm for warden CPI\n- Clean up stemcell if creation failed\n- Deleting stemcell should return nil\n- untar should be running as root\n- Support create/delete disk in warden CPI\n- Support attach/detach disk to/from a VM\n- Add device pool for available device numbers\n- Ignore raised error in rescue\n- chown /var/vcap to vcap in create_vm\n- Clean up temp dir after unit test\n- Raise error when delete vm with disks attached\n- Attach/Detach image to/from loop device\n- Refactor disk related unit test\n- Include Models in console\n- allow loop device rw and mknod in warden\n- Refactor spec\n- Attach image file to device at create time\n- Double quote instead of single quote\n- Refactor cloud error\n- Remove un-necessary TODOs\n- Start agent inside warden when starting\n- Add gem dependency\n- Fix nil field in agent setting\n- Agent setting should merge agent properties\n- Make /var/vcap/bosh writable for all\n- Fix warden agent to create image file\n- Fix warden CPI according to the warden agent fix\n- mkdir -p disk image directory in setup\n- Update agent for warden CPI\n- Warden CPI should save env when attach/detach disk\n- More robust disk usage collector\n- Remove addressed TODOs\n- Clean up if VM creation failed\n- delete_vm returns nil\n- Make director support warden_cpi\n- delete_vm should delete db entry first\n- Use copy_out's owner option\n- get/set agent setting without chmod /var/vcap/bosh\n- Make /tmp rw-able to all in warden\n- Fix for settings data format\n- Use /tmp for temp file\n- Add example manifest for warden CPI\n- Some minor fix in sample manifest\n- Respect TMPDIR, use /tmp inside warden container\n- Merge pull request #23 from anfernee/warden-cpi\n- Merge branch 'master' into warden-cpi\n- Rename warden_cpi -> bosh_warden_cpi\n- Fix Gemfile for bosh_warden_cpi\n- Rename warden_cpi -> bosh_warden_cpi\n- Fix dependencies\n- Remove Gemfile from bosh_warden_cpi\n- spec_helper is in the load path\n- Remove Rakefile\n- Fix spec/unit/attach_disk_spec.rb\n- Fix spec/unit/cloud_spec.rb\n- Fix spec/unit/disk_spec.rb\n- Fix spec/unit/stemcell_spec.rb\n- Style in spec/unit/pool_spec.rb\n- Style in spec/unit/helper_spec.rb\n- Fix spec/unit/vm_spec.rb\n- Clean up unused includes\n- Merge remote-tracking branch 'origin' into warden-cpi\n- Small improvements to the warden-cpi tests\n- Merge branch 'master' into warden-cpi\n- Merge branch 'master' into warden-cpi-wip\n- Merge branch 'warden-cpi-wip' into warden-cpi\n- Woops didn't run specs before commiting.\n- Removed old agent code\n- Remove Rakefile from bosh_warden_cpi gemspec\n- Fix and include migration in bosh_warden_cpi gem\n- wip\n- chmod +x apply.sh\n- Ignore bosh_warden_cpi assets\n- Include bosh_warden_cpi in Gemfile\n- Fix broken merge 700983e by including df79707..2ca8119\n- Add upstart scripts\n- Add bosh-redis upstart script and forward stdout/err to logs\n- bump warden\n- Add startup script for all bosh director jobs\n- Modify deployment manifest for current testing\n- Add a 'bundle install' prescript for warden\n- Change Vagrant OS from precise to lucid\n- Add blobstore shared folder\n- Give the vagrant box more memory\n- Install warden rootfs as part of provisioning\n- Compile ruby from source\n- Compile redis from source\n- Don't need postgres, using sqlite\n- Add additional packages\n- use correct binary for gem\n- Remove unnecessary tasks, create dirs for bosh, and copy in startup scripts\n- Change warden rootfs back to /tmp/warden/rootfs\n- Change uuid for the director\n- Bump warden\n- Update vagrant box info\n- WIP: Everything but a proper stemcell\n- fix broken warden pointer\n- fix broken warden submodule - add .gitmodules\n- update Gemfile\n- [finish #52767005] Add has_vm for warden_cpi\n- add mkfs run path\n- Fix migration file path in warden_cpi spec\n- fix return in has_vm for warden_cpi\n- install lucid kernel patch for warden in warden\n- mock sudo command in the warden_cpi unit test\n- update bosh_warden_cpi.gemspec to refer BOSH_VERSION file\n- folder clean up\n- [#53686051] Initial change to use bind_mount for warden_cpi persistent disk\n- [#53686051] Agent fix to get warden_cpi bind mount persistent disk work\n- Fix has_vm? in warden cpi\n- warden_cpi: Add umount retry guard when umount the persistent disk\n- Merge remote-tracking branch 'origin/master' into warden-cpi\n- update Gemfile.lock and vendor/cache\n- Explicit make the bind-mount point in warden-cpi stemcell\n  -- File Changes --\n  A .gitmodules (4)\n  M Gemfile (1)\n  M Gemfile.lock (70)\n  M bosh-dev/lib/bosh/dev/tasks/gem.rake (1)\n  M bosh-dev/lib/bosh/dev/tasks/stemcell.rake (2)\n  A bosh_agent/lib/bosh_agent/infrastructure/warden.rb (16)\n  A bosh_agent/lib/bosh_agent/infrastructure/warden/settings.rb (26)\n  M bosh_agent/lib/bosh_agent/message/disk.rb (127)\n  M bosh_agent/lib/bosh_agent/platform/linux/disk.rb (6)\n  M bosh_agent/lib/bosh_agent/platform/linux/network.rb (2)\n  A bosh_agent/spec/unit/infrastructure/warden/setting_spec.rb (47)\n  A bosh_warden_cpi/README (16)\n  A bosh_warden_cpi/Vagrantfile (40)\n  A bosh_warden_cpi/bin/console (69)\n  A bosh_warden_cpi/bin/install.sh (49)\n  A bosh_warden_cpi/bosh_warden_cpi.gemspec (29)\n  A bosh_warden_cpi/chef/Cheffile (4)\n  A bosh_warden_cpi/chef/Cheffile.lock (23)\n  A bosh_warden_cpi/chef/director/recipes/install.rb (92)\n  A bosh_warden_cpi/chef/warden/definitions/execute_in_chroot.rb (11)\n  A bosh_warden_cpi/chef/warden/recipes/install.rb (34)\n  A bosh_warden_cpi/chef/warden/recipes/install_rootfs.rb (132)\n  A bosh_warden_cpi/config/warden.yml (41)\n  A bosh_warden_cpi/db/migrations/20130312211408_initial.rb (22)\n  A bosh_warden_cpi/example/dev.yml (233)\n  A bosh_warden_cpi/init/bosh-blobstore.conf (10)\n  A bosh_warden_cpi/init/bosh-director.conf (16)\n  A bosh_warden_cpi/init/bosh-nats.conf (8)\n  A bosh_warden_cpi/init/bosh-redis.conf (8)\n  A bosh_warden_cpi/init/bosh-warden.conf (16)\n  A bosh_warden_cpi/init/bosh-worker.conf (12)\n  A bosh_warden_cpi/init/bosh.conf (11)\n  A bosh_warden_cpi/lib/cloud/warden.rb (40)\n  A bosh_warden_cpi/lib/cloud/warden/cloud.rb (515)\n  A bosh_warden_cpi/lib/cloud/warden/device_pool.rb (36)\n  A bosh_warden_cpi/lib/cloud/warden/helpers.rb (40)\n  A bosh_warden_cpi/lib/cloud/warden/models/disk.rb (5)\n  A bosh_warden_cpi/lib/cloud/warden/models/vm.rb (5)\n  A bosh_warden_cpi/lib/cloud/warden/version.rb (7)\n  A bosh_warden_cpi/spec/assets/stemcell-warden-test.tgz (0)\n  A bosh_warden_cpi/spec/spec_helper.rb (97)\n  A bosh_warden_cpi/spec/unit/attach_disk_spec.rb (173)\n  A bosh_warden_cpi/spec/unit/cloud_spec.rb (9)\n  A bosh_warden_cpi/spec/unit/disk_spec.rb (114)\n  A bosh_warden_cpi/spec/unit/helper_spec.rb (11)\n  A bosh_warden_cpi/spec/unit/pool_spec.rb (52)\n  A bosh_warden_cpi/spec/unit/stemcell_spec.rb (88)\n  A bosh_warden_cpi/spec/unit/vm_spec.rb (204)\n  A bosh_warden_cpi/warden (1)\n  A director/db/migrations/warden_cpi/20121117165132_initial.rb (22)\n  M director/director.gemspec (4)\n  A director/vendor/cache/beefcake-0.3.7.gem (0)\n  A director/vendor/cache/bosh_warden_cpi-0.0.1.gem (0)\n  A director/vendor/cache/warden-client-0.0.7.gem (0)\n  A director/vendor/cache/warden-protocol-0.0.12.gem (0)\n  A stemcell_builder/spec/stemcell-warden.spec (33)\n  A stemcell_builder/stages/base_warden/apply.sh (25)\n  A stemcell_builder/stages/bosh_copy_root/apply.sh (15)\n  D vendor/cache/addressable-2.3.4.gem (0)\n  A vendor/cache/addressable-2.3.5.gem (0)\n  A vendor/cache/beefcake-0.3.7.gem (0)\n  D vendor/cache/ci_reporter-1.8.4.gem (0)\n  A vendor/cache/ci_reporter-1.9.0.gem (0)\n  D vendor/cache/debugger-1.6.0.gem (0)\n  A vendor/cache/debugger-1.6.1.gem (0)\n  D vendor/cache/ffi-1.7.0.gem (0)\n  A vendor/cache/ffi-1.9.0.gem (0)\n  D vendor/cache/jenkins_api_client-0.12.1.gem (0)\n  A vendor/cache/jenkins_api_client-0.13.0.gem (0)\n  D vendor/cache/json-1.7.7.gem (0)\n  A vendor/cache/json-1.8.0.gem (0)\n  A vendor/cache/librarian-0.1.0.gem (0)\n  D vendor/cache/listen-1.0.2.gem (0)\n  A vendor/cache/listen-1.2.2.gem (0)\n  D vendor/cache/lumberjack-1.0.3.gem (0)\n  A vendor/cache/lumberjack-1.0.4.gem (0)\n  D vendor/cache/multi_json-1.7.2.gem (0)\n  A vendor/cache/multi_json-1.7.7.gem (0)\n  D vendor/cache/mysql2-0.3.11.gem (0)\n  A vendor/cache/mysql2-0.3.13.gem (0)\n  D vendor/cache/net-scp-1.1.1.gem (0)\n  A vendor/cache/net-scp-1.1.2.gem (0)\n  D vendor/cache/net-ssh-2.6.7.gem (0)\n  A vendor/cache/net-ssh-2.6.8.gem (0)\n  A vendor/cache/nokogiri-1.5.10.gem (0)\n  D vendor/cache/nokogiri-1.5.6.gem (0)\n  D vendor/cache/parallel-0.6.4.gem (0)\n  A vendor/cache/parallel-0.7.1.gem (0)\n  D vendor/cache/parallel_tests-0.11.3.gem (0)\n  A vendor/cache/parallel_tests-0.15.0.gem (0)\n  D vendor/cache/pg-0.15.1.gem (0)\n  A vendor/cache/pg-0.16.0.gem (0)\n  D vendor/cache/pry-0.9.12.1.gem (0)\n  A vendor/cache/pry-0.9.12.2.gem (0)\n  D vendor/cache/redis-3.0.3.gem (0)\n  A vendor/cache/redis-3.0.4.gem (0)\n  A vendor/cache/rspec-mocks-2.13.0.gem (0)\n  D vendor/cache/rspec-mocks-2.13.1.gem (0)\n  D vendor/cache/rufus-scheduler-2.0.19.gem (0)\n  A vendor/cache/rufus-scheduler-2.0.22.gem (0)\n  D vendor/cache/rugged-0.16.0.gem (0)\n  A vendor/cache/rugged-0.19.0.gem (0)\n  D vendor/cache/timecop-0.6.1.gem (0)\n  A vendor/cache/timecop-0.6.2.2.gem (0)\n  A vendor/cache/warden-client-0.1.0.gem (0)\n  A vendor/cache/warden-protocol-0.1.3.gem (0)\n  D vendor/cache/webmock-1.11.0.gem (0)\n  A vendor/cache/webmock-1.13.0.gem (0)\n  -- Patch Links --\n  https://github.com/cloudfoundry/bosh/pull/369.patch\n  https://github.com/cloudfoundry/bosh/pull/369.diff\n. I think you want the \"p\" method?\n. I think you want the \"p\" method?\n. xoxo\n. xoxo\n. The PR is already ready to be merged into develop\n. The PR is already ready to be merged into develop\n. Do you think there's a higher order feature that is more useful?\n\nOn Wed, Sep 11, 2013 at 10:39 AM, Ferran Rodenas notifications@github.com\nwrote:\n\n@drnic @tsaleh  Added a command to list the current locks, but the way that Bosh sets the timeout doesn't help to figure out how long will take until the lock disappears: locks are renewed automatically if the lock is expired, and they're not released until the process requiring the lock is finished. So not sure if this feature will be useful at all.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/382#issuecomment-24260418\n. Do you think there's a higher order feature that is more useful?\n\nOn Wed, Sep 11, 2013 at 10:39 AM, Ferran Rodenas notifications@github.com\nwrote:\n\n@drnic @tsaleh  Added a command to list the current locks, but the way that Bosh sets the timeout doesn't help to figure out how long will take until the lock disappears: locks are renewed automatically if the lock is expired, and they're not released until the process requiring the lock is finished. So not sure if this feature will be useful at all.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/382#issuecomment-24260418\n. @tsaleh is yours within VPC and is getting the same static IP? or an EC2 VM\nand it got the same IP? That's cool if AWS gives you the same private IP. I\nhadn't tried to reproduce the bug on AWS yet.\n\nThis issue was discovered on OpenStack. So perhaps it is an issue caused\nthere. Its also made worse on openstack as there isn't a workaround yet\nafaik - you can't specify the floating IP of the microbosh as the\nregistry/dns IP because then the microbosh itself can't talk to it -\nsomething about openstack not being able to resolve a floating IP to a\nprivate IP.\n. @tsaleh is yours within VPC and is getting the same static IP? or an EC2 VM\nand it got the same IP? That's cool if AWS gives you the same private IP. I\nhadn't tried to reproduce the bug on AWS yet.\nThis issue was discovered on OpenStack. So perhaps it is an issue caused\nthere. Its also made worse on openstack as there isn't a workaround yet\nafaik - you can't specify the floating IP of the microbosh as the\nregistry/dns IP because then the microbosh itself can't talk to it -\nsomething about openstack not being able to resolve a floating IP to a\nprivate IP.\n. @frodenas & I discovered this just as he was heading off for a week on the\nCanary Islands. When he's back we'll have another chat about solutions.\n. @frodenas & I discovered this just as he was heading off for a week on the\nCanary Islands. When he's back we'll have another chat about solutions.\n. Ahhhhh right I forgot thanks.\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Aug 16, 2013 at 4:02 AM, Chris Brown notifications@github.com\nwrote:\n\nThe --deployment flag is a global flag and so it has to be before the sub command. The command bosh -d manifest.yml deploy should be used for this use case.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/388#issuecomment-22760006\n. Ahhhhh right I forgot thanks.\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Aug 16, 2013 at 4:02 AM, Chris Brown notifications@github.com\nwrote:\n\nThe --deployment flag is a global flag and so it has to be before the sub command. The command bosh -d manifest.yml deploy should be used for this use case.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/388#issuecomment-22760006\n. ## Ok sweet will do.\n\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 13, 2013 at 8:44 AM, Tammer Saleh notifications@github.com\nwrote:\n\nI'd totally merge a pull request that implemented that.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/389#issuecomment-22574397\n. hub pull-request -i XYZ let's me attach a branch to this issue; so no need to close it\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Aug 16, 2013 at 5:21 PM, John Foley notifications@github.com\nwrote:\n\n@drnic - should we go ahead and close this since the a PR will create an issue?\n@aramprice / @jfoley\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/389#issuecomment-22801791\n. hub pull-request -i XYZ let's me attach a branch to this issue; so no need to close it\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Aug 16, 2013 at 5:21 PM, John Foley notifications@github.com\nwrote:\n\n@drnic - should we go ahead and close this since the a PR will create an issue?\n@aramprice / @jfoley\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/389#issuecomment-22801791\n. Ok I'll try that, thx\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Aug 22, 2013 at 1:01 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\n@drnic I believe bundle exec bosh from the root of the project achieves the same effect, this is the workflow used by Pivotal's BOSH team.\nWe think it's best to leave it loadpath management to Rubygems/Bundler to avoid duplicating knowledge that's already in the gemspec.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/397#issuecomment-23114238\n. Ok I'll try that, thx\n\n```\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Thu, Aug 22, 2013 at 1:01 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\n@drnic I believe bundle exec bosh from the root of the project achieves the same effect, this is the workflow used by Pivotal's BOSH team.\nWe think it's best to leave it loadpath management to Rubygems/Bundler to avoid duplicating knowledge that's already in the gemspec.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/397#issuecomment-23114238\n. Can you share your deployment file via a gist perhaps?\n\nThanks!\n. Can you share your deployment file via a gist perhaps?\nThanks!\n. Ok, what you need to do is move postgres & nfs job templates into a shared job (say data/0) and all the other jobs can be collocated (say core/0)\n```\nThe data/0 will need a persistent disk.\nThe core/0 will need a disk too because of the syslog_aggregator job (I think).\nI am unsure if login & uaa can be collocated (for different issues).\u00a0\nEither don't run login (its the pretty SSO web app that's only useful if you have other web apps that need SSO); or run it in its own VM.\n--\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 6:50 PM, stevenbzb notifications@github.com\nwrote:\n\nhi drnic, thank for your comment.  Here is some text of  my deployments/cf.yml:\nname: cf\ndirector_uuid: 9522af13-558b-43a2-90a9-cf788a40e94a\nreleases:\n- name: appcloud\n  version: latest\n  compilation:\n  workers: 6\n  network: default\n  reuse_compilation_vms: true\n  cloud_properties:\n    instance_type: m1.medium\n  update:\n  canaries: 1\n  canary_watch_time: 30000-600000\n  update_watch_time: 30000-600000\n  max_in_flight: 4\n  max_errors: 1\n  networks:\n  - name: floating\n    type: vip\n    cloud_properties: {}\n  - name: default\n    type: dynamic\n    cloud_properties:\n    security_groups:\n    - default\n    - cf\n      resource_pools:\n  - name: dea\n    network: default\n    size: 1\n    stemcell:\n      name: bosh-stemcell\n      version: latest\n    cloud_properties:\n      instance_type: m1.small\n  - name: medium\n    network: default\n    size: 2\n    stemcell:\n      name: bosh-stemcell\n      version: latest\n    cloud_properties:\n      instance_type: m1.medium\n    jobs:\n  - name: core\n    template:\n    - syslog_aggregator\n    - nats\n    - postgres\n    - health_manager_next\n    - collector\n    - debian_nfs_server\n    - uaa\n    - login\n      instances: 1\n      resource_pool: medium\n      persistent_disk: 8192\n      networks:\n      - name: default\n        default:\n    - dns\n    - gateway\n      properties:\n      db: databases\n  - name: api\n    template:\n    - cloud_controller_ng\n    - gorouter\n      instances: 1\n      resource_pool: medium\n      networks:\n      - name: default\n        default:\n    - dns\n    - gateway\n      - name: floating\n        static_ips:\n    - 10.141.123.245\n      properties:\n      db: databases\n  - name: dea\n    template:\n    - dea_next\n      instances: 1\n      resource_pool: dea\n      networks:\n    - name: default\n      default: [dns, gateway]\n      properties:\n      domain: huawei.com\n      system_domain: huawei.com\n      system_domain_organization: \"mycloud\"\n      app_domains:\n      - huawei.com\n        networks:\n        apps: default\n        management: default\n        nats:\n        address: 0.core.default.cf.microbosh\n        port: 4222\n        user: nats\n        password: \"c1oudc0w\"\n        authorization_timeout: 5\n        router:\n        port: 8081\n        status:\n        port: 8080\n        user: gorouter\n        password: \"c1oudc0w\"\n        dea: &dea\n        max_memory: 4096\n        memory_mb: 4084\n        memory_overcommit_factor: 4\n        disk_mb: 4096\n        disk_overcommit_factor: 4\n        dea_next: dea\n        service_lifecycle:\n        serialization_data_server:\n      - 0.core.default.cf.microbosh\n        syslog_aggregator:\n        address: 0.core.default.cf.microbosh\n        port: 54321\n        serialization_data_server:\n        port: 8080\n        logging_level: debug\n        upload_token: 8f7COGvThwlmulIzAgOHxMXurBrG364k\n        upload_timeout: 10\n        collector:\n        deployment_name: cf\n        use_tsdb: false\n        use_aws_cloudwatch: false\n        use_datadog: false\n        nfs_server:\n        address: 0.core.default.cf.microbosh\n        #network: \".cf.microbosh\"\n        #idmapd_domain: iad1\n        debian_nfs_server:\n        no_root_squash: true\n        databases: &databases\n        db_scheme: postgres\n        address: 0.core.default.cf.microbosh\n        port: 5524\n        roles:\n    - tag: admin\n      name: ccadmin\n      password: \"c1oudc0w\"\n    - tag: admin\n      name: uaaadmin\n      password: \"c1oudc0w\"\n      databases:\n    - tag: cc\n      name: ccdb\n      citext: true\n    - tag: uaa\n      name: uaadb\n      citext: true\n      .................\n      .................\n      Command \"grep '<%= p(\"' jobs//templates/ | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\n      ccdb_postgres\n      cloud_controller_ng\n      collector\n      dashboard\n      dea_logging_agent\n      dea_next\n      debian_nfs_server\n      gorouter\n      health_manager_next\n      loggregator\n      login\n      saml_login\n      syslog_aggregator\n      uaa\n      Command \"grep 'properties:' jobs/*/spec | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\n      ccdb_postgres\n      cloud_controller_ng\n      collector\n      dashboard\n      dea_logging_agent\n      dea_next\n      debian_nfs_server\n      gorouter\n      health_manager_next\n      loggregator\n      login\n      nats\n      postgres\n      saml_login\n      syslog_aggregator\n      uaa\n      ---\n      Reply to this email directly or view it on GitHub:\n      https://github.com/cloudfoundry/bosh/issues/398#issuecomment-22990651\n. Ok, what you need to do is move postgres & nfs job templates into a shared job (say data/0) and all the other jobs can be collocated (say core/0)\n\n```\nThe data/0 will need a persistent disk.\nThe core/0 will need a disk too because of the syslog_aggregator job (I think).\nI am unsure if login & uaa can be collocated (for different issues).\u00a0\nEither don't run login (its the pretty SSO web app that's only useful if you have other web apps that need SSO); or run it in its own VM.\n--\n```\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Tue, Aug 20, 2013 at 6:50 PM, stevenbzb notifications@github.com\nwrote:\n\nhi drnic, thank for your comment.  Here is some text of  my deployments/cf.yml:\nname: cf\ndirector_uuid: 9522af13-558b-43a2-90a9-cf788a40e94a\nreleases:\n- name: appcloud\n  version: latest\n  compilation:\n  workers: 6\n  network: default\n  reuse_compilation_vms: true\n  cloud_properties:\n    instance_type: m1.medium\n  update:\n  canaries: 1\n  canary_watch_time: 30000-600000\n  update_watch_time: 30000-600000\n  max_in_flight: 4\n  max_errors: 1\n  networks:\n  - name: floating\n    type: vip\n    cloud_properties: {}\n  - name: default\n    type: dynamic\n    cloud_properties:\n    security_groups:\n    - default\n    - cf\n      resource_pools:\n  - name: dea\n    network: default\n    size: 1\n    stemcell:\n      name: bosh-stemcell\n      version: latest\n    cloud_properties:\n      instance_type: m1.small\n  - name: medium\n    network: default\n    size: 2\n    stemcell:\n      name: bosh-stemcell\n      version: latest\n    cloud_properties:\n      instance_type: m1.medium\n    jobs:\n  - name: core\n    template:\n    - syslog_aggregator\n    - nats\n    - postgres\n    - health_manager_next\n    - collector\n    - debian_nfs_server\n    - uaa\n    - login\n      instances: 1\n      resource_pool: medium\n      persistent_disk: 8192\n      networks:\n      - name: default\n        default:\n    - dns\n    - gateway\n      properties:\n      db: databases\n  - name: api\n    template:\n    - cloud_controller_ng\n    - gorouter\n      instances: 1\n      resource_pool: medium\n      networks:\n      - name: default\n        default:\n    - dns\n    - gateway\n      - name: floating\n        static_ips:\n    - 10.141.123.245\n      properties:\n      db: databases\n  - name: dea\n    template:\n    - dea_next\n      instances: 1\n      resource_pool: dea\n      networks:\n    - name: default\n      default: [dns, gateway]\n      properties:\n      domain: huawei.com\n      system_domain: huawei.com\n      system_domain_organization: \"mycloud\"\n      app_domains:\n      - huawei.com\n        networks:\n        apps: default\n        management: default\n        nats:\n        address: 0.core.default.cf.microbosh\n        port: 4222\n        user: nats\n        password: \"c1oudc0w\"\n        authorization_timeout: 5\n        router:\n        port: 8081\n        status:\n        port: 8080\n        user: gorouter\n        password: \"c1oudc0w\"\n        dea: &dea\n        max_memory: 4096\n        memory_mb: 4084\n        memory_overcommit_factor: 4\n        disk_mb: 4096\n        disk_overcommit_factor: 4\n        dea_next: dea\n        service_lifecycle:\n        serialization_data_server:\n      - 0.core.default.cf.microbosh\n        syslog_aggregator:\n        address: 0.core.default.cf.microbosh\n        port: 54321\n        serialization_data_server:\n        port: 8080\n        logging_level: debug\n        upload_token: 8f7COGvThwlmulIzAgOHxMXurBrG364k\n        upload_timeout: 10\n        collector:\n        deployment_name: cf\n        use_tsdb: false\n        use_aws_cloudwatch: false\n        use_datadog: false\n        nfs_server:\n        address: 0.core.default.cf.microbosh\n        #network: \".cf.microbosh\"\n        #idmapd_domain: iad1\n        debian_nfs_server:\n        no_root_squash: true\n        databases: &databases\n        db_scheme: postgres\n        address: 0.core.default.cf.microbosh\n        port: 5524\n        roles:\n    - tag: admin\n      name: ccadmin\n      password: \"c1oudc0w\"\n    - tag: admin\n      name: uaaadmin\n      password: \"c1oudc0w\"\n      databases:\n    - tag: cc\n      name: ccdb\n      citext: true\n    - tag: uaa\n      name: uaadb\n      citext: true\n      .................\n      .................\n      Command \"grep '<%= p(\"' jobs//templates/ | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\n      ccdb_postgres\n      cloud_controller_ng\n      collector\n      dashboard\n      dea_logging_agent\n      dea_next\n      debian_nfs_server\n      gorouter\n      health_manager_next\n      loggregator\n      login\n      saml_login\n      syslog_aggregator\n      uaa\n      Command \"grep 'properties:' jobs/*/spec | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\n      ccdb_postgres\n      cloud_controller_ng\n      collector\n      dashboard\n      dea_logging_agent\n      dea_next\n      debian_nfs_server\n      gorouter\n      health_manager_next\n      loggregator\n      login\n      nats\n      postgres\n      saml_login\n      syslog_aggregator\n      uaa\n      ---\n      Reply to this email directly or view it on GitHub:\n      https://github.com/cloudfoundry/bosh/issues/398#issuecomment-22990651\n. Ryan, the backlog is visible at http://cftracker.cfapps.io/bosh; not the\nicebox though.\n. Ryan, the backlog is visible at http://cftracker.cfapps.io/bosh; not the\nicebox though.\n. :love_letter: \n. :love_letter: \n. Can you suggest a test?\n\nOn Fri, Aug 30, 2013 at 10:27 AM, Chris Brown notifications@github.com\nwrote:\n\n@drnic \u2014 Thanks for this bugfix! Could you add some tests that check that this bug remains squashed in the future? Cheers.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/404#issuecomment-23576785\n. Can you suggest a test?\n\nOn Fri, Aug 30, 2013 at 10:27 AM, Chris Brown notifications@github.com\nwrote:\n\n@drnic \u2014 Thanks for this bugfix! Could you add some tests that check that this bug remains squashed in the future? Cheers.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/404#issuecomment-23576785\n. Mate, I know what tests are for :)\n\n```\nI was hoping that Chris, who'd just looked at the code, could spot a test he was thinking he wanted.\nI will come back to the PR later.\n```\nOn Fri, Aug 30, 2013 at 5:36 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\nA good test would be one that demonstrates the bugs existence if the fix is commented so it's obvious why the change is needed. Thanks!\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/404#issuecomment-23597021\n. Mate, I know what tests are for :)\n\n```\nI was hoping that Chris, who'd just looked at the code, could spot a test he was thinking he wanted.\nI will come back to the PR later.\n```\nOn Fri, Aug 30, 2013 at 5:36 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\nA good test would be one that demonstrates the bugs existence if the fix is commented so it's obvious why the change is needed. Thanks!\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/404#issuecomment-23597021\n. OK, extracted the menu builder into a jobs_and_indexes which is now tested; removed now unused single_job_index_option? method\n. OK, extracted the menu builder into a jobs_and_indexes which is now tested; removed now unused single_job_index_option? method\n. ```\n\n~/bosh-workspace/bosh/bin/bosh ssh\n\npostgres/0\nnfs_server/0\nnats/0\nsyslog_aggregator/0\nuaa/0\ncloud_controller/0\nrouter/0\nhealth_manager/0\ndea/0\nChoose an instance: \n```\n\nLooks lovely.\n. ```\n~/bosh-workspace/bosh/bin/bosh ssh\n\npostgres/0\nnfs_server/0\nnats/0\nsyslog_aggregator/0\nuaa/0\ncloud_controller/0\nrouter/0\nhealth_manager/0\ndea/0\nChoose an instance: \n```\n\nLooks lovely.\n. Ok switched from escape gem to Shellwords.shelljoin; thanks for that.\n. Ok switched from escape gem to Shellwords.shelljoin; thanks for that.\n. I've got a hard coded \"vcap\". Need to fix that too.\n. I've got a hard coded \"vcap\". Need to fix that too.\n. I'm not sure if anyone ever overrides vcap for ssh_user for aws or openstack. Nonetheless I think I need to load up InstanceManager.create to get the ssh_user etc.\n. I'm not sure if anyone ever overrides vcap for ssh_user for aws or openstack. Nonetheless I think I need to load up InstanceManager.create to get the ssh_user etc.\n. Ok, I like this now. It will use ssh_user from the cloud plugin properties if available.\n. Ok, I like this now. It will use ssh_user from the cloud plugin properties if available.\n. @hiremaga refactored the green/red idea into a #color_value helper\n. @hiremaga refactored the green/red idea into a #color_value helper\n. Ooh, need to fix CI - https://travis-ci.org/cloudfoundry/bosh/jobs/10825728#L294\n. Ooh, need to fix CI - https://travis-ci.org/cloudfoundry/bosh/jobs/10825728#L294\n. Self-bump.\nJust wanted to beat the community pair to it.\n. Self-bump.\nJust wanted to beat the community pair to it.\n. I'll close this ticket now. It was a little exercise in prettiness. I'm sorry I can't make the time to rewrite it and all the tests. \n. I'll close this ticket now. It was a little exercise in prettiness. I'm sorry I can't make the time to rewrite it and all the tests. \n. Lovely.\n. Lovely.\n. I just wanted you to start caring about 2.0.0. It's the present; 1.9.3 is\nthe past :) I thought bosh was made to work on 2.0.0; but then I saw that\ntravis ignored 2.0.0 errors.\u200b\n. I just wanted you to start caring about 2.0.0. It's the present; 1.9.3 is\nthe past :) I thought bosh was made to work on 2.0.0; but then I saw that\ntravis ignored 2.0.0 errors.\u200b\n. I believe the core issue was that bosh wants to start the jobs one at a time because they have dependencies on each other.\nHaving said that, there is a real issue - I don't think it starts them up in the right order (as per a normal deploy).\nOn Mon, Sep 30, 2013 at 11:21 AM, Mark Rushakoff notifications@github.com\nwrote:\n\n@tsaleh is this a feature you want prioritized?\n@mark-rushakoff / @d\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/417#issuecomment-25388693\n. I believe the core issue was that bosh wants to start the jobs one at a time because they have dependencies on each other.\n\nHaving said that, there is a real issue - I don't think it starts them up in the right order (as per a normal deploy).\nOn Mon, Sep 30, 2013 at 11:21 AM, Mark Rushakoff notifications@github.com\nwrote:\n\n@tsaleh is this a feature you want prioritized?\n@mark-rushakoff / @d\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/417#issuecomment-25388693\n. Updated the Issue title to \"bosh cck restarts jobs in a (random?) order instead of the order of the deployment spec\"\n. Updated the Issue title to \"bosh cck restarts jobs in a (random?) order instead of the order of the deployment spec\"\n. xoxox\n. Unsure that any bosh core team every assigned to look at it. Would appreciate it being merged. Any suggestions for improvements could go into future PRs.\n\nOn Tue, Sep 10, 2013 at 1:46 PM, Ferran Rodenas notifications@github.com\nwrote:\n\nFixes #282 \nYou can merge this Pull Request by running:\n  git pull https://github.com/cloudfoundry/bosh dev_name\nOr you can view, comment on it, or merge it online at:\n  https://github.com/cloudfoundry/bosh/pull/419\n-- Commit Summary --\n- [cli] Create dev release defaults to final name\n  -- File Changes --\n  M bosh_cli/lib/cli/commands/release.rb (7)\n  M bosh_cli/spec/unit/commands/release_spec.rb (76)\n  -- Patch Links --\n  https://github.com/cloudfoundry/bosh/pull/419.patch\n  https://github.com/cloudfoundry/bosh/pull/419.diff\n. I think this is a new requirement since the PR was submitted in Sept last year? ;)\n\nOn Wed, Mar 12, 2014 at 5:04 PM, Derek Richard notifications@github.com\nwrote:\n\nHi @frodenas,\nThanks for submitting this pull request. If possible, we would like you to change your uses of double to instance_double or class_double and your uses of stub to allow. Please also change your expectations to use the expect style rather than should. When those changes are made we would be happy to merge in your changes.\nCF Community Pair\n@calebamiles, @drich10\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/419#issuecomment-37464966\n. Awesome\n\nOn Fri, Apr 4, 2014 at 5:47 PM, Maria Shaldibina notifications@github.com\nwrote:\n\nClosed #419.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/419\n. You are a lover of the people!\n. Yep, thanks. The task is still running; but the bosh CLI makes it look like it stopped.\n\nOn Mon, Sep 30, 2013 at 11:02 AM, Mark Rushakoff notifications@github.com\nwrote:\n\nSo you want the task to stay attached if there is a REST API call exception like that?  We will create a feature in the BOSH icebox for that.\n@mark-rushakoff / @d\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/428#issuecomment-25387074\n. Jesse; that's how I ultimately found that hidden error message.\n\nStory: As an end user, I'd like errors that tell me what the error is.\nIf \"HTTP 413 Entity Too Large\" always maps to rate limit exceeded; then can\nwe show that message in the CLI output please? BOSH isn't just for expert\nbosh users :)\nNic\n. @rkoster I think you're referring to the configuration of the bosh blobstore used for source blobs & final release blobs. This PR is for blobstore configuration of a running bosh/microbosh (to store the uploaded release blobs & compiled packages)\n. Thanks for the update/feedback.\nI'll add the tests. I'll think more on your feedback on examples.\nOn Thu, Nov 21, 2013 at 11:46 AM, Rob Day-Reynolds\nnotifications@github.com wrote:\n\nA couple of things:\ndirector.yml.erb.erb changes should be tested in release/spec/director.yml.erb.erb_spec.rb to confirm that the logic is being properly parsed.\nAlso, it probably isn't necessary to include an entire example if only the blobstore section has changed from other OpenStack examples.  Perhaps only include the section required to change, and a comment at the top of the snippet.\n@monkeyherder & @cppforlife\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/450#issuecomment-29016928\n. Yes\n\nOn Sun, Dec 1, 2013 at 10:49 PM, James Bayer notifications@github.com\nwrote:\n\n@drnic this story is relatively high in the bosh backlog and marked as blocked waiting for some updates from you. are you planning on continuing this?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/450#issuecomment-29597705\n. Afaik there is no rackspace support by bosh? I know we've (@frodenas) used Hp blobstore at one client.\n\nOn Tue, Dec 10, 2013 at 11:48 AM, Tony Hansmann notifications@github.com\nwrote:\n\n@drnic @jbayer LGTU - waiting for an answer the 'rackspace' intentions - we're good either way, just want to clarify. \nJZ && TH\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/450#issuecomment-30261255\n. I wasn't \"not supporting\" it. Someone else who cares about rackspace can add config for it later. Can we merge the PR as it stands and rackspace users add RS support later?\n\nOn Tue, Dec 10, 2013 at 1:20 PM, Ferran Rodenas notifications@github.com\nwrote:\n\nRackspace Cloud Files is supported at the blobstore_client. If we drop support here, we should also drop it at the blobstore_client and bosh_cli.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/450#issuecomment-30268983\n. Leery of what? Want me to word it differently and less leerily?\n\nNo one had allowed any Swift support in the bosh release/microbosh. I've enabled two. I only needed one. I don't know anything about rackspace.\u00a0\nIf it makes you happy and less leeriful then I can make up something for rackspace. But not 100% sure how it became my task to do it in this PR just so we can use bosh/swift on our generic OpenStack.\nOn Tue, Dec 10, 2013 at 5:49 PM, Tony Hansmann notifications@github.com\nwrote:\n\n@drnic, @frodenas comments makes us leery, we've got a request out to @tsaleh for a product ruling.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/450#issuecomment-30287222\n. PowerDNS is needed for deployments being tested/developed that use powerdns in production.\n\nOn Thu, Nov 14, 2013 at 5:44 PM, Xiang Kai notifications@github.com\nwrote:\n\nsince we don't have powerdns running for bosh-lite and there is no need for it (right now).\nso the dns is disabled, we want the dns related test pending, and bat already have this mechanism but just broken.\nthink this is a general bug for the BAT test, so sent a PR.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/457#issuecomment-28541101\n. @rkoster I think they'll ask for a test case or more to go with the new code\n. Nice! Thanks for finishing this PR!\n\nOn Mon, Aug 18, 2014 at 2:11 PM, Kris Hicks notifications@github.com\nwrote:\n\n@rkoster https://github.com/rkoster This has been merged in as 55ef66d\nhttps://github.com/cloudfoundry/bosh/commit/55ef66d8fe4fdb334e37b54f9cf240815e0c5b71\n.\nCheers,\n@krishicks https://github.com/krishicks and @adamstegman\nhttps://github.com/adamstegman\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/458#issuecomment-52555830.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. If I'm blocking the merge, I'm sorry. Afai remember, the patched method only needed to test is disk_cid existed already, not the others.\nOn Thu, Jan 2, 2014 at 3:12 PM, Tony Hansmann notifications@github.com\nwrote:\n\nHi @drnic,\nWe're looking at this and bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb defines exists?:\n    def exists?\n      [state.vm_cid, state.stemcell_cid, state.disk_cid].any?\n    end\nIt checks that state.vm_cid is defined. Your PR is the same behavior we had, but narrows the cases where we'll try to update. Is there an issue checking the other two?\n@thansmann && @ytolsk\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/466#issuecomment-31493288\n. Sorry this new exists? looks new to me. I'm on my phone. Will investigate later.\n\nOn Thu, Jan 2, 2014 at 3:12 PM, Tony Hansmann notifications@github.com\nwrote:\n\nHi @drnic,\nWe're looking at this and bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb defines exists?:\n    def exists?\n      [state.vm_cid, state.stemcell_cid, state.disk_cid].any?\n    end\nIt checks that state.vm_cid is defined. Your PR is the same behavior we had, but narrows the cases where we'll try to update. Is there an issue checking the other two?\n@thansmann && @ytolsk\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/466#issuecomment-31493288\n. Yep, they aren't necessary at all for the scenario and they prevent the resurrection of microbosh from just a disk.\n\nOn Thu, Jan 2, 2014 at 3:12 PM, Tony Hansmann notifications@github.com\nwrote:\n\nHi @drnic,\nWe're looking at this and bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb defines exists?:\n    def exists?\n      [state.vm_cid, state.stemcell_cid, state.disk_cid].any?\n    end\nIt checks that state.vm_cid is defined. Your PR is the same behavior we had, but narrows the cases where we'll try to update. Is there an issue checking the other two?\n@thansmann && @ytolsk\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/466#issuecomment-31493288\n. @thansmann @ytolsk looking at the PR I think its still correct - in that method only vm_cid is important. If there is no vm, then there's no IP to detect. This is what was causing bugs. If only the disk cid exists, then it shouldn't fail. I don't think the alternate exists? is correct here.\n. Oops, clicked wrong button.\n. @thansmann I've reviewed the code on HEAD and I still don't think the current code on HEAD is correct. state.vm_cid must always have a value within this method (cloud.openstack.servers.get(state.vm_cid).floating_ip_address will fail otherwise). So the if statement should ensure that state.vm_cid has a value.\n\nConversely, this if statement doesn't care if any other of the current exists? list are set or not.\nThis method only needs to test state.vm_cid and no others.\n. I think this needs fixing in OpenStack\nOn Thu, Dec 12, 2013 at 9:32 AM, Tammer Saleh notifications@github.com\nwrote:\n\n\nThere is no way to name a VM.\nDo you mean in bosh or in openstack?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/472#issuecomment-30443519\n. Which one?\n\n\nOn Wed, Dec 11, 2013 at 4:01 PM, Rama Seshadri notifications@github.com\nwrote:\n\nBosh Blobstore is a single point of failure.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/476\n. There is no nfs for bosh; that's CF. For bosh we have nginx/dav; which probably doesn't scale to multiple nodes; so use s3 or swift (though still blocked on requests for more tests for swift support to be allowed on microbosh & bosh release)\n\nOn Mon, Dec 16, 2013 at 10:41 AM, Casey McTaggart\nnotifications@github.com wrote:\n\nHi @rd7869,\nThe BOSH Blobstore can be any one of S3, Swift, NFS, or a testing-only local blobstore. S3 is pretty hard to bring down. It may have avaliablity issues, but it's got 11 nines of data protection (S3 info). Swift can be configured to the nines you need. A clustered NFS like Isilon will easily give you 9-nines. \nThe local blobstore is a single-point of failure. We don't recommend it.\nCan you tell us more about your use case?\n~ @thansmann & @caseymct\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/476#issuecomment-30687376\n. It looks like the original exception has all the important information (name/index, via \"origin\"; and the message) and could be simply passed back up through.\n. Attached PR now provides an \"origin\" to the error handler to indicated to the end user where the error originated, since there is no relevant \"name/index\" during the ResourcePoolUpdater refill sequence.\n. @goehmen yes afaik it is still necessary and valid. Sorry, the PR is 2 mths old. I'm forgetful.\n. Its a long time since I wrote this so I have to guess why I didn't feel new tests were necessary. I believe it was because this is a refactoring/improvement within the confines of tests that already existed.\n\nIf you'd like additional tests, I'm super happy for you to add them as part of the merge.\nThis is a bug fix. Hopefully its useful to other users in time.\n. I'll close this issue now; and when someone else spots the same issue perhaps Google will find this for them. Perhaps they'll have the time to write up tests. I'm sorry I didn't have the time when I unearthed the bug. Bug hunting mentality often isn't the same mentality as \"write lots of new tests\" mentality.\n. Someone's write up on how to install CPAN on centos - http://forum.directadmin.com/showthread.php?t=42795\n. Did pivotal turn off our access keys or the AWS account they gave us to upload the cf-services-contrib-release blobs? @jwatters was very sure he wanted it in @cloudfoundry org and Pivotal's own s3 buckets.\nOn Fri, Jan 17, 2014 at 10:30 AM, David Julia notifications@github.com\nwrote:\n\n@rkoster I also just confirmed that Cloud Foundry as an organization no longer supports cf-contrib or manages the associated AWS accounts- @drnic will probably be able to check your privileges on the cf-contrib bucket for you.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/494#issuecomment-32631925\n. Did pivotal turn off our access keys or the AWS account they gave us to upload the cf-services-contrib-release blobs? @jwatters was very sure he wanted it in @cloudfoundry org and Pivotal's own s3 buckets.\n\nOn Fri, Jan 17, 2014 at 10:30 AM, David Julia notifications@github.com\nwrote:\n\n@rkoster I also just confirmed that Cloud Foundry as an organization no longer supports cf-contrib or manages the associated AWS accounts- @drnic will probably be able to check your privileges on the cf-contrib bucket for you.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/494#issuecomment-32631925\n. Perhaps create a new ticket for the \"better error message\"? And close this one?\n\nOn Fri, Jan 17, 2014 at 12:31 PM, Ruben Koster notifications@github.com\nwrote:\n\nIt indeed was a problem with write access. After regenerating keys I was able to create a final release. It would however still be nice if bosh could give a clearer error message. In the case of an access denied.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/494#issuecomment-32643626\n. Perhaps create a new ticket for the \"better error message\"? And close this one?\n\nOn Fri, Jan 17, 2014 at 12:31 PM, Ruben Koster notifications@github.com\nwrote:\n\nIt indeed was a problem with write access. After regenerating keys I was able to create a final release. It would however still be nice if bosh could give a clearer error message. In the case of an access denied.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/494#issuecomment-32643626\n. Another example would be PistonCloud's S3-compliant API.\n. Another example would be PistonCloud's S3-compliant API.\n. I would like this too. It's how we're doing CF blobstore configuration - any fog_connection can be provided.\n\nOn Fri, Jan 17, 2014 at 4:00 PM, Ryan Grenz notifications@github.com\nwrote:\n\nIt'd be really nice if we could somehow just push a hash from a BOSH manifest directly into aws_options in https://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb#L54 but blobstore_client doesn't currently facilitate that.\nI am finding I need to use :ssl_verify_peer and :s3_force_path_style to use Cleversafe's object store product, which I've hacked into aws_options to test on a temporary basis. \nI still haven't got blobstore create file working yet, but the point is it'd be nice to not have to specify every configuration option under the sun in a bosh manifest/spec/config template. I guess the same argument was presented when the Swift blobstore development happened in cc_ng?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/495#issuecomment-32665349\n. Ryan, that doesn't sound 100% right. Stemcells are all prepared for being a microbosh; where the blobstore is a local file system of bosh release packages. They are all also ready to be normal bosh stemcells - in which case the bosh tells it where the blobstore is and what type.\n\nOn Mon, Jan 20, 2014 at 7:42 AM, Ryan Grenz notifications@github.com\nwrote:\n\nAlso .. annoyingly .. the stemcell bakes in the blobstore client, so any changes I make in my forked BOSH repo won't reflect in the compilation VMs agent code .. I guess we can make our own stemcells, but separating agent code from the stemcell is on the roadmap I believe? Can anyone tell me when this is likely to be addressed please?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/495#issuecomment-32769862\n. Haha. Was just breaking them out into two separate issues :)\n\nOn Thu, Jan 16, 2014 at 12:21 PM, David Julia notifications@github.com\nwrote:\n\nClosed #499.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/499\n. Haha. Was just breaking them out into two separate issues :)\n\nOn Thu, Jan 16, 2014 at 12:21 PM, David Julia notifications@github.com\nwrote:\n\nClosed #499.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/499\n. We're going with the solution in PCF (metadata file) instead of spiff? Hopefully because it's better; not just because it was the \"on hours\" implementation.\n\nspiff is far better than keeping around examples which aren't tested and isn't obvious which bits to keep or override. And it exists today and is being used by many bosh releases.\nIf the proposed alternate doesn't exist yet; then I'd continue to request we create and support spiff templates asap.\nIn the future when the alternate exists and is functional then the spiff templates can be deprecated.\nOn Fri, Jan 17, 2014 at 11:25 AM, Tammer Saleh notifications@github.com\nwrote:\n\nHey Nic,\nSpiff was an off-hours temporary solution to the 'how do i configure this release' problem.  A supported solution has been put out commercially in pivotal cf operations manager, and we'd like to OSS it into BOSH.  A design doc is forthcoming.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/500#issuecomment-32638217\n. We're going with the solution in PCF (metadata file) instead of spiff? Hopefully because it's better; not just because it was the \"on hours\" implementation.\n\nspiff is far better than keeping around examples which aren't tested and isn't obvious which bits to keep or override. And it exists today and is being used by many bosh releases.\nIf the proposed alternate doesn't exist yet; then I'd continue to request we create and support spiff templates asap.\nIn the future when the alternate exists and is functional then the spiff templates can be deprecated.\nOn Fri, Jan 17, 2014 at 11:25 AM, Tammer Saleh notifications@github.com\nwrote:\n\nHey Nic,\nSpiff was an off-hours temporary solution to the 'how do i configure this release' problem.  A supported solution has been put out commercially in pivotal cf operations manager, and we'd like to OSS it into BOSH.  A design doc is forthcoming.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/500#issuecomment-32638217\n. https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/api/controllers/stemcells_controller.rb#L23 is where this data comes from \n. https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/api/controllers/stemcells_controller.rb#L23 is where this data comes from \n. I think this deployments field is only being used by the CLI to indicated whether a stemcell is or isn't being used by any deployments. Perhaps we change it to deployments_count and return a number. Or return a list of deployment names?\n. I think this deployments field is only being used by the CLI to indicated whether a stemcell is or isn't being used by any deployments. Perhaps we change it to deployments_count and return a number. Or return a list of deployment names?\n. @tsaleh PR is attached. For the CLI code it's specs reference the old API output and the new API output and test for support for both.\n. I rethought the solution. Let's keep deployments as an Array; instead of an array of junk, it can be an array of deployment names. Shouldn't break compatibility.\n\nOn Fri, Jan 31, 2014 at 3:35 PM, goehmen notifications@github.com wrote:\n\n@mmb @ytolsk  @tsaleh  - No, it's not OK to break backward compatibility.  @drnic - Can you resubmit code change that is backwards compatible?  Glad to have a conversation to get on the same page.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/501#issuecomment-33853808\n. I rethought the solution. Let's keep deployments as an Array; instead of an array of junk, it can be an array of deployment names. Shouldn't break compatibility.\n\nOn Fri, Jan 31, 2014 at 3:35 PM, goehmen notifications@github.com wrote:\n\n@mmb @ytolsk  @tsaleh  - No, it's not OK to break backward compatibility.  @drnic - Can you resubmit code change that is backwards compatible?  Glad to have a conversation to get on the same page.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/501#issuecomment-33853808\n. Going to create new PR\n. Going to create new PR\n. Solution now in #512 \n. Solution now in #512 \n. Neat - I'd wondered how to just update a single gem in a Gemfile.lock.\n\nOn Fri, Jan 24, 2014 at 5:29 PM, August Toman-Yih\nnotifications@github.com wrote:\n\n@mrdavidlaing We were able to get apparent success by using bundle update --source aws-sdk, which should update only that gem and none of its dependencies\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-33277305\n. Neat - I'd wondered how to just update a single gem in a Gemfile.lock.\n\nOn Fri, Jan 24, 2014 at 5:29 PM, August Toman-Yih\nnotifications@github.com wrote:\n\n@mrdavidlaing We were able to get apparent success by using bundle update --source aws-sdk, which should update only that gem and none of its dependencies\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-33277305\n. Sexy!\n. Sexy!\n. I'm not aware of the effort required to support ruby 2.1.0 for the CLI. I'm\nnot sure if its a priority. Thought I'd let you know it doesn't work.\n. I'm not aware of the effort required to support ruby 2.1.0 for the CLI. I'm\nnot sure if its a priority. Thought I'd let you know it doesn't work.\n. \"as intended\" isn't corrector helpful \u00a0- ruby 2.1.0 is the current ruby people are using on their machines. Only for bosh CLI do I need to constantly switch back to legacy rubies.\n\nOn Tue, Mar 11, 2014 at 3:41 AM, Jesse Zhang notifications@github.com\nwrote:\n\nThis is working as intended: the BOSH CLI is not designed to run on Ruby 2.1 (yet). We do have a little future-proofing in our CI that runs the same builds under Ruby 2.0 but they are currently all broken.\nFWIW I am running the BOSH CLI in Ruby 2.0 (this is unsupported behavior) and my production environment is not blown up (yet).\nBefore we follow Vagrant's precedence to ship our own Ruby, is this still an issue for you, @matjohn2 ? Can we close this?\nJesse\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/511#issuecomment-37270082\n. \"as intended\" isn't corrector helpful \u00a0- ruby 2.1.0 is the current ruby people are using on their machines. Only for bosh CLI do I need to constantly switch back to legacy rubies.\n\nOn Tue, Mar 11, 2014 at 3:41 AM, Jesse Zhang notifications@github.com\nwrote:\n\nThis is working as intended: the BOSH CLI is not designed to run on Ruby 2.1 (yet). We do have a little future-proofing in our CI that runs the same builds under Ruby 2.0 but they are currently all broken.\nFWIW I am running the BOSH CLI in Ruby 2.0 (this is unsupported behavior) and my production environment is not blown up (yet).\nBefore we follow Vagrant's precedence to ship our own Ruby, is this still an issue for you, @matjohn2 ? Can we close this?\nJesse\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/511#issuecomment-37270082\n. In ok with Dmitry's idea. Sounds great that there's another example to borrow from for consistency.\n\nOn Mon, Feb 10, 2014 at 9:39 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic bump\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/512#issuecomment-34659015\n. In ok with Dmitry's idea. Sounds great that there's another example to borrow from for consistency.\n\nOn Mon, Feb 10, 2014 at 9:39 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic bump\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/512#issuecomment-34659015\n. Thanks.\n. Thanks.\n. The alternate endpoint is necessary for services like PistonCloud's Swift which support the full s3 API.\n\nOn Thu, Feb 13, 2014 at 11:02 AM, Ren\u00e9e Hendricksen\nnotifications@github.com wrote:\n\nHi David,\nMy code is giving people the option of specifying just the endpoint instead of all 3 (the host, port, and ssl). Also, I updated the Readme and re-factored the tests to cover the different scenarios.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/516#issuecomment-35012970\n. The alternate endpoint is necessary for services like PistonCloud's Swift which support the full s3 API.\n\nOn Thu, Feb 13, 2014 at 11:02 AM, Ren\u00e9e Hendricksen\nnotifications@github.com wrote:\n\nHi David,\nMy code is giving people the option of specifying just the endpoint instead of all 3 (the host, port, and ssl). Also, I updated the Readme and re-factored the tests to cover the different scenarios.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/516#issuecomment-35012970\n. They weren't running because bosh had never gotten around to starting/applying them yet. Perhaps a state of \"pending\" would be useful?\n\nPerhaps they are \"running\" because it's basic monit with empty.monitrc only?\nOn Thu, Feb 13, 2014 at 10:16 AM, Eric Malm notifications@github.com\nwrote:\n\nHi @byllc,\nWould you be able to provide the result of monit status and /var/vcap/monit/monit.log for these jobs? How did you determine that these jobs were not running?\nThanks,\nCF Community Pair (@ematpl & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/518#issuecomment-35008083\n. They weren't running because bosh had never gotten around to starting/applying them yet. Perhaps a state of \"pending\" would be useful?\n\nPerhaps they are \"running\" because it's basic monit with empty.monitrc only?\nOn Thu, Feb 13, 2014 at 10:16 AM, Eric Malm notifications@github.com\nwrote:\n\nHi @byllc,\nWould you be able to provide the result of monit status and /var/vcap/monit/monit.log for these jobs? How did you determine that these jobs were not running?\nThanks,\nCF Community Pair (@ematpl & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/518#issuecomment-35008083\n. Jesse, what is the correct/expected state of a job that has been bound but\nnot yet started?\n. Jesse, what is the correct/expected state of a job that has been bound but\nnot yet started?\n. Yes we're talking about the same thing.\n\nIt was a deployment with two jobs. Run in serial. The second job, which had two instances, hadn't been applied yet. The deployment failed at the first job. But \"vms\" said that the VMs for the 2nd job were running; but they'd really never been applied in the first place.\nOn Sat, Feb 15, 2014 at 11:51 PM, Jesse Zhang notifications@github.com\nwrote:\n\n@drnic I'm not sure we can understand your question. Is this related to the issue @byllc has? Do you know how @byllc got into that state? How did he know the 2nd and 3rd job \"never started\"?\nJesse\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/518#issuecomment-35179583\n. Yes we're talking about the same thing.\n\nIt was a deployment with two jobs. Run in serial. The second job, which had two instances, hadn't been applied yet. The deployment failed at the first job. But \"vms\" said that the VMs for the 2nd job were running; but they'd really never been applied in the first place.\nOn Sat, Feb 15, 2014 at 11:51 PM, Jesse Zhang notifications@github.com\nwrote:\n\n@drnic I'm not sure we can understand your question. Is this related to the issue @byllc has? Do you know how @byllc got into that state? How did he know the 2nd and 3rd job \"never started\"?\nJesse\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/518#issuecomment-35179583\n. Do you also get the issue for ruby 2.0.0 or 1.9.3?\n\nOn Thu, Feb 13, 2014 at 10:01 PM, cf-gitbot notifications@github.com\nwrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65779916\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/519#issuecomment-35058735\n. Do you also get the issue for ruby 2.0.0 or 1.9.3?\n\nOn Thu, Feb 13, 2014 at 10:01 PM, cf-gitbot notifications@github.com\nwrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65779916\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/519#issuecomment-35058735\n. A Gemfile might help to ensure the correct dependency resolution too.\n\nOn Fri, Feb 14, 2014 at 6:40 AM, Jesse Zhang notifications@github.com\nwrote:\n\nHave you tried uninstalling bosh_deployer and aws-sdk, then re-installing bosh_cli?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/519#issuecomment-35088614\n. A Gemfile might help to ensure the correct dependency resolution too.\n\nOn Fri, Feb 14, 2014 at 6:40 AM, Jesse Zhang notifications@github.com\nwrote:\n\nHave you tried uninstalling bosh_deployer and aws-sdk, then re-installing bosh_cli?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/519#issuecomment-35088614\n. I have no objections to any path forward.\n\nOn Fri, Feb 14, 2014 at 10:39 AM, Ren\u00e9e Hendricksen\nnotifications@github.com wrote:\n\nAlso, per the comments on #496 it's indicated that this is a better way to talk to swift.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/520#issuecomment-35111710\n. I have no objections to any path forward.\n\nOn Fri, Feb 14, 2014 at 10:39 AM, Ren\u00e9e Hendricksen\nnotifications@github.com wrote:\n\nAlso, per the comments on #496 it's indicated that this is a better way to talk to swift.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/520#issuecomment-35111710\n. For reference to anyone - we've been trying to get a PR like this into BOSH for 5 mths (since https://github.com/cloudfoundry/bosh/pull/450).\n. For reference to anyone - we've been trying to get a PR like this into BOSH for 5 mths (since https://github.com/cloudfoundry/bosh/pull/450).\n. No wasn't going to fix it; just noticed it when I quickly played with centos stemcell. I don't know much about centos; like the .bashrc loading you mentioned.\u00a0\n\nIf I regularly was using it I'd probably fix it.\nThough I wish I would stop ssh'ing into VMs in the first place :)\nClose the ticket if you like.\nOn Tue, Feb 18, 2014 at 4:10 PM, Matthew M. Boedicker\nnotifications@github.com wrote:\n\nIn Ubuntu, runningsudo su - also sources the .bashrc. The same is not true on CentOS. It looks like we're only adding /var/vcap/bosh/bin to the PATH in .bashrc files, hence the discrepancy between Ubuntu and CentOS.\n@drnic Do you plan on fixing this with a PR, or should we put this in the BOSH icebox for prioritization?\nCF Community Pair (@dsabeti & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/522#issuecomment-35451570\n. No wasn't going to fix it; just noticed it when I quickly played with centos stemcell. I don't know much about centos; like the .bashrc loading you mentioned.\u00a0\n\nIf I regularly was using it I'd probably fix it.\nThough I wish I would stop ssh'ing into VMs in the first place :)\nClose the ticket if you like.\nOn Tue, Feb 18, 2014 at 4:10 PM, Matthew M. Boedicker\nnotifications@github.com wrote:\n\nIn Ubuntu, runningsudo su - also sources the .bashrc. The same is not true on CentOS. It looks like we're only adding /var/vcap/bosh/bin to the PATH in .bashrc files, hence the discrepancy between Ubuntu and CentOS.\n@drnic Do you plan on fixing this with a PR, or should we put this in the BOSH icebox for prioritization?\nCF Community Pair (@dsabeti & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/522#issuecomment-35451570\n. Ok thx\n\nOn Tue, Feb 18, 2014 at 4:38 PM, Matthew M. Boedicker\nnotifications@github.com wrote:\n\nOk, we'll be put this in the BOSH icebox and it will get done at some point. In the meantime, we'll close the ticket.\nCF Community Pair (@dsabeti & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/522#issuecomment-35453506\n. Ok thx\n\nOn Tue, Feb 18, 2014 at 4:38 PM, Matthew M. Boedicker\nnotifications@github.com wrote:\n\nOk, we'll be put this in the BOSH icebox and it will get done at some point. In the meantime, we'll close the ticket.\nCF Community Pair (@dsabeti & @mmb)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/522#issuecomment-35453506\n. @cppforlife perhaps link the tracker URL into tickets when closing github issue? For non-core people, finding the correct tracker, then finding the ticket is a bit of an overhead to try to determine if/when this issue (which is still an issue even though \"closed\") might be resolved.\n. @cppforlife perhaps link the tracker URL into tickets when closing github issue? For non-core people, finding the correct tracker, then finding the ticket is a bit of an overhead to try to determine if/when this issue (which is still an issue even though \"closed\") might be resolved.\n. Why close?\n\nOn Sun, Mar 2, 2014 at 9:00 AM, Will Pragnell notifications@github.com\nwrote:\n\nClosed #536.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/536\n. There's also the \"vagrant-berkshelf plugin is deprecated\" article https://sethvargo.com/the-future-of-vagrant-berkshelf/\n. Thanks!\n\nOn Tue, Mar 18, 2014 at 3:30 PM, Alexander Rohmann\nnotifications@github.com wrote:\n\n@drnic Are you just looking for a way to install https://github.com/berkshelf/vagrant-berkshelf/pull/158 as a vagrant plugin?\nDefinitely a workaround, but you can do this:\ngit clone https://github.com/chulkilee/vagrant-berkshelf.git\ncd vagrant-berkshelf\ngit checkout vagrant-1.5\ngem build vagrant-berkshelf.gemspec\nvagrant plugin install --plugin-version 1.4.0.dev1 vagrant-berkshelf-1.4.0.dev1.gem\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/541#issuecomment-37977314\n. I worked around it by migrating to librarian locally.\n\nOn Tue, Mar 18, 2014 at 3:30 PM, Alexander Rohmann\nnotifications@github.com wrote:\n\n@drnic Are you just looking for a way to install https://github.com/berkshelf/vagrant-berkshelf/pull/158 as a vagrant plugin?\nDefinitely a workaround, but you can do this:\ngit clone https://github.com/chulkilee/vagrant-berkshelf.git\ncd vagrant-berkshelf\ngit checkout vagrant-1.5\ngem build vagrant-berkshelf.gemspec\nvagrant plugin install --plugin-version 1.4.0.dev1 vagrant-berkshelf-1.4.0.dev1.gem\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/541#issuecomment-37977314\n. Doh.\n\n$ vagrant plugin install --plugin-version 1.4.0.dev1 vagrant-berkshelf-1.4.0.dev1.gem\nInstalling the 'vagrant-berkshelf-1.4.0.dev1.gem --version '1.4.0.dev1'' plugin. This can take a few minutes...\n/Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/installer.rb:184:in `check_executable_overwrite': \"minitar\" from minitar conflicts with installed executable from archive-tar-minitar (Gem::InstallError)\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/installer.rb:384:in `block in generate_bin'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/installer.rb:371:in `each'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/installer.rb:371:in `generate_bin'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/installer.rb:231:in `install'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/dependency_installer.rb:379:in `block in install'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `each'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `each_with_index'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/dependency_installer.rb:339:in `install'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/bundler.rb:110:in `block in install_local'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/bundler.rb:254:in `block in with_isolated_gem'\n    from /Applications/Vagrant/embedded/lib/ruby/2.0.0/rubygems/user_interaction.rb:40:in `use_ui'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/bundler.rb:253:in `with_isolated_gem'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/bundler.rb:107:in `install_local'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/plugin/manager.rb:47:in `install_plugin'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/action/install_gem.rb:28:in `call'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/action/warden.rb:34:in `call'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/action/builder.rb:116:in `call'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/action/runner.rb:69:in `block in run'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/util/busy.rb:19:in `busy'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/action/runner.rb:69:in `run'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/command/base.rb:14:in `action'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/command/install.rb:32:in `block in execute'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/command/install.rb:31:in `each'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/command/install.rb:31:in `execute'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/plugins/commands/plugin/command/root.rb:56:in `execute'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/cli.rb:42:in `execute'\n    from /Applications/Vagrant/embedded/gems/gems/vagrant-1.5.1/lib/vagrant/environment.rb:248:in `cli'\n    from /Applications/Vagrant/bin/../embedded/gems/gems/vagrant-1.5.1/bin/vagrant:158:in `<main>'\n. For anyone else trying to do local dev and then testing stemcells, I had to make the following changes in my Vagranfile:\n``` ruby\n  config.vm.provision :shell do |shell|\n    shell.inline = <<-BASH\n      mkdir -p /mnt/stemcells\n      chown -R ubuntu /mnt/stemcells\n      mkdir -p /bosh/tmp\n      chown -R ubuntu:ubuntu /bosh\n    BASH\n  end\nconfig.vm.synced_folder '..', '/bosh'\n``\n. And then runvagrant provision remote`, which also did the rsync of the project to /bosh (without git them flushing any local uncommitted changes)\n. I thought I'd already created that ticket.\n. Close the one that's the least useful to you.\n. This ticket is about stemcells not bosh releases\nOn Mon, Mar 31, 2014 at 2:47 PM, Karl Isenberg notifications@github.com\nwrote:\n\nMy understanding is that pre-packaging has internet access because it's done locally, but packaging is not guaranteed to have internet access.\n@goehmen Do you want to address this?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/547#issuecomment-39146380\n. @cppforlife - this issue blocked my progress; I haven't tried again. Do you think the root cause might have been fixed?\n. Thanks for patching those.\n. The CLI doesn't change so much that a docs.cf.org page (which can be found\nvia Google) is an expensive option to maintain; and its value (especially\ngoogle search discovery) would be high I think.\n\nIn addition, perhaps we'll get PRs to include example usage in the docs.\n. @metadave sorry I forgot to mention develop vs master branch. Several CF\nrepos are moving to having a develop branch for PRs; and a master branch\nrepresenting the latest commits that passed CI.\nOn Wed, Apr 23, 2014 at 10:23 AM, John Foley notifications@github.comwrote:\n\n@metadave https://github.com/metadave\nThanks for the PR! However we can't accept this as is, since the BOSH team\nworks off of the develop branch so you will need to file a new PR against\nthat branch.\nIt also appears to have broken the travis build, but it also looks like\nthe failure is unrelated to your change as far as we can tell.\nPinging @goehmen https://github.com/goehmen to see if this is a change\nthat we would like to accept.\nThanks,\nCF Community Pair (@jfoley https://github.com/jfoley & @jtuchschererhttps://github.com/jtuchscherer\n)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/569#issuecomment-41189599\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. David, thanks so much for creating the demo video!\nOn Wed, May 14, 2014 at 5:17 AM, David Laing notifications@github.comwrote:\n\n@mariash https://github.com/mariash Test intentions updated as\nsuggested.\n@goehmen https://github.com/goehmen A screencast of deploying a CF\ncluster to AWS using spot instanceshttps://www.youtube.com/watch?v=4YbnQqmkas8\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/577#issuecomment-43072908\n.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. bosh-lite uses the same bosh repo but it's on a slower release cycle; and mcf (which trycf.starkandwayne.com uses) is on a slower cycle that that (months old)\n@ccpforlife we'd be happy to help with bosh-lite/mcf releases if you'd like\nOn Tue, Aug 5, 2014 at 6:07 AM, Doug Davis notifications@github.com\nwrote:\n\ngreat! thanks!\nHow does bosh-lite pickup features like this?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-51194335\n. Is it ok to explicitly set ulimit in your bosh jobs?\n\nOn Wed, May 14, 2014 at 6:09 AM, Peter Jones notifications@github.com\nwrote:\n\nWhen using bosh-lite to run processes that need many open file descriptors (eg Cassandra, Jenkins etc.), the default 1024 limit is insufficient. This allows us to change these limits within the warden containers by adding them to the director.yml:\n``` yml\nname: Bosh Lite Director\n...\ncloud:\n  plugin: warden\n  properties:\n    warden:\n      unix_domain_socket: \"/tmp/warden.sock\"\n      rlimits:\n        nofiles: 10240\n...\n```\nAll options that can be specified within the rlimits property are defined in Warden::Protocol::ResourceLimits\nYou can merge this Pull Request by running:\n  git pull https://github.com/cloudfoundry/bosh warden-cpi-resource-limits\nOr you can view, comment on it, or merge it online at:\n  https://github.com/cloudfoundry/bosh/pull/579\n-- Commit Summary --\n- Allow resource limits to be specified for Warden in the director config\n  -- File Changes --\n  M bosh_warden_cpi/lib/cloud/warden/helpers.rb (14)\n  M bosh_warden_cpi/spec/unit/helper_spec.rb (2)\n  -- Patch Links --\n  https://github.com/cloudfoundry/bosh/pull/579.patch\n  https://github.com/cloudfoundry/bosh/pull/579.diff\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/579\n. ooh there's also: ulimit -v unlimited [1]\n\nI learnt about that yesterday for running consul within warden containers -\nsince consul allocates 40G virtual apparently.\n[1]\nhttps://github.com/cloudfoundry-community/consul-boshrelease/blob/master/jobs/consul/templates/bin/consul_ctl#L17\n. If it helps with diagnosis - the error came from the goagent code, and I'm using AWS in a way that Pivotal doesn't use (nor perhaps test for) which is pure AWS EC2.\n. You may be asked to write a little unit test before this is merged. Let me know if you'd like help with that.\n. Is this travis failure anything to do with this PR? https://travis-ci.org/cloudfoundry/bosh/jobs/28364234#L3865\n. @tedsuo thx mate\n. BTW, when I uploaded the release normally (rather than from an HTTP URL) it worked as expected. Odd.\n. Is it possibly related to the in-progress revisions to release versioning? https://groups.google.com/a/cloudfoundry.org/forum/#!searchin/bosh-users/release$20versioning/bosh-users/x43XGNLgnCA/0jB6FulmopkJ\n. Accidentally closed the issue\n. THe issue may also be that the target BOSH was bosh-lite and this warning:\nYou are using CLI > 1.2579.0 with a director that doesn't support the new version format you are using. Upgrade your director to match the version of your CLI or downgrade your CLI to 1.2579.0 to avoid versioning mismatch issues.\nConverting to old format                                     OK\n. Hurray!\n. @DanLavine  @leoRoss thanks guys\n. @cppforlife perhaps merge this 1st implementation and your backlog story can stay and a \"better\" implementation can come down the pipeline later?\n. @cppforlife perhaps merge this 1st implementation and your backlog story can stay and a \"better\" implementation can come down the pipeline later?\n. Fix for CI coming (hopefully).\n@cppforlife I'm not 100% sure on the future path of errands, so I implemented this simple helper. Also want to add a menu to choose an errand if you don't provide a name. I'd like more menus (like bosh ssh) for commands.\n. Fix for CI coming (hopefully).\n@cppforlife I'm not 100% sure on the future path of errands, so I implemented this simple helper. Also want to add a menu to choose an errand if you don't provide a name. I'd like more menus (like bosh ssh) for commands.\n. Merging into #625 \n. Merging into #625 \n. As per #624 the list_errands helper can be upgraded later to call an API method if later implemented\n. As per #624 the list_errands helper can be upgraded later to call an API method if later implemented\n. Ok thx @cppforlife @mkocher I'll:\n- [x] upgrade the director API\n- [x] remove the VMs column (wasn't sure if there was ever going to be a use for 2+ vms)\n. Ok thx @cppforlife @mkocher I'll:\n- [x] upgrade the director API\n- [x] remove the VMs column (wasn't sure if there was ever going to be a use for 2+ vms)\n. @cppforlife I am thinking that /deployments/NAME/errands will return a list of Hash {name: errand-name}; which will allow for more fields later if needed; without forcing errands to look like jobs.\n$ curl -k -u admin:admin https://54.84.122.114:25555/deployments/tree-aws/errands                                                                                                  \n[{\"name\":\"tree\"}]\n. @cppforlife I am thinking that /deployments/NAME/errands will return a list of Hash {name: errand-name}; which will allow for more fields later if needed; without forcing errands to look like jobs.\n$ curl -k -u admin:admin https://54.84.122.114:25555/deployments/tree-aws/errands                                                                                                  \n[{\"name\":\"tree\"}]\n. @cppforlife  I don't think CI failures are related to this PR\n. @cppforlife  I don't think CI failures are related to this PR\n. Thanks for the update. I will get back to this at some point :)\nOn Tue, Aug 12, 2014 at 3:28 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic The director controllers have been refactored; you will need to rebase your commits onto develop before we can accept this pull request.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51986670\n. Thanks for the update. I will get back to this at some point :)\n\nOn Tue, Aug 12, 2014 at 3:28 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic The director controllers have been refactored; you will need to rebase your commits onto develop before we can accept this pull request.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51986670\n. Oh you're awesome!! Thanks! xoxo\n\nOn Tue, Aug 12, 2014 at 4:43 PM, rboshman notifications@github.com\nwrote:\n\nWe'll do it to keep it fair =)\nOn Tue, Aug 12, 2014 at 4:22 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nThanks for the update. I will get back to this at some point :)\nOn Tue, Aug 12, 2014 at 3:28 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic The director controllers have been refactored; you will need to\nrebase your commits onto develop before we can accept this pull request.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51986670\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51991258.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51992929\n. Oh you're awesome!! Thanks! xoxo\n\n\nOn Tue, Aug 12, 2014 at 4:43 PM, rboshman notifications@github.com\nwrote:\n\nWe'll do it to keep it fair =)\nOn Tue, Aug 12, 2014 at 4:22 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nThanks for the update. I will get back to this at some point :)\nOn Tue, Aug 12, 2014 at 3:28 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic The director controllers have been refactored; you will need to\nrebase your commits onto develop before we can accept this pull request.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51986670\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51991258.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51992929\n. I think there is still changes to be made from suggests above. They might be out of scope of \"rebasing\" :)\n. I think there is still changes to be made from suggests above. They might be out of scope of \"rebasing\" :)\n. Confirmed bosh errands working on upgraded microbosh:\n\n\n```\n$ bosh errands\nProcessing deployment manifest\n+------------------+\n| Name             |\n+------------------+\n| acceptance_tests |\n| smoke_tests      |\n+------------------+\n$ bosh run errand\nProcessing deployment manifest\n\nacceptance_tests\nsmoke_tests\nChoose an errand: 2\n```\n\nHurray!\n. Confirmed bosh errands working on upgraded microbosh:\n```\n$ bosh errands\nProcessing deployment manifest\n+------------------+\n| Name             |\n+------------------+\n| acceptance_tests |\n| smoke_tests      |\n+------------------+\n$ bosh run errand\nProcessing deployment manifest\n\nacceptance_tests\nsmoke_tests\nChoose an errand: 2\n```\n\nHurray!\n. Perhaps extract errands out of jobs: section of the manifest. As an example, the compilation: section is not in jobs:. Then like compilation: assign a dedicated VM in an assigned network/cloud_properties outside of the general resource_pools.\n. Perhaps extract errands out of jobs: section of the manifest. As an example, the compilation: section is not in jobs:. Then like compilation: assign a dedicated VM in an assigned network/cloud_properties outside of the general resource_pools.\n. Ok will wait for next bosh final release (am running 94)\nOn Tue, Aug 12, 2014 at 5:11 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #633.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/633#event-152160408\n. Ok will wait for next bosh final release (am running 94)\n\nOn Tue, Aug 12, 2014 at 5:11 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #633.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/633#event-152160408\n. Ooh there is now 98.\n\nOn Tue, Aug 12, 2014 at 5:12 PM, Dr Nic Williams drnicwilliams@gmail.com\nwrote:\n\nOk will wait for next bosh final release (am running 94)\nOn Tue, Aug 12, 2014 at 5:11 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #633.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/633#event-152160408\n. Ooh there is now 98.\n\n\nOn Tue, Aug 12, 2014 at 5:12 PM, Dr Nic Williams drnicwilliams@gmail.com\nwrote:\n\nOk will wait for next bosh final release (am running 94)\nOn Tue, Aug 12, 2014 at 5:11 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #633.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/633#event-152160408\n. On another bosh environment this feature works as expected. Not sure what the cause of the first environment's issue was.\n. On another bosh environment this feature works as expected. Not sure what the cause of the first environment's issue was.\n. Sorry, mate, if you don't see an issue then close the ticket. I discovered\nthis when preparing/delivering training. Perhaps it was a local-only issue.\n\n\nOn Wed, Sep 3, 2014 at 9:30 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic https://github.com/drnic bump\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/636#issuecomment-54324491.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Sorry, mate, if you don't see an issue then close the ticket. I discovered\nthis when preparing/delivering training. Perhaps it was a local-only issue.\nOn Wed, Sep 3, 2014 at 9:30 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@drnic https://github.com/drnic bump\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/636#issuecomment-54324491.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Ok, will investigate\n. Ok, will investigate\n. Filtered list updated\n. Filtered list updated\n. Oh thanks!!\nOn Mon, Aug 25, 2014 at 5:48 PM, Kris Hicks notifications@github.com\nwrote:\n\nClosed #642.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/642#event-157073577\n. Oh thanks!!\n\nOn Mon, Aug 25, 2014 at 5:48 PM, Kris Hicks notifications@github.com\nwrote:\n\nClosed #642.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/642#event-157073577\n. Oh nice. What's an example for configuring it?\n\nOn Thu, Oct 30, 2014 at 12:33 PM, Dmitriy Kalinin\nnotifications@github.com wrote:\n\nClosed #663.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/663#event-186168967\n. Oh nice. What's an example for configuring it?\n\nOn Thu, Oct 30, 2014 at 12:33 PM, Dmitriy Kalinin\nnotifications@github.com wrote:\n\nClosed #663.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/663#event-186168967\n. Ahh sorry, should have taken a moment to go to PR from the email notification. Thanks.\n\nOn Thu, Oct 30, 2014 at 1:10 PM, Aaron Huber notifications@github.com\nwrote:\n\nIt's shown above, but basically:\n    resources:\n      persistent_disk: 20480\n      persistent_disk_cloud_properties:\n        type: volume_type\n      cloud_properties:\n        instance_type: flavor\n        availability_zone: zone\nThis depends on support in the CPI for disk pool configuration, which is present only in the AWS CPI at the moment (and will be in OpenStack once they accept my pull request for that).\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/663#issuecomment-61161277\n. Ahh sorry, should have taken a moment to go to PR from the email notification. Thanks.\n\nOn Thu, Oct 30, 2014 at 1:10 PM, Aaron Huber notifications@github.com\nwrote:\n\nIt's shown above, but basically:\n    resources:\n      persistent_disk: 20480\n      persistent_disk_cloud_properties:\n        type: volume_type\n      cloud_properties:\n        instance_type: flavor\n        availability_zone: zone\nThis depends on support in the CPI for disk pool configuration, which is present only in the AWS CPI at the moment (and will be in OpenStack once they accept my pull request for that).\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/663#issuecomment-61161277\n. For anyone else looking to recover a Micro BOSH from just its persistent\ndisk, here's a little blog post\nhttps://blog.starkandwayne.com/2014/10/10/restore-micro-bosh-from-just-its-persistent-disk/\n\nOn Fri, Oct 10, 2014 at 2:37 AM, trastle notifications@github.com wrote:\n\nHi @cppforlife https://github.com/cppforlife no worries. I agree\nprobably no point in fixing. The info here is enough for anyone who hits\nthe issue to work around.\nHow close is the new deployer to being useable?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/672#issuecomment-58633599.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. BTW, the aws plugin was the only place I found documentation for setting up\nELBs (the TCP:80 health check, 80, 443, 4443 listeners etc) for CF. Perhaps\nask the CF/ops docs ppl to write up a little something about this.\n. BTW, the aws plugin was the only place I found documentation for setting up\nELBs (the TCP:80 health check, 80, 443, 4443 listeners etc) for CF. Perhaps\nask the CF/ops docs ppl to write up a little something about this.\n. Oooh I like the sound of it!\nOn Fri, Oct 17, 2014 at 2:52 AM, cfdreddbot notifications@github.com\nwrote:\n\nHey luan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/684#issuecomment-59473056\n. Oooh I like the sound of it!\n\nOn Fri, Oct 17, 2014 at 2:52 AM, cfdreddbot notifications@github.com\nwrote:\n\nHey luan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/684#issuecomment-59473056\n. The build is broken only in the same way its been broken for every other commit.\n. The build is broken only in the same way its been broken for every other commit.\n. @cppforlife if the env doesn't have internet then they can configure it to disable ntp with ntp: []\n. @cppforlife if the env doesn't have internet then they can configure it to disable ntp with ntp: []\n. But my guess is that ntpdate will log an error into the log file and that'll be that.\n. But my guess is that ntpdate will log an error into the log file and that'll be that.\n. Travis passed except for the eternally failing coverage tests\n. Travis passed except for the eternally failing coverage tests\n. Closing due to PR being against wrong branch\n. Closing. traveling-ruby shipped a native version of yajl-ruby.\n. Our Gems depend on other gems that use old nokogiri. I'm trying to track my attempt to upgrade them all.\n\nPerhaps there is no bosh gems per se to PR; just the Gemfile.lock. I might have gotten lost a bit trying to hunt down all the ~> 1.5 dependencies\nOn Fri, Dec 19, 2014 at 3:55 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nSorry I will review traveling-ruby integration in greater detail a bit later, but looking at this PR I don't understand why we have to update gems for all internal bosh bits. Don't we just want bosh_cli gems to be compatible with traveling-ruby?\nSent from my iPhone\n\nOn Dec 19, 2014, at 3:33 PM, Dr Nic Williams notifications@github.com wrote:\nTo support traveling-ruby (for the traveling-bosh / http://bosh-cli.cfapps.io/ project), we need to upgrade to nokogiri 1.6.5 (its the shipped native version http://traveling-ruby.s3-us-west-2.amazonaws.com/list.html)\nTodo items:\ncreate PR to patch foodcritic acrmp/foodcritic#291\n foodcritic new version released\n create PR for vcloud cpi vchs/bosh_vcloud_cpi#12\n vcloud PR accepted and new version released\n PR for BOSH to use new gems + new jenkins_api_client\n\u2014\nReply to this email directly or view it on GitHub.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/719#issuecomment-67714323\n. This is an issue not a PR yet\n\n\nOn Fri, Dec 19, 2014 at 3:55 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nSorry I will review traveling-ruby integration in greater detail a bit later, but looking at this PR I don't understand why we have to update gems for all internal bosh bits. Don't we just want bosh_cli gems to be compatible with traveling-ruby?\nSent from my iPhone\n\nOn Dec 19, 2014, at 3:33 PM, Dr Nic Williams notifications@github.com wrote:\nTo support traveling-ruby (for the traveling-bosh / http://bosh-cli.cfapps.io/ project), we need to upgrade to nokogiri 1.6.5 (its the shipped native version http://traveling-ruby.s3-us-west-2.amazonaws.com/list.html)\nTodo items:\ncreate PR to patch foodcritic acrmp/foodcritic#291\n foodcritic new version released\n create PR for vcloud cpi vchs/bosh_vcloud_cpi#12\n vcloud PR accepted and new version released\n PR for BOSH to use new gems + new jenkins_api_client\n\u2014\nReply to this email directly or view it on GitHub.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/719#issuecomment-67714323\n. bosh_vsphere_cpi is coded to restrict to ~> 1.5.X too\n\n\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/bosh_vsphere_cpi.gemspec#L31\n. WIP branch to bring the changes in and test them. https://github.com/drnic/bosh/tree/nokogiri\n. Which bosh release were you working upon?\u200b\n. There is a page on the resurrector in docs.cloudfoundry.org and how to enable/configure it.\nOn Fri, Mar 13, 2015 at 10:11 AM, asanramon notifications@github.com\nwrote:\n\nHi drnic/cppforlife,\nFor some reason I am able to deploy bosh now from microbosh. I didn't change anything on my manifest. The only reason I can think of that cause the issue was that someone else was deploying something when I deploy bosh.\nOff topic, is it possible to enable resurrector on microbosh? All our deployments are deployed through microbosh, and we would like to automatically rebuild instances when it goes down.\nThanks for all the reply.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/723#issuecomment-79145106\n. Ideas on an algorithm:\u00a0\n1. grow a pool of compilation workers upto the number in the manifest\n2. If the size is 0 then there's a major issue (creds, networking, etc) and stop\n3. If the size is 1+ then compile packages with the pool.\n\nOn Fri, Jan 23, 2015 at 12:41 PM, Dmitriy Kalinin\nnotifications@github.com wrote:\n\nI am concerned about cases when IaaS fails to create extra VMs and Director would decide to compile everything on a single compilation VM which might take forever. Thoughts?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71232093\n. If you only have one compilation VM then that's the limit isn't it? Is there a reason to stop because there's only one?\n\nOn Fri, Jan 23, 2015 at 12:41 PM, Dmitriy Kalinin\nnotifications@github.com wrote:\n\nI am concerned about cases when IaaS fails to create extra VMs and Director would decide to compile everything on a single compilation VM which might take forever. Thoughts?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71232093\n. Is that a problem?\n\nOn Fri, Jan 23, 2015 at 1:15 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nI meant something like this:\n- user requests 6 compilation VMs to compile CF (workers: 6)\n- Director only gets 1 compilation VM\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237500\n. The user request is a maximum number. The reality might be I can't have that (quotas hit us all)\n\nOn Fri, Jan 23, 2015 at 1:15 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nI meant something like this:\n- user requests 6 compilation VMs to compile CF (workers: 6)\n- Director only gets 1 compilation VM\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237500\n. xoxo\n\nOn Fri, Jan 23, 2015 at 1:20 PM, bosh-ci-push-pull\nnotifications@github.com wrote:\n\nI'll think more about it but it sounds like a thing we can definitely fix.\nOn Fri, Jan 23, 2015 at 10:17 AM, Dr Nic Williams notifications@github.com\nwrote:\n\nThe user request is a maximum number. The reality might be I can't have\nthat (quotas hit us all)\nOn Fri, Jan 23, 2015 at 1:15 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nI meant something like this:\n- user requests 6 compilation VMs to compile CF (workers: 6)\n- Director only gets 1 compilation VM\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237500\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237906.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71238301\n. Neither of the bosh_[aws|openstack]_cpi/README.md document their CLI either.\n. The PR is a start. Openstack's README doesn't mention the manifest scheme. AWS does, afaict.\n. The command bosh prepare deployment is from bosh-workspace - its entirely\noptional to use it to upload stemcells. Manually upload the new stemcell to\nyour bosh.\n. Ahh, you're patching terraform-aws-cf-install. It might be that you need to\nrun the patching inside the created jumpbox and run the commands manually.\n\n\n/cc @longnguyen11288\nOn Thu, May 28, 2015 at 9:35 AM, Dr Nic Williams drnicwilliams@gmail.com\nwrote:\n\nThe command bosh prepare deployment is from bosh-workspace - its entirely\noptional to use it to upload stemcells. Manually upload the new stemcell to\nyour bosh.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. Correct. After it fails, you should be able Ssh into the jumpbox and continue.\nOn Thu, May 28, 2015 at 9:38 AM, Riccardo M. Cefala\nnotifications@github.com wrote:\n\nThanks for the reply.\nYes, you are right, but the provision.sh script is uploaded to the jump/bastion instance. Is this what you mean?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/755#issuecomment-106473628\n. BOOM!!\n\nOn Wed, Apr 22, 2015 at 4:35 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #762.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/762#event-287790044\n. If you're interested in adding support, I think\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/config.rb\nis a good place to start (+ an update to the specs + README)\n. Awesome.\n\nWill/could \"available\" support a list of ranges (like reserved/static do?)\n. Lovely\nOn Sat, Mar 21, 2015 at 1:51 AM, Xiaochuan Wang notifications@github.com\nwrote:\n\nSure. This option supports a list of ranges, thus gives us full control of multiple releases' network deployment.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/778#issuecomment-84283162\n. A quick look at cloudcheck.rb and I can't quite figure out how the --auto feature is supposed to work. Sorry for no PR.\n. Awesome! Thanks!!\n\nOn Thu, Apr 16, 2015 at 7:24 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #779.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/779#event-283542470\n. I got bit by this today when a spec wasn't valid YAML (although my text editor thought it was) - a value needed \"...\" quotes around it\n. Thanks James for starting this!\n\nOn Sun, May 3, 2015 at 8:43 AM, cfdreddbot notifications@github.com\nwrote:\n\nHey JamesClonk!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/816#issuecomment-98496409\n. @hochm I think all James has to do its make his membership of the GitHub org public and the bot does the rest. I think.\n\nOn Sun, May 3, 2015 at 11:25 AM, hochm notifications@github.com wrote:\n\nJamesClonk is member of the Swisscom Team / see signed ccla;\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/816#issuecomment-98519785\n. This happened again (same BOSH btw) so I wrote a blog post https://blog.starkandwayne.com/2015/06/08/unlocking-bosh-locks/\n. Actually, same issue happened on two diff boshes - deployments that didn't cancel after 6+ hours.\n. Afaik if you change NTP via your bosh you then need to restart/recreate each individual VM. There is no bulk method for rolling out this change to running VMs.\n\nOn Tue, Aug 4, 2015 at 11:23 AM, liuxiaoxi2237 notifications@github.com\nwrote:\n\nThanks a lot for your help.\nBTW, can NTPserver  of CF VMs be changed after deployment? In my test ,seems it can not be changed.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/906#issuecomment-127683332\n. I started experiencing this quirk I think with the docker-boshrelease on bosh-lite/garden. The docker process is running just fine and there are no errors in the log - monit restart docker makes it work; but monit summary originally thought docker failed (and bosh vms thinks its failing).\n. More audit logs would be lovely\n\nOn Thu, Aug 27, 2015 at 5:56 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nMakes sense. Btw I'm thinking to introduce an audit log feature with records about which CPI calls made, which agent calls made and of course which resources were affected etc.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/940#issuecomment-135594313\n. Hurray for BOSH being able to fetch releases for itself!!\n. Thanks for spotting that. My use cases have been via settings.yml only I missed it.\n\nDo you think there's a simple PR you can do to fix?\nOn Thu, Sep 24, 2015 at 1:21 PM, berniedurfee-ge notifications@github.com\nwrote:\n\nIt appears that the 'bosh bootstrap' command requires that the AWS key be hardcoded in the settings.yml file. Instead, we would like to run the 'bosh bootstrap' command on an EC2 instance that is assigned an EC2 role, so we don't have to hardcode credentials.\nAdditionally, 'bosh bootstrap' should support the standard AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\nLooks like there is a 'credentials_source' option that should work, just doesn't seem to be recognized in settings.yml.\nBosh 1.3074.0\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/962\n. bosh-bootstrap would have the role of asking questions and creating the new larger manifest; then running bosh-init\n\nOn Fri, Sep 25, 2015 at 5:24 AM, berniedurfee-ge notifications@github.com\nwrote:\n\n@drnic Is it even worth it? Isn't bosh-init going to ultimately replace 'bosh bootstrap'?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/962#issuecomment-143203153\n. bosh-workspace is a separate rubygem that BOSH CLI loads up dynamically and\noverrides bosh deploy and bosh deployment commands.\n\nTry installing the gem:\ngem install bosh-workspace\nOr better still, use a Gemfile to install everything.\ncf-boshworkspace repo has one; run:\nbundle install\nOn Wed, Nov 18, 2015 at 11:32 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nlooking at the stacktrace there is a mention of bosh-workspace. thats not\nsomething that vanilla bosh cli tool provides.\nSent from my iPhone\n\nOn Nov 18, 2015, at 11:20 AM, ChandraNarayanasamy \nnotifications@github.com wrote:\nWe are not using any plug-ins. just did an update and executed command\nbosh.\n[root@hello commands]# bosh\nFailed to load plugin\n/usr/local/share/gems/gems/bosh-workspace-0.9.4/lib/bosh/cli/commands/deployment_patch.rb:\ncannot load such file -- bosh/workspace\n\u2014\nReply to this email directly or view it on GitHub.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1034#issuecomment-157834343.\n\n\nDr Nic Williams\nStark & Wayne LLC - consultancy for Cloud Foundry users\nhttp://drnicwilliams.com\nhttp://starkandwayne.com\ncell +1 (415) 860-2185\ntwitter @drnic\n. It's long winded - but you could look at the top of the task logs where the deployment manifest is cached:\nbosh task  --debug | head\nOn Wed, Jan 20, 2016 at 8:15 PM, Peter G\u00f6tz notifications@github.com\nwrote:\n\nI can't find a way to see when a certain version of a release was deployed. This makes debugging a faulty component very difficult, because it's unclear with what version a certain misbehavior was introduced. The bosh tasks recent command also doesn't help here, because it doesn't allow to show deployment actions for only one release.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/1103\n. Can this be rolled out now?\n. Sweet.\n. @dpb587-pivotal to confirm what I see - the work was merged onto develop on July 1; but isn't yet shipped? Or what does the last comment on the tracker ticket mean?\n. Hehe funny stuff\n\nSent from Outlook Mobile\nOn Fri, Apr 1, 2016 at 11:10 AM -0700, \"Dmitriy Kalinin\" notifications@github.com wrote:\nClosed #1191.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Oh specifically \"bosh ssh\". Would love to be able to run that without the manifest. \n. Ok, trawling thru task logs I can see things like:\nD, [2016-10-11 06:26:31 #11395] [canary_update(cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45))] DEBUG -- DirectorJobRunner: networks_changed? obsolete reservations: [{ip=10.9.1.191, network=etcd1, instance=cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45), reserved=true, type=dynamic}]\nD, [2016-10-11 06:26:31 #11395] [canary_update(cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45))] DEBUG -- DirectorJobRunner: networks_changed? desired reservations: [{type=static, ip=10.9.1.191, network=cell2, instance=cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45)}]\nD, [2016-10-11 06:26:31 #11395] [canary_update(cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45))] DEBUG -- DirectorJobRunner: Networks have changed. Recreating VM\nD, [2016-10-11 06:26:31 #11395] [canary_update(cell_z2/0 (8493a031-31c8-4783-b743-129f78584e45))] DEBUG -- DirectorJobRunner: Failed to update in place. Recreating VM\nUnrelated; but no idea why director things the VM was in a different network.\n. the deployment was sharing an AWS subnet across multiple bosh networks; and there were overlaps of available dynamic ranges between the BOSH networks. Perhaps the issue occurs because of this?\n. @cppforlife awesome; thanks didn't know about that\n. LOL, I need a wider laptop screen:\n\n. One option to protect from unset vars is to use:\n${ROOTFS:?required}\nThis will fail if the car is unset or empty.\nOn Mon, Oct 24, 2016 at 11:42 PM +1000, \"James Wen\" notifications@github.com wrote:\nIn our Concourse, one of our jobs runs:\nbosh create release --force --with-tarball --name $ROOTFS_RELEASE --version 212.0.$(date +\"%s\")\nThis was ran with the $ROOTFS_RELEASE env var not set and a release called --version was created. \nhttps://buildpacks.ci.cf-app.com/teams/main/pipelines/stacks/jobs/deploy-rootfs-to-stacks/builds/31\nGenerated /tmp/build/395d4d9d/cflinuxfs2-rootfs-release/dev_releases/--version/--version-0+dev.1.tgz\nRelease name: --version\nRelease version: 0+dev.1\nRelease manifest: /tmp/build/395d4d9d/cflinuxfs2-rootfs-release/dev_releases/--version/--version-0+dev.1.yml\nRelease tarball (258.4M): /tmp/build/395d4d9d/cflinuxfs2-rootfs-release/dev_releases/--version/--version-0+dev.1.tgz\nBOSH CLI version:\nbash-4.3# bosh --version\nBOSH 1.3262.4.0\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ok cool thx\nOn Mon, Jan 16, 2017 at 3:24 PM -0800, \"Dmitriy Kalinin\" notifications@github.com wrote:\n@drnic yup. that's the result (cosmetic) of warden stemcells having older agent temporarily. there is a story to address that: https://www.pivotaltracker.com/story/show/134182747.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I saw this error message today and misinterpreted it too and had to go Googling (and found this ticket).\nPerhaps alternate error: \"Your deployment manifest contains 'stemcells' section, but your director does not yet have a cloud-config.\". Ok lovely. Closing as the various questions are answered/covered above or in other tickets.. Thanks; that reads well.\nOn Tue, Mar 14, 2017 at 8:55 PM -0400, \"Dmitriy Kalinin\" notifications@github.com wrote:\nClosed #1618.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Sorry, this is a bosh-cli create-env question. moving.. I'm ok with not supporting src package collocation fundamentally but I think we still need a way to re-upload releases that contain new compiled packages for a new stemcell major release. As it stands now, I think we will cause deployments to fail if we upgrade major stemcells but one or more releases don't also need upgrading (their version is same so no way to install new compiled nor source packages).\nPerhaps the algorithm could be for upload-release: check the version and sha1 of the release; if the version is different then we have some new jobs or packages to upload. If the sha1 is different then we have new compiled packages (for new major stemcell) to upload.\n. I think I've covered my discovered issue - new stemcells but with old releases - in the other ticket. If the solution isn't via flexible releases; then we'll need a way to re-upload releases to inject new compiled packages (as per other ticket).. To set the vcap password try:\nresource_pools:\n- name: bosh\n  env:\n    bosh:\n      password: (( param \"Please generate a SHA-512 crypted password the\nvcap system user\" ))\n. This issue did not reproduce itself today. Don't know what changed. \ud83d\ude15 . I can reproduce this. Against a bosh-lite running v261.4 (latest at time of writing):\n```\ngit clone https://github.com/cloudfoundry-community/broker-registrar-boshrelease -b links-and-cloud-config\ncd broker-registrar-boshrelease\nbosh2 create-release && bosh2 upload-release\nexport BOSH_DEPLOYMENT=broker-registrar-demo\nbosh2 -n deploy manifests/failing-test.yml\n```\nThe output will include the error:\n04:46:20 | Preparing deployment: Preparing deployment (00:00:00)\n            L Error: Can't generate network settings without an IP. @miyer-pivotal the discussion on this issue stoppped so I moved away from links for sharing this info between two errands and am duplicating the properties on both errands. Ok to make the link public?\n. @cppforlife there are many cases where you're consuming a link from an arbitrary other bosh release (say sql job) and you want to consume properties - but there isn't a requirement for providers to follow any pattern/style, so you might need to support a range of possible properties from many known or future provider jobs.\nAt this point I'd like to also ask that we get rid of \"type: some-restrictive-label\" from links. They are devaluing links from allowing reuse amongst independently developed releases.\n. Perhaps https://github.com/cloudfoundry/cf-release or https://github.com/cloudfoundry/cf-deployment projects are good ones to discuss consul-in-cf.\nOr join https://slack.cloudfoundry.org and talk to lovely ppl in #cf-deployment channel. https://github.com/cppforlife/bosh-hub\n@cppforlife - perhaps this can be moved into a cloudfoundry* org so its\nmore findable?\n. As an aside, Puma is the default web server for new Rails 5 apps since mid 2016 https://richonrails.com/articles/the-rails-5-0-default-files\nThe history of Puma is that it was a fork of the unmaintained Mongrel - a threaded webserver - by the creator of Rubinius @evanphx. Puma has gone from strength to strength.. This issue has gone away. If I can reproduce it again in future, I'll reopen the ticket with more information.. Update: I can see now that you've pasted in Pivotal Opsmgr metadata rather than pure BOSH manifest. Perhaps use bosh manifest to fetch the resulting manifest to confirm that it looks ok.\nI think you've accidentally got consumes: value as a string\nconsumes: |\n      credhub: {from: credhub, deployment: \"(( ..cf.deployment_name ))\"}\nInstead its an object:\nconsumes:\n      credhub: {from: credhub, deployment: \"(( ..cf.deployment_name ))\"}. If you'd like to explore using bbr backup, perhaps try deploying BOSH via https://github.com/starkandwayne/bucc which will pre-configure BBR within your director VM to be ready to go.\n\nhttps://www.starkandwayne.com/blog/bucc-bbr-finally/\nhttps://www.starkandwayne.com/blog/is-bucc-is-better-than-raw-bosh-cli/\n\nThen either continue with BUCC; or borrow/explore the operator files in state/manifests that add BBR to your VM.. In this video is an explanation of a jumpbox vs bosh vs deployments like CF https://youtu.be/aDg9kuv6hjg\n. Checkout:\n https://bosh.io/docs/package-vendoring/\n https://www.cloudfoundry.org/blog/build-better-bosh-releases-faster-with-language-packs/. Full walk thru of bosh/uaa/credhub (bucc) and cfcr at https://github.com/starkandwayne/cfcr-compiled-deployment. does this mean 9 warden containers?\n. Alternate: ip = floating_ip || service_ip I think\n. ooh, another variation you can use is:\nruby\nip = cloud.openstack.servers.get(state.vm_cid).floating_ip_address\nip ||= service_ip\nbut I like what you have now as it has semantically meaningful variable names\n. the revised comment is contextual against the old comment; which a new reader won't see.\nPerhaps: \"a sane number of retries (~30 minutes)\"\n. What does this do?\n. Perhaps pass name of deployment via argument (instead of hardcoded \"cloud\")?\n. Why is this bad? One deployments folder to house many deployments? (It is a plural name after all :)\n. Perhaps rename with \"vsphere\" suffix in preparation for other CPIs?\n. Yep\n. I'm not able to accept PRs; but I think it's a good idea and might as well add the other templates now whilst you're in the mood!\n. Assume everything useful is accepted. Otherwise you might spend more energy worrying about whether its accepted than the feature :)\n. Change URL to http://admin:admin@127.0.0.1:8080/ to include the basic auth credentials.\n. print_internal_info coming up\n. Yep, that'll be good for this info.\n. Changing it to /internal_config to keep a distinction in the API & the CLI that its about the director's internal config; not the CLI's config to talk to director\n. Both suggestions implemented\n. ## No, accidental, will remove. I need it to test CLI changes.\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Sat, Jun 22, 2013 at 5:45 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\n\n@@ -1,5 +1,6 @@\n #!/usr/bin/env ruby\n+$:.unshift(File.expand_path(\"../../lib\", FILE))\nIntentional?\nI like the bosh.dev idea.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/266/files#r4832138\n. Dmitriy, should\u00a0VERIFY_NONE be set?\n\u200b\n\u200b\u00a0\n\n\nSee\u00a0http://jamesgolick.com/2011/2/15/verify-none..html\u00a0for why I ask.\n. Its not really the \"raw_properties\" if its been modified, imo.\n. But perhaps that's ok for the purpose of the ticket.\n. I'll have a look asap. Didn't know about Shellwords. Thx.\n\"Not modified since 2007\" could be a good thing :)\nOn Fri, Aug 30, 2013 at 6:51 PM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\n\n@@ -18,6 +18,7 @@ Gem::Specification.new do |s|\n   s.require_paths = ['lib', 'config']\ns.add_dependency 'sqlite3', '~>1.3.7'\n-  s.add_dependency 'escape'\n  Interesting, I hadn't seen the escape gem before. \n  It looks like it hasn't been modified since 2007 though, does it do something that Ruby's built-in Shellwords doesn't?\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/405/files#r6098903\n. Its just formatting. I can take out the red/green if you don't like it. I thought it looked pretty. I'm not testing the stdout at all. Is anyone else testing stdout?\n. Aesthetic consistency with how we get values from Hashes in bosh code.\n\n\nOn Wed, Jan 29, 2014 at 1:41 PM, Jesse Zhang notifications@github.com\nwrote:\n\n\n\n[sc['name'], \"#{sc['version']}#{deployments.empty? ? '' : '*'}\", sc['cid']]\n[sc['name'], \"#{sc['version']}#{any_deployments ? '*' : ''}\", sc['cid']]\nend\n  +\ndef deployments_count(sc)\nsc.fetch('deployments_count', nil) || sc.fetch('deployments', []).size\n  what's the point of using fetch if we specify a default of nil?\n\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/501/files#r9285843\n. What will go wrong outside of bundler?\n\n\nOn Tue, Dec 23, 2014 at 4:26 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n\n@@ -30,7 +30,9 @@ def start\ncmd = \"bosh-registry -c #{@registry_config.path}\"\n-      @registry_pid = Process.spawn(cmd)\n-      Bundler.with_clean_env {\n  I dont think we can do that because CLI plugin does not depend on bundler. Tests happen to pass because they run in bundle exec?\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/720/files#r22241953\n. It's only cleaning the environment.\n\n\nOn Tue, Dec 23, 2014 at 4:26 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n\n@@ -30,7 +30,9 @@ def start\ncmd = \"bosh-registry -c #{@registry_config.path}\"\n-      @registry_pid = Process.spawn(cmd)\n-      Bundler.with_clean_env {\n  I dont think we can do that because CLI plugin does not depend on bundler. Tests happen to pass because they run in bundle exec?\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/720/files#r22241953\n. Ok, thanks for noting that. I'll figure out how to solve the problem externally where I must be leaking bundler stuff.\n. \n\n",
    "pmenglund": "You can simplify the tests by using\nlet(:options) {\n  Put a working hash here\n}\nAnd then use\noptions.dup.delete(key)\nIn your tests\n. This pull request is now outdated due to refactoring of the aws cpi.\n. Morning mate! You need to rebase...\n. We can't just release deployer 1.4.1, as the deployer depends on a bunch of other gems. I looked at going back in time to before they started to mess with the gemspec, but then you get a really old version which uses aws-sdk 1.3.9!\n. Fixed in 2e5d128766e53284348fa216660717ad6d319bea\n. The fix is in the aws cpi, and it is needed in three places:\n- bosh_aws_deployer\n- director\n- micro bosh stemcell\nYou can use the deployer directly from the bosh repo:\ncd ~/bosh; bundle exec bosh micro delete\nBuilding new stemcells is currently broken, so until that is fixed we can't release new stemcells, but it will hopefully be fixed soon so we can release new ones.\n. We can force delete a stemcell without any references, but if it is in use by a deployment, we can't force delete it.\n. I don't think we'll be able to automatically reconnect a deleted stemcell image, as it involves a lot of IaaS magic which we haven't got access to from the director.\n. use HEAD and it is solved\n. https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L346\nThe AWS API documentation says it returns a Hash, but it doesn't!\n. It is because it loads all requirements, and bat is one of them, which uses rspec\n. I'll need to respin the release of new stemcells, so this will be included in that release.\n. When it changed to the new format stemcell-copy was bundled in the deployer gem, so it is being run through a rubygems wrapper, so I guess your root user don't have ruby in its PATH.\n. Can you extract to a Unix base class? I plan to write a Solaris CPI :)\n. The Solaris (SmartOS) CPI exist, but it isn't public yet\n. FYI I switched sys-file system for sigar (as we already use that gem I the agent)\n. AWS slowness? I wonder if we can tune the ssh timeout setting?\n. Pre-compiled packages only work when you use exactly the same stemcell, so it'll be great for regular deployments, but trickier when you build micro versions. \n. Could you add some more testing?\n. The travis build is failing...\n. travis is still failing: https://travis-ci.org/cloudfoundry/bosh/jobs/5098580\n. The unit test should not do that test, move it into a platform specific functional test, so we can run those based on which linux distro we run.\n. I agree with Nic, the only thing we should have in the stemcell is \"enough\" to be able to install packages & jobs. The problem with having gcc as a package is that we need gcc to compile gcc on the compilation VM. In theory we could use a special compilation stemcell, but then things become much harder to manage...\n. FYI: I've just committed change 560eb68b16269ffd114f2bbff348e11ef46f79ff that moves the stemcell builder stuff from the guts of bosh_agent to the top level so you two'll have to rebase...\n. not your fault!\nsomeone else broke it - no names ;)\n. :D\n. Since you get a HTTP 404 error, are you sure you are using the right server?\n. Did the initial VM creation fail? It might be the case that the recreate code assumes we have successfully created the VM, as it wants to get the stored apply_spec, but that only gets stored when the VM has been successfully created.\n. The deployer code need to be refactored to be more clear (and simpler). It is currently cobbling together the apply specification that is sent to the micro bosh agent, using several inputs:\n- the apply_spec.yml from the micro bosh stemcell .tgz\n- the micro_bosh.yml\n- information from the current infrastructure (e.g. the VMs IP address)\nThe agent section in the micro_bosh.yml apply_spec is overrides to the default, which gets merged into the properties, so p('ntp') is right.\n. Excellent! Thanks for giving the vSphere code some TLC :)\n. fix merged\n. Yes, adding that will fix the problem. We don't have rspec coverage for vsphere :(\nTake a look at the aws counterpart:\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_deployer/spec/unit/aws/instance_manager_spec.rb\nand add a corresponding things for vSphere.\n. Merged Stefan's changes\n. I ran in to the same issue myself, and this is what I think needs to be done. I haven't tested it yet, as my vSphere test environment is not in order.\n\u00b1 me+kx |master \u2717| \u2192 git di\ndiff --git a/release/jobs/director/templates/director.yml.erb b/release/jobs/director/templates/director.yml.erb\nindex 1e7feda..3e6cf04 100644\n--- a/release/jobs/director/templates/director.yml.erb\n+++ b/release/jobs/director/templates/director.yml.erb\n@@ -133,15 +133,15 @@ cloud:\n         password: <%= password %>\n         datacenters:\n           <% datacenters.each do |dc| %>\n-          - name: <%= dc.name %>\n-            vm_folder: <%= dc.vm_folder || \"BOSH_VMs\" %>\n-            template_folder: <%= dc.template_folder || \"BOSH_Templates\" %>\n-            disk_path: <%= dc.disk_path || \"BOSH_Disks\" %>\n-            datastore_pattern: <%= dc.datastore_pattern %>\n-            persistent_datastore_pattern: <%= dc.persistent_datastore_pattern %>\n-            allow_mixed_datastores: <%= dc.allow_mixed_datastores || true %>\n+          - name: <%= dc['name'] %>\n+            vm_folder: <%= dc['vm_folder'] || \"BOSH_VMs\" %>\n+            template_folder: <%= dc['template_folder'] || \"BOSH_Templates\" %>\n+            disk_path: <%= dc['disk_path'] || \"BOSH_Disks\" %>\n+            datastore_pattern: <%= dc['datastore_pattern'] %>\n+            persistent_datastore_pattern: <%= dc['persistent_datastore_pattern'] %>\n+            allow_mixed_datastores: <%= dc['allow_mixed_datastores'] || true %>\n             clusters:\n-              <% dc.clusters.each do |cluster| %>\n+              <% dc['clusters'].each do |cluster| %>\n                 <% case cluster\n                    when OpenStruct %>\n                   <% cluster_hash = cluster.marshal_dump %>\n. We are trying to move away from accessing the properties directly, i.e. use the p() helpers, and as\ndatacenters.each do |dc|\nnow yield a Hash, we should be using it instead (or perhaps turn dc into an OpenStruct).\n. Merged Stefan's changes\n. Can you include the stacktrace?\n. This should have been fixed by a change I made, but if it shows up again, just reopen the issue...\n. A test broke: https://travis-ci.org/cloudfoundry/bosh/jobs/5283580\n. You can't - but I can, and have...\n. I think you need to rebase...\n. Holy smokes - that's a lot of changes! I'll see if I can go through it during this week...\n. Can you try setting proxy_url in your micro_bosh.yml:\ncloud:\n  plugin: aws\n  properties:\n    aws:\n      access_key_id: ...\n      secret_access_key: ...\n      proxy_uri: http://your-proxy.com\napply_spec:\n  properties:\n    aws:\n      access_key_id: ...\n      secret_access_key: ...\n      proxy_uri: http://your-proxy.com\nThat should take care of the communication with the AWS API endpoint, but there are later steps which I still think will fail...\n. It looks like it is correct\n. FYI: I just implemented that :)\nhttps://github.com/cloudfoundry/bosh/commit/05e2e2c2b736ae524cf624158f186a9e6421f0ca\n/M\nOn Mon, Mar 18, 2013 at 5:24 PM, gabis notifications@github.com wrote:\n\nWe have successfully used 8 in us-east-1. Did you have other activities\ngoing on within the account?\nI did add a story in our icebox to add throttle functionality to the AWS\nCPI, but I think reducing the default in bosh-cloudfoundry is also a good\nidea. We use 3 in our AWS template.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/81#issuecomment-15090666\n.\n\n\nMartin Englund, martin@englund.nu\nhttp://blog.codenursery.com/\n. The regression happened as our CI system unfortunately doesn't test upgrades, just clean installs. Sorry about that, I'll make sure someone looks at fixing it.\n. :-1: we need to implement disk flavors to be able to support this, as we currently can't pass cloud properties to the disk creation, so your code doesn't work - it isn't guaranteed that there is an instance tied to the disk creation\n. I'm not doubting that the code works, but the problem is that we need a generic solution to the problem of having different kinds of disks, and this only solves it for AWS (and in a hacky way as you stash config data in the registry).\nI'd rather see you work on that...\n. There are now an epic that describes the functionality:\nhttps://www.pivotaltracker.com/projects/767913/epics/584767\n. FYI my centos stemcell builder hack (I've just made it work, not polished it or made it work alongside ubuntu) is here: https://github.com/pmenglund/bosh/tree/centos\n. @frodenas haha - I'm sure it will be nit-picky :)\n. closing this pull request - some files snuck in which should not be there\n. abandoning due to the need for a rebase...\n. Why would we not be able to restore? There are two points in the normal cycle when we have good state: stemcell update and job/package update. Before we do one of those, but after the job(s) have been stopped, a snapshot will be taken, and you'll then be able to restore that if the update went south...\nYou can also elect to take a snapshot and stopping/flushing the job - might not be for CF, but it is a useful feature for other users of bosh.\n. When taking a snapshot the director will send a lifecycle event to the agent, which tells the job(s) running on the VM to drain, flush all state to disk and hold, then the snapshot is taken and the job(s) can resume. Exactly what needs to be done (and if it can be done) depends on the job, and has to be implemented in the job-specific script.\nWhen it comes to the normal update cycle (job/stemcell) we already drain & stop the service, so we will always be in a know good state before taking the snapshot, so it will be possible to easily revert to that.\nIt is possible to implement a deployment wide freeze and snapshot the whole system, however, it may take a while! Depending on the application it could be useful, e.g. having a good known starting point for CI.\n. E.g. here is how to take a usable snapshot of a MySQL database:\nhttp://alestic.com/2009/09/ec2-consistent-snapshot\n. I intend to add a field in the database which will let you know if the snapshot was clean or not (and a flag to the command) so you know if it can be used to restore or if you should use it in a distaster situation (when all else fails).\nI'll also add a \"flush\" lifecycle event, which jobs may optionally implement, where the job owner should do what the link above did.\nThis will take a snapshot and call the flush life cycle event, and if the job doesn't implement it, it'll give an error\nbosh snapshot ccdb 0\nthen you have to use:\nbosh snapshot ccdb 0 --dirty\nwhich will ignore the flush and thus the snapshot may contain incomplete data, but it is better than no snapshot!\n. Oh, and one more usecase for snapshot - which is the original reason I wanted to implement it: for (security) forensics purposes. We need a way to take a snapshot of a compromised VM (and all attached disks).\n. I closed it as it broke in travis https://travis-ci.org/cloudfoundry/bosh/jobs/6941753 and I don't have time to track down why. I might retry it this week as I'll have some spare time in the mornings again :)\n. We did this to secure the VM for the DEAs as they run potentially malicious code and we don't want it to be able fill /tmp - however now that we use warden it is no longer an issue, but I'm still wondering if it is a good idea to ship with restrictive /tmp and have the control script set up TMPDIR if you need it. Whaddyathink?\n. Agreed, we should provide a normal /tmp\n. How is that for feedback speed :)\n. You snooze you loose :)\n. wooooosh :)\n. What we also really need is a different user which can not use sudo so people can access the systems to troubleshoot but not make any modifications.\n. We already have a shell_cmd implementation, see bosh common sh\n. :+1: \n. Btw, the reasoning for enforcing bosh ssh was to get an audit trail of who accessed which system.\n. I agree with @mmb always running as root is asking for trouble - if you have to explicitly use sudo there is no excuse when you wedge the VM because your script failed. Just because you can run with a cocked shotgun doesn't mean you should...\n. Yay! Finally it'll shut up :)\n. Now you are just being plain lazy ;)\n. Bah, template problems - closing and creating a new PR...\n. Resque job that pulls it down, verifies it, then imports the stemcell. No curl - don't shell out!\n. I'm not sure I'd like the deployment to do the download, instead use something like:\nbosh download public stemcell --director\nDoing large file downloads in Ruby is fine if you do it the right wait, i.e. read it in chunks:\nNet::HTTP.start(\"site.com\") do |http|\n  File.open(\"/path/to/file.mov\", 'wb') do |file|\n    http.request_get('/' + URI.encode(\"stemcell.tgz\")) do |response|\n      response.read_body do |segment|\n        file.write(segment)\n      end\n    end\n  end\nend\n. As long as it kicks of two tasks, one to download the stemcell and one to do the deployment I'm on board. I just don't want to kick off a deployment and have it take an hour extra because it is pulling down a stemcell from the outside...\n. We have talked about having dynamic resource pools, where you can set an upper bound on the instance count for the pool, and then expose an endpoint (and cli option) to increase the count.\n. The pool instance count will have to be extracted from the bundled deployment manifest.\n. Right, you should be able to do something like:\nbosh instance dea +3\n. bosh vms\nlists all known vms, while\nbosh instance <instance> <delta/absolute>\nwould change the number of instances (and hence the vms too)\n. :+1: We talked about this last week and I agree, it is a very useful feature!\n. The failed output gets displayed by the cli, but the successful output is just discarded. @msackman has a valid point that we should store the successful output, and the best location is probably the blobstore (which then needs to be cleaned up when the compiled package is removed from the bosh DB)\n. :+1: the aws plugin should be split out into its own, and also cut up so you can do a bosh only deployment\n. :+1: bosh should do a much better job of building a dependency graph so we can compile in parallel too\n. I see that I worded it badly - we don't compile in an optimal order: we should start with the package with the most dependencies. This isn't visible when you do something with a simple dependency graph, but for something like CF we could make it faster by compiling so that we maximize the parallelism\n. @aramprice yes :)\n. Yay for cleanup \ud83d\ude04 \n. You mean like this: https://github.com/bbatsov/rubocop/pull/355\n. Whoa! Something went horribly wrong when I tried to squash the commits and rebase... closing and redoing :)\n. No steak knives for you guys...\n. Yaaaay!\n. Right, if you set director_uuid to ignore it works, but there is no warning that you now have the potential to target your production instance and deploy your test manifest - and nuke everything! I'd like to have it at least display a warning so you have a chance to catch it...\nA better solution would have been to let you specify the director uuid when you deploy the director.\n. @ryantang has a very good point and solution! Making the director refuse to accept manifests without a matching uuid is a good solution for putting the safety back on the loaded shotgun :)\n. :tada: \n. you don't need to define a new method, just assign it to a local variable, i.e.\nmock_cloud_options = { ...\nand just call it mock_options, it is more intuitive\n. using a hardcoded temp directory is bad - Tempfile by default will use TMPDIR if set\n. Instead of shelling out via back ticks, use Bosh::Common::Exec which will raise an error it the command fails\n. Bosh::Common::Exec is the preferred way of shelling out, and it is also easier to test :)\n. This line is commented out - remove it if it isn't needed\n. it would be even better if you changed it from shelling out to just reading /proc/sys/dev/cdrom/info in Ruby\n. much better, but can you add a test case too?\n. can you use sigar instead to get the mount?\n. sigar can do everything sys-filesystem can, so there is no need to pull in another gem\nhttp://www.hyperic.com/support/docs/sigar/\n. nit - missing newline\n. this is problem we already have, and I don't know if you have an idea how to decouple the infrastructure from the platform? right now, if you add another platform, you need to update the infrastructure too - but I don't know how to make a clean separation of the two...\n. you should be able to use retryable here\n. use sh instead\n. since you're messing with the file anyway, fix uses of backtick to be sh\n. you can use %r{} instead of // which will make it easier to read the regexp\n. use\nexpect {\n  ...\n}.to raise_error ...\ninstead of\nlambda {\n  ...\n}.should ...\n. wrap the contents in a\npending \"network config refactoring\" do\n    ...\n  end\ninstead\n. why an Object instead of a mock or a double?\n. wrap in a pending block\n. it would be good if you can do the linux/ubuntu/rhel abstraction the whole way - right now some linux pieces require ubuntu\n. why the change?\n. if you do this, the CPIs needs to be updated:\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/lib/cloud/vsphere/cloud.rb#L864\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_vcloud_cpi/lib/cloud/vcloud/cloud.rb#L531\n. but the retryable gem is removed - so this will break. we also make sure we don't introduce backwards incompatible changes in all bosh modules (if you use ~> 1.5)\n. sure, not your fault :)\n. no worries :)\nit is recommended to use double over mock and stub\nhttp://pragprog.com/book/achbd/the-rspec-book\nand they raise nice errors when called with an unknown method (instead of method_not_found which you get on an Object)\nFor example, this should be close to what you want:\nlet(:disk) { double(Bosh::Agent::Platform::Linux::Disk) }\n. I know this isn't your fault, but can you change or not to || ! - the operator precedence of and, or and not is not the same as for &&, || and !\n. just call it DisposableFile so if we have to change implementation again we won't have to rename all uses of it\n. found one more and :)\n. have the redis debug output ever helped you? :) I've made it configurable, and if we need it, it can be changed in the director.yml\n. good point - fixed it\n. I'm wondering if we should add an optional parameter to snapshot_disk(disk_id, options={}) so that we can pass in data to set the description for the snapshot, so it contains the deployment, job & index and possibly also the job & package versions\n. The error message is a bit misleading: there are other cloud providers available it is just that there is no template for it. \n. is this id or cid?\n. typo: Snapshot taken\n. It would be good with some yard docs for this call, as it is so complex and can do so many things :)\n. replace with\nrequire 'spec_helper'\n. shouldn't this recurse into sub-hashes and replace the keys of those too?\n. We use the /etc/sudoers.d directory, so you'll have to add the #includedir /etc/sudoers.d in what you add.\n. Don't shell out, use FileUtils\n. Same here...\n. And here :)\n. wrong paramater type :)\n. extract this into a method so you just call:\nprint_value(internal_info)\n. I think /config if a better endpoint name. is there more things than blobstore that should be exposed?\n. It is a technical debt from the rhel implementation, the classes need Platform::Centos so if it is moved to the top it is not yet defined. I'm not keen on refactoring everything :)\n. Right, it is backwards and should be fixed some day ;) It is just the OS flavor code that has this, the rest is \"normal\".\n. ",
    "cf-frameworks": "@ankurcha\n. The changes to the stages are in #18.\n. Will be done building on updates to bosh stemcell builder by the bosh team.\n. @ankurcha\n. Will be done building on updates to bosh stemcell builder by the bosh team.\n. ",
    "ankurcha": "-1\nThis stage is broken. You haven't updated any of the other parts of this stage. Please update /mcf_agent/apply.sh and any other pieces. It throws warnings which may cause problems (if something were to depend on the exit codes). Specifically the file copy\nAlso, I would suggest adding some tests to verify correct behavior of the script.\n. -1\nThis stage is broken. You haven't updated any of the other parts of this stage. Please update /mcf_agent/apply.sh and any other pieces. It throws warnings which may cause problems (if something were to depend on the exit codes). Specifically the file copy\nAlso, I would suggest adding some tests to verify correct behavior of the script.\n. There are still references to the old path and bundle install.\nAlso, I don't see any tests to verify whether these scripts work.\n. There are still references to the old path and bundle install.\nAlso, I don't see any tests to verify whether these scripts work.\n. I agree.\n. I agree.\n. +1\n. +1\n. Heh .. we just discussed the sys-filesystem route about an hour ago ourselves.\n. I would keep it simple for now. When you actually get around the Solaris CPI, I think that's when we should cross the  abstraction bridge. \n. If I am understanding this correctly, you are suggesting if settings['env'] && settings['env']['bosh'] or something else?\n. Cool I just made the changes. I'll wait for the tests run and go green.\n. I guess we both were working on this and I beat you to it :) Have a look at he current version, I added some tests around this so let me know what you think.\n. Closing this pull request as changes are merged into #60 (for cleanliness).\n. Thanks to @stefanschneider for the non-shell out way to parse the cdrom device\n. Waiting for CI to bless the pull request.\n. Why did the CI break?\n. \n;)\n. Take your time.\n. @pmenglund I made some changes as per your comments. Have a look.\n. This pull request is stale. Killing it.\n. What are you using the stemcell builder to build? aws stem cells or openstack or vsphere?\n. I don't know why this test fails but, if I check the deployment manually it passes.\n. Fix disk_usage when `df' has long output \u2026\ndisk_usage relies on the column based awk to parse the result of df. It\nworks fine in most cases but failed to handle the special cases when df\nhas long output like this:\n$ df -l\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/mapper/VolGroup00-LogVol00  49898164   2187920  45134680   5% /\n/dev/sda1               101086     13039     82828  14% /boot\ntmpfs                   190224         0    190224   0% /dev/shm\nNew method aligns at Use%, and slice all the characters before that in\neach line.\n. Can we be sure that it won't change behavior? Also, tests for this change?\n. tests? I am pretty sure tests would(should) break if the new kind of exception is thrown. correct?\n. My bad. I took care of it.\n. I don't particularly see why these need to be strings. Plus I don't see any direct dependency that cannot handle them beings int. Anyone from the pivotal team care to offer an opinion?\n. I made the quotes consistent with the rest of the file. Everything in the file uses single quotes.\n. It's a pretty benign change especially because it's perfectly backwards compatible and zero risk. I can if anyone else also opposes to this change, I will revert.\n. That's why i checked in the gemlock file.\n. That might happen too often. In what case do you suppose we need the log line and at what level?\n. I am merging this line directly into the code. I added some test cases and i can update them along with using Bosh::Exec.sh change. \nIt might be a much cleaner commit then\n. I thought the preferred way was to use Bosh::Exec#sh\n. Seems like an overkill. Can you please elaborate on the need for this class? I can see only sshd as the use case for this, which doesn't make much sense given that we would seldom want to turn off/on restart services. Also, this can be pretty dangerous.\n. Why put all the templates into the same location. There is a clear dichotomy in the platforms. Some of them are shared, others aren't. I would rather keep commons ones together and specific ones separate.\n. Can you comment on this choice of device bus path. I know it works but are you sure that all scsi devices will appear in the host=0, channel=0 and lun=0 format? How are we enforcing this?\n. Please comment on the need for this?\n. No this doesn't seem right. The test case depends on the builder's platform for test behavior? I would want the tests to be completely agnostic. What would happen if I execute the tests on a mac?\n. You are mocking out detect_block_device. This is a pretty integral part and I don't see tests for it. I would not recommend adding specific tests for that. I have seen the agent go into an infinite loop if there is a problem in detecting the scsi device.\n. restart_networking_service is pretty important. We want to make sure the right commands are being executed. Please add specific test cases.\n. This seems kinda iffy. The Logrotate class instance is described by the spec and stays in the namespace as a service class through out the life of the agent. install is a behavior of this particular instance of the log rotator.\nPlease explain your choice here.\n. +1 for deep test\n. I didn't know that. Didn't see it in the diff.\n. 1. You shouldn't need to start sshd service on either of the stemcells. We need to enforce that. What services are you accessing via ssh that it needs to be open? \n2. Someone might start an unprotected ftp service or any number of unknows service. String interpolation for shell execution is a clear risk vector.\n. Have a look at my repo too. I did the separation maybe you can pull up some of the code directly from there. Will save some effort.\n. How did you get this line then? As far as I know, we should be looking at /proc/scsi/scsi or something to that tune. Please add a TODO so that we keep that in mind.\n. This will just make the tests very platform dependent which is pretty bad. I would rather have mock out the very core file system interaction things. Things like the platform can be easily parameterized.\nThough, +1 for writing more non-mock tests.\n. Have a look at the platform branch or just do a diff on it. It should be fairly evident. Be advised that I have not yet merged your changes so the directory for redhat support is called rhel\n. sigar? can you give me a pointer to its usage. Any specific reason for not using sys-filesystem?\n. I'll have a look\n. Removed sys-filesystem. Please review and comment on usage.\n. I would rather have my direct dependencies declared explicitly. So that if common one day changes we are insulated from it. \nOn Mar 11, 2013, at 10:58, cf-bosh notifications@github.com wrote:\n\nIn bosh_agent/lib/bosh_agent/platform/linux/disk.rb:\n\n@@ -0,0 +1,136 @@\n+# Copyright (c) 2009-2012 VMware, Inc.\n+require 'bosh_agent/platform/linux'\n+\n+require 'sigar'\n+require 'retryable'\nwe incude retryable in bosh common, so you should be able to remove the require. see 3fb27c4\n\n\u2014\nReply to this email directly or view it on GitHub.\n. I saw that. But I would like to tackle one problem at a time especially given this is my first stab at the bosh_agent codebase. Though something we should add to the backlog as technical debt so that we can take time to address this problem. \n. I'll take care of it.\n. Not my test so didn't want to touch it.\n. That's my bad. Didn't mean to leave ubuntu there.\n. Why require the gem when it's already provided by the standard library? I prefer to keep out gem dependencies to a minimum. It doesn't change any semantics.\n. New to ruby. Don't know all the bells and whistles yet. Can you give me a reference for this?\n. I'll take care of it.\n. Yea I took care of it.\n. Done.\n. Done\n. Done.\n. Done\n. Stupid git updated this file for inconsistent CRLF usage.\n. There may be a better way to write these tests.\n. Running in \"dummy\" mode needs bosh agent to also run with \"dummy\" infrastructure and \"dummy\" platform.\n. I made this change so as to allow the user to view the output of the individual bosh commands if environment variable DEBUG is set. This makes life a lot easier when trying to figure things out.\n. Having the logs organized into folders makes things easier to reason/read than just having a big list of files.\n. \n",
    "mmb": "Sorry for closing this as the wrong user. These changes have already been fixed by the bosh team. @ajackson and I have been working on some more fixes for the Micro Cloud Foundry build that will be merged soon.\n. I see all dependencies on multi_xml as being removed a few days ago. Do you have any hanging around in your bosh Gemfile.lock?\n. When resolving the list of gems to copy it wasn't taking into account Gemfile.lock.\nI had the same issue with it trying to copy old blobstore_client dependencies because I had bosh_cli installed which has a dependency on blobstore_client 0.4.0. Since the old dependencies weren't in the bundle they weren't there to copy.\n. When resolving the list of gems to copy it wasn't taking into account Gemfile.lock.\nI had the same issue with it trying to copy old blobstore_client dependencies because I had bosh_cli installed which has a dependency on blobstore_client 0.4.0. Since the old dependencies weren't in the bundle they weren't there to copy.\n. This is for the micro cloud foundry build so the vsphere one.\nOn Mar 11, 2013 9:26 PM, \"Ankur Chauhan\" notifications@github.com wrote:\n\nWhat are you using the stemcell builder to build? aws stem cells or\nopenstack or vsphere?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/76#issuecomment-14757991\n.\n. This is for the micro cloud foundry build so the vsphere one.\nOn Mar 11, 2013 9:26 PM, \"Ankur Chauhan\" notifications@github.com wrote:\nWhat are you using the stemcell builder to build? aws stem cells or\nopenstack or vsphere?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/76#issuecomment-14757991\n.\n. If that's the case this should be fixed in MCF. Thanks.\n. If that's the case this should be fixed in MCF. Thanks.\n. Did some more work on this. Removed all shelling out as @pmenglund suggested.\n\nI verified that the bosh_sudoers stage in the stemcell builder is already adding the line to include sudoers.d.\nThe code to ask for the password is still there and should be removed if this gets merged.\n. Did some more work on this. Removed all shelling out as @pmenglund suggested.\nI verified that the bosh_sudoers stage in the stemcell builder is already adding the line to include sudoers.d.\nThe code to ask for the password is still there and should be removed if this gets merged.\n. Made changes after feedback from @hiremaga.\nshell_cmd could probably be moved to its own class and the spec could be DRYed up.\n. Made changes after feedback from @hiremaga.\nshell_cmd could probably be moved to its own class and the spec could be DRYed up.\n. This is ready for a little more testing and merge if we decide to take it.\n. This is ready for a little more testing and merge if we decide to take it.\n. I think there is still value in creating the single-use users even though it takes longer. It's more secure and it avoids issues with managing the authorized keys if multiple users are sshing into the same user on the instance.\nsudo still seems like a good idea for the same reason Linux distros use it (principle of least privilege, auditability, prevent accidents).\nHopefully we can get to the point where sudo is rarely required.\n. I think there is still value in creating the single-use users even though it takes longer. It's more secure and it avoids issues with managing the authorized keys if multiple users are sshing into the same user on the instance.\nsudo still seems like a good idea for the same reason Linux distros use it (principle of least privilege, auditability, prevent accidents).\nHopefully we can get to the point where sudo is rarely required.\n. Merged https://github.com/cloudfoundry/bosh/commit/6191c586cbb52b5a84381314fb641c8c9dfc0a02 but reverted https://github.com/cloudfoundry/bosh/commit/e28eada489179cda128ca96b396950222aa670fa pending future changes to ssh.\n. Merged https://github.com/cloudfoundry/bosh/commit/6191c586cbb52b5a84381314fb641c8c9dfc0a02 but reverted https://github.com/cloudfoundry/bosh/commit/e28eada489179cda128ca96b396950222aa670fa pending future changes to ssh.\n. +@hiremaga: Rebased against master and merged.\n. - @hiremaga: Rebased against master and merged.\n. +@hiremaga: Rebased against master and merged.\n. +@hiremaga: Rebased against master and merged.\n. Merged.\n-Matt and Gaston\n. Merged.\n-Matt and Gaston\n. Merged.\n-Matt and Gaston\n. Merged.\n-Matt and Gaston\n. This was merged:\nhttps://github.com/cloudfoundry/bosh/commit/636229225378a3286ab5ac7cb557b68c54ff0887\n. This was merged:\nhttps://github.com/cloudfoundry/bosh/commit/636229225378a3286ab5ac7cb557b68c54ff0887\n. +1 for making bosh itself easier versus wrapping it with external tools.\nShould there be a stemcell SHA in the manifest that the director can verify after download?\nI like Martin's idea of the client checking if stemcells exist and downloading before deploy, but not as part of the deploy.\n. +1 for making bosh itself easier versus wrapping it with external tools.\nShould there be a stemcell SHA in the manifest that the director can verify after download?\nI like Martin's idea of the client checking if stemcells exist and downloading before deploy, but not as part of the deploy.\n. @tsaleh Is there any plan to implement this?\n@mmb / @matthewmcnew \n. :+1:\n. :+1:\n. Merged.\n. Merged.\n. Merged to master. - MB / AP\n. Merged to master. - MB / AP\n. Added vendored gems.\n. Added vendored gems.\n. Merged https://github.com/cloudfoundry/bosh/commit/1cd5440f151ea9ad7ffda3c1beabcadb4d91cc66\n. Merged https://github.com/cloudfoundry/bosh/commit/1cd5440f151ea9ad7ffda3c1beabcadb4d91cc66\n. Merged to master. - MB / AP\n. Merged to master. - MB / AP\n. This pull request has been merged in fe5c19d0bd655e41f6b9531a24383a42571a9e36\n@frodenas Can this branch be deleted?\n. This pull request has been merged in fe5c19d0bd655e41f6b9531a24383a42571a9e36\n@frodenas Can this branch be deleted?\n. Merged to develop branch.\n@mmb / @AndreasMaier\n. Merged to develop branch.\n@mmb / @AndreasMaier\n. @frodenas Can you confirm that we can delete the blobstore_exceptions branch now?\n. @frodenas Can you confirm that we can delete the blobstore_exceptions branch now?\n. Reviewed and still failing specs locally and on Travis with:\nFailure/Error: aws.bootstrap_micro\nTypeError:\ncan't convert nil into String\n@mmb / @matthewmcnew\n. Reviewed and still failing specs locally and on Travis with:\nFailure/Error: aws.bootstrap_micro\nTypeError:\ncan't convert nil into String\n@mmb / @matthewmcnew\n. Also the director.yml.erb and registry.yml.erb templates will need to pass the ssl_verify_peer openstack option to the job.\nIt can be assume that the value will always be present if it is added as a job property with a default.\n@mmb / @matthewmcnew\n. Also the director.yml.erb and registry.yml.erb templates will need to pass the ssl_verify_peer openstack option to the job.\nIt can be assume that the value will always be present if it is added as a job property with a default.\n@mmb / @matthewmcnew\n. We saw the same error this morning running bosh vms on a Pivotal CF deployment that had gotten into a bad state.\n``\nDeploymentcf-c3692b2ed65396f3add9'\nDirector task 33\nError 100: PG::Error: ERROR:  invalid input syntax for integer: \"\"\nLINE 1: ...yment_id\" = 1) AND (\"job\" IS NULL) AND (\"index\" = '')) LIMIT...\n                                                             ^\nTask 33 error\nFailed to fetch VMs information from director\n```\n@mmb and @cunnie\n. We saw the same error this morning running bosh vms on a Pivotal CF deployment that had gotten into a bad state.\n``\nDeploymentcf-c3692b2ed65396f3add9'\nDirector task 33\nError 100: PG::Error: ERROR:  invalid input syntax for integer: \"\"\nLINE 1: ...yment_id\" = 1) AND (\"job\" IS NULL) AND (\"index\" = '')) LIMIT...\n                                                             ^\nTask 33 error\nFailed to fetch VMs information from director\n```\n@mmb and @cunnie\n. @drnic , we've moved this into the BOSH icebox for prioritization by @goehmen \nCF Community Pair (@dsabeti & @mmb)\n. @drnic , we've moved this into the BOSH icebox for prioritization by @goehmen \nCF Community Pair (@dsabeti & @mmb)\n. We're currently talking with PMs and the BOSH team to determine the complete strategy for supporting different blobstores and formats. When there's a clear idea for how to move forward, we'll   leave notes on these PRs and hopefully merge some of them in.\nCF Community Pair (@dsabeti & @mmb)\n. We're currently talking with PMs and the BOSH team to determine the complete strategy for supporting different blobstores and formats. When there's a clear idea for how to move forward, we'll   leave notes on these PRs and hopefully merge some of them in.\nCF Community Pair (@dsabeti & @mmb)\n. If we remove the deployments key older clients will falsely report that the stemcell is not in use.\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/stemcell.rb#L158\nIs it ok to break backward compatibility?\n- @mmb / @ytolsk\n. We're currently talking with PMs and the BOSH team to determine the complete strategy for supporting different blobstores and formats. When there's a clear idea for how to move forward, we'll   leave notes on these PRs and hopefully merge some of them in.\nCF Community Pair (@dsabeti & @mmb)\n. We're currently talking with PMs and the BOSH team to determine the complete strategy for supporting different blobstores and formats. When there's a clear idea for how to move forward, we'll leave notes on these PRs and hopefully merge some of them in.\nCF Community Pair (@dsabeti & @mmb)\n. We're currently talking with PMs and the BOSH team to determine the complete strategy for supporting different blobstores and formats. When there's a clear idea for how to move forward, we'll   leave notes on these PRs and hopefully merge some of them in.\nCF Community Pair (@dsabeti & @mmb)\n. In Ubuntu, runningsudo su - also sources the .bashrc. The same is not true on CentOS. It looks like we're only adding /var/vcap/bosh/bin to the PATH in .bashrc files, hence the discrepancy between Ubuntu and CentOS.\nThe recommended fix is to create a .bash_profile during stemcell create that sources .bashrc -- .bash_profile should always be sourced on login. See https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/bosh_users/apply.sh\n@drnic Do you plan on fixing this with a PR, or should we put this in the BOSH icebox for prioritization?\nCF Community Pair (@dsabeti & @mmb)\n. Ok, we'll be put this in the BOSH icebox and it will get done at some point. In the meantime, we'll close the ticket.\nCF Community Pair (@dsabeti & @mmb)\n. Hi @heei3k,\nCan you confirm that you have a cluster named \"test\" in the vCenter Hosts and Clusters view?\nAlso, can you also confirm that there is a folder called \"template_folder\" in the datacenter in the vCenter Inventory view?\nCF Community Pair (@dsabeti & @mmb)\n. It looks like you may have found a bug (we still need to look into it more). But we do have a work-around that you can use in the meantime. In your micro-bosh configuration file, replace this section:\nclusters:\n  - test\nand use this instead:\nclusters:\n  - test:\n      resource_pool: test_pool\nBefore you try to deploy this, you also need to add a resource pool to that cluster.\nCF Community Pair (@dsabeti & @mmb)\n. We've filed a bug in the BOSH icebox for this case. In the meantime, you can use the work-around above to deploy micro bosh.\nCF Community Pair (@dsabeti & @mmb)\n. If you remove the following lines:\nsnapshot_schedule: false\nself_snapshot_schedule: false\nthen BOSH defaults those fields to job defaults of 0 0 7 * * * and 0 0 6 * * *, respectively. Are you hoping to be able to disable the scheduler, or are you just using this example manifest and need to fix the error?\n@frodenas Is there any reason not to remove these from the example to get the job defaults?\n. @matjohn2 Are you interested in submitting a pull request to remove these 2 lines from the example?\nCF Community Pair (@dsabeti & @mmb)\n. Hi @gberche-orange,\nWould you be willing to add tests to cover the following cases?\n- environment variable https_proxy gets passed to Net::HTTP::start as the proxy\n- if find_proxy returns anything it gets passed to Net::HTTP::start as the proxy\n- instance manager retries on HTTPClient::ConnectTimeoutError, HTTPClient::BadResponseError when waiting for agent and director\nCF Community Pair (@dsabeti & @mmb)\n. Hi @gberche-orange \nThere were a couple rubocop failures that can be seen here: https://travis-ci.org/cloudfoundry/bosh/jobs/19252438\nAlso would it be possible to test the new connection errors in a way that makes sure the code does the right thing when these errors are raised? Currently, the test you added to instance_manager_spec.rb only asserts that errors have been added to the list (rather than testing any logic pertaining to those errors). We're aware that the existing coverage in that file is small, but it's our policy to make sure that all PRs cover the added functionality.\nCF Community Pair (@dsabeti @mmb)\n. Can you paste your (sanitized) .yml configuration in a gist? The yaml parser is very picky about whitespace, so if that's wrong validation of the configuration could fail. Make sure the resource_pool property is indented one additional level from the line that reads - test:\nCF Community Pair (@dsabeti & @mmb)\n. Looks like the indentation of the resource_pool field needs to change. Try adding four spaces to it. The section of the config should look exactly like this:\nclusters:\n  - test:\n      resource_pool: test_pool\n. The line:\nDatastore not accessible to host, datastore1, \"datastore2\"\nindicates that the it chose the datastore that was on the other host. If you want to have multiple hosts in a cluster they must all have access to the same shared datastores (usually accomplished with shared storage like a SAN or NAS). I was going to suggest removing one of your hosts from the cluster to fix it but it sounds like you got it all working. Nice job and good luck with CF.\n. The commit looks good, but we can't accept any pull requests without appropriate test coverage. Could you add a spec that covers the case where the parameter is configured?\nCF Community Pair (@dsabeti @mmb)\n. @matjohn2 \nA miscommunication on our part: by 'specs' we meant 'tests' (terms that are often interchanged around here). The overloaded meaning, however, was lucky -- you also needed to add the property to the job spec, so good work!\nThe tests that you need to add pertain to helpers_spec.rb (and the wait_resource method in particular), as well as the director.yml.erb.erb_spec.rb which tests the how the director configuration is rendered. In the helpers_spec, you should test that the correct parameter is passed to sleep, and the in the director spec, you should test that new option appears in the rendered template.\nCF Community Pair (@dsabeti & @mmb)\n. Moved to the BOSH icebox for prioritization.\nCF Community Pair (@dsabeti @mmb)\n. The datastore pattern will choose the datastore but it will not choose the host. Any host in the cluster could be chosen by DRS and all hosts are expected to have access to the same datastores. In your case, where you have multiple hosts without shared storage you might be better off making two clusters with one host each and defining both clusters in your manifest.\nThere are two types of 'resource_pools' here; they are completely unrelated, but are unfortunately called the same thing. \nThe first is a vCenter resource_pool, defined in the clusters section of your manifest (in your case, test_pool). It is a collection of VMs that have an aggregate limit on resource (e.g. all VMs together can only use 12GB or RAM, etc.).\nThe second is a Bosh resource_pool, defined in top of your manifest in the following section:\nresource_pools:\n- name: small\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu\n    version: 1868\n  network: default\n  size: 5\n  cloud_properties:\n    ram: 512\n    disk: 8192\n    cpu: 1\n- name: director\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu\n    version: 1868\n  network: default\n  size: 1\n  cloud_properties:\n    ram: 2048\n    disk: 8192\n    cpu: 2\nThis defines VM settings for BOSH jobs, such as which stemcell to use and other cloud properties like RAM/disk/CPU per VM. For example, a VM from the director resource_pool uses stemcell bosh-vsphere-esxi-ubuntu, version 1868, and has 512MB of RAM, 8GB of disk space and 1 CPU.\nCF Community Pair (@dsabeti & @mmb)\n. All the subdirectory specs do this because when you run from the top level, sub_dir/spec isn't in the load path. Maybe we can come up with a better way to do this.\n. Recursion might cause some extra keys to appear because the original behavior adds keys with empty string values if they don't exist. If this behavior is a bug and not a feature, we could remove it and do the whole manifest with one recursive call.\n. It would be better to set the default of 1 here instead of as a default in the code.\nCF Community Pair (@dsabeti & @mmb)\n. You can remove the if_p logic here if you define a default in the spec above.\n. See the comment below: if you provide a default in the spec, you can get rid of this default here.\n. No need to do this check if you provide the default in the spec.\n. ",
    "frodenas": "@drnic Should be solved with change 8cbd6522\n. This is why I didn't close this issue ;)\n. Force delete stemcell: d1dc310\n. Fixed at https://github.com/cloudfoundry/bosh/commit/490275e989c21c15c7ea026de945b1486f42f6ff, but unless you're building your own stemcell, you'll need to wait and we publish a new one.\n. @anfernee @d If you're going to fix it, don't shell out the df command, use the sys-filesystem gem instead.\n. Your glance endpoint doesn't return the version, hence the \"300 Multiple Choices\" error. You should change your OpenStack glance endpoint URL and add a version:\nkeystone endpoint-create --region <your_region> --service_id <Image Service ID> \\\n--publicurl \"http://192.168.1.2:9292/v1\" \\\n--adminurl -\"http://192.168.1.2:9292/v1\" \\\n--internalurl \"http://192.168.1.2:9292/v1\"\n. It should be the same.\nEl 11/02/2013, a las 23:42, tpradeep notifications@github.com escribi\u00f3:\n\nI had figured that in one of ur earlier responses to another issue of the \nsame kind. I want to know whether v1.0 & v1 is considered the same or not ? \n- Pradeep \nOn Tuesday, February 12, 2013, Ferran Rodenas wrote: \n\nYour glance endpoint doesn't return the version, hence the \"300 Multiple \nChoices\" error. You should change your OpenStack glance endpoint URL and \nadd a version: \nkeystone endpoint-create --region  --service_id  \\ \n--publicurl \"http://192.168.1.2:9292/v1\" \\ \n--adminurl -\"http://192.168.1.2:9292/v1\" \\ \n--internalurl \"http://192.168.1.2:9292/v1\" \n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/42#issuecomment-13420728. \n\n\nSent from iPhone\n\u2014\nReply to this email directly or view it on GitHub.\n. @drnic Yes, just try to list images (using the glance endpoint) and you'll get that error.\n. Good catch, it seems we forgot to add several required components when we reorganized the repo. Fixed at 91a1e59. \n. @rkoster Can you please ssh to your microBosh instance and check the contents of the dns.server config option at /var/vcap/jobs/director/config/director.yml? Check also at the wordpress vm the contents of /etc/dhcp3/dhclient.conf, which is the prepend domain-name-servers?\n\nBTW, the floating IP address should be set for the nginx job (not the wordpress job). There's a deployment manifest example at https://github.com/cloudfoundry/bosh-sample-release/blob/master/wordpress-openstack.yml\n. Deployer didn't update the dns server address (it's 127.0.0.1 in your director.yml), so the agent is not injecting automatically the dns server for each vm. The 10.200.7.1 address at resolv.conf is injected by nova (it should be the default gateway for your network). Which version of bosh_deployer are you using?\nA bypass if you don't want to rebuild your microBosh is to modify manually the director.yml.erb, changing the dns.server address, and restarting the director (sudo monit restart director).\n. Director.yml.erb has been changed to use the new style properties (08cccdd). I guess there's now some new mandatory property, so if it's not filled then it doesn't detect that cpi plugin should be aws. The only thing it has been changed it's the region. Can you please confirm that you've a region property on the micro_bosh.yml aws credentials.\n. @rkoster What version of OpenStack are you using? Seems OS is not returning a servers hash when fog calls list_server_details. Can you please use fog to connect to your OS environment and execute a simple openstack.servers?\n. Regarding the original issue, it has been fixed. The Bosh cck command now allows you to delete the vm reference, so you can recreate it later. I'm closing this issue, if it happens again, feel free to reopen it.\n@rkoster Regarding the registry issue, we'll follow it in #96 \n. No matter what you set at update/canary settings, the compilation VMs are killed one by one. Nasty behaviour, added to the backlog.\n. /var/tmp/bosh is where the stemcell is stored after build. Do you mean delete all contents (mnt, chroot, ...) except the stemcell tgz?\n. @rkoster Another change was needed: plugin -> provider\n. Seems that the registry is losing the connection to OpenStack, it tries to reauthenticate but it fails.\n. There's a bug in the fog gem. Once a user token has expired, it doesn't reauthenticate because it using the same token again and doesn't ask for another new token.\n. @drnic, the 698 stemcell doesn't contain the #235 PR, it'll be in stemcell >=704 (not yet published). The user-data  usually doesn't contain the user/pwd for the registry (except in the microbosh vm). The bosh_registry implements a security mechanism when reading settings that checks that the ip of the vm asking for settings is the same as the ip of the settings requested. It'll be useful to see the vm logs to check exactly what's happening in your case.\nRegarding cancelling a compilation VM, actually it's not possible. We've an story in our backlog to deal with this issue.\n. Correction: The patch is included in stemcell >= 703 (it has been published just a few minutes ago).\n@rkoster Can you please try the latest stemcell?\n. @rkoster Did your agents lost the connection again?\n. Thanks @rkoster for reporting back! Reopen the issue if the bug appears again.\n. @rkoster Can you please confirm if your OpenStack version is Essex? This problem has been fixed for Folsom, but seems that Essex returns a different over limit message.\n. @rkoster Solved per PR #101 (OpenStack CPI helper will now detect \"overLimitFault\"). But until we release a new stemcell, you should need to create your own stemcell.\n. Demo'd and test these changes on a call with Gabi today.\n. @pmenglund Fixed both comments.\n. @pmenglund Be ready for your next PR!!! :)\n. @drnic We've not seen this error in our local environment nor in our integration tests because we're using ruby 1.9.3-p327 installed via rvm or rbenv. But I've reproduced the error when using bosh-bootstrap. Seems an issue with ruby + psych + libyaml version. If I log in into my inception VM I get a Psych version 1.2.2, and if I load a yaml file, I get the error when invoking the root method. In my local env, using 1.9.3-p392, I get a Psych version 1.3.4, and I'm able to invoke the root method.\nI'm trying to figure out how to install the latest psych/yaml code without compiling ruby.\n. @rkoster It's a know regression bug, it should be addressed in a later comit https://github.com/cloudfoundry/bosh/commit/4e1aaf68b2d309a71b84b602de56e9903ed2e970, but I have not yet finished all my tests (I suspect the flip flopping dns issue is not fixed). Sorry for any inconvenience and thanks for your patience!\n. Not now, the .tgz is a bare stemcell (so you need to upload it to AWS), and the _ami.tgz and the _light.tgz are actually tied to us-west-1 (or east, I don't remember exactly which one). In the future, when we publish them to other regions, the only ones that will survive should be the .tgz and *_light.tgz, and the last one will contain ami's ids for all regions, so no need to add the region to the stemcell name.\n. @TieWei Check if the flavor you're using to deploy microbosh has ephemeral space. The agent expect to find a device /dev/vdb (https://github.com/cloudfoundry/bosh/blob/master/bosh_agent/lib/bosh_agent/platform/linux/disk.rb#L41 and https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb#L670) so it waits until the device is ready. If there's no ephemeral disk, it'll wait indefinitely.\n. @TieWei Yes, persistent disk will be an OpenStack volume, usually /dev/vdc.\n. No, because when deployer spins up the new vm it tells the agent that there's no persistent disk, so the agent will not wait for that disk. Later, when we can assure that the vm and the agent have started correctly (hence the wait_until_agent_ready), we attach the disk and tell the agent to mount the persistent disk\n. We need to wait until a new fog gem is released, and then modify the CI system to publish the remote stemcells.\n. And can you please do the same on the openstack cpi? xoxo\n. Sorry, didn't see that WIP. \nBut why won't work? sed pattern will return kvm-buildnumber, exactly what we need to download the file (ie: s3://bosh-ci-pipeline/stemcells/openstack/micro/microbosh-stemcell-openstack-kvm-buildnumber.tgz). Then we link it to a unversioned filename, and that filename is what we're going to download.\n. Thanks @drnic!\n. :)\n. Added unit test.\n. Check your cinder-volume.log to find out the reason why OpenStack cannot create a volume. Check also the free space on the cinder-volumes (or stack-volumes) LVM group (vgdisplay). If you're using devstack, by default allocates a total of 5GB for volumes (it can be overridden by VOLUME_BACKING_FILE_SIZE var), so if the size of all volumes in your OpenStack environment is greater that that, all new volumes will fail.\n. Bosh doesn't route ssh through the microbosh by default. You should tell that you want to use a gateway by using bosh ssh --gateway_host <microbosh IP> --gateway_user <vcap>.\n. @drnic Closing this issue. If you have any other questions or want more clarification, please reopen again.\n. @foexle Thanks for submitting this PR! We really appreciate external contributions. \nBut there's a problem with the approach you've taken. If you untar any stemcell you'll see a stemcell.MF file, this file is created during the stemcell build process. When director calls create_stemcell, the cloud_properties that it passes are the ones that are inside the stemcell (we should rename cloud_properties to stemcell_properties to avoid confusion btw). That means that we should have 2 different stemcells, one with a property setting visibility to public, and another one setting it to private.\nYesterday, after your post at the bosh group, I modified the OpenStack CPI to allow setting stemcell's visibility during the (micro)Bosh deploy. So instead of having different stemcells, we can use the same old ones, and users, when deploying a (micro)Bosh, can set a property in micro_bosh.yml (or at the full Bosh deployment manifest) with the visibility of stemcells. You can see my changes on PR #254.\n. @foexle The fact that stemcell is public doesn't mean that anyone could see all settings, password, ... The image only contains the bits for the Bosh agent (and in a microBosh stemcell, the bits for a mini director). It's when you deploy a (micro)Bosh that the vm (not the image) is populated will all the settings. Anyway, let's take the restrictive option and set it false by default. As @gabis commented, I've modified #254 to set public visibility false by default.\n. Check first if you can attach a volume: Goto the OpenStack Dashboard and create a 16Gb volume. Once the volume is created, try attaching it to an existing server (or create a new one). Log in the server and check that the disk is attached (sudo fdisk -l). If anything goes wrong, check for errors at the cinder-volume log file on your OpenStack box.\n. @TieWei You can download an updated OpenStack stemcell from:\n- microBosh: http://bosh-jenkins-artifacts.s3.amazonaws.com/last_successful_micro-bosh-stemcell-openstack.tgz\n- Bosh: http://bosh-jenkins-artifacts.s3.amazonaws.com/last_successful_bosh-stemcell-openstack.tgz\n. Not sure why this is happening when you create a new stemcell, we never faced this problem. The builder process installs libyaml-dev in chroot, so ruby configure should detect it. \n. Closing this PR as behaviour between OpenStack versions made it incompatible (some versions require a device name, others works with \"auto\", and other with nil). \n. @hiremaga Sometimes my PRs are not related to tracker stories, so I manually open stories in tracker. For this one: https://www.pivotaltracker.com/story/show/51775131\n. @yuanyangen Use the vcap user when sshing to your vm (http://docs.cloudfoundry.com/docs/running/deploying-cf/openstack/deploying_microbosh.html#microbosh_ssh).\n. @yuanyangen Use the vcap user when sshing to your vm (http://docs.cloudfoundry.com/docs/running/deploying-cf/openstack/deploying_microbosh.html#microbosh_ssh).\n. Seems everything is ok, as the director is running. Try following the next steps at the guide (target director). I believe this was a transient error when trying to connect to the director.\n. Seems everything is ok, as the director is running. Try following the next steps at the guide (target director). I believe this was a transient error when trying to connect to the director.\n. @yuanyangen Did you get your MicroBosh working?\n. @TieWei Good catch! The prepare_network_change message never calls the post_prepare_network_change method because there's a bug at the publish method (it doesn't pass the closure to nats). I'll submit a fix soon.\n. @TieWei Good catch! The prepare_network_change message never calls the post_prepare_network_change method because there's a bug at the publish method (it doesn't pass the closure to nats). I'll submit a fix soon.\n. This is a bug with the OpenStack environment you're using. The error happened because OpenStack was unable to take a volume snapshot (Snapshot 39 state is error). From here, if you try to delete the volume, then OpenStack is unable to do that because it has a dependent snapshot, regardless of the snapshot status (Volume still has 1 dependent snapshots). You can delete the vm reference using cck, but the volume and the snapshots will be there. The only way to delete them is that a user with OpenStack admin privileges deletes the failing snapshot, and then the volume.\nThere's a feature in the backlog to enable/disable snapshotting (51842705). It will be usefull when your environment doesn't support snapshotting.\n. This is a bug with the OpenStack environment you're using. The error happened because OpenStack was unable to take a volume snapshot (Snapshot 39 state is error). From here, if you try to delete the volume, then OpenStack is unable to do that because it has a dependent snapshot, regardless of the snapshot status (Volume still has 1 dependent snapshots). You can delete the vm reference using cck, but the volume and the snapshots will be there. The only way to delete them is that a user with OpenStack admin privileges deletes the failing snapshot, and then the volume.\nThere's a feature in the backlog to enable/disable snapshotting (51842705). It will be usefull when your environment doesn't support snapshotting.\n. PR #325 will allow you to enable/disable snapshotting (it will be disabled by default).\n. @drnic New gems and stemcells have been published. Now we can enable/disable snapshotting using the director.enable_snapshots property, by default is set to false.\n. @oppegard Not sure why, I looked at that commit, but there isn't any comment about the change reasons. \n. @msackman A quick bypass until we fix this, ssh into your MicroBOSH vm, goto the /var/vcap/packages/director/gem_home/gems/bosh_aws_cpi-1.5.0.pre.3/lib/cloud/aws directory and modify:\n- cloud.rb#L274: name << devices.first.split('/').last unless devices.empty?\n- cloud.rb#L282: TagManager.tag(snapshot, :device, devices.first) unless devices.empty?\nThen restart director monit restart director and redeploy. Let me know if it works please.\n. @gabis ok, will take a look.\n. Rebased against master to allow merge.\n. Rebased against master to allow merge.\n. If you have an ephemeral disk, you can control how much space is allocated for Bosh Agent data (via flavor). If you don't have ephemeral disk, you're tied to a 10Gb disk and you don't have swap.\n. PR #365 upgrades the fog gem.\n. PR #365 upgrades the fog gem.\n. @drnic @tsaleh  Added a command to list the current locks, but the way that Bosh sets the timeout doesn't help to figure out how long will take until the lock disappears: locks are renewed automatically if the lock is expired, and they're not released until the process requiring the lock is finished. So not sure if this feature will be useful at all.\n. @drnic @tsaleh  Added a command to list the current locks, but the way that Bosh sets the timeout doesn't help to figure out how long will take until the lock disappears: locks are renewed automatically if the lock is expired, and they're not released until the process requiring the lock is finished. So not sure if this feature will be useful at all.\n. No comments ;)\n. No comments ;)\n. @mmb Deleted.\n. @mmb Deleted.\n. @mmb It can be deleted.\n. @mmb It can be deleted.\n. AFAIK this has never been fixed\n. AFAIK this has never been fixed\n. The deprecated option is still around in multiple files.\n. The deprecated option is still around in multiple files.\n. Yep, the \"Preparing package compilation\" stage doesn't behave like the rest of stages.\n. Yep, the \"Preparing package compilation\" stage doesn't behave like the rest of stages.\n. Nise BOSH uses a /24 network, that means that IP address .0 and .255 are reserved for network and broadcast respectively. If your local IP address is .255 it will conflict with the broadcast address. /cc @yudai\n. Nise BOSH uses a /24 network, that means that IP address .0 and .255 are reserved for network and broadcast respectively. If your local IP address is .255 it will conflict with the broadcast address. /cc @yudai\n. Not yet, waiting to be merged.\n. Not yet, waiting to be merged.\n. This is still an issue. OpenStack environment with libguestfs enabled fails when Bosh tries to create vm's.\n. This is still an issue. OpenStack environment with libguestfs enabled fails when Bosh tries to create vm's.\n. Still happens.\n. Still happens.\n. You'll need to modify the Bosh OpenStack CPI and the Bosh Registry in order to pass the connection options to fog.\nAlso, modify the Bosh release to accept other connection options (both director and registry). Be careful because we cannot use environment vars here.\n. You'll need to modify the Bosh OpenStack CPI and the Bosh Registry in order to pass the connection options to fog.\nAlso, modify the Bosh release to accept other connection options (both director and registry). Be careful because we cannot use environment vars here.\n. Director & Registry reads the config options from config files generated using templates (located at director and registry jobs). So the 1st step is to allow the required options at the template files.\nThen they pass that options to the CPI and Registry. So we need to read the new options and pass them to fog (director and registry).\nAnd that's all!\n. Director & Registry reads the config options from config files generated using templates (located at director and registry jobs). So the 1st step is to allow the required options at the template files.\nThen they pass that options to the CPI and Registry. So we need to read the new options and pass them to fog (director and registry).\nAnd that's all!\n. ```\nbosh locks\n+------------+---------------------+-------------------------+\n| Type       | Resource            | Expires at              |\n+------------+---------------------+-------------------------+\n| deployment | wordpress-openstack | 2013-09-11 15:47:07 UTC |\n+------------+---------------------+-------------------------+\nLocks total: 1\n.\nbosh locks\n+------------+---------------------+-------------------------+\n| Type       | Resource            | Expires at              |\n+------------+---------------------+-------------------------+\n| deployment | wordpress-openstack | 2013-09-11 15:47:07 UTC |\n+------------+---------------------+-------------------------+\nLocks total: 1\n```\n. See https://www.pivotaltracker.com/story/show/62353024 and https://www.pivotaltracker.com/story/show/56811806\n. See https://www.pivotaltracker.com/story/show/62353024 and https://www.pivotaltracker.com/story/show/56811806\n. I'm closing the PR. It solves the original problem, but can create another ones, so waiting until we rethink how cck works.\n. @thansmann I believe this PR has been superseded by #461. @mikepatterson @blueboxjesse Can you please give a try using latest stemcell (1744)? It should work just adding a 'connection_options' parm with 'ssl_verify_peer: false' in your micro_bosh.yml file.\n. @thansmann I believe this PR has been superseded by #461. @mikepatterson @blueboxjesse Can you please give a try using latest stemcell (1744)? It should work just adding a 'connection_options' parm with 'ssl_verify_peer: false' in your micro_bosh.yml file.\n. @goehmen I guess so.\n. Rackspace Cloud Files is supported at the blobstore_client. If we drop support here, we should also drop it at the blobstore_client and bosh_cli.\n. Merged at https://github.com/cloudfoundry/bosh/commit/ddf110300dcceb7c81b6ab549fbc1a249affc941.\n. @mmb They can be removed. I guess the whole example is outdated now.\n. You'll also need to modify the registry template: https://github.com/cloudfoundry/bosh/blob/master/release/jobs/registry/templates/registry.yml.erb#L36\n. This is related to https://github.com/cloudfoundry/bosh/issues/558.\n. @jerenkrantz AFAIK the Retry-After is not a fixed value, at least in Grizzly and Havana, not sure in IceHouse. I fetched several logs from a Grizzly environment and I got different Retry-After headers:\nroot@inception:~# bosh task 8 --debug | grep \"Retry-After\"\n  response => #<Excon::Response:0x000000041c4270 @data={:body=>\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", :headers=>{\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:35:34 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"5\"}, :status=>413, :remote_ip=>\"[REDACTED]\"}, @body=\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", @headers={\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:35:34 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"5\"}, @status=413, @remote_ip=\"[REDACTED]\"> (Excon::Errors::RequestEntityTooLarge)\n  response => #<Excon::Response:0x00000004d88db8 @data={:body=>\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", :headers=>{\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:35:40 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"6\"}, :status=>413, :remote_ip=>\"[REDACTED]\"}, @body=\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", @headers={\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:35:40 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"6\"}, @status=413, @remote_ip=\"[REDACTED]\"> (Excon::Errors::RequestEntityTooLarge)\n  response => #<Excon::Response:0x000000051a8060 @data={:body=>\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", :headers=>{\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:36:10 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"4\"}, :status=>413, :remote_ip=>\"[REDACTED]\"}, @body=\"{\\\"overLimitFault\\\": {\\\"message\\\": \\\"This request was rate-limited.\\\", \\\"code\\\": 413, \\\"details\\\": \\\"Only 10 POST request(s) can be made to * every minute.\\\"}}\", @headers={\"Server\"=>\"nginx/1.1.19\", \"Date\"=>\"Tue, 13 May 2014 01:36:10 GMT\", \"Content-Type\"=>\"text/html; charset=UTF-8\", \"Content-Length\"=>\"147\", \"Connection\"=>\"keep-alive\", \"Retry-After\"=>\"4\"}, @status=413, @remote_ip=\"[REDACTED]\"> (Excon::Errors::RequestEntityTooLarge)\nUsually what we do in environment with default or small rate limits is to decrease the maximum number of concurrent bosh director threads (to 3?) in order to decrease the number of POST requests.\nAnyway, I agree that we should improve the retry logic.\n. @shinji62 OSS it or doesn't exists! :)\n. @ronakbanka Awesome! Thanks!\n. @cppforlife Is this still on the roadmap? I'm trying to get programmatically the manifest from a failed deployment, but the manifest is always null.. @kkallday no, I'm using the director golang package.. Thanks @kkallday, but I prefer to avoid using workarounds if this is going to be fixed anytime soon. . Any release that uses the encryption_key feature (like the networking release) will hit this problem if using a bosh cli version > 3144. See this post.\n. I was also bitten by this, and the documentation is not clear:\nAvailable instance object methods:\n* address [String, non-empty]: IPv4, IPv6 or DNS record. Closing and reopening to pick up the right membership, as the corporate CLA has been signed by Stark & Wayne.. Reopen the PR.. DNS is disabled for this bosh director.\nBTW, now the bosh errands command is not returning an error, it's returning Deployment has no available errands. We didn't add any new deployment/release, the only change was an unfinished deployment that now has finished so might be related to https://github.com/cloudfoundry/bosh/issues/1048.. Merge or die :(\n. Right, thanks!\n. Readiness matters! Thanks!\n. I like the idea of stfu for redis, but if we want to debug it sometimes, shouldn't it be better if we allow to configure the desired redis logging level (redis.logging.level option in director template and info as default value in specs)?\n. This could provoke an infinite retry loop. We should implement a MAX_CREATE_VM_TRIES, similar to what we do when attaching a disk (https://github.com/cloudfoundry/bosh/blob/master/director/lib/director/instance_updater.rb#L281)\n. Director returns snapshot_id (https://github.com/cloudfoundry/bosh/commit/645c2c70217830684ebef4a44ba7001d8c156b3b#L0R47), but it should be snapshot_cid in both places. I'll modify it.\n. Yup. Thanks!\n. Oh, I thought AWS was migrating to OpenStack! ;) Thanks mate!\n. ",
    "cf-bosh": "We released 1.4.1 which depends explicitly on httparty.\n4cb52e71caa7d939be677cb71d95526648b89fd9\n@mkocher & @pmenglund \n. The root cause of the issue is that the generated bin stub for the stemcell-copy script assumed the script was ruby when in fact it was bash.   We worked around this and it is now be fixed in MASTER.   We still require password-less sudo on the inception box so we can write out the stemcell AMI to the AWS EBS device.   Luckily the amazon ubuntu AMI has this by default.\n- Alex (ajackson) and Sean (seansweda)\n. Hey @mmb, since you're just a few desks over, come find us some time and we can help you with this if you run into it again.\n- @Amit-PivotalLabs \n. [VF + CB] Merged.\n. Redoing this PR\n. Manually merged into master https://github.com/cloudfoundry/bosh/commit/8917eacf51061cab8b2d4af5a380eb1475b470dd\n-rdr/dk\n. we incude retryable in bosh common, so you should be able to remove the require. see https://github.com/cloudfoundry/bosh/commit/3fb27c47feaf2287e4a4f51870bf5d6c02803c7d\n. ",
    "anfernee": "@drnic warden CPI could only be running on single machine. \n. On Fri, Feb 1, 2013 at 6:32 AM, Dr Nic Williams notifications@github.comwrote:\n\nWhich part of the manifest says which machine it is; or is it the local\nmachine?\nIt's not in the manifest. It's running on the same machine with director.\nAll the warden stuff is configured in the config/warden.yml, which is a\ndirector config.\nAre the stemcell info used? (Since its containers not guest VMs)\n\nyeah, and warden has a specific stemcell builder.\n\n\nDr Nic Williams\nhttp://drnicwilliams.com\ncell +1 (415) 860-2185\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/23#issuecomment-12969966.\n\n\nBest Regards,\nYongkun\n. what i did is I deployed a microbosh, then i installed warden and\nwarden-cpi, and I replaced the director config. After starting warden and\nrestarting director, it should work (bosh life-cycle mgmt). I would add an\ninstallation guide later.\nOn Fri, Feb 1, 2013 at 7:16 AM, Dr Nic Williams notifications@github.comwrote:\n\nSo I would deploy a microbosh; then I need to deploy a bosh-release with\njust a director?\nHow do I build the warden-enabled stemcell?\n\nDr Nic Williams\nhttp://drnicwilliams.com\ncell +1 (415) 860-2185\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/23#issuecomment-12971896.\n\n\nBest Regards,\nYongkun\n. fixed per martin and jesse suggests.\n. ping.\n. @frodenas there's a simple fix for it, i sent another pull request. \n. i have to merge the master, because the du cmd is changed to sys::filesystem. \n. @pmenglund  the relative changes live in here: https://github.com/anfernee/bosh/tree/platform. but i want to send pull request after this is merged. \n. @pmenglund the base class is still linux, but actually it is defaultly ubuntu implementation. \n. @pmenglund thanks for the heads-up. I blindly kept everything in master when I was resolving the conflict :P it's cool (one less gem).\n. yeah, sure. work in progress for rehl. Need integration and more testing for rhel. But for ubuntu, I think it's not that risky. \n. the tests under unit/platform/ubuntu mock some of the important cross-platform code. I unmock some of them. \nalso the tests under unit/platform/ubuntu are very ubuntu related, they are common work-flow, I plan to refactor some of them. \n. will add network related tests 2mr. \n. @pmenglund here I test when the device file is /dev/sda, the partition device file /dev/sda1 is also there. \nI thought it is guaranteed on all redhat and ubuntu machine, but seems travis machine has some special setup. \n. @pmenglund the intention is to test those things across systems. I want to be sure that the partition is really there and it's block device. do you have any suggestions?\n. @pmenglund \nyeah. is there anywhere you guys put the platform specific functional test in, or i create another rake task, like spec:functional, spec:platform or something. \n. closed because ankur will integrate bosh agent in redhat. \nankur will cherry pick from these commits and send out pull request from his repo. \n. i really want the tempfile under /tmp, because inside warden, /tmp is there. otherwise it is possible the TMPDIR doesn't exist inside warden. \n. oh, yeah, will do. \n. yeah, it means 9 containers. \n. @d I created 2 tempfiles, one on the host, the other inside warden. they share the same name.  the TMPDIR might not exist inside warden. \nok. so what i will do is i will respect TMPDIR on the host and create a hard-coded /tmp inside warden so that at least on the host it looks good. \n. @pmenglund there are back ticks all over the code base. is it preferred to use Bosh::Common::Exec over back tick for all the shelling out for new bosh code? \n@ankurcha it won't. \n. @pmenglund that makes sense. originally i wanted to make the minimal changes, without doing 2 things the same time ;\uff09\n@ankurcha why? didn't get you.\n. anything changed this line? does it contain trailing spaces?\n. we'd better lock the version of it.\n. we'd better keep the format  as it is. There are other components relying on the this.\n. this is going to encoded into json, and sent out to nats as heart beat, which will be consumed by others like health monitor.\n. oh, didn't see that. though i think it's not related to this particular commit and should be moved to separate one. \n. 2 spaces after minus. \n. it might be good to log something here.\n. could be a warning. but do not swallow exceptions ;)\n. Bosh::Exec is imported above, so only need to call sh. \n. It's necessary, because the ssh service is called ssh on ubuntu, but sshd in centos/redhat, but they are the same service. i don't quite understand the dangerous part. \n. Yeah, I knew that. It was in my todo list. it's not perfect, but it works.\n. i am not sure. that might involve bosh/vsphere behavior. that's why an integration is needed ASAP. \n. it's skipped. pending. if you don't like them, they can be commented out. I just want to gain even a little more confidence about all the methods that are mocked in other platform tests. \n. I cannot find the commit in your repo, but anyway, I know how to do it. \n. ",
    "dhruvkapil": "Hi Martin,\nWhen this fix will be available in bosh-release. How can i include it my\nproduction environment?\nDo i have to take the latest code from\nhttps://github.com/cloudfoundry/bosh/commit/2e5d128766e53284348fa216660717ad6d319bea\n& then make a gem out of it. & then uninstall the previous bosh_aws_cpi gem\n& install the new one on the system.\nIs it recommended for production environment? will it not break the\ndependency of the bosh_aws_cpi gem?\nHow does this flow works? like if somebody wants to include the fix\nimmediately on the production environment, what is the process alike?\nOr do i have to wait till the next bosh-release is available?\nWhat is the frequency of new releases from bosh & cloud foundry.\nRegards,\nDhruv Kapil\nOn 1 February 2013 10:03, Martin Englund notifications@github.com wrote:\n\nFixed in 2e5d128https://github.com/cloudfoundry/bosh/commit/2e5d128766e53284348fa216660717ad6d319bea\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/26#issuecomment-12980402.\n. Hi Martin,\n\nWhen this fix will be available in bosh-release. How can i include it my\nproduction environment?\nDo i have to take the latest code from\nhttps://github.com/cloudfoundry/bosh/commit/2e5d128766e53284348fa216660717ad6d319bea\n& then make a gem out of it. & then uninstall the previous bosh_aws_cpi gem\n& install the new one on the system.\nIs it recommended for production environment? will it not break the\ndependency of the bosh_aws_cpi gem?\nHow does this flow works? like if somebody wants to include the fix\nimmediately on the production environment, what is the process alike?\nOr do i have to wait till the next bosh-release is available?\nWhat is the frequency of new releases from bosh & cloud foundry.\nRegards,\nDhruv Kapil\nOn 1 February 2013 10:03, Martin Englund notifications@github.com wrote:\n\nFixed in 2e5d128https://github.com/cloudfoundry/bosh/commit/2e5d128766e53284348fa216660717ad6d319bea\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/26#issuecomment-12980402.\n. \n",
    "gabis": "In this case where the SC does not exist, the 'force' is only to force the bosh db to match reality.  There are other issues like this (eg bosh reports a persistent disk is not attached, vSphere thinks it is).  Some way to sync up the bosh DB with reality is useful (additional features in bosh cck?)\nMaybe in the case where the stemcell is in use by a deployment, but the image is missing (accidentally deleted, eaten by your IaaS, etc), we need bosh upload stemcell --force, so a new copy of the required version of the stemcell can be uploaded and the database updated to reflect the new CID.\nWe need a solution for a missing stemcell that does not require deleting an active release.\n. We have successfully used 8 in us-east-1.  Did you have other activities going on within the account?  \nI did add a story in our icebox to add throttle functionality to the AWS CPI, but I think reducing the default in bosh-cloudfoundry is also a good idea.  We use 3 in our AWS template.\n. Both of those stories exist.  We have the failed update bug at the top of the backlog so it will get picked up next.  We have CI upgrade stories for each platform for micro and full bosh which are also prioritized in the backlog.  CI improvements are the current focus of the team, so we anticipate coverage for these cases within a few weeks.\n. Sounds OK to me, but I'd like to run it by the backend and services teams before we pull this into the agent in case they need or want to make any job changes before we create a stemcell with this change.\n. We have a story to change bosh ssh to just get in as root.  I believe @mmb is on the side of direct ssh as vcap and then passwordless sudo.  It does seem like a slightly better habit to be in, but it's not (currently) any more secure.  Maybe that will give us more flexibility to incrementally add users/groups/acls without a huge UX change, and we could potentially leverage sudo configuration to control which commands can be run at which access level.    Thoughts?\n. Thanks Nic.  I put a story in our tracker for this.\n. @foexie it looks like in PR 254 the default is set to false or the value of the optional property.   A user would have to explicitly set the value to 'true' if public visibility was wanted.\n-  openstack.stemcell_public_visibility:\n-    description: Set public visibility for stemcells (optional, false by default)\n-    default: false\n. Prioritized for review.\n. We have a story in regarding this test being flaky so the failure may have just been timing related and not caused by your change.  I put a note in the story to let you know when it's fixed.\n. @drnic, I'm using a recent AWS micro (1.5.0.pre.721) and I get n/a under jobs.  Please describe how to verify the API support.\n. Prioritized for review\n. Prioritized for review\n. Prioritized for review\n. Agree with @pmenglund on the naming.  The property that indicates the headcount of any job is 'instances'.    I think implementation of this merits more discussion.  The question of whether we should continue to consider the local manifest as the source of truth v. the director has been coming up more often lately.  \nI am also not convinced that adding CLI manifest editing in bosh is the way to go.   If we start to think of the manifest as it exists now as infrastructure details only and the manifest describing the deployment more like part of the release maybe API calls to the director to make config changes are OK.\nIf we decided that CLI manifest changes are a great idea I think we would want to use one verb for that like 'bosh conf' or 'bosh edit' eg 'bosh conf jobs.dea_next.instances 16' or 'bosh conf properties.dea.max_memory 8192'\n. Selective updating seems like a valuable feature of bosh.  I have reservations about 'always update'.\n. Backend team is on board with updating these properties.  I think that the error potentially provides valuable feedback to a release developer (for all the future non-CF bosh releases! which we love!) and that updating every time without explanation would be a source of confusion.  I would like to give this one a little more time before breaking selective update.\n. @danhigham, I am concerned about the failing tests.  It looks like create_nat_instance is not getting the right args.  I think this a good direction but we will need to get all the changes lined up so we aren't publishing code/stemcells that don't play nicely together.  @drnic had some ideas about publishing a multi-AZ light stemcell, so maybe you guys can put your heads together?\n. :+1: I'd hold off on code until we resolve https://github.com/cloudfoundry/bosh/pull/223 since it may change the requirements slightly\n. @pmenglund can you comment on implementation?   How does it currently work when it fails?\nI can think of several options...\nSend logs back to director as part of debug log, or as a separate compile log stored with the event, debug, cpi logs\nThis has the downside of locking them to the task, but feels easier to implement.\nHave a separate compile logs area on the director where you kept them as long as you had that release (cleaning up as part of bosh cleanup).  Makes it easy to retrieve by version number per @msackman's suggestion, but then we are dispersing more logs in more places.\nPut on blobstore in parallel structure with compiled packages.  This has potential for future users of shared compiled packages (global package cache) with burning curiousity also retrieve these logs.\nWait for per-VM logging and send people to their syslog server which they obviously have and send them there (ok I hate that one)\n. Fully agree, we have a story in the backlog.  Implementation details TBD.\n. Wasn't sure, so I checked.  Yes you can set canaries to 0\n\n- name: dea_next\n  template: dea_next\n  instances: 5\n  resource_pool: dea\n  update:\n    canaries: 0\n    max_in_flight: 5\n\n\nUpdating job dea_next\n  dea_next/3 (00:01:53)\n  dea_next/1 (00:02:02)\n  dea_next/4 (00:02:02)\n  dea_next/0 (00:02:04)\n  dea_next/2 (00:02:08)\nDone                    5/5 00:02:08\n\nI agree that it gets complicated to get into more flexible solutions when considering colocation.    I think if you wanted them in groups you could use different job names with the same template, set the canaries to 0 and then each job would get updated sequentially.  eg 5 instances of job rabbit_cluster_1 with template rabbit, 5 instances of rabbit_cluser_2 with template rabbit, etc\nDoesn't mean your suggestion wouldn't work, but this should get you what you're looking for with features available today.\n. Well, like my example, you apparently get 2 entries in bosh targets, one with the alias and one with the IP.  You can add an alias as an arg eg \"bosh target $myip $myalias\", then you can target it later by alias and forget the IP entirely!\nI still think the correct behavior for this command would be 3 columns.  Name matching the name in bosh status, Alias matching the alias in .bosh_config and Director URL\nSeems like bosh status should also have an Alias field for consistenty\nFinally, since 'name' is set in the manifest, the help for bosh target should be updated to reflect that it is an alias, currently the description reflects this but not the usage line\n\n$ bosh help target\ntarget [] []\n    Choose director to talk to (optionally creating an alias). If no arguments given, show currently targeted director\n\n. Congrats!!!! \nI changed my mind about having the Alias appear in bosh status, b/c it is mixing up local info with info retrieved from the director.  I put in stories for the other 2 UX fixes.\n. I don't feel enthusiastic about this.  Shouldn't the user get a say in whether it's cached?\nWe have a story in to make this settable in .bosh_config, but we haven't settled on UI yet.  I'm not 100% happy with any of the ideas I've had so far.\n- bosh target $ip --gateway_host xx --gateway_user xx --public_key xx\nconveys the idea that you're setting it per target, but seems 'hacky'\n- bosh gateway .... etc\nset the gateway for the current target (or --target), without args show current gateway\nseems weird to add another verb\n- use bosh properties\nin theory this could allow you to set a gateway per deployment (rather than per target), but currently it is not really utilized, not sure we should revive it\n- force hard core users to set it with vi in the .bosh_config\n:-1: obvious flaws\n. I'm not sure this would work.  VMs aren't created one at a time.  Typically it's up to 32 at a time.  \nIt sounds like what you really want is 'if VM creation partially fails, I shouldn't have to run cck before I can deploy again'  \nDoes that sound right?  \nThis seems like an ordering problem to me.  The DB should not reflect that the VMs have assignments before 'binding instance VMs' has occurred.  So if some of your VMs failed to create (network glitch, bad day on AWS, config problem, working ^C, etc), you could just run bosh deploy again, then it would roll right into binding instance VMs when the right number were available.\n. OK, sounds like we're on the same page.  I'll put a story in for this.\n. requires a significant architecture discussion, but agree that this is a desirable use case\n. try http://bosh-jenkins-artifacts.s3.amazonaws.com/bosh-stemcell/aws/latest-light-bosh-stemcell-aws.tgz\n. prioritized in backlog\n. I don't see why not, but what would you use it for?\n. I've seen it but usually with bosh cck you can reboot or recreate.  When iterating on releases and blocked on this I've manually killed the process and/or cleaned up monit artifacts (like the pidfile).\n. Yeah, we didn't have that option in staging so we tried some creative things, including sending crafted nats messages to reapply the spec.  Fun things to try at home :)  You may have had a bad pidfile laying around or something that prevented something from starting?  Leaving this in tracker to see if we can reproduce the problem.\n. moved to backlog.  Changes to bosh diff are low priority b/c it needs to be replaced more than it needs to be fixed.\n. moved to backlog.  Changes to bosh diff are low priority b/c it needs to be replaced more than it needs to be fixed.\n. Please open a new issue if the stemcells @xoebus linked to do not work\n. prioritized in backlog, but maybe @frodenas can take a look at this sooner.\n. Great!  Maybe this will come out soon.  Looks like release about once a month, last one was June 10.\n. added to backlog, thanks!\n. @frodenas looks like fog came out today 1.13.0 July 19, 2013 (1.13 MB)\n. Not opposed to this but not a current priority.  Added to bottom of backlog.\n. bosh diff should be the one to stay\n. review prioritized in backlog\n. @drnic can you suggest a test case to validate this is working?\n. Is this new in .799?\n. prioritized in backlog\n. prioritized in backlog\n. prioritized for review\n. ",
    "ajackson": "Are you still having this issue?   Another issue you opened seemed to suggest you got past this.   How did you resolve it?   I have run into similar issues when using an older version of ruby.   Ideally you would be using ruby 1.9.3p327 or above.  \n. Which version of ruby are you using. Also do you happend to know if it was compiled with libxml support or not?\n. This was a bug in the dependency resolution phase of the build scripts.   It's has been fixed.   Please reopen if you see this again.\n. Which version of ruby are you using.   Also do you happend to know if it was compiled with libxml support or not?\n. The build script not takes into account the Gemfile.lock.   Please reopen if you still have this issue.\n. Glenn, can you please address the test failures before we can accept this.\n. Let's use https for this given the s3 url.\n. Do we need a migration for existing blobs in the production blobstore?   Also if we do, how do we maintain compatibility for old atmos clients?\n. @cppforlife any update on locking down rpcinfo on the stemcell?   The tracker story seems to have been deleted / moved.   . ",
    "d": "anfernee, your patch LGTM in general but I have mixed feelings about the df patch. On one hand, it's an enhancement, OTOH, I'd say BOSH was doing a terrible thing shelling out to \"df\" instead of using the standard statvfs system call\n. anfernee, your patch LGTM in general but I have mixed feelings about the df patch. On one hand, it's an enhancement, OTOH, I'd say BOSH was doing a terrible thing shelling out to \"df\" instead of using the standard statvfs system call\n. @anfernee To do a slightly less terrible thing in using df , try the --portable flag to get a guarantee that each mount is one its own line (no overflowing for 20 columns or stuff)\nin an ideal world though, statvfs (which might mean doing FFI) should be preferred (tangential to this pull request)\n. @anfernee To do a slightly less terrible thing in using df , try the --portable flag to get a guarantee that each mount is one its own line (no overflowing for 20 columns or stuff)\nin an ideal world though, statvfs (which might mean doing FFI) should be preferred (tangential to this pull request)\n. -1.\nInstead of going down the submodule rabbit hole, I think we should add negative glob to the package spec\n. -1.\nInstead of going down the submodule rabbit hole, I think we should add negative glob to the package spec\n. @blueboxjesse yes this unit-level coverage and code LGTM. I am setting up an env w/ self-signed SSL cert for running acceptance test now.\n. @blueboxjesse yes this unit-level coverage and code LGTM. I am setting up an env w/ self-signed SSL cert for running acceptance test now.\n. dealing with a massive (but seemingly manageable) merge conflict\n. dealing with a massive (but seemingly manageable) merge conflict\n. a local merge looks fine. running unit tests while setting up openstack\n. a local merge looks fine. running unit tests while setting up openstack\n. - weeded out some whitespace\n- took out a tab in a yaml\n- added back a necessary colon\n. - weeded out some whitespace\n- took out a tab in a yaml\n- added back a necessary colon\n. looks like we'll need similar changes to the micro deployer to support this. i'll take a closer look after lunch\n. looks like we'll need similar changes to the micro deployer to support this. i'll take a closer look after lunch\n. Hi @drnic \n  Does bosh task last --debug not work for you? I understand your desire to have a more descriptive error on the CLI, but on the other hand, I seem to have developed the habit of associating HTTP 413 Entity Too Large with \"OpenStack rate limit exceeded\". Will that mental model help a bit?\n@d and @mark-rushakoff \n. Hi @drnic \n  Does bosh task last --debug not work for you? I understand your desire to have a more descriptive error on the CLI, but on the other hand, I seem to have developed the habit of associating HTTP 413 Entity Too Large with \"OpenStack rate limit exceeded\". Will that mental model help a bit?\n@d and @mark-rushakoff \n. Hi @kkbankol,\n  A couple quick questions:\n- How can we reproduce this issue? We are building Ubuntu stemcells for OpenStack and we haven't run into this issue at all. I recall Ubuntu is smart enough to mount the root disk its initial ramdisk runs on.\n- It looks like there wasn't unit test coverage for this. Can you add a corresponding test in bosh-stemcell/spec/stemcells/ubuntu_spec.rb\n- Can you share with us your result of running BAT (BOSH Acceptance Tests) against the stemcell built with this patch?\n@d and @sbrady \nTeam Cloud Foundry\n. Hi @kkbankol,\n  A couple quick questions:\n- How can we reproduce this issue? We are building Ubuntu stemcells for OpenStack and we haven't run into this issue at all. I recall Ubuntu is smart enough to mount the root disk its initial ramdisk runs on.\n- It looks like there wasn't unit test coverage for this. Can you add a corresponding test in bosh-stemcell/spec/stemcells/ubuntu_spec.rb\n- Can you share with us your result of running BAT (BOSH Acceptance Tests) against the stemcell built with this patch?\n@d and @sbrady \nTeam Cloud Foundry\n. Hi Kalonji,\n  Hooray for better coverage! \n- We are not seeing this issue with Ubuntu on OpenStack. Can you share with us your use case? How are you building your stemcell?\n- Can you let us know the results of your BATs runs?\n@d and @sbrady\n Team Cloud Foundry\n. Hi Kalonji,\n  Hooray for better coverage! \n- We are not seeing this issue with Ubuntu on OpenStack. Can you share with us your use case? How are you building your stemcell?\n- Can you let us know the results of your BATs runs?\n@d and @sbrady\n Team Cloud Foundry\n. Hi Kalonji,\n  Can you help us to get a minimal series of steps to reproduce this problem?\n  My understanding is using the stemcell builder from the BOSH repo does not cause such a problem. To be sure, I'm not saying there's anything wrong with your patch, but we need a broken test to begin developing a fix.\n@d and @sbrady \nTeam Cloud Foundry\n. Hi Kalonji,\n  Can you help us to get a minimal series of steps to reproduce this problem?\n  My understanding is using the stemcell builder from the BOSH repo does not cause such a problem. To be sure, I'm not saying there's anything wrong with your patch, but we need a broken test to begin developing a fix.\n@d and @sbrady \nTeam Cloud Foundry\n. Hi Kalonji,\n  If we take the stock stemcell stages from BOSH repo and build a stemcell, deploy it to an OpenStack Grizzly environment, we should see the same problem. Did I understand you correctly?\n@d and @sbrady \nTeam Cloud Foundry\n. Hi Kalonji,\n  If we take the stock stemcell stages from BOSH repo and build a stemcell, deploy it to an OpenStack Grizzly environment, we should see the same problem. Did I understand you correctly?\n@d and @sbrady \nTeam Cloud Foundry\n. Hi Kalonji,\n  @sbrady and I got hold of a Grizzly installation and we ran BAT with a stock Ubuntu stemcell. We did not experience the failure you saw. The micro deployer proceeded with creating the VM and starting up a MicroBOSH without a problem.\nJesse and Sean,\nTeam Cloud Foundry\n. Hi Kalonji,\n  @sbrady and I got hold of a Grizzly installation and we ran BAT with a stock Ubuntu stemcell. We did not experience the failure you saw. The micro deployer proceeded with creating the VM and starting up a MicroBOSH without a problem.\nJesse and Sean,\nTeam Cloud Foundry\n. This is a feature, not a bug. Allowing this will cause a circular dependency during the deployment process, and the director will not be able to figure out a proper order.\nJesse\n. This is a feature, not a bug. Allowing this will cause a circular dependency during the deployment process, and the director will not be able to figure out a proper order.\nJesse\n. @mkocher a simplified first deployment:\n- haproxy/0: ip1\n- cc/0: ip2\nand you are asking for this in the second deployment:\n- haproxy/0: ip2\n- cc/0: ip3\nLet's find the proper order here together: to make ip2 available it seems cc/0 needs to be updated before haproxy/0, so let's order them that way. Now say your deploy aborted after updating haproxy/0, how do we expect the operator to recover? The classic operator experience would have been the operator simply deploying an older manifest. But in this case, in the recovery scenario, the dependency is reversed, in that haproxy/0 needs to be rolled back before cc/0 because of the ip address. To take the idea further, what if ip3 == ip1? What should the order be? Nothing is impossible if we can make stronger assumptions, say if we can assume people don't care about down time, we can certainly stop everything and change every job in a big bang.\nJesse\n. @rd7869 As the team is working on this as a long term goal, can you close this issue?\nJesse\n. This PR is merged into develop. We are looking into CI failures to determine whether it is related to the merge. Leaving the PR open for now.\nJesse and Yulia ( @d / @ytolsk )\nTeam Cloud Foundry\n. Hi @gberche-orange thanks for posting the bug. We are aware of the issue but please bear with us-we are waiting on @nahi to bump httpclient on rubygems.\nJesse and @jfoley,\nCloud Foundry Team\n. Closing, please ignore.\n. This is working as intended: the BOSH CLI is not designed to run on Ruby 2.1 (yet). We do have a little future-proofing in our CI that runs the same builds under Ruby 2.0 but they are currently all broken.\nFWIW I am running the BOSH CLI in Ruby 2.0 (this is unsupported behavior) and my production environment is not blown up (yet).\nBefore we follow Vagrant's precedence to ship our own Ruby, is this still an issue for you, @matjohn2 ? Can we close this?\nJesse\n. @drnic I can feel your pain in switching Rubies. But IMHO this highlights the mistake of delivering our CLI as a Ruby gem. Look at how many random issues people had before Vagrant bundled their own Ruby, sigh.\nJesse\n. Hi @byllc can you help us better reproduce the problem by providing us:\n- the release\n- your (sanitized) manifest\n- and the (sanitized) cli output during the failed deploy?\nJesse\n. @drnic I'm not sure we can understand your question. Is this related to the issue @byllc has? Do you know how @byllc got into that state? How did he know the 2nd and 3rd job \"never started\"?\nJesse\n. - bosh_cli is aiming for 2.0 and 2.1 compatibility but they are not supported\n- this is likely a bosh_deployer issue, not bosh_cli because bosh_deployer depends on a higher version of aws-sdk\n. Have you tried uninstalling bosh_deployer and aws-sdk, then re-installing bosh_cli?\n. Hi Ren\u00e9e, requesting a pull that contains merges is generally not considered a good practice. Is it possible you rebase / squash your changes so the history will only brach off once?\nJesse\n. Hi @heei3k we'd love to help you start playing with Cloud Foundry on vSphere but please provide us more information. Specifically, can you paste the complete stack trace from bosh_cli here so that we can better pinpoint what the problem is?\nJesse\n. Hi @heei3k,\n  I have verified that the file you mentioned is not corrupt from our website. Can you provide the md5 and sha1 of the file ~/cf-release/81d38be6003360bdbfe5d5aaefc4716a95ac4dce ?\nJesse\n. Hi @heei3k,\n  Obviously the manifest you used does not match cf-release. this is not a BOSH bug.\nJesse\n. @heei3k to work around this issue, can you try downloading the packages first by doing a bosh create release releases/cf-159.yml (ensuring your pwd is cf-release)?\nJesse\nCloud Foundry Team\n. Each VPC belongs to a region, and under that each subnet belongs to a zone and can only be used by EC2 instances in that zone. It looks like you have declared to put your compilation VMs in one AZ us-east-1a into a subnet that in another AZ us-east-1d.\nCan you post your deployment manifest here?\n. @duglin Hi Doug, if you're creating dev releases, you do have a chance to name them (to differ from the final release names). The first time you create a dev release, the CLI will prompt you for a dev release name, FYI.\nJesse\n. @drnic Nic it'll work but not when you're in a container.\n. Hi Ryan,\nIt may help the BOSH team if you share with them which process you and David found were listening on the port.\nJesse\n. Hi Sven,\nI think you're already able to do this, by bosh export release:\n1. Let's assume you have a staging environment, because hey, you're serious about your production\n2. Your staging deployment uses the exact same stemcell and release versions as your production.\n3. Before the \"next big\" deploy to production, upgrade your staging environment\n4. Of course you'll run tests\n5. bosh export release\n6. Now take that output and upload it to your production BOSH, things will go way faster :)\nc.f. https://bosh.io/docs/compiled-releases.html\nI hope this helps.\n. Anfernee I must be missing something because I share Martin's concerns. Why do we need to hardcode where temp files live? (I understand that we don't care about portability because cgroup is Linux-only, but still...)\n. whitespace\n. what's the point of using fetch if we specify a default of nil?\n. Hi David, any reason why this spec needs to change here? This pull request doesn't seem to be doing anything related to RDS here.\nJesse\nTeam Cloud Foundry\n. Hi David,\n  I should have read your earlier comment. I took a quick glance, it seems the aws-sdk is not doing method_messing or anything wicked there, just normal meta programming. So the RDS Client object should have that method defined. My gut reaction is we should be cautious about turning some of the double's into instance_double's because we lose some of the typing protection there. We also won't likely have integration test for this feature, which increases the area with little coverage hidden under a conditional.\nJesse\n. ",
    "nand2": "I am having the same problem, a 38mn timeout, and then a 7mn timeout, with the same error and logs. With t1.micro instances. Now trying bigger instances to see if it makes any difference..\n. Yes, made a difference with a m1.medium, 3min waiting for the agent and then it proceed to the next steps.\n. ",
    "Amit-PivotalLabs": "Hey @drnic, are you still having this issue?\n. The next bosh release will include a package cache feature, which will be helpful for regular deployments, allowing the package compilation step to be skipped.  This won't help if you're building your own micro stemcells though, at least not yet, but that could be really desirable.  How often do people build their own micro stemcells vs using public ones?\n. Hey @mheath, if you compare cf-release/jobs/cloud_controller_ng/spec to cf-release/jobs/nats/spec you'll see the difference between \"new style\" (ccng) and \"old style\" (nats).  Bascially the new style requires you to specify all the types of properties (and optional defaults) that the job templates will need, and makes them easier to access from the templates using the helper method p.  I'll ask around if there's any good documentation on using the new style, in the meantime you can check out this thread from the bosh-users group:\nhttps://groups.google.com/a/cloudfoundry.org/forum/?fromgroups=#!searchin/bosh-users/bosh$20properties$20revamp/bosh-users/FNnxwwwm2HY/U6Y3ZQu8RzkJ\nThis could be caused by trying to colocate two jobs where one has new style and the other has old style, but that's just a guess.  Spinning up a highly collocated CF deployment is the next thing we're working on, so if it's a real bug we'll definitely have to fix it.\n- @Amit-PivotalLabs and @seansweda \n. This PR covers multiple stories in OSS Release Integration team's backlog:\nhttps://www.pivotaltracker.com/n/projects/1382120\nSome of the stories have been rejected, and others are still in flight, so this PR is not ready to merge.  I will close this PR, and we will resubmit after stories have been accepted on the fork.\n. @smokingfly \nIf you run \nruby -ryaml -e\"p YAML.load_file('PATH/TO/YOUR/MANIFEST')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']['ssl_pem'].chomp\"\nWhat do you see?  Using the example manifest where you replaced the values, I get:\n$ ruby -ryaml -e\"p YAML.load_file('cf-release/example_manifests/minimal-aws.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']['ssl_pem'].chomp\"\n\"REPLACE_WITH_SSL_CERT_AND_KEY\"\n. Ah yeah, should have expected that.  Which of the following lines fails?\n```\nruby -ryaml -e\"p YAML.load_file('/home/cfuser/cf-deployment/cloudfoundry.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']['ssl_pem']\"\nruby -ryaml -e\"p YAML.load_file('/home/cfuser/cf-deployment/cloudfoundry.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']\"\nruby -ryaml -e\"p YAML.load_file('/home/cfuser/cf-deployment/cloudfoundry.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']\"\n```\nWhat I suspect is you just have some misalignment or something in your YAML causing some value to not be present in the expected place in the YAML structure.\n. Since this is a different problem than the original, I will close this issue.\nSince you've already asked this question on the mailing list, I will ask the core team to track the mailing list question directly.\nYou can see it here in our Tracker: https://www.pivotaltracker.com/story/show/109660170\n. Hi @CAFxX \nI don't think we're too eager to expand the p(...) interface for this use case, as @cppforlife mentioned.  As there hasn't been activity on this issue for some time, we will close this out for now.  Please feel free to re-open if you'd still like to discuss this further.\nCheers,\nAmit\n/cc @mfine30 @dpb587 @jfmyers9 . Hi all,\nTo answer the original question about extending the timeout, it is not configurable, and this is intentional since the problem is likely symptomatic of other root cause problems that should be addressed.  There have been improvements made to how the Agent interacts with monit since the issue was opened which might address those root causes.  If this issue is still a challenge, please give the latest stemcells a shot (or a stemcell built with the latest Agent code if you're using a stemcell not maintained by the core BOSH maintainers).  We'll close this issue out due to inactivity, please feel free to re-open if the issue persists with the latest Agent code.\nThanks,\nAmit\n/cc @jfmyers9 @mfine30 @dpb587 . Hey @djvdorp \nNot sure if you saw my response to your Slack question: https://cloudfoundry.slack.com/archives/general/p1465834173000016\nThe docs are in an intermediate state essentially waiting on new tooling, and the new tooling is in progress, but we can guide you in using the new tooling if you're willing to try it out.  It intentionally doesn't do everything the old \"bosh aws bootstrap\" tools did, for instance it doesn't create RDS instances (and assumes you'll deploy CF with the built-in postgres job, or manage RDS or some other external database yourself).  If you're interested in this guidance, let me know.\n. We'd like to remove the \"bosh aws\" from the documentation as soon as possible, so if you're willing to try option 3 the feedback would be very valuable for us, and should get us to a place where we can replace those docs with something that works sooner.\nOption 3 is currently experimental and lives in the pivotal-cf-experimental org, but we'll be moving it over to the cloudfoundry-incubator org soon: https://github.com/pivotal-cf-experimental/bosh-bootloader.  If you open an issue there we can describe a workflow that should get you started with CF (later on, a polished set of docs will describe the workflow), though we're waiting on one feature from BOSH that's just around the corner, to support \"BOSH 1.0\" style manifests, since the existing tooling for generating CF manifests only supports the 1.0 style.\n. We'd like to remove the \"bosh aws\" from the documentation as soon as possible, so if you're willing to try option 3 the feedback would be very valuable for us, and should get us to a place where we can replace those docs with something that works sooner.\nOption 3 is currently experimental and lives in the pivotal-cf-experimental org, but we'll be moving it over to the cloudfoundry-incubator org soon: https://github.com/pivotal-cf-experimental/bosh-bootloader.  If you open an issue there we can describe a workflow that should get you started with CF (later on, a polished set of docs will describe the workflow), though we're waiting on one feature from BOSH that's just around the corner, to support \"BOSH 1.0\" style manifests, since the existing tooling for generating CF manifests only supports the 1.0 style.\n. @tylerschultz wrote:\n\nI believe this can be explained by a bug that has been present for a long time.  When director tells monit to stop, it may take a while for monit to get around to actually running the ctl stop command. Monit may be busy doing other things, stopping other jobs, daydreaming... whatever it does. Unfortunately, the agent believes stopping is complete because it has issued the monit stop foo command. The agent reports back to director that the job is stopped. Director moves on and then issues an apply call. This call will result in the new job templates replacing the old templates (the templates that would stop dnsmasq are now replaced.) Monit now finnaly gets around to issuing the ctl stop command and the stop runs against the replaced templates!!!!  \nAlas, there is a fix:\nNow I can't find the job property on a quick search, hopefully it has been released. Enabling the property will cause agent and director to wait for the stop command to complete, within a timeout, before moving on to the apply step. This seems like something that should be turned on by default, but there is concern that there are many jobs that don't stop well, and it'd be a breaking change for many releases.\nLooking at tracker, it appears the fix may not yet be released. Thinking about it more, it also occurs to me that it's odd that all of the instances suffered from this problem. This leads me to believe something else may be going on.\n\n[source]\nThanks.  Yeah, my suspicions are the same.  We know about the race that can happen around stop, but this happened on many VMs in many different environments.  So it's kinda mysterious.  Let me know if you have any further thoughts on that first question.\nAs for the second question, does that sound like a reasonable approach?  Note that I believe I had to do the deploy with the --recreate since a simple bosh deploy left the VMs in a stopped state, and bosh start didn't start the VMs back up in the correct order (according to the manifest).\n. @tylerschultz wrote:\n\nI believe this can be explained by a bug that has been present for a long time.  When director tells monit to stop, it may take a while for monit to get around to actually running the ctl stop command. Monit may be busy doing other things, stopping other jobs, daydreaming... whatever it does. Unfortunately, the agent believes stopping is complete because it has issued the monit stop foo command. The agent reports back to director that the job is stopped. Director moves on and then issues an apply call. This call will result in the new job templates replacing the old templates (the templates that would stop dnsmasq are now replaced.) Monit now finnaly gets around to issuing the ctl stop command and the stop runs against the replaced templates!!!!  \nAlas, there is a fix:\nNow I can't find the job property on a quick search, hopefully it has been released. Enabling the property will cause agent and director to wait for the stop command to complete, within a timeout, before moving on to the apply step. This seems like something that should be turned on by default, but there is concern that there are many jobs that don't stop well, and it'd be a breaking change for many releases.\nLooking at tracker, it appears the fix may not yet be released. Thinking about it more, it also occurs to me that it's odd that all of the instances suffered from this problem. This leads me to believe something else may be going on.\n\n[source]\nThanks.  Yeah, my suspicions are the same.  We know about the race that can happen around stop, but this happened on many VMs in many different environments.  So it's kinda mysterious.  Let me know if you have any further thoughts on that first question.\nAs for the second question, does that sound like a reasonable approach?  Note that I believe I had to do the deploy with the --recreate since a simple bosh deploy left the VMs in a stopped state, and bosh start didn't start the VMs back up in the correct order (according to the manifest).\n. I'll close this old issue of mine.\n/cc @mfine30 @dpb587 @jfmyers9 . I'll close this old issue of mine.\n/cc @mfine30 @dpb587 @jfmyers9 . Given what you said, seems like the above code would be equivalent to\n_, network = spec.networks.marshal_dump.first\nnetwork.ip\nIn fact, is spec.networks an array of things which each have a .ip method which is guaranteed to return an IP?  If so, can the above be replaced with:\nspec.networks.first.ip\n\nWe'll look into the addressable stuff when it comes out.\n\nSomething awkward must be going on with the original code.  I suspect some jobs would behave oddly if their discovered external IP changed between updates.  So is the above perhaps relying on some implicit determinism in the order of spec.networks.marshal_dump?  I.e. perhaps Ruby enumerates the hash deterministically, and we're just getting lucky...\n. Given what you said, seems like the above code would be equivalent to\n_, network = spec.networks.marshal_dump.first\nnetwork.ip\nIn fact, is spec.networks an array of things which each have a .ip method which is guaranteed to return an IP?  If so, can the above be replaced with:\nspec.networks.first.ip\n\nWe'll look into the addressable stuff when it comes out.\n\nSomething awkward must be going on with the original code.  I suspect some jobs would behave oddly if their discovered external IP changed between updates.  So is the above perhaps relying on some implicit determinism in the order of spec.networks.marshal_dump?  I.e. perhaps Ruby enumerates the hash deterministically, and we're just getting lucky...\n. @tylerschultz any thoughts on those questions above?  No rush, but it would be good to know because this could lead to cleanup in a ton of releases out there, especially applicable to consul-release and etcd-release. The complexity of the current logic has made debugging some recent issues hard. /cc @christianang\n. @tylerschultz any thoughts on those questions above?  No rush, but it would be good to know because this could lead to cleanup in a ton of releases out there, especially applicable to consul-release and etcd-release. The complexity of the current logic has made debugging some recent issues hard. /cc @christianang\n. @claudio-benfatto I'm not sure this would be possible.  In the web_server_process job in your example, you'd need to have its job spec in the release specify which links it consumes, and this spec file is a static YAML file, it can't be dynamically changed at runtime to specify that it consumes a different set of links.  Your ERB templates can only reference links that your job spec explicitly states that it consumes.\nCan you describe what your exact use case is?  Why exactly would a single (micro)service need to talk to a dynamic set of databases?. @claudio-benfatto I'm not sure this would be possible.  In the web_server_process job in your example, you'd need to have its job spec in the release specify which links it consumes, and this spec file is a static YAML file, it can't be dynamically changed at runtime to specify that it consumes a different set of links.  Your ERB templates can only reference links that your job spec explicitly states that it consumes.\nCan you describe what your exact use case is?  Why exactly would a single (micro)service need to talk to a dynamic set of databases?. @claudio-benfatto Is it a hard requirement to have separate clusters per AZ, versus having a single cluster that spans AZs?  If so, I don't think what you're looking for can be implemented via links (at least not now or in the near future).\nIn some sense you're looking for a service registration solution, and though you may not need something so dynamic nor want to take on this operational complexity, consul-template is something Cloud Foundry uses for diego-ssh-proxys to dynamically register with a load balancing HAProxy, where the consul-template process colocated with haproxy watches Consul for newly registered SSH proxies and rewrites the HAProxy config file and restarts/reloads it (see here and here).. @claudio-benfatto Is it a hard requirement to have separate clusters per AZ, versus having a single cluster that spans AZs?  If so, I don't think what you're looking for can be implemented via links (at least not now or in the near future).\nIn some sense you're looking for a service registration solution, and though you may not need something so dynamic nor want to take on this operational complexity, consul-template is something Cloud Foundry uses for diego-ssh-proxys to dynamically register with a load balancing HAProxy, where the consul-template process colocated with haproxy watches Consul for newly registered SSH proxies and rewrites the HAProxy config file and restarts/reloads it (see here and here).. @valeriap For the future we're thinking about adding \"pluggable types\" to CredHub so that a manifest can specify that it needs a variable of type: postgres or type: uaa_client.  BOSH would ask CredHub to generate and store the credentials, passing through the type and additional parameters, and then the CredHub plugin would use those parameters to create a user/password/database in Postgres, or a client/secret in UAA, respectively.\nThis way:\n\nNone of the credentials need to be in any manifest\nThe manifest for Postgres doesn't need to know what consumers it will have, and don't need to be updated when there's a new consumer\nThe manifest for the consuming processes just needs to declare what it wants, and it's all handled by BOSH/CredHub/plugin with no operator overhead\n\nThere's a lot to be figured out for that proposal to become a reality, but it's on the roadmap.. @valeriap For the future we're thinking about adding \"pluggable types\" to CredHub so that a manifest can specify that it needs a variable of type: postgres or type: uaa_client.  BOSH would ask CredHub to generate and store the credentials, passing through the type and additional parameters, and then the CredHub plugin would use those parameters to create a user/password/database in Postgres, or a client/secret in UAA, respectively.\nThis way:\n\nNone of the credentials need to be in any manifest\nThe manifest for Postgres doesn't need to know what consumers it will have, and don't need to be updated when there's a new consumer\nThe manifest for the consuming processes just needs to declare what it wants, and it's all handled by BOSH/CredHub/plugin with no operator overhead\n\nThere's a lot to be figured out for that proposal to become a reality, but it's on the roadmap.. @voelzmo @valeriap I've written some ideas down here, let me know your thoughts: https://docs.google.com/document/d/1dt9P3jZCpU3B4owTPKEYydurr2aRuJYK4VZtxv8chUo/edit#. @voelzmo @valeriap I've written some ideas down here, let me know your thoughts: https://docs.google.com/document/d/1dt9P3jZCpU3B4owTPKEYydurr2aRuJYK4VZtxv8chUo/edit#. Done.\nOn Tue, Apr 25, 2017 at 5:15 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nOk to make the link public?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1649#issuecomment-297201444,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC9t3nrv79-xItU7FhekOsz9ISC3D7Rqks5rzoyVgaJpZM4M2FkW\n.\n. Done.\n\nOn Tue, Apr 25, 2017 at 5:15 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nOk to make the link public?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1649#issuecomment-297201444,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AC9t3nrv79-xItU7FhekOsz9ISC3D7Rqks5rzoyVgaJpZM4M2FkW\n.\n. bosh ignore is often used to manually bootstrapped a failing clustered data service like etcd or MySQL, so it's important for disaster scenarios.  Crashing when ignoring the bootstrap instance can be pretty painful thing to hit when trying to do disaster recovery.\n\n/cc @APShirley @ctaymor @ndhanushkodi\n@luan @mfine30 @jfmyers9 @dpb587 . bosh ignore is often used to manually bootstrapped a failing clustered data service like etcd or MySQL, so it's important for disaster scenarios.  Crashing when ignoring the bootstrap instance can be pretty painful thing to hit when trying to do disaster recovery.\n/cc @APShirley @ctaymor @ndhanushkodi\n@luan @mfine30 @jfmyers9 @dpb587 . @dpb587-pivotal I don't have my environment anymore, but I just followed the instructions linked in my original post.  Unfortunately I don't recall what version of bbl I used, and what Azure region I deployed to.. @dpb587-pivotal I don't have my environment anymore, but I just followed the instructions linked in my original post.  Unfortunately I don't recall what version of bbl I used, and what Azure region I deployed to.. ",
    "tpradeep": "I had figured that in one of ur earlier responses to another issue of the\nsame kind. I want to know whether v1.0 & v1 is considered the same or not ?\n- Pradeep\nOn Tuesday, February 12, 2013, Ferran Rodenas wrote:\n\nYour glance endpoint doesn't return the version, hence the \"300 Multiple\nChoices\" error. You should change your OpenStack glance endpoint URL and\nadd a version:\nkeystone endpoint-create --region  --service_id  \\\n--publicurl \"http://192.168.1.2:9292/v1\" \\\n--adminurl -\"http://192.168.1.2:9292/v1\" \\\n--internalurl \"http://192.168.1.2:9292/v1\"\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/42#issuecomment-13420728.\n\n\nSent from iPhone\n. I had figured that in one of ur earlier responses to another issue of the\nsame kind. I want to know whether v1.0 & v1 is considered the same or not ?\n- Pradeep\nOn Tuesday, February 12, 2013, Ferran Rodenas wrote:\n\nYour glance endpoint doesn't return the version, hence the \"300 Multiple\nChoices\" error. You should change your OpenStack glance endpoint URL and\nadd a version:\nkeystone endpoint-create --region  --service_id  \\\n--publicurl \"http://192.168.1.2:9292/v1\" \\\n--adminurl -\"http://192.168.1.2:9292/v1\" \\\n--internalurl \"http://192.168.1.2:9292/v1\"\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/42#issuecomment-13420728.\n\n\nSent from iPhone\n. That worked. Thanks Ferdy.\n@Dr. Nic - that sounds good.\n- Pradeep\nOn Tuesday, February 12, 2013, Dr Nic Williams wrote:\n\nFerdy, is there a nice test we could do upon an OpenStack API (in\nbosh-bootstrap) to check that they've setup OpenStack correctly? I think\nthis is the 2nd or 3rd of these errors?\nNic\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/42#issuecomment-13438966.\n\n\nSent from iPhone\n. That worked. Thanks Ferdy.\n@Dr. Nic - that sounds good.\n- Pradeep\nOn Tuesday, February 12, 2013, Dr Nic Williams wrote:\n\nFerdy, is there a nice test we could do upon an OpenStack API (in\nbosh-bootstrap) to check that they've setup OpenStack correctly? I think\nthis is the 2nd or 3rd of these errors?\nNic\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/42#issuecomment-13438966.\n\n\nSent from iPhone\n. As extreme step..I could delete unresponsive agents which were not getting removed through cloudcheck by deleting deployment using \"--force\"\n. ",
    "rkoster": "Have uninstalled the 1.9.3 ruby from unboxedconsulting and installed ruby 1.9.3p327 from source. This however does not solve the above problem.\n. After adding some logging to the gem_with_deps rake task I got the following:\n\"bosh_aws_registry deps: #<Bundler::SpecSet:0x000000030ffd68 \n@specs=[aws-sdk (1.6.9), bosh_aws_registry (0.2.3), daemons (1.1.9), eventmachine (1.0.0), \nhttparty (0.10.2), json (1.7.7), multi_json (1.5.1), multi_xml (0.5.3), nokogiri (1.5.6), \nrack (1.5.2), rack-protection (1.3.2), sequel (3.44.0), sinatra (1.3.4), thin (1.5.0), \ntilt (1.3.3), uuidtools (2.1.3), yajl-ruby (1.1.0)]>\"\ncp /tmp/all_the_gems/multi_json-*.gem .\ncp /tmp/all_the_gems/pg*.gem .\ncp /tmp/all_the_gems/multi_xml-*.gem .\ncp: cannot stat `/tmp/all_the_gems/multi_xml-*.gem': No such file or directory\nrake aborted!\nThe problem is that sinatra (1.3.4) is listed as a dependency while 1.2.8 is specified in bosh_aws_registry/bosh_aws_registry.gemspec\nWhen running a gem dependency bosh_aws_registry I get:\nGem bosh_aws_registry-0.2.3\n  aws-sdk (~> 1.6.9)\n  sequel (>= 0)\n  sinatra (>= 0)\n  thin (>= 0)\n  yajl-ruby (>= 0)\nSo It looks like the context of the Gemfile is not set here.\n. bundle install --deployment solves the problem\n. The already installed gems are in the system directories. By using the --deployment flag bundler will ignore system gems and install all gems in vendor/bundle. \n\nThe --deployment flag activates a number of deployment-friendly conventions:\n- Isolate all gems into vendor/bundle\n- Require an up-to-date Gemfile.lock\n- If bundle package was run, do not fetch gems from rubygems.org. Instead, only use gems in the checked in vendor/cache\n\nsource\n. I forgot the manifest file:\n```\nname: wordpress\ndirector_uuid: e483842b-87f0-4d50-9fa0-04f885c882a3\nrelease:\n  name: wordpress\n  version: latest\ncompilation:\n  workers: 3\n  network: default\n  reuse_compilation_vms: true\n  cloud_properties:\n    instance_type: m1.medium\nupdate:\n  canaries: 1\n  canary_watch_time: 3000-120000\n  update_watch_time: 3000-120000\n  max_in_flight: 4\n  max_errors: 1\nnetworks:\n  - name: elastic\n    type: vip\n    cloud_properties: {}\n  - name: default\n    type: dynamic\n    cloud_properties:\n      security_groups: \n      - microbosh-openstack\n      - wp-bosh\nresource_pools:\n  - name: common\n    network: default\n    size: 4\n    stemcell:\n      name: bosh-stemcell\n      version: 1.5.0.pre # CHANGE: Stemcell version\n    cloud_properties:\n      instance_type: m1.medium\n      availability_zone:\n      key_name:\njobs:\n  - name: mysql\n    template: mysql\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\n  - name: nfs\n    template: debian_nfs_server\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\n  - name: wordpress\n    template: wordpress\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\n      - name: elastic\n        static_ips:\n        - \n  - name: nginx\n    template: nginx\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\nproperties:\n  mysql:\n    address: 0.mysql.default.wordpress.microbosh\n    port: 3306\n    password: rootpass\n  nfs_server:\n    address: 0.nfs.default.wordpress.microbosh\n    network: 0.wordpress.default.wordpress.microbosh/255.255.255.255 # CHANGE: Floating IP 3\n  debian_nfs_server:\n    no_root_squash: true\n  wordpress:\n    admin: foo@bar.com\n    port: 8008\n    servers:\n      - 0.wordpress.default.wordpress.microbosh\n    servername: \n    db:\n      name: wp\n      user: wordpress\n      pass: w0rdpr3ss\n    auth_key: random key\n    secure_auth_key: random key\n    logged_in_key: random key\n    nonce_key: random key\n    auth_salt: random key\n    secure_auth_salt: random key\n    logged_in_salt: random key\n    nonce_salt: random key\n  nginx:\n    workers: 1\ncloud:\n plugin: openstack\n properties:\n    openstack:\n     default_key_name: admin-keypair\n     default_security_groups: [\"default\"]\n     private_key: /root/.ssh/admin-keypair.pem\n```\n. Thanks @frodenas for the quick response. Below you will find the requested information.\ncat /var/vcap/jobs/director/config/director.yml | grep dns:  -C 1\ndns:\n  server: 127.0.0.1\n```\ncat /etc/dhcp3/dhclient.conf\noption rfc3442-classless-static-routes code 121 = array of unsigned integer 8;\nsend host-name \"\";\nprepend domain-name-servers 127.0.0.1;\nrequest subnet-mask, broadcast-address, time-offset, routers,\n    domain-name, domain-name-servers, domain-search, host-name,\n    netbios-name-servers, netbios-scope, interface-mtu,\n    rfc3442-classless-static-routes, ntp-servers;\n``\n. I'm usingbosh-bootstrapwith--edge-stemcellbut without--edge-deployer` (because it was bugged for some time). So I'm using:\ngem list --local bosh_deployer\nbosh_deployer (1.4.1)\nI will try again with --edge-deployer and report back.\n. The latest bosh_deployer indeed fixed this problem.\n. I also receive this error. The thing is I only have one compute node. Also the pinging means sending a ping request to the agent on the new stemcell which should respond with a pong.\nI have tried the investigate further. First I had to get into the freshly created stemcell:\nSSH into unresponsive stemcell\nLogin via console\nvcap/c1oudc0w                                                                                                                            \nsu -                                                                                                                                     \nc1oudc0w\nInstall nano                                                                                                                               \napt-get install nano\nEnable ssh password login                                                                                                                  \nnano /etc/ssh/sshd-config                                                                                                                \nChallengeResponseAuthentication yes                                                                                                      \n/etc/init.d/ssh restart\nLogin to stemcell                                                                                                                          \nbosh-bootstrap ssh                                                                                                                       \nssh vcap@{privat_ip}                                                                                                                     \nc1oudc0w                                                                                                                                 \nsu -                                                                                                                                     \nc1oudc0w\nThen I found the following messages in the logs /var/vcap/bosh/log:\nINFO: got user_data: {\"registry\"=>{\"endpoint\"=>\"http://10.200.7.4:25777\"}, \"server\"=>{\"name\"=>\"vm-be441e38-442c-4b9c-a46e-a2ffd4f8a841\"}, \"dns\"=>{\"nameserver\"=>[\"10.200.7.4\", \"10.200.7.1\"]}}\n2013-03-15_09:02:21.03547 #[26846] INFO: failed to load infrastructure settings: Cannot read settings for `http://10.200.7.4:25777/servers/vm-be441e38-442c-4b9c-a46e-a2ffd4f8a841/settings' from registry, got HTTP 500\nWhen visiting http://10.200.7.4:25777/servers/vm-be441e38-442c-4b9c-a46e-a2ffd4f8a841/settings using sshuttle I got the following:\n\n. Here the stacktrace from the openstack_registry on the micro bosh node:\nNoMethodError - undefined method `body' for #<Hash:0x00000002306310>:\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/openstack/compute.rb:338:in `rescue in request'\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/openstack/compute.rb:326:in `request'\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/openstack/requests/compute/list_servers_detail.rb:15:in `list_servers_detail'\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/openstack/models/compute/servers.rb:21:in `all'\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/core/collection.rb:141:in `lazy_load'\n /var/vcap/packages/openstack_registry/gem_home/gems/fog-1.9.0/lib/fog/core/collection.rb:22:in `each'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/server_manager.rb:67:in `find'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/server_manager.rb:67:in `server_ips'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/server_manager.rb:47:in `check_instance_ips'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/server_manager.rb:34:in `read_settings'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/api_controller.rb:22:in `block in <class:ApiController>'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:1175:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:1175:in `block in compile!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:739:in `instance_eval'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:739:in `route_eval'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:723:in `block (2 levels) in route!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:773:in `block in process_route'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:770:in `catch'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:770:in `process_route'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:722:in `block in route!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:721:in `each'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:721:in `route!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:857:in `dispatch!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:659:in `block in call!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:823:in `block in invoke'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:823:in `catch'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:823:in `invoke'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:659:in `call!'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/base.rb:644:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/rack-1.5.2/lib/rack/head.rb:11:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/sinatra-1.2.8/lib/sinatra/showexceptions.rb:21:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/rack-1.5.2/lib/rack/builder.rb:138:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:65:in `block in call'\n /var/vcap/packages/openstack_registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `each'\n /var/vcap/packages/openstack_registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `call'\n /var/vcap/packages/openstack_registry/gem_home/gems/thin-1.5.0/lib/thin/connection.rb:81:in `block in pre_process'\n /var/vcap/packages/openstack_registry/gem_home/gems/thin-1.5.0/lib/thin/connection.rb:79:in `catch'\n /var/vcap/packages/openstack_registry/gem_home/gems/thin-1.5.0/lib/thin/connection.rb:79:in `pre_process'\n /var/vcap/packages/openstack_registry/gem_home/gems/thin-1.5.0/lib/thin/connection.rb:54:in `process'\n /var/vcap/packages/openstack_registry/gem_home/gems/thin-1.5.0/lib/thin/connection.rb:39:in `receive_data'\n /var/vcap/packages/openstack_registry/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run_machine'\n /var/vcap/packages/openstack_registry/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/lib/openstack_registry/runner.rb:23:in `run'\n /var/vcap/packages/openstack_registry/gem_home/gems/bosh_openstack_registry-1.5.0.pre2/bin/openstack_registry:28:in `<top (required)>'\n /var/vcap/packages/openstack_registry/bin/openstack_registry:23:in `load'\n /var/vcap/packages/openstack_registry/bin/openstack_registry:23:in `<main>'\n. As can been seen in the stacktrace I'm using micro bosh and stemcell with version 1.5.0.pre2 \n. I have just redeployed the microbosh with the --update flag (same stemcell). And now the problem has gone away. But I have had this problem before so I think it will resurface again. I looks like the problem occurs over time.\n. The problem of the registry getting stuck over time is still a issue in newer versions of BOSH.\nBut when I read back this whole issue it looks like this was not the initial problem being described.\nI have no experience with running a multinode openstack so on second thought I think they should be two separate issues. And the debugging information I posted above seems to be more related to https://github.com/cloudfoundry/bosh/issues/96. Sorry for hijacking this issue.\n. Running fdisk -l gives:\n```\nDisk /dev/vda: 21.5 GB, 21474836480 bytes\n4 heads, 32 sectors/track, 327680 cylinders\nUnits = cylinders of 128 * 512 = 65536 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x000e7c91\nDevice Boot      Start         End      Blocks   Id  System\n/dev/vda1               1       21042     1346648+  83  Linux\nDisk /dev/vdb: 21.5 GB, 21474836480 bytes\n16 heads, 63 sectors/track, 41610 cylinders\nUnits = cylinders of 1008 * 512 = 516096 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\nDevice Boot      Start         End      Blocks   Id  System\n/dev/vdb1               1        4080     2056319+  82  Linux swap / Solaris\n/dev/vdb2            4081       41610    18915120   83  Linux\n```\nRunning df -h gives:\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/vda1             1.3G  1.1G  158M  88% /\nnone                  999M  164K  999M   1% /dev\nnone                 1005M     0 1005M   0% /dev/shm\nnone                 1005M   56K 1005M   1% /var/run\nnone                 1005M     0 1005M   0% /var/lock\nnone                 1005M     0 1005M   0% /lib/init/rw\nnone                  1.3G  1.1G  158M  88% /var/lib/ureadahead/debugfs\n/dev/vdb2              18G  250M   17G   2% /var/vcap/data\n/dev/loop0            124M  5.6M  118M   5% /tmp\n. Running swapon -s show:\nFilename                Type        Size    Used    Priority\n/dev/vdb1                               partition   2056312 0   -1\nI did a swapoff -a; e2fsck -f -y -v /dev/vdb2 as described here.\nThe problem has gone now. Could be that swapoff -a already fixed it but I did not check in between.\nRunning cat /proc/swaps gives:\nFilename                Type        Size    Used    Priority\nSo maybe this does not work.\n. On other machine with same problem:\nrunning cat /proc/swaps gives:\nFilename                Type        Size    Used    Priority\n/dev/vdb1                               partition   2056312 0   -1\nOnly running swapoff -a did indeed made the problem go away.\n. There seems to be some changes in master compared to 1.5.0.pre2 will build stemcell and try again.\n. When editing /var/vcap/bosh/settings.json by replacing:\n\"blobstore\"=>{\"plugin\"=>\"local\", \"properties\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}\nby\n\"blobstore\"=>{\"plugin\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}\nThe agent starts normally\n. With the above fix I now get the following /var/vcap/bosh/log/current\nINFO: #<Bosh::Agent::MessageHandlerError: Invalid client provider, available providers are: [\"simple\", \"s3\", \"swift\", \"atmos\", \"local\"]: \n[\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/blobstore_client-1.5.0.pre2/lib/blobstore_client/client.rb:10:in `create'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/util.rb:20:in `unpack_blob'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/apply_plan/job.rb:70:in `fetch_template'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/apply_plan/job.rb:50:in `install'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/apply_plan/plan.rb:73:in `block in install_jobs'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/apply_plan/plan.rb:72:in `each'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/apply_plan/plan.rb:72:in `install_jobs'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:117:in `apply_job'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:77:in `apply'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:10:in `process'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:274:in `process'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:259:in `process_long_running'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:181:in `block in process_in_thread'\",\n\"<internal:prelude>:10:in `synchronize'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:179:in `process_in_thread'\",\n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:160:in `block in handle_message'\"]>: [\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:83:in `rescue in apply'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:75:in `apply'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/message/apply.rb:10:in `process'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:274:in `process'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:259:in `process_long_running'\", \"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:181:in `block in process_in_thread'\", \n\"<internal:prelude>:10:in `synchronize'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:179:in `process_in_thread'\", \n\"/var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre2/lib/bosh_agent/handler.rb:160:in `block in handle_message'\"]\n. I'm currently using the ones from CI. \n+---------------+---------+--------------------------------------+\n| Name          | Version | CID                                  |\n+---------------+---------+--------------------------------------+\n| bosh-stemcell | 661     | 80eaa2bb-ef8e-46ae-99b1-c7332e549453 |\n+---------------+---------+--------------------------------------+\nBut still have the problem from time to time. I found however a workaround: doing a monit stop registry && monit start registry on the microbosh. \nI currently have a microbosh deployed so what would be helpfull logging to further debug this problem?\n. I'm currently running Version    1.5.0.pre.661 (release:2a3c861a bosh:2a3c861a)\nWIll get back on logs.\n. I found the following stacks-trace in /var/vcap/sys/log/egistry/registry.stderr.log:\nExcon::Errors::Unauthorized - Expected([200, 204]) <=> Actual(401 Unauthorized):\n        /var/vcap/packages/registry/gem_home/gems/excon-0.22.1/lib/excon/middlewares/expects.rb:10:in `response_call'\n        /var/vcap/packages/registry/gem_home/gems/excon-0.22.1/lib/excon/connection.rb:355:in `response'\n        /var/vcap/packages/registry/gem_home/gems/excon-0.22.1/lib/excon/connection.rb:249:in `request'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/core/connection.rb:21:in `request'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack.rb:194:in `retrieve_tokens_v2'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack.rb:87:in `authenticate_v2'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack/compute.rb:387:in `authenticate'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack/compute.rb:347:in `rescue in request'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack/compute.rb:333:in `request'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack/requests/compute/list_servers_detail.rb:15:in `list_servers_detail'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/openstack/models/compute/servers.rb:21:in `all'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/core/collection.rb:141:in `lazy_load'\n        /var/vcap/packages/registry/gem_home/gems/fog-1.11.1/lib/fog/core/collection.rb:22:in `each'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/instance_manager/openstack.rb:45:in `find'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/instance_manager/openstack.rb:45:in `instance_ips'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/instance_manager.rb:45:in `check_instance_ips'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/instance_manager.rb:29:in `read_settings'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/api_controller.rb:22:in `block in <class:ApiController>'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1415:in `call'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1415:in `block in compile!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `[]'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `block (3 levels) in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:960:in `route_eval'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `block (2 levels) in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:981:in `block in process_route'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:979:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:979:in `process_route'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:943:in `block in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:942:in `each'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:942:in `route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1053:in `block in dispatch!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `block in invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1050:in `dispatch!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:878:in `block in call!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `block in invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:878:in `call!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:864:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/xss_header.rb:18:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/path_traversal.rb:16:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/json_csrf.rb:18:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/base.rb:49:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/base.rb:49:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-protection-1.5.0/lib/rack/protection/frame_options.rb:31:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/nulllogger.rb:9:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/head.rb:11:in `call'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/showexceptions.rb:21:in `call'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:172:in `call'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1947:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/builder.rb:138:in `call'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:65:in `block in call'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `each'\n        /var/vcap/packages/registry/gem_home/gems/rack-1.5.2/lib/rack/urlmap.rb:50:in `call'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/connection.rb:81:in `block in pre_process'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/connection.rb:79:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/connection.rb:79:in `pre_process'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/connection.rb:54:in `process'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/connection.rb:39:in `receive_data'\n        /var/vcap/packages/registry/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run_machine'\n        /var/vcap/packages/registry/gem_home/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/backends/base.rb:63:in `start'\n        /var/vcap/packages/registry/gem_home/gems/thin-1.5.1/lib/thin/server.rb:159:in `start'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/runner.rb:34:in `start_http_server'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/lib/bosh_registry/runner.rb:18:in `run'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.661/bin/bosh_registry:28:in `<top (required)>'\n        /var/vcap/packages/registry/bin/bosh_registry:23:in `load'\n        /var/vcap/packages/registry/bin/bosh_registry:23:in `<main>'\n. I encountered the problem again today. It happened when I added the echo service to my deployment. For this packages needed to be compiled and as a consequence new vms needed to be created. The creation process all went fine (vms where created when checking in horizon) but then the compiling did not start.\nWhile the bosh deploy task was still waiting on the compilation vms, when the above fix was applied (registry restart) the deployment continued.\n. I have deployed 703 but the director has some problems while starting.\ncat /var/vcap/sys/log/director/migrate.stderr.log\n/var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `initialize': PG::Error: FATAL:  role \"bosh\" does not exist (Sequel::DatabaseConnectionError)\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `new'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:208:in `connect'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/connection_pool.rb:94:in `make_new'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:164:in `make_new'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:137:in `available'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:127:in `block in acquire'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:195:in `block in sync'\n        from <internal:prelude>:10:in `synchronize'\n. Seems like default bosh postgress user has been changed\nAlready tried:\n/var/vcap/packages/postgres/bin/psql -d bosh -U vcap -c \"create role \\\"bosh\\\" NOSUPERUSER LOGIN INHERIT CREATEDB\"\n/var/vcap/packages/postgres/bin/psql -d bosh -U vcap -c \"alter role \\\"bosh\\\" with password 'bosh'\"\ntail /var/vcap/sys/log/director/migrate.stderr.log now gives:\n/var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `exec': PG::Error: ERROR:  relation \"schema_migrations\" already exists (Sequel::DatabaseError)\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `block in execute_query'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/database/logging.rb:33:in `log_yield'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `execute_query'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `block in execute'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:111:in `check_disconnect_errors'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `execute'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:413:in `_execute'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block (2 levels) in execute'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:425:in `check_database_errors'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block in execute'\n        from /var/vcap/packages/director/gem_home/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'\n. Fixed the problem of the failed postgres migration with:\n/var/vcap/packages/postgres/bin/psql -d bosh -U vcap -c \"REASSIGN OWNED BY postgres TO bosh\"\nNow have successfully deployed microbosh 703.\nIf the problem still persists I should manifest itself within one day.\nWill report back tomorrow.\n. The problem did not reappeared. Have tried increasing the size of the deployment and the machines were added without problems. I also don't see the connection problem anymore in the registry log.\nI only see the following stacktace which does not seem to cause problems. If this stacktrace is not expected I will  create an new issue for it.\nBosh::Registry::InstanceNotFound - Can't find instance `vm-bedb1b6d-1e0e-4c2a-96fb-f70eb97d5093':\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.703/lib/bosh_registry/instance_manager.rb:57:in `get_instance'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.703/lib/bosh_registry/instance_manager.rb:31:in `read_settings'\n        /var/vcap/packages/registry/gem_home/gems/bosh_registry-1.5.0.pre.703/lib/bosh_registry/api_controller.rb:22:in `block in <class:ApiController>'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1415:in `call'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1415:in `block in compile!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `[]'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `block (3 levels) in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:960:in `route_eval'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:944:in `block (2 levels) in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:981:in `block in process_route'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:979:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:979:in `process_route'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:943:in `block in route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:942:in `each'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:942:in `route!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1053:in `block in dispatch!'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `block in invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `catch'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1035:in `invoke'\n        /var/vcap/packages/registry/gem_home/gems/sinatra-1.4.2/lib/sinatra/base.rb:1050:in `dispatch!'\n. Yes I'm using Essex.\n.  Thanks @frodenas, problem indeed seems to be fixed in master.\n. The problem can be traced back to this line\n. removing .root seems to fix the problem in my case but I don't know if it brakes other things.\n. When on the director cat /etc/resolv.conf gives:\nnameserver 10.200.7.1\ndomain novalocal\nsearch novalocal\n. When on the director cat /etc/dhcp3/dhclient.conf gives:\n```\nConfiguration file for /sbin/dhclient, which is included in Debian's\ndhcp3-client package.\n\nThis is a sample configuration file for dhclient. See dhclient.conf's\nman page for more information about the syntax of this file\nand a more comprehensive list of the parameters understood by\ndhclient.\n\nNormally, if the DHCP server provides reasonable information and does\nnot leave anything out (like the domain name, for example), then\nfew changes must be made to this file, if any.\n\noption rfc3442-classless-static-routes code 121 = array of unsigned integer 8;\nsend host-name \"\";\nsend dhcp-client-identifier 1:0:a0:24:ab:fb:9c;\nsend dhcp-lease-time 3600;\nsupersede domain-name \"fugue.com home.vix.com\";\nprepend domain-name-servers 127.0.0.1;\nrequest subnet-mask, broadcast-address, time-offset, routers,\n    domain-name, domain-name-servers, domain-search, host-name,\n    netbios-name-servers, netbios-scope, interface-mtu,\n    rfc3442-classless-static-routes, ntp-servers;\nrequire subnet-mask, domain-name-servers;\ntimeout 60;\nretry 60;\nreboot 10;\nselect-timeout 5;\ninitial-interval 2;\nscript \"/etc/dhcp3/dhclient-script\";\nmedia \"-link0 -link1 -link2\", \"link0 link1\";\nreject 192.33.137.209;\nalias {\ninterface \"eth0\";\nfixed-address 192.5.5.213;\noption subnet-mask 255.255.255.255;\n}\nlease {\ninterface \"eth0\";\nfixed-address 192.33.137.200;\nmedium \"link0 link1\";\noption host-name \"andare.swiftmedia.com\";\noption subnet-mask 255.255.255.0;\noption broadcast-address 192.33.137.255;\noption routers 192.33.137.250;\noption domain-name-servers 127.0.0.1;\nrenew 2 2000/1/12 00:00:01;\nrebind 2 2000/1/12 00:00:01;\nexpire 2 2000/1/12 00:00:01;\n}\n```\n. The problem looks similar to https://github.com/cloudfoundry/bosh/issues/53\n. Beginning to suspect the used deployer gem:\ngem list --local | grep bosh\nbosh_aws_cpi (1.5.0.pre.446)\nbosh_cli (1.5.0.pre.446)\nbosh_common (1.5.0.pre.446)\nbosh_cpi (1.5.0.pre.446)\nbosh_deployer (1.5.0.pre.446)\nbosh_openstack_cpi (1.5.0.pre.446)\nbosh_registry (1.5.0.pre.446)\nbosh_vcloud_cpi (1.5.0.pre.446)\nbosh_vsphere_cpi (1.5.0.pre.446)\n. running ssh {user_name}@10.200.7.10 with the username being the user from the log above, I'm able the login to the server. So the temporary user gets created and the public key is added.\n. works when using master and bundle install from inside the bosh directory.\nSo I think the 1.5.0.pre.446 is the being problematic. \nWill report issue at bosh-bootstrap https://github.com/StarkAndWayne/bosh-bootstrap/issues/176.\n. A new fog version (1.12.1) has been released. Is there already a issue or pivotal story for modifying CI?\n. This issue is similar to https://github.com/cloudfoundry/bosh/issues/377\n. Nice\n. If ci would publish light stemcells than the need for inception vms would be eliminated. Because the director can already fetch normal stemcells.\n. I found the reference to the property method here use p does not help.\nssl_cert: <%= p(\"ssl_cert\") %> gives: 'p': Insecure operation 'p' at level 4\n. I'm trying to use it without bosh diff. As described here\n. I was not able to reproduce this problem to get more debug information. Will reopen when I got more information.\n. I was able to reproduce the problem while updating a deployment.\nError 450002: Timed out sending `get_task' to cdca8198-2586-4fee-85a6-7f3846a2b541 after 45 seconds\nWhile I ssh into the node I found the following in /var/vcap/bosh/log/current\n2013-08-26_09:03:02.13583 /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/nats-0.4.28/lib/nats/client.rb:567:in `block in connection_completed': 'Unresponsive client detected, connection dropped' (NATS::ServerErr\nor)\n2013-08-26_09:03:02.13590       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/nats-0.4.28/lib/nats/client.rb:506:in `call'\n2013-08-26_09:03:02.13592       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/nats-0.4.28/lib/nats/client.rb:506:in `receive_data'\n2013-08-26_09:03:02.13593       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run_machine'\n2013-08-26_09:03:02.13596       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/eventmachine-0.12.10/lib/eventmachine.rb:256:in `run'\n2013-08-26_09:03:02.13598       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/handler.rb:72:in `start'\n2013-08-26_09:03:02.13601       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/handler.rb:12:in `start'\n2013-08-26_09:03:02.13602       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:120:in `start'\n2013-08-26_09:03:02.13603       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:85:in `run'\n2013-08-26_09:03:02.13605       from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/bin/bosh_agent:102:in `<top (required)>'\n2013-08-26_09:03:02.13606       from /var/vcap/bosh/bin/bosh_agent:23:in `load'\n2013-08-26_09:03:02.13607       from /var/vcap/bosh/bin/bosh_agent:23:in `<main>'\n2013-08-26_09:03:02.61913 #[3515] INFO: Starting agent 1.5.0.pre.3...\n2013-08-26_09:03:02.61919 #[3515] INFO: Configuring agent...\n2013-08-26_09:03:02.62199 #[3515] INFO: Configuring instance\n. Errors seems to be raised on the nats server. Will try chancing ping interval.\n. As can be seen here. The pull request was put into the backlog because of swift blobstore access issues.\n@drnic Could this be solved by giving access to the swift used for your zookeeper project?\ncc @calebamiles\n. @drnic no I looks like this PR is blocked because the bosh team does not have access to a swift deployment. From the bosh pivotal tracker: \"blocked on getting a swift object storage account\". So I thought maybe S&W could help them out.\n. @calebamiles @drich10 could you please review my changes. Thanks in advance\n. @adamstegman Good one. It worked when the PR was initially submitted. But since then the connection_options have been refactored. I have not tried to get it working after this.\n. @adamstegman Yes I have started implementing a solution but got bussy with other stuff.\n. @adamstegman Yes I have started implementing a solution but got bussy with other stuff.\n. Since creating a final release does not involve a Director and is a pure local operation. I will provide some information about my local setup:\nruby:\nruby 1.9.3p448 (2013-06-27 revision 41675) [x86_64-darwin13.0.2]\nosx:\nSystem Version: OS X 10.9 (13A3017)\nKernel Version: Darwin 13.0.2\nbosh:\nBOSH 1.1761.0\nThe config file used for the blobstore final.yml\nIn combination with a private.yml containing:\n``` yml\nblobstore:\n  s3:\n    access_key_id: [redacted]\n    secret_access_key: [redacted]\n```\n. Since creating a final release does not involve a Director and is a pure local operation. I will provide some information about my local setup:\nruby:\nruby 1.9.3p448 (2013-06-27 revision 41675) [x86_64-darwin13.0.2]\nosx:\nSystem Version: OS X 10.9 (13A3017)\nKernel Version: Darwin 13.0.2\nbosh:\nBOSH 1.1761.0\nThe config file used for the blobstore final.yml\nIn combination with a private.yml containing:\n``` yml\nblobstore:\n  s3:\n    access_key_id: [redacted]\n    secret_access_key: [redacted]\n. As requested a gist containing the full cli output. https://gist.github.com/rkoster/8453995\n. As requested a gist containing the full cli output. https://gist.github.com/rkoster/8453995\n. @dajulia3 yes it is a repeatable problem, I got the same error when I tried to create a final release for cf-services-contrib-release from a ubuntu vm.\n. @dajulia3 yes it is a repeatable problem, I got the same error when I tried to create a final release for cf-services-contrib-release from a ubuntu vm.\n. It indeed was a problem with write access. After regenerating keys I was able to create a final release. It would however still be nice if bosh could give a clearer error message. In the case of an access denied.\n. It indeed was a problem with write access. After regenerating keys I was able to create a final release. It would however still be nice if bosh could give a clearer error message. In the case of an access denied.\n. After further investigation I think this issue can be closed. Because the case I encountered was due to using valid aws credentials which did not gave access to the target bucket. This feels a bit like a edge case and is documented well enough now in this issue.\n. Maybe we could take some inspiration from a [similar discussion](https://github.com/cloudfoundry/cf-release/pull/232) which took place for cf-release.\nThe final solution was https://github.com/cloudfoundry/cf-release/commit/60abe861da34ea5f2ee24075d3670e20caec504e.\n. Maybe we could take some inspiration from a [similar discussion](https://github.com/cloudfoundry/cf-release/pull/232) which took place for cf-release.\nThe final solution was https://github.com/cloudfoundry/cf-release/commit/60abe861da34ea5f2ee24075d3670e20caec504e.\n. @mariash Piston 3\n. @mariash Piston 3\n. @cppforlife we were able to successfully upgrade our microbosh to use the 2739 stemcell by using `config_drive: disk`. The bosh upgrade was successful as well which confirms vm creation works.\n. @cppforlife we were able to successfully upgrade our microbosh to use the 2739 stemcell by using `config_drive: disk`. The bosh upgrade was successful as well which confirms vm creation works.\n. @cppforlife figured out the simplest solution which would not affect the current behaviour is adding a Host header. I have created a PR which implements that solution #806.\n. @cppforlife figured out the simplest solution which would not affect the current behaviour is adding a Host header. I have created a PR which implements that solution #806.\n. Nice will have a look\n. Nice will have a look\n. thanks @cppforlife \n. thanks @cppforlife \n. would be nice if we could just parse `env.http(s)_proxy` and dynamically set username/password:\n2.2.3 :005 > uri = URI(\"https://foo:bar@example.com:8080\")\n => #\n2.2.3 :006 > uri.user\n => \"foo\"\n2.2.3 :007 > uri.password\n => \"bar\"\n```\nThis would be more consistent with other bosh releases. For example this already works with concourse.\nMy personal reason for wanting concourse and bosh to behave the same way with regard to proxy's is the --proxy flag in bucc.. would be nice if we could just parse env.http(s)_proxy and dynamically set username/password:\n2.2.3 :005 > uri = URI(\"https://foo:bar@example.com:8080\")\n => #<URI::HTTPS https://foo:bar@example.com:8080>\n2.2.3 :006 > uri.user\n => \"foo\"\n2.2.3 :007 > uri.password\n => \"bar\"\nThis would be more consistent with other bosh releases. For example this already works with concourse.\nMy personal reason for wanting concourse and bosh to behave the same way with regard to proxy's is the --proxy flag in bucc.. It seemed like a good idea since more and more releases are switching to using: postgres-release, instead of maintaining their own postgres (cf-deployment, concourse).\nAlso we have been using the same postgres for concourse and the director for almost a year now. And I don't see an easy way to migrate existing users to 2 different postgres processes without manual steps (as in just 1 bosh create-env).\n. It seemed like a good idea since more and more releases are switching to using: postgres-release, instead of maintaining their own postgres (cf-deployment, concourse).\nAlso we have been using the same postgres for concourse and the director for almost a year now. And I don't see an easy way to migrate existing users to 2 different postgres processes without manual steps (as in just 1 bosh create-env).\n. The instrumentor is included here in the connection_options. So there are no deployment file changes needed to use it. If we would chose to make it configurable the people that will benefit from this functionality will be limited.\n. ",
    "kushmerick": "i have seen this too.  tried to colocate UAA and CCNG.  doesn't work because bosh doesn't give CCNG its default properties.\n. for sure services jobs don't spec their properties today.\nit makes sense in smallish CFs to colocate service-gateways, so for sure all foobar_gateway jobs should be upgraded.\nbut IMHO it is a bad idea to colocate service nodes, except perhaps for toy smoke-test scenarios.  the colocated services will step all over each other.  not in the sense of (eg) writing to the same files [ie, the reason you can't colocate jobs that use vcap_registrar].  Rather, VMs can become full because service nodes assume they are the only consumers of disk/memory.  One can argue we should remove this assumption [though we thought long & hard about how to achieve lights-out operation of each service and I don't think see an alternative].  in the meantime, we don't need to bother to upgrade any of the foobar_node_ng jobs.\n. insert newline\n. ",
    "mreider": "Ok. We talked about this a lot today and yesterday. Basically we'll have to convert jobs to use the new property format for anything in CF that needs to be colocated. I talked to @abic and he said it took him a day and a half to convert the BOSH Director over. He also said @kowshik was the first one to convert a kernel job over last year. So the first thing we need to do is to answer the question \"What jobs might be colocated?\" and then update those jobs to use the new format. Otherwise we just need to go through all jobs and update them, which is a major exercise, but important nonetheless.\nI am also curious what @dsboulder and @kushmerick think about all this - since colocating services is  important? Are they all just using old style properties so the problem never surfaced?\n. Ok. We talked about this a lot today and yesterday. Basically we'll have to convert jobs to use the new property format for anything in CF that needs to be colocated. I talked to @abic and he said it took him a day and a half to convert the BOSH Director over. He also said @kowshik was the first one to convert a kernel job over last year. So the first thing we need to do is to answer the question \"What jobs might be colocated?\" and then update those jobs to use the new format. Otherwise we just need to go through all jobs and update them, which is a major exercise, but important nonetheless.\nI am also curious what @dsboulder and @kushmerick think about all this - since colocating services is  important? Are they all just using old style properties so the problem never surfaced?\n. We are working on CentOS. We are going to keep issues open that reflect future work.\n. We are working on CentOS. We are going to keep issues open that reflect future work.\n. We talked about it during the inception - and we think the only way to handle this stuff is to do it at the infrastructure level - stopping apps before you pause VM's, then snapshotting the whole thing, then doing an update with apps still stopped, and then rolling back if the updates fail.\nSo back to the question. What is the point of snapshotting things you can't restore?\n. Ok. Lets keep talking through this. In Cloud Foundry we are worried about data loss, so taking snapshots of something would require some kind of maintenance mode for the entire system, so that transactions can complete, and no other requests come through.\nOk. So for BOSH outside of CloudFoundry, is there information that the Director needs to keep consistent when it updates stemcells or packages? Things that it would get confused about if you rolled back what it thought was a successful update?\n. Interesting article. I would think that your intent is not to explore each and every job's behavior for flushing, but to take a brutal approach since this may never be used in production, and is more for CI?\nCan you implement some kind of message that snapshots are for CI? I would hate for someone to think they could resurrect a Cloud Foundry using these commands. Maybe even call them CI snapshot overtly?\n. ",
    "oppegard": "We've added code that will give a better error when trying to mix old-style properties with new-style properties. We also have tasks underway to convert jobs in cf-release to new-style properties.\n. Are you still having similar issues with newer versions of BOSH?\n. Are you still having this problem with OpenStack stemcells? If so, have you tried building a newer one that may have addressed the issue?\n. How recent is the microbosh code ('bosh status' should show the git sha). Do the registry logs under /var/vcap/sys/log/registry show anything of interest?\n@frodenas have you seen behavior like this on openstack?\n. CentOS is currently not supported for BOSH stemcells, specifically in the bosh_agent codebase. If a 3rd party is building stemcells for BOSH, I'd suggest contacting them about issues you're having with their stemcells.\n. Look like this was fixed. Please re-open if you encounter this issue again.\n. Hello and thanks for contributing this pull request. Since we haven't seen any activity on it for over a month we're going to close this PR. Please feel free to address the points raised by @pmenglund and @mark-rushakoff and open a new PR.\nThanks!\n. I'll update the docs.\n. Doh, let me fix the tests first.\n. Will do Ferdy :koala: \n. Hey Ferdy,\ndownload_latest_stemcell won't work for openstack as-is because 'kvm' is in the stemcell name. Sean and I have a WIP to use CI pipeline for openstack and aws here:\nhttps://github.com/cloudfoundry/bosh/commit/f9958abd12dfa9daf661491443ca0cb3c8e5db3f\nHe'll be finishing that work this week on the bosh-better pair.\n. ",
    "andypiper": "Actually that makes a lot of sense - however I guess I would then ask, what happens when frameworks and runtimes become buildpacks? Do buildpacks have the power to install dependencies within a stemcell? #confused\n. Actually that makes a lot of sense - however I guess I would then ask, what happens when frameworks and runtimes become buildpacks? Do buildpacks have the power to install dependencies within a stemcell? #confused\n. OK. Closing... will see where this should fit, if I can.... wish me luck :-)\n. OK. Closing... will see where this should fit, if I can.... wish me luck :-)\n. Absolutely, nevertheless would like to understand how things are \"expected\" to work (agreed beyond a PR).\n\"install the package (as a bosh package...)\" the concepts here regarding installing as a bosh package and how those become available to a CF release are not quite clear to me yet, going to have a dig. Thanks.\n. Absolutely, nevertheless would like to understand how things are \"expected\" to work (agreed beyond a PR).\n\"install the package (as a bosh package...)\" the concepts here regarding installing as a bosh package and how those become available to a CF release are not quite clear to me yet, going to have a dig. Thanks.\n. doh! :+1: \n. doh! :+1: \n. ",
    "stefanschneider": "Much better now :)\nHere is the snipped executed with irb on a vSphere stemcell:\nirb(main):045:0> puts File.read(\"/proc/sys/dev/cdrom/info\").slice(/drive name:\\s_\\S_/).slice(/\\S_\\z/)\nsr0\n=> nil\nirb(main):046:0> puts File.read(\"/proc/sys/dev/cdrom/info\").slice(/drive name:\\s_\\S*/)\ndrive name:     sr0\n=> nil\nirb(main):047:0> puts File.read(\"/proc/sys/dev/cdrom/info\")\nCD-ROM information, Id: cdrom.c 3.20 2003/12/17\ndrive name:     sr0\ndrive speed:        1\n...\n. Thanks @ankurcha for adding the tests so quickly. :+1:  Saved me some time.\n. Thx @pmenglund. I'll make a pull request soon.\n. I've put and tested the following fix and worked Ok. I'm not sure which one is more hacky and/or future proof. Which one should go for a pull request?\n``` git\ndiff --git a/release/jobs/director/templates/director.yml.erb b/release/jobs/director/templates/director.yml.erb\nindex 1e7feda..400ec60 100644\n--- a/release/jobs/director/templates/director.yml.erb\n+++ b/release/jobs/director/templates/director.yml.erb\n@@ -132,7 +132,7 @@ cloud:\n         user: <%= user %>\n         password: <%= password %>\n         datacenters:\n-          <% datacenters.each do |dc| %>\n+          <% properties.vcenter.datacenters.each do |dc| %>\n           - name: <%= dc.name %>\n             vm_folder: <%= dc.vm_folder || \"BOSH_VMs\" %>\n             template_folder: <%= dc.template_folder || \"BOSH_Templates\" %>\n```\n. Can't believe I forgot about the CI. Gerrit would have sent an email :)\n. Not sure why the build failed for a director spec: https://travis-ci.org/cloudfoundry/bosh/jobs/5293030/#L437\nEverything passed on my dev box.\nCan I re-trigger the travis build somehow?\n. Rebase done and build is green :)\n. I also like to have a plugin system, but I don't see an easy way to implement this. From what I can remember, in the old Gerrit days, there was a push-back to update the blobstore gem in bosh_agent due to heavy dependencies for OpenStack Swift. Maybe @frodenas has some insight into this.\nVCR looks looks like a good solution for doing offline tests. I'll give it a try.\nDoes Pivotal use some kind of offline testing tricks for S3 and Swift, or just provide the CI with access keys to the real services?\n. @aramprice / @mariash I commited the changes. \nRubocop is happy now and all specs pass - at least on my dev machine. Hope that Travis will be green now.\nThanks\n--Stefan\n. The build failed because it could not load the gem: https://travis-ci.org/cloudfoundry/bosh/jobs/9894326#L333\nAdded it back in Gemfile under the 'test' group.\n. The unit build passed. Not sure why the integration tests failed, but doesn't look it has something to do with the blobstore_client changes.\nShould I also leave the vcr gem in the blobstore_client.gemspec?\n. ",
    "harikrishna1210": "Hi,\nYes i am using right vcenter server credentials only.Here it's trying to connect the https://10.112.118.100/sdk/vimService and getting for local issuer certificate this is not happening.Once  i tried to open  this url in browser \"https://10.112.118.100/sdk/vimService\" but nothing is opened.I am new to vsphere also,can you please tell me in vsphere i have to start/enable any \"vimservice\" service or any thing i am missing.If it's required to enable suggest me how to do that.\nThis my micro_bosh config file.\n\nname: vsphere\nnetwork:\n  ip: 10.117.38.171\n  netmask: 255.255.255.0\n  gateway: 10.117.38.253\n  dns:\n- 10.117.0.1\n- 10.117.0.2\n  cloud_properties:\n    name: VM Network\n  resources:\n  persistent_disk: 16384\n  cloud_properties:\n    ram: 4096\n    disk: 16384\n    cpu: 2\n  cloud:\n  plugin: vsphere\n  properties:\n    agent:\n      ntp:\n       - time.windows.com\n       - 0.asia.pool.ntp.org\n    vcenters:\n      - host: 10.112.118.100\n        user: Administrator\n        password: chorus123!\n        datacenters:\n          - name: bosh\n            vm_folder: MicroBOSH_VMs\n            template_folder: MicroBOSH_Templates\n            disk_path: MicroBOSH_Disks\n            datastore_pattern: boshdatastore\n            persistent_datastore_pattern: boshdatastore\n            allow_mixed_datastores: true\n            clusters:\n              - boshcluster\n  apply_spec:\n  agent:\n    blobstore:\n      address: 10.117.38.171\nnats:\n  address: 10.117.38.171\n\nproperties: {}\nThanks,\nHari.\n. hi,\n  We are doing these operations under proxy.May be this is the reason or some things else for failure.Bosh user \"Administrator\" have all privileges(permissions). Can you please suggest me what's going wrong here or any mistake i did.\nThanks,\nHari.\n. Thanks,Its working fine.\n. I am using this below as  manifest file for deployment.\ncentos-deploy1.yml file.\n\nname: centos-deploy1\ndirector_uuid: 6ed4b624-2407-46c6-8051-c4235ba7de0b\nrelease:\n  name: zab-rel\n  version: latest\nnetworks:\n- name: default\n  subnets:\n  - reserved:\n    - 10.117.38.1 - 10.117.38.177\n      static:\n    - 10.117.38.178 - 10.117.38.180\n      range: 10.117.38.0/24\n      gateway: 10.117.38.253\n      dns:\n    - 10.117.0.1\n    - 10.117.0.2\n      cloud_properties:\n      name: VM Network\nresource_pools:\n- name: medium\n  stemcell:\n    name: bosh-centos-stemcell\n    version: 1.5.0.pre2\n  network: default\n  size: 1\n  cloud_properties:\n    ram: 1024\n    disk: 8192\n    cpu: 1\ncompilation:\n  workers: 10\n  network: default\n  cloud_properties:\n    ram: 2048\n    disk: 4096\n    cpu: 4\nupdate:\n  canaries: 1\n  canary_watch_time: 60000\n  update_watch_time: 60000\n  max_in_flight: 1\n  max_errors: 2\njobs:\n- name: zabbix\n  template: zabbix\n  instances: 1\n  resource_pool: medium\n  networks:\n  - name: default\n    static_ips:\n    - 10.117.38.178\nproperties:\n  vcenter:\n    address: 10.112.118.100\n    user: Administrator\n    password: chorus123!\n    datacenters:\n      - name: bosh\n        vm_folder: CF_VMs\n        template_folder: CF_Templates\n        disk_path: CF_Disks\n        datastore_pattern: boshdatastore\n        persistent_datastore_pattern: boshdatastore\n        allow_mixed_datastores: true\n        clusters:\n        - boshcluster\nNote: In  resource pool section i changed stemcell attributes,Then please see below as result.  \nFor centos:\nstemcell:\n    name: bosh-centos-stemcell\n    version: 1.5.0.pre2\nresult: It's not working.\nFor ubuntu:\nstemcell:\n    name: bosh-stemcell\n    version: 0.6.7\nresult: It's working perfectly\nNote: Can you please check once and tell me what needs to be done for fixing this above error.\nThanks,\nHari.\n. ",
    "danielkwinsor": "Yes and no.  The VM is created on openstack ok, and probably boots up and all that.  What happened is that, after creating x number of VMs, the x+1th VM is put on a compute node that is different than the compute node that the director VM is on.  For some reason the network is set up such that VMs on different compute nodes can't ping each other.  Thus the VM itself may have been created ok, but I get the error \"Timed out pinging to b5d59cdc-a4e0-417d-977d-d5d1ca967cc8 after 600 seconds (00:10:50)\" when doing a \"bosh deploy\"  Make sense?\n. ",
    "resouer": "Sorry, I have destroyed that VM and use a stable bosh_cli gem. But I remember the error occurs at:\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_common/lib/common/common.rb#L8\nand it's a normal uninitialized constant error even though I required the needed gems. I'm not sure whether it is because my ENV is corrupted.\n. @tsaleh  Yes, for now,  this problem only happens for the dav blobstore client. I think  @m1093782566 have more details to share.\n. ",
    "ajpande": "Yes this worked ! Thanks a lot :)\nBut I am getting a new error when i execute  \"bosh micro deploy ami-64d59436\"\nI have kept key pair in ~/.ssh/ and name of the key is bosh. So I dont know why i am getting error below. Please let me know where exactly i should keep this key. \napande@pnq-apande:~/deployments$ bosh micro deploy ami-64d59436\nDeploying new micro BOSH instance aws/micro_bosh.yml' tohttp://aws:25555' (type 'yes' to continue): yes\nDeploy Micro BOSH\n  using existing stemcell (00:00:00)\nCreating VM from ami-64d59436       |oo                      | 1/11 00:00:02  ETA: --:--:--/usr/lib/ruby/gems/1.9.1/gems/aws-sdk-1.6.9/lib/aws/core/client.rb:318:in return_or_raise': The key pair 'bosh' does not exist (AWS::EC2::Errors::InvalidKeyPair::NotFound)\n    from /usr/lib/ruby/gems/1.9.1/gems/aws-sdk-1.6.9/lib/aws/core/client.rb:419:inclient_request'\n    from (eval):3:in run_instances'\n    from /usr/lib/ruby/gems/1.9.1/gems/aws-sdk-1.6.9/lib/aws/ec2/instance_collection.rb:265:increate'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_aws_cpi-0.7.0/lib/cloud/aws/cloud.rb:119:in block in create_vm'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_common-0.5.4/lib/common/thread_formatter.rb:46:inwith_thread_name'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_aws_cpi-0.7.0/lib/cloud/aws/cloud.rb:91:in create_vm'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:224:increate_vm'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:134:in block in create'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:84:instep'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:133:in create'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:104:inblock in create_deployment'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:97:in with_lifecycle'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/deployer/instance_manager.rb:103:increate_deployment'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_deployer-1.4.1/lib/bosh/cli/commands/micro.rb:171:in perform'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/command_handler.rb:57:inrun'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/runner.rb:61:in run'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/lib/cli/runner.rb:18:inrun'\n    from /usr/lib/ruby/gems/1.9.1/gems/bosh_cli-1.0.3/bin/bosh:16:in <top (required)>'\n    from /usr/local/bin/bosh:19:inload'\n    from /usr/local/bin/bosh:19:in `'\nRegards\nAjay \n. Dr .Nic , Any idea ?\nRegards\nAjay \n. ",
    "bluesalt": "It doesn't pass the CI. However, I re-trigger some other jobs that has been built successfully and they fail with same cases. Is it possible the problem of travis-ci environment ?\n. It doesn't pass the CI. However, I re-trigger some other jobs that has been built successfully and they fail with same cases. Is it possible the problem of travis-ci environment ?\n. ",
    "qch1843": "Hi,\nI had a try by adding proxy_uri into cloud/properties/aws section, but bosh micro deploy still failed with same error. My micro_bosh.yml is attached below, I don't have apply_spec/properties/aws section, not sure whether it's ciricial.\n``` ruby\nname: aws\nlogging:\n  level: DEBUG\nnetwork:\n  type: dynamic\n  vip: xx.xx.xx.xx\nresources:\n  persistent_disk: 20000\n  cloud_properties:\n    instance_type: m1.small\n    availability_zone: eu-west-1a\ncloud:\n  plugin: aws\n  properties:\n    aws:\n      access_key_id: xxxxxxxxxxxxxxxxxxx\n      secret_access_key:  xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n      proxy_uri: http://10.144.1.10:8080\n      default_key_name: bosh\n      default_security_groups: [\"bosh\"]\n      ec2_private_key: ~/.ssh/bosh.pem\n      ec2_endpoint: ec2.eu-west-1.amazonaws.com\napply_spec:\n  agent:\n    blobstore:\n      address: xx.xx.xx.xx\n    nats:\n      address: xx.xx.xx.xx\n  properties:\n    aws_registry:\n      address: xx.xx.xx.xx\n```\n. ",
    "jfoley": "@tsaleh - Can you comment?\n@aramprice / @jfoley\n. @mreider - is this still something you are tracking?\n@aramprice / @jfoley\n/cc @tsaleh \n. Closing stale issue\nCF Community Pair (@chou & @jfoley)\n. @drnic - we can't accept this PR without some tests for the changes (even though the code is currently not under test). If you can provide some tests, we should be able to get this accepted.\n@aramprice / @jfoley\n/cc @tsaleh \n. @drnic - Thanks, closing.\n@aramprice / @jfoley\n. @youngm - I'm not sure if you're still having this problem, but here are the reasons that we could come up with as to why this might be happening:\n- either the bosh cli, or the director (on the stemcell) may be of a version which does not know about #property\n- #property may be being called from a job template - this function is only available for deployment manifest templates\nDoes either of these sound likely?\nThere seems to be a lot of issues around #property (for manifest templates), #p (for job templates), as well as some confusion among the BOSH team. Hopefully the usage will become more intuitive in the future.\n@aramprice / @jfoley\n. Looks like this is an old issue so we are closing.\n@aramprice / @jfoley\n. @tsaleh - this appears to still be an issue, is this something that we are going to be tackling any time soon or should this be closed?\n@aramprice / @jfoley\n. Closing since this appears to be fixed by #419 \nCF Community Pair (@chou & @jfoley)\n. @tsaleh - what's the status of these stories, will we be getting to them?\n@aramprice / @jfoley\n. @tsaleh - can you weigh in on this?\n@aramprice / @jfoley\n. @drnic - this seems like a good idea (the CF team has an incantation in our deploy checklist for working around this). Do you have any sense of how big this change would be?\n@tsaleh - is this something we'd accept a PR for / have the bandwidth to implement?\n@aramprice / @jfoley\n. @tsaleh - what's the status of this?\n@aramprice / @jfoley\n. @tsaleh - is this still in discussion?\n@aramprice / @jfoley\n. Could this possibly be related to https://github.com/cloudfoundry/bosh/issues/315 ?\n@aramprice / @jfoley\n. @drnic - is this something you can submit a PR for? In the mean time, we are closing this issue.\n@aramprice / @jfoley\n. @drnic \nIs this still an issue? If not, we would like to close this since it looks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\n. @tsaleh - Is this something we would consider either implementing or adding to the BOSH docs?\n@aramprice / @jfoley\n. @drnic - closing this issue, but it sounds like a pull request would get accepted making this change.\n@aramprice / @jfoley\n/cc @tsaleh \n. @tsaleh - this seems like an issue we should take a look at.\n@aramprice / @jfoley\n. @drnic - We merged this manually, if it makes it through the BATs it'll be on master by the weekend (https://github.com/cloudfoundry/bosh/commit/c240ecd7841ae486d3a394bfda9337966ce99f3f).\n@aramprice / @jfoley\n. @drnic - The 804 stemcells are pretty old, is this still an issue or can we close it?\n@aramprice / @jfoley\n. @TieWei - is this still being worked on?\n@aramprice / @jfoley\n. @drnic - This seems like a good suggestion, and I think this would get accepted if you have a pull request. In the mean time, we are going to close this issue for now to clean up our issue list.\n@aramprice / @jfoley\n/cc @tsaleh \n. @james-masson - this looks like a reasonable change to me, @tsaleh is this something that we would accept a PR for?\n@aramprice / @jfoley\n. Merged manually to the develop branch (https://github.com/cloudfoundry/bosh/commit/f694e3d244075e6a6d5e5bf34a6528f9d52599a0), this should be in master in a few hours.\n@aramprice / @jfoley\n. @Rikbruggink - it looks like there is an issue with loading the CDROM on this instance (failed to load infrastructure settings: No bosh cdrom env: #) which might be the reason why the persistent disk is not being loaded. It appears that the cached settings data which is being loaded does not contain any persistent disk info (\"persistent\"=>{}). Can you look in to why this CDROM might be failing to mount? The code which mounts the CD is in bosh_agent/lib/bosh_agent/infrastructure/vsphere/settings.rb:49-99 - which could arguably use some cleanup.\nFor a bit of background, on vSphere, Bosh mounts a CDROM as a way of passing data about the infrastructure from the director to the agent such as persistent disks, network settings, etc.\n@aramprice / @jfoley\n. Closing, see #388 \n@aramprice / @jfoley\n. Closing\n@aramprice / @jfoley\n. @drnic - should we go ahead and close this since the a PR will create an issue?\n@aramprice / @jfoley\n. @drnic - Merged this manually with some minor changes: (https://github.com/cloudfoundry/bosh/commit/7b919eb7a21be42c1617ac2da5cf99507539ea13)\n@aramprice / @jfoley\n. Looks like this was fixed by the PR above, closing.\nCF Community Pair (@jfoley & @jtuchscherer)\n. @xoebus @goonzoid \nJust checking for a status update- any news?\nThanks,\nCF Community Pair (@jfoley & @jtuchscherer)\n. @goehmen,\nWould you mind updating on the state of this PR?\nCF Community Pair (@jfoley & @jtuchscherer)\n. @zoujin \nThanks for the PR. However, the BOSH team works off of the develop branch so in order for this to be accepted, please resubmit it against that branch.\nAlso, as the bot said, we will need to have a signed CLA in order to accept any changes.\nWe are closing this PR for now.\nThanks,\nCF Community Pair (@jfoley & @jtuchscherer)\n. @jhellan We were not able to reproduce the rspec failure from a clean checkout of the develop branch. \nbundle exec rspec spec/unit/bosh/deployer/deployer_renderer_spec.rb\n[...]\nFinished in 0.00703 seconds\n8 examples, 0 failures\nIn addition, we would not be able to accept this PR as is since the BOSH team works off of develop.\nThat being said, looking more closely at the code, you are correct in that the test and implementation  are slightly different and could certainly cause flakey test failures. Converting time to strings is probably a bandaid on the issue though, so perhaps we can come up with something more clever (e.g. better usage of timecop).\nThanks,\nCF Community Pair (@jfoley & @jtuchscherer)\n/cc @goehmen \n. @metadave \nThanks for the PR! However we can't accept this as is, since the BOSH team works off of the develop branch so you will need to file a new PR against that branch. \nIt also appears to have broken the travis build, but it also looks like the failure is unrelated to your change as far as we can tell.\nPinging @goehmen to see if this is a change that we would like to accept.\nThanks,\nCF Community Pair (@jfoley & @jtuchscherer)\n. @metadave,\nThanks for resubmitting- it also looks like the build didn't flake on you this time :+1: \n@goehmen would you take a look and see if this is something you want to merge?\nThanks,\nCF Community Pair (@jfoley & @jtuchscherer)\n. Closed as it was merged in.\n. Hi @peterellisjones,\nIs there a reason why you would need to symbolize the keys?\ncc @phanle \n. This will limit the resources for the agent process itself, not the job processes inside the containers (started by monit).\n. ",
    "tammersaleh": "Hi @qch1843 - proxy support is something we'd be happy to accept if given a pull request, but not something that's currently prioritized on the roadmap.  I'm going to close this issue.  Please reference it if you (or anyone else) want to take it on as a PR.\n. Hi @qch1843 - proxy support is something we'd be happy to accept if given a pull request, but not something that's currently prioritized on the roadmap.  I'm going to close this issue.  Please reference it if you (or anyone else) want to take it on as a PR.\n. Interesting.  Isn't this functionality what the drain scripts are for?  The DEAs, for instance, know to migrate running customer applications to another box before they're decommissioned.\n. Interesting.  Isn't this functionality what the drain scripts are for?  The DEAs, for instance, know to migrate running customer applications to another box before they're decommissioned.\n. Thanks, @ryantang.  That's correct.  It definitely feels like a regression to me, but not for a feature that we use, or that we hear about many people using.  It's unlikely that we'll have time to fix this in the near future.  Feel free to send a pull request if it's something you're relying on.\n. Thanks, @ryantang.  That's correct.  It definitely feels like a regression to me, but not for a feature that we use, or that we hear about many people using.  It's unlikely that we'll have time to fix this in the near future.  Feel free to send a pull request if it's something you're relying on.\n. Yeah, I'm divided on whether this is the right way to go about managing app scale.  I'm going to close it for now.\n. We don't have it in our backlog, but we'd welcome a PR for it.  That being said, it might be a good idea to hold off on the PR for a couple of weeks, since it'll probably involve the stemcell building code, which is under heavy refactoring as part of our CentOS work.\n. Agreed.  Send us a PR with updated documentation and we'll merge it in.\n. Agreed.  Send us a PR with updated documentation and we'll merge it in.\n. We agree, but we haven't found a good pattern for dealing with this.  I'm going to close this issue in order to reduce the noise for the community pair.\n. We agree, but we haven't found a good pattern for dealing with this.  I'm going to close this issue in order to reduce the noise for the community pair.\n. That does sound like annoying UX for a release developer.  We'd gladly accept a PR to address it.\n. That does sound like annoying UX for a release developer.  We'd gladly accept a PR to address it.\n. There's a lot of optimization that could be done here, but it's not something we'll have the cycles to do any time soon.  We are starting to improve the package cache system such that compilation of jobs will only need to happen once.\n. There's a lot of optimization that could be done here, but it's not something we'll have the cycles to do any time soon.  We are starting to improve the package cache system such that compilation of jobs will only need to happen once.\n. Hey @drnic - thanks for the PR.  Can you make this PR use _ consistently with the existing CLI options and submit a second PR for changing _ to - across the board?  If you resubmit with just the addition of --gateway_identity_file, then we can continue with this PR.\n. Hey @drnic - thanks for the PR.  Can you make this PR use _ consistently with the existing CLI options and submit a second PR for changing _ to - across the board?  If you resubmit with just the addition of --gateway_identity_file, then we can continue with this PR.\n. Seems like caching is the wrong approach - being able to explicitly save a gateway seems more predictable...  For example, if I accidentally specify a --gateway option, and then try to ssh again without it, would it still try to use the old gateway settings?  Seems prone to confusion and error.\n. Seems like caching is the wrong approach - being able to explicitly save a gateway seems more predictable...  For example, if I accidentally specify a --gateway option, and then try to ssh again without it, would it still try to use the old gateway settings?  Seems prone to confusion and error.\n. We haven't been able to recreate this.  \nI'm more inclined to remove this tool from public consumption than to add enhancements to it.  The tool is too dangerous to use outside of the CF development team, who are guaranteed to all be working in isolated AWS environments.\n. We haven't been able to recreate this.  \nI'm more inclined to remove this tool from public consumption than to add enhancements to it.  The tool is too dangerous to use outside of the CF development team, who are guaranteed to all be working in isolated AWS environments.\n. Man, I really hate to say this, but we're not going to be able to merge this in.\nI try to do a good job of saying \"no\" if we're not going to accept something before a lot of work has been put into it, but I failed on this one.  It looked like a fairly self-contained pull request, but the team is worried about CI, documentation, and overall maintenance if we pull it in.\nYou've been really great in working with us on this feature, and I wish I had another answer for you.\nFor the record, we absolutely do need to split the blobstore gem out into a set of compatible gems that the user can choose when creating a release.  This is also the case for the CPI, and for stemcells in general.  This would allow the community to manage this sort of feature independent of the bosh team.  We have a small team with a lot of priorities, which means this refactoring isn't going to happen any time soon.\nAgain, I'm sorry to have to close this, given how great you've been to work with.\n. @allomov currently we're focused on rewriting the bosh agent in golang, and then in the CPI extraction.  The agent requires a blobstore as well, so that complicates the rubygem path.  We're not sure how we'll proceed - might be a blobstore proxy - honestly not sure.\n. Agreed.  We'd be happy to accept a PR on this.\n. Agreed.  We'd be happy to accept a PR on this.\n. This feels like a pretty huge change.  We'd love to see implementations, but don't have the bandwidth to take this on internally.\n. This feels like a pretty huge change.  We'd love to see implementations, but don't have the bandwidth to take this on internally.\n. /cc @goehmen, the new BOSH PM.  \nIt's very similar, but with the following changes:\n1. We're using templates instead of template when using the hash syntax.  We will eventually deprecate the old behavior of specifying an array of template names under the singular template key.  It's confusing and makes the parsing difficult.\n2. We're not going to rearrange the job directory structure at this time.  Instead, we're going to deny deployments with multiple releases with conflicting job names.  We can revisit that later if it's an issue for people.\nPeople can track progress via this tracker epic.\n. /cc @goehmen, the new BOSH PM.  \nIt's very similar, but with the following changes:\n1. We're using templates instead of template when using the hash syntax.  We will eventually deprecate the old behavior of specifying an array of template names under the singular template key.  It's confusing and makes the parsing difficult.\n2. We're not going to rearrange the job directory structure at this time.  Instead, we're going to deny deployments with multiple releases with conflicting job names.  We can revisit that later if it's an issue for people.\nPeople can track progress via this tracker epic.\n. Hi Borovsky,\nsorry about the delay.  I'm taking over PM duties for BOSH.  This is a useful feature, but the size of this pull request means it's a non-trivial job to review, merge & test it.  I can't say when we're going to be able to get to it.  Given the size of the feature, I don't suppose it's possible to break this up into smaller chunks?\n. Hi Borovsky,\nsorry about the delay.  I'm taking over PM duties for BOSH.  This is a useful feature, but the size of this pull request means it's a non-trivial job to review, merge & test it.  I can't say when we're going to be able to get to it.  Given the size of the feature, I don't suppose it's possible to break this up into smaller chunks?\n. I'm asking the team (@hiremaga) to review the provider refactoring, so hold off on making the requested changes till I get the ok.  If the refactoring looks good, I'd still like to see the AwsProvider class explicitly tested instead of indirectly tested through the other components. \n. I'm asking the team (@hiremaga) to review the provider refactoring, so hold off on making the requested changes till I get the ok.  If the refactoring looks good, I'd still like to see the AwsProvider class explicitly tested instead of indirectly tested through the other components. \n. So it feels to me like this is really a request for a tool outside of bosh diff that produces manifests.  We're building something similar for our on-premise installer.  Putting that logic in the properties system feels like the wrong place, imho.\n. So it feels to me like this is really a request for a tool outside of bosh diff that produces manifests.  We're building something similar for our on-premise installer.  Putting that logic in the properties system feels like the wrong place, imho.\n. Haven't seen a comment in a couple months.  Closing.  Ping me here, if it's still an issue, and I'll reopen.\n. We're not currently prioritizing non-vpc installs on AWS.  We've been informed that it's being phased out, and vpcs are \"the future\".\n. We've started discussing with the bosh-users and bosh-dev community lists the idea of agressively publishing gems/stemcells/releases.  This is in the backlog.\n. ok, closing this issue.  Please reopen it if you see this again.\n. Hi guys,\nThis looks good, but we'd like one change before merging:  bosh stemcell add --skip-if-exists doesn't seem to work with light and remote stemcells.  -- @aramprice \n. Any update on this?\n. Hi @mmatuska, can you add a unit test to prove this out?\n. See response in #240 \n. Yeah, that usage is clearly incorrect.  I agree, it'd be nice to be able to update the micro without re-specifying the stemcell.\n. What we really need to do is break these out into different sets of migrations: microbosh, bosh, and cloudfoundry.  I'd rather see this done as a holistic solution than a quick workaround.\n. Sounds like a good addition.  I'd love to see it as well.\n. @drnic is this only on bosh micro deploy --update?  I just finished recovering a terminated micro, and bosh micro deploy ami-xxx successfully spun up a new one with the same private IP address.\n. Interesting.  @cppforlife - can you comment?\n. I think that's basically the right solution, but for now we just have to roll back this change.  +@cppforlife \n. The issue is that this codepath is currently entirely untested, and shared between all of the various stemcells.  As small as this change is, it's actually fairly dangerous to the other stemcells.  We'll need to hold off on this until we have a good set of tests surrounding the process.\n. Yeah, just comment here later and I'll reopen it.\n. Hi @TieWei \nI talked with the developers about this, and it looks like it's actually a big chunk of work to do if we're going to do it right.  I'm gonna close this PR in order to keep it off the community team's radar.  Feel free to comment on it for feedback, etc.\nI'm sorry for having to say no to the work you put into this, but we need to make sure we keep the bosh codebase moving in a good direction, architecturally.\n. What?  Why?  @oppegard @ajackson \n. Why closed?\n. I think we need more discussion about how cck should work in general.  A PR to address the fact that the order is incorrect would be appreciated.\n. Prioritized!\n. @drnic is right, we don't need him to add rackspace support to accept this PR.  We're confusing rackspace cloudfiles and swift.  I see no blockers - merge at will.\n. Yeah, I think this would have to be configurable.  High timeouts can slow things down for people on more reliable networks.  Also, is this just for the dav blobstore client?\n. Sounds like a great idea.  We'd be happy to take a PR for this.\n. > There is no way to name a VM.\nDo you mean in bosh or in openstack?\n. Hi @rd7869 - This is a known architectural issue, and one that we plan on addressing.  It's a big enough effort that I'm not sure it's helpful to leave it around as an open issue.\n. Hi @rd7869 - This is a known architectural issue, and one that we plan on addressing.  It's a big enough effort that I'm not sure it's helpful to leave it around as an open issue.\n. Hi @rd7869 - This is a known architectural issue, and one that we plan on addressing.  It's a big enough effort that I'm not sure it's helpful to leave it around as an open issue.\n. The issue with supporting the aws::sdk's aws_options directly it that it would tie us to that particular library.  This would become a major issue for moving to golang, or for changing the AWS library we use.\n. Also, are the changes proposed by this PR covered by #508?\n. @grenzr That's a good argument.  Pinging @goehmen (the new bosh pm) to get his thoughts.\n. Hey Nic,\nSpiff was an off-hours temporary solution to the 'how do i configure this release' problem.  A supported solution has been put out commercially in pivotal cf operations manager, and we'd like to OSS it into BOSH.  A design doc is forthcoming.\n. It's @goehmen's call, but I don't believe we can.\n. Hi @molteanu - We're not adding support for any new infrastructures directly to the bosh codebase at this time.  Instead, we're focused on enabling external CPIs and Stemcells.\n. I have an issue with ignoring invalid switches - it gives no feedback when the switch was fat-fingered.  If we allow arbitrary hashes, we'd need to ensure the library that eventually uses the hash will throw an error if the has is incorrect.  This complicates the dual-library story, but I suppose the hashes could be scoped by underlying library (allowing both fog_s3_options and bosh_agent_s3_options)\n. Need to remove these comments\n. Let's pull the Guardfile into a completely separate pull request (or just remove it).\n. ruby\nwhen false then false\nelse true\nThat's...  a joke, right?  ...right?\n. @matthewmcnew / @jmtuley - Any reason not to modify Bosh::Monitor::Events::Alerts to support a deployment name?\n. ",
    "james-masson": "For my needs, just simple script triggers on the box - similar to the job scripts.\nNATS/director integration would be nice, but a bit overkill\n. Drain scripts were what was originally suggested in the discussion thread I linked to, but unless something has changed, drain happens on every job shutdown, not on a decommission.\nI envision this functionality being used when you have something you need to do once, at a particular point during commissioning/decommissioning.\nFor me, this would be triggering a few hundred GB of data migration away from the persistent data disk - and that's not something I'd want done on every job shutdown.\n. this seems to work for me now on 1.5.0-pre-3\nclosing\n. We're running the latest stemcell releases for vsphere -\nbosh-stemcell-vsphere-992.tgz and micro-bosh-stemcell-vsphere-912.tgz\n- is this bug supposed to be fixed in these versions?\nWe're running into the same issue when changing DNS properties for an existing deployment\nNetworks\ndefault\nsubnets\n10.20.219.0/24\nchanged dns: \n- 10.20.216.1\n- 10.20.216.2\n- 10.20.219.2\nError 450002: Timed out sending `prepare_network_change' to 25d37b9c-ff51-4f40-b0a6-e80d044367d5 after 45 seconds\n. OK, let me rephrase a bit...\nWe're running the latest known good stemcells for vsphere ( and at the time they were very hit and miss ) when we started this project a few weeks ago.\nbosh-stemcell/vsphere/bosh-stemcell-992-vsphere-esxi-ubuntu.tgz\n\n2013-09-13T08:00:20.000Z\n\"80a8c3cffb4d3c55a0e13fbd5f96de12\"\n414323120\nSTANDARD\nmicro-bosh-stemcell/vsphere/micro-bosh-stemcell-912-vsphere-esxi-ubuntu.tgz\n\n2013-08-20T04:17:57.000Z\n\"90ab297578089bb66a357753c9b75ed9\"\n415541332\nSTANDARD\nThe fix listed here was committed 3 months ago.\nThe MD5 sum of bosh_agent/lib/bosh_agent/handler.rb on git master is a44f1836c8b898dc4cdceaa739541776 - and that matches the md5sum of that file on my micro-bosh instance. \nSo, yes, it appears I have the fix from PR320 - but I still have the same problem.\n. In case the behaviour here is new and different...\n\"bosh deploy\" successfully updates the DNS configuration, but fails to deploy any further release updates, and does not update the Director with the new applied DNS config. This broken state then seems to persist - ie. no further release upgrades to this system are possible.\nTo fix this, I've gone into the Postgres DBs \"releases\" table, and manually made the DNS config agree with the settings.json on the agent. I can then deploy new releases again.\n. +1 for this bug - we've just hit it with Cisco on accessing EU blobstores from the US\n. I'm not sure where the fault lies. To replicate, create a blobstore outside the US (eg. EU-west-1) , try to use it with a BOSH release while located inside the US.\nLikely it will work as long as you have the private.yml with full creds ( and perhaps the s3 host parameter pointing at the eu-west S3 api target).\nBut this doesn't work for anonymous access, and results in the 301 (moved) error, mentioned in Chris' original bug report. Likely because S3 is trying to pass you between regions.\n. Well, this is the problem....\nhttps://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/simple_blobstore_client.rb#L37-46\nif response.status != 200\n  explode\n'simple' blobstore client seems an apt name :-)\n. It's possible that someone has already considered this use case.\nThe code defaults to the US blobstore\nENDPOINT = 'https://s3.amazonaws.com'\nBut this seems to take an override parameter 'host' in the config\ns3_endpoint: @options.fetch(:host, URI.parse(S3BlobstoreClient::ENDPOINT).host),\nbut adding this host parameter, like so...\n```\nfinal_name: redis\nmin_cli_version: 0.1\nblobstore:\n  provider: s3\n  options:\n    host: https://s3-ap-northeast-1.amazonaws.com\n    bucket_name: hole-in-my-bucket\n```\n... doesn't work - with or without the http(s):// - either in the private.yml, or final.yml\n. According to the docs, neither 'host' nor 'endpoint' are valid for the S3 provider, but 'endpoint' is valid for the 'basic' provider:\nhttps://github.com/cloudfoundry/bosh/blob/master/blobstore_client/README.md\nHowever, the following suggests we should be able to override the default host from somewhere, passed in as part of the options hash:\nhttps://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb#L27-39\n```\n      def initialize(options)\n        super(options)\n        @bucket_name    = @options[:bucket_name]\n        @encryption_key = @options[:encryption_key]\n    aws_options = {\n      access_key_id: @options[:access_key_id],\n      secret_access_key: @options[:secret_access_key],\n      use_ssl: @options.fetch(:use_ssl, true),\n      s3_port: @options.fetch(:port, 443),\n      s3_endpoint: @options.fetch(:host, URI.parse(S3BlobstoreClient::ENDPOINT).host),\n      s3_force_path_style: true,\n    }\n\n```\nWe tried the following for a Tokyo region blobstore and much success!\n```\n\u00b1  |master \u2717| \u2192 cat config/final.yml\n\nfinal_name: redis\nmin_cli_version: 0.1\nblobstore:\n  provider: s3\n  options:\n    endpoint: https://s3-ap-northeast-1.amazonaws.com\n    bucket_name: bucketname\n|ruby-1.9.3-p392| tbolt-02 in ~/workspace/C3CI-private/C3CI/redis-release\n\u00b1  |master \u2717| \u2192 bosh sync blobs\nSyncing blobs...\nTotal: 1, 1.0M\nNo blobs to upload\n```\nhttps:// is needed. (will bomb out in httpclient gem without).\nThis still isn't going to handle a 301 if we get thrown one, but should help others work around the problem in the meantime.\n@james-masson @matjohn2\n. Ran into this again today...\nyou'll get something similar to this:\njmasson@x220 ~/bosh/cloudcredo-product $ bosh target https://1.2.3.4:443\nTarget set to `Bosh Lite Director'\njmasson@x220 ~/bosh/cloudcredo-product $ bosh login\nYour username: director\nEnter password: ***************\n[WARNING] cannot access director, trying 4 more times...\n^C\nExiting...\nat this point you edit ~/.bosh_config to...\n```\ntarget: https://1.2.3.4:443   <--- :443 added here\ntarget_name: Bosh Lite Director\ntarget_version: 1.2411.0 (f71e2276)\n```\nand now bosh login works:\njmasson@x220 ~/bosh/cloudcredo-product $ bosh login\nYour username: director\nEnter password: ************\nLogged in as `director'\n. Hmmm - this sounds very like my conversation with Martin and Ferdy about a year ago.\nhttps://groups.google.com/a/cloudfoundry.org/forum/#!searchin/bosh-users/inside$20vpc/bosh-users/_wgUVXpzSeA/qKDUUIUIs40J\nIf so,  +1 for this feature - It's not a good assumption that your BOSH Micro is the sole guardian and gateway to your VPC, and all client interactions come from outside.\n. From my reading, aren't these stories a little different?\nI'm asking to be able to deploy microbosh to EC2 CPI without requiring any form of Public IP - and I thought that's what @drnic was after too.\nIsn't https://github.com/cloudfoundry/bosh/pull/575 about being able to use 'on-demand' AWS Public IPs through AssociatePublicIpAddress - an alternative to EIP - rather than the CPI allowing you to completely miss out all the Public IP configuration.\nOr are you suggesting that https://github.com/cloudfoundry/bosh/pull/575 also fixes what I was requesting?\n. I'm sure your PR works just fine - I'm just not sure it fixes what was requested in this issue.\n@adamstegman Can you elaborate more on why this issue has been closed?\n. In theory, it's an easy change... I was hoping to provide you with a pull request\nUpdate version listed here - https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/bosh_monit/apply.sh\nProvide new version in assets dir.\nBuild stemcell\nFind new monit used.\nIn reality - I can't get that to work. The image that is created with stemcell:build[aws,ubuntu,lucid,ruby,bosh-os-images,bosh-ubuntu-lucid-os-image.tgz] - always seems to have the original 5.2.4 monit version\n. Figured it out...\nTwo step process.\nrake stemcell:build_os_image\nrake stemcell:build_with_local_os_image\nWorks for us on some sample BOSH releases.\nPull request - https://github.com/cloudfoundry/bosh/pull/590\n. 2768 - we tried a few versions around the same time, and had the same result.\n. ",
    "msackman": "Agreed: post-install and pre-removal scripts for jobs-deployed-on-vms would be very useful for me too. Currently I'm having to do things every time a job starts rather than once once it's been installed. The obvious variants of \"once this job has been started for the first time\" and \"before this job is stopped for the last time\" are very useful too.\n. FWIW, I've just had this again, exactly the same, but on a different package. So it can't be that rare... I would probably guess that the eof is maybe an attempt to read of a network socket that's already closed - something like that.\n. FWIW, I've just had this again, exactly the same, but on a different package. So it can't be that rare... I would probably guess that the eof is maybe an attempt to read of a network socket that's already closed - something like that.\n. The use case is you're developing a release/service, and before you get as far as writing your monit and general job scripts, you just want to deploy all the software that should be compiled, and check it ends up in the right place. Thus you create a job that just depends on all your packages, and you have a deployment manifest that has a suitable job entry. But you've not gone through the hell of writing your templates etc at this point.\n. Also, given the epic pain of the 10-minute timeout whenever a job update/deployment goes wrong, it's much nicer to develop such a phantom job which just deploys all the software and has no monit jobs and to then develop on such a machine to initially get things up and running.\n. I don't follow that Martin - bosh does compile in parallel... what am I missing?\n. I don't follow that Martin - bosh does compile in parallel... what am I missing?\n. Ahh, gotcha, that makes sense. Also, I was wondering (and am too lazy to read the code): does BOSH preallocate compilation jobs to machines, or does it have a general work queue and does work stealing (accomodating dependencies)? If the former, that essentially assumes that all compilations are going to take the same amount of time.\n. Ahh, gotcha, that makes sense. Also, I was wondering (and am too lazy to read the code): does BOSH preallocate compilation jobs to machines, or does it have a general work queue and does work stealing (accomodating dependencies)? If the former, that essentially assumes that all compilations are going to take the same amount of time.\n. Hi @gabis. Yes, I discovered you could do canaries 0. You then also need to do max-in-flight >= cluster size, which I hadn't realised when I posted this originally.\nYes, absolutely, I can get around it today. It's just unfortunate that when you, for example, introduce a new cluster, you have to check to see if it's bigger than the max-in-flight setting, and if so, update that too. Plus of course it's not impossible that you have some jobs for which you do want the canary, and some where you don't (or rather, you want the more complex grouped behaviour). Certainly not show-stoppers right now, but in the long term it'd be great to see these issues addressed.\n. Hi @gabis. Yes, I discovered you could do canaries 0. You then also need to do max-in-flight >= cluster size, which I hadn't realised when I posted this originally.\nYes, absolutely, I can get around it today. It's just unfortunate that when you, for example, introduce a new cluster, you have to check to see if it's bigger than the max-in-flight setting, and if so, update that too. Plus of course it's not impossible that you have some jobs for which you do want the canary, and some where you don't (or rather, you want the more complex grouped behaviour). Certainly not show-stoppers right now, but in the long term it'd be great to see these issues addressed.\n. To give a little more context, this happened after a space capacity node was converted into a job node. So in the resource pool I had size greater than needed for the number of jobs. A job then increased its instances. Odd things then seemed to occur, which actually involved the spare capacity being terminated and a new machine being spun up (all on AWS). This error then eventually occurred.\n. @frodenas Many thanks for the steps. I've applied these as requested. I'm afraid I actually managed to work around the problem earlier so I can't be sure if these steps have exactly solved this particular issue. I shall try and recreate the exact scenario later today.\n. ",
    "mark-rushakoff": "@tsaleh Is this a story you want prioritized?\n@mark-rushakoff / @d\n. This is in the BOSH backlog but it's a bit in the future for now. (It's #53196967 for anyone who has access to the BOSH tracker.)\n. We've added a bug to the BOSH icebox for this (it's #57940600 for those with access to that Tracker).\n. There's a story for this in the BOSH icebox; it hasn't been prioritized yet.  (It's story #53568023 if you have access to that Tracker.)\n. @tsaleh is there a feature to be extracted from this issue?\n@mark-rushakoff / @d\n. It sounds like this might be good to continue this discussion on the mailing list, because @msackman has a clear problem definition that sounds like it affects many other people as well.\n@mark-rushakoff / @d\n. @drnic is the alias a good enough solution to close this issue? \n@mark-rushakoff / @d\n. @tsaleh We couldn't find a story that Gabi said she made.  Is there one already or should there be a new story for this?\n@mark-rushakoff  / @d\n. Revisiting some issues that have been open for a while. @drnic can you explain the problem this pull request is solving? It's not entirely clear to us.  Thanks!\n@mark-rushakoff / @d\n. Looks like I pushed against an already red CI.\n. Can this issue be closed? It sounds like #400 has been merged in and fixes this issue.\n@mark-rushakoff / @d\n. @rkoster If the issue was with the hosting, can we close this issue? Or is this still a BOSH bug?\n@mark-rushakoff / @d\n. Okay, I've added a story to the BOSH icebox to properly handle exceptions in the compilation VM, and I've asked that they updated this issue when they fix the bug. (It's #57952778 for those with access to that Tracker.)\n. @tsaleh is this a feature you want prioritized?\n@mark-rushakoff / @d\n. So you want the task to stay attached if there is a REST API call exception like that?  We will create a feature in the BOSH icebox for that.\n@mark-rushakoff / @d\n. Closing this issue because the feature has been delivered already and has a story in the BOSH tracker.\n@mark-rushakoff / @d\n. @blueboxjesse I don't think there's any way through the Travis UI to force it to build a pull request.  Try amending the most recent commit and force-pushing maybe?\n. Thanks for the pull request! We've added a chore to the BOSH icebox for the BOSH team to review this.  (It's #57766092 for anyone who has access to that Tracker.)\n@mark-rushakoff / @d\n. @blueboxjesse Really sorry you're having trouble getting this PR to run with Travis.  Everything looks okay on your side and all our config seems okay, so I tweeted at TravisCI asking for assistance from their end.\n. @drogus No problem for the noise.  Thanks very much for looking into this!\n. Just noting that we still have a chore that has been prioritized in the BOSH backlog for their team to review this. (It's #57766092 for anyone who has access.)\n@mark-rushakoff / @d\n. @tsaleh Is this pull request aligned with the direction that Bosh is going?  Can we merge it?\n@mark-rushakoff / @d\n. Hi Jesse,\n  Yes. Because of the way we set up the project's single Gemfile, bundle exec bosh will run the local repository version of bosh_cli.\n@mark-rushakoff / @d\n. ",
    "adamstegman": "@goehmen I think post_install is already a thing; would the changes we are making to drain scripts cover the pre-removal use case?\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. This issue looks pretty stale, so we are going to close it. If you run into this issue again, please feel free to re-open with some task debug logs (bosh task 480 --debug) so we can troubleshoot.\nCF Community Pair (@adamstegman & @mbhave)\n. @goehmen @monkeyherder Is this something we still want to fix? The story is not in our public backlog anymore but is still open.\nCF Community Pair (@adamstegman & @mbhave)\n. @goehmen @monkeyherder Is this still something that needs to be fixed? The story seems to be gone.\nCF Community Pair (@adamstegman & @mbhave)\n. @msackman We are closing this issue because it looks like it would be better to discuss this on the mailing list. If you already started a discussion for this on the mailing list please provide a link for it here. \nThanks,\nCF Community Pair (@adamstegman & @mbhave) \n. @goehmen @monkeyherder Do we need a story for what @gabis mentioned above?\n. We have created an issue in Pivotal Tracker to manage this: https://www.pivotaltracker.com/story/show/70769956.\nCF Community Pair (@adamstegman & @mbhave)\n. Since this issue is stale and looks like it's related to #315 we will close it. Please track #315 for updates to this issue.\nCF Community Pair (@adamstegman & @mbhave)\n. @drnic This issue seems pretty stale. Are you still running into this? Can you give us some more details about how to get into this situation?\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @drnic We couldn't reproduce this by just deploying a job that failed to start (i.e. did not create a pidfile) and/or failed to stop (i.e. did not remove the pidfile). Can you give us any more information about what caused your job to fail during start/stop?\nCF Community Pair (@adamstegman & @mbhave)\n. @drnic This issue seems pretty old so we are going to close it. If we had some more information on how Monit got into that state we could try to reproduce it or come up with a better workaround.\nCF Community Pair (@adamstegman & @mbhave)\n. @TieWei Thanks for reporting this issue. Limiting your network to only the static IPs that your jobs will use is not supported with bosh right now. Bosh requires a dynamic IP for every static IP that you use.\nWe are going to close this issue because we don't plan on supporting a static-only network configuration anytime soon.\nCF Community Pair (@adamstegman & @mbhave)\n. Closing this since #421 was merged.\nCF Community Pair (@adamstegman & @mbhave)\n. This doesn't seem to be an issue anymore. Bosh no longer has this feature. It won't let you deploy if your uuid doesn't match. So we are closing this issue.\nCF Community Pair (@adamstegman & @mbhave)\n. We are closing this issue because its pull requests (namely #434) have been resolved.\nThanks for the contribution,\nCF Community Pair (@adamstegman & @mbhave)\n. This was fixed in #447.\n. @amhuber \n1. Yes, you are correct. Target the develop branch when you create your PR.\n2. That's a good point; maybe BATs are not a good fit. Since the deployment looks the same, a CPI lifecycle test may be better suited. The challenge would be configuring the OpenStack environment in a context so that you can run the lifecycle test and clean up afterward. Is that something you could do with Fog (or another client)?\n3. Thanks!\n4. If you want to refactor the whole test, that would be great! But we don't expect you to do that, just changing your own additions is fine.\n5. See #2.\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @amhuber I think we can accept this with vanilla doubles. I'll hold off on a full code review for now and leave it for when we can dedicate some time to it. Once you open your PR we can prioritize it in our backlog.\nThanks!\n. > Have reverted the inclusion of :connection_options => {:instrumentor => Bosh::OpenStackCloud::ExconLoggingInstrumentor} since connection_options can now be specified in the deployment manifest.\n@rkoster How did you specify it? The Excon::Connection constructor needs a Class, but via YAML you'd have to give it a String, which makes it blow up.\n. This looks like a good idea for us to have always on, but the logs should end up in the task CPI log file, not the director log file. This way they would be accessible via bosh task 41 --cpi.\nCan you alter the instrumentor to print the requests out to Config.cloud_options[\"properties\"][\"cpi_log\"]? This setting is created by Bosh::Director::JobRunner and points to the right file. Then you can put it back in the defaults for the OpenStack CPI so it is always used.\n. @rkoster Did you have a chance to look at my last comment?\nCF Community Pair (@adamstegman & @jtuchscherer)\n. Merged this into develop, a release will come soon.\n. @gberche-orange Sorry for being silent for so long. We just talked to the bosh team (@monkeyherder) about this. Seems like the biggest part of this change is contained in #535. Regarding the change to the microbosh CLI, given that you already found a work-around for this, do you still need this change in the bosh code? Otherwise, we would like to go ahead and close this PR.\nThanks,\nCF Community Pair (@adamstegman & @jtuchscherer)\n. We merged this into develop today and refactored the tests a bit (c2a0c08).\n. Thanks @shinji62\u2014since #628 implements this feature in a simpler way, we will go ahead and close this PR. Thanks for the contribution @gberche-orange, please let us know if you need anything else from this issue.\n@adamstegman and @xingzhou\n. @grenzr This is now in the bosh backlog to look at soon. Thanks for the contribution!\nCF Community Pair (@adamstegman & @jtuchscherer)\n. Merged in 52ff99a.\n. Replaced by #562.\n. @yuhuan00 This looks a bit stale, so we are going to close it based on @monkeyherder's comment about the age of the stemcell.\nIf this continues to be an issue please feel free to reopen this issue.\nThanks,\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @omarreiss Did you have a chance to look at @cppforlife 's comment? It would be great if you could add tests and also 'quote-ify' the other properties.\nThanks,\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @omarreiss Thanks for the update to the PR. It looks good to us and it is in the Bosh Tracker backlog, so it should be looked at soon by the Bosh team.\nCF Community Pair (@adamstegman & @jtuchscherer)\n. We looked at the failing Travis build and the issue seems to be something going wrong with the go unit test processes.\ngo build testmain: signal: killed\ngo build testmain: signal: killed\ngo build testmain: signal: killed\ngo build testmain: signal: killed\ngo build testmain: signal: killed\nWe aren't sure what would cause this, but it doesn't seem related to these changes.\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @omarreiss FYI, the bosh team works off of the develop branch so future PRs should be against that one.\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @omarreiss Sure, that would be very helpful. Thank you for taking care of that!\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @drnic We are closing this issue in favor of #574. \nThanks,\nCF Community Pair (@adamstegman @mbhave)\n. @akranga It was likely an assumption made at some point that your micro would be targetable from anywhere, rather than only from a jumpbox. We can't find any reason that it would require a publically-accessible IP.\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @james-masson I see the distinction, thank you for pointing it out. I'll reopen this issue so we can prioritize this.\n. @duglin Based on the comments from @goehmen and the response from the docs team via @chou we will close this issue with the CLI page as it is now.\nIf you have an idea for something more thorough, the bosh docs repo is very receptive to pull requests.\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @singerdmx when you pull the latest bosh source, that will also fix the stemcell building issue you're having. That repository no longer exists, so we vendor the s3cli.\n. Thanks @molteanu, this was merged in yesterday: e90baa0\n. @omarreiss Thanks for the new PR. We put the new story in the Bosh Backlog right where the previous one used to be.\nCF Community Pair (@adamstegman & @jtuchscherer)\n. @goehmen @monkeyherder Can you comment on this?\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @akranga We are closing this issue in favor of #574.\nThanks,\nCF Community Pair (@adamstegman @mbhave)\n. Looks like we didn't close this last time\u2014doing it now, but in favor of #575 since it replaces #574.\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @goehmen Would you like to take a look at this?\nThanks,\nCF Community Pair (@adamstegman @mbhave)\n. @akranga Looks like this PR should be recreated against the develop branch rather than master. That should eliminate the unrelated changes in the review.\nThanks,\nCF Community Pair (@adamstegman @mbhave)\n. @goehmen Should we move this into the icebox? (See #558 for more discussion)\nThanks,\nCF Community Pair (@adamstegman & @mbhave)\n. @goehmen Using the story for #573: https://www.pivotaltracker.com/story/show/70923242\nCF Community Pair (@adamstegman & @mbhave)\n. @akranga The BOSH team will discuss this and prioritize it.\n. @goehmen I have put this in the BOSH icebox.\n. @singerdmx Can you close this PR and reopen against the develop branch instead?\nThe test failures look unrelated to your change. We think they might be transient. When you reopen your PR they will run again and we can see what happens.\n. @gerhard BOSH is not compatible with Ruby 2.x yet. If you search for label:\"ruby-2.x\" in our tracker you can see the stories to make that happen. In the meantime, please use Ruby 1.9 when using BOSH.\n. @dengwa Thanks for making those changes. The story in Tracker hasn't been prioritized yet, and we'd like to add some acceptance tests around this code prior to bringing it in. Specifically, a test that goes from using a single manual network to multiple manual networks and back. You could add this test in this PR, or wait for us to prioritize and work on this story.\nIf you'd like to work on this test, see bat/spec/system/network_configuration_spec.rb for some examples. You'd have to add some configuration around a second network and another IP address, and use pending to guard this test to only run in OpenStack environments.\n@adamstegman && @krishicks\n. @drnic The director controllers have been refactored; you will need to rebase your commits onto develop before we can accept this pull request.\n@adamstegman && @krishicks\n. @shinji62 Thanks for taking care of that! One more quick fix and I think we can merge this in.\n@adamstegman and @xingzhou\n. Thanks for the contribution @shinji62. Merged in 03d2f50.\n. Thanks for the contribution @dkoper! Merged in 992c921.\n. @molteanu Should the CPI be updated as well to raise NotSupported and recreate the VM?\n. Thanks @liuhewei! Merged in af2a1d4.\n. Thanks for the contribution @sra! We merged this in 73af42c.\n. Forgot to add my integration test to this\u2014I'll add it when I get a chance\n. This is easily reproducible if you create or delete 15-20 instances at once.\n. The changes in the AWS SDK moved the client methods to nested subclasses that implement different versions of the API. If you make this a instance_double('AWS::RDS::Client::V20130909') the tests pass correctly.\nEDIT: did this in ea56fa9579\n. This might be better as an expectation of the desired behavior rather than internal state. expect(@cloud).to receive(:sleep).with(3)\n. expect(instance_manager.instance_params.keys).to include(:subnet)\n. defined above probably don't need to redefine in every test?\n. is this necessary for this pull request? Seems like a separate issue.\n. These versioning changes also look unrelated.\n. These tests should use the public interface\u2014that it falls back to the vmdk, or fails if there is none.\n. This method is now private; these tests should be eliminated or integrated into the settings load tests.\n. Seems like the latter part of this is just duplicating which. Why not just use which instead of this method? If the bin_path is necessary, just use which inside this method rather than copying it over.\n. Does calling f.write so many times cause a lot of I/O? I wonder if it would be safer to just generate 1 MB in memory and write it once.\n. This and the iso_file are paths created by one function and used by another. They seem like natural return values for those functions and natural arguments for the others to illustrate the dependency.\n. @goehmen Do we support vSphere 4?\n. It looks like the only difference is that which takes an array, which means this loop could just be which([bin]). Am I missing anything?\n. Exactly, though convert_vmdk_to_esx_type should probably return both vmdks it creates. Also, generate_vmdk_iso should return the file it creates.\n. Yeah, basically. I did overlook the return value when the binary isn't found, which makes this kind of unfortunate. But I like reusing the code in this case since the implementations should stay in sync.\n. You switched this method from public to private. I'm suggesting that you test via the public interface rather than doing private-method-specific tests.\n. Using if_p could eliminate the extra conditional:\n<% if_p('director.env.http_proxy') do |proxy| %>\nexport HTTP_PROXY=<%= proxy %>\nexport http_proxy=<%= proxy %>\n<% end %>\n. ",
    "cppforlife": "@james-masson Drain scripts now have a way to determine if persistent disk size is going to change (0 size means that either persistent disk was removed or instance is going away). You can get that value by parsing BOSH_JOB_STATE & BOSH_JOB_NEXT_STATE environment variables (https://github.com/cloudfoundry/bosh-agent/blob/master/agent/drain/concrete_script.go) if you are using new BOSH Agent (written in go).\n. Proposed workflow will be confusing if you were building dev releases and then decided to do git pull. If git pull brought new final releases which one should be uploaded: previously built dev release or newer final release?\n. Closing. bosh ssh into compilation VMs was implemented earlier in the year.\n. I've made a note about this feature being requested. Closing.\n. @drnic would be ok with --gateway flag that would use previously saved settings?\n. --gateway or (or some shorter version) would be an explicit way to say use last gateway saved for that target e.g.\n```\n$ bosh ssh dea/0\nFailed to SSH: Failed to connect to 10.10.0.1\n$ bosh ssh dea/0 --gateway_host dir.blah.com --gateway_user vcap --identity_file ~/.ssh/fancy_key\nSaving gateway settings\nCreating SSH user\nConnected to 10.10.0.1\n...\nExiting SSH session\n$ bosh ssh dea/0 -g # or bosh ssh dea/0 --gateway\nUsing saved gateway settings: --gateway_host dir.blah.com --gateway_user vcap --identity_file ~/.ssh/fancy_key\nCreating SSH user\nConnected to 10.10.0.1\n...\nExiting SSH session\n$ bosh ssh dea/0\nFailed to SSH: Failed to connect to 10.10.0.1\nNote: Add -g to use saved gateway settings (--gateway_host dir.blah.com --gateway_user vcap --identity_file ~/.ssh/fancy_key)\n$ bosh scp --download /var/vcap/sys/logs -g\nUsing saved gateway settings: --gateway_host dir.blah.com --gateway_user vcap --identity_file ~/.ssh/fancy_key\nDownloading /var/vcap/sys/logs\n...\n```\nImho that's pretty usable (especially with a new github font).\n. afaik min_cli_version is not saved anymore. Closing.\n. I will be creating multiple stories for better collocation - namespacing and more. Mailing list discussion coming up. Closing github issue.\n. Closing this for now. User management in BOSH needs to be rethought. I've captured this concern in the details of user management epic in pivotal tracker.\n. Imho having director uuid in the manifest breaks abstraction layer - manifests have nothing to do with which director should be targeted, they describe a deployment. This is purely a UI problem and should be solved via bosh_cli with a descriptive prompt when bosh deploy is invoked if more safety is desired.\nGetting back to our change - ignore option is something that user has to opt in explicitly (by typing in ignore) so trying to protect the user from themselves at that point is like asking 'are you sure' twice.\n. BOSH Agent (written in go; all latest stemcells have it) would have avoided 'dropping nats client' problem, since yagnats (go nats library we are using) will automatically reconnect.\n. Closing. We are slowly converging to having bosh deploy --fix having ability to recover deployments in a irregular state. Ultimately we may even turn bosh cck into a command that just reports problems instead of allowing to interactively fix them.\n. Reopen if you still see 'REST API call exception: execution expired'. It is now being properly retried afaik.\n. Agreed with @drnic. Showing more informative error from the CPI layer would be helpful.\n. vm restart is typically not bosh involved so thats up to openstack. vm recreation  gurantees that root and ephemeral disks are new. packages are reinstalled everytime vm is created.\nif you need to store some data i would recommend placing it on /var/vcap/store which is for persistent data.\nSent from my iPhone\n\nOn May 26, 2017, at 6:41 AM, shashankmjain notifications@github.com wrote:\nDoes the boot from volume survive restart, If some data exists on boot volume(say some packages), will Bosh boot from the same volume in case of say VM restart or recreation?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. We've set of stories that is not yet prioritized but would allow usage of clustered nats. Closing for now. Reopen if needed.\n. httpclient was bumped to 2.4.0. Reopen if issue is not fixed.\n. bosh cli now supports on Ruby 2x. Reopen if this issue occurs again.\n. Below is a current response for /deployments:\n\n[{\n  \"name\":\"cf-warden\",\n  \"releases\":[{\n    \"name\":\"cf\",\"version\":\"154.11-dev\"\n  }],\n  \"stemcells\":[{\n    \"name\":\"bosh-warden-boshlite-ubuntu\",\n    \"version\":\"24\"\n  }]\n}]\nEach deployment has a name as its attributes and 2 relations.\nAnd here is current (before pull request) response from /stemcells:\n[{\n  \"name\":\"bosh-warden-boshlite-ubuntu\",\n  \"version\":\"24\",\n  \"cid\":\"stemcell-68933ed2-1149-48f7-b83a-df4882ef75d7\",\n  \"deployments\":[\"#<Bosh::Director::Models::Deployment:0x00000004d64940>\"]\n}]\nCan we reconsider just returning names for deployments key? I think it would be much better if we return collection of deployment objects like so:\n[{\n  \"name\":\"bosh-warden-boshlite-ubuntu\",\n  \"version\":\"24\",\n  \"cid\":\"stemcell-68933ed2-1149-48f7-b83a-df4882ef75d7\",\n  \"deployments\":[\n     {\"name\":\"cf-warden\"},\n     {\"name\":\"cf-warden-other-deployment\"},\n  ]\n}]\nThis makes response more consistent with all other endpoints in that we return direct attributes from the object and exclude nested relations.\n. @drnic bump\n. got resolved in 931797b. closing\n. We've a set of features coming up about vm/job states. I'll be sending more detailed info to the bosh-dev mailing list soon. Closing this issue for now.\n. @heei3k this error usually occurs when bosh_cli_plugin_micro fails to upload stemcell to the datastore disk_path. Double check that specified disk_path is present in the datastore(s). Make sure that disk_path specified is a full path.\nCould you also include sanitized bosh_micro_deploy.log?\nRegarding 172.16.71.133, it should not be available at this point (step 1) because failure above is happening before VM creation (step 2). Here are major steps that happen during new MicroBOSH deploy:\n1) Stemcell (VM template) is uploaded to the datastore(s)\n2) MicroBOSH VM is created based on the uploaded template\n3) Persistent disk is created and attached to the MicroBOSH VM\n4) MicroBOSH Director is available\nBTW adding ``` before and after template, manifest, log contents results in a more readable rendering\n. closing issue. story prioritized in the BOSH backlog.\n. Sounds good, here it is: https://www.pivotaltracker.com/story/show/68414164\n. Hey @gberche-orange, \nLooks good so far. One minor thing: use stub_const('ENV', {...}) instead of adding with_env helper.\nRegarding setting of ENV variables, we think it should happen in the launchers for each component with appropriate spec properties added (director.http_proxy and director.https_proxy).\n- @monkeyherder , @cppforlife\n. no activity - closing. @xoebus update?\n. @drnic did you figure it out?\n. Change looks good. Please add appropriate tests to https://github.com/cloudfoundry/bosh/blob/master/release/spec/director.yml.erb.erb_spec.rb. \nSince you are modifying openstack section, it's worth quoting other fields under the same section: auth_url, username, api_key, tenant, region, endpoint_type, default_key_name, default_security_groups.\nThanks,\n- @cppforlife, @maximilien\n. Closing this issue. We have a story that's almost at the top of the backlog (will be done within a week) to fix this.\n. Just to clarify what you are asking: you want micro to be targeted by private ip address (because you do not want to give your micro publicly accessible ip)? I'm assuming you have a jumpbox inside vpc that you are using to deploying a micro.\nThis sounds reasonable, @goehmen can prioritize this.\nThanks,\n- @cppforlife\n. Nothing comes to mind. Did you try modifying bosh and see if powerdns accepts it?\n- @cppforlife\n. Pull latest bosh source, that repository got moved to pivotal-golang.\n. CLI now supports ruby 1.9.3, 2.0, and 2.1. Pulling in a separate gem for this seems a bit too heavy.\n. I wonder if a better approach would be to switch to something like this: https://github.com/cppforlife/bosh-warden-cpi-release/blob/master/jobs/cpi/templates/cpi.json.erb\nSince we are working on external CPIs all of the configuration for each CPI will actually not be included here.\n. I think it would make more sense to explicitly specify a separate vip network that is configured to give out public addresses.\nSomething like this:\n```\nnetworks:\n- name: net-with-public-ips\n  type: vip\n  cloud_properties:\n    associate_public_ip_address: true\njobs:\n- name: my-job\n  networks:\n  - name: net-with-public-ips\n``\n. Support for --name got introduced inc4d047a. Next bosh release should have it.\n. @peterellisjones @gerhard sorry to keep you guys waiting for so long. are you guys still seeing this problem in Cloud Controller?\n. I don't think we want to encourage people to use more monit features. In future we will move away from monit. That will allow us to cross platform (windows), more modular and hopefully less people would have to deal with monit annoyances.\n. Merged inade4decandb8ede78. merged to develop (610827d)\n. Issue captured in Tracker in 'openstack cpi improvements' epic. Closing here.\n. no activity - closing.\n. @tinateng please addblobstore.max_upload_sizeproperty torelease/jobs/blobstore/spec` file with a default.\n. Hey @johnmcteague,\nRoot partitions are preset for our stemcells. For OpenStack it's about 10G. Since you configured your OpenStack flavor without ephemeral disk, BOSH Agent does not have any disk to mount at /var/vcap/data to a separate disk so whatever BOSH Director places into its tmp directory (/var/vcap/data/tmp/director) ends up on the root partition.\nWe have a story coming up to make BOSH Agent automatically partition root disk if there is extra space available and mount it at /var/vcap/data.\n. No activity - closing.\n. Tracked in https://www.pivotaltracker.com/story/show/74744712. Waiting for our CI to go through for this change...\n. we'll squash these two commits and merge this in once our internal builds go through. thanks!\n. good point. will file a story.\n. BOSH supports different network types. You can specify network type in networks block e.g.\nnetworks:\n- name: my_fancy_net\n  type: dynamic\nSpecifying network type as dynamic tells BOSH to not manage IP addresses and delegate it to the IaaS, which is exactly what you want in your case.\nReopen if dynamic networks do not solve your problem.\n. Does plain ssh work?\n. Please reopen if issue still exists.\n. I'm not sure why it works with syslog_aggregator properties being empty but it looks like your networks property is not set. You should add something like this to your properties:\nnetworks:\n  apps: <name of your network from networks section>\n(btw cloudfoundry/cf-release might be a better place to ask about cf-release failures)\n. Can you give us more details? It does not look like bosh requires zc.\n. no activity - closing.\n. Cool. We had a similar story somewhere in the icebox for listing errands. \nI don't think that manifest should be the source of data for listing errands. Until bosh deploy succeeds errands are not available or are not updated with latest properties. So I think it would make sense to add GET /deployments/:deployment_name/errands to ErrandsController and have cli fetch that information.\nLater on it could be extended to show which tasks were associated with errands and if they succeeded or not.\n. I was not planning to schedule that story anytime soon, so i'm reluctant to accept this PR as this. Switching to API approach is fairly small change - controller action would just have to read errands from the saved manifest in the db.\nIn addition to the API call change, could you also remove 'VMs' column from the table. Errands do not currently support multiple vms which makes that column slightly confusing. It could be added once we figure out what should happen when errand is places on multiple instances (e.g. run bin/run in parallel on multiple vms).\n. Thanks for adding Director API.\nIn addition to code comments made above, I don't think it's a good idea to run an errand automatically if there is only one errand. Most of the time people run commands to find arguments they take so it would be confusing (and possibly dangerous) to run an errand by just typing in bosh run errand. We will be removing similar functionality from bosh ssh.\n. @drnic We have merged bosh errands feature to master. Ok to close this PR?\n. Closing this PR. We have stories prioritized about disk pools coming up this week.\n. @jmprice Thanks for noticing that. I've prioritized story to get rid of origin & tail files during cleanup stage of the stemcell builder.\n. Bosh version 100 (2690) should include fix for this. Thanks for reporting.\n. Please add/modify tests.\n. Hey @viperf,\nWhen bosh micro deploy <stemcell> exits it usually leaves a bosh-deployments.yml in the same directory. That file has a reference to VM id and persistent disk id. Next time you run bosh micro deploy <stemcell> --update CLI will detach disk, delete vm, create new vm, reattach old disk with all information on it.\nReopen if you have more questions about micro deploy.\n. You can now use dynamically sized resource pools (no size key) which would remove extra vms. We postponed solving a problem of running multiple errands at the same time but it's planned.\n. Hey @drnic,\nI cannot reproduce the problem. I've tried both bosh run errand errand --download-logs and bosh run errand errand --download-logs --logs-dir .\n\u00b1 dk |master \u2717| \u2192 tar tvf /Users/pivotal/workspace/errand.0.2014-08-25-10-48-52.tgz\ndrwxr-xr-x  0 root   root        0 Aug 25 10:48 ./\ndrwxr-xr-x  0 root   root        0 Aug 25 10:48 ./dummy_errand/\n-rw-r--r--  0 root   root        0 Aug 25 10:48 ./dummy_errand/lol.stderr.log\n-rw-r--r--  0 root   root       33 Aug 25 10:48 ./lol.log\nIt appears that somehow compilation logs got into your archive. When running an errand BOSH Director sends fetch_logs command to the agent so that it captures everything inside /var/vcap/sys/log directory. Were you using reuse_compilation_vms option? May be somehow VM got reused for running a job\"\nRegarding errand output missing: did you redirect your errand stdout/stderr to a log file in /var/vcap/sys/log? Currently you have to explicitly redirect it to a file so that it gets saved into a log bundle instead of printed out under [stdout/stderr] banner.\n. @drnic bump\n. If you did gem update in your bosh-lite to get new director this error is expected since we recently changed nginx configs to use upload module.\n. 'Cannot stop job: Service Unavailable' error is propagated from monit. Monit takes some time to reset back to queryable state after monit commands are issued. If you run bosh deploy shortly after that error it should work. We have a story prioritized to take next week to address related issue.\nI would also recommend updating to go agent stemcells since they are now default supported stemcells. You can get them from http://bosh-artifacts.cfapps.io\nTo debug your second problem - core/2 is failing - I would need to see output from bosh task X --debug for that task.\n. no activity. closing.\n. We have a story in tracker but it's not coming up any time soon. PR would be nice =)\n. sounds like we want to introduce security_group_ids cloud property for resource pools in openstack cpi. currently network_configurator.rb [1] and cloud.rb [2] use security_groups setting. would you be interested in filing a PR for this?\n[1] https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/network_configurator.rb\n[2] https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb\n. You can look for vm_cid in bosh-deployments.yml that was generated by that command. That's the VM it created in OpenStack. Does it exist? It seems that it failed to find a server in your openstack environment.\nIf you are not upgrading an existing microbosh, try deleting bosh-deployments.yml and running deploy command again.\n. No activity closing. Please reopen if still see this issue.\n. Hmm we are running integration test that runs this command in CI and it passes. I'm going to look into this more.\n. Interesting. DRS lock was recently added. \nI believe you can delete 'drs_lock' attrubute to unblock your issue. This of course does not tell us why it was not deleted automatically when it was cleaning up. \nCan you provide sanitized debug logs for first bosh depoy --recreate task (bosh task X --debug)?\n. Recently vsphere cpi introduced new feature to support drs rules to specify vm anti-affinity. To add/remove drs rules in a atomic way we have to acquire a lock. This seems to have introduced flakiness to your deploys. How many vms do you guys deploy?\n. We've updated vSphere CPI (bosh release 106; 2719 stemcell build) to avoid going through that code path in #delete_vm cpi method.\nPlease reopen if still having this problem.\n. I'll run bootstrap manually against a new account tomorrow.\n. @andyp1per As @techdragon mentioned bosh aws create works ok in the case if everything is empty in the account. \nbosh aws create tool was made originally to create dev envs when aws support was introduced to bosh. i'm looking into alternatives for creating/maintaining surrounding env for bosh deploys - e.g. terraform fits here perfectly. \ni'm also planning to update docs for setting up bosh from a default aws account without using any tools.\n. @DanielCloudCredo out of curiosity, in which environments are you planning to use this option?\n. New CLI (https://github.com/cloudfoundry/bosh-cli) does not look at the director UUID.\n. Tracked in https://www.pivotaltracker.com/story/show/79503492\n. @abylaw means we are waiting for our CI to become green before pulling in this change. Nothing to do on your part.\n. It seems that registry (bosh micro component needed for bootstrapping) is failing to start up. It is starting on port 25888. Do you have something else starting on that port? There is also a log file that should be created in the same directory. What's in there?\n. latest stacktrace shows that it's trying to create_vm (it has went further than registry set up). it appears that your vm cannot access aws apis. are you able to curl google.com?\n. could you include bosh/bosh.yml (make sure to remove sensitive details)?\n. seems like ssh vcap@54.210.142.162 does not work. make sure you can do so manually. it also appears you are using veyr old stemcells. please use bosh public stemcells to find latest light-bosh stemcell for aws.\n. ami that you are trying to use (ami-979dc6fe) is very old. you should use one of the latest stemcells published here: http://bosh-artifacts.cfapps.io/ or by the CLI.\n. Seems like your machine cannot access 54.210.142.162:6868. Is that IP pingable? If you ssh into that machine do you see anything running on port 6868 (lsof -i :6868)?\n. To make this change CPI independent could you change this to support following definition for disk pool (w/ cloud_properties):\nresources:\n  persistent_disk_pool: \n    disk_size: 20_480\n    cloud_properties: \n      type: gp2\n  cloud_properties:\n    instance_type: flavor\n    availability_zone: az1\nFollowing config should still be supported:\nresources:\n  persistent_disk: 20_480\n  cloud_properties:\n    instance_type: flavor\n    availability_zone: az1\nAlso please add unit tests for changed code?\n. This is available in 2754.\n. btw New micro rewrite uses regular bosh deployment manifest format. This feature will be implemented via disk_pool at some point.\n. Looking into that. Thanks for checking it out.\n. @rkoster if you want to give it a shot, we have added config_drive option to openstack cpi. i have not yet accepted those stories (my env is not in a good shape for non-cdrom config drive) but our engineers have tried it few days ago. we started supporting config_drive: cdrom and config_drive: disk options. configuration has to match how the OpenStack is configured, otherwise you will run into problems with persistent disks.\n. Travis seems to kill our builds every now and then, probably because of resource limits. No worries, we'll run the tests.\n. @amhuber do you know if the volumes api calls are compatible with: Folsom, Grizzly and Havana?\n. Here is the story link: https://www.pivotaltracker.com/story/show/80231858\n. top of the queue though i don't have an exact date. \nif you are using microbosh + multi-vm bosh set up then you can create a dev BOSH release and update multi-vm BOSH to use it that release until we officially pull this in. let me know if you need help with that.\n. hey @amhuber, we have merged everything into master and shipped a release. did you try it out yet?\n. Looks reasonable. We'll pull this in tomorrow.\n. Introduced a BOSH_MICRO_ENABLED env variable instead.\n. Changes merged into develop and as 2690.5.\n. Hey @HelloLin, which bosh version are you using? I think you have a very old version of the BOSH director. I would recommend upgrading to the latest which supports templates: property on jobs.\nPlease reopen if issue is not resolved.\n. Thanks for catching that. We'll pull this in tomorrow.\n. Are you referring to the following?\n```\n~/workspace/dummy-boshrelease2 $ rm -rf packages/\n~/workspace/dummy-boshrelease2 $ bosh create release --force\nSorry, your current directory doesn't look like release directory\n```\nIf so it appears that you just have to have that directory present but it does not have to contain packages. I think we can definitely drop that requirement.\n. I was thinking more about this. I'm now leaning towards keeping packages/ directory (people can check it in with .gitkeep file inside) because having src, jobs and packages directories immediately indicates that this is a bosh release.\nMaking packages directive optional in job specs makes sense I think.\n. Closing - decided to require packages/ directory.\n.  bosh_cli_plugin_aws is deprecated. We will be removing source for it soon.\n. Closing - decided to require packages/ directory.\n. @nterry I don't think James was able to find your CLA. Could you submit one so we can pull in this PR? See links in the above comment to find PDFs.\n. nice!\n. build 2765 includes this.\n. We have explicitly avoided doing this to make sure the interface to the CPI is explicit and not necessarily tied down to aws gem. Is there some specific option you are missing?\n. We can definitely expose them if you guys are actively using these config values, but it would be nicer if they are explicitly called out as properties in the spec file just like access_key_id.\nRegarding http_wire_trace configuration it would be awesome if those logs would go to the cpi log for each Director task. OpenStack [1] and vSphere CPIs actually already redirect detailed IaaS requests to the CPI logs and could be accessed via bosh task X --cpi.\n[1] https://github.com/cloudfoundry/bosh/commit/55ef66d8fe4fdb334e37b54f9cf240815e0c5b71\n. Btw I think you have to modify https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/spec to make those properties configurable by the user.\n. @petardp You don't have to sign CLA if you work for SAP but to avoid the warning in the future please associate your github account with SAP organization. That's how we know if we have corporate CLA.\nPR looks good. We'll pull it in soon.\n. Included in the latest builds. Thanks for contributing.\n. Included in the latest builds.\n. We've pulled in this fix + https://github.com/cloudfoundry/bosh/commit/7ee54035a0a9815c72c3e49933c7a20449f413e3 but it failed building in our ci. Could you run stemcell builder locally.\n. Based on our discussion of new aws vms spinning up and spinning down with new public IPs, could we close this PR without merging?\n. Is this rebased on develop?\n. This error looks benign. Of course we will fix it but it's unlikely that it would cause task queueing behavior. Could you send us full sanitized director logs?\n. @johnmcteague any updates regarding the unknown partition?\n. Prioritized the story - https://www.pivotaltracker.com/story/show/83141758. Taking a look at it today.\n. @johnmcteague build 2780 included a change to bosh-agent (https://github.com/cloudfoundry/bosh-agent/commit/1272a8f) that we think fixes this problem. mind giving it a try?\n. We'll have to see what happens when the environment does not have the internet connection. \n. Merged into master.\n. Merged into master. Thanks for contributing.\n. Will merge. Thanks!\n. Merged into master. Thanks for contributing.\n. Wow. That's really interesting. Which stemcell are you using?\n. @james-masson this must be related to how we compile job templates and upload them into the microbosh. we'll take a closer look as we rewrite bosh-micro CLI.\n. This was fixed in the stemcell.\n. Will take a look at it tomorrow.\n. Thanks! will take a look.\n. this error happens only on certain machines. we have not pinpointed direct cause of it. is it hurting your deploys -- deploys typically finish successfully when it happens? since we are almost done with new bosh-micro cli we did not have immediate plans to hunt this error down.\n. what do you mean there is no floating ip? what is 172.28.132.239?\n. \"any job has static ip will not bring up after VM created.\" not sure what you mean by that. is there an error returned by bosh deploy?\n. Closing. Reopen if issue still is not resolved.\n. Seems like current bosh micro CLI cares which directory it's running from:\n```\n~/workspace/bosh $ bosh micro status\nStemcell CID   n/a\nStemcell name  n/a\nVM CID         n/a\nDisk CID       n/a\nMicro BOSH CID bm-49bde49f-1759-4569-aac1-9ff024f95bc9\nDeployment     /Users/pivotal/workspace/deployments-aws/idora/deployments/micro/micro_bosh.yml\nTarget         https://ip\n~/workspace/bosh $ cd /Users/pivotal/workspace/deployments-aws/idora/deployments/micro/\n~/workspace/deployments-aws/idora/deployments/micro $ bosh micro status\nStemcell CID   ami-xxx light\nStemcell name  light-bosh-stemcell-2776-aws-xen-hvm-ubuntu-trusty-go_agent\nVM CID         i-xxx\nDisk CID       vol-xxx\nMicro BOSH CID bm-f5539c7c-e284-4a39-b608-7424ba3d6050\nDeployment     /Users/pivotal/workspace/deployments-aws/idora/deployments/micro/micro_bosh.yml\nTarget         https://ip\n``\n. lgtm. will merge soon.\n. I've prioritized this PR to be reviewed, fixed if needed and merged: https://www.pivotaltracker.com/story/show/84810956\n. Confirmed. Here is the story: https://www.pivotaltracker.com/story/show/84156058 to find out more reliable way.\n. We have completed investigation and are now implementing by-id change to the Agent. Follow the work here: https://www.pivotaltracker.com/story/show/84717348\n. @ryanmoran I believe you can useexcluded_files:afterfiles:` in your release package spec to not include the symlink. I agree with you that BOSH packaging should handle this.\n. @petarz Please sign CLA so we can merge this in.\n. We'll be pulling it in this week asap: https://www.pivotaltracker.com/story/show/84559336\n. @petarz Can you please confirm that merged in changes work as expected in your environment?\n. Filed as a story: https://www.pivotaltracker.com/story/show/84896868\n. Sorry I will review traveling-ruby integration in greater detail a bit later, but looking at this PR I don't understand why we have to update gems for all internal bosh bits. Don't we just want bosh_cli gems to be compatible with traveling-ruby?\nSent from my iPhone\n\nOn Dec 19, 2014, at 3:33 PM, Dr Nic Williams notifications@github.com wrote:\nTo support traveling-ruby (for the traveling-bosh / http://bosh-cli.cfapps.io/ project), we need to upgrade to nokogiri 1.6.5 (its the shipped native version http://traveling-ruby.s3-us-west-2.amazonaws.com/list.html)\nTodo items:\ncreate PR to patch foodcritic acrmp/foodcritic#291\n foodcritic new version released\n create PR for vcloud cpi vchs/bosh_vcloud_cpi#12\n vcloud PR accepted and new version released\n PR for BOSH to use new gems + new jenkins_api_client\n\u2014\nReply to this email directly or view it on GitHub.\n. @asanramon what is the output of bosh releases?\n. @johnmcteague can you confirm this feature works as you expected in the latest build?\n. @epoelke thanks. we'll pull this in soon: https://www.pivotaltracker.com/story/show/86960650\n. I am concerned about cases when IaaS fails to create extra VMs and Director would decide to compile everything on a single compilation VM which might take forever. Thoughts?\n. I meant something like this:\n- user requests 6 compilation VMs to compile CF (workers: 6)\n- Director only gets 1 compilation VM\n. can paste networks section from before and after? also the output of bosh deploy.\n. closing, no activity. feel free to reopen.\n. @uzzz Sounds reasonable. We'll review if in the minor release there were any significant changes. Thanks.\n. Merged.\n. @ronakbanka Nice! We'll pull this in.\nBy default the timeout is 3600 ( 1 hour ) but can reduce in cases where either firewall or other network issues we are loosing connections to database.\n\nWhat are you guys setting the timeout to in your environment?\n. @shinji62 that sounds awesome. very exciting.\n. @fckbo vsphere cpi options did not change. i think problem with origin manifest is ':' after your cluster name. even though it's valid yaml i think it would trip up one of the validations that vsphere cpi was doing.  i definitely run few of my own env without resource pools and did not have to change that. let us know if that does not work.\n. Agreed. Good suggestion.\n. @simonjohansson I think I figured out what may be the problem. You are using full stemcell which means bosh micro deploy will try to create an AMI on the fly. Now your jumpbox instance is in us-east-1 -- guessing from vm-in-us-east-1 hostname. When AWS CPI tries to make an AMI on the fly it actually uses instance it's running on as a bootstrapping env and to do that it needs to find it in the AWS by id. That's not going to work since you have picked completely different region. \nI think if you move your jumpbox to the same region where you are trying to deploy, AWS CPI will properly create a stemcell.\nPlease reopen if this is not the case.\n. As of 2941 light AWS stemcells are multi-region. Try it out!\n. Are you using traveling-bosh? Some one was telling me that it always returns 0. /cc @drnic \n. makes sense to make an ENV for it. e.g. BOSH_COLOR=false to make it consistent with cf cli?\n. Currently bosh-agent on vsphere stemcells is configured to do manual configuration; however, with recent changes to bosh-agent (stemcell 2865+) it might be possible to use dynamic network. I'm not sure though if micro CLI allows it. It would be awesome if you can confirm!\n. Afaik you should just be able to put a dhcp server on your subnet without configuring anything in vsphere. \n. yeah looks like dead property.\n. it's in template_only section.\n. We will be removing bosh_cli_plugin_aws soon.\n. I would recommend not using it directly, but if you must to make this more maintainable https://github.com/bonzofenix/bosh/blob/master/bosh_cli/lib/cli/errors.rb should be changed to define \nmodule Bosh\n  module Cli\ninstead of adding module Bosh; end to this file.\n. Because it's a private implementation detail and can change at any time - not an official API.\n. Merged.\n. Please include the error message and reopen.\n. Probably at some point OpenStack was not happy with >1TB volumes or may be was flaky. Feel free to submit a PR to remove that check.\n. We are the in the middle of implemented https://github.com/cloudfoundry/bosh-notes/blob/master/global-networking.md on global-net branch. To simplify future migration for users, we will try to pull this PR in once complete global networking work.\n. Agreed, --auto should recreate the vm by default. \nauto_resolution is what you are looking for the code e.g. https://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/problem_handlers/unbound_instance_vm.rb#L8 so for missing vm it could be auto_resolution: recreate_vm\n. As of v2915 of stemcell, bosh cck --auto will recreate missing vms.\n. Is there a particular area of BOSH you are concerned about that uses 1.9? The Director runs on Ruby 2. The CLI currently supports 1.9, 2.x.\n. Interesting. Are you saying that @elb.load_balancers [1] does not limit visibility of the LBs to the current user? If so that I guess makes sense to put that into ignore list. PR?\n[1] https://github.com/cloudfoundry/bosh/blob/master/bosh_aws_cpi/lib/cloud/aws/instance.rb#L93\n. You should be able to specify openstack.ignore_server_availability_zone: true for the CPI configuration to avoid using az name that your server is using for volumes.\nReopen if that does not help.\n. Awesome thanks!\n. Updated. Thanks!\n. third argument to the create_vm CPI call is cloud_properties for the VM configuration. during normal deployment that comes from resource pool and during compilation it comes from compilation's config.\n. From agent log above: \"App setup Running bootstrap: Setting up ephemeral disk: No ephemeral disk found, cannot use root partition as ephemeral disk\".\nYou either have to configure ephemeral disk on used flavor or increase root disk to such that Agent can partition it for swap and ephemeral data. \n. @guanglinlv i just noticed you were referring to installing cf on centos. cf currently only officially runs on ubuntu trusty due to different centos fs and kernel compatibility problems. im still surprised that compiling ruby failed. have you tried official centos 7 stemcell from bosh.io?\n. inline \n\nQ1, the consul_agent execute failed because of /etc/resolvconf/resolv.conf.d/head isnot exist in centos.\n\ncorrect on centos resolv.conf.d does not exist which means that release is not compatible with that os. it might make sense to change centos stemcell to support resolv.conf.d convention.\n\nQ2, the /etc/resolv.conf is wrong int the VM instance of CF, it only include nameserver <bosh-powerdns-ip>, i also need search openstacklocal and openstack-sub-net-dhcp-ip\n\nyou can configure dns in the deployment manifest in the network configuration section. see dns property in http://bosh.io/docs/networks.html.\n\nQ3, the CF deploying with centos 7 stemcell cant work correctly,i can not start any application at that.\n\ncf-release does not fully support centos 7. it only works on ubuntu trusty stemcells. in future cf may work on centos but the timeline is not determined.\n. @jrbudnack since we are trying to avoid making more changes to micro CLI plugin since we are pushing people to use bosh-init. Also I'm not sure if some people may depend on this behavior already. Have you tried bosh-init?\n. It's hard to read pasted in config file, but it seems that your agent configuration in director.yml still has use_ssl and ssl_verify_peer set to true which means agent will try to follow that.\n. Fair enough, I'll close this issue and will update the PR when we pull it in.\n. Good point. Referenced info: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/query-api-troubleshooting.html\n. Thanks for the contribution. Latest release allows configuration of the symmetric and asymmetric keys.\n. Closing. Switched to using concourse to build all stemcells: https://github.com/cloudfoundry/bosh/tree/master/ci/pipelines/stemcells\n. I am assuming this is for rhel stemcell. We have not officially confirmed that it builds for vsphere.\n. It looks like you forgot to put comma after 10.244.9.2. e.g. [10.244.9.2, 10.244.9.6]\n. @allomov have you tried running os image build on master? we have not seen this failure in our ci.\n. Closing. Fixed few months ago.\n. Ah you are right. Any chance for a PR?\n. Since proposed changes are only in the aws CPI, you can:\n- clone repo\n- cd bosh_aws_cpi\n- use bundle install to install all required gems\n- make changes to the aws code & associated tests in spec/unit directory\n- run unit tests via bundle exec rspec spec/unit\n- submit PR\n. Correct, until we get proper HA BOSH whenever that comes.\n. Can you include a stacktrace shown at the end of the bosh task 3 --debug?\n. That's correct. You should place your PowerDNS job on the same VM as Director after the changes we have made to properly flush the PowerDNS entries.\n. @Jam-Lin which certificate are you referring to? Do you mean the ssh key?\n. @byllc that's an interesting addition. i'm curious how are you guys using it propagated consul events?\n. There is no way to propagate job defaults to your tests. Ideally happy path would be covered by an integration test; however, in this case it might not be worth it given the setup is fairly expensive. Btw as a side effect of moving defaults into the job spec you can remove conditionals from the erb templates:\n- if_p(\"hm.consul_event_forwarder.port\")\n- if_p(\"hm.consul_event_forwarder.protocol\")\n- if_p(\"hm.consul_event_forwarder.events\")\n- if_p(\"hm.consul_event_forwarder.heartbeats_as_alerts\")\n. I wonder if somehow there was a runaway worker process?\n. Are you sure you don't have tasks running? bosh tasks recent --no-filter. Or bosh tasks --no-filter.\nSent from my iPhone\n\nOn Sep 15, 2015, at 7:13 AM, mtekel notifications@github.com wrote:\n+1. Same problem on cf210 release. Luckily it goes away after a while...\n\u2014\nReply to this email directly or view it on GitHub.\n. Could you provide a bit more info why do we need to make these changes?\n. BOSH does not guarantee the order of release jobs (templates) starting/stopping/running on the VM so it does not consider that to be a change.\n. Fix was merged. Thanks!\n. lgtm. i wonder if it's better to ask for the network name and template can determine the ip addresses.\n. As of bosh-release 255.4 we now track deployment for each Director task (when it makes sense).\n. I've checked on the centos 7 stemcell and root user has /var/vcap/bosh/bin on the $PATH and I'm able to run monit summary without problems.\n. Fix was included in 2999.\n. what does cat /var/vcap/bosh/log/current | grep -B2 \"Starting agent\" on the created VM show? most likely agent is failing to bootstrap either due to networking or disk problem.\n. Reopen if issue persists.\n. Let's just do what Ubuntu does:\n\n```\nroot@4f2e1302-9a4e-4996-6059-5519e7fa70e3:/home/vcap# cat /root/.profile\n~/.profile: executed by Bourne-compatible login shells.\nif [ \"$BASH\" ]; then\n  if [ -f ~/.bashrc ]; then\n    . ~/.bashrc\n  fi\nfi\n``\n. @kennetham We'll need to collect a bit more info. Runbosh vms --detailsand find the VM with Agent ID mentioned by the error you see. SSH to that VM,sudo su, and then rungrep -R -B2 'Starting agent' /var/vcap/bosh/log.\n. Please include sanitized output frombosh task 80 --debugthat should show error stacktrace.\n. It appears that connection to the openstack API from the Director VM is broken (No route to host - connect(2) for 10.232.x.x:8774 (Errno::EHOSTUNREACH). Are you able to reach openstack API from that VM via a curl?\n. In 1.2999.0 of the CLI. Thanks for the contribution!\n. This problem has been there for long time. We are currently working on fixing it: https://www.pivotaltracker.com/story/show/62354622\n. There is an interesting UX challenge here. Currently simple diff-ing happens in the CLI and the rest on the Director at which point user does not have any input except cancelling the task. Debug log shows a bit more info if you grep forchanges:. We are moving towards a way wherebosh recreate` will always touch only single VM but currently it's blocked on IP mgmt changes we are making in global-networking epic.\n. Awesome PR! \nBy vendor I mean we will actually include the source tarball for ixgbevf when pulling in this PR. \nOne thing I noticed is that sriov flag is set for all AWS stemcells, which means it will be included for CentOS. Do you happen to know if this work for CentOS stemcells?\n. Also it appears that we do not have a CLA for you.\n. Yeah, including tgz is fine.\nThere are some units tests around the bosh-stemcell & stemcell_builder mostly for stemcell building code itself.\n. Closing no activity.\n. Closing no activity.\n. @dpb587 Thanks for the reminder. Sorry it took some time during the CPI transition.\n. We have recently made changes to the bosh release to properly escape registry configuration. I believe this error is not fixed.\n. @xingzhou after looking over these changes I think there was a reason why grub.conf was being generated as comment states:\n\nGRUB 0.97 configuration (used only on Amazon PV hosts) must have same kernel params as GRUB 2\n\ni think we should bring it back.\n. Out of curiosity why do you need to know bosh's volume via tags instead of just looking at what bosh-init recorded in the *-state.json file? It seems that only AWS has tagging support for volumes.\n. What do you mean bosh version 1.3? What does bosh status show?\n. No activity, closing.\n. Im a bit confused. Do you mean the other way: 1492 is required and 1500 is set? Where is 1492 default coming from?\nWe've recently switched back to using DHCP for manual networks on OpenStack unless you opt out: https://github.com/cloudfoundry/bosh/commit/ee3bf7a1814f17b7f6d23a7c91c75674dbe9c215 and https://github.com/cloudfoundry/bosh-agent/commit/533ac3615acf55775502238479c2c9dc28e135db. Now CPI can specify use_dhcp to tell agent to use DHCP for that specific network. For AWS CPI it always set it.\nBtw NetworkingType is no longer used since agent is smart enough to pick what to do: https://github.com/cloudfoundry/bosh-agent/blob/master/infrastructure/settings_source_factory.go#L14\n. Can you add a section similar to https://github.com/cloudfoundry/bosh/tree/master/bosh-stemcell#special-requirements-for-building-a-rhel-os-image about Photon ISO and where to get it from.\n. I believe latest bosh release no longer raises this error.\n. Issue is fixed in the latest bosh release 200.\n. Story was completed some time ago.\n. I've asked legal to let us know if it's ok to bump to a newer version. \n@sykesm which bugs (besides check process) are you running into and which versions of monit fix it? Would bumping to 5.2.5 solve your problems?\nCode simplicity of this change does not reflect effort needed to confirm that update of monit does not break all kinds of other things, bosh-agent and releases that do non-pid checking come to expect of monit. \n. It sounds like for now we can only update to 5.2.5. \n. @sykesm since we are not seeing this issue in our environments, it would be great if you guys can make a stemcell with 5.2.5 monit and confirm that it resolves your problem in your environments and does not show any other side effects. once confirmed i think we will be more comfortable bumping it for everyone.\n/cc @maximilien \n. The existing tools were already included long time ago and we cannot remove them for backwards compatibility. There are several security guidelines that different companies have about including debugging tooling (like nc, etc.) that do not allow adding more tools for production envs. Overtime we want to be completely passing even the strictest guidelines, hence do not want to pick up extra executables.\n. Keystone v3 support is available: http://bosh.io/docs/openstack-keystonev3.html\n. It's fine we'll rebase off of develop.\n. Interesting. Seems like a bug. Which version of vcenter are you using?\n. Story filed: https://www.pivotaltracker.com/story/show/102611608\n. http://bosh.io/docs/cpi-api-v1.html#create-vm mentions disk_cids as one of its arguments.\n\ndisk_cids [Array of strings] Array of disk cloud IDs for each disk that created VM will most likely be attached; they could be used to optimize VM placement so that disks are located nearby.\n\nWe do not guarantee that it will always be populated since Director may make later decisions in the deployment flow about which disks to attach. \nI'm not sure though I understand what you mean by \"enable local storage type\". Do you mean you want to create ephemeral disk on local storage or do you mean you want persistent disks to be on the local storage?\n. Are you thinking that CPI would pick specific hosts for VM creation based on where persistent disk is located? Would you also be able to move persistent disks from host to host if needed?\nSeparately from that, are you planning to support network attached storage for persistent disks?\nCPIs do manage ephemeral disk but transparently inside create_vm call. If you want to pass in special configuration for ephemeral disk you should do that through resource pool's cloud properties.\nWe planning to support a similar configuration in vsphere cpi so that we can optionally place persistent disks onto local storage; however, it would not be a default in our case.\n. @poblin-orange fair enough. Unless I missed something I believe disk_cids provide you with enough information for moving things around. Can this issue be closed?\n. @poblin-orange Hmm. It doesnt feed the disk CID on initial create I believe because we first create VM and then the disk, but on subsequent recreates of the VM I believe it does send it it.\n. @Jonty to understand better, why do you parse the stemcells output of the CLI?\n. I'd like to build out some higher level functionality in bosh CLI that most people need when operating bosh. bosh cleanup is a good example of that where most bosh operators need to clean up stemcells.\nwhat functionality is missing from cleanup command?\n. > I agree that a cleanup with additional options would help here. In this case a good option to the clean up is specify how many releases to keep (in this case --all is misleading, it keeps 2 releases). We could add --keep NUM to keep latest NUM releases.\n--all removes all unused releases. without --all (which is a default) it only keeps last 2 unused releases/stemcells.\n. > The current output of bosh commands makes it really difficult to handle in scripting.\nthat's somewhat purposeful to prevent people reinventing the wheel; or inventing it and not contributing it to bosh CLI itself.\n\nOther example of difficult to parse output is bosh releases output, and upload a release to finally find out that has been already uploaded is really slow.\n\nThis is an excellent example why we do not want to introduce --terse for this kind of thing =) Goal is for most of the commands to be idempotent including bosh upload release. We actually have a story coming up to improve upload release command to be faster in a noop case: https://www.pivotaltracker.com/story/show/101726210.\n. > Making something difficult for users of a project is not the way to get them to contribute back.\nimho it's better to understand the underlying issue user is trying to solve (e.g. cleanup stemcells/releases) and work through it with them, than to encourage a way to write scripts to automate bosh.\n. @Jonty \n\nIf you wish to talk about extending the cleanup command I'm happy to do so in a new issue, but continuing the discussion here derails from getting this change merged.\n\nSure, we would be interested in a better cleanup command. \n\nAre the bosh maintainers actively against adding machine-readable output?\n\nI'd like to first find out how you are planning to use these APIs, and which commands potentially can be improved to avoid extra wrappers, etc.\n. @mtekel \n\nE.g. imagine you want to sync what stemcells you have in multiple bosh instances.\n\nTo address common scenarios like your example, we are trying to make all commands idempotent so that you can just write bosh upload stemcell X --skip-if-exists (in future skip-if-exists flag wouldn't even be necessary) in a loop. You shouldnt have to check against a list of stemcells that already exist on a director.\n\n...so that you can explore every single possible use-case?\n\nIt's ideal to know what people are trying to do because I have seen so many duplicate bash scripts doing same exact thing. Most likely if you are trying to write a script around bosh cli, at least few other people already done it.\n\n...Or is the intention here to force people using ruby instead?\n\nnope.\n\nIn that case, you should remove --terse output from other CLI commands that currently have it...\n\nThere is one command doing it and it was changed in 2012.\nLet me reiterate from above: My goal here is not to make your life harder than necessary, but to learn about use-cases people write scripts for and make them a product feature if that is something a lot of people are trying to do.\n. Given the demand for this feature let's pull this (and similar) PRs in. I still hope that other use cases will be shared with the community even if there is an easy way to automate through some bash scripting. \nWe have also recently documented the Director API in its basic form: https://bosh.io/docs/director-api-v1.html.\n. Apologies for the delay on making this happen. We are on the verge of releasing rewritten bosh CLI (https://github.com/cloudfoundry/bosh-cli) that specifically addresses variety of problems collected from community such as this one. It might be best to close this PR at this point - we probably won't backport this sort of functionality to ruby CLI.\nWe've made table formatting a first class option so that all commands are script friendly (json and cut/grep). Here is an example of it in action:\n$ bosh stemcells|cat\nbosh-aws-xen-centos-7-go_agent          3232.custom*    centos-7        ami-3849af55\nbosh-aws-xen-hvm-centos-7-go_agent      3232            centos-7        ami-ab3822c1\nbosh-aws-xen-hvm-ubuntu-trusty-go_agent 3280*           ubuntu-trusty   ami-d9a6c3ce light\nbosh-aws-xen-hvm-ubuntu-trusty-go_agent 3262.7*         ubuntu-trusty   ami-3b77ee2c light\nbosh-aws-xen-hvm-ubuntu-trusty-go_agent 3232            ubuntu-trusty   ami-c63b21ac\nbosh-aws-xen-hvm-ubuntu-trusty-go_agent 3231            ubuntu-trusty   ami-ec140e86\nbosh-aws-xen-ubuntu-trusty-go_agent     3290*           ubuntu-trusty   ami-4e5a2c59 light\nbosh-aws-xen-ubuntu-trusty-go_agent     3262.12         ubuntu-trusty   ami-96dca881 light\nbosh-aws-xen-ubuntu-trusty-go_agent     3262.9*         ubuntu-trusty   ami-5e6c0349 light\nHere is the pipeline that produces CLI binaries if you want to try it out and let us know if it fits in your workflows or if you have suggestions on the formatting of the new CLI: https://main.bosh-ci.cf-app.com/pipelines/bosh:cli. (Few differences in the new CLI: https://github.com/cloudfoundry/docs-bosh/blob/master/cli-v2.html.md.erb).\n. We recently introduced --strict_host_key_checking flag. We also have a few stories comping up about automatically checking host key fingerprint: https://www.pivotaltracker.com/story/show/102530088\n. Can you also update blobstore_client/spec/functional/dav_spec.rb with tests about deletion of not found objects?\n. > Should I expect the VM automatically come to running state after the stopping/starting operation?\nNope since ephemeral disk is lost.\n\nIf so, how should I fix the failing state?\n\nyou can run bosh recreate job_name job_index\n\nIf not, should I delete the failing VM, recreate one and attach the persistent disk manually?\n\nnope, just let bosh recreate do that for you.\n\nMaybe the same issue happened in AWS?\n\nyes it does.\nBOSH expects that machines that were stopped outside of its control will be brought up through BOSH, either automatically via resurrector or manually via bosh cck/recreate.\n. I think for now (not sure how common such configuration is for other releases) it makes sense to introduce a configuration for dea_next (cf-release) and cell (diego-release). Wdyt?\n. Oops I meant 3072. Added 3074... \nre cf-release: I believe it's still a draft /cc @Amit-PivotalLabs \n. look at github releases tab. it says that in the changelog.md. some builds dont include details since they may only contain internal changes or unannounced features that will be announced later when necessary fixes (remaining features) are included.\nSent from my iPhone\n\nOn Mar 2, 2016, at 3:34 AM, mtekel notifications@github.com wrote:\n3202 release notes missing again. Is it that hard? Just make it mandatory for your release process. Changelog.MD has last update 2 months ago...\n\u2014\nReply to this email directly or view it on GitHub.\n. @mtekel may be i need to adjust which details get shared as part of the release. right now started but not complete features are left out of release notes to avoid confusion of them being done. i typically note them once they are complete and verified by few internal consumers after a few releases (e.g. upcoming uaa team scopes). \n\nsince we are working on different features in parallel, they progress differently (some are started, almost done, just completed, etc.). we also typically avoid branch-per-feature given that makes it harder to refactor (we do occasionally use longer running branch for a bigger/riskier feature). so as a result each build that comes out of a CI has a combination of finished, in progress, etc. features. \nwe releases those builds for all different types of consumers: some teams pull them in via ci automatically, some pick specific builds for specific features, etc. that way we get faster feedback for certain features without trying to finish it, but not necessarily announcing it to everyone.\ni'll try to include more \"hidden/internal-ish\" details in release notes so let me know if that's better.\n. Story: https://www.pivotaltracker.com/story/show/103218714\n. Closing issue on bosh. bosh-bootstrap is a separate project.\n. Planning to merge it in this story: https://www.pivotaltracker.com/story/show/104454938\n. Rebased as new PR: https://github.com/cloudfoundry/bosh/pull/998. Closing this one.\n. @AbelHu registry changes were merged. i believe we can now use unmodified bosh release when deploying the director. you should be able to update CI scripts to use official bosh release.\n. Currently users created by bosh are not passwordless sudoers, so CLI asks for a password or expects a password to be provided by --default_password flag. We are planning to make them passwordless sudoers soon; meanwhile you can try something like this:\nbosh ssh node 0 --default_password p \"echo p | sudo -S something\"\n. Seems like latest net-ssh maintainers decided to stop supporting 1.9.3 (it is a deprecated version). You can try uninstalling net-ssh 3.x and gem install net-ssh -v 2.9.2 before installing bosh_cli. Any reason you prefer using 1.9.3? Just relying on default ruby on ubuntu?\n. \"Force deleting is set, ignoring exception\" is in the logs which means that --force flag is set. When force flag is set the Director ignores failures and continues on. I believe opsmgr team is aware of this and they will be fixing it in some future build.\ncc @mreider \n. I can confirm I also saw this problem.\n. i believe this issue was fixed some time ago once we switched to go 1.5.\n. In CLI 1.3120.0\n. In CLI 1.3120.0\n. Story was rejected: https://www.pivotaltracker.com/story/show/100344958.\n. Agreed. I believe we may actually get proposed behavior for free once we merge our global-net branch, because we immediately assign roles to the VMs instead of what we currently do.\n. Story in progress: https://www.pivotaltracker.com/story/show/105585496\n. Fixed in bosh-release v215 (3104)\n. In CLI 1.3120.0\n. Story created: https://www.pivotaltracker.com/story/show/105868858\nWe are just planning to rename cpi jobs for now. Eventually we can have more comprehensive solution.\n. In v219.\n. We are actually planning to fix it in this story: https://www.pivotaltracker.com/story/show/104923910. Root problem here is that Ruby 1.9 vs 2.0 have different behaviors for JSON.dump(...). It works on Ruby 2.0.\nOut of curiosity why are you building a bosh-lite machine instead of using provided one?\n. v219 fixes this problem.\n. Do you have BOSH_USER/PASSWORD env vars set with wrong credentials?\n. Fix was made: https://github.com/cloudfoundry/bosh/commit/9f2c37ae207325e6ab499485701a235800dfd0d7.\n. We found that bug earlier and decided we will fix it as part of our work on global-net branch. I expect the fix will be available when we get to the following marker: https://www.pivotaltracker.com/story/show/100170490\n. Merged.\n. afaik vrealize is abstracting vsphere at a higher level than bosh requires so i'm not sure if it makes sense to write a vrealize cpi. we havent heard so far any of our vsphere enterprise customers ask for such integration. is there some functionality missing from vsphere cpi that you feel is missing?\n. Thanks for more details. BOSH CPI core team does not have immediate plans to support vrealize, but I will leave this issue open to see if more people respond.\n. After looking into this more I do think we may have a bug regarding improperly configure multiple nets. Will have more details tomorrow.\n. Story created: https://www.pivotaltracker.com/story/show/107452992. We will pick it up tomorrow.\n. @jcarrothers-sap we believe 3137 stemcell fixes this problem. please reopen if issue persists.\n@poblin-orange i dont believe your issue is the same; moved it here: https://github.com/cloudfoundry-incubator/bosh-vcloud-cpi-release/issues/9\n. @fraenkel please try 3148. we believe https://github.com/cloudfoundry/bosh/commit/dddb47433d75f3087d650d71d9ff47030b562980 fixes this problem. reopen if problem persists.\n. @simonleung8 this issue has been closed for a long time. please open a different issue and include necessary info like cli and director version, error stacktrace, etc.\n. I believe this was merged.\n. @bonzofenix It doesnt really make sense to set dns.domain if rest of dns properties are not set. Btw do you have a manifest snippet that fails?\n. Resolved in bosh v219. Sorry for the inconvenience.\n. Fixed in v219\n. Problem here is that because upload to nginx takes so long (for large releases), by the time token makes it into the Director it has already expired. There is no good way to fix this unless uploading is separated into another step so that it returns a token that never expires. I think for now we should just set the access token validity to some reasonable time that is greater than maximum upload time.\ndocs-bosh PR?\n. I've updated the story with some changes to how CLI should behave.\n. @guoger they may be identical in content (fingerprint is the same) but because tar generates different file for the same content sha1 may not match.\n. It really depends how you setup your routing. \nTo configure VMs to have additional nics you can specify multiple IPs in the manifest. See http://bosh.io/docs/networks.html for more details.\nRegarding segregating traffic it kind of depends what you consider as internal traffic. There are components that communicate through public facing LB (e.g. api talking to uaa), which means that machines on private network must be able to route to public network.\n. just to clarify can you login into machine via the console?\nSent from my iPhone\n\nOn Nov 5, 2015, at 9:51 AM, Amit Gupta notifications@github.com wrote:\nOriginal issue: cloudfoundry/cf-release#841\nOriginal poster: @yyl8815\nHi there,\nI need a favor to help me out from my situation. I am trying to deploy cf-release 200. I have struggled to the step of \"bosh -n deploy\". However, the deployment always stoped at \"start creating bunding VMS\". Some post says retry \"bosh -n deploy\" can fix this. However, i tried with no luck.\nThe retry does fix some VMs. However, there are still some VMs which just keep failing. error code like below:\nStarted creating bound missing vms\n  Started creating bound missing vms > small_z1/0\n  Started creating bound missing vms > small_z1/1\n  Started creating bound missing vms > medium_z1/0\n  Started creating bound missing vms > medium_z1/1\n  Started creating bound missing vms > medium_z1/2\n  Started creating bound missing vms > medium_z1/3\n  Started creating bound missing vms > router_z1/0\n  Started creating bound missing vms > router_z1/1. Failed: Timed out pinging to a8eecfaf-8254-4b04-a2e8-668cd5e0b3fb after 600 seconds (00:11:08)\n   Failed creating bound missing vms > small_z1/1: Timed out pinging to e8f56631-9e35-482c-8829-\nb6cb14d6724c after 600 seconds (00:11:08)\n   Failed creating bound missing vms > medium_z1/3: Timed out pinging to 65d7ef2e-e594-4941-b3e8-fc85ca729d4e after 600 seconds (00:11:09)\nI use vsphere client to check the VMs before they got deleted and got below screen shots:\n\u2014\nReply to this email directly or view it on GitHub.\n. Interesting. Are the VMs failing to start get scheduled on specific hosts? It's unlikely that it is a problem with stemcell, since some VMs start successfully?\n. Please include bosh deploy output when this happens. Compilation VMs typically get deleted before other parts of the deploy starts.\n. Closing. no activity. As suggested please try using bosh-init.\n. Error returned:\n\ngetaddrinfo: Name or service not known\nindicates that vcenter address could not be resolved to an IP.\n. Merged.\n. Unfortunetly nokogiri version in BOSH is tied with all the other gems and some expect it to be >1.5. I'm surprised that this was an issue since there are several people in the office that use El Capitan with newer nokogiri.\n. There is already option called host (same line you modified) that allows to set S3 endpoint. Is that not sufficient?\n. How about endpoint if it's for readonly?\n. @xingzhou Does this prevent agent restarts after 24 times since agent sets passwords?\n. looking at the stacktrace there is a mention of bosh-workspace. thats not something that vanilla bosh cli tool provides.\nSent from my iPhone\n\nOn Nov 18, 2015, at 11:20 AM, ChandraNarayanasamy notifications@github.com wrote:\nWe are not using any plug-ins. just did an update and executed command bosh.\n[root@hello commands]# bosh\nFailed to load plugin /usr/local/share/gems/gems/bosh-workspace-0.9.4/lib/bosh/cli/commands/deployment_patch.rb: cannot load such file -- bosh/workspace\n\u2014\nReply to this email directly or view it on GitHub.\n. Closing.\n. See http://bosh.io/docs/tips.html#unreachable-agent\n. Can you please include sanitized agent log from the VM as suggested in the entry?\n. Feel free to reopen when you get a chance to share the logs.\n. did you follow bosh.io/docs/init-aws.html?\n\nSent from my iPhone\n\nOn Dec 4, 2015, at 2:20 AM, smokingfly notifications@github.com wrote:\nThe agents do not have any public IP assigned to them, could that be part of the problem? See the attached AWS screenshot.\n\u2014\nReply to this email directly or view it on GitHub.\n. check out http://bosh.io/docs/aws-cpi.html#errors section. i believe it's there.\n\nSent from my iPhone\n\nOn Dec 4, 2015, at 8:28 AM, smokingfly notifications@github.com wrote:\nI followed the new bosh documentation as suggested above and made progress. \"bosh deploy\" was now able to compile lot more packages with \"Done compiling package .....\" following \"Started compiling package...\". But I ran into binding issues when the process reaches \"Started preparing dns > Binding DNS. Done\". See below for the error message:\nStarted creating bound missing vms\nStarted creating bound missing vms > small_z1/0\nStarted creating bound missing vms > small_z1/1\nStarted creating bound missing vms > small_z1/2\nStarted creating bound missing vms > small_z1/3\nStarted creating bound missing vms > small_z1/4\nStarted creating bound missing vms > small_z1/5\nStarted creating bound missing vms > small_z1/6\nStarted creating bound missing vms > small_z1/7\nStarted creating bound missing vms > small_z1/8\nStarted creating bound missing vms > small_z1/9\nFailed creating bound missing vms > small_z1/5: Unknown CPI error 'Unknown' with message 'resource eipalloc-f6b84292 is already associated with associate-id eipassoc-8c288df5' (00:01:26)\nDone creating bound missing vms > small_z1/2 (00:01:59)\nDone creating bound missing vms > small_z1/4 (00:02:07)\nDone creating bound missing vms > small_z1/3 (00:02:10)\nDone creating bound missing vms > small_z1/6 (00:02:11)\nDone creating bound missing vms > small_z1/0 (00:02:14)\nDone creating bound missing vms > small_z1/8 (00:02:14)\nDone creating bound missing vms > small_z1/7 (00:02:14)\nDone creating bound missing vms > small_z1/1 (00:02:20)\nDone creating bound missing vms > small_z1/9 (00:02:22)\nError 100: Unknown CPI error 'Unknown' with message 'resource eipalloc-f6b84292 is already associated with associate-id eipassoc-8c288df5'\nAttached are the debug log and configuration files.\nAny input will be greatly appreciated. Thanks.\nbosh.yml.txt\ncloudfoundry.yml.txt\ntask3-debug.txt\n\u2014\nReply to this email directly or view it on GitHub.\n. This is a planned future enhancement.\n. Let's keep this open until this is fixed.\n. Did you successfully deploy before downloading the manifest? Currently Director only updates the manifest on a successful deploy, which means it will return empty string before the first deploy.\n\nWe are planning to eventually save the manifest immediately so this case will not come up.\n. We are planning to save the manifest regardless of the successful deploy or not in future; however, I'm not sure how you are able to run errand and a deploy in parallel since there is an exclusive lock on a deployment. \n. What behavior would you expect given that deployment was not successfully deployed? running old versions of errand?\n. Currently vSphere CPI requires hosts in each cluster to have access to associated datastores. In your example: first and second host must have access to nfs_first and nfs_second. This may be improved with future versions of CPI. cc @ryantang \n. That's an expected behavior due to how AWS treats instance storage. \n. What kernel version are you running on? Official ubuntu stemcells are on a recent 3.19 kernel: 3.19.0-33-generic #38~14.04.1-Ubuntu.\n. each version of stemcell ships with recent version of packages for trusty. there is no need to run any upgrade procedure since the stemcell is updated as whole.\nSent from my iPhone\n\nOn Dec 2, 2015, at 6:17 PM, Matt Cui notifications@github.com wrote:\n@cppforlife We ran \"apt-get -y install --install-recommends linux-generic-ltd-vivid\" to upgrade to the latest level of 3.19.0.x for Ubuntu 14.04. Actually, I have more concerns about \"apt-get upgrade\", do you think if we could run this command to upgrade library packages in the stemcell? Thanks.\n\u2014\nReply to this email directly or view it on GitHub.\n. Probably, we have never tried to run upgrade commands on a live machine.\n. @yyl8815 See http://bosh.io/docs/tips.html#unreachable-agent for more details.\n. Oops. Misread. Looking at your cf manifest I've noticed you have placed datacenters key in each resource pool outside of cloud_properties key. datacenters is an IaaS specific key (in this case vsphere) so it should go under each cloud_properties section. See first example in http://bosh.io/docs/vsphere-cpi.html#resource-pools.\n. I was referring to the following datacenters key. Global configuration under vsphere key is fine.\n\nresource_pools:\n- cloud_properties:\n    cpu: 1\n    disk: 4096\n    ram: 1024\n    datacenters: #<---- should be under cloud_properties \n    - name: cf\n      clusters:\n      - cf_z1\n  name: small_z1\n  network: cf1\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu-trusty-go_agent\n    version: latest\n. vsphere section is only necessary in the bosh manifest.\n. Looks like docs were incorrect. It's actually something like this: https://github.com/cloudfoundry/docs-bosh/commit/90361f33ab299e0759cedd5179eec28f197ce483. Will be updated on bosh.io next push.\n. Please include manifest snippet and error message.\n. Closing, no activity.\n. Not much. We use standard ruby yaml package which means if it makes a decision to add a ! based on the value we cant control this.\n. @zhang-hua instead of configuring files let's just check that x server is not installed.\n. Im going to close this issue here, since Release Integration team responded on the mailing list.\n. @barthy1 Any tests that can be added to catch this?\n. You cannot see codeclimate result after logging in? (I wouldnt bother doing much about codeclimate result as it is something that we use as a global metric).\nWe are also about to merge change to networking which makes this error show up as:\n```\nAre you sure you want to deploy? (type 'yes' to continue): yes\nDirector task 217\n  Started preparing deployment > Preparing deployment. Failed: Job 'db3' with network 'private' declares static ip '11.10.64.124', which belongs to no subnet (00:00:00)\nError 150002: Job 'db3' with network 'private' declares static ip '11.10.64.124', which belongs to no subnet\nTask 217 error\n```\nIs that acceptable error message in your opinion?\n. Please add a unit test.\n. @fabianschwarzfritz Can you please add a unit test.\n. Where do you configure 3222 port as your ssh port? Bosh currently only works through port 22 for ssh.\n. Currently bosh does not support ordered start within in a VM. For majority of cases it's best to make your apps retry connecting to dependent services when starting. Do you have more details about what you are trying to start in specified order? Regarding monit directive: we found that it's flakey at best in practice so I would not recommend using it.\n. Are you using bosh-lite. If you are you can just delete contents of tmp/ folder next to your Vagrantfile. If you are not then you can reconfigure your bosh director manifest.\n. We are planning to add comprehensive diff to replace what CLI currently provides: https://www.pivotaltracker.com/story/show/109914094. Coming up soon.\n. Are you using latest bosh cli?\n. Does this happen consistently? This error is shown when downloaded blob did not match expected sha1. That may happen when somehow either remote blob is corrupted or download procedure was somehow corrupted and transferred bits are not correct.\n. Are you configuring NTP servers as shown in the example: http://bosh.io/docs/init-aws.html?\n. Story: https://www.pivotaltracker.com/story/show/111192606\n. That makes sense. \n. @sudhindrarao Team has confirmed that issue is fixed in later Director builds.\n. You will have to expose that configuration in https://github.com/cloudfoundry/bosh/blob/master/release/jobs/powerdns/spec and use it in https://github.com/cloudfoundry/bosh/blob/master/release/jobs/powerdns/templates/pdns.conf.erb.\n. @sdheepa Just have to build a new bosh release and use bosh-init to update Director. For example see http://bosh.io/docs/init-aws.html how it points to bosh release. You can use url: file://~/bosh-rel.tgz and skip sha1 when using local file.\n. That's expected. pre_start scripts run when before jobs are not configured with monit. You'll have run your ctl explicitly (e.g. blah_ctl start, do some work, blah_ctl stop).\n. Fixed in v248 of bosh-release.\n. We have introduced a feature called compiled releases in the Director & bosh-init. You can use bosh export release command to export compiled release in your CI. At some point other CF teams will also incorporate that behavior in their pipelines.\nRe package cache: compiled releases meant to solve this specific problem but in a more generic way. Shipping compiled releases allows people to use still take advantage of compiled artifacts even when they have no Internet access from their environment.\n. We recently started work on https://github.com/cloudfoundry/bosh-notes/blob/master/events.md. \"events\" label in tracker.\n. Events were added few months ago: https://bosh.io/docs/events.html. Over time we are expanding information recorded in/as events as we change/add Director functionality.\n. yup. unfortunelty our integration testing didnt catch this migration breakage. next build will it.\nSent from my iPhone\n\nOn Jan 21, 2016, at 7:52 AM, Geoff Franks notifications@github.com wrote:\nWe're running into some DB schema issues when trying to delete a deployment on a BOSH that we've upgraded from v219 to v246. The error being thrown is:\nPG::Error: ERROR: update or delete on table \"deployments\" violates foreign key constraint \"vms_deployment_id_fkey\" on table \"vms\" DETAIL: Key (id)=(18) is still referenced from table \"vms\".\nIt's occurring on BOSH's hooked up to both AWS + vSphere. When we take a look at the VMs table, there are a bunch of VMs in there with cids who do not match any live instances. It looks like the data from the vms table was migrated into the instances table, somewhere between v219 and v246, but never cleaned up. Is the vms table no longer used for anything?\nIf so, can it be cleaned up via another schema migration?\nReproduction steps should be deploy BOSH v219.. deploy a boshrelease with VMs. Upgrade BOSH to v246. Try running a bosh delete deployment\n\u2014\nReply to this email directly or view it on GitHub.\n. @calebamiles will seoul region just work if added to this list?\n. Closing. Please use bosh-init with bosh-aws-cpi-release.\n. @mattcui we are still waiting for ubuntu to publish updated kernel. will publish a new stemcell when that happens. meanwhile diego team is confirming that their fix works in our dev envs.\n. Closing, since there was a fix in https://github.com/cloudfoundry/bosh/releases/tag/stable-3192\n. @voelzmo mind giving it a try to see if it solved this problem?\n. Associated story: https://www.pivotaltracker.com/story/show/112833065\n. Associated story: https://www.pivotaltracker.com/story/show/112825405\n. can you get stacktrace from the director? we cant repro it for some reason.\n\nSent from my iPhone\n\nOn Feb 7, 2016, at 4:35 AM, Marco Voelz notifications@github.com wrote:\n@wcamarao making sure that the keys are strings seems to make sense to me.\nSome steps to reproduce what I described above:\ndeploy the above manifest with global properties looking like this:\nproperties:\n  what:\n    is:\n      going:\n        on: here\nNote: I saw the deploy fail with recent stemcells with this error\nError 100: undefined method `to_sym' for true:TrueClass\nHowever, I was experimenting with older stemcells at that point, and e.g. with v3140 I could actually deploy it. Maybe due to another ruby version?\n\u2014\nReply to this email directly or view it on GitHub.\n. @subhankarc while bosh vms is showing information successfully. can you record the ips shown and try to ssh into one of the vms. from there please grab http://bosh.io/docs/job-logs.html#agent-logs. we've not seen this situation happen after a few mins. this typically would happen after laptop goes to sleep (https://github.com/cloudfoundry/bosh-lite/blob/master/docs/bosh-cck.md).\n. @mattcui Update looks good. We'll pull it in for the next release.\n. Story: https://www.pivotaltracker.com/story/show/115721221\n. pulled in.\n. Filed as story: https://www.pivotaltracker.com/story/show/114891805\n. @voelzmo issue marked as \"planned-enhancement\". i will add a story when we are getting closer to implementing this functionality.\n. Story: https://www.pivotaltracker.com/story/show/115721441\n. @sdheepa This seems to be cf-release specific issue. I would recommend filing this against cf-release repo. \n. You should switch to using bosh-init (http://bosh.io/docs/install-bosh-init.html). bosh micro CLI is deprecated.\n. @wendorf make sure you are using recent enough version of the director. \n. I wonder if this is some kind of cross thread class loading issue. Is this something you are seeing consistently?\n. We have resolved this issue in recent bosh releases by switching Director to use s3cli and davcli binaries that are used by the Agent as well (to maintain nice consistency).\n. what @mbhave said. still looking into possible solutions...\n. @beyhan it should run regardless of failures.\n. @AbelHu what's in the update? why do we need to bump?\n. Does that mean older stemcells will stop working because they dont support newer protocol? \n. Got it. Story: https://www.pivotaltracker.com/story/show/115712317\n. Waiting for cf-release to switch over to webdav server before rolling this out.\n. bosh-release v258+ in combination with stemcells 33xx+ should will now wait for monit stop to finish before continuing onto a next step. \n. Lower priority since postgres is configured to listen on a local port: https://www.pivotaltracker.com/story/show/114884951\n. I dont know which stemcell you are running on but we are using latest available package: http://packages.ubuntu.com/trusty-updates/libc-bin\n. @longnguyen11288 yup. currently there is no way to show redacted values from the manifest. im assuming you want that feature?\n. Sounds like it's worth introducing an option to show redacted values e.g. bosh deploy --show-redacted or something like that.\n. @voelzmo i would like to be more paranoid by default (hide potentially sensitive info).\n. Story: https://www.pivotaltracker.com/story/show/114912611\n. As part of v255.4 we added --no-redact.\n. Can you please add a unit test for this change.\n. Story: https://www.pivotaltracker.com/story/show/115721889\n. We follow Ubuntu's severity recommendations. If there is a USN with high severity or above we immediately issue a hotfix release, otherwsie bumps to OS packages happen in subsequent builds.\n. which IaaS are you requesting this for? on vSphere for example we use name to find machines, so naming it someone else would not work properly.\n. Related issue: https://github.com/cloudfoundry/bosh/issues/1128\n. @StanleyShen so just to clarify you would want to this to be included at the deployment level because different deployments will have a different prefix?\n. Makes sense.\n. Story: https://www.pivotaltracker.com/story/show/115793569\n. @ChaosEternal bump. thoughts on what @dpb587-pivotal said?\n. @mattcui we are adding env variable to disable micro building as part of stemcell build process: https://www.pivotaltracker.com/story/show/114537425. this is a step towards removing it completely. we will be promoting this change soon, so i would recommend to set it when building softlayer stemcells.\n\n@liuweichu it's strongly recommended to not use bosh's ruby because it's used internally (currently in stemcell building). it will be removed from stemcells completely without notice in coming months. we are not planning to upgrade it.\n. Given above explanation I am going to close this issue.\n. Currently BOSH team is waiting for an ok on FIPS compatibility work from CF foundation. Since Ubuntu does not have pre-built packages for openssl FIPS that they maintain this would be non-zero maintenance cost for the BOSH team. Once CF starts moving towards FIPS compatibility then BOSH can also take it on.\n. We can change the unit tests for that too. We always wanted to change default behavior anyway. One problem I see though with parallel uploads is that it still will error most likely at the director side. We'll have to check if update_stemcell.rb background job deals with this properly.\n. That's fair. I'll try to write up a few steps to clean up the DB in emergency case or if you have a chance I would be happy to take a PR to docs-bosh with the steps you did. Im assuming you did them via director_ctl console?\n. Either this is a networking problem, duplicate IPs in use, or an agent issue. I would check:\n- does bosh vms keep on returning all as running several times?\n- do you have IPs associated with these VMs used by something else?\n- log into one of the agents and see if there are error in the agent log\n\nRecreate fails due to IP conflicts being detected.\n\nDo you mean \"Cannot create new VM because of IP conflicts with other VMs on the same\" error? If so this does mean that you have multiple VMs using the same IPs. You'll have to delete those VMs to avoid IP conflicts. \n. Fixed in bosh v255.6\n. Story: https://www.pivotaltracker.com/story/show/116224413\n. Most likely ARP caching issue. Amit sent instructions to confirm.\n. We do also have a story for to tackle this: https://www.pivotaltracker.com/story/show/118979067\n. Story: https://www.pivotaltracker.com/story/show/116487229\n. Story: https://www.pivotaltracker.com/story/show/116469377\n. Fixed in 255.8\n. Since --ca-cert is a global flag you have to put it before the command eg bosh --ca-cert X target ...\n. We will be changing CLI soon to support both ways.\n. Merged into develop.\n. cc @voelzmo \n. I'll close this issue here since you've opened one against capi-release. BOSH just passes properties to the releases and it's up to releases to define what happens then. Not sure if it ever was supported by cloud controller to update quotas via manifest.\n. yup. should bump. story: https://www.pivotaltracker.com/story/show/117398595\n. Would BOSH_MICRO_ENABLED env variable make last 2-3 PRs unnecessary?\n. @StanleyShen as you find out there is currently no way to rename a deployment. IDs dont necessarily make UX nicer. previously it was briefly mentioned that it may makes sense to add a command/config for renaming of deployments.\nif you want to hacky way of doing it, you should be able to rename a deployment via DB. See http://bosh.io/docs/tips.html#director-db for how to access DB via Ruby. You should be able to run Bosh::Director::Models::Deployments.where(name: \"old\").update(name: \"new\") (check on a test deployment).\n. @dpb587-pivotal can you please update @allomov on the changes we are planning to make regarding this issue.\n. @allomov yup deletion always causes more problems then it solves imho. we are planning to add first class team support tasks -- each task belongs to set of teams -- which should eliminate this problem.\n. sync blobs problems are resolved in the new bosh cli. it also defaults to 5 parallel workers.\n. We have added \"deployment\" field for each task. That should be authoritative source for which deployment tasks belong to.\n. Can you describe a bit more why it's necessary?\n. Story: https://www.pivotaltracker.com/story/show/117646763 as requested.\n. yup when stemcell was cut from master to update packages, it picked up older revision of bosh release since we have been producing bosh release from 255.x branch most recently. 255.9 bosh release should ve considered yanked and we will release 255.10 from 255.x branch.\nSent from my iPhone\n\nOn Apr 16, 2016, at 7:52 AM, Alexander Lomov notifications@github.com wrote:\nLooks like bosh.io points to commit 8f47e6a7 for bosh version 255.8, this commit contains missing migration files.\nAt the same time bosh release version 255.9 in bosh.io points to 47cafe56, which doesn't have this files.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. To clarify, please git squash the commits into a single commit so it's easy to merge. We had difficulties merging this in as is.\n. @barthy1 after looking at the content, let's change it to two keys before and after. before/after keys should include stemcells/releases as they were/are. that way i think it will be a little easier to understand state transitions.\n. I believe this was fixed in later versions since that code is no longer present.\n. i'll try to add something to bosh.io/docs.\n. yup we've run into this bug. fix is via this story: https://www.pivotaltracker.com/story/show/117557245\n\nwill be in bosh v256.\n. @Niranjana-5588 when instance is stopped, all the container that it was running (from bosh's perspective vms in this case) die. to bring them back you'll have to tell bosh to resurrect them: https://github.com/cloudfoundry/bosh-lite/blob/master/docs/bosh-cck.md.\n. @Niranjana-5588 if one of the instances is \"failing\" you should take a look at the logs (http://bosh.io/docs/job-logs.html#job-logs).\n. @Niranjana-5588 may be I misunderstood... are you using aws cpi which spins up aws machines (more than one) or are you using bosh-lite?\n. @Niranjana-5588 I'm very surprised then since VMs in bosh-lite are really just containers. I'm not sure how you can end up with such long wait times. This might be related to: https://www.pivotaltracker.com/story/show/118143997 which will be fixed in the next release.\nAlso how big is your bosh-lite instance on AWS?\n. As we are changing tasks to have direct association with teams, this problem will be resolved. Story: https://www.pivotaltracker.com/story/show/117845011\n. but we should make that a default for new cli. will do that.. That seems reasonable to make bosh-init run drain script if agent is accessible.\n. Updating fog and excon sounds ok.\n. Reopened that one.\n. I am bit surprised you are adding descriptions to your configuration files. I don't think I've seen that before. We are planning to add property types soon to the Director and may consider this as a use case; however, I am not too eager to expand the p(...) interface only for this use case.\n. I would be curious to see debug log for this failure. Typically CPI should raise NotFound error for disks 1 and then record will be removed from the DB.\ncc @voelzmo \n. Small correction:\n\nOriginally BOSH director create only one DNS record per instance.\n\nDirector creates DNS record per instance-network pair.\n@Freyert: I would also say that DNS is not necessary when working with BOSH (really depends on the support of static IPs by the IaaS). Now that we have links (http://bosh.io/docs/links.html), DNS records just like static IPs are not something operator should have to think too much about.\n. Agreed.\n. @longnguyen11288 I think that makes sense. \n. Closing. We are slowly converging to having bosh deploy --fix having ability to recover deployments in a irregular state. Ultimately we may even turn bosh cck into a command that just reports problems instead of allowing to interactively fix them.\n. @keymon looking at the code, looks like some blobstore error is happening preventing to continue deletion. even when we do -f we still do not let you ignore undeleted blobstore records. do you see any blobstore errors in the debug log?\ncc @voelzmo \n. @sax would it be simpler to keep ~/.bosh/cache folder in sync? \n. ubuntu bosh stemcells typically follow lts ubuntu releases. \nubuntu community maintains multiple lts releases at a time. for example trustry is fully supported and updated for the next 2 years. so we are running on a secure and supported version of ubuntu.\nat the same time we also pick up new lts releases and make a new stemcell lines based on that. typically its not an urgent thing to do as most bosh releases do not depend on specific os version. we wait a bit for new lts to stabilize and get all the new release bugs out.\ncurrently we are in progress of preparing xenial based stemcell and hoping to release as a beta in next month or so. (next lts version based stemcell - 18.04 - may come a bit quicker though).\nSent from my iPhone\n\nOn Jan 8, 2018, at 8:46 AM, Holger Winkelmann notifications@github.com wrote:\nWe are just entering into the CF world. Was a bit a surprise that the latest supported ubuntu release is 14.04. Is there any more recent, 16.04 LTE available soon. Especially with the latest Security updates it would be a good catch to have it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @jianqiu we are going to take a stab at fixing tests.\n. @mattcui @maximilien can we arrange a debug session or retrieve agent logs? i'd like to understand exactly what's happening on the agent side. this might be related to start being synchronous operation, which means correct thing to do here would be to switch it to async like many other agent ops.\n. thanks @CAFxX \n. UI deprecation of index is a result of https://github.com/cloudfoundry/bosh-notes/blob/master/finished/availability-zones.md\n. Story: https://www.pivotaltracker.com/story/show/115537581\n. I think this is covered by recent stemcells as they are now produced from concourse.\n. @gu-bin what you are describing here was actually a non-deterministic bug. (Director was not properly making a deep copy of properties hash). intent was always to require listing of all properties in the job spec. if you look around most of the releases if they use a property in the erb templates, then define it in the properties section of the the release.\n. @gu-bin just to clarify. it seems that you relied on buggy behavior and now that this behaviour is fixed, you will have to update your release with correct property definitions.\n\n/cc @maximilien\n. Planning to bump to latest 9.4.x: https://www.pivotaltracker.com/story/show/119379617\n. @mattcui i would rather not bump to another minor version. we just recently added 9.4.x series. any security vulnerabilities would have been fixed in the latest 9.4.x \n. there should be a stacktrace in the ruby debug log. could you please include it?\nSent from my iPhone\n\nOn May 11, 2016, at 3:21 AM, Justin Carter notifications@github.com wrote:\nTrying to deploy to vsphere returned:\nStarted updating job bosh > bosh/0 (d402205c-3de2-49ce-8764-21a937394aac) (canary). Failed: undefined method `disk_cid' for nil:NilClass (00:00:04)\nWhen looking at the logs from bosh task 3822 --debug it shows that the cpi is returning null when no disk space could be located:\nD, [2016-05-11 10:08:06 #1310] [canary_update(bosh/0 (d402205c-3de2-49ce-8764-21a937394aac))] DEBUG -- DirectorJobRunner: External CPI got response: {\"result\":null,\"error\":{\"type\":\"Bosh::Clouds::NoDiskSpace\",\"message\":\"Couldn't find a 'persistent' datastore with 100000MB\nof free space. Found:\\n vol310 (32136MB free of 286811MB capacity)\\n vol309 (27959MB free of 162648MB capacity)\\n\",\"ok_to_retry\":true}, \"log\": \"...\"}\"\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Seems like we already fixed this issue in https://github.com/cloudfoundry/bosh/commit/7d6a19b9696e7534046c249028f7d01249ce6ba2. Please update to 256.x Director.\n. Interesting I dont think we've seen that error ever in any of our environments.\n- What kind instance size are you using for the Director?\n- Have you seen this error before this version?\n\nRe Unable to look up VM apply spec error: We are adding 'Delete VM' option to cck to cover othe edge cases.\n. Did you previously run 255.x? I wonder if this is related to bumping eventmachine to 1.0.4 from 1.0.3.\nAlso how big if your deployment size? I'm gonna try to reproduce the problem in my account.\n. do you mind sharing your sanitized director manifest?\n. awesome. one more question which db are you using? how many connections does it allow?\n. @saliceti how big is your director vm?\nSent from my iPhone\n\nOn May 12, 2016, at 6:27 AM, saliceti notifications@github.com wrote:\n+1\nI'm testing with the same versions as @mtekel and I'm experiencing the same issues.\nCreate a new CF deployment. Run delete deployment and it hangs. In the bosh logs I get the errors \"Unresponsive client detected\", Bosh::Registry::InstanceNotFound, etc.\nI could delete it with the \"--force \" option only.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\n. i havent been able to repro this problem so far with a local db and local blobstore. i'm going to switch it over to rds now. \n\n@voelzmo i know that you are not using rds, are you using external blobstore?\nalso are you all deploying multiple things to the director in parallel. anyone has some cpu/mem usage from the director around when this problem occurs? also how many vms are managed by the director across all deployments?\n. also any other software colocated on that instance like UAA?\n. i also see that you have not changed max_threads (default 32). how does your update section of cf manifest look like?\n. (btw if you anyone is willing to do some screen sharing to debug the environment with the problem, let me know)\n. ah. my env was not using 32 threads. ill give that a shot in the morning.\nSent from my iPhone\n\nOn May 13, 2016, at 2:32 AM, mtekel notifications@github.com wrote:\nWe are using RDS and S3 as blobstore. UAA is not running on this VM or at all. The microbosh is deployed from the manifest I shared above. The issue happens specifically during creation or deletion of VMs. During this time, AWS CPI consumes 100% performance of a single core, the second core is about at around 20% cpu load. The issue happens exactly at the time when the \"NATS error\" is logged in the worker error log.\nWe use 32 threads and when we run deploy/delete it starts creating/deleting 32 VMs in parallel. But this was fine with 255.8 BOSH and the only issues we had with that previously was with hitting AWS API throttling, which is now solved.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. ...or rather it did have 32 threads max but it didnt need all of them during my bosh deploy.\n\nSent from my iPhone\n\nOn May 13, 2016, at 2:32 AM, mtekel notifications@github.com wrote:\nWe are using RDS and S3 as blobstore. UAA is not running on this VM or at all. The microbosh is deployed from the manifest I shared above. The issue happens specifically during creation or deletion of VMs. During this time, AWS CPI consumes 100% performance of a single core, the second core is about at around 20% cpu load. The issue happens exactly at the time when the \"NATS error\" is logged in the worker error log.\nWe use 32 threads and when we run deploy/delete it starts creating/deleting 32 VMs in parallel. But this was fine with 255.8 BOSH and the only issues we had with that previously was with hitting AWS API throttling, which is now solved.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. @mtekel I've not yet reproduced the NATS connectivity issue but I did run into task timeout (35+ threads on the same instance type). I've also seen CPU spike to 100% on both cores and memory is consumed to the fullest and swap is in use.\n\nWhile I continue investigating I would recommend scaling down to a smaller number of threads.\nI'm also thinking about introducing a separate configuration value that would be responsible for concurrency of bulk CPI operations like create/delete VMs.\n. @dpb587 interesting. sounds like we are not properly searching vm_extensions.\n. Story: https://www.pivotaltracker.com/story/show/119611031\n. hmm yeah should have hm check if instance is resurrectable.\n. Changes were shipped in v258+.\n. Not raising an error seems like a bug: https://www.pivotaltracker.com/story/show/119844019\n. @keymon yup the help text for the command is poorly worded and incorrect for UAA use case. we do support UAA clients for non-interactive usage: https://bosh.io/docs/director-users-uaa.html#client-login\nI believe BOSH_USER/PASSWORD env vars are not meant to be used with login command but rather you can just set them before running any other command (when basic auth is used) and you will be authenticated. https://github.com/cloudfoundry/bosh/blob/695bed549bd93dd263285b6da08b4c23fe24e98c/bosh_cli/lib/cli/base_command.rb#L160\n. sleeps should be replaced with agent.wait_until_ready if this is done due to rebooting of the machine. this could be useful to cpis that reboot machines after disk attachment. though agent will not properly startup after disk migration if this is the case. also unit tests should be added for such change.\nregarding added pkgs please elaborate in detail why they are required and how they will be used.\nbosh-agent conflicts with cloud init so im not sure we can accept this change as is. if you describe necessary functionality we may find a better way.\nSent from my iPhone\n\nOn May 18, 2016, at 9:43 PM, Alexander Lomov notifications@github.com wrote:\nHey, @krishnanrs.\nThis changes seem to be very cloud specific, so the best way to provide them will be developing your external CPI for specifically for Bracket Computing infrastructure.\nYou can move delay for 30 seconds to attach_disk method of CPI and it will be executed before agent will get command to mount the disk.\nTalking about changes in stemcell builder, they also should be decoupled in the other way, because this changes can fail on other IaaSes. For instance, you can build you pipeline that builds BRKT stemcell on base of current pipelines for stemcell building (which are here), you can patch stemcell builder inside this pipeline.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. @holgero if you run monit reload on the director node. does it put the state back to running? there is potentially a monit problem that makes it confused about the state.\n. @Petahhh it's in the queue to be pulled in.\n. @jmcarp we have actually switched director to use s3cli (https://github.com/pivotal-golang/s3cli) so that it's more consistent with what agent uses to download upload/download blobs. if your intent is to make the director use this configuration, i think you will have to add this to s3cli.\n. which version of bosh are you using (bosh status output)?\n\nSent from my iPhone\n\nOn May 25, 2016, at 10:25 AM, chrisrana notifications@github.com wrote:\nI have cloud foundry deployment .I have 3 kafka,3 zookeper and 3- vault/consul instance.\nkafka/0 10.20.0.174 \nzookeeper/2 10.20.0.242 \nvault_consul/0 10.20.0.247 \nFor testing purpose i delete one of the vm from each .All the vm's are resurrecting and these vm's have new IP now,but dns is missing in them. \nbosh vms output is as follows\nkafka/0             | running | service-net-medium | 10.20.0.251 | n/a  | \n zookeeper/2      | running | service-net-medium | 10.20.0.250 | n/a  |\n vault_consul/0   | failing |   small | 10.20.0.25 | n/a \nOn Analysing the Microbosh DB I found dns name are still mapped with old ip.\nMicrobosh PostgresDB output\n./psql bosh -c \"select id, name, type, content from records;\"\n46 | 0.kafka.ccc-service-net.cf-devtest21vik.microbosh | A | 10.20.0.174 \n47 | 174.0.20.10.in-addr.arpa | PTR | 0.kafka.ccc-service-net.cf-devtest21vik.microbosh\n28 | 2.zookeeper.ccc-service-net.cf-devtest21vik.microbosh | A | 10.20.0.242 \n29 | 242.0.20.10.in-addr.arpa | PTR | 2.zookeeper.ccc-service-net.cf-devtest21vik.microbosh\n67 | 0.vault-consul.ccc-service-net.cf-devtest21vik.microbosh | A | 10.20.0.247\n70 | 247.0.20.10.in-addr.arpa | PTR | 0.vault-consul.ccc-service-net.cf-devtest21vik.microbosh\nMicrobosh manifest file\n<% load 'micro01/micro_bosh.rb' %>\nname: <%= $os_tenant %>\nlogging: level: DEBUG\nnetwork: type: manual\nip: <%= $ip_address %> \ncloud_properties: name: ccc-bosh-net \nnet_id: <%= $net_id %>\nresources: persistent_disk: 50000 \ncloud_properties: instance_type: <%= $bosh_flavor %> disk: 16384 cloud: plugin: openstack properties: openstack: auth_url: <%= $os_auth_url %> username: <%= $os_username %> api_key: <%= $os_password %> tenant: <%= $os_tenant %> default_security_groups: [vms-security-group] default_key_name: <%= $key_name %> private_key: <%= $key_file %> ignore_server_availability_zone: true\napply_spec: \nproperties: director: \nmax_threads: 32 hm: \nresurrector_enabled: true \ndirector_account: user: <%= $bosh_username %> \npassword: <%= $bosh_password %> \nntp: - <%= $ntp1 %> - <%= $ntp2 %> \ndns: \nrecursor: 8.8.8.8 env: bosh: password: <%= $sha_password %>\nIs is possible for VM to have same ip if not at least it should update it MIcrobosh DM.Let me know how to achieve this.\nIs there any config missing in this. Help me to resolve this issue.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. ah older version. since that version we have fixed several issues with how we track dns records in the director. i would recommend updating your director to the latest version to get those dns fixes.\n\nSent from my iPhone\n\nOn May 25, 2016, at 11:06 AM, chrisrana notifications@github.com wrote:\nbosh --version\nBOSH 1.3071.0\nroot@dev-inception-vm21:/home/ubuntu# bosh status\nConfig\n/root/.bosh_config\nDirector\nName installdev2\nURL https://10.20.0.55:25555\nVersion 1.3155.0 (00000000)\nUser admin\nUUID 3dc7bb73-4fa6-4c83-ab33-e9c870861e05\nCPI openstack\ndns enabled (domain_name: microbosh)\ncompiled_package_cache disabled\nsnapshots disabled\nDeployment\nManifest /opt/installer/tenant-devtest21vik/cf-deploy/cf-devtest21vik.yml\nroot@dev-inception-vm21:/home/ubuntu#\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\n. to clarify you should update your director not the cli to the latest version. though you should also update your cli while you are updating.\n\nsee https://bosh.io for the latest versions of bosh release.\nSent from my iPhone\n\nOn May 25, 2016, at 11:17 AM, chrisrana notifications@github.com wrote:\nWhat is the latest version of bosh cli .which one I should use\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\n. We are trying to avoid making any non-small changes to the CLI as we are moving towards https://github.com/cloudfoundry/bosh-cli.\n. @rishigits just to properly boot the system. nothing special i believe.\n. @aristotelesneto out of curiosity, does providing a fake access_key and secret and specifying env_or_profile credentials source fix it?\n. are you sure? thats registry code which is not used by the cli in the finalize release path.\n\nSent from my iPhone\n\nOn Jun 6, 2016, at 2:24 PM, Aristoteles Neto notifications@github.com wrote:\nDoesn't I'm afraid - fails here: https://github.com/cloudfoundry/bosh/blob/93a2522b3a6a58cc3d24f6ea10a9dba0a9a421bb/bosh-registry/lib/bosh/registry/instance_manager/aws.rb#L67\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. You should be able to find changes in the github releases. v256 release notes for example mention auditd now being installed. Each stemcell also comes with stemcell_dpkg_l.txt file (http://bosh.io/docs/build-stemcell.html#tarball-structure) which lists all packages and their versions. More details changes of course can be obtainer by using git log and filtering down to specific director (stemcell_builder for example).\n\nRe auditd configuration, we would be interested to know how you all configure it and potentially pull in missing changes. So far we have been following STIG/CIS recommendations.\n. Closing. Question was answered in comments above.\n. @baschno https://github.com/cloudfoundry/bosh/releases/tag/stable-3232 did enable auditd and few other sec related changes. that may be somehow causing this problem. anything in dmesg related to this?\nthis may be related: we now mount /tmp with noexec. may be that's causing the problem?\nroot@371be127-9908-48e2-47e4-40322589d3fc:/home/vcap# mount|grep /tmp\n/var/vcap/data/root_tmp on /tmp type ext4 (rw,noexec)\n. @baschno any findings?\n. Failure remained undiagnosed in earlier stemcell versions. Feel free to reopen if new information comes up.\n. @philippthun we have made the fix https://github.com/cloudfoundry/bosh/commit/91a975355e5769ad06b374960c5ba61d914389bc but have not yet published new release. sorry for the inconvenience.\n. Closing. Fix available since v258+.\n. cc @Amit-PivotalLabs \n. @jmcarp I think that properties also have to be added to the bosh release: in the spec (https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/spec#L260) and ERB template (https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/director.yml.erb.erb#L135)\n. @sklevenz sounds like a good suggestion. \n. @geofffranks it does sound exactly what @dpb587-pivotal describes.\n. yeah, we havent decided how to solve this problem yet. given that a stemcell definition is not tied to any specific iaas or version providing a url doesnt necessarily make a lot of sense (unless i guess you are specifying a nsme instead of an os). i could be convinced adding url/sha1 to each stemcell optionally if it makes the workflow easier. currently releng team is making changes to their envs to use 2.0 features including cloud config so im expecting to receive feedback/suggestions from them about this issue as well.\nSent from my iPhone\n\nOn Jun 15, 2016, at 8:43 AM, mtekel notifications@github.com wrote:\nBOSH is able to read stemcell version and URL from the deployment manifest and check if it already has it. If not, it will download it and make available for deployment.\nHowever, if you switch to cloud-config, stemcells in the manifest are specified in the new way, which doesn't include the stemcell URL. This means that BOSH doesn't obtain stemcells itself anymore and requires operators to explicitly pre-upload the stemcells to BOSH prior to deployment.\nIs there a way to get BOSH handle the stemcells like it did previously with cloud-config?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. We have filed an issue with Canonical:\nWe have identified a problem with recent stemcell versions that we tracked down to a change in lsb-base package. It appears that updating from lsb_4.1+Debian11ubuntu6 to lsb_4.1+Debian11ubuntu6.1 introduces a file called /lib/lsb/init-functions.d/01-upstart-lsb. After that change when we boot the OS, upstart (start auditd command) fails to start auditd with the log line \"start: Job is already running: auditd\". We have checked that auditd process is not running (ps auxww|grep auditd). Before this file was introduced auditd service would properly start and process would run.\n\ni've entered a story https://www.pivotaltracker.com/story/show/121694777 to apply a fix without relying on fixed package. we'll release 3232.x shortly.\n. Canonical helped us to identify the problem. We believe that https://github.com/cloudfoundry/bosh/blob/695bed549bd93dd263285b6da08b4c23fe24e98c/stemcell_builder/stages/bosh_audit/apply.sh#L23 is unnecessary for auditd startup. We are planning to remove this configuration.\n. Fixed in 3232.11. \n. Will this do? https://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh&version=256.10#p=director.generate_vm_passwords\n. I think if such configuration is necessary user_add addon can be used to add additional users.\n. Can you explain a bit more about why we need gdisk? Who uses it? I dont believe bosh-agent requires it.\n. @tcnksm I think keeping release packaging for now makes sense from stemcell maintenance perspective. Will re-evaluate in coming months while we improve syslog-release/rsyslog integration (and think about more default logging subsystem).\nOut of curiosity how long does it take to compile?\ncc @ajackson \n. @sykesm fell off the truck. we'll rebase it.. started merging... hunting down integration tests.. > HTTPClient::ReceiveTimeoutError: execution expired\nFlaky network. CLI already retries as much as it's reasonable (a few times). Percentage of course is incorrect after a few retries so I've recorded this as a fix to do.\n. @ChrisMcGowan not a normal behavior. \n\nTurns out I had to add those users to bosh.read in order to be able to use bosh ssh. \n\nto clarify, once you've added bosh.read, you could use bosh ssh?\n. Closing. This was fixed in v258.\n. This is expected from monit since we remove all monit configurations (aside from builtin system one) between stops and starts. \n. @Amit-PivotalLabs not sure if you are proposing to disregard AZ isolation. this somewhat follows the current behavior with max_in_flight + z1/zx splits.\n. @YuPengZTE seems like the problem is coming from one of the pre_packaging script (for dea_next job) found in cf-release, not the cli itself.\n. Moved to cf-release. Closing here.\n. @combor was this resolved in 4.4 kernel (3263.x+ stemcells)?\n. any reason to have 'size = nil' instead of just 'size'?\nSent from my iPhone\n\nOn Jul 13, 2016, at 2:12 AM, cfdreddbot notifications@github.com wrote:\nHey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. this is a race between who sets up the hostname. give this method a try for installing bosh-lite: https://github.com/cloudfoundry/bosh-deployment/blob/master/docs/bosh-lite-on-vbox.md. it doesnt rely on vagrant and does not have this problem.\n\nSent from my iPhone\n\nOn Jan 22, 2017, at 8:05 AM, Erik Nelson notifications@github.com wrote:\n@carolmorneau Actually yes, SSH into the machine and monit restart the workers. Should get the task started and out of queued.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i think this would make sense.\n\nSent from my iPhone\n\nOn Jul 14, 2016, at 9:00 AM, @dpb587 notifications@github.com wrote:\n@cppforlife, this seems like poor UX. Would we be willing to accept a PR for a new property with something like p('compiled_package_cache.options.region', p('blobstore.s3_region', nil)) for sane behavior but maintaining existing unexpectedness. Since compiled package cache could be a separate bucket, it seems like region should be independently configurable.\n@aristotelesneto, nice investigation.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Story: https://www.pivotaltracker.com/story/show/118979067\n. @gu-bin you should be able to run bosh deploy and bring those records back.\n. @gu-bin yeah we removed 'dns_record_name' since it's not supposed to be sent to the agent. not sure how to avoid it. i think we would be open to the pr to ignore that key in networks_changed? method.\n. @sharms in one of the aws cpi releases we have removed default_availability_zone as a configuration in favor of setting it explicitly in the resource pool (as you pointed out above). without updating the deployments with manifests that specify availability_zone the cpi will not be able to recreate machines (since definition is missing availability_zone). can you confirm that issue goes away once cloud properties are updated to include availability_zone?\n. one thing we did notice before is using make with -j configuration introduces a bit of flakiness. may be we could do ... || make.\n. @CAFxX yup. we saw builds fail 1/50 times -- we assume that there might be race conditions in project's make configurations.\n. @lexsys27 will look into this.\n. @lexsys27 were you migrating from non-azs configuration to azs configuration? \n. @lexsys27 im going to close this issue since i think it behaves as expected if you use migrated_from. feel free to reopen if you observe otherwise.\n. @bodymindarts will be fixed in the next bosh release. meanwhile i believe you can just specify another addon with a different name.\n. sounds reasonable. new cli (https://github.com/cloudfoundry/bosh-init/tree/cli) simplified some of the blobs directory management -- doesnt have symlinks anymore, so it should be easier to implement this functionality. i think a better flag would be --fix.\n. @mtekel if you delete the blob when you dont need it, it would mean that you wouldn't be able to recreate your release from prev versions -- useful when making patches. may be that's something you are less concerned about given a specific release workflows (for example when always moving forward). i think new commend delete-blob makes sense. you would give it a path and it will update blobs.yml and delete the blob from the blobstore.\n. Merged.\n. cc @ljfranklin \n. @penrod seems like a reasonable change to https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release, though naming of the key might be a bit weird since we just have disk as a key for the size.\n. Seems like we can consider this closed.\n. Updated new bosh CLI (https://github.com/cloudfoundry/bosh-cli/issues/24) to latest s3cli which supports everything that director/bosh-agent supports when talking to S3 (https://github.com/pivotal-golang/s3cli#usage). \n. @jmcarp any reason why we need to save tags into db? we already save entire manifest so might as well read it off from there? wdyt?\n. @jmcarp looks about right. have to guard against empty string or null i think.\n. @jmcarp we are pulling this in and rearranging some tests/implementations.\n. @jmcarp misunderstanding on their part, we are doing what you did in memory. code coming shortly with additional tests.\n. I think this was delivered in v258 release.\n. Thanks for getting the ball rolling on this.\n. @willwillgo should use releases (plural) key instead of release in your manifest.\n. we are planning to remove nginx_ctl feature in favor of cli making it easier to generate certs (like bbl).\n\nSent from my iPhone\n\nOn Aug 23, 2016, at 9:05 AM, Tom Kiemes notifications@github.com wrote:\nHi,\nNGINX of BOSH director creates its own server certificate in nginx_ctl if not provided in deployment manifest.\nThis certificates are missing the IP Subject Alternative Name (SAN). This is even critical for the new golang bosh-cli but will also effect other applications which validate the certificate of BOSH.\n$ bosh login -e https://192.168.147.137:25555 --ca-cert /Users/cpi/Desktop/*.pem\nFetching info:\n  Performing request GET 'https://192.168.147.137:25555/info':\n    Performing GET request:\n      Get https://192.168.147.137:25555/info: x509: cannot validate certificate for 192.168.147.137 because it doesn't contain any IP SANs\nExit code 1\nOn the mentioned bosh.io documentation for creating certificates by the user, it is actually already included.\nBest regards,\nTom\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @mattcui 3262.5 was produced with 4.4 kernel.\n. Should be covered by expects_vm API field now respected by the HM in v258+.\n. @Kiemes let's merge (uniq) the agent reported ips with whatever database has. database will only contain ips if deployment is switched to global networking.\ncc @voelzmo \n. Currently we track releases on per deployment basis (instead of per instance for example) for the sake of simplicity. Until deploy succeeds bosh will consider any releases referenced by the manifest and DB are active since they may have been deployed partially.\n. Story: https://www.pivotaltracker.com/story/show/130847907\n. Fixed in v257.15 bosh release.\n. better place for cf issues is cloudfoundry/cf-release github repo or slack channel.\n\nSent from my iPhone\n\nOn Sep 12, 2016, at 8:07 AM, Tyler Schultz notifications@github.com wrote:\nHi @Niranjana-5588,\nThis is not a question about BOSH. I'm sorry, I don't know of a better place to ask your question. You may want to try joining https://cloudfoundry.slack.com and asking in the #cf-users channel. Someone should be able to direct you to someone who can help.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @knm3000 agree with @dpb587-pivotal. let's just return them both no matter what.\n. @sharms Yup. That's exactly the plan to have bosh-agent provision individual users for each job installed on the system. We are planning to do that as we fulfilling each one of the CIS/STIG benchmarks.\n. does this happen consistently? can you throw in links to releases i can use to repro in my env?\n\nSent from my iPhone\n\nOn Sep 12, 2016, at 8:05 PM, Amit Gupta notifications@github.com wrote:\n@tylerschultz wrote:\nI believe this can be explained by a bug that has been present for a long time. When director tells monit to stop, it may take a while for monit to get around to actually running the ctl stop command. Monit may be busy doing other things, stopping other jobs, daydreaming... whatever it does. Unfortunately, the agent believes stopping is complete because it has issued the monit stop foo command. The agent reports back to director that the job is stopped. Director moves on and then issues an apply call. This call will result in the new job templates replacing the old templates (the templates that would stop dnsmasq are now replaced.) Monit now finnaly gets around to issuing the ctl stop command and the stop runs against the replaced templates!!!!\nAlas, there is a fix:\nNow I can't find the job property on a quick search, hopefully it has been released. Enabling the property will cause agent and director to wait for the stop command to complete, within a timeout, before moving on to the apply step. This seems like something that should be turned on by default, but there is concern that there are many jobs that don't stop well, and it'd be a breaking change for many releases.\nLooking at tracker, it appears the fix may not yet be released. Thinking about it more, it also occurs to me that it's odd that all of the instances suffered from this problem. This leads me to believe something else may be going on.\n[source]\nThanks. Yeah, my suspicions are the same. We know about the race that can happen around stop, but this happened on many VMs in many different environments. So it's kinda mysterious. Let me know if you have any further thoughts on that first question.\nAs for the second question, does that sound like a reasonable approach? Note that I believe I had to do the deploy with the --recreate since a simple bosh deploy left the VMs in a stopped state, and bosh start didn't start the VMs back up in the correct order (according to the manifest).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @Prospecta i'm going to close this for now but if you have any updated info feel free to reopen it.\n. i wonder if we should remove timeout altogether.\n. we are planning to require users to enter --target (-t) in the new bosh cli for every command since we are removing director_uuid altogether.\n. @swilliamsvi yup. looks like uaa release configuration.\n. @blade2005 does your compilation machine has internet access? doesnt look like it's in a subnet that has network connectivity.\n. @blade2005 since you configured director to use s3 blobstore all your machines have to have outbound internet access so that they can download/upload blobs necessary during the deploy. typically people use nat gateway/instance to provide outbound access. alternatively you can look up how to use private s3 endpoints.\n. I think this can be closed now.\n. shebang must be on the first line of the rendered template. clarification docs pr would be nice.\n\nSent from my iPhone\n\nOn Sep 27, 2016, at 12:36 PM, Henry Stanley notifications@github.com wrote:\nWhen building a release, I noticed the pre-start script was failing with a generic error:\nresult: 1 of 1 pre-start scripts failed. Failed Jobs: my_job.\nSSHing onto the machine and running the script manually worked fine. Weird.\nThe issue seems to be caused by using erb tag markers (<% ... %>) before the shebang in the script, like this:\n<%\n  my_variable = p('whatever')\n%>\n!/usr/bin/env bash\n...\nNot sure if this is a known bug, or if it should be documented somewhere \u2013 I couldn't see any mention of this in the docs and it wasn't obvious to me. Happy to PR in a docs edit if that's the right thing to do.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @kitsirota future CLI (https://github.com/cloudfoundry/bosh-cli) doesnt provide interactive menu since it doesnt scale when there are a lot of instances.\n. > --- FAIL: TestDialTimeout (0.03 seconds)\n    dial_test.go:96: all connections connected; expected some to time out\n\nabove means that Go's test failed on the machine you are running this on. given that test is about timeouts my guess would be this relates to perf of the machine. \nwe've seen this pass on 4.4 kernel so i highly doubt that it relates to that.\n. @gu-bin not cf services contrib but other releases that use go. there is not much that we can do to fix golang's flaky test.\n. @maximilien this is very old version of golang btw, so im not sure they would be interested. closing as this is not a bosh issue.\n. moved story to backlog.\n. Should be covered by https://github.com/cloudfoundry/bosh-cli/issues/24 in new bosh cli.\n. @johnsonj we are trying to switch over to cloudfoundry/bosh-cli soon which means that configuration covered by https://github.com/pivotal-golang/s3cli will work. im not sure it makes sense for us to pull in this change.\n. Closed in favor of fixes made in https://github.com/cloudfoundry/bosh-cli/issues/24.\n. > you just include the image's UUID in your deployment manifest\nnot sure how you would do that. wouldnt you want to wrap that into a light stemcell and have cpi just use that uuid?\n. @voelzmo i think it makes sense to add a short script to bosh-openstack-cpi-release repo that will take whatever openstack specific info (uuid?) is necessary and produce a stemcell tarball for it. whoever needs to create light stemcell can just clone the git repo and run the script.\n. why does it need to talk to openstack. user just need to provide uuid. i think git clone and bash script is enough but it is your call. \nSent from my iPhone\n\nOn Oct 14, 2016, at 8:21 AM, Marco Voelz notifications@github.com wrote:\nWhat do you think about taking https://github.com/cloudfoundry-incubator/aws-light-stemcell-builder and making an openstack-light-stemcell-builder which takes your OpenStack credentials and the UUID of the image to produce a light stemcell?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i'm still conflicted about introducing this feature. we'll take this into account during discussion of how certain jobs should wait for other jobs to start/stop.\n. definitely want to ssh into errand vms, otherwise it will be hard to debug if you are usin keep alive.\n\nSent from my iPhone\n\nOn Oct 9, 2016, at 3:13 PM, Marco Voelz notifications@github.com wrote:\n@dpb587 @barthy1 I'm not sure if checking for vm_cid == nil is what you want to do. That means you now got different behavior depending on whether an errand is running when you call bosh ssh or not. What about checking the expects_vm flag on an instances as suggested in cloudfoundry/bosh-cli#18 (comment)?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @voelzmo not sure i agree. if i see the vm in bosh instances and run bosh ssh -c 'blah' command i would expect it will cover all machines. i dont think it would be obvious why some machines wouldnt be affected.\n. check out 'bosh events'. it shows why instance is being updated in the context column. 'bosh events --instance blah/blah' will show you only for that instance.\n\nhttp://bosh.io/docs/events.html\nSent from my iPhone\n\nOn Oct 11, 2016, at 3:57 AM, Dr Nic Williams notifications@github.com wrote:\nthe deployment was sharing an AWS subnet across multiple bosh networks; and there were overlaps of available dynamic ranges between the BOSH networks. Perhaps the issue occurs because of this?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > since postgresql seems to completely ignore the uniqueness constraint if one of the fields is NULL\n\n@MatthiasWinzeler should be we use \"\" instead of NULL then. i would like us to have db enforced constraint is possible.\n. not everything, blobstore and nats still stay in the cpi for the moment. will pull them out of cpi completely at some point.\nSent from my iPhone\n\nOn Oct 14, 2016, at 3:33 PM, Lyle Franklin notifications@github.com wrote:\n@cppforlife @MatthiasWinzeler question about the context hash handling in the CPIs, e.g. cloud_properties['openstack'].merge!(context['openstack']). I like that the global CPI configuration is now dynamic, but we seem to have this weird mix of dynamic values vs static values in the cpi.json file.\nSince the expectation is for all cloud.properties.openstack values to be pulled at runtime from the context, I'd like to push back against the merge call and go back to overriding: cloud_properties['openstack'] = context['openstack']. Having to add properties to spec files and ERB templates seems like a waste when its only use is to provide default values. I think it would be easier as a developer to have default values in the code instead and make everything dynamic. I'd imagine that users don't look at the CPI spec file for defaults, but would use the bosh.io CPI docs instead.\nBig thanks for the PR btw \ud83d\udc4d\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. thinking behind separation was that cpi config isnt versioned just like cloud config. its likely that you will have to update/shift configuration over time for your cpi (cred change, etc) and i dont think we want users to have to redeploy for that.\n\nSent from my iPhone\n\nOn Oct 26, 2016, at 7:44 AM, Lyle Franklin notifications@github.com wrote:\n@cppforlife @MatthiasWinzeler I realize this thought is coming a little late, but do we actually need a separate cpi-config file and command or could we just add those properties to the cloud-config?\nThe distinction is more or less an implementation detail as the cloud config is already full of properties that will be passed to the CPI. I'm worried users not familiar with the internals of BOSH won't understand why some IaaS properties need to go in cloud-config and others in cpi-config. I'd imagine this also simplifies the multi-cpi implementation and solves the issue mentioned by @tjvman where multi-cpi could break bosh recreate. Let me know if I'm missing something.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @MatthiasWinzeler we are planning to take a look at the changes next week.\n. @holgero would be nice to log dropped events.\n. Thank you. Story: https://www.pivotaltracker.com/story/show/132265151\n. @holgero We have made improvements in the latest release; however, still observed lots of events with missing info. We've ended up pulling changes in and should make them available in the upcoming release version.\n. @Kiemes do you happen to know if this was a director side problem or cli?\n. @Kiemes do you happen to know if this was a director side problem or cli?\n. To confirm you are saying it should be \"Started updating instance group web\"?\n. To confirm you are saying it should be \"Started updating instance group web\"?\n. @Amit-PivotalLabs bosh release 258 brings spec.ip which returns ipv4 for network marked as default for gateway. or if operator want to select a different network they can explicitly mark it as addressable.\n\nnetworks:\n- name: blah\n  default: [addressable] # <---- spec.ip will return blah's ip for this machine\n- name: other\n  default: [dns, gateway]\nspec.networks is a hash so there is no order. what you typically want is a network that is marked as default for gateway. snippet in the first comment isnt entirely correct either since you may have networks that are only default for either dns or gateway, but that's pretty rare.\nultimately switching to spec.ip would remove all of the ambiguity out of releases and put operator in control of which ip is given to the release. \nkeep in mind that spec.ip is only available for current instance's ip. there is no .ip accessor on links' instances, only .address.\n. @Amit-PivotalLabs bosh release 258 brings spec.ip which returns ipv4 for network marked as default for gateway. or if operator want to select a different network they can explicitly mark it as addressable.\nnetworks:\n- name: blah\n  default: [addressable] # <---- spec.ip will return blah's ip for this machine\n- name: other\n  default: [dns, gateway]\nspec.networks is a hash so there is no order. what you typically want is a network that is marked as default for gateway. snippet in the first comment isnt entirely correct either since you may have networks that are only default for either dns or gateway, but that's pretty rare.\nultimately switching to spec.ip would remove all of the ambiguity out of releases and put operator in control of which ip is given to the release. \nkeep in mind that spec.ip is only available for current instance's ip. there is no .ip accessor on links' instances, only .address.\n. @Amit-PivotalLabs im going to call this ticket closed given addition of spec.ip.\n. @Amit-PivotalLabs im going to call this ticket closed given addition of spec.ip.\n. Will be fixed by this PR: https://github.com/cloudfoundry/bosh/pull/1467\n. Will be fixed by this PR: https://github.com/cloudfoundry/bosh/pull/1467\n. good by me, hit it.\nSent from my iPhone\n\nOn Nov 3, 2016, at 12:04 AM, Marco Voelz notifications@github.com wrote:\nI can also hit 'merge' if that's alright with you \u2013 just wanted to check if something is missing. Don't feel stressed because of me checking in ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. good by me, hit it.\n\nSent from my iPhone\n\nOn Nov 3, 2016, at 12:04 AM, Marco Voelz notifications@github.com wrote:\nI can also hit 'merge' if that's alright with you \u2013 just wanted to check if something is missing. Don't feel stressed because of me checking in ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. pretty sure rmdir will fail if there are any files inside the dir which there will be some in the worst case. lets do rm_rf without a conditional.\n\nSent from my iPhone\n\nOn Oct 20, 2016, at 8:46 AM, Yulia Gaponenko notifications@github.com wrote:\n114582319\nSigned-off-by: Konstantin Maksimov kmaksimov@ru.ibm.com\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/cloudfoundry/bosh/pull/1483\nCommit Summary\nCleanup task output directory before start\nFile Changes\nM bosh-director/lib/bosh/director/job_queue.rb     (1)\nM bosh-director/spec/unit/job_queue_spec.rb (7)\nPatch Links:\nhttps://github.com/cloudfoundry/bosh/pull/1483.patch\nhttps://github.com/cloudfoundry/bosh/pull/1483.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. pretty sure rmdir will fail if there are any files inside the dir which there will be some in the worst case. lets do rm_rf without a conditional.\n\nSent from my iPhone\n\nOn Oct 20, 2016, at 8:46 AM, Yulia Gaponenko notifications@github.com wrote:\n114582319\nSigned-off-by: Konstantin Maksimov kmaksimov@ru.ibm.com\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/cloudfoundry/bosh/pull/1483\nCommit Summary\nCleanup task output directory before start\nFile Changes\nM bosh-director/lib/bosh/director/job_queue.rb     (1)\nM bosh-director/spec/unit/job_queue_spec.rb (7)\nPatch Links:\nhttps://github.com/cloudfoundry/bosh/pull/1483.patch\nhttps://github.com/cloudfoundry/bosh/pull/1483.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lnguyen most of the larger features are not enabled by default as they get rolled out. this fell into that category.\n. @lnguyen most of the larger features are not enabled by default as they get rolled out. this fell into that category.\n. @lnguyen we could switch it on in the next few releases.\n. @lnguyen we could switch it on in the next few releases.\n. @RochesterinNYC I would recommend switching over to github.com/cloudfoundry/bosh-cli at this point to avoid this problem. We are unlikely to fix this existing behaviour in the Ruby CLI.\n. @RochesterinNYC I would recommend switching over to github.com/cloudfoundry/bosh-cli at this point to avoid this problem. We are unlikely to fix this existing behaviour in the Ruby CLI.\n. @paolostivanin hmm. we have a 258 director running and managing few deployments. cpu usage is regular. any more info about your env that you think may help?\n. @paolostivanin hmm. we have a 258 director running and managing few deployments. cpu usage is regular. any more info about your env that you think may help?\n. hmm. whats happening in the debug log of the director? also access logs?\n\nSent from my iPhone\n\nOn Oct 27, 2016, at 1:43 AM, Paolo Stivanin notifications@github.com wrote:\n@cppforlife to answer your slack request :) better pasting code here than there!\n[pid 30015] select(17, [10 12 15 16], [15 16], [], {0, 0}) = 4 (in [15 16], out [15 16], left {0, 0})\n[pid 30015] writev(15, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(15, \"\", 16384)         = 0\n[pid 30015] writev(16, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(16, \"\", 16384)         = 0\n[pid 30015] shutdown(15, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(15)                   = 0\n[pid 30015] shutdown(16, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(16)                   = 0\n[pid 30015] gettimeofday({1477554939, 730315}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 730363}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 820718}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 820801}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 911154}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 911242}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 1821}, NULL) = 0\n[pid 30015] gettimeofday({1477554940, 1900}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 92300}, NULL) = 0\nI truncated the output but there were like tons of select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\nlsof -p 30015 -ad 10,12\nCOMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF   NODE NAME\nruby    30015 vcap   10r  FIFO   0,10      0t0 803493 pipe\nruby    30015 vcap   12u  IPv4 803494      0t0    TCP localhost:25556 (LISTEN)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. hmm. whats happening in the debug log of the director? also access logs?\n\nSent from my iPhone\n\nOn Oct 27, 2016, at 1:43 AM, Paolo Stivanin notifications@github.com wrote:\n@cppforlife to answer your slack request :) better pasting code here than there!\n[pid 30015] select(17, [10 12 15 16], [15 16], [], {0, 0}) = 4 (in [15 16], out [15 16], left {0, 0})\n[pid 30015] writev(15, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(15, \"\", 16384)         = 0\n[pid 30015] writev(16, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(16, \"\", 16384)         = 0\n[pid 30015] shutdown(15, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(15)                   = 0\n[pid 30015] shutdown(16, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(16)                   = 0\n[pid 30015] gettimeofday({1477554939, 730315}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 730363}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 820718}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 820801}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 911154}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 911242}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 1821}, NULL) = 0\n[pid 30015] gettimeofday({1477554940, 1900}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 92300}, NULL) = 0\nI truncated the output but there were like tons of select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\nlsof -p 30015 -ad 10,12\nCOMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF   NODE NAME\nruby    30015 vcap   10r  FIFO   0,10      0t0 803493 pipe\nruby    30015 vcap   12u  IPv4 803494      0t0    TCP localhost:25556 (LISTEN)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @paolostivanin We think we know what's going on. /deployments/cf-diego/instances is doing some heavy lifting associated with expects_vm. we are not sure how many instances is too many but director definitely looks busy when that call happens. we'll dig into this first thing monday.\n\ncc @voelzmo \n. @paolostivanin We think we know what's going on. /deployments/cf-diego/instances is doing some heavy lifting associated with expects_vm. we are not sure how many instances is too many but director definitely looks busy when that call happens. we'll dig into this first thing monday.\ncc @voelzmo \n. Story: https://www.pivotaltracker.com/story/show/133468217\n. Story: https://www.pivotaltracker.com/story/show/133468217\n. Fixed in v260. Thanks for letting us know.\n. Fixed in v260. Thanks for letting us know.\n. @cjcjameson are you sure that VM wasnt missing as well? you can check out few things in debug log:\nbosh task 16350 --debug|grep changes # shows why instance is considered changed\nbosh task 16350 --debug|grep create_vm # confirms that vm is created\n. @cjcjameson are you sure that VM wasnt missing as well? you can check out few things in debug log:\nbosh task 16350 --debug|grep changes # shows why instance is considered changed\nbosh task 16350 --debug|grep create_vm # confirms that vm is created\n. @zhutao11 Closing per above comment. Feel free to reopen.\n. @zhutao11 Closing per above comment. Feel free to reopen.\n. @ljfranklin @cunnie \n. @ljfranklin @cunnie \n. (really belongs in https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release)\n. (really belongs in https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release)\n. looking at your log this error is most likely happening on the director (check the stack trace) where its usibg postgres. are you running out of resources on the director vm? its unlikely that it has to do with stemcell version.\nSent from my iPhone\n\nOn Nov 9, 2016, at 3:32 AM, Matt Cui notifications@github.com wrote:\nI met a very urgent compilation problem when moving to the new stemcell 3263.10, seems all problematic packages are all from cf-services-contrib-release. I logged in the worker VM but didn't see anything wrong with the disk, there is enough space in /var/vcap/data. All other packages (from cf-release) have all been compiled successfully. Does anyone ever see such problem? I even didn't see the problem with stemcell 3263.8.\nStarted preparing deployment > Preparing deployment. Done (00:00:02)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:00)\nStarted compiling packages\n  Started compiling packages > ruby-2.2.3/d3aa743e213f60f06c331adb10828d2a2ccb0d11\n  Started compiling packages > libpq/60e5862e813fb7eb2d76b546079c524b64f8c3ee\n  Started compiling packages > redis26/8e5852c4ad180af808fb291db1c4ae8cc34c4603\n  Started compiling packages > redis24/77237c6676e0788a6c7f565d7ca973ed1ee745e3\n  Started compiling packages > rabbit_node_ng/55ed052562adb714f83e71128e5644796959ace2\n   Failed compiling packages > libpq/60e5862e813fb7eb2d76b546079c524b64f8c3ee: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:10:52)\n   Failed compiling packages > redis26/8e5852c4ad180af808fb291db1c4ae8cc34c4603: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:12:17)\n   Failed compiling packages > redis24/77237c6676e0788a6c7f565d7ca973ed1ee745e3: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:13:22)\n   Failed compiling packages > rabbit_node_ng/55ed052562adb714f83e71128e5644796959ace2: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:14:06)\nFailed compiling packages > ruby-2.2.3/d3aa743e213f60f06c331adb10828d2a2ccb0d11: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:18:43)\nError 100: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\nTask 2262 error\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. looking at your log this error is most likely happening on the director (check the stack trace) where its usibg postgres. are you running out of resources on the director vm? its unlikely that it has to do with stemcell version.\n\nSent from my iPhone\n\nOn Nov 9, 2016, at 3:32 AM, Matt Cui notifications@github.com wrote:\nI met a very urgent compilation problem when moving to the new stemcell 3263.10, seems all problematic packages are all from cf-services-contrib-release. I logged in the worker VM but didn't see anything wrong with the disk, there is enough space in /var/vcap/data. All other packages (from cf-release) have all been compiled successfully. Does anyone ever see such problem? I even didn't see the problem with stemcell 3263.8.\nStarted preparing deployment > Preparing deployment. Done (00:00:02)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:00)\nStarted compiling packages\n  Started compiling packages > ruby-2.2.3/d3aa743e213f60f06c331adb10828d2a2ccb0d11\n  Started compiling packages > libpq/60e5862e813fb7eb2d76b546079c524b64f8c3ee\n  Started compiling packages > redis26/8e5852c4ad180af808fb291db1c4ae8cc34c4603\n  Started compiling packages > redis24/77237c6676e0788a6c7f565d7ca973ed1ee745e3\n  Started compiling packages > rabbit_node_ng/55ed052562adb714f83e71128e5644796959ace2\n   Failed compiling packages > libpq/60e5862e813fb7eb2d76b546079c524b64f8c3ee: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:10:52)\n   Failed compiling packages > redis26/8e5852c4ad180af808fb291db1c4ae8cc34c4603: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:12:17)\n   Failed compiling packages > redis24/77237c6676e0788a6c7f565d7ca973ed1ee745e3: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:13:22)\n   Failed compiling packages > rabbit_node_ng/55ed052562adb714f83e71128e5644796959ace2: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:14:06)\nFailed compiling packages > ruby-2.2.3/d3aa743e213f60f06c331adb10828d2a2ccb0d11: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\n (00:18:43)\nError 100: PG::Error: ERROR:  could not extend file \"base/16384/17032\": Input/output error\nHINT:  Check free disk space.\nTask 2262 error\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. we ll take a look at it today. thanks. \n\nSent from my iPhone\n\nOn Nov 9, 2016, at 4:45 AM, Beyhan Veli notifications@github.com wrote:\nOur OpenStack pipelines are failing with stemcell 3306. Here is one failing job with 3306. bosh-init fails to open the SSH tunnel in order to make registry available.\nThis commit changed the order of boot.settingsService.PublicSSHKeyForUsername and boot.settingsService.LoadSettings. Settings are loaded now before ssh setup. This doesn't work with bosh-init because it needs to have the authorized_keys available before the registry information is retrieved as described here. In agent log we see following error:\n2016-11-09_10:21:38.55729 [settingsService] 2016/11/09 10:21:38 ERROR - Failed loading settings via fetcher: Getting settings from url: Get http://127.0.0.1:6901/instances/vm-6d3e1a6f-dbdb-4c81-baa3-cdc1b8b6ecdb/settings: dial tcp 127.0.0.1:6901: getsockopt: connection refused\n127.0.0.1:6901 is not available as bosh-init could not open the reverse tunnel.\nAlso booting a VM with this BOSH stemcell directly via Horizon works but after that you can't login with the configured SSH key. There is of course a different reason why the loadSettings call fails, but it's the reason why the ssh key never gets placed into authorized_keys.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. we ll take a look at it today. thanks. \n\nSent from my iPhone\n\nOn Nov 9, 2016, at 4:45 AM, Beyhan Veli notifications@github.com wrote:\nOur OpenStack pipelines are failing with stemcell 3306. Here is one failing job with 3306. bosh-init fails to open the SSH tunnel in order to make registry available.\nThis commit changed the order of boot.settingsService.PublicSSHKeyForUsername and boot.settingsService.LoadSettings. Settings are loaded now before ssh setup. This doesn't work with bosh-init because it needs to have the authorized_keys available before the registry information is retrieved as described here. In agent log we see following error:\n2016-11-09_10:21:38.55729 [settingsService] 2016/11/09 10:21:38 ERROR - Failed loading settings via fetcher: Getting settings from url: Get http://127.0.0.1:6901/instances/vm-6d3e1a6f-dbdb-4c81-baa3-cdc1b8b6ecdb/settings: dial tcp 127.0.0.1:6901: getsockopt: connection refused\n127.0.0.1:6901 is not available as bosh-init could not open the reverse tunnel.\nAlso booting a VM with this BOSH stemcell directly via Horizon works but after that you can't login with the configured SSH key. There is of course a different reason why the loadSettings call fails, but it's the reason why the ssh key never gets placed into authorized_keys.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Resolved in 3308.\n. Resolved in 3308.\n. cc @voelzmo any ideas why we are not seeing this? @zhutao11 who is your openstack provider?\n. cc @voelzmo any ideas why we are not seeing this? @zhutao11 who is your openstack provider?\n. i see, given that this is something fog is doing this sounds that it needs to be adjusted in fog-openstack.\n\nSent from my iPhone\n\nOn Nov 9, 2016, at 8:11 PM, zhutao11 notifications@github.com wrote:\nIt's not the openstack's issue. The response from openstack will be handled at Web Server,and the UTF-8 in header comes from Tomcat.\nI think the UTF-8 contributes to response analysis, but should not be a limit.\nThank you very much!\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i see, given that this is something fog is doing this sounds that it needs to be adjusted in fog-openstack.\n\nSent from my iPhone\n\nOn Nov 9, 2016, at 8:11 PM, zhutao11 notifications@github.com wrote:\nIt's not the openstack's issue. The response from openstack will be handled at Web Server,and the UTF-8 in header comes from Tomcat.\nI think the UTF-8 contributes to response analysis, but should not be a limit.\nThank you very much!\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. you should switch to using multi az syntax with a single subnet.\n\nnetworks:\n- name: net\n  type: dynamic\n  subnets:\n  - azs: [z1,z2]\n     cloud_properties: {...}\nSent from my iPhone\n\nOn Nov 10, 2016, at 3:30 AM, Craig Furman notifications@github.com wrote:\nIn cloud config, when using the single-az syntax (no subnets array) for a dynamic network, you can't specify an \"azs\" array.\nWe are using GCP, where subnetworks span all AZs in a region. This affects our ability to explicitly specify which AZs an instance group should be balanced across.\nCraig & @camelpunch\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. you should switch to using multi az syntax with a single subnet.\n\nnetworks:\n- name: net\n  type: dynamic\n  subnets:\n  - azs: [z1,z2]\n     cloud_properties: {...}\nSent from my iPhone\n\nOn Nov 10, 2016, at 3:30 AM, Craig Furman notifications@github.com wrote:\nIn cloud config, when using the single-az syntax (no subnets array) for a dynamic network, you can't specify an \"azs\" array.\nWe are using GCP, where subnetworks span all AZs in a region. This affects our ability to explicitly specify which AZs an instance group should be balanced across.\nCraig & @camelpunch\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @camelpunch there are two different az configurations: \n1. on a network (via subnets): specifies what azs bosh network spans\n2. on an instance group: specifies where group's instances should go into\n\nsingle subnet syntax will be eventually deprecated since it's covered by the multi subnet syntax (with single subnet). that will also make it more consistent with manual networks.\n. @camelpunch there are two different az configurations: \n1. on a network (via subnets): specifies what azs bosh network spans\n2. on an instance group: specifies where group's instances should go into\nsingle subnet syntax will be eventually deprecated since it's covered by the multi subnet syntax (with single subnet). that will also make it more consistent with manual networks.\n. Closing. Feel free to reopen if answer above didnt answer the question.\n. Closing. Feel free to reopen if answer above didnt answer the question.\n. @manno any stemcell test we can add to verify that efi bootloader isnt installed?\n. @manno any stemcell test we can add to verify that efi bootloader isnt installed?\n. @manno right, but we would like to be aware if we break it ourselves and a unit test will help with that.\n. @manno yup that's exactly what i mean.\n. Pulled this. Thanks.. @mtekel sounds good. story: https://www.pivotaltracker.com/story/show/134188243\n. @mtekel sounds good. story: https://www.pivotaltracker.com/story/show/134188243\n. @mtekel Done: https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/releases/tag/v61\n. new cli (https://github.com/cloudfoundry/bosh-cli) fixes this inconsistency matching your expectation.\n. new cli (https://github.com/cloudfoundry/bosh-cli) fixes this inconsistency matching your expectation.\n. @goupeng212 sounds like stop script for that job is not killing old process hard enough.\n. @goupeng212 sounds like stop script for that job is not killing old process hard enough.\n. @goupeng212 we'll try to dig in further: https://www.pivotaltracker.com/story/show/134614735\n. @goupeng212 bump. can you provide director and stemcell versions.. @keymon agreen. i wonder if we should add generic extra tags and apply them to as many places as possible.\n. it says here that it doesnt apply to 14.04 glibc: https://people.canonical.com/~ubuntu-security/cve/2016/CVE-2016-6323.html\nSent from my iPhone\n\nOn Nov 16, 2016, at 11:15 PM, Matt Cui notifications@github.com wrote:\nThe detailed info is in CVE-2016-6323\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @andyliuliming i think it may work since sequel has an adapter. we are not running tests for it since it has not gained traction as a backend to bosh.\n. @StBurcher seems like nats machines are not reachable/failing?. @tushar-dadlani we'll look into that. default logger should be prefixing lines. may be some puts somehow got in there?. Merged.. Merged.. Pulled in. Thanks.. > Don't just capture a commit sha in a BOSH release, but also the source it was generated from, e.g. the URL to a github repo. This could simply be an optional parameter to bosh create release or it could be deduced automatically from git remote.\n\nmake sense.\n\nAttach an id to each deployment (it's unclear to me from the bosh events output if this already exists anyways). These ids can then be used in bosh diff deployment  .\n\neach event in the event log has ids.\n\nWhen each deployment knows its release versions, then the above proposals should be straightforward to implement.\n\nit's definitely possible, not easy though. you may also would have to record entire manifest, cloud-config, etc in the events if you want to see that as part of diffing. if you are just referring to release diffing for now then we already record release version information in the events.. > Don't just capture a commit sha in a BOSH release, but also the source it was generated from, e.g. the URL to a github repo. This could simply be an optional parameter to bosh create release or it could be deduced automatically from git remote.\nmake sense.\n\nAttach an id to each deployment (it's unclear to me from the bosh events output if this already exists anyways). These ids can then be used in bosh diff deployment  .\n\neach event in the event log has ids.\n\nWhen each deployment knows its release versions, then the above proposals should be straightforward to implement.\n\nit's definitely possible, not easy though. you may also would have to record entire manifest, cloud-config, etc in the events if you want to see that as part of diffing. if you are just referring to release diffing for now then we already record release version information in the events.. @petergtz what do you think about relying on events table for such information lookup? regarding rollback command i would be interesting in seeing how to make bosh -d name deploy manifest.yml + bosh events command work together. \nroughly thinking about it, it could look something like this using new CLI (https://github.com/cloudfoundry/bosh-cli):\nbosh -d dep deploy <(bosh interpolate <(bosh event 123 --context) --path /before/manifest)\nfor the above to work, director would have to save manifest to the event and CLI would have to expose context information via --context flag. regarding cloud config and runtime config, i'm not sure if it would make sense to use older versions, as you might have already changed iaas itself.\nyou can already look at the history of a deployment via bosh -d dep events, but we don't yet have diff functionality. i think recording entire contents of cloud configs, runtime configs, and deployment manifests within deployment update events would be a good start.. @petergtz what do you think about relying on events table for such information lookup? regarding rollback command i would be interesting in seeing how to make bosh -d name deploy manifest.yml + bosh events command work together. \nroughly thinking about it, it could look something like this using new CLI (https://github.com/cloudfoundry/bosh-cli):\nbosh -d dep deploy <(bosh interpolate <(bosh event 123 --context) --path /before/manifest)\nfor the above to work, director would have to save manifest to the event and CLI would have to expose context information via --context flag. regarding cloud config and runtime config, i'm not sure if it would make sense to use older versions, as you might have already changed iaas itself.\nyou can already look at the history of a deployment via bosh -d dep events, but we don't yet have diff functionality. i think recording entire contents of cloud configs, runtime configs, and deployment manifests within deployment update events would be a good start.. > It can certainly be used to implement what I proposed, but it feels a bit like that would be a misuse of its actual purpose.\nI kind of agree; however, I think it might be useful to have first iteration for this feature to understand its upsides/downsides. Also should be cheaper to implement as MVP.\n\nnot \"type-safe\"\n\nwell it could validated and so on. it's still in a db so we can run migrations and what not.\n\n... Their naming and syntax just comes easy and naturally. The above doesn't follow that style.\n\nagreed; however, that was more of a glue representation in its raw form. we can of course add commands to simplify commonly used interaction but underneath it would just use the same primitives.. @petergtz holidays took over... now back. feel free to give it a try implementing this. we are focusing right now dns, config server, and cliv2 so the queue is full at the moment. i typically only try to place stuff in tracker if they are going to happen in upcoming weeks.. @Kaixiang yeah that's would be great!. @Kaixiang yeah that's would be great!. Desired behavior that you describe makes sense. We'll look into this and thanks for the test to demonstrate it. This might be the case of just us doing slightly different \"rebalancing\" (not balancing) for persistent disks where we just remove machines with higher indices.. Desired behavior that you describe makes sense. We'll look into this and thanks for the test to demonstrate it. This might be the case of just us doing slightly different \"rebalancing\" (not balancing) for persistent disks where we just remove machines with higher indices.. @tylerschultz what do you think?. @tylerschultz what do you think?. @MatthiasWinzeler depending how your deployed software behaves you could shrink to 1 and then scale up to 3 again and that should place them into new azs.. @MatthiasWinzeler depending how your deployed software behaves you could shrink to 1 and then scale up to 3 again and that should place them into new azs.. we are not yet ready to enable local_dns by default; however, i also do not want us to use IPs for links on dynamic networks since that's getting us farther from where we want to end up.. we are not yet ready to enable local_dns by default; however, i also do not want us to use IPs for links on dynamic networks since that's getting us farther from where we want to end up.. this was always links behaviour for dynamic networks.\nSent from my iPhone\n\nOn Dec 6, 2016, at 12:55 AM, Marco Voelz notifications@github.com wrote:\n@cppforlife still, that's a breaking change \u2013 I don't think you should just do it like that.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. this was always links behaviour for dynamic networks.\n\nSent from my iPhone\n\nOn Dec 6, 2016, at 12:55 AM, Marco Voelz notifications@github.com wrote:\n@cppforlife still, that's a breaking change \u2013 I don't think you should just do it like that.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @ljfranklin im pretty sure we werent using dynamic networks.. @ljfranklin im pretty sure we werent using dynamic networks.. @liuhewei will take a look. we typically just save entire contents of manifest as is and return that to you. not sure why it would behave differently in this case. do you know if this problem also affects new CLI (https://github.com/cloudfoundry/bosh-cli)?. @liuhewei will take a look. we typically just save entire contents of manifest as is and return that to you. not sure why it would behave differently in this case. do you know if this problem also affects new CLI (https://github.com/cloudfoundry/bosh-cli)?. > Is varchar(255)... We think a UUID of some kind would be sufficient, so it could be shorter.\n\nlet's make it a bit shorter for now: varchar(64).\n\nShould we add an index for the new context_id column? \n\nlet's do that. see other migrations for adding indices (later ones for good examples).\n\nIs any user input santisation necessary for the X-Bosh-Context-Id header?\n\ni think it's fine as it is. it's opaque to the Director at this point.\n\nShould the request be rejected if the header is larger than the database field?\n\nsure.. > Is varchar(255)... We think a UUID of some kind would be sufficient, so it could be shorter.\nlet's make it a bit shorter for now: varchar(64).\n\nShould we add an index for the new context_id column? \n\nlet's do that. see other migrations for adding indices (later ones for good examples).\n\nIs any user input santisation necessary for the X-Bosh-Context-Id header?\n\ni think it's fine as it is. it's opaque to the Director at this point.\n\nShould the request be rejected if the header is larger than the database field?\n\nsure.. @fhanik we've thought about adding this before, however, there are interesting cases that don't make this entirely consistent -- specifying some properties makes additional properties required. any thoughts on that?. @fhanik we've thought about adding this before, however, there are interesting cases that don't make this entirely consistent -- specifying some properties makes additional properties required. any thoughts on that?. there is a --dry-run flag on deploy. i dont remember if its only available in the new cli though.\nSent from my iPhone\n\nOn Jan 11, 2017, at 5:10 PM, liuweichu notifications@github.com wrote:\nIMO, BOSH should provide a way that user can get rendered templates without actually deploying.\nRendering and deploying should be decoupled so that release developers/BOSH admins can get a much faster feedback. (actually I don't know why BOSH haven't provided this yet)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. there is a --dry-run flag on deploy. i dont remember if its only available in the new cli though.\n\nSent from my iPhone\n\nOn Jan 11, 2017, at 5:10 PM, liuweichu notifications@github.com wrote:\nIMO, BOSH should provide a way that user can get rendered templates without actually deploying.\nRendering and deploying should be decoupled so that release developers/BOSH admins can get a much faster feedback. (actually I don't know why BOSH haven't provided this yet)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @xiujiao im not sure that this change fixes situation you are describing. at the worst prev code will call kill (with an empty string) which will error but because script is not running with set -e it will continue on executing.. closing in favor of completing bpm-izing of the bosh release: https://www.pivotaltracker.com/story/show/154148761. @paolostivanin i think you just though of what i will ask. let's make it configurable so that current (default) behaviour isnt affected.. @voelzmo nice. i wonder if we should raise an error in ERB if hm.syslog_event_forwarder_enabled specified so that it's easier to discover that this functionality is gone and ppl should use syslog-release.. @dpb587-pivotal what do you think?. could be a cli level translation to avoid changes to internal format. @StBurcher please see for debugging intstructions: http://bosh.io/docs/tips#unreachable-agent. running grep -i error /var/vcap/bosh/log/current typically will point to the error.. @StBurcher potentially looks like registry problem. cc @voelzmo is that right?. Merged into develop. Changed it to be enabled by default.. cc @frodenas . cc @voelzmo . @drnic yup. that's the result (cosmetic) of warden stemcells having older agent temporarily. there is a story to address that: https://www.pivotaltracker.com/story/show/134182747. . what's output from bosh task x --debug|grep 'FROM:'? im not sure why configuration hash changed, but it's most likely related to stricter ordering of templates. \nWe are wondering if it's normal to have to update all jobs in the first deployment after BOSH migration from old version to v2\n\nBOSH reserves the right to update its internal mechanisms to how it does determination of differences, so this behavior is normal. We don't do this frequently but over longer periods of time internals do change and that typically that does cause recreate of machines.. > Have bosh vms display stopped for instances that it thinks are detached, and aren't responding anymore (since they were turned off intentionally)\nIn cli v2: bosh -e root-bosh is -i shows state in a separate column from process state.\n\nHave the Prepare deployment phase ignore unresponsive agents that were intentionally stopped\n\nwe are going to soon do some work to make bosh deploy be a bit more granular. i will see how much we can get away with when we compute desired configuration.. we also fixed resurrector/director interplay bug (in 261+) that possibly got you in this case in the first place.. Interesting data point: vsphere does not allow >2TB extension from the iaas pespective.\nCorrection: looks like VM has to be turned off.. @fdhex @mattcui @maximilien and i looked into this just now. seems like we could augment stemcell to make it possible to start that service. it seems like unlike auditd service it doesnt create log directory on the fly: https://www.pivotaltracker.com/story/show/140456537. Resoled in stemcell 3421: https://github.com/cloudfoundry/bosh-linux-stemcell-builder/releases/tag/v3421. Fixed in e89c8222c8d73292b04c6c6c917af134abe18ce5.. @camelpunch mind rebasing off of develop?. @monkeyherder any relation to bats switching to cli v2?. i suspect you are using older version of bosh-init cli to deploy your director. rerun bosh-init deploy with newer version.\nSent from my iPhone\n\nOn Feb 9, 2017, at 9:08 AM, Nguyen Duy Tho notifications@github.com wrote:\nMy bosh director has the following version of release and CPI\nBosh\nhttps://bosh.io/d/github.com/cloudfoundry/bosh?v=260.6\nsha1: 1506526f39f7406d97ac6edc7601e1c29fce5df5\nAzure CPI\nhttps://bosh.io/d/github.com/cloudfoundry-incubator/bosh-azure-cpi-release?v=20\nsha1: fb0180c685714d064b5a467eb6f2b34388fadf0a\nWhen I upload a new release:\nbosh upload release https://bosh.io/d/github.com/cloudfoundry-community/vault-boshrelease Acting as user 'admin' on 'bosh' Using remote release 'https://bosh.io/d/github.com/cloudfoundry-community/vault-boshrelease' HTTP 500:\nOn bosh director, /var/vcap/data/sys/log/director/director.debug.log\nD, [2017-02-09 17:02:54 #53888] [] DEBUG -- Director: (0.000079s) SELECT NULL\nD, [2017-02-09 17:02:54 #53888] [] DEBUG -- Director: (0.000124s) SELECT * FROM \"deployments\" ORDER BY \"name\" ASC\nD, [2017-02-09 17:03:54 #53888] [] DEBUG -- Director: (0.000201s) SELECT NULL\nD, [2017-02-09 17:03:54 #53888] [] DEBUG -- Director: (0.000174s) SELECT * FROM \"cloud_configs\" ORDER BY \"id\" DESC LIMIT 1\nD, [2017-02-09 17:03:54 #53888] [] DEBUG -- Director: (0.000079s) SELECT NULL\nD, [2017-02-09 17:03:54 #53888] [] DEBUG -- Director: (0.000135s) SELECT * FROM \"deployments\" ORDER BY \"name\" ASC\nD, [2017-02-09 17:04:54 #53888] [] DEBUG -- Director: (0.000209s) SELECT NULL\nD, [2017-02-09 17:04:54 #53888] [] DEBUG -- Director: (0.000172s) SELECT * FROM \"cloud_configs\" ORDER BY \"id\" DESC LIMIT 1\nD, [2017-02-09 17:04:54 #53888] [] DEBUG -- Director: (0.000077s) SELECT NULL\nD, [2017-02-09 17:04:54 #53888] [] DEBUG -- Director: (0.000119s) SELECT * FROM \"deployments\" ORDER BY \"name\" ASC\nD, [2017-02-09 17:05:24 #53888] [] DEBUG -- Director: (0.000206s) SELECT NULL\nD, [2017-02-09 17:05:24 #53888] [] DEBUG -- Director: (0.000108s) BEGIN\nD, [2017-02-09 17:05:24 #53888] [] DEBUG -- Director: (0.000280s) INSERT INTO \"tasks\" (\"username\", \"type\", \"description\", \"state\", \"deployment_name\", \"timestamp\", \"checkpoint_time\") VALUES ('admin', 'update_release', 'create release', 'queued', NULL, '2017-02-09 17:05:24.001634+0000', '2017-02-09 17:05:24.001637+0000') RETURNING *\nD, [2017-02-09 17:05:24 #53888] [] DEBUG -- Director: (0.004212s) COMMIT\nE, [2017-02-09 17:05:24 #53888] [] ERROR -- Director: Errno::EACCES - Permission denied @ dir_s_mkdir - /var/vcap/store/director/tasks:\n/var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:253:in mkdir' /var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:253:infu_mkdir'\n/var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:227:in block (2 levels) in mkdir_p' /var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:225:inreverse_each'\n/var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:225:in block in mkdir_p' /var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:211:ineach'\n/var/vcap/packages/ruby/lib/ruby/2.3.0/fileutils.rb:211:in mkdir_p' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-260.6.0/lib/bosh/director/job_queue.rb:34:increate_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-260.6.0/lib/bosh/director/job_queue.rb:9:in enqueue' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-260.6.0/lib/bosh/director/api/release_manager.rb:81:increate_release_from_url'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-260.6.0/lib/bosh/director/api/controllers/releases_controller.rb:21:in block in ' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1611:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1611:in block in compile!' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:inblock (3 levels) in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:994:in route_eval' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:inblock (2 levels) in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1015:in block in process_route' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:in process_route' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:973:inblock in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:in each' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:inroute!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1085:in block in dispatch!' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:inblock in invoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in catch' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:ininvoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1082:in dispatch!' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:inblock in call!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in block in invoke' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in invoke' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:incall!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:895:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/xss_header.rb:18:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/path_traversal.rb:16:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/json_csrf.rb:18:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/frame_options.rb:31:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/nulllogger.rb:9:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/head.rb:13:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:182:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:2013:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:66:inblock in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:50:in each' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:50:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/commonlogger.rb:33:in call' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:219:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:81:in block in pre_process' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:79:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:79:in pre_process' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:54:inprocess'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:39:in receive_data' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:187:inrun_machine'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:187:in run' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/backends/base.rb:63:instart'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/server.rb:159:in start' /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-260.6.0/bin/bosh-director:37:in'\n/var/vcap/packages/director/bin/bosh-director:16:in load' /var/vcap/packages/director/bin/bosh-director:16:in\n'\nI have to create manually this folder\nmkdir -p /var/vcap/store/director/tasks && chown vcap.vcap /var/vcap/store/director/tasks\nbefore able to upload a release.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > I need access to a large number of properties\n\nwhat exactly is your job doing?. This is currently working as expected since we would not be able to know\nwhich version of the same you are referring to in your manifest, hence we\nenforce that overlapping releases in deployment manifest & runtime config\nhave the same version. In your case conflict is between 9.2 and 9.2+dev.1\nOn Thu, Feb 16, 2017 at 1:07 AM, Anton Soroko notifications@github.com\nwrote:\n\nThe same for 261.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1586#issuecomment-280274130,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9y-yKB4fF9q1EylWP0g0D7PdJDeGks5rdBHHgaJpZM4MB4Oj\n.\n. @antonsoroko correct, we are planning to add deployment level addons which might be something that may fit your use case better (add a job to all vms in current deployment?). Interesting that our mysql tests are not catching this. What is your mysql\nversion?\n\nOn Thu, Feb 16, 2017 at 12:09 AM, Sascha Matzke notifications@github.com\nwrote:\n\nMysql2::Error: Cannot change column 'variable_set_id': used in a foreign\nkey constraint 'instances_ibfk_3'\nIt seems that MySQL doesn't like column modification when there's a\nforeign key contraint associated with that column.\nThe migration set first adds the foreign key\nalter_table(:instances) do\n     add_foreign_key :variable_set_id, :variable_sets, :null => trueend\nand later tries to set the column to \"not null\"\nalter_table(:instances) do\n     set_column_not_null :variable_set_idend\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1588, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV95aRvmL-7qdGlDlG3LXarFf4T9ayks5rdAQfgaJpZM4MCs5_\n.\n. Story: https://www.pivotaltracker.com/story/show/140011419. @x6j8x would you mind sharing a bit more about how you are installing your server. or any particular my.cnf settings we should set. we cannot seem to reproduce the problem.. @x6j8x we were able to reproduce the problem. we will shortly release 261.2 bosh release with the fix and improve our testing procedure.. We believe we fixed it in 261.2. This feels related to https://github.com/cloudfoundry/bosh-cli/issues/110.\nCan you give bosh-init 0.0.100+ or bosh-cli v2 2.0.1+ with new stemcell a\ntry?\n\ncc @voelzmo\nOn Thu, Feb 16, 2017 at 12:53 AM, Anton Soroko notifications@github.com\nwrote:\n\nHi.\nOld bosh.yml:\nreleases:\n- name: bosh\n  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=260\n  sha1: f8f086974d9769263078fb6cb7927655744dacbc\n- name: bosh-openstack-cpi\n  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-openstack-cpi-release?v=27\n  sha1: 85e6244978f775c888bbd303b874a2c158eb43c4\nresource_pools:\n- name: vms\n  network: private\n  stemcell:\n    url: https://bosh.io/d/stemcells/bosh-openstack-kvm-ubuntu-trusty-go_agent?v=3312\n    sha1: 36dd5035c3868aa9f2da32291e6338636b63c345\nNew bosh.yml:\nreleases:\n- name: bosh\n  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=261\n  sha1: b7855edc33ece92e9e8e1b5d03c5e2c897d426d8\n- name: bosh-openstack-cpi\n  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-openstack-cpi-release?v=30\n  sha1: 2fff8e1c241a91267ddd099a553c1339d2709821\nresource_pools:\n- name: vms\n  network: private\n  stemcell:\n    url: https://bosh.io/d/stemcells/bosh-openstack-kvm-ubuntu-trusty-go_agent?v=3363\n    sha1: 9db642b70eb93bb4092f967d8c1716be67af1a77\nError:\nStarted deploying\n  Waiting for the agent on VM 'fd0e4139-f54f-467e-b91c-bd560d6ce057'... Finished (00:00:00)\n  Stopping jobs on instance 'unknown/0'... Finished (00:00:01)\n  Unmounting disk 'c21c7672-84b4-4fc5-8b4b-318cf2cb1557'... Finished (00:00:03)\n  Deleting VM 'fd0e4139-f54f-467e-b91c-bd560d6ce057'... Finished (00:00:14)\n  Creating VM for instance 'bosh/0' from stemcell '652416c5-b1f2-4449-835e-193e3d63831a'... Finished (00:01:09)\n  Waiting for the agent on VM 'd9289248-2a87-425c-bd50-9d71c11fc965' to be ready... Finished (00:00:16)\n  Attaching disk 'c21c7672-84b4-4fc5-8b4b-318cf2cb1557' to VM 'd9289248-2a87-425c-bd50-9d71c11fc965'... Failed (00:00:34)\nFailed deploying (00:02:27)\nStopping registry... Finished (00:00:00)\nCleaning up rendered CPI jobs... Finished (00:00:00)\nCommand 'deploy' failed:\n  Deploying:\n    Creating instance 'bosh/0':\n      Updating instance disks:\n        Updating disks:\n          Deploying disk:\n            Mounting disk:\n              Sending 'get_task' to the agent:\n                Agent responded with error: Action Failed get_task: Task 46cb2187-932b-487d-4b42-aeeb4f47dabf result: Persistent disk with volume id 'c21c7672-84b4-4f\nc5-8b4b-318cf2cb1557' could not be found\nActually, in openstack console and inside of VM I see that disk was\nattached, but was not mounted.\nUnfortunately, the root password is not working in this half-upgraded VM,\nso I can't tell you more.\nAfter the rollback to the previous version of the stemcell - update has\nbeen successfully completed.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1589, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV91OvHDDMenS3voy5ZyFKWFbmbUlVks5rdA6HgaJpZM4MCvKK\n.\n. @antonsoroko I dont think we can do much in the stemcell as it seems to be a problem that was resolved in bosh-init 100+. Thanks for confirmation that bosh-init 100 resolved the problem.. @knm3000 after thinking a bit more and looking at the code, i think we should change the direction and have it be more consistent with other \"configs\" (cpi, runtime, cloud). instead of having bosh pause-tasks command let's have bosh update-task-config and bosh task-config. that yaml like other command will allow to specify paused: true/false (let's default to false). this way this becomes a first class feature and we dont have to keep on expanding surface area of /info endpoint. this will also be helpful when we expand pause criteria to be able to pause particular team's tasks, etc.. @knm3000 @barthy1 closing this one since generic configs made it to master.. Fixed in 261.1 bosh release. @cnelson Thank you for helping out.. @avade hmm interesting; didnt notice this before.\n\nthe act of running an errand completed successfully (done state) but one of errands within run-errand task may have resulted in non-0 code. currently client is responsible for evaluation of exit code(s). this further extends to running multiple copies of an errand on multiple instances, hence it may return multiple results ( {...}{...}{...} ) with different exit codes.\ni think this is also inline with bosh vms for example. even though some agent may return errors for communication bosh vms task did succeed completing without task level errors.\nchanging this though would be a backwards incompatible change...\ncc @voelzmo @tjvman i think you guys may also have opinions on this (semi related to bosh task returning an error).. please see @tylerschultz reply in slack: https://cloudfoundry.slack.com/archives/bosh/p1487695987007184. @David-Pivotal i think it really depends on how you configure UAA. cc @sreetummidi @fhanik. can you check with latest bosh-init?\nSent from my iPhone\n\nOn Feb 21, 2017, at 12:30 AM, Chou Hu notifications@github.com wrote:\nWith Ubuntu stemcell 3363 or 3363.1 on Azure, it always fails in mount_disk after attaching a persistent data disk to a new created VM\nError Message:\nCommand 'deploy' failed:\n  Deploying:\n    Creating instance 'bosh/0':\n      Updating instance disks:\n        Updating disks:\n          Deploying disk:\n            Mounting disk:\n              Sending 'get_task' to the agent:\n                Agent responded with error: Action Failed get_task: Task 6c04dfb6-05b1-4aa3-7f19-c894efc9c19c result: Persistent disk with volume id 'bosh-data-v565cmvwhxhwkcfdefaultsa-feb5364d-f8c5-42eb-852b-c436af9c8698-None' could not be found\nPartial logs of bosh-agent inside the VM ( We cannot switch users with sudo su)\n2017-02-21_05:01:19.05422 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Running command 'route -n'\n2017-02-21_05:01:19.05460 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Stdout: Kernel IP routing table\n2017-02-21_05:01:19.05460 Destination     Gateway         Genmask         Flags Metric Ref    Use Iface\n2017-02-21_05:01:19.05461 0.0.0.0         10.0.0.1        0.0.0.0         UG    0      0        0 eth0\n2017-02-21_05:01:19.05461 10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 eth0\n2017-02-21_05:01:19.05461 168.63.129.16   10.0.0.1        255.255.255.255 UGH   0      0        0 eth0\n2017-02-21_05:01:19.05461 169.254.169.254 10.0.0.1        255.255.255.255 UGH   0      0        0 eth0\n2017-02-21_05:01:19.05462 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Stderr: \n2017-02-21_05:01:19.05463 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Successful: true (0)\n2017-02-21_05:01:19.05484 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Running command 'usermod -p * root'\n2017-02-21_05:01:19.14829 [unlimitedRetryStrategy] 2017/02/21 05:01:19 DEBUG - Making attempt #1\n2017-02-21_05:01:19.14840 [DelayedAuditLogger] 2017/02/21 05:01:19 ERROR - Unix syslog delivery error\n2017-02-21_05:01:19.24863 [unlimitedRetryStrategy] 2017/02/21 05:01:19 DEBUG - Making attempt #2\n2017-02-21_05:01:19.24871 [DelayedAuditLogger] 2017/02/21 05:01:19 ERROR - Unix syslog delivery error\n...\n2017-02-21_05:01:19.55395 [Cmd Runner] 2017/02/21 05:01:19 DEBUG - Running command 'ifup --force eth0'\n2017-02-21_05:01:19.64971 [unlimitedRetryStrategy] 2017/02/21 05:01:19 DEBUG - Making attempt #6\n2017-02-21_05:01:19.64972 [DelayedAuditLogger] 2017/02/21 05:01:19 ERROR - Unix syslog delivery error\n2017-02-21_05:01:19.74991 [unlimitedRetryStrategy] 2017/02/21 05:01:19 DEBUG - Making attempt #7\n2017-02-21_05:01:19.74993 [DelayedAuditLogger] 2017/02/21 05:01:19 ERROR - Unix syslog delivery error\n...\n2017-02-21_05:01:26.46830 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Stdout: \n2017-02-21_05:01:26.46830 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Stderr: Internet Systems Consortium DHCP Client 4.2.4\n2017-02-21_05:01:26.46831 Copyright 2004-2012 Internet Systems Consortium.\n2017-02-21_05:01:26.46831 All rights reserved.\n2017-02-21_05:01:26.46831 For info, please visit https://www.isc.org/software/dhcp/\n2017-02-21_05:01:26.46831 \n2017-02-21_05:01:26.46832 Listening on LPF/eth0/00:0d:3a:14:3c:a7\n2017-02-21_05:01:26.46832 Sending on   LPF/eth0/00:0d:3a:14:3c:a7\n2017-02-21_05:01:26.46832 Sending on   Socket/fallback\n2017-02-21_05:01:26.46832 DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 3 (xid=0xfd3b9234)\n2017-02-21_05:01:26.46832 DHCPREQUEST of 10.0.0.4 on eth0 to 255.255.255.255 port 67 (xid=0x34923bfd)\n2017-02-21_05:01:26.46833 DHCPOFFER of 10.0.0.4 from 168.63.129.16\n2017-02-21_05:01:26.46833 DHCPACK of 10.0.0.4 from 168.63.129.16\n2017-02-21_05:01:26.46833 bound to 10.0.0.4 -- renewal in 4294967295 seconds.\n2017-02-21_05:01:26.46833 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Successful: true (0)\n2017-02-21_05:01:26.46844 [File System] 2017/02/21 05:01:26 DEBUG - Reading file /etc/resolv.conf\n2017-02-21_05:01:26.46844 [File System] 2017/02/21 05:01:26 DEBUG - Read content\n2017-02-21_05:01:26.46845 ****\n2017-02-21_05:01:26.46845 nameserver 168.63.129.16\n2017-02-21_05:01:26.46845 nameserver 8.8.8.8\n2017-02-21_05:01:26.46845 search 5srimcthboouvnqetxmxg03q3f.bx.internal.cloudapp.net\n2017-02-21_05:01:26.46845 \n2017-02-21_05:01:26.46846 ***\n2017-02-21_05:01:26.46846 [File System] 2017/02/21 05:01:26 DEBUG - Writing /var/vcap/bosh/etc/ntpserver\n2017-02-21_05:01:26.46847 [File System] 2017/02/21 05:01:26 DEBUG - Making dir /var/vcap/bosh/etc with perm 0777\n2017-02-21_05:01:26.46848 [File System] 2017/02/21 05:01:26 DEBUG - Write content\n2017-02-21_05:01:26.46848 **\n2017-02-21_05:01:26.46848 0.north-america.pool.ntp.org\n2017-02-21_05:01:26.46848 *****\n2017-02-21_05:01:26.47016 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Running command 'ntpdate'\n2017-02-21_05:01:26.47017 [arping] 2017/02/21 05:01:26 DEBUG - Broadcasting MAC addresses\n2017-02-21_05:01:26.47018 [File System] 2017/02/21 05:01:26 DEBUG - Checking if file exists /sys/class/net/eth0\n2017-02-21_05:01:26.47018 [File System] 2017/02/21 05:01:26 DEBUG - Stat '/sys/class/net/eth0'\n2017-02-21_05:01:26.47018 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Running command 'arping -c 1 -U -I eth0 10.0.0.4'\n2017-02-21_05:01:26.47031 [DelayedAuditLogger] 2017/02/21 05:01:26 ERROR - Unix syslog delivery error\n2017-02-21_05:01:26.47031 [unlimitedRetryStrategy] 2017/02/21 05:01:26 DEBUG - Making attempt #74\n2017-02-21_05:01:26.52069 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Stdout: ARPING 10.0.0.4 from 10.0.0.4 eth0\n2017-02-21_05:01:26.52070 Sent 1 probes (1 broadcast(s))\n2017-02-21_05:01:26.52070 Received 0 response(s)\n2017-02-21_05:01:26.52070 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Stderr: \n2017-02-21_05:01:26.52071 [Cmd Runner] 2017/02/21 05:01:26 DEBUG - Successful: true (0)\n2017-02-21_05:01:26.57053 [DelayedAuditLogger] 2017/02/21 05:01:26 ERROR - Unix syslog delivery error\n...\n2017-02-21_05:03:01.75201 [unlimitedRetryStrategy] 2017/02/21 05:03:01 DEBUG - Making attempt #1024\n2017-02-21_05:03:01.75211 [DelayedAuditLogger] 2017/02/21 05:03:01 ERROR - Unix syslog delivery error\n2017-02-21_05:03:01.77540 [Cmd Runner] 2017/02/21 05:03:01 DEBUG - Running command 'arping -c 1 -U -I eth0 10.0.0.4'\n2017-02-21_05:03:01.79124 [Cmd Runner] 2017/02/21 05:03:01 DEBUG - Stdout: ARPING 10.0.0.4 from 10.0.0.4 eth0\n2017-02-21_05:03:01.79125 Sent 1 probes (1 broadcast(s))\n2017-02-21_05:03:01.79125 Received 0 response(s)\n2017-02-21_05:03:01.79126 [Cmd Runner] 2017/02/21 05:03:01 DEBUG - Stderr: \n2017-02-21_05:03:01.79126 [Cmd Runner] 2017/02/21 05:03:01 DEBUG - Successful: true (0)\n2017-02-21_05:03:01.85237 [unlimitedRetryStrategy] 2017/02/21 05:03:01 DEBUG - Making attempt #1025\n2017-02-21_05:03:01.85240 [DelayedAuditLogger] 2017/02/21 05:03:01 ERROR - Unix syslog delivery error\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. so effectively splitting off type and description columns.. cc @frodenas . do you have dns turned on or turned off?\n\nSent from my iPhone\n\nOn Feb 22, 2017, at 7:02 AM, Ferran Rodenas notifications@github.com wrote:\nBOSH CLI version: BOSH 1.3262.24.0\nBOSH Release: 261.2\nBOSH Stemcell: 3363.1\nIf I run the bosh errands command and there're no errands on my deployment, bosh returns a HTTP 500 error.\nThese are the logs from the BOSH director:\nE, [2017-02-22 14:58:27 #22779] [] ERROR -- Director: NoMethodError - undefined method downcase' for nil:NilClass:\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/dns/canonicalizer.rb:6:incanonicalize'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/deployment_plan/deployment_repo.rb:31:in create_for_attributes'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/deployment_plan/deployment_repo.rb:25:infind_or_create_by_name'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/deployment_plan/planner_factory.rb:58:in parse_from_manifest'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/deployment_plan/planner_factory.rb:43:increate_from_manifest'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/deployment_plan/planner_factory.rb:39:in create_from_model'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/api/controllers/deployments_controller.rb:461:inload_deployment_plan'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/lib/bosh/director/api/controllers/deployments_controller.rb:444:in block in <class:DeploymentsController>'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1611:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1611:in block in compile!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:inblock (3 levels) in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:994:in route_eval'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:inblock (2 levels) in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1015:in block in process_route'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:in process_route'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:973:inblock in route!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:in each'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:inroute!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1085:in block in dispatch!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:inblock in invoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in catch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:ininvoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1082:in dispatch!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:inblock in call!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in block in invoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in invoke'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:incall!'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:895:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/xss_header.rb:18:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/path_traversal.rb:16:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/json_csrf.rb:18:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/frame_options.rb:31:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/nulllogger.rb:9:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/head.rb:13:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:182:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:2013:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:66:inblock in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:50:in each'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/urlmap.rb:50:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/rack-1.6.4/lib/rack/commonlogger.rb:33:in call'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:219:incall'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:81:in block in pre_process'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:79:incatch'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:79:in pre_process'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:54:inprocess'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/connection.rb:39:in receive_data'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:187:inrun_machine'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:187:in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/backends/base.rb:63:instart'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/thin-1.5.1/lib/thin/server.rb:159:in start'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.2.0/bin/bosh-director:37:in'\n/var/vcap/packages/director/bin/bosh-director:16:in load'\n/var/vcap/packages/director/bin/bosh-director:16:in'\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @tyyko never tried it myself. i would recommend if you are going to do that to potentially consider copying over entire database and then deleting things that are not relevant (foreign keys should help here). \n\nas a more general question, why are you trying to do this?. @liuhewei did you stemcell get deleted from vsphere?. let me close this ticket then in favor of cloudfoundry-incubator/bosh-vsphere-cpi-release#22. current thinking is something on azure envs causes parted to not do anything. could be interesting to run strace?. is this built from master, not 261.x?\nSent from my iPhone\n\nOn Mar 3, 2017, at 12:35 AM, Marco Voelz notifications@github.com wrote:\nuploading 3363.9 with information from http://bosh.io/stemcells/bosh-openstack-kvm-ubuntu-trusty-go_agent\n$ bosh us https://s3.amazonaws.com/bosh-core-stemcells/openstack/bosh-stemcell-3363.9-openstack-kvm-ubuntu-trusty-go\n_agent.tgz --sha1=1cddb531c96cc4022920b169a37eda71069c87dd                                                                                                                Using environment '192.168.1.12' as client 'admin'\nTask 351\n08:28:01 | Update stemcell: Downloading remote stemcell (00:00:29)\n08:28:30 | Update stemcell: Verifying remote stemcell (00:00:00)\n            L Error: panic: Parsing multiple digest: Unable to parse digest string. Digest and algorithm key can only contain alpha-numeric characters.\ngoroutine 1 [running]:\npanic(0x51cc80, 0xc420016580)\n        /usr/local/go/src/runtime/panic.go:500 +0x1a1\ngithub.com/cloudfoundry/bosh-utils/crypto.MustParseMultipleDigest(0x7ffc0b44bbee, 0x2a, 0x10000c420039c70, 0x5ed126dd0000005d, 0x2)\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/crypto/multiple_digest.go:29 +0x12e\nmain.MultiDigestCommand.Execute(0x7ffc0b44bba4, 0x49, 0x7ffc0b44bbee, 0x2a, 0xc42000e570, 0x0, 0x3, 0x488a01, 0x7f5ed7f47000)\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/main/verify_multidigest.go:52 +0x39\nmain.(MultiDigestCommand).Execute(0xc420010140, 0xc42000e570, 0x0, 0x3, 0x1, 0x3)\n        :1 +0x8b\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.(Parser).ParseArgs(0xc4200180c0, 0xc42000a190, 0x3, 0x3, 0xc420066000, 0xc4200180c0, 0x7ffc0b44bb7\nd, 0x0, 0x0)\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:316 +0x8e6\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.(*Parser).Parse(0xc4200180c0, 0xc420010140, 0x16, 0xc4200180c0, 0x4a3b01, 0xc420010140)\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:186 +0x74\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.Parse(0x516100, 0xc420010140, 0x54f8bc, 0x28, 0x5dd1a0, 0xc420016360, 0xc4200001a0)\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:133 +0x4c\nmain.main()\n        /tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/main/verify_multidigest.go:29 +0x77\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i think this is resolved now on master.. @janaurka not sure if you've identified what the problem was. can you provide output from cli v2 (bosh -e prod instances -i). it will include process & instance state which will give us more info about what's going on.. @Kiemes are you changing gemfile in any way? our ci seems to be passing and there is no blobstore_client in the Gemfile/Gemfile.lock. Added story for cloud config interpolation: https://www.pivotaltracker.com/story/show/141743773. you ll have to use bosh-init 0.0.100+ or cli v2 2.0.5+\n\nSent from my iPhone\n\nOn Mar 6, 2017, at 2:19 PM, fghorbel notifications@github.com wrote:\nwhen i upgrade to version v3363.9 and v=261.3 of the following products respectively.\nstemcell: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3363.9\nbosh release:  https://bosh.io/d/github.com/cloudfoundry/bosh?v=261.3\nI got the this error.\nCommand 'deploy' failed:\n  Deploying:\n    Creating instance 'bosh/0':\n      Updating instance disks:\n        Updating disks:\n          Deploying disk:\n            Mounting disk:\n              Sending 'get_task' to the agent:\n                Agent responded with error: Action Failed get_task: Task 2dcd6e93-119a-4e17-5276-2288284ffc3f result: Persistent disk with volume id 'vol-0f0f59973907b16ec' could not be found\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. i think you could do that via exclude jobs configuration.\n\nSent from my iPhone\n\nOn Mar 9, 2017, at 1:44 PM, Slawek Ligus notifications@github.com wrote:\nPer bosh runtime-config doc an include option exists. Word has it the latest bosh also supports exclude keyword. We'd like for the exclude keyword to allow for exclusion of errand VMs.\nProblem: one of our bosh addons has a pretty expensive init step, i.e. it downloads a database locally. This is a pointless operation on an errand VM because the VM goes away. Additionally, that particular errand runs on a nano instance with only 512mb of RAM and causes the CF deployment to fail.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @oozie does that work for you?. @voelzmo this is a tough one since preventing people from modifying cloud and cpi configs by correlating to kept state is possible, but im not sure worth of additional complexity introduced.. @drnic this was done on purpose to further isolate our CI (that produces final releases and stemcells) from) from potential malicious actors. we currently have no plans to expand access. all pipeline to my knowledge should be checked in our public repos.. @drnic i was just chatting with @tylerschultz last week about having finalize release work with compiled releases. that way consumption of releases continues to work via official ways (git repo with yaml receipts).. Added a note about floating: https://github.com/cloudfoundry/docs-bosh/commit/445d34a49b39586a2b93c56630e8b82a21b73fc0. Please reopen if doc update misses something.. @drnic releases and stemcells are considered to be unrelated. for compiled releases of course there is a relationship but it allows for floating version constraints. i cant think of any easy way how director would now such information when it's downloading release from a single url.\n\nsuggestion to include source in compiled releases, doesnt really make sense to me since you are using compiled releases in a first place to avoid compilation.. note to self: how would this work with git urls.. @AbelHu how is this manifesting in an environment? how are you getting this error?. @tylerschultz @dpb587-pivotal looks good?. @drnic @voelzmo we are purposefully avoiding including source because:\n\nproducers of compiled releases want to obfuscate source of the release\nadditional size consideration -- sometimes it's almost double the size if source is included\n\nwe also have plans to strip packages from compiled releases that are only necessary for compilation.. @fghorbel we recommend that you dont use vcap for ssh access. consider colocating os-conf's user_add job for additional user configuration: https://github.com/cloudfoundry/os-conf-release/blob/master/jobs/user_add/spec. PR was merged. will be in v262.. @wendorf that works as desired since we only return \"id\" (for stemcells it's name+version) of referenced objects in api responses. to solve this problem client library should fetch missing info lazily from /stemcells endpoint which provides complete stemcell object descriptions. an example of Deployment object doing it: https://github.com/cloudfoundry/bosh-cli/blob/master/director/deployment.go#L51\ncc @drich10 . @evandbrown we had something like this for networks (configure_networks CPI method) but it was quite a maintenance cost. given that it becomes a less traveled path it becomes harder to harden. also how frequently users are actually changing tags, etc that this would be worth the continuous maintenance cost?. @schmidtsv need to start being strict about what keys could be present in configs.. we now have a rich events feed (bosh events) that has fairly detailed info about activity.\nbosh.io/docs/events\nyou should be able to pull out necessary info out of this feed based ok user/object type/etc \nSent from my iPhone\n\nOn Apr 4, 2017, at 1:36 PM, Peter G\u00f6tz notifications@github.com wrote:\nDoes BOSH emit metrics about its resurrection activity? If so, how can they be consumed / where are they documented?\nThe idea is, that by emitting a counter every time a VM is resurrected, one could set up alarms that only trigger when a certain threshold of resurrection is reached. This should help to separate true random hardware failures (which BOSH is supposed to take care of) from systematic issues (which BOSH should not hide from the operator).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > It requires an additional failsafe process somewhere (which is not a trivial thing to solve).\n\njust colocate it with the director. i've been slowly working on https://github.com/cppforlife/bosh-stats-release to export some of stats for general director info. \nnot sure what you mean by \"failsafe\".\n\nIt adds more load on the director.\n\nnot too much additional load from my tests and even so can just bump director vm size.. > If the same metrics job runs on all stand-in directors than you'd get metrics multiple times and add up to wrong values.\nthat would be the case if you have HM produce metrics. all three HMs would be producing metrics then.. @claudio-benfatto i was chatting with @geofffranks about this feature set. it was planned for next iteration on links. though of course applying such feature would not make life easier for link producers (uaa produces X clients for all kinds of diff purposes) vs link consumers (ha proxy consumes backends).. @vito btw is there functionality in concourse to keep on retrying work until at least one worker is available?. @tintoy do you mind sharing which provider you are trying to integrate with? for production workloads we typically expect that this is possible since moving data around for data services is very expensive (scp, etc.).\nas @cunnie mentioned above if you are not using persistent disk functionality in your manifest these CPI methods wont be called. alternatively you might have some luck implementing data movement yourself (within the CPI).. @drnic type is necessary for implicit linking. additionally it's meant to encourage necessary conversations and some type of consistency between releases.\n\nan attempt to converse about converging on a defacto type: servicebroker type label; but participants have died off.\n\nthat's unfortunate. might be an indicator that it's not something that people are ready to commit to yet. from that discussion, i think this is extremely valuable:\n\nI would like to propose a convention for how service broker jobs provide/describe their link.... you can throw in &light=false to get a download for a full stemcell.. we are planning to remove postgres job in the next bosh release.. @pivotal-danjahner probably.. @voelzmo do you all run into this problem?. If you need to add a user to a VM you can colocate user_add job: https://bosh.io/jobs/user_add?source=github.com/cloudfoundry/os-conf-release&version=12. that's what bosh-deployment does for director vm access: https://github.com/cloudfoundry/bosh-deployment/blob/master/jumpbox-user.yml. otherwise for vms managed by the director you can of course use bosh ssh that will create a user on the fly.. looks like the problem is related to specifying property groups (eg redis.broker) vs individual properties (eg redis.broker.protocol, etc.).. yup, in fact i was just changing prod director to show that as well as a test.. Story: https://www.pivotaltracker.com/story/show/144906309. @geofffranks \nmy_link.p(\"property-that-links-might-provide\", p(\"property-from-spec-to-default-to\")\n\nhow come is that case? is it for transition period?. See http://bosh.io/docs/cli-env-deps.html for additional cli dependencies.. > We're having the reverse problem: a job with optional linking that's picking up an unwanted link through implicit linking. We need to explicitly nil out a consumer link.\nis there a concern about explicitly nilling it out?. story: https://www.pivotaltracker.com/story/show/151894692\nOn Mon, Apr 9, 2018 at 9:53 AM, Danny Berger notifications@github.com\nwrote:\n\n\ud83d\udd17 Slack, cloudfoundry#bosh\nhttps://cloudfoundry.slack.com/archives/C02HPPYQ2/p1523055768000061 -\nsimilar use case where it would be easier to provides: nil a link, than\nto go through updating multiple consumers to use the correct link\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1675#issuecomment-379819937,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV95s947dED14JBo5tMaxjgHpuPPJrks5tm5H3gaJpZM4NSM-Z\n.\n. @rwwohl this is an artifact of using YAML. In YAML no is a shorthand for false. Dont think there is a way to turn this off in a ruby YAML library.. i think we ve named all are things \"orphan\" so route should be \"/orphan_disks\".\n\nSent from my iPhone\n\nOn May 24, 2017, at 9:09 AM, Danny Berger notifications@github.com wrote:\n@dpb587-pivotal requested changes on this pull request.\nThanks! A couple minor suggestions.\nIn src/bosh-director/lib/bosh/director/api/controllers/orphan_disks_controller.rb:\n\n@@ -0,0 +1,13 @@\n+require 'bosh/director/api/controllers/base_controller'\n+\n+module Bosh::Director\n+  module Api::Controllers\n+    class OrphanDisksController < BaseController\n+      get '/' do\n+        content_type(:json)\n+        orphan_json = OrphanDiskManager.new(@logger).list_orphan_disks\nTraditionally we try to create these managers once in the initialize, then in the function invoke the desired method (example). Not critical, but helps us keep our newers in a consistent place until we can do better at DI.\n\nIn src/bosh-director/lib/bosh/director/api/controllers/orphan_disks_controller.rb:\n\n@@ -0,0 +1,13 @@\n+require 'bosh/director/api/controllers/base_controller'\n+\n+module Bosh::Director\n+  module Api::Controllers\n+    class OrphanDisksController < BaseController\nFor consistency with the route, can we rename this class/file to be \"orphaned\".\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. closing this one in favor of https://github.com/cloudfoundry/bosh/pull/1769. @dpb587 while you are there may want to switch to using https://github.com/cloudfoundry/bosh-deployment/blob/master/docker/unix-sock.yml instead of tls certs for the docker daemon?. @Everlag i think you are running old ruby version. im pretty sure ruby 2+ introduces that syntax\n\nirb(main):001:0> { 'instance_id': 123}\n=> {:instance_id=>123}\n\nAs an aside, this is the kind of thing that #1616 makes more difficult to confirm.\n\ni could see us may be tagging git repo with such info.. >  In both cases, there will be downtime due to processes being stopped during the update\nto clarify danny's note, downtime will happen in bosh of these cases if you just have 1 instance in an instance group.. @henryaj see how to configure additional user on the director vm: https://github.com/cloudfoundry/bosh-deployment/blob/master/docs/jumpbox-user.md. @drnic fixed. much appreciated.. ivan: if you have docker daemon running you can just use docker/cpi.yml in bosh-deployment. here is how it would look: https://github.com/cloudfoundry/bosh-deployment/blob/master/test.sh#L435.it uses docker cpi to run director and uses same cpi to run other containers.\nSent from my iPhone\n\nOn Jun 22, 2017, at 6:07 AM, Ivan Davidov notifications@github.com wrote:\nThank you, @allomov!\nCurrently the Docker image is the closest and fastest thing to what I'm trying to achieve.\nFrom what I see, the Docker image versions are following the same pattern as the Vagrant based bosh-lite VM. I know for sure that bosh-lite is not going to have new updates (in favor of the new bosh 2 CLI which manages the VM), which makes me think that the Docker image won't get updates either. Nevertheless, this Docker image might be just what I need.\nI consider the issue resolved since I got the initial \"push\" which I need in order to investigate this area further.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. delete deployment tries to drain machines where possible. there is --force flag available that will ignore failures to contact agents.\n\nSent from my iPhone\n\nOn Jun 26, 2017, at 6:35 PM, Andrew Poydence notifications@github.com wrote:\nWhile trying to delete a deployment with several unresponsive agents, we noticed that the delete often failed on the unhealthy instance groups.\nDoes this imply to delete the deployment the director reaches out to the VM before deleting it?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @Kiemes for blobstore access?. we are also enabling parted as a default in upcoming xenial stemcell.\n\nSent from my iPhone\n\nOn Mar 30, 2018, at 10:17 AM, Eric Johnson notifications@github.com wrote:\n@cunnie @dpb587-pivotal - we have confirmation that >2TB disks will force parted and unlocks using XFS on GCP.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @mattcui we have a queued item to update postgres. this one is low/med given that director database runs locally to the director.. @cwb124 have you run through http://bosh.io/docs/cli-env-deps.html?. @barthy1 so we are saying that it works? ive repro-ed this on 262.x version so i am a bit surprised that it wouldnt be a problem on master.. what is the size of the director? (instance type, disk type, etc)\n\nSent from my iPhone\n\nOn Jul 27, 2017, at 6:49 AM, Marco Voelz notifications@github.com wrote:\nSeems like there is a connection to pool_timeout on the director db connections options \u2013 not really sure why that seems to only be relevant in the cloud-config case and otherwise works fine?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. when you reference \"pgpool\", what do you mean by it? \n\nSent from my iPhone\n\nOn Jul 27, 2017, at 7:19 AM, Paolo Stivanin notifications@github.com wrote:\n@cppforlife After multiple tries, I can confirm that setting pgpool to 80 solved our issue on Azure, OpenStack and GCP.\n        L Error: timeout: 60.0, elapsed: 63.44875963\n\n14:16:02 | Creating missing vms: consul_z2/d533090f-cbba-49ef-bea2-4fb1b1fa1658 (0) (00:01:35)\n            L Error: timeout: 60.0, elapsed: 61.342340354\n14:15:47 | Creating missing vms: doppler_z1/2a95cd3a-5a9b-447e-a441-2fcafa5f9a4e (0) (00:01:20)\n            L Error: timeout: 60.0, elapsed: 63.651453009\n14:16:26 | Creating missing vms: router_z1/511d5279-71e2-4f1e-8130-26876246fab0 (0) (00:01:59)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @DRuggeri happy to hear that there is more demand for ipv6. @cunnie and i are working for getting ipv6 working for vsphere and aws. we have a prototype that we've seen it work on virtualbox (same as vsphere setup) and aws and hoping to start submitting prs shortly.. progress is good. we actually have deployed zookeeper release on vsphere\nwith everything (cli, director, vms) ipv6 recently. we've used latest\ndirector and stemcell to do so.\n\nwe are currently writing up a guide how to get that going:\nhttps://github.com/cloudfoundry/docs-bosh/commit/af488501c5d7131c28dc05a743412933205349da\n(ipv6 branch of docs-bosh)\nwhich iaas are you looking for support? we currently handle manual\nnetworking without dhcp (typical configuration for vshere for example). our\nnext task is to get manual networking with dhcp (aws, some openstacks)\nworking.\nOn Fri, Dec 22, 2017 at 11:44 AM, Holger Winkelmann \nnotifications@github.com wrote:\n\nhow far we are with IPv6 now? We have the need to Deploy IPv6 kubernetes\nApplications.Therefore the whole chain BOSH, CFCR (k8s) and IaaS must\nsupport IPv6.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1747#issuecomment-353663625,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV918vHNhAXk3SO0scZoBdTiDLyvycks5tDAaogaJpZM4OqGWM\n.\n. @asmonter i would recommend asking for help in cloudfoundry-incubator/consul-release. @MatthiasWinzeler \nNetwork connectivity between all zones is given, i.e. instances in z1 can communicate with z4\n\nis the network subnet spanning two azs or just one. if so that means that ip will be changing. im not entirely sure if you gonna have any problems with some of the software that doesnt like ip changes.\n\nWe're facing a bigger migration and need to move our existing deployments to a new set of zones.\n\nwhat's the timeline? how urgent?\n. i havent come up with any good solution for this one. technically we still want to see ssh events. should we somehow bucket based on type similar to tasks...?. will be fixed up in next bosh.io update.\nOn Wed, Aug 23, 2017 at 11:41 AM, Goutam Tadi notifications@github.com\nwrote:\n\nThere's a space between \"upload\" and \"release\", when there should be a\ndash. For example:\nhttps://bosh.io/releases/github.com/cloudfoundry/grootfs-release\n0.25.0 \u2014 Details / Download sha1: 9e517f58669c5a4e6d368d607843749b1e10654d\nUpload latest version, currently 0.25.0\n$ bosh upload release https://bosh.io/d/github.com/cloudfoundry/grootfs-release\nUpload specific version\n$ bosh upload release https://bosh.io/d/github.com/cloudfoundry/grootfs-release?v=0.25.0\n0.24.0 \u2014 Details / Download sha1: b342db5da3a7bd7bb2e81c2f6456d75cf38e9e0c\n$ bosh upload release https://bosh.io/d/github.com/cloudfoundry/grootfs-release?v=0.24.0\ncc/ @cjcjameson https://github.com/cjcjameson; sorry if it's a duplicate\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1765, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV91U9iu5x0NcRsM_QghQhC4J1_b0Aks5sbHJbgaJpZM4PAa7l\n.\n. Closed in favor of: https://github.com/cloudfoundry/bosh/pull/1774. @gerhard what do you mean by \"always\" in this case? hooks will run if changes are being made, ie property is updated, etc. anytime director issues monit start.. they do not run when monit decides to restart process on its own (without director's concern).. > Having to go through a create-env after I closed the lid is really\nannoying and definitely a huge degradation compared to the Vagrant solution\nwe had before (which was rock-solid).\n\ninteresting, when im putting laptop to sleep and bring it up after, it\ncontinues to work. did you configure virtualbox to shut off vms on sleep?\ndont we are doing anything differently from vagrant.\nOn Mon, Oct 2, 2017 at 12:07 PM, Steffen Uhlig notifications@github.com\nwrote:\n\nI have this problem all the time (with the most recent VirtualBox).\nWhenever my laptop goes to sleep, the director not unaccessible anymore\nafterwards.\nHaving to go through a create-env after I closed the lid is really\nannoying and definitely a huge degradation compared to the Vagrant solution\nwe had before (which was rock-solid).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1783#issuecomment-333633898,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV987t6MumlCOte978tp1DvsfLAjQJks5soTSJgaJpZM4PZ_ya\n.\n. @petergtz sorry it's taken some time to respond. ive been thinking a bit more about this problem recently and have thought about an interesting feature combination. @voelzmo and team is adding support for generic configs (https://github.com/cloudfoundry/bosh-notes/blob/master/configs-api.md) which should allow to store cloud, runtime, cpi, etc configs in a consistent manner. after some thought i think we can add new type: deployment, to represent deployment manifest updates. eventually i think we should replace manifest command to just use generic config api endpoint. given that generic configs are versioned and store some additional metadata (created at), we will get history effectively for free with recorded manifests. deployment events could record config ids (for deployment, runtime, cloud types etc) in their events. what do you think about this approach?\n\ncc @dpb587-pivotal . > Also, the notes\nhttps://github.com/cloudfoundry/bosh-notes/blob/master/configs-api.md state\nwhat is implemented.\nimplemented there was referring to presence of that type of config in the\ndirector, not it being consolidated. a bit confusing i would agree.\n\nI'm doing this because just having the versioned manifests is not enough,\nbecause it might only refer to release versions as latest. To get precise\ndiffs we need those exact release versions. So I'm wondering how we could\nsupport this. Suggestions?\n\nhmm, that is an interesting question, i wonder if we need to add another\ntype of config, like deployment-processed that would have all that info in\nthere resolved.\nOn Wed, Sep 27, 2017 at 12:55 PM, Peter G\u00f6tz notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife It's an excellent initiative\nand makes perfect sense. I already noticed while implementing my change\nthat part of it is very similar to cloud and runtime config. So I think\ngeneralizing these things avoids a lot duplication and definitely makes\nsense.\nOne thing I'm wondering how we would do it though in your proposal above:\nin my PR, I'm taking special care of the bosh releases (and stemcells), to\nmake sure a deployment revision knows the exact versions of the bosh\nreleases it deploys. I'm doing this because just having the versioned\nmanifests is not enough, because it might only refer to release versions as\nlatest. To get precise diffs we need those exact release versions. So I'm\nwondering how we could support this. Suggestions?\nAlso, the notes\nhttps://github.com/cloudfoundry/bosh-notes/blob/master/configs-api.md\nstate what is implemented. Does that mean it's already on master? Or\nwhat is the exact state. Maybe @voelzmo https://github.com/voelzmo can\nalso shed some light on the current state and the intended timeline. Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1788#issuecomment-332637132,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9yvuUguh91TDu5SEkEGVjvey_Cehks5smqhIgaJpZM4Pc_iy\n.\n. related slack discussion: https://cloudfoundry.slack.com/archives/C02HPPYQ2/p1505896541000151. @sharms we havent seen any high cves related to powerdns that compromise the system and hence it was not proactively bumped. is there specific cve you are concerned about? we are also on the verge of replacing powerdns with bosh-dns which should simplify this mgmt.. @jasonkeene recent versions of cli already kick off multiple upload release tasks in parallel.. closing as i think this happens already.. going to switch to bpm: https://www.pivotaltracker.com/story/show/153189574. going to switch to bpm: https://www.pivotaltracker.com/story/show/153189574. any numbers to compare?\n\nOn Mon, Oct 2, 2017 at 8:00 AM, cfdreddbot notifications@github.com wrote:\n\nHey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients\nof the pull request that you and the commit authors have already signed the\nCLA.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1800#issuecomment-333561006,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9wKz2zaB2f-1sQbJE4NtaKiZRWbhks5soPqUgaJpZM4Pqxbx\n.\n. story: https://www.pivotaltracker.com/story/show/151854582. @Kiemes dont think we can improve monits error.. @Jagarnath123 please reach out to pivotal support for help with ospmgr.. @parthasarathi204 you'll have to use file content as a value and before calling underlying client, make a tmp file for it. though i would strongly recommend to figure out how avoid using file for such content.. @jiaoyan2017 did you end up installing deps from https://bosh.io/docs/cli-env-deps?. @EdwardStudy i would open this issue against softlayer cpi repo (https://github.com/cloudfoundry/bosh-softlayer-cpi-release). since this is a softlayer specific limit, cpi should make sure that values set are truncated. (may be also include ... if it's truncated). google cpi i believe does something similar.. @voelzmo can you please add examples of each type of colocation you have in mind?. story to address errands at least: https://www.pivotaltracker.com/story/show/155197799. > I want to know if we have considered the SPOF of bosh-registry\n\nit's equivalent to the director today since it's on the same machine.\nwe will be removing it shortly to simplify overall architecture.. @voelzmo is this going to be resolved (or improved) thru puma and other perf stories yall been doing?. we'll backport to 3468.x\nOn Thu, Dec 7, 2017 at 8:29 AM, Marco Voelz notifications@github.com\nwrote:\n\n@dpb587-pivotal https://github.com/dpb587-pivotal We would be happy\nwith an officially released stemcell with this fix, either new series or a\nbackport to an older one. We're not married to a specific line of\nstemcells, so we could switch to a newer one, if that's easier to build.\nBackporting to 3468.x is also fine.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1840#issuecomment-350020650,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV99FYtaRlJslsDG8SpdgitghqzFAxks5s-BKFgaJpZM4Qn6Je\n.\n. i take it back, we'll cut a new one.\n\nOn Thu, Dec 7, 2017 at 8:34 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nwe'll backport to 3468.x\nOn Thu, Dec 7, 2017 at 8:29 AM, Marco Voelz notifications@github.com\nwrote:\n\n@dpb587-pivotal https://github.com/dpb587-pivotal We would be happy\nwith an officially released stemcell with this fix, either new series or\na\nbackport to an older one. We're not married to a specific line of\nstemcells, so we could switch to a newer one, if that's easier to build.\nBackporting to 3468.x is also fine.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1840#issuecomment-350020650\n,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/\nAALV99FYtaRlJslsDG8SpdgitghqzFAxks5s-BKFgaJpZM4Qn6Je\n.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1840#issuecomment-350022131,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV90Ye_1MMgJL3_hqQCykVpW5Jf3y9ks5s-BOQgaJpZM4Qn6Je\n.\n. @sauravmndl what i suspect is that cpi expects to have disks with particular names. may be @AbelHu has an idea how to support what you are trying to do? (i assume its some disks that you have to recover). closing. feel free to reopen if more details are found.. > The create strategy would work for us if the old instances get deleted after the deploy is finished.\n\ni dont think above strategy will work since vms that are being replaced are not added temporarily to available resources. (ie only 1 active vm per each instance). \ni think this would be a nice additional strategy to support though.\n@monkeyherder @GarfieldIsAPhilosopher while you are on the current track, i think it would be interesting to discuss briefly how would this other strategy affect things (if at all).. (added to bosh notes: https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/update-strategy-surge.md). cc @emalm . Closing per above note from @emalm.. @friegger yup, looks like a bug. . > Not certain if this has impact beyond cosmetic.\ni believe it's only cosmetic impact at this point.\nim going to keep track of this request in https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/centos-stemcells-improvements-v1.md.. @prashantgnet only latest versions of CentOS stemcells are maintained (currently CentOS 7 3468.x). we believe they contain latest available fixes from upstream.\ndo let us know if you think we are missing any package updates for CentOS.. we have some plans to add ha director support, however, it would be different from what was known as multi vm director. \nha director would involve running multiple copies of the director backed by the same db. in case of one director becoming unhealthy, other director would be able to take over. \nis there particular scenario you are trying to cover by mutli vm director?\nSent from my iPhone\n\nOn Jan 23, 2018, at 9:10 PM, prashantgnet notifications@github.com wrote:\nHello,\nAny plan to add support for full bosh director (multi-vm) using BOSH CLI v2?\nThanks,\nPrashant\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > we are looking for \"multi vm director\" for performance perspective.\n\ninteresting, do tell more. do you want to scale out number of workers, or\nsome other component?\nOn Tue, Jan 23, 2018 at 9:29 PM, prashantgnet notifications@github.com\nwrote:\n\nha director support will be helpful for us. we are looking for \"multi vm\ndirector\" for performance perspective.\nThanks,\nPrashant\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1878#issuecomment-360025263,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9-B31HYyfZZPcMd9QfmjTMk_225_ks5tNr_GgaJpZM4Rqury\n.\n. right, but what performance limitations are you seeing with a single vm\ndirector such that you have to use multi-vm configuration. for example, for\none of our production envs we are running 10 workers, 32 threads\nconfiguration and it seems to work fine.\n\nOn Tue, Jan 23, 2018 at 10:13 PM, prashantgnet notifications@github.com\nwrote:\n\nWe are right now creating full bosh environment (using normal deployment,\nYML file) under single-vm bosh-init based director. Full bosh environment\ncontains following six vms with respective jobs under it along with max\nthreads set to 32 and workers set to 8.\nfull-bosh-1-blobstore\nfull-bosh-1-director\nfull-bosh-1-health_monitor\nfull-bosh-1-nats\nfull-bosh-1-postgres\nfull-bosh-1-registry\nDue to #1715 https://github.com/cloudfoundry/bosh/issues/1715 issue we\nare not able to place jobs on separate vms.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1878#issuecomment-360031479,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV90KqVDwxVtAGXabf2laCuyT1nAWZks5tNsn_gaJpZM4Rqury\n.\n. i would recommend following standard single vm configuration (\nhttps://github.com/cloudfoundry/bosh-deployment) and see if you hit any\nproblems. feel free to open new issues regarding any perf concerns.\n\nOn Tue, Jan 23, 2018 at 10:31 PM, prashantgnet notifications@github.com\nwrote:\n\nhmm...I might be wrong! I am thinking, we will get good performance if we\nplace each job on separate vm along with max threads and workers. I have\nnot compare the actual performance between single vm and multi vm bosh\ndirector but you may close this issue as we can fine tune any performance\nparameters using single vm bosh director. ha support looks very good.\nThanks,\nPrashant\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1878#issuecomment-360034141,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9xaBmqqh4BzESDIQmX137cS6I9Taks5tNs4wgaJpZM4Rqury\n.\n. closing, not enough info.. @chirangaalwis are you running bosh create-env from a jumpbox or from outside of this vpc?. > Is there a better solution provided for sharing network volumes between BOSH instances via a BOSH Release?\n\nshared volumes isnt something that typical iaases provided. your best bet would be to make a job that configures nfs or some other shared fs.. is it trying to auth based on an ip? since 10.244... is a container ip,\nyour host machine will not see it.\nOn Mon, Jan 29, 2018 at 6:05 AM, Chiranga Alwis notifications@github.com\nwrote:\n\nI've been attempting to share a directory which resides within my local\nmachine using NFS with BOSH instances. I am using BOSH Lite\nhttps://bosh.io/docs/bosh-lite as the Director VM.\nI have setup NFS in my local machine (which uses Ubuntu) using this\nhttps://help.ubuntu.com/community/SettingUpNFSHowTo guide. I have\ncreated the following directory structure in my local machine.\nchiranga@chiranga-ThinkPad-X1-Carbon-5th:/$ ls -la /export/\ntotal 12\ndrwxrwxrwx  3 root     root     4096 \u0da2\u0db1   29 16:30 .\ndrwxr-xr-x 29 root     root     4096 \u0da2\u0db1   29 16:33 ..\ndrwxrwxrwx  2 chiranga chiranga 4096 \u0da2\u0db1   29 18:03 users\nchiranga@chiranga-ThinkPad-X1-Carbon-5th:/$ tree export/\nexport/\n\u2514\u2500\u2500 users\n    \u2514\u2500\u2500 file\n1 directory, 1 file\nI have provided 777 permissions to the /export and /export/users\ndirectories. This is how my /etc/exports file looks like.\n/export       10.244.15.2(rw,fsid=0,insecure,no_subtree_check,async)\n/export/users 10.244.15.2(rw,nohide,insecure,no_subtree_check,async)\nPlease note that 10.244.15.2 is the IP address of one of my job VMs.\nAlso, I install nfs-common package in my job control script.\nAfter logging into the job VM, I attempt to manually mount the directory\nto my job VM.\njob_1/5b721203-b05b-46e9-bb99-e3faff5fbbd6:~$ sudo mount -v -t nfs -o proto=tcp,port=2049 192.168.50.1:/users /mnt\nmount.nfs: timeout set for Mon Jan 29 13:28:26 2018\nmount.nfs: trying text-based options 'proto=tcp,port=2049,vers=4,addr=192.168.50.1,clientaddr=10.244.15.2'\nmount.nfs: mount(2): Permission denied\nmount.nfs: access denied by server while mounting 192.168.50.1:/users\nBut I tend to get an mount.nfs: access denied by server while mounting\n192.168.50.1:/users. When checking the mount information of the host\nmachine from the job VM, it shows up the export list as follows:\njob_1/5b721203-b05b-46e9-bb99-e3faff5fbbd6:~$ showmount -e 192.168.50.1\nExport list for 192.168.50.1:\n/export/users 10.244.15.2\n/export       10.244.15.2\nI tried manual mounting of the same directory to an Ubuntu 16.04 machine\nspawned using Vagrant and it mounted successfully.\nPlease correct me if I am wrong but, how can I mount a shared directory\nusing NFS to the job VMs (containers) running within the BOSH Lite Director\nVM?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1882, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV944lA5wJFBuPN-DgAnQcLiwzqrPQks5tPdAkgaJpZM4RwpVJ\n.\n. i believe you want to allow vm's ip address instead of a container's ip as container ip is not meaningful outside of the vm. if you ran create-env with internal_ip=192.168.50.6, try using 192.168.50.6 for allowing access.. > A cloud server could be a container, VM or even a physical machine (as defined here), depending on the infrastructure.\n\nyup\n\nHence, I don't see how I can achieve the target by simply mounting the directory to the Director VM in the case of BOSH Lite.\n\nim not suggesting to mount it to the director vm. you still have to mount it in the container (via a job); however, you have to consider network topology thats particular to each \"iaas\". in bosh-lite case everything is nat-ed hence nfs server will not see container ip, but will see \"bosh-lite\" vm ip as the client. for authentication reasons i believe you have to configure server to allow director vm ip to access mount even though client runs in a container.\n. @geofffranks BOSH_CLIENT/SECRET are used with uaa clients (oauth client credential grant). interactive login uses regular uaa users (oauth password grant). regular uaa user login may include sso type questions send from uaa, so it cannot be done consistently in non-interactive mode, hence non-interactive mode only allows uaa clients.. @nsharmacovs are you referring to ssh integration with ldap?. todo: need to figure out how to make this change in a backwards compatible manner.. @VincenzoDo that's a really weird error (just 403). it appears that that portion of the code is trying to login into vcenter [1]. may be you have wrong creds?\n[1] https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/blob/master/src/vsphere_cpi/lib/cloud/vsphere/vcenter_client.rb#L19\n. @spunky11 wihtout a more concrete error message its hard to help. i would recommend following http://bosh.io/docs/bosh-lite to get a feel for it and then extending it to cf.. @muralisc returning this information from cpi is not necessary, since director should already know what it sends to the cpi. you should be able to grab it from persistent disk model (holds disk size, and disk cloud properties in its db table). manually merged.. > You may also want to check out bosh-deployment as an alternative to\nmanaging your own manifest.\nstrongly recommended\nOn Mon, Mar 12, 2018 at 2:26 PM, Danny Berger notifications@github.com\nwrote:\n\nHi - the postgres job was removed in v262\nhttps://github.com/cloudfoundry/bosh/releases/tag/v262, and you will\nprobably want to switch to postgres-9.4. I would recommend reviewing the\nrelease notes, as there are other breaking changes since the version you\nare upgrading from.\nYou may also want to check out bosh-deployment\nhttps://github.com/cloudfoundry/bosh-deployment as an alternative to\nmanaging your own manifest.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1909#issuecomment-372468298,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV9_ZxnPYliB8R8V_4TPTdXI2SPIAGks5tdugbgaJpZM4SmtBd\n.\n. @nsharmacovs rotation is done by cloudfoundry/bosh-dns-release out of the box. retries i believe are also done.. @challa team creation is based on UAA scopes (bosh.teams.X.admin where X is a team name). as long as you provision UAA users/clients with scope in that format (or formats listed above) Director will create a team.. > The stop script for rabbitmq-server job doesn't get called. This is on bosh-google-kvm-ubuntu-trusty-go_agent/3541.9 & 264.7.0.\n\nhave you observed this problem on other stemcell lines? eg 3468.x?. @gerhard i m not sure, i wouldnt mind pairing with you to debug this.. @muralisc would you mind signing cla or showing sap github org (sap-cloudfoundry).. @muralisc @ashishjain14 i think i misunderstood the change that yall did. in the above change, it appears that we are just copying metadata from an unrelated disk (previously used disk) to the new disk. i dont think that this makes sense as these two disks are not related. \nwhat i was proposing in one of the issues is that we copy orphan's disk metadata onto new disk, but i see that already happens correctly in the case when disk is \"unorphaned\": https://github.com/cloudfoundry/bosh/blob/ce5b73ff8d821237cb614392d78154bf3e70dc7b/src/bosh-director/lib/bosh/director/orphan_disk_manager.rb#L50-L51\n@voelzmo what do you think about this change? should we do it?. @voelzmo im still not not convinced that this is somethign that could be done well in drain scripts in this particular case given that other components have to keep on running for this to be useful. what if nats job introduces drain script, etc.\ni think a better solution is to have a CLI command that would rely on https://github.com/cloudfoundry/bosh/pull/1863 to stop picking up new tasks. once all tasks are done then this command could continue with delete-env. one update is done, command would unblock and director would start picking up enqueued tasks.. @keithkroeger \n\nIt looks like BBR is the way to go here. Is that the preferred option? I've seen references to bosh backup but this option doesn't seem to be available in bosh v 2.0.48\n\nyup, bbr is the preferred option. we have opted into not keeping bosh backup command as it was not anythign else on top of bbr functionality.. @whgibbo we currently only support vcenter api via vsphere cpi (https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/). one could write a pure esxi cpi, but im not aware of anyone who's done it already.. closing for now. feel free to reopen.. btw later versions of vsphere cpi should eliminate this problem.. @jmcarp is this from master or already published director version?. > I suspect this problem has to do with the deployment size, it's a very large deployment containing thousands of VMs, we don't see such problem on a smaller deployment (like hundreds of VMs).\nyeah typically we recommend to have many smaller deployments (with couple of hundred of vms).\n\nWe experienced a problem where bosh director returned 500 or 502 error sometimes when bosh-cli \n\ncan you see what's goign on in the debug logs. what is the process doing?\n. cc @maximilien. (asked IBM folks to try to add retry to CLI for 502 and see if it would be enough).. @mattcui \n\nThe problem I mentioned in above comment should be a known issue, we are going to increase postgres.max_connections to 200 in our environments\n\nnote that we bumped postgres max connections default to 200 some time ago, in 263.x and above.. story: https://www.pivotaltracker.com/story/show/156740776. bosh start --hard as jamil mentioned is the official way for temporarily stopping vms. i would recommend  opening issue with a particular release or deployment (like cf-deployment) if it doesnt gracefully stop. \nfeel free to also reopen this issue if you have more info about trouble with stop --hard.. @svrc-pivotal could you include your manifest to have a more details? i could think of a few ways to tackle these kinds of issues but that typically requires a bit more surrounding context for more effective suggestion.. > Need to understand why components/instance (bosh/0, jumbox/0, bbl-env-saimaa-2018-04-02t10-13z-nat) have been created in aws infrastructure when executing the below command\nbbl up is what creates these components. it configures env to work in a secure by default manner. is there a particular question about this setup you have? @evanfarrar @genevieve from the bbl team may have more info.. > Is there a reason why the analyze_agent doesn't check the expects_vm? state like analyze_instance does?\nif that information is available i dont see a reason why not.. > You can use bosh delete-deployment --force but VMs created by the IaaS will remain\ndid you dig into why that would be the case? delete deployment will call delete vm but sounds like ot receives some error.\nSent from my iPhone\n\nOn Apr 13, 2018, at 7:47 AM, Beyhan Veli notifications@github.com wrote:\nYou can use bosh delete-deployment --force but VMs created by the IaaS will remain\n. cc @xtreme-sameer-vohra . @arthfl \nbut the following bosh create-release didn't upload anything, although it exited with \"Succeeded\".\n\nit succeeded because we currently dont check whether blobs exist if we already have records for them. it would too expensive to check all blobs everytime (might be 1000s).\n\nAlso changing the blobstore should be supported, i'm probably not the only one who uses the local filesystem blobstore for early development stages and wants to change to something like S3 when going final.\n\nyeah, i sometimes do a similar thing when developing.\nmarked as a feature request.. > IIRC, there should be a story which allows packages of the same name from different releases even if fingerprints aren't the same. Does that sound like a story in your backlog @cppforlife?\nwe do want to support such configuration, but it's not that high in priority at the moment since most of the time renaming does the job.. (also, somewhat invasive problem to solve unfortunately).. > According to the release notes it should work\nthis would only work when fingerprints and their dependencies match perfectly. that could be achieved via vendor-package cmd.. on master we've switched to bpm. i see:\nbosh/0:/home/jumpbox# cat /var/vcap/sys/log/director/nginx.err.log\nnginx: [alert] could not open error log file: open() \"/var/vcap/packages/nginx/logs/error.log\" failed (30: Read-only file system)\nit's probably some default configuration of nginx before it gets reconfigured to use error_log. @cunnie do you know if this is a similar problem in nginx-release?. @antonsoroko is there a better rule you can write, potentially using teams selection? (you can find teams via bosh deployments cmd). @robtec one suggestion i've been giving to people is to take advantage of https://github.com/bosh-prometheus/. to implement rule above you would have prometheus cluster with node exporter colocated on all nodes. then you can use alert manager to set up an alert in actually almost same syntax as you imagined above.. > Is this a feature in your backlog/icebox?\nnope, just recorded here for now.. @dprotaso i could see adding a way to do so. not currently possible since each template is being rendered in isolation in memory.. @Gerg going to think more about it: https://www.pivotaltracker.com/story/show/158377279. @Onke we've decided to remove these packages from the stemcell as they are not needed by 99% of releases (we've been planning to do so for some time now but just did not want to do it on trusty stemcell). more targeted solution here is for capi-release (or some additional release) to include nfs bits instead of relying on the stemcell to have them.\ncapi team is aware of this change in xenial stemcell and will be working to address it at some point. i would recommend reaching out to them via slack or filing issue against their repo.. i would recommend writing a little script that does that (since we dont\nhave --follow option for now in the cli), or just going into the database\nand running that query.\nOn Mon, Jul 2, 2018 at 2:15 PM, Aaron Gershman notifications@github.com\nwrote:\n\nThe more I think about it, the more I think using events might not be the\nbest approach to what I actually need to get done.\nWhat I need to do is pull the total number of VMs which were created /\ndestroyed. Every single one. I know this sounds a bit silly, as the total\nnumber of VMs churned over doesn't really matter that much. But I could\nreally use the data to impress some of the more, ah, \"old school\" members\nof the corporation I work for. I fully understand that in BOSH land, \"total\nVMs churned\" is not particularly impressive, but considering there are\npeople I have to debate with who are doing manual VM\nprovisioning/upgrading, to them this seems like an impressive metric & it\nwould be nice to be able to shove it in their face.\ntl;dr, this is for demonstrating the value\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1992#issuecomment-401938802,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALV900QLnzpGEvo8jDOCoVNK_b002Qvks5uCo2AgaJpZM4U4-2C\n.\n. > (Note, that I must have 2 separate networks because my datacenters use 2 different CIDR ranges) Because of the fact that I am using 2 different networks\n\nyou should be able to define network so that it covers multiple ranges spanning azs. for example:\nnetworks:\n- name: net1\n  subnets:\n  - range: 10.0.0.0/24\n    azs: [z1]\n    ...\n  - range: 10.20.0.0/24\n    azs: [z2]\n    ...\nwith above configuration you should be able to keep for now same instance group.\n\nThere are some other instance groups that need to consume mysql link from both mysql_dc1 and mysql_dc2. It would be nice If will be able to do this. \n\nregardless of my suggestion above it would be nice to be able to combine links, however, because links carry properties there is no easy way to merge them together. for example: one instance group may use one port for mysql server and another instance group could use another.\nthere might be interesting special cases to the above behavior:\n- all properties are the same, so just take the first copy (might be quite common case)\n- there are no properties on the links so nothing to merge in the first place\nadditionally with dns integration, there is a question of how to expose singe dns address for the group of instance groups (it's doable but we are not quite there yet).\nanother way to tackle this problem is to support consuming multiple links as an array with main disadvantage that job has to know about consuming multiple links instead of usual one. this behaviour would be useful for other cases though.. Better to use e.insect instead of message to include err class name\n. At this point this method should just be split into two upload_local _stemcell and upload_remote_stemcells since 2nd param is provided just for that.\n. Any reason not to call it 'location' in the payload since we are posting to stemcells resource.\n. Do we set global ssl VERIFY_NONE option somewhere?\n. Probably unless director is set up to handle verification properly. Self signed certs are still a problem even then.\n. context should reflect infrastructure name.\n. looks like above block is not properly indented\n. context should reflect infrastructure.\n. this let is not necessary since you can just access infra\n. if we are going to use vsphere based class in here, better approach is to delegate instead of re-opening vsphere class. e.g.\n``` ruby\nrequire 'forwardable'\nrequire 'bosh_agent/infrastructure/vsphere'\nmodule Bosh::Agent\n  class Infrastructure::Vcloud\n    extend Forwardable\ndef initialize\n  @vsphere = Infrastructure::Vsphere.new\nend\n\ndef_delegators :@vsphere, :load_settings, :get_network_settings\n\nend\nend\n```\n. indent (this and couple of cases above). better is to break it up like this:\nruby\nexpect(stage_collection.all_stages).to eq(\n  centos_stages + agent_stages + vcloud_centos_infrastructure_stages)\n. parenthesis in this file.\n. change error message to mention vcds e.g. 'Must have exactly one VCD'\n. use e.inspect so that class name of exception is shown with message\n. itemd -> items. also extra space at the end of the sentence.\n. use 'nats' for password. we will fix that for vsphere BatDeploymentManifest.\n. line up arrow\n. move initialize_client_and_vdc inside clean method since it tries to do actual vdc finding and not just object initialization.\n. is there @vdc.vapp_exists? similar to @catalog.catalog_exists?\n. use instance_double('VCloudSdk::VDC') instead of double to strengthen assertions. same for lines below.\n. context should be infrastructure specific.\n. instead of duplicating vsphere stemcell building stages to image_vcloud_ovf, image_vcloud_prepare_stemcell, image_vcloud_vmx please make existing vsphere stages more generic like so:\nStages:\n- image_vsphere_ovf becomes image_ovf_generate\n- image_vsphere_prepare_stemcell becomes image_ovf_prepare_stemcell\n- image_vsphere_vmx becomes image_ovf_vmx\nVariables:\n- $work/vsphere becomes $work/ovf\n- image_vsphere_ovf_ovftool_path becomes image_ovftool_path\nAfter doing that the stemcell builder runner which lists which stages to run needs to be adjusted to use new stages.\n. why do we return bool if we do not use it?\n. missing test cases for error conditions. (more like this below)\n. extra line to remove\n. add a logging line?\n. add empty line before this one\n. make sure that err is not returned from SetupDhcp (also in tests below)\n. extra line\n. wrong description for the test\n. please inline these consts into appropriate It-s so that expectation values are closer to the their tests\n. extra line\n. remove comment\n. remove comment\n. use &:can_run_as_errand?\n. remove comment\n. combine map into a single line\n. should be moved to bosh_cli/lib/cli/client/errands_client.rb\n. move to errands_client_spec.rb\n. unindent\n. pull out consts in this class\n. unindent\n. add error message (e.inspect) to the summary\n. 2 spaces\n. remove logger.\n. remove curlies\n. slow down test\n. inline\n. remove new line\n. remove parens (more ruby-ish style)\n. no parens\n. no parens\n. it would be nice to include agent to allow people to bootstrap full bosh system\n. Agreed. We will change it when we pull this in.\n. @mikedillion what happens when resource_pool['scheduler_hints'] is nil? Does fog/openstack allow that value?\n. I dont think we can do that because CLI plugin does not depend on bundler. Tests happen to pass because they run in bundle exec?\n. Bundler classes are not available.\n. convention is to use snake case\n. use #{delete_server_error.inspect}\\n#{delete_server_error.backtrace.join(\"\\n\")} to get the class name and the backtrace of the error\n. no need to keep local var c\n. Why not do?\ngem 'serverspec', '0.15.4'\nSame below.\n. Seems like chunk above is commented out. Remove?\n. nothing to do with cf.  BoshLogTemplate?\n. inline <%= p('hm.syslog_event_forwarder_enabled') %> and remove properties.sh.erb.\nedit: in fact let's do something line this:\n<% if p('hm.syslog_event_forwarder_enabled', false) %>\n  /var/vcap/packages/health_monitor/bin/setup_syslog_event_forwarder.sh $CONFIG_DIR\n<% end %>\n. service should be on the path. no need to specify full path.\n. add default value here e.g. p(\"hm.syslog_event_forwarder_enabled\", false)\nuntil we switch to bosh-init defaults are not respected by bosh micro deploy.\n. extra line\n. subject(:plugin) { described_class.new.tap(&:run) }\n. ifconfig should also be on the path.\n. shouldn't this be spec.name?\n. spec.index is already an integer afaik.\n. Fair enough.\n. You can only do a merge on a Hash, this was an array.\n. You may have to create a mock client off of region which has register_image method stubbed out. It should return image_id and is one of the images in the region.images.\n. indention is funky.\n. Agreed.\n. @xingzhou Maria meant to put that comment in the code.\n. Instead of making an extra guaranteed call to the blobstore (exists?), let's just catch not found error from delete and log then.\n. same here.\n. i dont think =nil is necessary since we always call the method with 3 params. also may have to adjust tests to call with 3 params.\n. same as above.\n. change to:\nRelease SHA1 '#{release_hash}' does not match the expected SHA1 '#{sha1}'\n. change to:\nVerifying remote stemcell\n. Let's verify remote release before extracting it.\n. Let's also verify sha1 even if it's a locally uploaded tarball -> if sha1 (just like stemcells)\n. change to:\nStemcell SHA1 '#{stemcell_hash}' does not match the expected SHA1 '#{@stemcell_sha1}\n. whitespace\n. change to BOSH_CACHE_DIR\n. change to:\nExpected SHA1 when specifying remote URL for release `#{release[\"name\"]}'\n. change to:\nInvalid URL format for release #{release['name']}' with URL#{release['url']}'. Supported schemes: file, http, https.\n. space...\n. no need to quote sha1\n. similar error msg change to above.\n. no need to quote sha1\n. similar error msg change to above.\n. sha1 of the remote release -> SHA1 of the remote release\n. sha1 of the remote release -> SHA1 of the remote release\n. space\n. space\n. BOSH_CACHE_DIR\n. BOSH_CACHE_DIR\n. @xingzhou can we check on response code (404) inside the delete_object and raise NotFound? \n. Let's do that:\n- local seems to already handle that\n- s3 is almost there except it does not raise NotFound (how come delete_object does not use AWS::S3::Errors::NoSuchKey error just like get_object?)\n. should colorize be set to true when BOSH_COLOR==false?\n. Does this break command look up?\n. Does bosh ssh actually work after this change?\n. i think this and 10 or so properties after this one should also be removed.\n. i dont think agent cares about NetworkingType anymore.\n. why do we need rootdelay? what about earlyprintk?\ni believe we set console=ttyS0,115200n8 configuration for openstack stemcells. i think we can always configure it for all stemcells.\n. If you grep bosh-agent source code, there is no NetworkingType. May be you want to set use_dhcp flag on each network: https://github.com/cloudfoundry/bosh-agent/blob/master/settings/settings.go#L124 like AWS CPI does: https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/blob/ec62d78c16c857f1a6538a9365b4c8f3204d0a57/src/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L784\n. need to add sha sum verification\n. where do we delete tmp file for the agent?\n. let's just call delete and ignore blob not found call.\n. what cleans up file created by blobstore.get(blobstore_id)?\n. oops forgot to mention. let's make sure to log to debug about failure ignore. i believe we do similar thing in other places.\n. Please include blobstore_id and e.inspect in the error log.\n. why was the separator removed? we still want to the separator between instances if --ps is specified.\n. what if there are no processes?\n. what if there are no processes?\n. still want to keep this for older agents.\n. fair enough.\n. (Hmm. I lost my prev comment...)\nGiven that we skip bosh_micro_go stage we can also skip bosh_ruby which also means we dont have to make it work on ppc. Wdyt?\n. Can we pull this (RbConfig::CONFIG['host_cpu'] == 'powerpc64le') out into some constant?\n. use $DISTRIB_CODENAME like below for non-ppc?\n. why do we need to include src deb locations?\n. should we just remove all of these comments?\n. is there a global config.sh so we can set stemcell_arch=ppc64el and use it instead of checking uname everywhere?\n. could always run pkg_mgr install \"rsyslog rsyslog-relp rsyslog-gnutls\" and then install conditionally rsyslog-mmjsonparse\n. shouldnt be checking for ubuntu vs centos. we support other os-es like photon and rhel. what can we do to keep this file as generic as possible?\n. extra line.\n. can we not run rsyslogd at all to check a version?\n. i thought you guys mentioned that restrict 127... causes ntpd to fail actually fetching time from remote servers?\n. any reason to use ntpd packaged as an asset instead of installing it from an appropriate package manager?\n. should this be set everytime we enqueue?\n. did you run the tests? this line would fail.\n. agent.json below contains:\n{\n  \"Platform\": {\n    \"Linux\": {\n      \"CreatePartitionIfNoEphemeralDisk\": false\n    }\n  },\n  \"Infrastructure\": {\n    \"Settings\": {\n      \"Sources\": [\n        {\n          \"Type\": \"File\",\n          \"SettingsPath\": \"/var/vcap/bosh/user_data.json\"\n        }\n      ],\n      \"UseRegistry\": true\n    }\n  }\n}\nand tests assert on things that are not there. can you email me full stemcell log? cc @maximilien \n. wouldnt this result in never deleting any vms? did you run this in your env?\n. Right so if enable_virtual_delete_vm=true then there is no way to delete VM ever. bosh delete deployment command will not delete VMs. I thought you actually wanted that.\n. - use do-end syntax for multiple lines\n- also add a test\n. - please add unit test\n. to clarify unit test should be sufficient here. see vm_creator_spec.rb example.\n. lets add checksum for these binaries.\n. single |?\n. let's use Bosh::Common::DeepCopy to make sure old_network_settings arent modified.\n. does this work when :job is empty (needed for entire deployment)?\n. glob is actually '/var/vcap/jobs/*/bin/bosh-monitor/*'. let's also send alert. seems a bit weird to cast it to string given that some of these values are ints: https://github.com/cloudfoundry/bosh/blob/c6c419f9eff844c35c8c7307e977427f05a23a51/src/bosh-monitor/lib/bosh/monitor/events/heartbeat.rb#L107. allow alerts as well.. i see. seems reasonable then.. not sure if we need to turn it back into integer: https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/api/event_manager.rb#L12 ?. 404?. what is sources.list is going to be?. what's going on with auditd? does it not start?. if it works i guess not. im somewhat surprised.. let's only rescue particular error, i think it's NotImplemented. i dont think we should do this since task may be already in progress.. @knm3000 i was just thinking more about this, should we have a new controller: /tasks_configs which provides GET and PUT with a single value {\"paused\": true|false}. that way endpoints are a bit more symmetric.\ncc @tylerschultz \n. \"paused\" shouldnt be a state, but rather inactivity of a queue.. dont forget about updating this for a looser check on where pam_cracklib.so lives, to avoid branching.. sounds like majority of people suggested to rename to \"/task_configs\". let's also add 'get /task_configs' and make it run similarly to runtime_configs.. @dpb587-pivotal i would say not for this case.. let's make it VM instead of vm in UI text. why not apt-get install ruby? http://bosh.io/docs/cli-env-deps.html. let's rename resurrection to rules. i've updated bosh-notes as well: https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/resurrection-config.md.. let's rename tasks key to rules. i've updated bosh-notes as well: https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/tasks-config.md.. let's change this to be director.networks.enable_cpi_management (director.networks is more consistent with director .disks). src/bosh-director/bin/dummy_cpi_config file shouldnt exist afaik.. let's not add network lifecycle info to /info. we are trying to cut down on stuff included in /info.. this check has a race with other deployments. i think we would need to add lock around network change. see release/deployment locks for example.. let's hide logic about being managed into ManualNetwork object. should only check if it's managed? which would make managed_network? unnecessary since we can just inline network.managed?. we should also send netmask similar to sending it in create_vm call (https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/deployment_plan/manual_network.rb#L56). this method is way too big. let's split looping over networks vs network creation. let's also create an object called ManagedNetwork#initialize(network) that would also have create method that covers indivdual network creatino.. remove empty space. delete comment?. empty line. space. dont think we currently limit network names.. should be create/delete_network. since we wont be including it in /info, env command should not contain this info.. if we can turn this into positive assertion, it would be more reliable in future. eg invocations =~ ['create_vm', '...'] (doesnt include network related cpi calls).. we should probably grab create_network result value in this test and use it later, instead of relying on create_vm call since that's what under the test below. \nalso we could probably combine previous test with this test.. probably should raise an error on parsing cloud config.. test should be modified to have different cloud properties and ranges so that we can make assertions that each create_network cpi call receives correct subnet details.. ",
    "yudai": "I'm sorry. I realized that the reason why the files ware broken was I did not close them before copying them.\nAnyway, flushing the buffer do not bring in any trouble, and I hope it would help careless users like me.\nThanks you.\n. Thank you for the comment.\nYes, we can exclude .git files by adding negative globs to specs. I understand that my proposal is adding a special care only to git submodules and it might bring in some inconsistency.\nHowever, I think this special care to git submodules is natural to BOSH because BOSH itself is build on the top of Git and submodules are very common way to include src files. In addition to that, I, personally, don't want to add negative globs to each specs.\nThank you again for the comment. We can use your idea until we get Git 1.7.10.\n. Closing because this commit does not support git 1.7.0.4 (lucid default), which did not support separated .git directories.\n. What would happen when releases has a package whose name is included in multiple releases? It's not allowed for now to collocate jobs that has conflicting packages?\nI prefer /var/vcap/jobs/RELEASE/NAME style names as it looks user-friendly.\nBy the way, I believe that release maintainers must not use hard-coded '/var/vca/jobs/directory names in their packaging, config and ctl scripts. We should provide an environment variable that users can use in these scripts for future changes like changing the root directory from/var/vcapto/var/bosh`. The current scripts in cf-release are very weak to this kind of changes.\n. Hi,\nAny progress or comments for this PR? I think this is a somewhat critical bug.\n. ",
    "Kaixiang": "It's tested and I do create instances with io1 disk attached. see my env here\nhttps://console.aws.amazon.com/ec2/home?region=us-east-1#s=Volumes\nI put iops option in registry when creating instance(so can be pass from cloud properties), and read the registry for instance while create_disk given an instance_id.\nwe only need iops disk attached to some instance like services right? so instance_id is always valid while doing that.\n. and the aws setup is shanghai\n. Thanks Martin... I see.\nhere is another attempt https://github.com/cloudfoundry/bosh/pull/106 \n. since we don't have powerdns running for bosh-lite and there is no need for it (right now).\nso the dns is disabled, we want the dns related test pending, and bat already have this mechanism but just broken.\nthink this is a general bug for the BAT test, so sent a PR. \n. Thanks Nic, understand that, bat test is smart enough, it go ask director for it's feature and store it in info['features'].\nthe func dns? is a test for whether the dns is supported in the director. if no, it will pending some DNS related tests(3 in total).\nbut the thing is the dns status should be stored in info['features']['dns']['status']. it's a boolean value.\nhowever, bat test use info['features']['dns'] to test it, which is a Hash. so it appears true always, which we think it's a bug/typo for BAT.\n. Yes, the dns issue remains with latest director(1.2005.0)\n. Thanks @st3v ! I do learn something from the comment and the link.\nyes you are right , I fixed that commit as your suggested and it seems working. (https://github.com/Kaixiang/bosh/commit/08edced05048b7b3d730005d309df01b9ab012ee).\nBut never mind , I see your new implementation with timestamp which is much more clearer. \n. awesome! Thanks!. ",
    "hiremaga": "Coveralls looks cool - why'd you close this @pmenglund? Should we add this to Tracker for @gabis to look at?\n. Coveralls looks cool - why'd you close this @pmenglund? Should we add this to Tracker for @gabis to look at?\n. @drnic Come find me if I don't remember to come over and say hi on Monday :)\n. @drnic Come find me if I don't remember to come over and say hi on Monday :)\n. @pmenglund Makes sense, thanks.\nI've dropped a chore in our icebox so a pair can take another look at Coveralls, if you don't get to it first that is :)\n@drnic After Travis is green again of course.\n. This pull request was created before we setup automatic Tracker story creation for pull requests. I've just created manually a story to review and then hopefully merge this.\nThere's a very large list of stories for @gabis to review this week, hopefully she'll get a chance to get to this one soon :)\n. This pull request was created before we setup automatic Tracker story creation for pull requests. I've just created manually a story to review and then hopefully merge this.\nThere's a very large list of stories for @gabis to review this week, hopefully she'll get a chance to get to this one soon :)\n. @hatofmonkeys - thoughts?\n. @hatofmonkeys - thoughts?\n. @drnic oh c'mon - we haven't even met yet, please be kind :)\nWe get notified when Travis breaks via CCMenu and the numerous radiators you've seen all over our office. And for what it's worth we've recognized at our standup and retros that we need to get better at treating red builds as line stops.\n. @drnic :) \nI spent a good portion of last weekend watching your presentations, thanks for those and everything else you've been doing.\n. Not sure I follow James. BOSH currently uses an .rvmrc file.\nFor context we have 2 proposed stories in our Tracker icebox: to change to a .ruby-version file & separately to update to 1.9.3-p392\n. API docs for the director sound like a great idea. How good are Swagger docs at remaining fresh?\nI've had luck with rspec_api_documentation on past projects. The nice this with that is the docs are generated by specs so are less likely to get stale.\nSwagger does look really pretty :)\nHave you had much experience using Swagger?\n. Right, the nice thing about rspec_api_documenation is specs fail if they\nget stale.\nI'm personally much more likely to keep specs upto date than source\nannotation.\nI suppose we could achieve something similar with swagger by writing a\nsuite of specs that hit the docs?\nEither way docs > no docs :)\nOn Monday, June 3, 2013, Scott Frederick wrote:\n\nI've looked into using this for documenting the Cloud Controller API, and\nI'd still like to make that happen. I'm in favor of using it as a way to\ndocument CF APIs in general.\nSwagger docs would remain as fresh as the meta-data that feeds into them.\nAnnotating source code is the way to go, as it makes it more likely that\nthe docs will stay fresh as the code is updated.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/237#issuecomment-18845494\n.\n. Thanks for jumping on this Ferdy.\n\nDoes splitting the specs result in more frequent output because our specs run in parallel?\nI'm might struggle to like remember this, I wonder if there's a way to keep the specs together while also making Travis happy. Either by speeding up the specs or logging a pulse while we're waiting for whatever is taking so long perhaps.\nSomething to circle back to now that the build is green again?\n. We'd like a pair to take a closer look at Swagger first and see if it's something the team's comfortable living with in the long run.\nWe'll aim to loop back to this PR once that has happened.\n. We'd like a pair to take a closer look at Swagger first and see if it's something the team's comfortable living with in the long run.\nWe'll aim to loop back to this PR once that has happened.\n. Sounds good Matt.\n. Sounds good Matt.\n. @mstine and I paired on reviewing this pull request today.\nWe feel that source2swagger, being at version 0.0.2, isn't an appropriate runtime dependency for the director.\nWe'd like to close this PR for now but leave the conversation about ways to document the director API open (perhaps take this to the bosh-dev list?)\n. What's your current workflow minus this change?\n. What's your current workflow minus this change?\n. Moar!\n. Moar!\n. :+1:\nSniffing around the affected code I see a great candidate for some \"Replace Conditional With Polymorphism\" treatment.\n. :+1:\nSniffing around the affected code I see a great candidate for some \"Replace Conditional With Polymorphism\" treatment.\n. Maybe this is part of the debt @pmenglund was referring to in an another conversation. Looks easy enough to extract a BlockDevice or something similar to avoid the duplication.\n. Maybe this is part of the debt @pmenglund was referring to in an another conversation. Looks easy enough to extract a BlockDevice or something similar to avoid the duplication.\n. @frodenas glanced around Tracker and couldn't see a story for this. Does one need to be created?\nThe current assumption is that all your PRs will be related to open stories so we aren't automatically create new ones.\n. @frodenas glanced around Tracker and couldn't see a story for this. Does one need to be created?\nThe current assumption is that all your PRs will be related to open stories so we aren't automatically create new ones.\n. Perfect, thanks.\n. Perfect, thanks.\n. Thanks so much for taking the time to put this together @stefanschneider! I have a few thoughts about how we might move forward with this.\nFor starters, we'll definitely need to find a way to make the tests pass on Travis. Perhaps by using something like VCR?\nOrthogonally - while this does look useful, I wonder if it's an example of something that should live as an external extension rather than part of BOSH itself - bosh-blobs-azure perhaps? \nI understand this flys in the face of precedent. In thinking about your pull request, I've encountered a meta question about how BOSH's infrastructure specific code should be organized.\nIn part this is driven by a need for sustainable CI. At Pivotal we have a Jenkins based CI build for BOSH (currently private, we'd like to publicize this eventually) which includes system tests for each infrastructure. This currently takes well over 3 hours to complete and is getting slower as we improve system test coverage. That's < 8 runs in a day right now, best case.\nThoughts?\n. Thanks so much for taking the time to put this together @stefanschneider! I have a few thoughts about how we might move forward with this.\nFor starters, we'll definitely need to find a way to make the tests pass on Travis. Perhaps by using something like VCR?\nOrthogonally - while this does look useful, I wonder if it's an example of something that should live as an external extension rather than part of BOSH itself - bosh-blobs-azure perhaps? \nI understand this flys in the face of precedent. In thinking about your pull request, I've encountered a meta question about how BOSH's infrastructure specific code should be organized.\nIn part this is driven by a need for sustainable CI. At Pivotal we have a Jenkins based CI build for BOSH (currently private, we'd like to publicize this eventually) which includes system tests for each infrastructure. This currently takes well over 3 hours to complete and is getting slower as we improve system test coverage. That's < 8 runs in a day right now, best case.\nThoughts?\n. > Are there other blobstore plugins for the director and for the CLI yet; or is this a new idea?\nProbably a new idea from a BOSH newb :)\nI see what you mean about releases. Bummer - makes interesting exploration like this a bigger commitment.\n. > Are there other blobstore plugins for the director and for the CLI yet; or is this a new idea?\nProbably a new idea from a BOSH newb :)\nI see what you mean about releases. Bummer - makes interesting exploration like this a bigger commitment.\n. Builds on previous work by @pmenglund in https://github.com/cloudfoundry/bosh/pull/158\n. Builds on previous work by @pmenglund in https://github.com/cloudfoundry/bosh/pull/158\n. Seems like there's a bit of work needed to figure out how to configure Coveralls correctly given that we have multiple test suites. The 95% that it has reported in the past seems a little on the high side and might also be only the director?\n@mmb and I think it's best to postpone introducing Coveralls. We'll try to keep the change to avoid colliding with colorize though.\n. Seems like there's a bit of work needed to figure out how to configure Coveralls correctly given that we have multiple test suites. The 95% that it has reported in the past seems a little on the high side and might also be only the director?\n@mmb and I think it's best to postpone introducing Coveralls. We'll try to keep the change to avoid colliding with colorize though.\n. @borovsky For the most recent commits - how have you established confidence that there haven't been any regressions?\nHave re-run the affected bosh aws commands manually?\nHave you run the AWS BAT suite?\nThanks.\n. @borovsky For the most recent commits - how have you established confidence that there haven't been any regressions?\nHave re-run the affected bosh aws commands manually?\nHave you run the AWS BAT suite?\nThanks.\n. It is a little amusing that ironic that we'd now be shelling out to detect new files. This said, I can't think of an easy way to avoid it :)\nMy sense is that this change should go into the RubCop codebase instead. If they had their own Rake task like RSpec does we could simply import that.\n. It is a little amusing that ironic that we'd now be shelling out to detect new files. This said, I can't think of an easy way to avoid it :)\nMy sense is that this change should go into the RubCop codebase instead. If they had their own Rake task like RSpec does we could simply import that.\n. I just looked closer and saw that the \"new\" file stuff was already there :) Bed time.\n. I just looked closer and saw that the \"new\" file stuff was already there :) Bed time.\n. @amitkgupta Thanks for this PR. How have you tested these changes? Run BATs locally? Lifecycle specs?\n. @amitkgupta Thanks for this PR. How have you tested these changes? Run BATs locally? Lifecycle specs?\n. Final, hopefully tiny nitpick: If you take another shot at this, could you please explain how each refactor improves the code in question in each pull request and also summarize what motivated these refactors in the description of the PR. Thanks!\n. Final, hopefully tiny nitpick: If you take another shot at this, could you please explain how each refactor improves the code in question in each pull request and also summarize what motivated these refactors in the description of the PR. Thanks!\n. @amitkgupta closing this PR for now since it has been a few weeks since I originally commented. Please come cross-team pair with us or discuss with @tsaleh if this change is blocking your team.\n. @drnic I believe bundle exec bosh from the root of the project achieves the same effect, this is the workflow used by Pivotal's BOSH team.\nWe think it's best to leave it loadpath management to Rubygems/Bundler to avoid duplicating knowledge that's already in the gemspec.\n. A good test would be one that demonstrates the bugs existence if the fix is commented so it's obvious why the change is needed. Thanks!\n. Sorry :) It's the best I could come up with when Chris walked over to chat about this, it's been a long week.\nPerhaps you could return a fake by stubbing prompt_for_job_and_index.fetch('jobs') on line 246. I'd imagine this fake might look something like the jobs that helped you notice this bug in the first place and the spec's expectation might mirror your own at that time?\nDeploymentHelper definitely looks like it needs some love, thanks for taking the time to make it better.\n. Nice refactor! \n@AndreasMaier and I backfilled a spec for prompt_for_job_and_index since it changed as well. Jenkins is currently red so this is sitting on a branch. We have a chore to merge this in as soon as CI goes green again. This should evenutally close this PR and #410.\n. Thanks for catching my copy-pasta @fraenkel!\n. Jesse, I'm curious what you're using the locally built gems for.\nWe tend to bundle exec bosh in the bosh dir when trying to exercise our\nsource locally.\nThanks!\nOn Sunday, September 29, 2013, Jesse Proudman wrote:\n\nI needed a method to build local gems without pushing them to S3. I didn't\nsee an obvious way to do that with the existing rake tasks, so I build\nthese methods.\nYou can merge this Pull Request by running\ngit pull https://github.com/blueboxgroup/bosh build-gems-locally\nOr view, comment on, or merge it at:\nhttps://github.com/cloudfoundry/bosh/pull/435\nCommit Summary\n- Build gems locally without publishing.\nFile Changes\n- M bosh-dev/lib/bosh/dev/gems_generator.rbhttps://github.com/cloudfoundry/bosh/pull/435/files#diff-0(10)\n- M bosh-dev/lib/bosh/dev/tasks/ci.rakehttps://github.com/cloudfoundry/bosh/pull/435/files#diff-1(7)\nPatch Links:\n- https://github.com/cloudfoundry/bosh/pull/435.patch\n- https://github.com/cloudfoundry/bosh/pull/435.diff\n\n\ntiny keyboard + fat fingers\n. Exactly - the \"gemspec\" directives in the Gemfile make this happen.\nOn Tuesday, October 1, 2013, Jesse Proudman wrote:\n\nTo confirm, bundle exec bosh within the bosh directory itself will run the\nlocal bosh version (and not the installed versions)?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/435#issuecomment-25501451\n.\n\n\ntiny keyboard + fat fingers\n. We're working on fixing this right now.\n. We fixed this a few minutes after my earlier comment. Apologies for the slow update on my part.\n. @tader Thanks for making this change. It looks like a step in the right direction to me on visual inspection. A pair still need to exercise the specs etc.\nWould you mind squashing the two commits? We don't typically separate the introduction of production code from their specs.\nIt's also worth noting this change doesn't necessarily mean regions other than us-east-1 will work seamlessly since the Pivotal team only exercises the one region at this time. It wouldn't surprise me if more work needs to be done to ensure bosh aws works properly in other regions.\n. Very pretty output, and a thoughtful PR description!\nYou guessed right of course, this needs tests to go in.\nIn fact, it'd be great if you extracted some of the affected code into a\npresenter. The command classes have grown bloated over time, they should be\nskinny like well factored Rails controllers. I expect a presenter will also\nbe much easier to test.\nSlimming down the commands will make adding and maintaining cool features\nlike this one much easier for all of us.\nOn Wednesday, November 13, 2013, Dr Nic Williams wrote:\n\nIn advance of any questions, no, I don't think this warrants a unit test.\nPlease don't make me create a whole test infrastructure for this just to\nlet us all enjoy beautiful colors. [image: :+1:]\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/456#issuecomment-28445576\n.\n\n\ntiny keyboard + fat fingers\n. These specs are specific about package versions so upgrades to the stemcells we publish are always intentional. We'd hate to accidentally impose an update on folks who use BOSH in their production systems.\nThe only way I'm aware of to make a custom stemcell is to fork BOSH; it should be easy to update these specs in your fork if you're using a different version of Ubuntu than we do. \nIMHO it isn't unreasonable to have to update specs when making a change to something as significant as the OS version. In fact, being a test driven codebase, it's reasonable to expect any change to BOSH, however small, will need to be accompanied by a spec change.\n. @goehmen I see that https://www.pivotaltracker.com/story/show/73556684 was turned into a chore and delivered but the story to merge this PR hasn't yet been prioritized: https://www.pivotaltracker.com/story/show/73635400\nAny idea when this will be merged? I'm a little worried that these changes will get harder to merge as the develop branch drifts ahead of this PR.\n. Is it important that these requires go inside Platform::Centos instead of at the top of the file as is common Ruby convention?\nI misread these a being calls to include on my first pass :)\n. Oh interesting! Is this a common style in this codebase? I've never encountered it before.\nNot suggesting this block the pull request - but at first glance it does seem like these requires are backwards. For instance \"bosh_agent/platform/centos/disk\" should probably require \"lib/bosh_agent/platform/centos\" not the other way around.\n. I'll drop a chore in Tracker :)\nOn Monday, June 10, 2013, Martin Englund wrote:\n\nIn bosh_agent/lib/bosh_agent/platform/centos.rb:\n\n@@ -0,0 +1,19 @@\n+module Bosh::Agent\n+\n-  class Platform::Centos < Platform::Linux\n-    require 'bosh_agent/platform/centos/disk'\n-    require 'bosh_agent/platform/linux/logrotate'\n-    require 'bosh_agent/platform/rhel/network'\n-    require 'bosh_agent/platform/linux/password'\n  +\n\nRight, it is backwards and should be fixed some day ;) It is just the OS\nflavor code that has this, the rest is \"normal\".\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/274/files#r4627545\n.\n. Intentional?\n\nI like the bosh.dev idea.\n. We shouldn't require rubocop unless we're running this task. I tend to prefer placing require statements inside the task block that need them.\n. The extraction of this class is definitely an improvement; good use of an AbstractFactory IMO.\nI'll add that calling this class \"Bosh::Aws::Zone\" means we'd be able to remove the comment above its name. Pivotal's BOSH team prefers specific names and RSpec examples over comments.\n. Why are we using lazy initialization in this method and similar methods? Is this to avoid a performance penalty? Or is it because there is state in the object being created that must be carried forward?\nIf the lazy initialization is important a spec should convey this and describe why.\n. I see now that this line preserves the lazy initialization that was already present. Specs for the new class would still be useful even it all it says in this case is: it 'lazy initializes regions like it did before the spec was written!'\nThis creates an opportunity for another developer to consider removing this behavior.\n. Interesting, I hadn't seen the escape gem before. \nIt looks like it hasn't been modified since 2007 though, does it do something that Ruby's built-in Shellwords doesn't?\n. It wonder if it might be possible to avoid duplicating https://github.com/drnic/bosh/blob/8f6a321cd284d97e1f700d60f903e80384d1f438/bosh_cli_plugin_micro/lib/bosh/cli/commands/micro.rb#L98 and also provide a spec for the else portion of this ternary so it doesn't get inadvertently taken away :)\n. The duplication's tiny but probably the more import thing thing to address.\nI agree about the formatting, it's fine untested. It's the \"not set\" in the\nelse portion of the ternary that looks like it might be significant and\nmight warrant a spec that expresses why it's needed.\nBeing unfamiliar with this part of the code, I'd find this spec useful\nif/when I'd have to change it.\nThanks.\nOn Friday, August 30, 2013, Dr Nic Williams wrote:\n\nIn bosh_cli_plugin_micro/lib/bosh/cli/commands/micro.rb:\n\n@@ -98,6 +99,17 @@ def status\n       say(\"Target\".ljust(15) + target_name)\n     end\n-    usage \"micro ssh\"\n-    desc \"Open SSH terminal or run a command via SSH upon micro BOSH instance\"\n-    def micro_ssh(*args)\n-      username = \"vcap\"\n-      host = URI.parse(target).host\n-      target_name = config.target_name ? config.target_name.make_green : \"not set\".make_red\n\nIts just formatting. I can take out the red/green if you don't like it. I\nthought it looked pretty. I'm not testing the stdout at all. Is anyone else\ntesting stdout?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/405/files#r6098960\n.\n. \n",
    "tiewei": "@frodenas thank you for your response, I'm trying some double-check around this issue. \nAs you said \"setup_data_disk\" is for ephemeral disk on /dev/vdb and yep I check the codes. Still checking \"mount_persistent_disk\" (https://github.com/cloudfoundry/bosh/blob/master/bosh_agent/lib/bosh_agent/bootstrap.rb#L54) is for which disk, by the name I guess is for /dev/vdc which is done by  deployer step 5, and there is still an issue. \nI'll do more checking tonight, if it's my mistake, I'll close this issue and pull request.\nThanks again, your idea truly helps a lot. \n. @frodenas  , if mount_persistent_disk is for openstack volume, and the volume will be attached after deployer do update_persistent_disk , then the agent will wait here for volume-attached until succeed or timeout, but the deployer is still blocked in wait_until_agent_ready ,  which will never attach the volume, at the same time because agent hang in mount_persistent_disk, looks like the HTTPHandler will never start.\nIs it executed as I describe , at least the code seems like that\n. After looking into the code , yes you are right, the new vm start without persistent disk mounted, I think I mistaked about the sequence of when the registry setting up the persisten disk for agent. \nThanks again\n. Hi  @ajackson ,\nThank you for response, today I did 2 commits, one  just merge my code with cloudfoundry/bosh master, the other did a adjustment with my code.\nNow it can pass the travis test, please review.\n.  In #159 Ferran and I found out my mistake of the execution sequence, I dead lock is not exist, close the pull request\n. I created stemcell by rake stemcell:basic[\"openstack\"] , it will create bosh-stemcell-openstack-kvm-1.5.0.pre.3.tgz which without libyaml. \n. And because REST API of bosh_registry(old is bosh_openstack_registry) changed from #{@registry_endpoint}/servers/#{current_server_id}/settings to #{@registry_endpoint}/instances/#{current_server_id}/settings, old stemcell is not working well with bosh 1.5.0.pre.3.\nAnd there is only 1.5.0.pre.2 in public stemcells, so I have to create one myself.\n. @frodenas  ok, I'll have a try, but why create a stemcell will meet the error ? there must something wrong...\n. From the code can't find any problem, close it before any one meet it again\n. According to codes, everything should be alright, since has done \"apt-get install libyaml-dev\" when build the stemcell, if anyone meet it again, I'll request a new pull\n. Hi,\nYou can get more log from /var/vcap/bosh/log/current and /var/vcap/sys/log/director/director.log in the target vm (your 50.50.10.9 )  , seems your director is not working as expect, please attach more details. that may help to resolve the problem\nWei Tie\nOn Jun 19, 2013, at 2:40 PM, yuanyangen notifications@github.com wrote:\n\n/usr/local/rvm/gems/ruby-2.0.0-p195/gems/httpclient-2.2.4/lib/httpclient/session.rb:775:in initialize': execution expired (HTTPClient::ConnectTimeoutError)\n. @yuanyangen , use ssh vcap@<ip> -i /path/to/pem , and password for root is c1oudc0w\n. Do bosh target <microbosh_ip_address>:25555 , replace the ip to the address of your microbosh_ip.(if port is 25555 as default, it can be omission)\nIf the deploy succeed,  it will say Target set to  <your_deployment_name>\n\nand then bosh status will provide detail of the director.\nWei Tie\nOn Jun 20, 2013, at 12:30 PM, yuanyangen notifications@github.com wrote:\n\n@frodenas \n@TieWei \nthank you for your reply, I try to do this:\nroot@ubuntu:~/bosh-workspace/deployments# bosh target\nTarget not set\nIt seems that setting the target is part of the deploy of micro bosh:\nthe belows are from the correct deploy:\nWARNING! Your target has been changed to https://:25555'!\nDeployment set to '~/bosh-workspace/deployments/microbosh-openstack/micro_bosh.yml'\nDeployedmicrobosh-openstack/micro_bosh.yml' to `https://:25555', took 00:04:19 to complete\ni think i need to set the target manually and deploy the micro_bosh.yml manually,\nhow can I implement this?\nBests regards\nyuanyangen\n\u2014\nReply to this email directly or view it on GitHub.\n. @jfoley  i'm still working on it, i'll create a PR this weekend\n. @jfoley  Sorry for can't work it out on time. Currently it still can't pass all tests, and would you please tell me how to do the unit/integration test in my local machine?\n. @jfoley  - I think the  #396  PR will solve the issue (code change set is just like #394 ), but #394 could pass all Travis CI test and #395 #396 could not pass (according to the log the problem seems not related to the code change set ). Would you like figure out how could I work around this? \n. wrong branch\n. @abic Thank you for your comments,\nBecause the director will fill refilling resource pools with idle vms, which ip is dynamic ip from network ip pool, my PR will not working in a good way, will update the code and restart a new PR.\n. @ryantang , this issue makes bosh can not work in openstack grizzly (of course the vm instance can not boot up correctly). These days I received at least 3 mails from guys blocked here. \nSure when the centos support done , I'll resubmit the PR if it's not merged here and the issue stay.\nWould you like tell me some information/schedule about the CentOS branch, I believe it's everyone looking for\n. Sure, should i close it until your works done? \n. Created a clean PR https://github.com/cloudfoundry/bosh/pull/396 , close this one\n. will rebase to newest develop branch\n. @ryantang  Cool, and I'm not sure why the Travis CI integration test errored.\n. Agree, I thought this PR as a \"just work\" one, the whole network reserve work related several steps, this change set just fix the problem but not resolve the whole one. Waiting for your work.\n. \n",
    "aramprice": "@drnic, et al - what would you think about pulling this method entirely and leaving it up to the stemcell builder to setup /tmp as we expect (ex. 777)?\n. @drnic is this still an issue with the latest release/code?\n. @drnic - I don't know whether or not this was resolved, I'm asking since this is two months old.\n. This change looks reasonable but is there any way we could extract some of the code from package_compiler/bin/package_compiler and put it under test? The package_compiler module is under-tested and I'm reluctant to pull in changes without adding test coverage.\n. @drnic - apologies, I commented & closed when I should have just commented. Feel free to reopen, or I will when I'm back at my desk. \n@aramprice\n. @youngm - is this still happening with the current code/release? If so, can you provide an example to reproduce?\n. @drnic - are you still seeing this issue? Given the age of the issue I'm wondering if it's still worth digging into this.\n. @drnic - is this reproducible with current code?\n. is this the same root issue as #246?\n. is this the same root issue as #245?\n. @KristianOellegaard,\nI was able to run the command @drnic suggested\nbash\n\u25cb \u2192 gem install bosh_cli_plugin_micro -v \"~> 1.5.0.pre\" --source https://s3.amazonaws.com/bosh-jenkins-gems/\nFetching: bosh_common-1.5.0.pre.875.gem (100%)\nFetching: blobstore_client-1.5.0.pre.875.gem (100%)\nFetching: bosh_cli-1.5.0.pre.875.gem (100%)\nFetching: bosh_cpi-1.5.0.pre.875.gem (100%)\nFetching: ruby_vim_sdk-1.5.0.pre.875.gem (100%)\nFetching: bosh_vsphere_cpi-1.5.0.pre.875.gem (100%)\nFetching: bosh_registry-1.5.0.pre.875.gem (100%)\nFetching: bosh_aws_cpi-1.5.0.pre.875.gem (100%)\nFetching: bosh_openstack_cpi-1.5.0.pre.875.gem (100%)\nFetching: agent_client-1.5.0.pre.875.gem (100%)\nFetching: bosh_cli_plugin_micro-1.5.0.pre.875.gem (100%)\nSuccessfully installed bosh_common-1.5.0.pre.875\nSuccessfully installed blobstore_client-1.5.0.pre.875\nSuccessfully installed bosh_cli-1.5.0.pre.875\nSuccessfully installed bosh_cpi-1.5.0.pre.875\nSuccessfully installed ruby_vim_sdk-1.5.0.pre.875\nSuccessfully installed bosh_vsphere_cpi-1.5.0.pre.875\nSuccessfully installed bosh_registry-1.5.0.pre.875\nSuccessfully installed bosh_aws_cpi-1.5.0.pre.875\nSuccessfully installed bosh_openstack_cpi-1.5.0.pre.875\nSuccessfully installed agent_client-1.5.0.pre.875\nSuccessfully installed bosh_cli_plugin_micro-1.5.0.pre.875\n11 gems installed\nI'm going to close this particular issue.\n. @drnic - asking product to weigh in.\n. @drnic,\nI'm seeing the same thing. I believe this is a problem with the YAML using !binary instead of string for the sha1 data, ex:\ndiff\ndiff --git a/release/.final_builds/packages/libpq/index.yml b/release/.final_builds/packages/libpq/index.yml\nindex 90ed8c6..7293798 100644\n--- a/release/.final_builds/packages/libpq/index.yml\n+++ b/release/.final_builds/packages/libpq/index.yml\n@@ -7,4 +7,5 @@ builds:\n   aeb5aa4c222cb0650018170172dd3ac19f00e57d:\n     blobstore_id: rest/objects/4e4e78bca51e122204e4e9863f28f3050d308b21e877\n     version: 2\n-    sha1: e3a6b45f6edf01d6c91c37497ef83d19fa423b85\n+    sha1: !binary |-\n+      ZTNhNmI0NWY2ZWRmMDFkNmM5MWMzNzQ5N2VmODNkMTlmYTQyM2I4NQ==\nIf you're seeing diff's different from the one above, that probably deserves its own issue.\nWe have a bug open for this in the backlog. \nAny insight as to why the YAML dumper is using !binary |- instead of plain text would be welcome.\n- @aramprice / @mariash \n. @drnic - Product is evaluating this.\n. @drnic - is this covered by #275? If not could you open a different issue/pr with the change you'd like to see.\n. @drnic does the implementation of bosh upload stemcell http://example.com/foo.tgz cover this? If so can the issue be closed?\n. closing.\n. @drnic - asking product for input.\n. @drnic - I'm guessing this is still an issue, could you please confirm.\n. 221 has been merged, asking product to look at this again.\n- corrected to remove misleading link to issue 221\n. @drnic you are correct (about the pr# as well), I missed the \"reverted\" not at the bottom of the thread.\n. @msackman / @cdavisafc / @seansweda  - do any of you have insight into how to reproduce this, or capture more info when the error happens?\n. cross-linking #290, #291, and #292 seem to be related\n. cross-linking #290, #291, and #292 seem to be related\n. cross-linking #290, #291, and #292 seem to be related\n. @pmenglund it sounds like #295 could feed into better compile-order decisions, does that sound right?\n@msackman - it looks like (from package_compiler.rb:157) that we are just blindly assigning \"ready\" (e.g. w/o uncompiled dependencies) compile tasks to the available compile workers\n. @stefanschneider - This looks good, I think we can merge it with a few changes.\n- can you update your branch so the merge is automatic?\n  - this has languished a while, our apologies.\n- can you fix the rubocop errors from Travis\n  - you can see these by running rake rubocop, we will be adding this to spec:unit shortly\n- can you move vcr from the Gemfile into blobstore_client.gemspec using .add_development_dependency\nthanks,\n@aramprice / @mariash \n. Closing as this was merged manually by @xoebus \n- https://github.com/cloudfoundry/bosh/commit/ed5177a0f206c3d74d0b2903ddd175d2025f248b\n@aramprice / @mariash \n. Closing as the PR has been merged.\n. Merged manually. \n. @frodenas,\nWe merged this manually in https://github.com/cloudfoundry/bosh/commit/0db4a76ed9b663063a348de98f2ce3fe928ecf89, it should be promoted to master via CI shortly.\n@aramprice / @mariash\n. @lukebakken,\nThe specific error key not found: \"AWS_ACCESS_KEY_ID_FOR_STEMCELLS_JENKINS_ACCOUNT\" should be resolved in a day or so. We are removing the need for credentials when the Pipeline object downloads publicly available build artifacts (stemcells, gems, or in this case a bosh release).\nBased on the diff in this PR, I'm not certain if you wanted to avoid the AWS key issue, or if you actually wanted to force the stemcell:micro task to use a locally created Micro Bosh release.\nSince the noted issues should be resolved soon, and since we are trying to reduce logic in rake tasks I am going to close this.\nIf you did intend to change the remote-bosh-release vs. local-bosh-release behavior in the rake task, then it would be best to extract the logic from the rake task into a tested lib, and submit a new PR.\nSee https://github.com/cloudfoundry/bosh/commit/baa1b0605b15abf6c3c2c3f9c3f83ee312806cea for an example of the direction we are going.\nthanks,\n- @aramprice / @mariash \n. Beyond compacting the ERB code, it's not clear what problem this pull request is solving.\nThe TemplateEvaluationContext is used in generating more than just jobs' specs, it's not clear how or whether these other uses will be impacted.\nBased on these considerations we are going to close this pull request.\n- @aramprice / @mariash \n. Looks good, merging into https://github.com/cloudfoundry/bosh/tree/develop\n- @aramprice / @mariash \n. @drnic this is in the icebox, asking the pm to weigh in.\n. @kkbankol \nWe spoke to @frodenas and it appears that the metadata endpoint should be configured by default on OpenStack.\nThe relevant docs are at http://docs.openstack.org/grizzly/openstack-compute/admin/content/metadata-service.html - we believe they are similar for other version of OpenStack.\nThanks,\n@aramprice and @sbrady \nTeam Cloud Foundry\n. @kkbankol \nWe will check with @frodenas to see whether we still have our old OpenStack configurations available. While we used to maintain our own OpenStack installation, we currently test against a hosted OpenStack environment and we do not have direct access to those configuration files.\nThanks,\n@aramprice and @sbrady \nTeam Cloud Foundry\n. It's confusing that agent_uri still has authentication credentials, but these added below (user / password) and that on package_compiler/bin/package_compiler:64 the value of options[\"agent_uri\"] is set to a credential-less URI.\n. As an alternative to disabling ssl verification globally (Excon.defaults[:ssl_verify_peer] = false) perhaps something the following could be used:\nruby\n      openstack_params = {\n        :provider => \"OpenStack\",\n        :openstack_auth_url => @openstack_properties[\"auth_url\"],\n        :openstack_username => @openstack_properties[\"username\"],\n        :openstack_api_key => @openstack_properties[\"api_key\"],\n        :openstack_tenant => @openstack_properties[\"tenant\"],\n        :openstack_region => @openstack_properties[\"region\"],\n        :openstack_endpoint_type => @openstack_properties[\"endpoint_type\"],\n        :connection_options => {:ssl_verify_peer => ssl_verify_peer_value(@openstack_properties[\"ssl_verify_peer\"])}\n      }\nWhere #ssl_verify_peer_value looks something like (defaulting to true unless explicitly set):\nruby\n    def ssl_verify_peer_value(ssl_verify_peer_option)\n      case ssl_verify_peer_option\n        when 'false' then false\n        when false then false\n          else true\n      end\n    end\nAs the code is written both openstack_params and glance_params (Fog::Image.new(glance_params)) will need to be updated but the hashes appear to be identical so the usage can likely be collapsed.\n@aramprice / @sbrady \n. ",
    "seansweda": "if we are going to leave the non-privileged account then please, please, please change the damn unix permissions so that it has read access to /var/vcap\n. use case for \"set property\" is allowing setting secrets on the command line rather than storing them in the manifest\nsee:\nhttps://groups.google.com/a/cloudfoundry.org/forum/#!topicsearchin/bosh-users/%22set$20property%22/bosh-users/ZXf0SbqpLRk\n. why not simply remove the requirement for uuid entirely, and leave the old behavior whenever uuid is declared?  that way if you want to be an abstraction layer zealot you can leave it out of your manifest and damn the torpedos\n. Again, why not simply make director_uuid optional.  That way, when you upload a manifest with director_uuid set it means you are pinning the deployment to the director.  The director should enforce this.\nThe cases are:\ncli manifest has uuid, director manifest has same uuid, ok to deploy\ncli manifest has uuid, director manifest has no uuid, ok to deploy and you're now pinning deployment to director\ncli manifest has no uuid, director manifest has uuid, director rejects deployment (Martin's disaster case)\ncli manifest has no uuid, director manifest has no uuid, bombs away (Dmitri's case)\nand finally you need to add the ability to allow the cli to override the director's manifest, so that you migrate from using uuid not not using uuid\n. I'm not proposing eliminating the director's uuid, the cli already uses it to identify the director.  I believe if your .bosh_config file has a matching uuid for your manifest the cli will actually switch the target for you automatically when you run \"bosh deployment foo\".\nUsing the existing property name has the benefit of being backwards compatible with existing manifests.\n. ",
    "ryantang": "Looks like an old issue, so we're closing it.  Please re-open if it's still outstanding.  Thanks.\n@ryantang\n. @youngm \u2014 I get the same undefined method 'property' error when following your procedure.  However, I see it on all bosh CLI commands that inspect the deployment manifest: bosh deploy, bosh stop, and so forth.  \nDigging into the code, I believe that this error results from a refactor that moved the property method to a different file.  This method is no longer adjacent to the ERB processing of the deployment manifest, and is therefore no longer in scope.\nI'm unclear on whether calling <%= properties(\"bosh.password\") %> in the manifest was truly a supported feature or just something that happened to work.  At the very least, folks I spoke to here at CloudFoundry have never heard of this use case.  \nMy recommendation is to hard code the property(\"bosh.password\") in your manifest file and just keep it safe.\n@ryantang\n. @youngm, @rkoster -- After examining (the very old) commit https://github.com/cloudfoundry/bosh/commit/b3741ea94c9f99b37230df4cd7c231fc04833440, we'd conclude that this is a regression \u2014 at least from the intentions expressed in @olegshaldybin's commit message. That being said, it seems like the current BOSH team is not really supporting this feature any more.\nTagging @tsaleh, who can weigh in on whether this is something that we would take a PR to fix. \n@jfoley / @ryantang\n. @tsaleh \u2014 Can you comment on the relative priority of this story?  It appears to be in the icebox of the BOSH tracker.\n@ryantang\n. @kei-yamazaki \u2014 We received your CLA.  Thank you.  \nWe're still looking forward to receiving a test that exercises this bug and demonstrates that this code change fixes it.  Thanks.\nRyan\n. I think we're turning off the validation in the wrong place.  It should be enforced by the BOSH director.  (@seansweda's recommendation is one possible implementation) .\nThe current implementation means that we're making the manifest file the proverbial safety-less gun.  I'm fine with the manifest file saying director_uuid: ignore, but the BOSH director should be able to determine whether it will accept such a setting.  And production BOSH clearly shouldn't.\n. @TieWei  \u2014 We're in the middle of adding CentOS support to the stemcell builder. This means we're making significant changes to it, including trying to bring it under test. We'd suggest waiting until these changes are completed and resubmitting a pull request with tests indicating why this change is needed at that point.\nIf you think we should make this change sooner, could you provide any information you think might help us prioritize this?\n\u2014 Abhi (@hiremaga) & Ryan\n. @TieWei  \u2014 Here's the publicly viewable tracker backlog for bosh:  http://cftracker.cfapps.io/bosh.  There's a number of CentOS items on there, and you can see how they're progressing over time.  Thanks.\n. Tagging @tsaleh so that he's aware of this issue.\n. @tsaleh -- This looks like a pretty substantial change, requiring review from the bosh team.  Can you please prioritize?\nAbhi (@hiremaga) & Ryan\n. @hiremaga -- Can you comment on this?\nRyan\n. ",
    "vito": "Awesome, thanks!\n. Awesome, thanks!\n. pls\n. pls\n. woops, just noticed I submitted this against master.\n. @cppforlife Not at the moment, no.. @cppforlife Not at the moment, no.. @mfine30 Somewhere in between. Maybe a five-off? It's not happening anymore at least. I can try to go through our build history and see how many times it's happened. . @mfine30 Somewhere in between. Maybe a five-off? It's not happening anymore at least. I can try to go through our build history and see how many times it's happened. . @dpb587-pivotal Just checked and it has the correct number of worker processes, and their uptime is all ~26 days ago.\n@mfine30 Went through our build history for TopGun, there are five occurrences across four builds:\n\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/179\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/197\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/199 (happened twice)\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/200\n\nThe oldest build was from 14 days ago, so the director should have been deployed and stable by then.. @dpb587-pivotal Just checked and it has the correct number of worker processes, and their uptime is all ~26 days ago.\n@mfine30 Went through our build history for TopGun, there are five occurrences across four builds:\n\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/179\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/197\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/199 (happened twice)\nhttps://ci.concourse-ci.org/teams/main/pipelines/main/jobs/bosh-topgun/builds/200\n\nThe oldest build was from 14 days ago, so the director should have been deployed and stable by then.. @dpb587-pivotal Unfortunately those tasks aren't around anymore. I guess they got reaped. :/. @dpb587-pivotal Unfortunately those tasks aren't around anymore. I guess they got reaped. :/. ",
    "scottfrederick": "I've looked into using this for documenting the Cloud Controller API, and I'd still like to make that happen. I'm in favor of using it as a way to document CF APIs in general. \nSwagger docs would remain as fresh as the meta-data that feeds into them. Annotating source code is the way to go, as it makes it more likely that the docs will stay fresh as the code is updated. \n. ",
    "mstine": "Here are the beginnings of this implementation: https://github.com/mstine/bosh/commit/ba5e013b6390d3562ef6daddc3eda52e3863a121\n. It's possible...I need to figure out the minimum viable environment for\nrunning the rspec suite.\nbundle exec rspec just goes BOOM\nOn Wed, Jun 5, 2013 at 2:14 PM, Dr Nic Williams notifications@github.comwrote:\n\nPerhaps the bug is related to the new http GET / response you added?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/252#issuecomment-19001552\n.\n\n\nMatt Stine | Community Engineer, Cloud Foundry | Pivotal\n901-493-5546 | mstine@goPivotal.com\ngoPivotal.com\n. Abhi-\nHappy to pair with you (or anyone else on the team) on this next week so\nthat we can get this settled.\nBest,\nMatt\nOn Fri, Jun 14, 2013 at 7:42 AM, Dr Nic Williams\nnotifications@github.comwrote:\n\nWhat does \"team's comfortable living with\" mean? As a member of the team\nof people who contribute to bosh, I think this is going to be a wonderful\nway for people to explore the API and the nest values that come back.\nEspecially since there seems a sad predisposition to \"no docs\" and actually\ndeleting docs within the project, it is even more critical to allow users\nto explore the values being passed around on their bosh.\nDr Nic Williams\nStark & Wayne LLC - the consultancy for Cloud Foundry\nhttp://starkandwayne.com\n+1 415 860 2185\ntwitter: drnic\nOn Fri, Jun 14, 2013 at 7:09 AM, Abhi Hiremagalur\nnotifications@github.com wrote:\n\nWe'd like a pair to take a closer look at Swagger first and see if it's\nsomething the team's comfortable living with in the long run.\nWe'll aim to loop back to this PR once that has happened.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/252#issuecomment-19458953\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/252#issuecomment-19460906\n.\n\n\nMatt Stine | Community Engineer, Cloud Foundry | Pivotal\n901-493-5546 | mstine@goPivotal.com\ngoPivotal.com\n. +1 for more modularity and pluggability in all areas of BOSH. Would love to\nhelp see this through.\nOn Sat, Jun 22, 2013 at 8:02 PM, Abhi Hiremagalur\nnotifications@github.comwrote:\n\nAre there other blobstore plugins for the director and for the CLI yet; or\nis this a new idea?\nProbably a new idea from a BOSH newb :)\nI see what you mean about releases. Bummer - makes interesting exploration\nlike this a bigger commitment.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/312#issuecomment-19868239\n.\n\n\nMatt Stine | Community Engineer, Cloud Foundry | Pivotal\n901-493-5546 | mstine@goPivotal.com\ngoPivotal.com\n. ",
    "youngm": "What is your definition of current?  I'm using bosh_cli 1.0.3 bosh server release #13.\nIs there a newer version of cli or server you want me to try?\n. I followed these instructions (http://docs.cloudfoundry.com/docs/running/bosh/setup/) to upgrade to the latest pre 1.5.0 it appears the mechanism to access properties has changed.  I'm using syntax like:\n<%= property(\"bosh.password\") %>\n\n(erb):55:in `<main>': undefined method `property' for main:Object (NoMethodError)\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/erb.rb:838:in `eval'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/erb.rb:838:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/lib/cli/core_ext.rb:72:in `load_yaml_file'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/lib/cli/commands/deployment.rb:20:in `set_current'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/lib/cli/command_handler.rb:57:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/lib/cli/runner.rb:59:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/lib/cli/runner.rb:18:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.875/bin/bosh:7:in `<top (required)>'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/bin/bosh:23:in `load'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p448/bin/bosh:23:in `<main>'\n. The properties works fine when executing bosh deploy.  It only fails on a subsequent bosh stop.\nIt is quite easy to duplicate with CLI 1.0.3 and bosh release 13.  I'm not using templates or anything.\n- Take simple bosh deploy yaml.\n- Deploy this yaml. (bosh deploy)\n- Add a property to the yaml. e.g.\nenv:\n    bosh:\n      password: <%= property(\"bosh.password\") %>\n- Add property to bosh deployment bosh set property bosh.password ~password~\n- Deploy yaml with property bosh deploy\n- Attempt to stop a job bosh stop nats 0\n- Notice that bosh cli detects changes to your deploy yaml which show that it is not processing the <%= property(\"bosh.password\") %> code.\nI wish I could test it with the latest snapshot build cli and bosh but I'm not in a position to attempt to deploy non stable bosh releases in my environment.\nIf you want to table or close this issue until I can attempt to reproduce with the next stable release of bosh and cli that is fine with me.  It is not your fault I cannot test with latest dev bosh release.  Just hurry and get a new bosh stable release out! :)\nMike\n. The properties works fine when executing bosh deploy.  It only fails on a subsequent bosh stop.\nIt is quite easy to duplicate with CLI 1.0.3 and bosh release 13.  I'm not using templates or anything.\n- Take simple bosh deploy yaml.\n- Deploy this yaml. (bosh deploy)\n- Add a property to the yaml. e.g.\nenv:\n    bosh:\n      password: <%= property(\"bosh.password\") %>\n- Add property to bosh deployment bosh set property bosh.password ~password~\n- Deploy yaml with property bosh deploy\n- Attempt to stop a job bosh stop nats 0\n- Notice that bosh cli detects changes to your deploy yaml which show that it is not processing the <%= property(\"bosh.password\") %> code.\nI wish I could test it with the latest snapshot build cli and bosh but I'm not in a position to attempt to deploy non stable bosh releases in my environment.\nIf you want to table or close this issue until I can attempt to reproduce with the next stable release of bosh and cli that is fine with me.  It is not your fault I cannot test with latest dev bosh release.  Just hurry and get a new bosh stable release out! :)\nMike\n. Thanks Ryan.  Perhaps you can help me understand then existence of the CLI commands.  bosh properties, bosh set property, bosh unset property?\nI thought the purpose of these commands were to provide deployment properties that can then be substituted in my deployment yaml files?  Is this entire properties utility in bosh now useless?\nMike\n. Thanks Ryan.  Perhaps you can help me understand then existence of the CLI commands.  bosh properties, bosh set property, bosh unset property?\nI thought the purpose of these commands were to provide deployment properties that can then be substituted in my deployment yaml files?  Is this entire properties utility in bosh now useless?\nMike\n. Ah!  I think we're finally getting somewhere.  So, how do I actually use these properties in my deployment manifest?  I thought I was supposed to use them with  <%= property(\"property.name})%> and let ERB processing populate them.  Though @ryantang didn't think so.\nAm I just supposed to name them hierarchically?  So perhaps given this example:\nproperties:\n  ccdb:\n    port: 2544\n    pool_size: 10\n    roles:\n    - tag: admin\n      name: cc\n      password: SomePassword\nTo use a property to set password would I just name the property correctly like so?\nbosh set property properties.ccdb.roles.password OverrideSomePassword\nperhaps that is how I should be using the properties?\n. Ah!  I think we're finally getting somewhere.  So, how do I actually use these properties in my deployment manifest?  I thought I was supposed to use them with  <%= property(\"property.name})%> and let ERB processing populate them.  Though @ryantang didn't think so.\nAm I just supposed to name them hierarchically?  So perhaps given this example:\nproperties:\n  ccdb:\n    port: 2544\n    pool_size: 10\n    roles:\n    - tag: admin\n      name: cc\n      password: SomePassword\nTo use a property to set password would I just name the property correctly like so?\nbosh set property properties.ccdb.roles.password OverrideSomePassword\nperhaps that is how I should be using the properties?\n. Can you please leave the issue open then?  And \"open for contribution\" so that others know about this?\nMike\n. Can you please leave the issue open then?  And \"open for contribution\" so that others know about this?\nMike\n. @drnic FYI, when I use p(\"bosh.password\") with the latest bosh cli I get the error below.  I don't know how valid my test is though since it is targeting a Bosh 13 server.  We're waiting for a new \"stable\" bosh release before we upgrade.\nAnyway, I'm glad the issue of \"bosh properties\" has been clarified as removed/deprecated.  Now I can focus on moving on to another solution instead of assuming that \"bosh properties\" was the recommended approach.\n(erb):66:in `p': Insecure operation `p' at level 4 (SecurityError)\n    from (erb):66:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:835:in `eval'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:835:in `block in result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:833:in `call'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:833:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/deployment_manifest_compiler.rb:18:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/deployment_helper.rb:44:in `prepare_deployment_manifest'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/commands/deployment.rb:91:in `perform'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/command_handler.rb:57:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/runner.rb:59:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/runner.rb:18:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/bin/bosh:7:in `<top (required)>'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/bin/bosh:23:in `load'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/bin/bosh:23:in `<main>'\n. @drnic FYI, when I use p(\"bosh.password\") with the latest bosh cli I get the error below.  I don't know how valid my test is though since it is targeting a Bosh 13 server.  We're waiting for a new \"stable\" bosh release before we upgrade.\nAnyway, I'm glad the issue of \"bosh properties\" has been clarified as removed/deprecated.  Now I can focus on moving on to another solution instead of assuming that \"bosh properties\" was the recommended approach.\n(erb):66:in `p': Insecure operation `p' at level 4 (SecurityError)\n    from (erb):66:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:835:in `eval'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:835:in `block in result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:833:in `call'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/1.9.1/erb.rb:833:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/deployment_manifest_compiler.rb:18:in `result'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/deployment_helper.rb:44:in `prepare_deployment_manifest'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/commands/deployment.rb:91:in `perform'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/command_handler.rb:57:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/runner.rb:59:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/lib/cli/runner.rb:18:in `run'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/lib/ruby/gems/1.9.1/gems/bosh_cli-1.5.0.pre.936/bin/bosh:7:in `<top (required)>'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/bin/bosh:23:in `load'\n    from /home/youngstrommj/.rbenv/versions/1.9.3-p392/bin/bosh:23:in `<main>'\n. @cppforlife Great!  Unfortunately we've already worked around the problem.  So, I may not have time to contribute a fix for a while.  In the meantime I'll let you decide if you want to leave the issue open or close it.  \nThanks,\nMike\n. @cppforlife Great!  Unfortunately we've already worked around the problem.  So, I may not have time to contribute a fix for a while.  In the meantime I'll let you decide if you want to leave the issue open or close it.  \nThanks,\nMike\n. @cppforlife Instead of submitting a PR to make the disk type configurable for each vm type can we instead allows the user to configure a global default disk type in the vcenter configuration?  Or possibly even just set the type to thin always so it doesn't differ from host to host?\nSeems like either of those would be a good first step, simpler, and adequate for our needs.  Perhaps someone else can contribute individual vmtype config in the future?\nMike\n. @cppforlife Instead of submitting a PR to make the disk type configurable for each vm type can we instead allows the user to configure a global default disk type in the vcenter configuration?  Or possibly even just set the type to thin always so it doesn't differ from host to host?\nSeems like either of those would be a good first step, simpler, and adequate for our needs.  Perhaps someone else can contribute individual vmtype config in the future?\nMike\n. @cppforlife So, I found the API call for making ephemeral disks thin:\nFlatVer2BackingInfo.thin_provisioned = true/false\nIt doesn't offer the full range of disk types persistent disks have.\nSince it seems creation of ephemeral disks is done quite different from persistent disks how would you feel about dividing the story into 2 stories.  One for global config for persistent disk and one for global config of ephemeral disk.  For ephemeral global config I'll probably present a \"thin_provision_ephemeral_disk\" config default to false.\nThoughts?\n. @cppforlife So, I found the API call for making ephemeral disks thin:\nFlatVer2BackingInfo.thin_provisioned = true/false\nIt doesn't offer the full range of disk types persistent disks have.\nSince it seems creation of ephemeral disks is done quite different from persistent disks how would you feel about dividing the story into 2 stories.  One for global config for persistent disk and one for global config of ephemeral disk.  For ephemeral global config I'll probably present a \"thin_provision_ephemeral_disk\" config default to false.\nThoughts?\n. @ljfranklin Yeah, that sounds better.  Because the options are preallocated and thin should we also set eagerlyScrub to true as well in either case?\nhttps://www.vmware.com/support/developer/converter-sdk/conv51_apireference/vim.vm.device.VirtualDisk.FlatVer2BackingInfo.html\n. @ljfranklin Yeah, that sounds better.  Because the options are preallocated and thin should we also set eagerlyScrub to true as well in either case?\nhttps://www.vmware.com/support/developer/converter-sdk/conv51_apireference/vim.vm.device.VirtualDisk.FlatVer2BackingInfo.html\n. @ljfranklin Ah, I mis-read the pre-allocated definition.  Missed that it says it zeros out \"on demand\".  Sounds good.\n. @ljfranklin Ah, I mis-read the pre-allocated definition.  Missed that it says it zeros out \"on demand\".  Sounds good.\n. @ljfranklin Great work on this issue.  However, looking at the main commit for this fix: https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/commit/26983129e258d90f3fc636b2e64ff0048fea1516 it appears to me that this commit only currently applies a default to persistent disks and not ephemeral.  Am I seeing it correctly?\n. @ljfranklin Great work on this issue.  However, looking at the main commit for this fix: https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/commit/26983129e258d90f3fc636b2e64ff0048fea1516 it appears to me that this commit only currently applies a default to persistent disks and not ephemeral.  Am I seeing it correctly?\n. @ljfranklin Ah, thanks.  We're trying to verify the change.  But having some trouble I'll keep digging.\n. @ljfranklin Ah, thanks.  We're trying to verify the change.  But having some trouble I'll keep digging.\n. @ljfranklin Found the problem I was having.  The config hasn't yet been added to the job spec yet.  Thanks.\n. @ljfranklin Found the problem I was having.  The config hasn't yet been added to the job spec yet.  Thanks.\n. @ljfranklin no need to feel embarrassed.  We caught it pre-release right? :)\n. @ljfranklin no need to feel embarrassed.  We caught it pre-release right? :)\n. Nice work @MatthiasWinzeler !  Thanks tons for contributing this!. Nice work @MatthiasWinzeler !  Thanks tons for contributing this!. ",
    "jbayer": "@youngm this issue is now labeled 'open-for-contribution'\n. Alexander, I added you to the CLA list for Altoros, can you send me your email for our records at jbayer at gopivotal com\n. @drnic this story is relatively high in the bosh backlog and marked as blocked waiting for some updates from you. are you planning on continuing this? \n. https://www.pivotaltracker.com/story/show/59889342\n. prioritized https://www.pivotaltracker.com/story/show/61442960\n. @goehmen this is still in the bosh icebox https://www.pivotaltracker.com/story/show/63929256 can we get this moved towards the top of the backlog please since it's ready to go? /cc @tsaleh \n. @mrdavidlaing i talked with @goehmen about this at the end of the week. i hope we can make some progress on this soon.\n. a few responses here:\nDo I need to convince people that starting spot instances from BOSH is useful?\nNo, it's well-understood that supporting spot instances is a very attractive additional capability.\nDo I need to refactor the code / add more tests?\nDo I need to find a way to focus the AWS-SDK update?\n@goehmen and the BOSH developers will provide you additional feedback if either of these are necessary.\nAt this point you can track the progress of the PR using the BOSH Tracker instance. There are currently many in-flight stories on the BOSH team including several other PRs in-flight and in the upcoming backlog. We're being very careful that when we add new capabilities that it's something the BOSH team can really stand behind, and that means that the team owning the repository, in this case the BOSH developers themselves must merge the code instead of a community team. I'm sorry it's taken this long, this journey of this particular PR is not representative of the normal timeline we expect for BOSH or Cloud Foundry PRs. The team did have some concerns to work out with Greg since we do not have any experience with spot instances. Working these out sometimes take time. Greg is working through it.\n. @mrdavidlaing i sent you email with blog authoring info.\nthe post is completely up to you, but if you're asking my personal advice i'd recommend a flow like:\n- great news! the bosh aws cpi now supports spot instances as of build 2086\n- brief summary of your problem space and why you use bosh\n- example syntax for what changes in a deployment manifest to try spot instances (not documentation, let the docs site take care of that)\n- note that this is a community contributed experimental feature, but that you expect it will come in very useful and invite people to play with it and share experiences on bosh-users\nideally you would have some type of diagram for people that don't like to read text and can visually understand the vast majority of the content of you post visually that you put towards the top of the post.\nlet me know if you're interested in this and if i or anyone else can help in any way\n/cc @goehmen \n. the author of this PR is covered by an existing CLA.\n. i'm not sure why this pull request is still marked 'open' since the story [1] shows as delivered and accepted. @goehmen \n[1] https://www.pivotaltracker.com/story/show/70136990\n. @mrdavidlaing \"acceptance criteria\" is essentially that the person that requested work or a story validate that it's what they asked for when delivered. if you're happy, then we're happy.\n. @mrdavidlaing this looks great! thanks for following up on this. we have recently moved the cloudfoundry.com blog content over to blog.gopivotal.com/cloud-foundry-pivotal/ in the time we first gave you and account and when this draft has been finished. let me get you a guest account there and publish it there. i think we can do that as early as today.\n. this PR is covered by an existing CLA, and is fine to merge if the technical validation is OK.\n. @cppforlife the CLA is on-file and this PR can proceed from that perspective. thanks!\n. @cppforlife the CLA is on-file and this PR can proceed from that perspective. thanks!\n. may also want to post on bosh-users@cloudfoundry.org which has many eyeballs\n. we have received the CLA, thank you!\n. consul can fail often. see here for failure recovery documentation: https://github.com/cloudfoundry-incubator/consul-release#failure-recovery. Is \"bets\" in these first 3 rows supposed to be \"beta\" like the rest? \n. ",
    "endwall": "Hi Dr.Nic, then how did you resolve this problem or it is not a problem?\nRecenlty I tried to upgrade my micro bosh and got the same error.\n2013-07-12_00:57:00.73869 #[3206] INFO: Starting agent 0.6.4...\n2013-07-12_00:57:00.73880 #[3206] INFO: Configuring agent...\n2013-07-12_00:57:00.74581 #[3206] INFO: Configuring instance\n2013-07-12_00:57:00.79142 #[3206] INFO: Loaded settings: {\"vm\"=>{\"name\"=>\"vm-f3759453-cbc3-483f-8649-6a27482b858a\"}, \"agent_id\"=>\"bm-e158c654-0b46-4d49-bd20-72ab38eac497\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{}, \"netmask\"=>nil, \"gateway\"=>nil, \"ip\"=>nil, \"dns\"=>nil, \"type\"=>\"dynamic\", \"default\"=>[\"dns\", \"gateway\"]}, \"vip\"=>{\"ip\"=>\"54.251.183.0\", \"type\"=>\"vip\", \"cloud_properties\"=>{}}}, \"disks\"=>{\"system\"=>\"/dev/sda1\", \"ephemeral\"=>\"/dev/sdb\", \"persistent\"=>{}}, \"env\"=>{\"bosh\"=>{\"password\"=>nil}}, \"ntp\"=>[], \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\"}\n2013-07-12_00:57:00.80636 /var/vcap/bosh/agent/lib/agent/bootstrap.rb:135:in merge!': can't convert nil into Hash (TypeError)\n2013-07-12_00:57:00.80660       from /var/vcap/bosh/agent/lib/agent/bootstrap.rb:135:inupdate_blobstore'\n2013-07-12_00:57:00.80665       from /var/vcap/bosh/agent/lib/agent/bootstrap.rb:44:in configure'\n2013-07-12_00:57:00.80669       from /var/vcap/bosh/agent/lib/agent.rb:93:instart'\n2013-07-12_00:57:00.80674       from /var/vcap/bosh/agent/lib/agent.rb:72:in run'\n2013-07-12_00:57:00.80678       from /var/vcap/bosh/agent/bin/agent:97:in'\n2013-07-12_00:57:03.91276 #[3227] INFO: Starting agent 0.6.4...\n2013-07-12_00:57:03.91286 #[3227] INFO: Configuring agent...\n2013-07-12_00:57:03.94437 #[3227] INFO: Configuring instance\n2013-07-12_00:57:03.99023 /var/vcap/bosh/agent/lib/agent/infrastructure/aws/registry.rb:70:in rescue in get_json_from_url': Error requesting registry information #<Errno::ECONNREFUSED: Connection refused - connect(2) (http://localhost:25888)> (RuntimeError)\n2013-07-12_00:57:03.99028       from /var/vcap/bosh/agent/lib/agent/infrastructure/aws/registry.rb:43:inget_json_from_url'\n2013-07-12_00:57:03.99029       from /var/vcap/bosh/agent/lib/agent/infrastructure/aws/registry.rb:91:in get_settings'\n2013-07-12_00:57:03.99030       from /var/vcap/bosh/agent/lib/agent/infrastructure/aws/settings.rb:33:inload_settings'\n2013-07-12_00:57:03.99030       from /var/vcap/bosh/agent/lib/agent/infrastructure/aws.rb:10:in load_settings'\n2013-07-12_00:57:03.99031       from /var/vcap/bosh/agent/lib/agent/bootstrap.rb:60:inload_settings'\n2013-07-12_00:57:03.99032       from /var/vcap/bosh/agent/lib/agent/bootstrap.rb:34:in configure'\n2013-07-12_00:57:03.99033       from /var/vcap/bosh/agent/lib/agent.rb:93:instart'\n2013-07-12_00:57:03.99033       from /var/vcap/bosh/agent/lib/agent.rb:72:in run'\n2013-07-12_00:57:03.99034       from /var/vcap/bosh/agent/bin/agent:97:in'\n. ",
    "ngtuna": "I got the same error, @drnic. I'm deploying CF on OpenStack using bosh-bootstrap. When running: bosh-bootstrap deploy, I got that error: https://gist.github.com/ngtuna/f6977e0ad2d53ae3d090\nLog from /var/vcap/bosh/log/current:\nhttps://gist.github.com/ngtuna/24ddb07f0055b427d14c\nHow can I fix that?\n. @drnic How do you think openstack-registry didn't run so that I got this error?\n. @drnic How do you think openstack-registry didn't run so that I got this error?\n. Yay. Thanks @drnic.\n--Tuna\nOn Wed, Aug 6, 2014 at 2:08 AM, Dr Nic Williams notifications@github.com\nwrote:\n\nUnfortunately I don't remember; I'm sorry.\nOn Tue, Aug 5, 2014 at 12:03 PM, ngtuna notifications@github.com wrote:\n\n@drnic How do you think openstack-registry didn't run so that I got this\nerror?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/243#issuecomment-51243922\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/243#issuecomment-51244448.\n. Yay. Thanks @drnic.\n\n--Tuna\nOn Wed, Aug 6, 2014 at 2:08 AM, Dr Nic Williams notifications@github.com\nwrote:\n\nUnfortunately I don't remember; I'm sorry.\nOn Tue, Aug 5, 2014 at 12:03 PM, ngtuna notifications@github.com wrote:\n\n@drnic How do you think openstack-registry didn't run so that I got this\nerror?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/243#issuecomment-51243922\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/243#issuecomment-51244448.\n. \n",
    "kkallday": "If the deployment fails, bosh download manifest will not download the manifest. Here is a go program that will extract the manifest for you by tapping into the bosh task debug logs: https://github.com/kkallday/deployment-extractor\n. If the deployment fails, bosh download manifest will not download the manifest. Here is a go program that will extract the manifest for you by tapping into the bosh task debug logs: https://github.com/kkallday/deployment-extractor\n. If the deployment fails, bosh download manifest will not download the manifest. Here is a go program that will extract the manifest for you by tapping into the bosh task debug logs: https://github.com/kkallday/deployment-extractor\n. If the deployment fails, bosh download manifest will not download the manifest. Here is a go program that will extract the manifest for you by tapping into the bosh task debug logs: https://github.com/kkallday/deployment-extractor\n. @blackbaud-chadsailer if you know the id of the task that created the deployment you can get the manifest by running \nbosh task <task id> --debug \nIn the output near the top of the file you can see the manifest you attempted to deploy.\nBy the way, here is a go program that will extract the manifest for you (in case you do this often): https://github.com/kkallday/deployment-extractor\n. @blackbaud-chadsailer if you know the id of the task that created the deployment you can get the manifest by running \nbosh task <task id> --debug \nIn the output near the top of the file you can see the manifest you attempted to deploy.\nBy the way, here is a go program that will extract the manifest for you (in case you do this often): https://github.com/kkallday/deployment-extractor\n. @frodenas are you using the Bosh command in my comment above?. @frodenas are you using the Bosh command in my comment above?. @frodenas  If you know the id of the task that performed the deployment you can see the manifest in the debug logs.\nI haven't tried this but these methods [1] [2] [3] might give you the debug logs and you can scan through it to find the manifest.\n\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#Client.Task\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#Client.TaskOutput\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#TaskImpl.DebugOutput. @frodenas  If you know the id of the task that performed the deployment you can see the manifest in the debug logs.\n\nI haven't tried this but these methods [1] [2] [3] might give you the debug logs and you can scan through it to find the manifest.\n\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#Client.Task\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#Client.TaskOutput\nhttps://godoc.org/github.com/cloudfoundry/bosh-cli/director#TaskImpl.DebugOutput. \n",
    "KristianOellegaard": "Thanks for the quick reply. The above gives:\n```\ngem install bosh_cli_plugin_micro \"~> 1.5.0.pre\" --source https://s3.amazonaws.com/bosh-jenkins-gems/\nERROR:  Could not find a valid gem 'bosh_cli_plugin_micro' (>= 0) in any repository\nERROR:  Could not find a valid gem '~> 1.5.0.pre' (>= 0) in any repository\n```\nI got the other name from the official docs here: http://docs.cloudfoundry.com/docs/running/deploying-cf/openstack/install_microbosh_openstack.html\n. That gives a dependency error as well:\n```\ngem install bosh_cli_plugin_micro -v \"~> 1.5.0.pre\" --source https://s3.amazonaws.com/bosh-jenkins-gems/\nERROR:  While executing gem ... (Gem::DependencyError)\n    Unable to resolve dependencies: thin requires rack (>= 1.0.0), eventmachine (>= 0.12.6), daemons (>= 1.0.9); ruby-atmos-pure requires log4r (>= 1.1.9), ruby-hmac (>= 0.4.0); fog requires mime-types (>= 0), ruby-hmac (>= 0); sinatra requires rack (>= 1.5.2, ~> 1.5), tilt (>= 1.3.4, ~> 1.3), rack-protection (~> 1.4); rest-client requires mime-types (>= 1.16)\n```\nAbout openstack, great you guys are giving it some attention - I think this could be big for CF! Meanwhile, I was pretty much hoping to setup CF on our Open Stack cloud this week. Do you think thats impossible right now?\n. This somehow worked when after the weekend for me - I'm currently thinking it might be an Open Stack issue where volumes cannot be attached if they are not on the same host as the virtual machine. Still investigating though, and in any case the micro bosh script should check if this failed instead of timing out, I guess.\n. Do you see the volume on Horizon? Can you mount it manually?\n. ",
    "foexle": "@frodenas you're right cloud_properties are not the correct topic for this settings my goal was simple as possible without many code changes :)\nI've read your PR and is still the same :) but in my opinion we should consider if th default value \"true\" are the correct way because it's a security issue if all users of openstack can see and use this image. You can see all seetings, passwords and whatever. So i would prefere to use the secure way as default.\nWhat do you mean ? \n. ",
    "fx": "+1\n. Same host here, likely not the issue. Not sure what's going on either, can't see any errors anywhere.\n. For me, apparently, it was a tgt/iscsi problem that only dropped a single line in syslog. Haven't resolved it yet.\n. Yea, it was a simple tgt misconfiguration issue for me.\n. ",
    "dpb587-pivotal": "Closing - hostnames and IPs automatically default to https and port 25555. Further CLI work is happening in https://github.com/cloudfoundry/bosh-cli.\n. Closing - new dev CLI uploads the newest version of dev+final releases (test). Give that workflow a try and feel free to discuss further in slack.\n. Closing - will be in the next stemcell via #1316.\n. Closing - upcoming CLI is written in Go so there is no longer potential shared Ruby code that can be refactored. The gobosh CLI does not do any special parsing of package dependencies, so all cyclical dependency checks end up happening on the director and a verbose error is given.\n. Closing - the relevant code has changed quite a bit since this was originally brought up and I don't think this is an issue anymore. Feel free to raise a new issue with updated CLI output if this is still an issue.\n. Closing - microbosh is deprecated and we use resolv.conf.d configuration files now.\n. Since this issue has been quiet and there are going to be no more changes and releases to ruby cli, I'm closing this. Feel free to continue discussion on a new issue in bosh-cli if this is still a feature of interest.. Closing - error messages are now handled by the external CPIs directly and have their own task log (bosh task --cpi 1234) where more details may be showing up nowadays. If this issue is still present, feel free to reopen on the Openstack repo.\n. @cppforlife, is this likely fixed since the s3cli merge in https://github.com/cloudfoundry/bosh/commit/01c6ab0ea96f162ffb82dfb8eb38379b5c8d95fc / since stable-3232?\n. Closing - this should no longer be an issue with the go client used by director and new bosh-cli.. Closing - stemcell execution environment has changed quite a bit since this was created and I doubt it's still an issue. Hopefully you're not needing to create custom stemcells anymore. If so, would like to hear why and if they can be done in a release instead of further stemcell sprawl; or feel free to create an issue in the new stemcell repo, cloudfoundry/bosh-linux-stemcell-builder.. Closing - if anyone is still interested, please feel free to send a PR to the warden CPI's repository now at https://github.com/cppforlife/bosh-warden-cpi-release.\n. Story #152339107 as part of Xenial stemcell work (won't be applied to ubuntu-trusty).. Closing - if this is still an issue or you're interested in seeing this change, please feel free to send a PR to the bosh-openstack-cpi-release repo.\n. Closing - feature set merged in https://github.com/cloudfoundry/bosh/pull/1027\n. Closing - no longer an issue.\n. Closing - fixed via 1e0d1c12a92346c6edf46368563aa31e5fb7f3a5.\n. Closing - fixed via #742.\n. Merged - thanks! Should show up in the next or following stemcell.\nRelated tracker story was #88572424.\n. Stemcell v3169 has monit 5.2.5.\n. Agreed, this error comes from the local system running out of space.\nSince this issue, we have released a new version of the BOSH CLI (found in cloudfoundry/bosh-cli). It handles error reporting slightly differently (and hopefully better in this situation), so I'm going to close this issue in this repo.\nPlease feel free to create new issues in the bosh-cli repo, or join us on cloudfoundry#bosh slack for more help or guidance when starting to use the new CLI!. Closing - bosh aws * commands are no longer supported. Some of this work is functionality is picked up by bosh-bootloader - please feel free to give that project a try.. @ljfranklin, I vaguely recall some work and refactoring around AKI selection since this issue was created. Do you happen to know if this is still an issue or can it be closed?. Closing - feel free to open an issue on https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release and reference this if you continue seeing this issue.. Thanks for the PR! The various CPIs have been extracted into their own repos now (see https://github.com/cloudfoundry-incubator/bosh-openstack-cpi-release), so we're closing this particular PR on the core bosh repo. If you're still interested in this boot volume sizing feature for openstack, perhaps bring it up on that repo.\n. Closing - we're no longer including a release in the stemcell so this shouldn't be necessary any more. Nice idea though (and sorry the PR sat for so long!)\n. Closing - I couldn't reproduce this in regular bosh, but we're also moving to a new go-based cli (dev currently happening in cloudfoundry/bosh-cli) which handles exits much more reliably and hasn't seen this issue. Feel free to give gobosh a try and provide feedback in that repo. Thanks!\n. Since we've mostly migrated to using cloud-configs at this point for shared network definitions, and we haven't heard this particular feature requested lately, I'm going to go ahead and close this. Feel free to reopen or add further comments if this is still of interest to you and you'd like to rebase the pull request with the latest code. Thanks!. Closing - director runs on ruby 2.3.1 and stemcells no longer include any version of ruby.. Closing - sounds like your director may not have been correctly re-deployed when testing config changes. Microbosh is now deprecated in favor of bosh-init - please feel free to give that a try (openstack doc) and reopen with updated information if the issue persists.\n. Closing - we're no longer supporting the validate-jobs command in the upcoming go cli. It was conflating deployment processes with release management processes and didn't catch all potential issues anyway.\n. Closing - original issue was resolved and [now bosh-init-managed] example manifests are maintained on bosh.io.\n. The routes job from the networking-release is a solution to this nowadays. Here's a blog post mentioning a similar, sample scenario.. This issue has been quiet for a while now. In the meantime, the director, agent, and openstack CPI have evolved quite a bit, so I suspect the issue may have been fixed. I'm going to close this, but if you're still experiencing this particular issue, please leave a comment with your current versions and error message to reopen (or feel free to open a new issue if anything else comes up).. Stemcell v3306+ with Director v258+ will wait up to 5 minutes for jobs to cleanly stop before it continues the update process.\n. This can error can happen when you try to download a file, but specify a destination file path instead of a destination directory. CLI generates the actual file name in that destination directory based on the job name where the file came from (useful when downloading from multiple instances). When passing a path, scp fails because it ends up trying to do /tmp/myfile.tgz/sourcefilename.ext.job_index.\nI don't think it is actually cleaning up the ssh artifacts before scp'ing, just that error occurred and it waits to dump out the error until everything is cleaned up.\n. Thanks, @coveralls?. @bortoelnino, try the solution mentioned in https://github.com/cloudfoundry/cf-release/issues/766\n. Closing - thanks, @uzzz, for mentioning this. Confirmed fixed.\n```\n$ bo ssh dummy 99\nActing as user 'admin' on deployment 'dummy' on 'director'\nTarget deployment is 'dummy'\nSetting up ssh artifacts\nDirector task 17\nError 70001: No instances matched {:job=>\"dummy\", :index=>[99], :deployment_id=>1}\nTask 17 error\nFailed to set up SSH: see task 17 log for details\n``\n. Someone else brought this up in [slack](https://cloudfoundry.slack.com/archives/bosh/p1454025113000259) a few weeks ago.\n. Director [v260+](https://github.com/cloudfoundry/bosh/releases/tag/v260) now supports configuring custom tags via [runtime config](http://bosh.io/docs/runtime-config.html#tags) and [deployment manifests](http://bosh.io/docs/manifest-v2.html#tags).\n. Closing - stemcell building-related code has been moved to [cloudfoundry/boxh-linux-stemcell-builder](https://github.com/cloudfoundry/bosh-linux-stemcell-builder), and there are no upcoming plans to bump monit due to reasons mentioned here and in #1295.. Closing - deployment-related tasks now include a deployment field (since [v255.4](https://github.com/cloudfoundry/bosh/releases/tag/stable-3213)).\n. fwiw... the incomplete scp command help also confuses the bosh devs :)\n. Successfully tested local merge of 78bf608...\n- scp - tested upload and download; tested older  ...` syntax\n- ssh - tested interactive and command argument\n- units are green\n- integrations are green\n```\n$ bosh help scp\nscp /   [--download] [--upload]\n[--gateway_host HOST] [--gateway_user USER] [--gateway_identity_file\nFILE] [--no_gateway]\n    Transfer files to (--upload) or from (--download) a job.\n    Note: for --download,  is treated as a directory\n    --download                                                Download  file from the job\n    --upload                                                  Upload  file to the job\n    --gateway_host HOST                                       Gateway host\n    --gateway_user USER                                       Gateway user\n    --gateway_identity_file FILE                              Gateway identity file\n    --no_gateway                                              Ignore gateway provided by the director\n$ bosh help ssh\nssh / [--gateway_host HOST] [--gateway_user USER]\n[--gateway_identity_file FILE] [--default_password PASSWORD]\n[--strict_host_key_checking ] [--no_gateway]\n    Execute command or start an interactive session\n    --gateway_host HOST                                                     Gateway host\n    --gateway_user USER                                                     Gateway user\n    --gateway_identity_file FILE                                            Gateway identity file\n    --default_password PASSWORD                                             Use default ssh password (NOT RECOMMENDED)\n    --strict_host_key_checking                                      Can use this flag to skip host key checking (NOT RECOMMENDED)\n    --no_gateway                                                            Ignore gateway provided by the director\n``\n. Closing - marker was completed a while ago and sounds like it's confirmed fix.\n. Closing - the VM-management work was released in v241 and this should be fixed now. Feel free to open a new issue with new stacktrace/details if you see it on newer versions of bosh.\n. Will be fixed as part of https://www.pivotaltracker.com/story/show/132719183. Closing - it happened :). Closing since there are existing workarounds.\n. Closing - the referenced story was fixed in [v234](https://github.com/cloudfoundry/bosh/releases/tag/stable-3148).\n. Closing - stemcell building-related code has been moved to [cloudfoundry/bosh-linux-stemcell-builder](https://github.com/cloudfoundry/bosh-linux-stemcell-builder), and there's no near-term plan for making this sort of ntp change. You may want to check out this [ntp-release](https://github.com/cloudfoundry-community/ntp-release) if you need more advanced NTP customization. Feel free to create an issue in the [cloudfoundry/bosh-linux-stemcell-builder](https://github.com/cloudfoundry/bosh-linux-stemcell-builder) repo with specific NTP requirements, or drop by [cloudfoundry#bosh](https://slack.cloudfoundry.org/) for further discussion.. Thebosh cckcommand is primarily used for fixing IAAS resources (like resurrecting lost VMs or fixing disks), whereas thebosh instances` shows details about what's running on the actual VMs.\nSo what you're seeing is expected behavior: all IAAS VMs are reporting so cck is happy, but instances checks the job processes and correctly notices the failure.\nbosh.io has some more details on the usage of the cck command.\n. As background... BOSH packages are frequently compiled from machines without internet access, so for this reason packages are designed to always expect files. If you're creating a release for public consumption, I'd encourage you to download the file and reference it from the package spec file. For more information about creating releases and blobs, you might be interested in this page on bosh.io which discusses how blobs are handled and how to avoid committing them to your repository.\nIf you're just doing wget-type for development it really is necessary for you, using a needless file is the only solution.\n. Closing - ssh only works with port 22, and we're moving to a new CLI where this shouldn't be an issue. Feel free to reopen there if you still experience this with the new cli, and include more detailed information about the commands you run and which hosts they're run on.\n. Versions older than 5.15 would start a parent service but would not necessarily wait until the process was healthy before starting the child.\n\nFixed: Issue #249: Implement hard dependency between services. If a service depends on another service, it will not start until the parent service check returns no errors. For example, if Apache depends on MySQL, Monit will not start Apache until MySQL is confirmed up and running and passes all its tests. Previously, Monit would start MySQL, but not wait for MySQL to be confirmed up and running before it started Apache.\nchangelog\n. Closing - hopefully superceded and cleaned up by other efforts.\n. Not sure what specific plans there are for autocompletion, but I think there will be some refactoring and I'd rather not try to keep the PR up to date with those efforts. If you're interested, you could try manually applying the pr patch locally and review the bosh shell-completion output for instructions on how to use it\n\nI hadn't seen that old chunk of bosh code referencing completion before - not sure how to use it or if it's still useful code.\n. Closing - bosh deploy --dry-run was introduced somewhere around v248/v3181 in the linked story. There's a related issue about specific details surrounding affected instances: https://github.com/cloudfoundry/bosh/issues/859\n. Oops... --dry-run is on develop but was never released in the CLI gem. But the new CLI has it if you want to give it a try... https://github.com/cloudfoundry/bosh-cli\n. Since you recreated the bosh-lite/director it has a new fingerprint. You should run ssh-keygen -R xx.xx.xx.xx in order to remove the old fingerprint from ~/.ssh/known_hosts. Then your existing bosh ssh ... command should work again.\n. I've run into this sort of thing as well; agree a fix would be nice.\n. Thanks for remembering and closing! Officially in v246+.\n. Closing - thanks for PR #1159 which was merged and fixed this issue!\n. # ubuntu\ninstalled packages\ndpkg --get-selections | awk '{ print $1 }'\npackage files\ndpkg-query -L $package_name | xargs file | grep -Ev ':\\s+directory\\s+$' | awk -F ':' '{ print $1 }'\nderived package list\nfrom the originally committed files\n- binutils\n- bison\n- build-essential\n- cmake\n- cpp\n- cpp-4.8\n- debhelper\n- dkms\n- dpkg-dev\n- flex\n- g++\n- g++-4.8\n- gcc\n- gcc-4.8\n- gettext\n- intltool-debian\n- libmpc3\n- make\n- patch\n- po-debconf\nbased on\n( while read file ; do dpkg -S $file | awk -F ':' '{ print $1 }' ; done ) < /tmp/ubuntu_dev_tools_file_list | sort | uniq\nre-generating the list from this package list results in the same size list\ncentos\ninstalled packages\nrpm -qa --qf \"%{NAME}\\n\" | sort\npackage files\nrpm -ql $package_name | xargs file | grep -Ev ':\\s+directory\\s+$' | awk -F ':' '{ print $1 }'\nderived package list\nfrom the originally committed files\n- autoconf\n- bison\n- cmake\n- cpp\n- flex\n- gcc\n- gcc-c++\n- gcc-gfortran\n- gettext\n- intltool\n- libmpc\n- libquadmath-devel\n- libstdc++-devel\n- make\n- patch\nbased on\n( while read file ; do for package in $( rpm -qf --qf \"%{NAME}\\n\" $file ) ; do if rpm -q $package > /dev/null 2>&1 ; then echo $package ; fi ; done ; done ) < /tmp/centos_dev_tools_file_list | sort | uniq\nre-generating the list from this package list results in 824 more files (diff in https://gist.github.com/dpb587-pivotal/ea6114fd4d3cd1c06ed6; additional files seem sane, though)\n. Thanks! Committed to develop via https://github.com/cloudfoundry/bosh/commit/614ded4d4efa184879d62f1f466718c4244fec8a\n. Tracker story for reference: https://www.pivotaltracker.com/story/show/111538726\n. Closing - httpclient was updated to 2.7.1 via f789444f4 and released as of 3189.\n. Closing - thanks for PR #1102 which was merged and fixed this issue!\n. I think this issue was fixed in bosh v250.\n. Closing - dogapi was upgraded to 1.21 via #1142.\n. I think this was fixed in v250 and can be closed.\n. Double check that your director is using the correct time. If it's noticeably off, AWS will respond with the error you mentioned.\n. Light stemcells include AMIs for ap-northeast-2 as of 3189...\n$ curl -s -L https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3189 | tar -xzOf- stemcell.MF | grep ap-northeast-2 | awk '{print $2}'\nami-37f63859\nRelated story: https://www.pivotaltracker.com/story/show/112368753\n. Closing - now corrected in the docs.\n. Which CPI/IaaS are you using? In AWS you can use resource_pools[*].cloud_properties.root_disk.size (docs) to get a larger root disk, but it doesn't look like other CPIs currently have this option.\n. As of v25 you can configure a root_disk.size on your resource pool (see the docs) for more details.\n. Related story: https://www.pivotaltracker.com/n/projects/1133984/stories/119844019\n. Closing - fixed in bosh-aws-cpi-release/v53.\n. I don't think the links feature is supported yet - the functionality is still evolving and specifications have changed since v246. Probably easiest to wait until it is fully merged in to bosh and more officially documented on bosh.io?\n. Closing - links are documented on bosh.io now.\n. Strange - I guess I'll try again once addons are merged. I was seeing it on a couple deployments, but don't really have additional details. Thanks for the update.\n. @voelzmo, is this still an issue?\n. @subhankarc, is this the same issue as the mailing list post? (just in case others are looking for more detail)\n. Closing due to inactivity - you might want to try using the default health monitor intervals. Feel free to reopen with more details and logs if you're still having problems after trying that.\n. Closing - fixed as of v256 via #115826379.\n. Related: #1045.. Closing - stemcell building-related code has been moved to cloudfoundry/bosh-linux-stemcell-builder, and there's no near-term plan for making this sort of ntp change. You may want to check out this ntp-release if you need more advanced NTP customization. Feel free to create an issue in the cloudfoundry/bosh-linux-stemcell-builder repo with specific NTP requirements, or drop by cloudfoundry#bosh for further discussion.. Related story: #114189259\n. Released in v255.4\n. Closing due to inactivity - related PR was closed and it sounds like it's no longer a priority.. Fixed in bosh/v255.4.\n. You mention io2, but it is not a public EBS Volume Type - double check you have the correct spelling in your manifest.\nWhen changing disk_pool.cloud_properties, you should indeed receive a new disk with the data copied over. The full docs are at bosh.io if you want to double check your manifest.\nOtherwise, please include the disk_pool configuration that you are using.\n. Looks like the error comes from this file.\nThere may be a few more people on the #general slack channel who have run into this CF issue before.\n. Alternatively, instead of doing the download twice thing: download once, use the local file scheme, and don't include sha1.\nreleases:\n- name: bosh\n  url: file:///Users/dpb587/Downloads/bosh-255.3.tgz\n. Double check that your director's AWS API credentials are still valid, and double check that the date/time on the director VM has not drifted and is correct.\n. I believe this is covered by Story 117716387 which is sitting on develop for our next release.\n. Upgraded and published in v255.4, 3213.\n. @Rylon, originally nothing was redacted and, for improved default security, we started redacting all properties. We want to retain the more secure, default behavior, so we're not planning on making it default again in the future, even through bosh config settings (since that would result in differing behavior between environments, which negates some of the \"expect secure defaults\" approach).. Not quite the same solution, but custom tags were mentioned in #936 for adding metadata about similar details.\n. Closing - fixed as of v256.\n. Hi @ChaosEternal - have you had a chance to verify this works as intended? The config also specifies size 5M which, if I recall correctly, takes precedence over time-based rotations.\nAlso, can you maybe explain your usage which is causing so many logs to end up in /var/log? Typically BOSH releases are configured to log everything in /var/vcap/sys/log (with a separate default logrotate setting of 50M), which means the /var/log logs are fairly quiet.\n. Closing due to inactivity and conflicts. Please feel free to reopen this PR once you have more details!\n. Related: story about supporting both v1 and v2 manifests\n. Since director ~v257, deployment manifests which have v1-type properties (e.g. networks) will ignore cloud-config settings.\n. Closing - we're not planning further enhancements or fixes to the CLI in favor of the upcoming bosh-cli. Feel free to give the beta a try if you're interested - it's much better at propagating failures.\n. @deepakdefender264, your screenshot is showing a different error than this issue. I'd recommend giving the suggestions from http://bosh.io/docs/tips#unreachable-agent a try. If you need further help, please make a new issue.. Closing - fixed as of v256.\n. Director v256+ with instance stemcells v3232+ respect a new director property (director.flush_arp) which will cause VMs to flush an IP when an instance is recreated.\n. Merged to develop f6114d9 - thanks!\nAll active development on BOSH happens on the develop branch. The master branch is our way of tracking what has already been released and published on bosh.io. Please make future pull requests to develop. :)\nThanks!\n. The mentioned behavior was released in director v259. A related pull request was #1424.\n. As of director v257, ruby 2.3.1 is used.\n. Story: https://www.pivotaltracker.com/story/show/118348007\n. Story: https://www.pivotaltracker.com/story/show/118348007\n. I believe this was fixed in bosh director v257+.\n. I believe this was fixed in bosh director v257+.\n. Thanks @allomov!\nAssuming all the integration tests also pass with this, I think this should be merged. Our current story about upgrading to a newer version of Ruby blocks on eventmachine upgrade anyway and 1.0.4 looks like it at least compiles on ruby 2.3.0.\n:+1: for integration testing and merge...\n. Thanks @allomov!\nAssuming all the integration tests also pass with this, I think this should be merged. Our current story about upgrading to a newer version of Ruby blocks on eventmachine upgrade anyway and 1.0.4 looks like it at least compiles on ruby 2.3.0.\n:+1: for integration testing and merge...\n. This was fixed in director v257.\n. This was fixed in director v257.\n. Last week we had to do some refactorings on code which will affect this PR. In #117557245 we started adding the deployment's teams onto the task's db record to allow deleted deployments to still be followed. That also means we can more cleanly handle the listings endpoint for deleted deployments like your PR deals with. Story #117845011 has been added to the backlog which deals with fixing this listing/deleted deployment issue in light of the new tasks.teams field.\n. Last week we had to do some refactorings on code which will affect this PR. In #117557245 we started adding the deployment's teams onto the task's db record to allow deleted deployments to still be followed. That also means we can more cleanly handle the listings endpoint for deleted deployments like your PR deals with. Story #117845011 has been added to the backlog which deals with fixing this listing/deleted deployment issue in light of the new tasks.teams field.\n. A deployment field was added to tasks in director v255.4.\n. A deployment field was added to tasks in director v255.4.\n. Closing - feel free to reopen with more details.\n. Closing - feel free to reopen with more details.\n. Updated in stemcell 3215.4 - https://github.com/cloudfoundry/bosh/releases/tag/stable-3215.4\n. Updated in stemcell 3215.4 - https://github.com/cloudfoundry/bosh/releases/tag/stable-3215.4\n. Generally it's a rolling forward release policy for the open source side of things. The 3146.* series that you're using is a bit of a special case and used by internal products. As of 3177+, the underlying data structures were changed and refactored, dropping the intermediate VM references and may have fixed the issue.\nI don't think the branching/release policy is officially documented anywhere. @cppforlife, perhaps that's something we can document, to help both internal and external expectations.\n. Generally it's a rolling forward release policy for the open source side of things. The 3146.* series that you're using is a bit of a special case and used by internal products. As of 3177+, the underlying data structures were changed and refactored, dropping the intermediate VM references and may have fixed the issue.\nI don't think the branching/release policy is officially documented anywhere. @cppforlife, perhaps that's something we can document, to help both internal and external expectations.\n. :link: someone also raised the branching strategy question in cloudfoundry#bosh on 2016-06-17T05:29Z\n. :link: someone also raised the branching strategy question in cloudfoundry#bosh on 2016-06-17T05:29Z\n. Closing - should be fixed as of v258.\n. Fixed in director v256.\n. Fixed in director v256.\n. @ramonskie: might want to check if you have configured the correct number of consul agents. Perhaps look for agent-client.is-last-node.result in your logs to verify your values, and also double check that you have correctly configured your list of consul.agent.servers.lan.\n. Closing - these particular errors are release-specific, and the previous comments recommend useful links for resolving these issues and more.. Closing - these particular errors are release-specific, and the previous comments recommend useful links for resolving these issues and more.. Closing - use a more powerful instance, or reconfigure your update_watch_time to be a shorter interval.\n. Closing - use a more powerful instance, or reconfigure your update_watch_time to be a shorter interval.\n. Fixed in director v257.\n. Fixed in director v257.\n. Closing - Ruby CLI is feature frozen in favor of development on the upcoming go-based CLI. Feel free to give it a try and provide feedback there. All commands in the go CLI respect a deployment flag.\n. Closing - Ruby CLI is feature frozen in favor of development on the upcoming go-based CLI. Feel free to give it a try and provide feedback there. All commands in the go CLI respect a deployment flag.\n. UUIDs were added around v244 as part of the efforts to make deployments availability zone-aware. There's no configuration option to disable UUID's and you should try to move away from the indices in favor of UUIDs.\nWith AZ-aware instances, index-based instances could result in gaps and confusion, so UUIDs were added and are now the preferred method of referencing VMs.\n. UUIDs were added around v244 as part of the efforts to make deployments availability zone-aware. There's no configuration option to disable UUID's and you should try to move away from the indices in favor of UUIDs.\nWith AZ-aware instances, index-based instances could result in gaps and confusion, so UUIDs were added and are now the preferred method of referencing VMs.\n. Can you provide the logs from either the failing monit processes or monit itself.\nYou also might try increasing the default memory/cpu available to the bosh-lite VM - the stop/update processes might be pushing the bosh-lite VM to its limits, causing it to hang.\n. Can you provide the logs from either the failing monit processes or monit itself.\nYou also might try increasing the default memory/cpu available to the bosh-lite VM - the stop/update processes might be pushing the bosh-lite VM to its limits, causing it to hang.\n. The VMs will try for ~90s to reload the monit processes (source), after which it will fail if the reload has not completed. Not sure if it's a agent behavior bug, monit bug, bosh limitation, or just a need for a better host for the bosh-lite VM.\n@cppforlife, any thoughts?\n. Related: https://github.com/cloudfoundry/bosh/issues/417\n. Related list topic: https://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/message/PXQYTMJCZ2ZR7F6AIDX42X3DANKEHBYF/\n. @jianqiu, sorry for the confusion - the Travis build failures are unrelated to your changes.\n. @jianqiu, sorry for the confusion - the Travis build failures are unrelated to your changes.\n. It might be helpful to see /var/vcap/bosh/log/current to know exactly what steps are taking so long for this. Should just be laying down pre-compiled templates and making directories, which shouldn't take long. Perhaps take a look at those, or provide them in anonymized form with full output or just the debug messages.\nfwiw, might be similar area of code as https://github.com/cloudfoundry/bosh/issues/1245\n. @cppforlife, was able to reproduce.\nProbably need to add proxy_set_header X-Forwarded-Port $server_port or similar in director's nginx.conf.\n. Merged - thanks!\n. Merged - thanks!\n. Merged - thanks!\n. Merged - thanks!\n. Closing - postgres was updated to the latest 9.4 version in v257, although the mentioned vulnerabilities did not affect 9.4, only 9.5.\n. Closing - postgres was updated to the latest 9.4 version in v257, although the mentioned vulnerabilities did not affect 9.4, only 9.5.\n. Caused by https://github.com/cloudfoundry/bosh/pull/1226\n. Caused by https://github.com/cloudfoundry/bosh/pull/1226\n. Merged - thanks!\n. Merged - thanks!\n. Fixed in v262.. 1. Double check that you actually have the docker-registry security group configured in your region The CPI has an unfortunate behavior where invalid security group names are silently ignored (issue).\n2. The t2 instance types don't come with instance storage for an ephemeral disk which BOSH needs, so the CPI automatically creates an EBS \"ephemeral\" disk for that purpose (example). In your case, the three attached disks are correct.\n. @StanleyShen, aws-cpi/v53 was released with improved security group lookups. Might want to give that a try and see if that helps?\n. Fixed in v260.1.. For merger...\n- reminder this was PR'd against master instead of develop; merge manually\n- unit tests (2.3.1/sqlite) were green when merged with develop at 7fffa21ddb99af510fda0aa62dbc3f2da0133077\n. @chrisrana, you can try using bosh_cli/1.3232.0 and director/256.2. DNS fixes are mostly director related, but would still recommend using the latest stemcell, currently 3232.4.\n. I see. The bosh micro deployment method is deprecated and will be discontinued soon. I'd recommend migrate to bosh-init and upgrade your versions. If you need microbosh, you can still use your current procedure, just bump STEMCELL_VERSION.\n. @allomov, fwiw the same potential problem code is in aws-cpi, not just vsphere-cpi.\n. Related: auditd does not start (#1305)\n. I've seen this as well. I believe adding content_type :json to the relevant controller methods before returning the actual JSON data would be a fix.\n. Hi @0x1mason - glad to hear you were able to get CLI working locally on Windows! Windows support has come up a couple times over the years, but it hasn't been a high priority.\nI think the CLI has some planned enhancements which may make the changes difficult to merge right now, but, community-wise, you could still push your changes to a branch in case others are interested in trying out the CLI on Windows in the meantime. I'd also personally be curious to see what kinds of changes you found were required.\nThanks for raising an issue to discuss this!\n. Closing - we won't be making further changes to ruby CLI in favor of continuing development of our go-based bosh-cli. There is currently an open issue about enabling builds for Windows if you want to follow that there - https://github.com/cloudfoundry/bosh-cli/issues/53.. Helpful thanks! We'll take another look.\nMentioned error looks like...\nPG::Error: ERROR:  update or delete on table \"packages\" violates foreign key constraint \"compiled_packages_package_id_fkey\" on table \"compiled_packages\"\nDETAIL:  Key (id)=(103) is still referenced from table \"compiled_packages\".\n. Interestingly, I don't think our fix has made it to an official release - but I'm happy to hear it's working! From our fix for this, it looked like the issue may have occurred when blobs could not be immediately deleted. We did refactoring to at least do a better job at raising full error details. Those changes should be in the next release.\nNext release should also fix the versioning inconsistency you noticed (I agree that is confusing).\n. Just for reference, some existing issues and discussions on this topic for more context...\n:link: merged PR, Update monit to 5.2.5\n:link: open PR, Update monit to 5.14\n\nIt sounds like for now we can only update to 5.2.5. (comment)\n\n:link: closed PR, Upgrade to monit 5.8.1\n\nIn future we will move away from monit. That will allow us to cross platform (windows), more modular and hopefully less people would have to deal with monit annoyances. (comment)\n. :link: someone also raised this issue in cloudfoundry#bosh on 2016-06-09T14:27Z\n. @djvdorp, I think you have been able to get CloudFoundry deployed now with some help from bbl, so I'm going to close this. Feel free to reopen if that's not the case, or start a new issue for other problems.\n. Merged in cec8c74e1be7f8730afe96c3ae669641ce612672 on develop. Thanks for your PR!\n\nAll active development on bosh happens on the develop branch. The master branch is used as a record of the latest release of bosh.\nThanks again! :)\n. This can happen if you uploaded the release on an older director which didn't understand the links v2 metadata. Try deleting and re-uploading the release to the director to ensure it has the latest metadata stored.\n:link: cloudfoundry#bosh@slack - same issue showed up on slack a couple months ago\n. This adds a fair bit of extra complexity that we have to maintain. Couldn't we just rely on the existing DJ queues mechanism, switching the local-dependent jobs to @queue = :local and append local when the workers see a director job colocated?\n. Ah, I misunderstood there would be multiple directors running against the same database. I don't think my idea is relevant there. Sorry. Ignore!\n. Merged in e77bcb226fc492e4d3847d18291638796e1b013a on develop. Thanks for your PR!\nAll active development on bosh happens on the develop branch. The master branch is used as a record of the latest release of bosh.\nThanks again! :)\n. It'll be merged to master at our next major stemcell release. I don't currently have an ETA - we've been doing minor releases of 257 lately while some other feature work settles in develop. I'm sure you've already found it, but here's the doc with some pointers for building stemcells if you wanted to build a local one with your changes and other recent commits for testing.\n. :link: follow bugfix up to https://github.com/cloudfoundry/bosh/pull/1310\n. Good idea on including the build link!\n. @cppforlife, I'd like to see this merged. Do you have any objections, barring me rerunning latest units/integrations? Provides better consistency in the external CPI API.\n. Merged via 2a626af7b30f44289c0363000be16405c355525e. For future readers, if the above suggestions don't work, the new BOSH CLI which is still under development handles TLS connection errors a bit more clearly if you want to give it a try and provide feedback. Link to cloudfoundry/bosh-cli repository to build and some migration notes.\nRuby CLI is mostly frozen, so it's probably not something we'll be trying to fix in favor of suggesting users switch to the new CLI.\n. Closing - not changing the Ruby CLI. Avoid using outdated, system Ruby versions in the meantime.\n. I believe this behavior was reverted in v257.3 where the mentioned DNS refactorings were fully reverted, but per cloudfoundry#bosh-core-dev discussion it sounds like dns_record_name should previously not have been null even when DNS is \"disabled\" (since DNS could be managed externally as well). With that in mind, you'll probably continue to see this in future bosh major versions and should plan for it accordingly.\n. Merged - thanks!\nAlso, it looks like the mime-types gem bump caused issues in our Ruby 1.9.3 tests, so we reverted just the mime-types gem in 91b3c4922f26c89fd7999d01afb301527c94c27d. It didn't look essential for the main fog/excon bumps, so this is just an FYI. Let us know if we need to discuss the mime-type revert further.\n. Thank you for your pull request! There's currently some work and refactoring happening on the CLI side of things, so we'll be a bit delayed in taking a closer look at this. We'll update this PR once we're unblocked from those other CLI changes and are better able to consider testing and merging this.\n. Ruby CLI is now frozen and we aren't planning on publishing any new versions in favor of further developing the new cli, so I'm going to close this. You can find the new CLI in the bosh-cli repo - feel free to give it a try and provide feedback there. There's a Windows-related issue about producing binaries - https://github.com/cloudfoundry/bosh-cli/issues/53.. I'm kind of surprised the \"expected scopes\" does not include bosh.teams.team1.admin in the error message since the user that created team11 deployment should have had that scope. Even more to see *.read listed.\nI know you said a brand new director, but can you confirm which director version it's running?\n. We were able to reproduce the issue. The ssh task is not properly propagating the deployment's teams so the SSH setup task could be not retrieved once started. Additionally, it looks like the \"One of the following scopes...\" message is currently showing an inaccurate message. Fixed on develop via https://github.com/cloudfoundry/bosh/commit/267be3ab5551864e3d05098f6b348b2b6c9a99f3.\n. Thanks for feedback! This is a duplicate of #1312 so I'll close and link this, but feel free to add any additional thoughts on that issue.\n. Looks good to me for a squash+merge. Independently verified the new, vendored gem checksums as of c62a51cba3d022e9520a42af76bfde982ada0e3c.\n. Merge note - make it to develop instead of master.\n. Nowadays, the new bosh CLI will respect the original target, so wildcard certs should work as expected. Hopefully you can give it a try, and feel free to open a new issue in that repo if you run into any issues. Since the ruby CLI is frozen, I'm going to close this issue.. We ran units locally against mysql and postgresql to double check.\n. Committed to develop via 3e7d904b7b478c5d1762d2faf372e96ecec2167a\n. As you mentioned, the previous, accidental behavior was indeed due to a bug. The intended behavior is for jobs to enumerate every property they need in order to function, and they should not rely on any other properties. One big benefit of this is that you can look at releases and know exactly which properties are going to influence the behavior. Another side benefit is that it's a bit more secure since other jobs should not be looking at unrelated properties, lest they be accidentally exposed. The explicitness can also help the director better know which exact jobs are being changed.\nFor the PaaS admin case, it's better to think about the properties as job-specific, not VM-specific. In that sense, if the haproxy job is failing, you should expect to find all related properties to have been dumped in haproxy's templates.\nThe correct fix is, like you said, to update your release job specs to include the additional properties that the job depends on.\n. I think it's intentionally hard to find / nonexistant. If anything, perhaps it should exist just to recommend not using it :)\nPre-packaging is not recommended because the steps do not necessarily happen in a clean environment which can lead to corrupted releases with no audit as to what happened. And since multiple developers or teams often work on a release, the development environments are usually a bit different and can lead to complicated issues. Regular packaging scripts, on the other hand, are always executed on compilation VMs where the build environment should always be clean.\nFrom bosh.io...\n\nNote: Use of the pre_packaging file is not recommended, ...\n\nA similar question was raised on the lists a while ago.\n. Merged - thanks!\n. Helpful - thanks!\n+1 from me, and vendor gems are verified.\n. Merged - thanks!\n. @jmcarp: unfortunately I'm not sure. I'm hoping we can release from develop sooner rather than later, but some other priorities and fixes for previous releases and stemcells have been taking precedence lately.\nFor the short-term, you might need to manually pull in this PR's patch on your director, or build a local release from it.\n. Do you/they happen to be using CLI <3232? I would have expected #1184 to have helped with this, but it was only added in 3232+.\n. Oh, interesting. Yes, the \"fix\" would have been in the latest CLI gem. I\nguess it would still need to be fixed after all.\nI suspect progress bars will be working correctly once the CLI\noverhaul work is finished. Thanks for reopening this in the meantime.\nOn Wednesday, July 20, 2016, Marco Voelz notifications@github.com wrote:\n\nReopened #1352 https://github.com/cloudfoundry/bosh/issues/1352.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1352#event-728883540, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/APYVcLn52xdQUOSfzwsSg1NCQv9hIopsks5qXhB9gaJpZM4JMfBv\n.\n\n\nDanny Berger\n. @cppforlife, this seems like poor UX. Would we be willing to accept a PR for a new property with something like p('compiled_package_cache.options.region', p('blobstore.s3_region', nil)) for sane behavior but maintaining existing unexpectedness. Since compiled package cache could be a separate bucket, it seems like region should be independently configurable.\n@aristotelesneto, nice investigation.\n. Closing - change was made in 3c5265a5971eda15a277d16e5b4f8069f506d80e.. :+1: - good fix. Team typically use these scripts from inside a vagrant from ci/docker/Vagrantfile where both vagrant and container will have /opt/bosh, so we overlooked this.\n. Dev merge note - manually make it to develop.\n. Thanks for the PR! At this point, ruby CLI is frozen and we're no longer publishing new gems while we focus on finishing up the new bosh-cli. I'm going to close this PR, but please feel free to give the new CLI a try when you get a chance - I don't think it exhibits this bug, and we'd appreciate your feedback.. Story: https://www.pivotaltracker.com/story/show/127824033\n. @lexsys27, based on code, I'm guessing you switched app from one availability zone to another. Any chance you recall and can confirm that was the scenario (or perhaps you had a different scenario that we should consider)? Thanks!\n. I believe the story for this is: https://www.pivotaltracker.com/story/show/119060367\n. Closing - ruby cli is frozen, so I'm closing. The new beta gobosh does a better job at rendering progress bars and I haven't seen any issues with it. Feel free to reopen something in that repo if you start seeing similar issues.\n. Closing - the new golang bosh CLI supports this behavior by configuring credentials_source as env_or_profile.. It does extract the release, but depending on the manifest/yaml libraries, ordering may not be guaranteed. You should instead unpack the original tarball, overwrite the release.MF and recreate your tarball before uploading.\n. Closing - please try and use the release commands and workflows supported by the CLIs.. @keymon, do you continue to see this failure on recent versions of director?. Closing - sounds like recent director versions have resolved this. Feel free to follow up and add updated details to reopen.. Related story: https://www.pivotaltracker.com/story/show/129890801\n. Great idea! But... I don't think it's working as expected. We tried this out locally and it seems like all three builds are running the last group of tests. Does this happen in your environment, or do you have suggestions on what other changes might be required?\nbuild1: bundle exec parallel_test 'spec/integration' -n 24 --only-group 3,6,9,12,15,18,21,24 --group-by filesize --type rspec -o '--format documentation'\nbuild2: bundle exec parallel_test 'spec/integration' -n 24 --only-group 3,6,9,12,15,18,21,24 --group-by filesize --type rspec -o '--format documentation'\nbuild3: bundle exec parallel_test 'spec/integration' -n 24 --only-group 3,6,9,12,15,18,21,24 --group-by filesize --type rspec -o '--format documentation'\n. Closing - ruby CLI is feature frozen in favor of gocli, so I'm going to close this. Since bosh commands require the environment/deployment options, this shouldn't be an issue.\n. Story - https://www.pivotaltracker.com/story/show/139906531. I guess it's more of a related story... trying to force certificates to be created/managed beforehand (optionally, via bosh-cli variables; but whatever they can still be generated locally by openssl). Currently paused for other priorities.\nOld ruby cli didn't do a good job with certificates, but it will continue to work with newer certificates. New cli requires the newer certificates.. Closing - fixed in v257.9.\n. Related/duplicate of #1269.\nAlso, <details>? Clever! Never considered that before.\n. Closing - stemcell 3262.12+ has some logrotate fixes in addition to mounting /var/log to ephemeral disk.\n. Merged - thanks!\n. This PR is into master, which we do not allow.\nWe will verify on develop, once it is merged on develop we will close this PR.\n. Thanks for the PR - this is now merged into develop via 5e03bccbcaae06f459912b2fa8cacb4467871a7f. The following was additionally verified: \n- sha1 of progressbar-0.21.0.gem\n- integrations (https://main.bosh-ci.cf-app.com/builds/149817)\n- units\n. Probably kept it historically to maintain backwards compatibility with existing tooling, so it probably won't be changed. Doesn't look like the new CLI supports --all, so that's good!\n$ gobosh logs -h\n...\n[logs command options]\n          --dir=            Destination directory (default: .)\n      -f, --follow          Follow logs via SSH\n          --num=            Last number of lines\n      -q, --quiet           Suppresses printing of headers when multiple files are being examined.\n          --job=            Limit to only specific jobs\n          --only=           Filter logs (comma-separated)\n          --agent           Include only agent logs\n. Closing - ruby cli is deprecated in favor of bosh-cli.. Commits were rebased as https://github.com/cloudfoundry/bosh/commit/6ba9c7004a336d491201d60bf79699d018de03cf, https://github.com/cloudfoundry/bosh/commit/9fe4cd8e76e689b5c2360bef16c7345e73d67780.\n. Closing - this behavior has changed with the new cli.. From cloudfoundry#bosh for posterity...\n\nmight be able to use \ufeff\u2060\u2060\u2060\u2060cmp\ufeff\u2060\u2060\u2060\u2060 and only run it if the file is different; maybe something like \ufeff\u2060\u2060\u2060\u2060cmp --silent smoke_tests.test blobs/smoke_tests.test || bosh add blob smoke_tests.test\ufeff\u2060\u2060\u2060\u2060\n\nRelevant piece of code is here. Seems reasonable to make it a warning though.\nIt seems like the new CLI treats it as a brand new blob and completely replaces it in blobs.yml, resulting in duplicate blobs in the blobstore. (/cc @cppforlife)\n. Closing - the ruby CLI is no longer maintained, and the new CLI has different expectations with blobs. Feel free to create an issue in the CLI repo with other bugs or concerns.. Closing - thanks @poblin-orange + @CAFxX for jumping in!. Which stemcell are you using? Possibly 3262.9? If so, you might want to try on 3262.12. The invoke-rc.d: dangling symlink: /etc/rc2.d/S20rsyslog message is suspicious and may have been due to improper rsyslog upstart configuration which was fixed in 3262.12.\nOtherwise, the post-installation script of rsyslog failed - perhaps investigate why that script failed and see what the output is which may make it easier to diagnose.\n. For what it's worth, BOSH does work this way - if a deployment sees the package is currently compiling it will wait (up to 15 minutes) and then re-check if the package was compiled before attempting to compile the package itself.\nIt sounds like 15 minutes is not sufficient in your environment. It does seem reasonable to increase that hard-coded timeout since locks will automatically be reaped if a compilation process dies. Some packages can definitely take longer than 15 minutes to compile.\nDo you have a suggestion on how long a deployment should block on some other deployment?\n@cppforlife probably has some thoughts about this and if we'd be able to merge a PR with some sort of change.\n. Your local release directory looks fine - it was able to download blobs and create the release tarball which it uploaded to the director.\nIt looks like your director is improperly configured for the S3 blobstore. Double check the access keys that you configured your director for, and perhaps review this doc. It sounds like you set provider to s3 but have not configured access_key_id and secret_access_key (properties).\n. This PR looks like it was superseded by https://github.com/cloudfoundry/bosh/pull/1454, although I don't see any reference to :rsyslog_config there. If this PR is still relevant after those changes, can you rebase/fix for whatever changes are still needed? Otherwise I think this can be closed.\n. Published bosh-init/0.0.97 and bosh-cli/0.0.67 with go/1.7.1.\n. Sounds like you may be using the 3263 stemcell version with an older version of garden. Make sure you're on a recent version of garden (v0.340+). There were incompatibilities with the 4.4 kernel which would show this error.\nYou can also try bumping the stemcell version, there have been a few fixes for GCE stemcells lately.\n. As @d suggested, bosh export release is a good solution. Another option you could consider is configuring a common compiled package cache between your environments.\n. Thanks for the docs PR! Closing this in favor of that PR.\n. You could try reviewing bosh task --debug 75 - it may provide you details on what it's waiting for. Alternatively, ssh in on the not-terminating VMs and tail the logs to see if any processes are still running which is causing things to hang; specifically /var/vcap/sys/log/**/drain*.\n. It is a release-specific concern, so perhaps start by looking into cf-release. The release keeps telling director to wait on shutting down the jobs because drain is still running, so director just keeps waiting. Drain scripts can genuinely take a long time in the cases of data migration, so director doesn't have a timeout on how long a drain takes.\nYou may want to check on the value of your dea_next.evacuation_bail_out_time_in_seconds property - it may be set higher than you want. It used as a timeout when the DEA wants to shutdown to try and evacuate all the cells off.\n. You'll find some documentation about drain scripts on bosh.io. The director/agent executes the script when the VM is about to shut down. Most releases take advantage of this to help relocate services or processes to other VMs (and avoid noticeable service interruption). In this particular case, the intent is to relocate your containers off and avoid app downtime.\nIn your case, it may be helpful to do a bosh stop --skip-drain, followed by a bosh delete deployment because it seems like bosh delete deployment is executing drain scripts.\n. Related story: https://www.pivotaltracker.com/story/show/131020691\n. Closing - stemcell-related code has moved to cloudfoundry/bosh-linux-stemcell-build. Feel free to fix the mentioned multiple NICs issue and submit a new pull request there when ready.. I think, in the future, the intention is to use bosh instances if you need to identify which specific instance you may want; then pass the name/id as the argument to bosh ssh. Generally, you already know which specific instance you're interested in before attempting to connect (from logs or alerts), or you're trying to multi-ssh on a full instance group.\nIf it's important to you, can you describe the use case in which you don't know the instance you want to connect to beforehand and need to rely on bosh ssh enumerating them?\n. Story: https://www.pivotaltracker.com/story/show/131738547\nThis repo doesn't currently use a pr/issue/tracker integration.\n. Merged onto develop via a206491fa1becca018288ff02359af7c7a0297c2.\n. Sounds like you have an old director. Upgrade to a recent director which understands the newer release metadata.\n. Merge story: https://www.pivotaltracker.com/story/show/132265303\n. imo, bosh ssh, without a specific instance, puts you at the mercy of director to connect to any particular instance which happens to be available. I think you should always have to give it an instance if you want interactive; and the only use for not specifying a slug is when the deployment contains one instance. /opinion\n. Closing - as of ~v260.1, this should be working properly.. Thanks!\n. Closing - starting ~v260.1, incomplete heartbeats should no longer be sent (via 76a6d7343 in #132265151).. Closing - changed via f6ab97d7953b11d019c5c9a9fcf6ab6598ce737a and available in v258+.. From @cppforlife on https://github.com/cloudfoundry/bosh-init/issues/114#issuecomment-253893358...\n\ndefinitely related to global cache\n. Thanks for looking into the issue! Looks like this is a duplicate of PR #1461, so I'll close this in favor of having further discussion in a single place. You could also try out that patch if you needed to make some progress in the meantime.\n. Story: https://www.pivotaltracker.com/story/show/132777717\n. Generally I don't think users should be caring about where director is storing off blobs, so it does seem like more debugging information where SSH'ing or reviewing the manifest are acceptable. The other fields are describing functionality, and this seems more like configuration. Can you describe a bit more about why adding it would be helpful?\n. Story: https://www.pivotaltracker.com/story/show/133637689\n\n@cppforlife prioritize; it's apparently keeping @voelzmo up at night!\n. No stress :)\nThanks!\nOn Thursday, November 3, 2016, Marco Voelz notifications@github.com wrote:\n\nMerged #1482 https://github.com/cloudfoundry/bosh/pull/1482.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1482#event-846116680, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/APYVcMnuYgpQhrmyQnwNY137SBHtFiBpks5q6a3KgaJpZM4KcAyE\n.\n\n\nDanny Berger\n. @tushar-dadlani was running into this yesterday as well.\n. We recently removed the --raw flag (it was broken). Can you describe a bit more about how you're using the raw output in your scripts? Perhaps we can find an alternative.\n. Oops, I misunderstood. We are currently working towards a new CLI written in go which is refactoring quite a few things. One of the new features is a --json flag which renders output in a script-friendly format. You might want to check out the work happening there (https://github.com/cloudfoundry/bosh-cli) and see if --json would be useful to you. The repository is still under active development and not yet stable, so things may end up changing (including the JSON output format) but it can at least give you an idea about where it's heading.\nAnother option may be for you to use the director library from that repository in your own go scripts - you can make the API calls directly and work with the responses in whatever way works best.\nDo you think either of those will help in your automation needs?\n. We're switching away from using indices in favor of IDs, so the change was intentional. Hopefully you can start transitioning your tooling to be ID-based? If you have any interesting stories or issues on that migration path, it would be interesting to hear about them.\n. Did anything show up in the director logs in /var/vcap/sys/log/director/*.log? Would also be curious to see what the memory usage of the running processes were.\n. Closing - ephemeral disk configuration changes cause a VM to be recreated. It seems odd to be attempting to relocate an ephemeral disk - the data on it is intended to be temporary. Feel free to reopen if this is a misunderstanding.. Closing due to inactivity - please feel free to respond with more details and answers to the questions above to reopen.. Closing - thanks for looking into this. We've recently moved stemcell functionality into a new repo (cloudfoundry/bosh-linux-stemcell-builder) and we no longer use STEMCELL_BUILD_NUMBER (in favor of CANDIDATE_BUILD_NUMBER).. Story: https://www.pivotaltracker.com/story/show/134017149\n. Story: https://www.pivotaltracker.com/story/show/134094913\n. Closing - this PR sounds reasonable, but the CLI is now frozen in favor of switching to the new, bosh-cli and we won't be publishing new versions of ruby CLI. The new CLI doesn't exhibit this particular bug, but if you get a chance to try it out and notice the issue (or any others), please feel free to raise an issue in that repo. Thank you!. @zankich yup. currently there is a small diff between agent in other stemcells vs warden stemcell. planning to fix that shortly (or at least rebase temporarily to latest).\n. @liuweichu, the file is installed at the stemcell level, so it should always be there.\nCan you clarify how you are customizing PS1?. @liuweichu, I'm not sure the stemcell should be adding conditionals for the files it is definitely installing. Instead of your job deleting the file, have you tried overwriting that file instead of .bash_aliases changes? Or tried emptying the 00-bosh-ps1 file?. Closing - fixed in recent stemcells.\n```\n$ bosh upload-stemcell bosh-stemcell-3363.12-warden-boshlite-ubuntu-trusty-go_agent.tgz\n$ bosh ssh\n...snip...\nLast login: Tue Mar 21 23:48:33 2017 from 192.168.50.1\nTo run a command as administrator (user \"root\"), use \"sudo \".\nSee \"man sudo_root\" for details.\nminio/f77bb507-9e6b-4542-bc21-b610f4973402:~$\n``. @goupeng212, are you still experiencing this on recent director/stemcells?. Should plugins be inheriting the tags configured by deployments+runtime-config?. Sincemetron_agentwas listed as not running, but it was running when you ranbosh instances --ps, you might just need to increase thecanary_watch_time/update_watch_time(http://bosh.io/docs/deployment-manifest.html#update). It might have just needed some additional time to start depending on your environment.. Related story [#146034015](https://www.pivotaltracker.com/story/show/146034015) / PR #1702. Related functionality has been merged and will be in the next director release.. If the environment is still around, are you able to verify that the log files still exist in/var/vcap/store/director/tasks/{task_id}/*? Does thebosh task {task_id}` still execute and return successfully?. @mfine30 FYI, the team did some previous investigation in bumping rsyslog on the 3469.x through story #155528288. The intent was to let people test out that stemcell before bumping it everywhere, but I don't think we ended up getting much feedback on it.\nFor what it's worth, we're still installing this same rsyslog in the Xenial stemcell (which is probably bad)...\nshell\n$ wget -qO- https://s3.amazonaws.com/bosh-aws-light-stemcells/light-bosh-stemcell-97-aws-xen-hvm-ubuntu-xenial-go_agent.tgz | tar -xOzf- packages.txt | grep rsyslog\nii  rsyslog                               8.22.0-0adiscon1trusty1                    amd64        a rocket-fast system for log processing\nii  rsyslog-gnutls                        8.22.0-0adiscon1trusty1                    amd64        TLS protocol support for rsyslog\nii  rsyslog-mmjsonparse                   8.22.0-0adiscon1trusty1                    amd64        Parsing/handling of CEE/Lumberjack JSON messages in rsyslog\nii  rsyslog-relp                          8.22.0-0adiscon1trusty1                    amd64        RELP protocol support for rsyslog\nWe may want to consider if this is something we should bump in a minor version, or if it warrants a full new version.. In the backlog as Story #159533409. I've gotten used to it; but I do think it would make for a better UX, so I think it's a good idea.\nCLI translation is a good idea. Assuming we won't be going to a world where director supports only spec.yml.. This was fixed in v261.. I believe this was fixed by https://github.com/cloudfoundry/bosh-linux-stemcell-builder/commit/c7b403b0aced4785cb8261fa2ecf525448bfc689.. Closing - sounds like this was answered. To summarize, occasionally major director upgrades may require VMs to be recreated due to the way state is configured and tracked.. Looks more like https://github.com/cloudfoundry-incubator/consul-release rather than the community one. Similar issues and resolutions from this search and this search might be a good first reference for troubleshooting.. Related current state... we recently heard that, when opting in to both enable_cpi_resize_disk and xfs persistent disks, filesystems are not being resized from the OS perspective and requires a manual xfs-growfs after. Tracked in #159212614.. Closing - BATS docs were removed from this repo as part of d37203b446a9a1fc1c3dee9aec14ad555a6045bd; and related docs can now be found in the cloudfoundry/bosh-acceptance-tests repository.. Oops; was curious and only looking at a single commit. Ignore me.. Fixed in v262 via story #139906531.. Story: https://www.pivotaltracker.com/n/projects/1954971/stories/139612921 (the story you found looks like it was the original and lost between projects; will probably delete that one).\nRelated PR: https://github.com/cloudfoundry/bosh/pull/1628. @allomov, bosh-cli and stemcell shouldn't be correlated - they're developed independently and CLI should general not care about which stemcell or director version they're using. In the director's case, the CLI may try hitting unsupported endpoints for newer features, but it should still function.\nFor this particular issue, I think there were two, independent undesired behaviors/bugs. First the stemcell started dealing with connections slightly differently (that was fixed in newer stemcells). But then that behavior change demonstrated another, otherwise hidden bug in the init/CLI (which then had to be fixed and backported).\nIn this case, I think the correct solution is to still upgrade the bosh-init/CLI, regardless of which stemcell version you might be using.. That's true. But, no need to worry about that API breaking... it won't be changing [relatively] ever. If we do, we'd probably do it in a backwards/fallback compatible way within our golang libraries.. Sounds like the recent https://github.com/cloudfoundry/bosh-cli/issues/110 issue. Which bosh-init version are you running? Perhaps try the latest version of bosh-init (v0.0.101).. Closing since discussion seems to have provided some unofficial workarounds.. @AbelHu, any findings on this?. If that's the underlying issue, a short polling loop does seem like it makes sense. Thanks for the update.. You might need to try running bosh start <job/partition> before cck will report and pay attention to it again.. Thanks, @nagelpat, for following up on how you solved it!. Closing - there are still no plans to expand access to Concourse. All CI-related tasks should be committed in the ci/ directory so others can run it against their own Concourse or Concourse lite installations.. I believe this is fixed in the bosh-cli/v2.0.30. Example...\nreleases:\n- name: zookeeper\n  version: 0.0.5\n  url: https://...compiled-bits.tgz\n  stemcell:\n    os: ubuntu-trusty\n    version: 3421. Another option would be to leave the ctl scripts the way they are and remove/change the nginx.conf files to not write pids or write to another pid file (do you happen to know if pid is optional?). This would help keep the PID file management in one place, where it would be easier to understand and would be more consistent.\nBy writing the PID in the ctl script, we avoid any ambiguity between when nginx is attempting to start/bind/write the PID file (to avoid monit trying to repeatedly start it again).. ^ the (( param ... )) syntax is not standard\nYou may want to use mkpasswd -s -m sha-512 to generate the value you need.\nPasswords are auto-generated in recent stemcells.. Make sure you use mkpasswd to generate a crypt-format password value.\nPassword should look something like...\n$6$GyY390IOH$DxiX1ai81ZYAbCR8JY7Re8EF92PpzsxUeXF1R.C.y6.Vzg7lk3x9B6e1daPD/rwVfXud86k4iPbh7oMFvVfvF0\n\n. Okay, because I was confused to see jihngfur... in your snippet when I expected to see something like $6$.... Was the password in /etc/shadow in the crypt format?. This is being implemented via PR #1653. Thanks for the well-documented bug report! This is fixed in director v261.. I'm a little unclear on this - you mention you got a deployment lock and then investigated it. How were you retaining the deployment lock, and what were you investigating?\nHealth monitor is a bit naive in how it notifies director to recreate vms. Did any tasks show up around your other tasks which may have been [trying] to interfere?. Related CLI issue: https://github.com/cloudfoundry/bosh-cli/issues/7. The cloud-config from cf-deployment uses 10.244.0.0/22, but the cloud-config from bosh-deployment has already configured 10.244.0.0/24 (and has existing VMs there). Currently BOSH does not support shrinking the network size, particularly if there are existing VMs running.\nTo fix, you'll either want to recreate your bosh-lite and avoid testing the zookeeper config, or you'll want to...\n\ndelete the zookeeper deployment\nupdate cloud-config from cf-deployment\ndeploy cf\n\nNote: you can redeploy zookeeper alongside cf, just don't try to resize the network.. Confirmed issue. Nice find!\nProbably should avoid merging unexpected keys from cloud-config.\nDanny Berger\n. Closing - duplicate of #1253. Hi - couple things...\n\nIt sounds like you may be trying to configure a load balancer for your compilation VMs - I don't think that's intentional.\nYou may also want to double check your load balancer configuration in general to ensure you're using the correct properties. Also make sure the load balancers already exist.\nIt would be helpful if you could include the azure and director version number in your report since code changes over time.\nYou might find the the bosh-azure-cpi-release Issues helpful for more context or to see if anyone else has experienced this issue.. Closing - thanks for following up with how you resolved.. Glad you're trying out Cloud Foundry! Although bosh is emitting this error, it's being caused by cf-release and we don't have enough experience with that individual release to provide suggestions. Please feel free to follow up with an issue on the cf-release repo, or join the cloudfoundry#cf-users slack for more help.\n\nAnother tip... I see the tail'd log output, but you should probably include a few more lines. It looks like a backtrace and it will be easier for them to debug if it includes the original error message which is probably a few lines earlier.\nClosing, but feel free to post another comment if you're having trouble finding the correct people to ask.. Have you tried following the suggestions for troubleshooting this error? If compilations were successful in previous deployments, perhaps firewall or some other configuration has changed since. Regardless, ssh'ing to review the agent logs on the compilations should be helpful.. Closing due to inactivity - please feel free to follow up with details learned from the suggested links.. Thanks for the well-written write-up! Since #1369 already discusses this, I'm going to close it as a duplicate. Feel free to +1/subscribe to that issue for notifications on progress.. They're both referring to not being able to update with an empty runtime-config (in the other issue, the cause happened to be they accidentally updated runtime-config instead of cloud-config, and then they were trying to clear out runtime-config). Since this is a director-level change, both CLIs would be affected.. Thanks for the PR! I know it's a small change, but if you wouldn't mind signing the CLA we can merge this change in more easily.. Agreed this is unfortunate behavior. Looks like there's even a latent todo around unreferenced releases in planner_factory.rb.. Nice find and thanks for reporting it - for what it's worth, the symlink should be unnecessary. At least this works for me on 3363.15, but curious if you saw different...\n$ sudo mv /usr/{s,}bin/tcpdump\n$ sudo tcpdump -i wd5ka5icqaed-1. Turns out this is a long-standing issue - https://github.com/cloudfoundry/bosh-lite/issues/256. Thanks for the PR! This reminded me that the spec should be changed in a slightly different way, so f80997af21311c8a855f0fb5b6b3c1a3168cb868 includes this change. Thanks again for mentioning it.\n\nPlease feel free to still sign the CLA for your next PR... then dreddbot won't feel quite so concerned about you next time :). Sounds like the questions have been answered, but please feel free to join us in cloudfoundry#bosh or create new issues if you revisit this and have more questions!. Ooh, that's good to know. I didn't realize that.. Closing - since v262+ the postgres job was removed in favor of postgres-9.4. We continue to package the older version, but only for the purposes of migrating the database of older director databases.. Have you tried using the new CLI? New version supports --json for some JSON output, but it is also more script-friendly with its cat/awk-friendly output. You might also find --column=name-type args useful, too.. These \"timed out pinging\" errors are typically due to misconfiguration of\nthe IaaS environment. Take a look at the suggestions documented here for a\nfew things to double check and how to find more details:\nhttp://bosh.io/docs/tips#unreachable-agent.\n-- \nDanny Berger\n. Closing - looks like this was fixed from v262+.\nThanks @h4xnoodle for finding the related commit!. Note that the only shared/potentially usable login on the stemcell is vcap (but it depends on which IaaS you're using for whether or not you can SSH into it directly). With AWS you can SSH with the original SSH Key used to create the VM, with Google you could add a key for the vcap user, and with vSphere there's no key used and the vcap password is randomly set; but the recommended way is to add a dedicated local user if you need it for testing and manual intervention (see jumpbox-user.yml from the bosh-deployment configurations for an example).\nFor what it's worth, to view details about a specific failed task, there's also the --debug option that you can pass to bosh task. For example, bosh task --debug 5.. Hi - this is usually due to firewall misconfiguration. Take a look at https://bosh.io/docs/tips#unreachable-agent for some specific suggestions on how to investigate this issue.. Related to your other comment, if you're expecting your VMs to be created using IaaS-managed network IPs, you should make sure you're using a dynamic network.. \ud83d\udd17 Slack, cloudfoundry#bosh - similar use case where it would be easier to provides: nil a link, than to go through updating multiple consumers to use the correct link. PR #1739. The default account is username  vcap and password c1oudc0w, but it depends on the IaaS you're using. For vSphere, you'd use that login; for AWS, you'd use the username vcap and SSH key you created the VM with; for GCP, you'd add your SSH key for the vcap user.. But note, once the VM comes up and communicates with director, it may change the default c1oudc0w password to a random one for improved security.. This is typically due to a misconfiguration on your security groups or director configuration - take a look at the suggestions in https://bosh.io/docs/tips.html#unreachable-agent for tracking down more information. For vsphere, you may want to deploy your director with a local user so you can connect - see jumpbox-user.yml for an example. For the compilation VMs, you should be able to use the user/password mentioned earlier.. Like @voelzmo mentioned, name and version are needed to detect if it already exists. Can't magically extract version from the URL.\nA particular release version tarball could technically have multiple sha1 values, so I don't think it's a strong enough key. Example being bosh.io may have one, but a locally create-release --tarball will have a different one, optimistically only due to timestamp differences. Seems weird to start having director try and track the sha1s of the wrapping tars for this single scenario.\nIf final release YAMLs included blob configuration, and if URL could point to that YAML file, it could directly look at the package/job sha1/fingerprints to detect which specific blobs to mirror. That would be more efficient bandwidth, too.\nSimilarly, in the git+https world, maybe we could support passing the path to a final release YAML (making version optional; if present it wouldn't need to fetch the repo every time to lookup the version from the YAML). Since it's a repo, the config/final.yml blob configuration could be referenced when needed. Double slash is a convention I've seen for including repo paths in a repo URI.\nreleases:\n- name: datadog-agent\n  url: git+https://github.com/onemedical/datadog-agent-boshrelease.git//releases/datadog-agent/datadog-agent-5.8.5.5.yml. Hi - yes, a deployment can achieve zero downtime. BOSH provides the primitives to make it possible, but how you go about that depends greatly on the software, environment, and service requirements.\n\nFor example, the Cloud Foundry / cf-release can be deployed on a single VM... but a single VM is not H/A. Instead, you would potentially deploy multiple VMs of each component in different AZs.\nFrom the BOSH side of things, there are two basic types of updates - one which will completely replace the VM (e.g. stemcell upgrade), and one which will stop/reconfigure/start process on existing VMs (e.g. patching configuration). In both cases, processes are temporarily stopped, so, as a deployment operator, you would ensure the service has other VMs picking up the slack. BOSH or not, a service should be able to handle the disappearance of a VM due to hardware failure (and in BOSH's case, it can automatically bring that VM back).\nWhen managing large numbers of VMs or VMs across multiple AZs, you can control how many VMs are being updated at a time (e.g. if you have 10 app servers per AZ, you could configure it to ensure there's at least 5 servers running at a time).. The \"timed out pinging\" errors typically mean there's a misconfiguration of security groups or NATS credentials. Check out the tips given on https://bosh.io/docs/tips.html#unreachable-agent for a few other suggestions.\n(and good suggestion, @ivandavidov - thanks!). Duplicate/semi-related to #1568. I think this closest thing we have for you right now is a prior note about this sort of thing in bosh-notes. We'll probably be communicating a bit more about it if we get closer to a recommendation for change.. Sounds like the director VM may not have enough resources (perhaps memory). It may be helpful to see more of the debug to capture the full stacktrace. You could try bosh task --debug 35 for more information about that task. How much CPU/memory is the director running with?. I liked how the story title said GET /orphan_disks and the description said GET /orphaned_disks - I couldn't decide which was correct :)\nSo I guess it's a route rename, then; not a class/file rename.. Tested against an v2 Concourse and it seems to work fine. I think it's safe to merge with peer review.\n```\n$ fly -t bosh e -c ci/tasks/integration-test/task.yml -i repo=. --privileged -x\n...\nDigest: sha256:5f6b58f1579d726f074ec9841971878b162137a8384f26c957be93ee9842dc95\nStatus: Downloaded newer image for dpb587/bosh-main-bosh-docker@sha256:5f6b58f1579d726f074ec9841971878b162137a8384f26c957be93ee9842dc95\nSuccessfully pulled dpb587/bosh-main-bosh-docker@sha256:5f6b58f1579d726f074ec9841971878b162137a8384f26c957be93ee9842dc95.\n...\nsucceeded\n``. A similar issue was brought up on the [mailing list](https://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/thread/F33RU56CMZUB7LFHIYXYG66NLUG72P4N/) a while ago. You may want to try re-downloading the stemcell (and verify the sha1 from bosh.io to ensure it wasn't corrupted during transfer), and trying again.. Your deployment manifests suggests yourgcp-director` is a bosh-lite VM. If that's the case, you should be uploading the warden stemcells (it looks like you're currently uploading stemcells meant for AWS).\nbosh upload-stemcell \\\n  --sha1= 54579945657c9be309523d6b93bc90d7c443956e \\\n  https://s3.amazonaws.com/bosh-core-stemcells/warden/bosh-stemcell-3421.6-warden-boshlite-ubuntu-trusty-go_agent.tgz\n\nI'd also encourage you to use sha1 verification, even if it's happening directly.. Looks like you might be combining a few different documentation instructions given the director name, listed stemcells, and AWS environment. If you're new to BOSH+CF, you may want to get started with a newer tool, cf-deployment which can more easily generate your Cloud Foundry deployment manifest.\nTo continue with the docs from the docs site, I'd recommend revisiting the Customizing the Cloud Foundry Deployment Manifest for AWS to make sure you're using a valid manifest.\nIt kind of looks like you're trying to deploy something other than CF (which is perfectly fine! just want to make sure I'm giving you the correct pointers). Whatever manifest you're using in the snippet above, it looks like it's using the newer style manifest which is definitely going to look different than the manifest from the docs.cloudfoundry.org site right now (but possibly similar to other sites/docs using the newer manifest style). Just good to keep in mind there are two different manifest styles as you refer to snippets from various sites.. There are generally two types of changes in a VM's lifecycle...\n\nnetwork/VM/stemcell/IaaS-related which will result in a VM being recreated\nsoftware/persistent disk-related which will keep the existing VM (I think this is the \"application release upgrade\" you're referring to)\n\nIn both cases, there will be downtime due to processes being stopped during the update; but in the first case there will generally be a longer delay due to the underlying VM also being recreated by the IaaS.. oops. you snuck in an edit with another question :)\nFrom the \"glossary\"...\n\nCanary instances are first instances updated within an instance group. Any update error in a canary instance causes the deployment to stop. Since only canaries are affected before an update stops, problem jobs and packages are prevented from taking over all instances.\n\nThere aren't explicit test scripts to determine canary success - it's more: are all processes running successfully by the time we expect them to be up and running, and did the post-start scripts run successfully?. Hi! Generally speaking, when you encounter this error it means there's a communication problem somewhere between director and agent. There are a few tips for resolving here, but typically it's due to network misconfiguration and firewall rules.\nInternally, director is sending a ping message expecting the agent to be connected and listening to NATS. This is how director knows the agent is up and ready, because, once it is connected, the agent will reply back with a simple pong-like message. Director takes that as confirmation and will continue the provisioning process.\nThe registry helps provide some live VM settings for some IaaSes that the agent needs when running. I'm not sure what error you're referring to which led you to that code - we rarely see issues there.\nI'd recommend you investigate these suggestions. If you're still experiencing issues, please include the original error messages long with more information about your environment such as director, cpi, and stemcell version information.. Not sure what you mean by confirming if \"the agent status is normal\" - you could ssh onto the vm and look at the /var/vcap/bosh/log/current log to see the agent log for more information? Agent is typically running fine though... issues are more to do with networking and misconfiguration on the director side. Please take a look at the suggestions for how to investigate the issues in more detail.. Do you have other legacy deployments which are also using the same network space as compilation_network? Director excludes IPs of old-style deployment manifests and may be a factor here.\nAlthough less likely, this could also happen if you have numerous deploys happening in parallel, all trying to compile different things. You should have ~24 IPs available for compilation.\nNext time, please use code fences for your snippets to ensure it is readable by others.. Can you check which version of director you're running? In this case, the fix was on director, not the CLI, and it should be in v261+.. Good thought. We're in the process of transitioning people away from cloudfoundry/bosh-lite to cloudfoundry/bosh-deployment because bosh-deployment uses the same deployment strategy that you'd use in production and helps maintain better parity. That repo is kept up to date with recent versions. I'd recommend you try that out, next time you get a chance (and definitely interested in feedback, if you have it).. also g2g \ud83d\udea2 . The Google IaaS doesn't use the ssh_tunnel, so it should be removed from your init manifest. It worked in the background on previous stemcell versions since the project SSH users/keys were quickly added to the VM, but that isn't supported in recent stemcells.\nSince the project SSH keys aren't currently supported, that's also why you'll need to use the referenced jumpbox op, if you want later SSH access.. Closing in favor of https://github.com/cloudfoundry-incubator/bosh-google-cpi-release/issues/199. Agreed with @penrod - to deploy a multi-VM BOSH, you'll want to deploy from an existing director. The bosh-init/bosh create-env commands are designed to only deploy a single VM.\nTypically, a single VM director will be more than sufficient. I'd strongly recommend starting with a single VM and only distributing jobs when it's specifically required.. Hi - when light stemcells are available, bosh.io will provide them by default at that download URL. If you specifically need a heavy for some reason, here are a couple options...\n\nyou can append the query string light=false to the URL (e.g. ?v=3421.9&light=false)\nuse the direct links from the \"All Versions\" section\n\nSeparately, is there a reason that light AWS stemcells don't work for you? They're generally interchangeable.. Can you provide the stemcell you're using? Also, does this happen every time?. Closing - this should be fixed via #1737 and is in v263+.. For anyone interested in thinking about this further, feel free to brainstorm and discuss suggestions. It's not a trivial issue; you'll want to consider things like...\n\nparallel other processes and tasks may be adding/removing/changing resource usage on their own. It becomes very difficult to synchronize that.\nAPI throttling itself is also related, but very difficult to synchronize as well; it's probably best left to CPIs to retry\nmulti-CPI also complicates this a bit as well since difference CPIs from cpi-config may have different limits, so it's no longer a global state; and while there may be multiple CPIs in cpi-config... they may actually be using the same account/limits. Accidentally squashed into https://github.com/cloudfoundry/bosh/commit/201f9999f442634e8668e5c4c65c297d1fda0bd2. @cppforlife is the bosh-notes still accurate? can it be updated? i'm going to forget the current/future apis. https://github.com/cloudfoundry/bosh-notes/blob/master/disks-api.md. Thanks!. Great answer @ematpl, thanks!. This sounds vaguely familiar from the past ~6 months, but I can't find a specific story. Do you experience this issue on recent versions of director?. Related internal story investigating this: #159494222. Closing - sounds like the crypt suggestion is the correct answer.. That sounds like an incorrect networking configuration or discovery somewhere with that setup, but for experimenting to learn more about the issue, you might want to try the set_mtu job from the networking release.. I'm not able to reproduce this locally - have you tried verifying the checksum of your download, or seeing if there's a different error emitted earlier in the output?\n\nAlternatively, you may want to try the newer, CLI v2. The ruby CLI is deprecated, but in this particular use case it should be working without an issue.. Closing - please feel free to follow up on this issue with the requested details if you need more help.. Some related PRs: #1774, cloudfoundry/bosh-agent#129, cloudfoundry/bosh-cli#307, cloudfoundry/bosh-linux-stemcell-builder#6. Hi - this is fairly release-specific so I'm not sure how helpful I can be, but from the BOSH perspective it sounds like it is deploying things correctly (according to the manifest it was given), and perhaps the deployment manifest was not fully configured.\nI noticed the following documentation page related to generate keys for consul, which may be helpful? http://docs.cloudfoundry.org/deploying/common/consul-security.html\nMore specifically, perhaps you can double check that your deployment manifest has been sufficiently configured. If you're getting started with cf-release, I'd recommend checking out cf-deployment which provides a great base CF manifest that you can use.\nI assume you're using the consul-release based on some of the log messages. I'd recommend you checkout that repository and perhaps their GitHub issues - they're more likely to have seen this error or know more information about it.. Hi - the base stemcell only installs a minimal number of packages (to keep it small and maintainable). When releases have dependencies on particular tools not available in the stemcell, they should include them in their release's packages.\nWe're not planning on adding the mailx command by default, so you might want to add it to your release. In case you hadn't found it yet, here's a link to creating packages which may be useful.\nHope that's helpful, but feel free to follow up if you have a question about it.. Semi-relevant... at one point there was discussion of using the director UUID instead of name. It would help in this sort of thing, but I think some of the original concern for using name was so that it could be predictable for some use case.. Agreed, I think this is confusing as well. Particular in the scenario where a new operator might not realize multiple jobs need the same link. Including job would help them realize it rather than spending time quadruple checking the configuration of the job they know and expect.. Hi - can you take a look at the job logs for the director release job name? You should be able to find error messages or stacktraces of some sort which can help point out what the misconfiguration might be. If the errors you find there are unclear, please feel free to follow up.. It doesn't look like there is anything wrong in the logs. Sometimes this can mean your VM is not fast enough. To verify this, you can try sshing onto the director after it comes up and check if monit summary shows all services as running (or check it after a short delay). If it is slow, you can try changing flavors or increase your *_watch_time settings in your deployment manifest.\nI also see that your provisioning is taking ~6 hours - this isn't normal (it should take less than 30 minutes when compiling from source). This suggests your VM is underpowered (probably slow clock speed), or the network connectivity from your workstation to the VM is very slow.. The max *_watch_time is separate from the total deploy time. You may want to increase it to 30 minutes to see if director eventually comes up cleanly.\nIf director comes up by manual start, it should generally work.\nHowever... it sounds like your hardware may be too underpowered or there's some other virtualization misconfiguration which is causing these installation issues, and if that root cause is not resolved, you may experience issues later on which are even more difficult to diagnose. I'd recommend spending some time testing the performance of your setup outside of bosh.. Closing - please feel free to follow up with more information about what you've found if you need more assistance.. This is not currently configurable - a select few VM-system events (namely SSH logins and monit service status changes) are forwarded to the agent/health monitor/events. As an alternative, you might be interested in increasing director.events.max_events from the default 10000 events if they're filling up too quickly.\nWould increasing the retention count be sufficient for you? Do you use any other source for monitoring SSH login events?. We plan to stop forwarding system SSH events via agent to the director in a future stemcell. This should fix the issue around too many events being tracked. Existing bosh tasks auditing will be unaffected and still show setup ssh/user requests.\nStory: https://www.pivotaltracker.com/story/show/154424559. I don't think this was merged correctly, but it looks like these changes made it to master. Will follow up to figure out what may have happened in the local merge.\nThanks!. Current implementation doesn't track tags for the resources so it doesn't know when they've changed. Similar behavior to changing properties on the CPI releases - they're not taken into affect until a [re]create.. We typically don't recommend implementing custom blobstores for either the CLI or for director. We provide several common providers with the expectation that they'll cover a provider that everyone can find access to. Custom CLI patches also make it very difficult for others to use the releases, and custom director patches make it difficult for maintenance. Additionally, blobstore providers should now be implemented in golang for portability and reusability.\nIf you need further information, you may find the following PRs useful where gcscli support was added to director and cli.\nIf you're intending on submitting these changes upstream, I'd be interested to hear more about the blobstore provider since we may not be able to support every provider which is out there.. If this is happening on AWS, can you take a look at /var/vcap/sys/log/registry/*.log logs? They may contain more information about what went wrong in the registry service. You may also want to double check that the registry is using the correct AWS credentials and database configuration.. Is your director under very high load when this happens, or the database locked for a period around this time?. Story #152339383. Closing - this was due to a race condition in task logic and was fixed in v264.1+.. You may have switched between the ruby CLI and an older version of the golang CLI. If so, try the bosh reset-release and then upload a new version of the release again.. Sorry we lost track of this one. This error usually happens when you have multiple deployments on your bosh-lite which have overlapping network configuration. You should update your network space to avoid overlap; or delete other deployments if they are no longer needed.. Thanks!. You'll need to rerun the create-env and route commands if your VM reboots. See the recommendation here.\n\n\nIn case VirtualBox VM shuts down or reboots, you will have to re-run create-env command from above. (You will have to remove current_manifest_sha line from state.json to force a redeploy.) The containers will be lost after a VM restart, but you can restore your deployment with bosh cck command. Alternatively Pause the VM from the VirtualBox UI before shutting down VirtualBox host, or making your computer sleep.\n. This is a Pivotal PR; mergers feel free to ignore @cfdreddbot in this case.\n\n\n@dannysullivan, might want to publish one of your orgs to keep @cfdreddbot happy in the future.. The create-env does not currently verify the stemcell os/version which compiled release tarballs include.\nDoes compiling the director from source work?. Interesting find (and thanks for digging into some code). This seems like it would indeed be an issue, particularly noticeable on PostgreSQL.. I think this may have been fixed as part of 268.0.1 and the related story around ensuring packages are installed in a deterministic way when there are multiple possibilities.\nCan you give the new version a try and see if you're still seeing issues?. @krishnajha99, I see you ran a ping, but it looks like that was from your local workstation - can you run that from the director (to verify the director's connectivity and DNS resolution)? Does running a regular curl -vvv https://ec2.us-east-1.amazonaws.com command on the director work? Have you configured your director with any special http_proxy settings? . Closing due to inactivity - feel free to reopen with more information.. Closing - v2.0.44 supports the BOSH_SHA2 environment variable.. Perhaps we can do a better job on redirecting the error output so it shows up in /var/vcap/sys/log/health_monitor/* rather than monit.log?. Agreed, perhaps we can implement some fake value generator. Typically it's a very derived value from the director, but we could probably approximate it for tests.\nWould you expect to be able to test that spec.az was actually embedded into the returned address? That will diverge from actual director behavior and DNS name generation, but would retain testability.. Agreed for improved UX. If a hard error is undesired, could emit a warning as part of the task output. Something along the lines of \"No instances matched the requested filter\".. I vaguely recall us doing some work around this a few months ago, but not seeing a specific story. Has anyone seen this issue recently?. I believe this can happen if your OpenStack environment is slower than typical. You may want to try increasing the openstack.state_timeout property of your director.. Looks like the state_timeout was placed at the wrong level. It should be a sibling of default_security_groups (not a child of connection_options). For example...\nopenstack:\n  default_security_groups: [default]\n  state_timeout: 3600\n  wait_resource_poll_interval: 15\n  connection_options:\n    ssl_verify_peer: false\n    connect_timeout: 60\n.... Story [#152443729](https://www.pivotaltracker.com/story/show/152443729). Closing - fixed in v264.1.. There was a bug in the Ruby CLI (which some older OpsMan versions use) that cause it to not recognize that a task has timed out.\n\nTo investigate the reason for a timeout, you may want to take a look at the debug logs of the task (e.g. bosh task --debug 7555) or take a look at the worker logs on the director (e.g. /var/vcap/sys/log/worker/*).\nSince you're working with OpsMan, you may want to follow up with support who knows more about your particular environment configuration.. I'm not sure where that manifest came from, but separately, it sounds like your director has not been properly configured. The blobstore settings for a VM are inherited from the director configuration. I'd recommend recreating your director and following the steps carefully, or trying to deploy the director with bosh-deployment.\nThe blobstore settings in your manifest.txt file are being ignored (unless your learn-bosh's app job has blobstore properties defined). I suspect you were trying to adapt the defaults to use S3 on the director? For more details on that, I'd recommend you review this doc on external blobstores.. Have you verified the access/secret key independently of BOSH?. @parthasarathi204 were you able to figure this out, or do you need further assistance?. This sounds very different than what the original issue was about. It sounds like you are trying to create a new blobstore backing for director and VM-side. This is a very involved process since it affects director/agent/stemcells, so I wouldn't recommend it until you are more familiar with the architecture and unless you have discussed your goals more in depth (since they would require integration with these upstream repositories). Currently, most environments are able to use at least one of the current, supported options.\nIn this case, it seems like you found a way to install bosh-blobstore-bmc onto the image (typically done when building the stemcell), but it is hard-coded to apikey.pem. Instead, the configuration options that it received from director were written to /var/vcap/bosh/etc/blobstore-bmc.json and passed as the -c option. You should be reading all configuration from that file.. As mentioned before, the agent takes care of writing the configuration file and automatically passes it as the -c option. The actual agent code which sets up blobstore is in app.go. This page describes the blobstore interface a little more. Also, I'd still recommend using s3/gcs/dav as they are much more widely supported and used.. That is curious. Have you figured anything else out since?\nIf you run that command with the environment variable BOSH_LOG_LEVEL=debug, can you provide the stacktrace which should be present near that error? Please don't post the whole log since it contains sensitive information; just the stacktrace which will probably be near cannot load such file.. Tracker story: https://www.pivotaltracker.com/story/show/144061967. Confirmed - this is probably a difference due to golang and ruby YAML parsing. Will need to consider a fix.. Related: https://github.com/cloudfoundry/bosh/issues/1613\n@voelzmo do you have an example of a release type which should be installed on compilation vms?. The include-compilation use cases sound a bit weird - both of those use cases sound racy to me. There's no guarantee the addons may have finished scanning by the time compilation was finished. For completeness though, I think it might make sense regardless.. Pull request for lifecycle filtering: https://github.com/cloudfoundry/bosh/pull/1902. Hi - director doesn't currently error on it, but all template installation paths should be relative and not prefixed by / (since it's not absolute). I guess errands is the first feature to implicitly force that. It looks like you have updated your release to make them relative, which is the correct thing to do.\nWe may be able to build tooling to help surface this sort of potential issue (e.g. cppforlife/bosh-lint#16), but not sure if director will start validating anytime soon since there may be many other releases and older release versions still rely on it.. Thanks for documenting this very clearly!. @voelzmo are you waiting for the new stemcell version with this, or would you just like a candidate stemcell build which includes this (bosh-linux-stemcell-builder repo is currently at 2.54.0), or are you looking for the change to be backported to an existing version?. Closing as duplicate of resolved #1857.. Hi - sounds like you might be upgrading from versions prior to v262, which is when we removed the postgres job in favor of postgres-9.4. The v262 release notes mention a few other changes, and I'd recommend checking the release notes of the other releases as well since there have been a few other changes in recent versions, too.. If there are particular docs you are following, please feel free to mention them and we can make sure they are updated if a sample 264.3.0 configuration is still referring to postgres.. It looks like this is fixed as part of https://github.com/cloudfoundry/bosh/commit/94aef202511d9f351709cefcee80f1110f03443c, and it will probably be released in v266+.. I agree, this is confusing. This has come up a few times on Slack.. Sorry, we seemed to have dropped this issue. Are you still experiencing this, or were you able to figure out what a solution was?. Just published bosh-template/2.1.0 with this fix. Thanks for the pull request and reminder!. This is a bit too generic, and is not an issue we have seen reported elsewhere.\nCan you take some time to more closely review processes running in your environment, and verify their memory consumption to see how they compare to your expectations? You have identified a few big picture data points, but I'd recommend manually reviewing ps (or using existing monitoring tooling) and comparing with your other vms/environments.\nYou may want to review the individual meminfo fields to see if they are what you expect for whatever combination of software you may be running in your environment (definitions).\nYour ps calculation method may additionally be double-counting shared memory, resulting in confusion and differences between the ways memory is calculated by different tools.. Conceptually, the resolution steps make sense to me -- re-attach if needed. The spike implemented it in the update_persistent_disk method which is a little bit odd to me since I think of that as \"verify director and agent have the same settings\" and less \"make changes so the director and agent have the same settings\". That's probably minor, but if you have other ideas where we can keep the call agent-focused I'd be interested. I'm not sure if there's a separate place where it makes more sense to reconcile IaaS-related stuff.\nThis fixes the expected-to-be-attached case. Is there still an issue in detach edge cases that could be fixed with this, too? I think I saw something mentioned about that scenario in this issue or story. Or does this essentially cause the disk to be re-attached later to an already failing mount (e.g. #2128)?\nThis seems similar to possible cck workflows of disks; are they affected by this in any way? Or end up overlapping?\n@jfmyers9 might have more thoughts.. Okay, sounds like you've covered it; I don't have any other ideas.\nIs this code correct? It's at least confusing to me to read, but I think it might also not be the case we want.\nreturn unless agent_disk_cid.nil? || agent_disk_cid != instance.model.managed_persistent_disk_cid. I see a couple interesting things from BOSH code side of things...\nFirst, it seems like we're assigning the same instance multiple times to the network...\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Assigning az 'z1' to instance 'mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)'\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Assigning az 'z3' to instance 'mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2)'\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Assigning az 'z1' to instance 'mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)'\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: Existing desired instance 'mysql/0' in az 'z1'\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: Existing desired instance 'mysql/2' in az 'z3'\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: Existing desired instance 'mysql/0' in az 'z1'\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: Obsolete instance 'mysql/1' in az 'z2'\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reservation {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic} belongs to azs: [\"z1\"]\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: For desired reservation {type=dynamic, ip=, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)} found existing reservation on the same network {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic}\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reusing existing reservation {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic} for '{type=dynamic, ip=, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)}'\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reservation {ip=10.0.48.4, network=default, instance=mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2), reserved=true, type=dynamic} belongs to azs: [\"z3\"]\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: For desired reservation {type=dynamic, ip=, network=default, instance=mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2)} found existing reservation on the same network {ip=10.0.48.4, network=default, instance=mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2), reserved=true, type=dynamic}\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reusing existing reservation {ip=10.0.48.4, network=default, instance=mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2), reserved=true, type=dynamic} for '{type=dynamic, ip=, network=default, instance=mysql/10c6161f-e104-40fe-ae0c-23669cd6b37d (2)}'\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reservation {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic} belongs to azs: [\"z1\"]\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: For desired reservation {type=dynamic, ip=, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)} found existing reservation on the same network {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic}\nD, [2018-01-10 17:04:39 #13837] [task:5727] DEBUG -- DirectorJobRunner: Reusing existing reservation {ip=10.0.16.19, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0), reserved=true, type=dynamic} for '{type=dynamic, ip=, network=default, instance=mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)}'\nIt looks like the bootstrap VM was decommissioned, and a new one was elected (mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42). But that elected bootstrap node was previously marked as ignored.\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: No existing bootstrap instance. Going to pick a new bootstrap instance.\nI, [2018-01-10 17:04:39 #13837] [task:5727]  INFO -- DirectorJobRunner: Marking new bootstrap instance 'mysql/957ddf1a-5ff0-425b-8515-5a5ff4652e42 (0)' in az 'z1'\nPerhaps the [unverified] repro scenario is...\n\nDeploy to 2 AZs\nIgnore all VMs in the non-bootstrap AZ\nDeploy, removing the AZ which has the bootstrap\nSee this failure?\n\nIf that's true, a possible recovery scenario would be...\n\nUnignore the VM which has the lowest index (since it will be elected bootstrap node)\nReattempt deploy. Thanks!. You'll need to provide much more context for us to be able to help. Can you please include things like what command you were running, where you saw this in BOSH, which versions of components you are using, and what you have tried already.. Closing - as mentioned, it sounds like you're using Ubuntu-builtins for managing software, but should probably consider BOSH conventions. I'd recommend checking out cloudfoundry/cf-mysql-release.. Can you double-check what bosh env tells you for your authentication username?. Sounds like you may be deploying BOSH via another BOSH. Has the \"outer\" BOSH which is deploying the other BOSH been upgraded past 264.2.0 which fixes the issue?. Related upstream pull request which fixes the need for monkey-patching - https://github.com/ruby/openssl/pull/185. I am unclear on what sort of integration you are looking for. If you're looking for integrating BOSH with LDAP authentication, you can do this via cloudfoundry/uaa-release and their LDAP support paired with BOSH's UAA integration. If you're looking for individual VMs authenticating and registering to an LDAP server, that's not something which is inherently supported, but you could build software that can be deployed onto VMs to do that.. Closing - thanks @dsabeti for the assist. Feel free to follow up if you are continuing to see issues around the routing to VMs.. Closing - I think the most recent comment answers your questions. Feel free to follow up if this is not the case.. Closing - feel free to reply if this is still an issue.. Hi @ywysuibian - are you using cloud-config or is this with legacy manifests?. Hi - the postgres job was removed in v262, and you will probably want to switch to postgres-9.4. I would recommend reviewing the release notes, as there are other breaking changes since the version you are upgrading from.\n\nYou may also want to check out bosh-deployment as an alternative to managing your own manifest.. Since packages are installed at predictable paths (e.g. /var/vcap/packages/postgres-9.4) you are not able to use two packages with the same name from different releases. Depending on your environment and needs, you should probably decide to use only one of the packages to provide the database service.. Hello - teams are automatically created within the director when they are necessary. You should never need to manually change the director database. In this case, when you create a deployment or config using a client/user with a team-specific scope (documented by the link you mentioned), the director will automatically create the necessary records. You'll then be able to verify by running the bosh deployments command to see the team listed in the Teams column.\nDoes that help?. @muralisc I see @SAP listed as your organization, but we technically need your GitHub org membership publicly visible. If you could give that a second look, I'd appreciate it.. @cppforlife please prioritize this (#156892576); @muralisc was asking. I don't think the following code is doing what you want...\nerb\n<%=\n  nats_port = p('nats.port')\n  respond_to?(:if_link) && if_link(\"nats\") do |link|\n    nats_port = link.p(\"nats.port\")\n  end\n%>\nThe if_link is not technically a boolean statement, but will always return an object which has an else method (to support the else behavior). Because of this, the value being returned is always going to end up being an in-memory object reference. Since you're outputting the result via <%= ... %>, this memory reference is rendered to template and changes every time, director wants to update the templates. You can probably verify the behavior by seeing what ended up in that template file on the VM; probably something like the following...\n```yaml\n\ninit_config:\n...\n```\nThe easiest solution is probably to switch to <% ... %> and avoid printing anything since that code looks more like setup.\nOne other useful tip for the future. Since bosh-agent currently keeps a previous version of templates on the VM, when you notice unexpected changes, you can often diff between previous and current versions of the rendered templates with something like diff -r /var/vcap/data/jobs/myjob/{oldid,newid}.. Have you tried looking at the debug logs as the error suggests? You can do this by using the --debug flag when looking at the task. For example:\nbosh task --debug 14\n\nYou will probably see a stack trace in there with more details which may help identify the issue, possibly around not being able to connect to the remote server.. I may not understand your steps or question, but:\n\njumpbox - this is often used as an access point to avoid all VMs needing to be publicly accessible. This allows bosh and the underlying Cloud Foundry VMs to remain only on private networks. Learn more about jump boxes.\nbosh/0 - this is BOSH and is the component used for deploying Cloud Foundry on other machines. With large clusters like CF, BOSH is considered the management plane to make sure everything gets started and stays running. Learn more about the BOSH architecture.\n\nDoes this answer your question?. There's not a story about this sort of feature. Any future work around this will require some community discussion since it will introduce some new release conventions, so this issue is probably the best place to watch for now until a proposal gets going.. @mkuratczyk @ljfranklin to tangent slightly... in this golang scenario, does this mean one of the jobs has golang listed as a job dependency? Typically golang is only necessary as a package dependency, which I believe sidesteps the issue since compilation dependencies are not considered for package name overlap. Might be worth double checking job dependencies in those cases.\n@ljfranklin yeah, using package names with exact version information would be an alternative. In this case, we have avoided that on golang because it has historically been a compile-only dependency, so we stop at the minor version. But for things like the ruby-release we did introduce a slightly odd revision suffix (e.g. ruby-2.4-r4) to help avoid conflicts in CPI world, among other reasons.. Alternatively, something like a proxy server job on the BOSH director which could translate/convert release references might work. It would avoid large, arbitrary deployment manifests needing to contain sensitive credentials and being passed around or shared between teams.. This came up in pivotal.slack.com again recently, so I wanted to add a few more thoughts around this.\nIntroducing more download reference data into the releases section is a little weird in my mind. As far as the \"deployment manifest\" is concerned, it only cares about the name/version. The url/sha1 were added as a convenience (which has certainly been useful), but expanding it more just adds noise which isn't useful to the deployment itself - it's for the upload-release. And even if that data is present in the deployment manifest, it can become misleading because some other user/process could have uploaded the release version from some other place (which could technically have different bits). So, I'd like to think a bit more broadly about what alternatives might be. At least for broader discussion and longer-term goals.\nOne shorter-term option could be a very small wrapper utility which replaces S3 URLs with their pre-signed equivalents. The manifest could be piped through it before being passed to the director/CLI. This avoids introducing blobstore-specific properties on the individual releases and would help avoid duplicate configuration that I imagine would be common across many releases in some deployment manifests. We'd have to consider s3 support, but also gcs/future ones. Trying out a wrapper utility might be useful as a step to evaluate how much need for this there is.\nLonger-term, I would like to see much of this responsibility end up on the director. The process of uploading releases requires a lot of trust of the individual deployment operator who may be uploading the releases. If the director was able to resolve these release references itself (based on name/version), the main BOSH director/CloudOps operator could define where releases must come from, and it can provide more of a gateway/unified artifact download process. This process could be externalized from core director itself in a pluggable way, then individual, colocated releases could find/download releases in whatever way they determine is best, including looking up private releases and downloading with private authentication. I also imagine this working for stemcells and, potentially, compiled releases, in the same way. I've done some experimenting on this and definitely have more thoughts around it if they become relevant :)\nAs a simplistic example for this case, there would be a plugin release that BOSH operators could colocate, and your users might have an ops file which includes the release with pre-configured buckets/servers/credentials for finding releases your team supports. If director doesn't recognize a release in a deployment manifest it receives, it uses the plugin(s) to find it and for bucket access, it can use whatever credentials are appropriate.\nWith the specific example of sharing read-only S3 keys... I'm not sure how those end up being shared, but it does seem a bit close to obscurely public. I still see the need for authenticated release downloads though, so I think it all still makes sense to discuss.\nI'm sure others have more opinions on this as well. /cc @mfine30. Closed the last PR. In the future, please keep pull requests open and push your changes to the existing branch rather than creating duplicate PRs in order to ease discussion and tracking. If you want to retain history prior to rebases, feel free to keep local branches or branches in your fork to avoid cluttering branch lists.. Hmm, if you are looking for automating authentication, it sounds more like you should be looking into UAA client keys and secrets - these are intended for systematic access. These values can be passed through environment variables (BOSH_CLIENT, BOSH_CLIENT_SECRET) and avoid the need for you to run bosh login. I'd recommend giving that a try.. Related: https://github.com/cloudfoundry/bosh/issues/1671. Does VirtualBox show the VM as having multiple NICs as being attached? Sounds like the NIC doesn't show up physically in the VM which suggests it may be a bug in bosh-virtualbox-cpi-release?. @scottgai what version of director is this output from? I'm not seeing this behavior in 266.x or 265.x.. I mentioned in pivotal slack (2018-08-15) that I still hadn't been able to reproduce and would be interested in using one of your test environments to reproduce if you're still interested in this...\n\ndo you happen to have a test environment for the azure delete-vm issue that I could get access to? or a few minutes to pair with you in the environment sometime to check a few things? might be easier than back and forth since i haven't been able to reproduce.. It looks like we do not encode CIDs when making the delete-vm request which can cause the web or app server to truncate the CID at the semicolon while decoding the URI. I think we'll want to change that behavior - we shouldn't be passing through arbitrary, unencoded data in the URLs.. There is not currently a BOSH_HOME environment variable. It does respect the generic HOME environment variable, if you want to give that a try. Would that work for you?. cc @xoebus @aramprice - heard of any requests or issues around releases trying to use syslog within a bpm container and expecting the messages to show up on the host?\n\nPossibly related to /dev/log being mounted from the host? Kind of doubtful that we should expect bpm to support that though, in the name of maintaining isolation and focusing on logs via stdout/stderr.. @voelzmo don't we need them to show up on the host for auditing/forwarding purposes? We can't know what forwarding processes are running on the host to run the same configuration within a container. And within the bpm container, we/bpm don't run any special/CEF log forwarding processes.. FWIW, CEF format to syslog was originally implemented as part of an audit requirement in #116909807 (label gov1). I'm not sure if it's a continued requirement, or nowadays-log forwarding would be sufficient for the original use case.. Regarding the first error (state is 'timeout')... this happens when the task is started by director, but at some point the worker misses heartbeats and is assumed died/crashed. This typically happens if your director is under very high load, or some other bug has caused the crash. If you want to investigate, you will probably find more information in the logs of /var/vcap/sys/log/director/*_worker.*.log files. As you noted, this is unfortunately a fairly low-level error and, surprisingly, happened very early in the process before the deletion fully started, causing minimal output.\nThe second error (timed out sending 'run_script')... this happens when the worker handling the task tries to communicate with the VM to tell it a command. Internally, the worker never received acknowledgement from the VM. This typically happens if the deployment VM is under high load and not able to respond in a timely manner.\nDoes that information match what may be going on in your environment?. Closing as duplicate of #1842. It is not currently possible to use proxies that require authentication from the director. Feel free to continue discussion about this in that issue.. I think this was the fixed in v266.6.0 via Story #158130962.\n\nFixed double deletion of compilation VMs. Yeah, we should update bosh-deployment.\n\ncc @mfine30 we may not want to block updating bosh-deployment's bosh on some of those other CI-related stories if they'll take a little while. Might see similar future stories reporting bugs that have already been fixed.. In general, this typically comes down to resource constraints on the VM, and increasing the VM resources (like CPU or disk speed) is typically the cheapest solution. If someone is interested in doing more research about this to narrow it down to disk/CPU/Puma threads/nginx timeouts, please feel free to discuss your findings - there may be better default configurations we can provide.\nFrom this particular case, it seems there were not additional error messages aside from the 502s and upstream closed errors which makes it more difficult. If looking BOSH_LOG_LEVEL=debug bosh ... may be useful, or you may want to try playing with some of the pre-defined director properties. Logs in /var/vcap/sys/log/director/* may be useful, or tweaking configs in /var/vcap/jobs/director/* might be helpful.\n@bingosummer Windows are a bit newer, but the public images only support light stemcells due to the additional licensing requirements being handled by the IaaS.. We have not seen any errors when using the ruby and database versions that we support. Can you provide more context on why this change would be necessary?. Thanks for the additional context. Right now we don't have mssql support on our roadmap, but if you/someone want to prove it out, feel free to continue using your fork for testing and developing this. For now, I'm going to close this until there's a bit more confidence in the need for these and other changes (I suspect there are several other changes that will end up being necessary) - it'll be great to see another, more comprehensive PR to discuss if it's something we can support.\nFYI, since migration scripts are a little fragile, there are some dependencies and tests relying on checksums of these files which would have changed. Be sure to update those and double check tests for the next PR.. A check-certificates subcommand seems too use-case specific to me as a first step. Can we can generalize this a bit more? I also think it'd be helpful to separate out the scenarios of create-env and deployment because they're significantly different.\nFor deployments, I see this as more dealing with generic variables. Currently I think vars exposes names/ids, but could we start providing additional metadata in that command? By metadata, I'd expect to minimally be things related to the \"quality\" of the variable, not configuration or actual values. This could also include values like the date a password was set for enforcing rotations/lifetimes.\nFor create-env, I think it's a bit more tricky because then we're introducing an entirely new director API for it to be self-aware of the services its running and their possible properties. For example, do we expose NATS, director, or what about CPI-specific OpenStack/vSphere certs, or external, colocated certs like databases or other APIs?\nI've seen this sort of thing implemented via banner messages before, often shown during login/authentication which would make it more noticeable than bosh env... but could become noisy in the client-auth world where it's shown for every command. Banner messages would need to be dynamically or internally built, which might not be cheap in runtime.\nAnother place I've seen this implemented is through health checks or telemetry. Services can report their cert lifetimes as metrics which allow monitors to be configured and watching. In create-env world, this, again, is a bit more difficult since we don't currently forward director metrics to the local health monitor as a lazy way to forward off the system. More generically, this is also a bit more difficult since we don't currently expose or have published conventions around telemetry.\nJust some food for thought as we continue discussing.. That pull request sounds interesting and seems like a reasonable way to watch expirations in CredHub values.\n\nSuch monitoring of credhub entries would however fail to detect that a bosh deployment isn't yet using the latest version of a given credential in credhub.\n\nThat's true; I'd probably consider it a[nother] metric worth monitoring - how far are deployments diverging from expected/future configurations. Similar to cloud-config/runtime-config. Not quite as easy to monitor right now, and it's something I imagine BOSH making it more easy to discover.. \ud83d\udd17 cloudfoundry#bosh - original discussion on this. \ud83d\udd17 Story #159376982. This can happen if your cloud-config is using ((dns_recursor_ip)) to pull variables from CredHub or your config server. In a deployment context, dns_recursor_ip becomes prefixed with a deployment-specific path; but for cloud-config, we ask that you be explicit about the variable path since it is not relative. So, instead, you probably want to refer to your cloud-config variables as ((/dns_recursor_ip)), with the starting /.. It sounds like the error has changed? Can we close this in favor of the other issue you created?. It looks like you have configured hostname as your endpoint somewhere, possibly under aws.ec2_endpoint. Please try using a valid domain instead.\nI'd also recommend that you have reviewed the instructions for Getting Started on AWS - they have some helpful pointers for bootstrapping new environments.. Sorry, I got the wrong IaaS. In this case, it sounds like your DNS may not be configured, same as this issue. You should make the changes suggested by this dns.yml ops file, and be sure to configure the internal_dns variable as an array with your DNS server(s).. @techie20122018 no, the most practical way would be to update your director's manifest and update it (probably by running create-env again).. Hi there - I herd I might be able to help with this issue. You're on the right trail with the env?/bosh/password approach... but create-env is needs a slightly different manifest format. You'll want to steer clear of instance_groups, and instead try mooving it into resource_pools. Rerun the create-env and, after ruminating, you should be able to sudo apt moo with your password. Hope this helps you stay upright.. @JunoJunho are you referring to this common error?. This may be fixed by https://github.com/cloudfoundry/bosh-cli/commit/aa7ed25e6cbbe6b07fe6414d5a7993489f5a312d. Just to note here, we chatted a bit about this in one of our calls this week. One suggestion was to use a dirty flag to indicate a version may not have finished being uploaded. A couple caveats would be around this being a new field in the HTTP API and possible complications around how to handle the situation that source and/or compiled releases for multiple stemcells could be uploaded in parallel.\n@voelzmo mentioned he'd start a story about this to discuss.. I have mixed feelings about this. From one perspective, I see the desire/need for disabling BBR on specific job configurations. From another perspective, I think it's an easy trap for someone to disable the backups, forget about it, see it being listed in the bbr status output, and assume it's all working as they desire. I think, currently, the bbr.enabled-type approach quietly skips without any informational/warning messages to the user.\nAdding flags to bbr during the backup/restore operation to explicitly limit behaviors leads to a very clear definition on what's happening. It does require the backup/restore operator to be a bit more familiar with the setup (to know which to enable/disable), and requires documenting/repeating those args during invocations though. More generally, it would also help the situation of someone trying to do a partial restore, or partial restores for different jobs from different backup times.\nIf flags aren't an option, I'd wish bbr had more official support for default flagging backup/restore of a job as disabled. If there was a file somewhere that it checks for enabled/disabled/other configs, it could handle some of this duplicate logic and be explicit in output when it's intentionally skipping a job.. Revisiting this, I'm still of the opinion that \"what\" gets backed up should be the responsibility of the backup agent/tool/operator. In environments where business continuity and standard provisioning may be separate teams, I think it's too risky to introduce job-level properties to disable backup support - that configuration is too far from where and how the backup is actually executed. Even if they're not separate teams, it seems to easy to disable job(s) in a development configuration without realizing the change gets promoted to production until it's too late.\nBut, let's see what thoughts the BBR team have on this scenario. This seems like it'd be a typical request, so I'd be interested to hear what discussions may already be out there. @glestaris, any thoughts or references for us to continue the discussion?. I guess I forgot that you already mentioned s3-versioned-blobstore-backup-restorer.enabled is a thing. Although I think it's unfortunate, it's a worthwhile precedent to mention.\nI do agree that it's a bit weird that external use requires specifically including an extra backup-restorer job that you don't want to forget. Optimally that gets encoded in an ops file to avoid most people needing to worry about it.\nI'll be a bit sad, but from a practicality perspective, I suppose we can easily-enough introduce a per-job bbr.enabled flag defaulting to true that we then remove if/when BBR supports better automation/selection options. I don't have a better idea right now.\n@mfine30 I leave this to you for further discussion and prioritization with whomever is best.. Closing - v267.5.0 version is now available from bosh-deployment.. Thanks - this should be fixed as of v5.3.1.. Thanks for providing all the details here. It looks like #1924 already describes this issue, so I'm going to close this in favor of consolidating discussion and tracking it there. Feel free to track it, or add more details there, if helpful.. This is a bit more UX-related. As @h4xnoodle mentions, releases is a general director command, not deployment-specific (despite the CLI having the --deployment flag for all commands... not all commands actually use it). I suppose the releases help message could be more explicit about that, if you think that would help?\nIn terms of suggesting a workaround... you might be interested in the bosh deployment command. It provides a releases (and stemcells) column for the specific deployment you're looking at. Something like bosh -d deployment1 deployment --column='Release(s)' might be functionally equivalent for you.. This typically occurs if health monitor is trying to talk to director over an IP or hostname which is not present in the director's certificate. Since you are using external IPs, I'd recommend double checking what your configuration/certificates look like and verify what host health monitor is trying to connect to.. A while ago, we saw a bug where multiple director worker processes could end up running and happen to both claim the same task and start running it. I think we fixed it... but if you have access to director, it might be worth checking if ps suggests there are multiple processes per worker number (aside from the usual fork'd process that happens when a task is genuinely running).. I don't think we've seen/diagnosed it before... but I suppose the CLI would retry a request if it thinks the director did not receive the request (e.g. 500 error). If you still have task logs... can you check for other tasks around task 1078436 which are ssh types of jobs? The information should also be available from bosh events with that dup username being listed in the context column.. Figured; it was a while ago. If it happens again, I think that'd be a good thing to check to help us debugging, barring any other clever ideas from the team.\nAssuming multiple tasks show up, it would be useful to capture the nginx logs in /var/vcap/sys/logs/director/{access,error}.log as well. Because the CLI internally retries requests without visibly logging, it's difficult to figure out what nginx responded with - so those logs may help.\nAlternatively, you could enable verbose logging with BOSH_LOG_LEVEL... but given the infrequency and unpredictability, it's probably not worth the extra noise in your build logs right now.. Closing - it looks like this was fixed since v268.1.0.. I'm not able to reproduce this. Can you please verify that you are not unintentionally changing other vm_resources or using the same configuration in the affected instance groups?. Tentatively, the policy is that we continually release new major or minor versions of bosh from master, per semantic versioning rules. This differs from our previous approach of creating majors occasionally, and then patching those long-running releases with minor versions. For some of the PCF/LTS-type releases, they would end up as patch releases on whatever major/minor version ended up in those distributions.. @voelzmo not sure what you would like to do with your sneaky-combo issue here, but FYI gnats was upgraded in the most recent v268.3.0, so those monitoring options may be practical now.. I'm not sure a warning would do much at the CLI level. It's easy to misconfigure in automation, and that is likely to be ignored. In theory, if people really want a v*, might as well allow them, albeit I think it's an odd decision, too.\nI do think director's ordering is bad. I think semver versions should have precedence, sorted correctly, and then any remaining ones sorted lexicographically. This makes it clear that semver is the recommendation, but continues to support releases needing other lex-based strategies, and happens to fix this sort of accidental, always-latest. Semver should definitely be the preferred way to version releases.. This makes sense to me; it's a popular endpoint to expose, regardless of what telemetry monitoring/forwarding you end up using. For what it's worth, I think it should be exposed on a localhost-only bind.. To recap an offline conversation from this group, it sounds like this would be a good change to make. It will help as we add expose more of this sort of metrics through the product(s).. Closing - available since v268.3.0.. It looks like you're trying to use a very old manifest which is using ERB-style interpolation. ERB interpolation is no longer supported with recent versions of the CLI. I'd recommend you take a look at this interpolation doc to learn more and, as a reminder, you no longer need to specify director_uuid in your manifest.. Unfortunately, CLIv1 is no longer supported (CLIv2+ has been available and stable for about a year and a half). I'd recommend investing some time to migrate. There should be minimal changes needed, and some command-specific migration notes can be found here.\nIn your specific case, the environment bbl creates causes you to rely on bosh's v2 capabilities for tunneling traffic to your VMs. This is not something that the v1 CLI supports. Once you have upgraded your tools and manifests, you should be able to get further. If, after upgrading, you experience new errors, please feel free to open a new issue.. The CLI does not currently dedup these packages. The issue is a lower priority because it is typically less of a concern in environments where compiled releases are used for director which is what we use and recommend.\nClosing since this is an issue in the bosh-cli - feel free to reopen there if you would like additional visibility on this or you want to continue the conversation there.. This error typically suggests a misconfiguration in the release job - either by referring to incorrect paths to the /var/vcap/packages/{name}/{file} it needs, or by forgetting to have the job depend on that package to make sure it gets installed.\nI'd recommend you look into the cas_controller job and verify it's spec file and cas_controller_ctl template.. Hi - technically this may be a possibility, however, it would likely be very difficult to differentiate between genuine IaaS issues, terrible compilation/package practices, and preempted/spot termination. Typically compilations are relatively quick and, given most IaaSes have even started minutely billing, the cost savings are minimal compared to the day-to-day operations.\nGiven the higher implementation cost and lower value for this edge case, it's not something that will be coming up in our roadmap anytime soon. I'm going to close this - if this is something you're passionate about, feel free to investigate it a bit more and reopen if you're looking to start a pull request.. I think this pull request makes sense. I like the hash syntax with support for more than just an env/landscape name in it.\nSome historical, related requests around this sort of functionality (I don't they should block a merge of this):\n\nforward deployment-level tags thru these plugins\na shared health monitor tags property which applies to all plugins\n\n@mfine30 this seems like a fairly cheap thing for us to merge/prioritize.. Hello - please avoid closing and reopening issues (and creating duplicate issues). If we have not had a chance to review your issue, feel free to ping with another comment and feel free to mention how/why this is important to you to better help us prioritize concerns.\nThis looks like the exact same error as https://github.com/cloudfoundry/bosh/issues/1633 - have you had a chance to search previous issues and try the resolution recommended there? Please give migrated_from a try and let us know how it goes.. It sounds like you're suggesting that BOSH lost its database? Assuming you were updating your existing deployment which was using the 10.1.40.5 IP, migrated_from should have used your existing persistent disk that has your BOSH database. Perhaps you can double check your before and after manifests to ensure they were configured correctly.\nOtherwise, please clarify what \"all deployment went to orphaned\" means in your case.. @h4xnoodle has a good suggestion, but before you do that it's probably worth investigating why BOSH did not think you wanted a persistent disk. Typically that means you are missing one of the persistent disk options in your manifest. If you don't resolve this, the next time BOSH recreates the VM your persistent disk will be missing again.\nDouble check that first and ensure you have a persistent disk attached, then you can follow those attach-disk steps to swap out the disk with the prior disk.. Given that \"meltdown state\" is a health monitor concept and not something the director \"knows\" about, do you have thoughts on how that sort of information might become exposed?. I don't think we have seen this before. It is very surprising that restarting the VM does not fix the issue, but recreating it does. If it is a runtime issue, restarting of process or VM should help. If it's external or host-related, recreates are more likely to fix it.\nThe following error seems most relevant to your issue. If it is a corrupted system or host memory-related issue, that might explain why the recreates fix it.\nWARNING:  terminating connection because of crash of another server process\nDETAIL:  The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and\n possibly corrupted shared memory.\nGiven this sounds specific to your environment and VMs, I'm not sure if we'll be able to be much help. Interested to hear what your investigation finds, or if the community has seen this issue.. The core BOSH director does not currently have functionality for reactive scaling based on that information. Some basic metrics (e.g. CPU, memory, disk) are emitted by health monitor which you can forward to external systems, and you may be able to respond to those metrics within those systems; but no, there's not built-in support for this.. Reminder: we currently support the following scenarios for running errands:\n\nInstance Group (historical) - on instance group X , execute all jobs with a bin/run script\nJobs w/ bin/run - all instance groups with job X are executed\n\nDoes that influence your recommendation for changing what is listed in bosh errands?. Technically, fingerprint (and its value-equivalent, version) are computed based on fingerprinting the data which gets included in a tarball; it does not account for some generally-inconsequential data such as timestamps. It is computed by the CLI when building a release as a method for dedup'ing content across release versions. Its value is then passed through to director which uses it as a similar dedup'ing identifier. The only content verification director does is to check whether the raw tarball bits match the expected checksum from sha1; it does not try to verify the fingerprint/version.\nThat's more of the technical side. What you're trying to do is not a supported operation. Should you need to manual patch and \"reverse engineer\" those values, I'd recommend verifying behavior and your assets in a test environment.. In addition to the original motivation behind this change, I'm very happy to see the formatting/stylistic/consistency changes as well. Thanks for taking that extra time!. Thanks!. This can happen when the last deployment did not succeed because the VM(s) initially failed but ultimately recovered. This is director's way of indicating there was still some metadata that needed to be updated once it verified the VM was successfully running. I would recommend checking the prior deploy(s) to see if this is the case.\nIf that is not the case, it may be a bug. However, logic around these operations have changed significantly over the past 14 months since 263.3.0 was released. Have you been able to reproduce this issue on a more recent director to provide us with a more relevant test case?. Closing - let us know if you see this on newer directors.. It seems we do not have access to the pipeline. Since this appears to be flaky behavior, we would need more details from the agent logs in /var/vcap/bosh/log (mind the credentials in there). Next time this occurs can you capture more logs?\nSpecifically, the agent should have received sync_dns messages from the director which trigger it downloading and installing the records.json file. There would likely be errors around those sync_dns messages if it was having trouble downloading those files which would be relevant.. Oops, I missed those original questions. It looks like the mkdir/chown was introduced for director and worker in 2013 as part of a \"fix stemcell creation\" changeset. I assume that meant running things more strictly as vcap and perhaps the recursive part was necessary for migration?\nSo, good thinking. I don't think we need to recurse, given we've been running as vcap for such a long time. I'd hope that nobody is trying to upgrade from v1.3262.25.0 to v268.4+ nowadays :). The ERRORs you've found in --debug happen to be expected in this case - director uses try-catch logic around IP allocations in order to support concurrency with other deployments.\nThe original errors you'll want to look into were the failed tasks from the normal output:\n\nTask with id ... could not be found probably means the agent (or VM) was unexpectedly restarted or crashed. Try again to see if its flakiness, otherwise see these recommendations for a similar error.\nUpdating certificates with retries typically means the VM is underpowered; try using a slightly more powerful instance type.\n1 of 2 post-start scripts failed. Failed Jobs: uaa. Successful Jobs: bosh-dns means the UAA job failed while it was starting up. You can find more details from the logs in /var/vcap/sys/log/uaa/*, possibly in the post-start.*.log files.. Sorry, but I don't follow what the current BOSH error you are facing. Can you restate what, single error you are now blocked on? Also please include the exact error output and steps you've already investigated.. This typically occurs due to misconfiguration of one of the jobs running on your instance. With create-env workflows, you may want to try installing a jumpbox user to help debug. Then you can SSH and check monit summary to see which jobs are failing, and then take a look at /var/vcap/sys/log/**/* files for underlying errors.\n\nIf you're not already using it, I would recommend deploying BOSH with bosh-deployment because it has quite a bit of defaults which reduce the chances of configuration errors while you're bringing up a new environment.. Make sure you're looking at logs in /var/vcap/sys/log/health_monitor/* for more detailed hints. Occasionally health monitor will not emit error messages for things like bad/untrusted certificates on the director's side or if it's communicating with an entirely incorrect director host/IP or using bad credentials.\nI'd recommend checking all of those configuration properties (might be easiest to look at the configuration values in /var/vcap/jobs/health_monitor/config/* if there's any doubt around what values your manifest might be using.\nAnd, again, a reminder about bosh-deployment - these issues are much less likely to occur if using that development deployment repository to get started.. There's not currently a way to disable agent from overwriting /etc/hosts. It does it whenever it receives new sync_dns messages. The /etc/hosts was a very limited, first version of \"BOSH DNS\", but assuming bosh-dns has been adopted, it's not really needed anymore since bosh-dns answers the same hosts.\nI think we should find a way to remove or disable agent's interference with /etc/hosts, possibly as opt-out behavior for a while. Do you have thoughts on how you would like to go about that, and would you be willing to contribute those changes to director/agent?. @aclevername can you speak to the desired operator experience in this situation from a general BBR perspective? Some questions:\n\nHow do you see BBR users dealing with configuring database backups?\nShould they need to know to differentiate between internal and external databases?\nShould they be expected to add bbr-database-X backup jobs if they're using an external database? How does that get communicated?\n\nIn this case our director apparently has taken responsibility for backing up the database. As an alternative to this pull request's change:\n\nShould the backup/restore functionality instead be removed from director and into the database-specific jobs which provide the local postgres database?\nShould the current director backup/restore functionality be fixed to work for both local and remote databases?. @miguelverissimo while it would be nice to provide more awareness into what might be happening in the backup, I don't believe standard output is streamed to the client where they would see that sort of message.\n\ncc @mfine30 @jfmyers9 for prioritizing/continuing this.. @mfine30 accurate summary.. Thanks! Do you have an example of a test where you're using this? Given that link(...).address is technically a method which accepts arguments, I'm wondering how we could help make those calls fully testable (e.g. that a release is specifically looking for \"healthy\" instances).\nHave any ideas about how to solve that, or examples where it might be useful in your template?. Hi @alext - thanks for raising this; I think it is a valid issue and worth discussing a bit more. The combination of light stemcells and their empty image checksums being used as a key in the compiled package cache is unfortunate. We've relied on sha1 as part of that cache key since before light stemcells were even a feature. Nowadays, I agree, we ought to consider different keys.\nThat being said, the compiled package cache feature is something we're not very interested in maintaining for much longer. The feature is a bit lacking - both in functionality and modern usage. One of its significant limitations is that it offers no integrity verification of the blobs it uses. The data is based purely on a [now-proven impartial] key and there's no checksum verification that can happen between environments. In theory, if you can lock down those assets and maintain your own audit and integrity of those files, it is workable. But since BOSH can't prove those assets in the same way it is able to track and prove all other assets, we haven't really been recommending this feature. Given all that, fixing this particular issue is a very low priority for us.\nI do know that it can be a convenient feature though. If you're comfortable discussing any of it here, are you open to the idea of disabling the compiled package cache in your environments? While we're at it, what impact would it have if we were to remove this feature at some point in the future? If we're considering other ways BOSH might be able to support this functionality, is your main use case to avoid the repeat compilation times across environments? Or is it for requirements that your environments use the exact same asset? Or to support environments which disable compilation?\nThanks for taking the time to track down this behavior to code and a repro case - not a trivial thing to diagnose!. Closing \u2013 it doesn't look like we currently document this in a public way (aside from what is exposed as job properties). I'm going to be optimistic that this doesn't come up frequently rather than spend much extra time adding negative feature docs. Feel free to PR and reference this if you'd like to see something specific.. Also possibly related to https://github.com/cloudfoundry/bosh-agent/issues/210.. This reminds me of the change we made around always cleaning up backend.sock in https://github.com/cloudfoundry/bosh/commit/075222d25c028e7e828394a45633e30bbbc7c127 and it's respective, internal story.. I think this is a duplicate of https://github.com/cloudfoundry/bosh/issues/2069 which appears to be fixed in v268.4.0. Are you able to verify the fix with a newer director?. Closing as duplicate of #2128.. @amuessig your scenario sounds like a different issue. The bosh clean-up --all would try to clean up unused stemcells, but, assuming the cleanup task is successful, would no longer be visible in bosh stemcells nor vCenter. Rather than getting into that here, please create a new issue to discuss that separately; and be sure to include the relevant task logs of the clean-up which claims to have removed the stemcell, along with the later stemcells and upload-stemcell output.. I'd believe that the reboot_vm method might fail if the agent's disk mounts information is inaccurate. Presumably it fails when the agent is coming back up and is running through it's bootstrapping?\nTheoretically, I could imagine situations where the agent's disk settings are in a bad enough state that mount calls may fail until it's rebooted or recreated. But I don't actually have a concrete scenario to verify that. Did registry-removal change the behavior of this in any way? i.e. would agent have been able to load corrected/fixed settings from registry on reboot and automatically recover the mount, whereas in new versions / v2 CPI the mount_disk is needed to pass updated settings?. This is likely due to bosh-dns-aliases not having any packages. When checking for needed uploads, CLI asks director what source/compiled versions it already has and, if it doesn't find a compiled package for the stemcell, it re-uploads it.\n@heyjcollins, I'd recommend that cf-deployment stop trying to compile the bosh-dns-aliases release. It has no packages needing to be compiled (it's essentially a no-op export-release for you today), and I think it'd happen to fix this particular example, too.\n@aemengo, if you're interested in broader discussion around this behavior for contexts other than cf-deployment, I'd recommend creating a new issue in bosh-cli.. We've touched on this before in some internal discussions a while back. One thing we need to keep in mind is compiled releases which float (e.g. exported release/1.2.3 for xenial/250.7 was uploaded, but it's being used by a deployment running xenial/250.11).\nI guess that's mentioned in the Slack thread, but worth calling out here. Especially with respect to some of the recent discussions around how director picks the floating compiled release to use.. @voelzmo yeah, probably; just something to keep in mind, at least.. Hi - we were taking a look at the latest changes and had a couple thoughts.\nFirst, is there a reason that EvaluationContext.== does not need to consider if @properties has changed? Seems like we check all the other publicly-accessible values. Semi-related, any way we can ensure we don't introduce new properties and then forget to include them in our custom == logic?\nSecond, can you add a test for the EvaluationContext.== change to verify the behavior is working as expected? Want to make sure that we don't regress on the behavior in the future.\n@dpb587-pivotal && @charleshansen . Doesn't this fail before monit considers it healthy, and doesn't monit automatically restart it when this happens? I'd expect the impact of this to be minimal. Is the goal to minimize log noise, or is there some functional impact from it?. If we're going to change this, what would you think of it failing with an error instead? The operator essentially requested an unsupported operation; so it seems weird to pretend it was successful. I'd rather push operators to not making the call at all if it's not something their environment supports.\nI remember finding this behavior confusing and concerning when I was first starting to use BOSH.. Hey, cool, looks like folder option is still supported.\n. \"Last Timestamp\" sounds a bit weird and doesn't really give context. Would \"Last Update\" or \"Updated\" or \"Last Activity\" perhaps be a bit more descriptive and helpful to users?\n. glad to know - thanks!\n. Why is this change needed? Tests seem to run fine without it.\n. When running this test, the events output includes the following seemingly-duplicate lines - two each for the create and delete event. It seems separate from the changes here, but curious if it's expected.\n| 13 <- 12 | Wed Apr 27 00:03:12 UTC 2016 | test | create   | vm           | 13540                                       | 5    | simple | id_job/c50eeb84-fa56-40be-8755-33faa968b881 | -       |\n| 12       | Wed Apr 27 00:03:12 UTC 2016 | test | create   | vm           | -                                           | 5    | simple | id_job/c50eeb84-fa56-40be-8755-33faa968b881 | -       |\n| 11 <- 10 | Wed Apr 27 00:03:12 UTC 2016 | test | delete   | vm           | 13349                                       | 5    | simple | id_job/c50eeb84-fa56-40be-8755-33faa968b881 | -       |\n| 10       | Wed Apr 27 00:03:12 UTC 2016 | test | delete   | vm           | 13349                                       | 5    | simple | id_job/c50eeb84-fa56-40be-8755-33faa968b881 | -       |\n. Nevermind - I see it's used by #1221.\n. Would it be simpler if we were just storing {} rather than null/\"null\"?\n. If we control which commands have the --fix option, do we need to be performing this check as well - I thought the regular opts/args parser would error?\n. I think it would be better for InstancePlan to check the state of the existing instance directly, similar to how it does all the other checks on the raw models/objects - this way we keep the logic centralized in this class and based on state. Otherwise we start relying on external, InstancePlan instantiators to perform the correct logic and correctly influence how instances are potentially going to be changing later. Maybe others have some thoughts though...\n. With the 4f5ef6a change (thanks!), this could probably be simplified/reverted to the original code.\n. With the 4f5ef6a change, this could be simplified since the value will no longer be null.\n. Since it was already merged at one point and our existing tests were a bit\nweird to begin with, maybe lets just re-merge and we clean up whatever we\nneed to. Seems easier and more helpful than back-and-forth on these details.\nOn Wednesday, September 7, 2016, Ming Xiao notifications@github.com wrote:\n\nIn bosh-director/spec/unit/deployment_plan/instance_plan_spec.rb\nhttps://github.com/cloudfoundry/bosh/pull/1411#discussion_r77882636:\n\n+\n-          it 'should ignore dns_record_name when comparing old and new network_settings' do\n-            new_network_settings = {\n-                'existing-network' =>{\n-                    'type' => 'dynamic',\n-                    'cloud_properties' =>{},\n-                    'dns' => '10.0.0.1',\n-                }\n-            }\n  +\n-            allow(logger).to receive(:debug)\n-            expect(logger).to_not receive(:debug).with(\n-                \"networks_changed? network settings changed FROM: #{network_settings} TO: #{new_network_settings} on instance #{instance_plan.existing_instance}\"\n-            )\n  +\n-            instance_plan.networks_changed?\n\nconsulted with @dpb587 https://github.com/dpb587 , and we think that\nthe result should be false. Since the network did not change and we are\nnow ignoring dns_record_name, networks_changed? should return false\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1411/files/533d03117c1a48c1bdad56bd197b0c7bde1fb15b#r77882636,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/APYVcFqPLSW8r2UfVrpI6QHbZzSM2XUPks5qnwhogaJpZM4JrxLq\n.\n. Should we always send back id and index? Including both values seems valid for an SSH results response. Then the UI/cli becomes responsible for showing the user whichever is appropriate. Or maybe that would impact the ruby CLI in ways we want to avoid? Dunno; just a thought.\n. Hmm, this would end up changing the data format of index which I don't think is intentional (maybe some clients require integers). Rather than having a conditional for target.ids_provided?, let's just do...\n\nresult[\"id\"] = instance.id\nresult[\"index\"] = instance.index\nOld CLI will look at index and have the same info as normal and new CLI will correctly show the uuid.\n. I don't see a test for when both include and exclude are configured, but they are not a complete union. Perhaps we can add one?\n. I see AddonInclude was renamed to AddonFilter - I think we should do the same thing here instead of IncludeExclude. e.g. RuntimeIncompleteFilterJobSection.\n. We're removing the Runtime namespace which makes sense, but perhaps we could keep an Addon prefix?. \ud83d\udc4d . Can we rename this class to Filter since it's now namespaced in Addon to avoid redundancy?. Same thing as Filter - make sense to rename to Bosh::Director::Addon::Parser?. Rather than a boolean value here, could we consider a constant representing the scope? For example, Bosh::Director::Addon::CONTEXT_RUNTIME (or DIRECTOR?) and ...::CONTEXT_DEPLOYMENT? Then we could also use it in the Addon class, too. It makes arguments a bit more explicit and we don't have to wonder what a boolean value means.. Should we be explicit about where the bad addon was located to avoid the scenario where a runtime config has an error, but the user is getting an error about something they can't find in their deployment manifest.. \ud83d\udc4d  reformatting. Nice cleanup. I disagree with using DELETE for both deletion, but also non-deletion behavior. Technically I think this endpoint should delete the CID, regardless of orphaning status (even if we error for active disks).\nCan we consider a post /:disk_cid/orphan endpoint instead?. Chatted about this more. Ignore my comment.\nLong-term, the committed code seems like the most practical approach (though I still think it's going to be a slightly confusing API outside of the CLI).. Do we care about backwards compatibility? We did GA the CLI after all.... Can this conditional be removed? As far as I can think, templates should always be affiliated with a particular release.. seems weird and distracting and it should be fixed sometime, but outside of scope of this change; thanks for the reminder. Traditionally we try to create these managers once in the initialize, then in the function invoke the desired method (example). Not critical, but helps us keep our newers in a consistent place until we can do better at DI.. For consistency with the route, can we rename this class/file to be \"orphaned\".. ruby apt only had 1.9.3 and i thought it'd be better to use the same as director. Possible to test a scenario where persistent disk updates fail, and then ensure the \"latest\" DNS version is the version prior to the update?. One of these days I'll go through and reformat all the lets in our specs to consistently use spacing inside the curly brackets.... should we be adding instance_id in this round of consistency fixing, too?. Well reasoned. Personally I consider tags to be global and shared across resources if they have the same meaning/context, so I generally lean towards consistency between the two resources. Optimistically, I think these tags would be namespaced with bosh. or something as well, but I'm not holding my breath for that idea :)\nBut you're right, these tags are definitely within the context of an instance, so they could theoretically be just group. Given current conventions,group might be more appropriate. I defer to @cppforlife.. good catch; odd that tests were incorrectly written. Hi! Sorry this PR has sat for so long. We were looking at this today, but had a question about why the log_exception change was necessary. It seems odd for a task to ignore an exception and not consider itself failed. If you recall more about this change, can you clarify for us?\nThe other changes make sense for better logging to UI about what happened or didn't happen. Just confused why task-level behavior needs to change, too.\nThanks!. Perhaps we can say (gone) instead of expired. Parens to specifically call out there's something special with the value, and \"gone\" instead of expired since expired typically means a lock was lost (rather than intentionally dropped). Wondering if people will read it and come to us saying \"my lock expired but the deployment still failed\". Just a though; whatever you think.. Extra space :). Unused second argument. Could this to be more restrictive to network IP? Otherwise we're allowing filtering by other IPs like netmask/gateway/dns and seems like it might be abused intentionally or not. e.g. %\\\\\\\"ip\\\\\\\":\\\\\\\"#{ip_address}\\\\\\\"%. I'm not sure of Sequel syntax, but instead of this dual-call thing which looks a little confusing and fragile, do you know if it's possible to convert filter hashes to arrays with something like the following? Mainly trying to think how to only have a single InstanceLookup.new.by_filter and deployment_id filter.\nfilter = [filter] if filter.is_a?(Hash). Theoretically, I suppose there could be ambiguity on whether it's a job or an IP address. Is it worth the complexity of only falling back to IP address if job name did not match? Probably not, but I wondered if you had considered the possibility.. Hand the whole `deployment` class down to `applies?` feels a bit heavy. Could we potentially just pass the teams from the model along with the `deployment_name`?. If we were to pass an array of team names to `applies?` it would be great if deployment had a method that would return said array of team names so that the caller does not have to understand or reach into the model and make a map itself.. the `has_teams?`and `has_applicable_team?` methods look to be mirroring the stemcell implementation which introduces a bit of necessary complexity. Could we remove the extra calls to `has_teams?` that appear in `has_applicable_team?`.. grammar should be \"when one of the teams matches one from the included spec\".. grammar correction here - \"when none of the teams match the filter spec\". Could we add a context here for the `when clause`. Code right now is a bad example but we are going to start to break these whens out.. Is this actually correct? The spec says it does not apply but our expectation says that it should apply. One of these two is wrong.. spacing of `=`. `for the specificed team`. `when a team is specified`. @barthy1 - wanted to bump this. Were you still looking at removing this logic. Really appreciate the PR. \ud83d\udc4d . This doesn't really test that parallel checks happen, but I see the value of ensuring results are not interleaved.. Should this be configurable?. I think this refactor introduces more confusion than it's worth. With a `parse` method on a singular class, I expect it to receive input to return me a singular object. I also don't like that we're moving in top-level, cloud-config `azs` type configuration data here since that should be out of scope of this naive model.\n\nI don't think this is the correct level for this refactoring. Should it be a class method of CloudManifestParser instead? Unless I'm missing something, mind re-refactoring this?. Aw, testing instance_variable_get? We should use public interfaces for testing and not referencing this sort of internal implementation. Can we test via get_name_for_az instead?. Similarly, we shouldn't be checking instance variables. Perhaps get? Perhaps expose the cloud properties from external CPI, or, as a \"short term\" verify expect on the last argument of ExternalCPI.new?. These should be removed.. remove. Verbose, but dynamically defensive; clever. I'm okay with it.. ",
    "niuqingqing1001": "hello, drnic.\nI understood the discussion above, but I can't get the method to solve the error after a lot of attempts.\nCan you give a hand?\nThank you very much.\nerror info:\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't\n. Thank you, drnic.\nI have see the thread at:\nhttp://grokbase.com/p/cloudfoundry.org/vcap-dev/136bfq5zfv/script-which-jobs-using-new-p-x-y-z-helpers-and-which-arent\nI think your point is this:\"To summarize: these two jobs(cloud_controller_ng and gorouter) can be colocated; but I think not with any other job today. Or something like that.\"\nmy cf.yml is at the back:\nyou can see cloud_controller_ng and gorouter are in the job \"api\", they are together. But my error is in the job \"core\".\nSo what can I do?\nI had another quesion. \nShould I manually modify the /jobs/*/templates files that use the old style into the new style to resolve the error?\nI had attempt to modify all the files to new style, but it didn't work, and the error still appear.\nor Should I manually add properties section to the jobs that had no this section to resolve the error?\nI had spent four days on this error, and hope you can resolve the problem with me.\nThank you.\n\nError 80010: Job core' has specs with conflicting property definition styles between its job spec templates.  This may occur if colocating jobs, one of which has a spec file includingproperties' and one which doesn't.\ncf.yml\njobs:\n- name: core\n  template:\n  - syslog_aggregator\n  - nats\n  - postgres\n  - health_manager_next\n  - collector\n  - debian_nfs_server\n  - uaa\n  - login\n    instances: 1\n    resource_pool: medium\n    persistent_disk: 8192\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    properties:\n    db: databases\n- name: api\n  template:\n  - cloud_controller_ng\n  - gorouter\n    instances: 1\n    resource_pool: medium\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    - name: floating\n      static_ips:\n  - 10.141.123.245\n    properties:\n    db: databases\n- name: dea\n  template:\n  - dea_next\n    instances: 1\n    resource_pool: dea\n    networks:\n  - name: default\n    default: [dns, gateway]\n. Hi, drnic.\nThe method of ripping out the properties: section from each of the spec files doesn't work.\nBut it doesn't matter.\nMy colleague stevenbzb  also open an issue for the same problem, the link is as follows: \nhttps://github.com/cloudfoundry/bosh/issues/398\nAccording to you suggestions, we modify the /deployment/cf.yml. At last the error is resolved.\nps: We didn't modify any file in the /jobs.\nThank you very much.\n. ",
    "amaltson": "I just ran into this and was banging my head for a while. In my case the release didn't even have any templates that had properties inside of them. I just had a line that had something like:\nhostname application.<%= index %>\nThat was the only interpolated aspect of the template.\nThe worst part was it worked without a properties: {} on bosh-lite but failed on the PCF 1.7 BOSH.\n. I just ran into this and was banging my head for a while. In my case the release didn't even have any templates that had properties inside of them. I just had a line that had something like:\nhostname application.<%= index %>\nThat was the only interpolated aspect of the template.\nThe worst part was it worked without a properties: {} on bosh-lite but failed on the PCF 1.7 BOSH.\n. ",
    "danhigham": "Yup, granted, but these two changes are necessary to do bosh aws create. Ditch the PR if you want and I will include these commits in one next week when I get the rest working.\n. Yup, granted, but these two changes are necessary to do bosh aws create. Ditch the PR if you want and I will include these commits in one next week when I get the rest working.\n. ",
    "xoebus": "Yep, looks as if the code surrounding this is already using some polymorphism for each of the different platforms so it should be possible to merge this conditional into that. There is some other low hanging fruit that Code Climate found in the CentOS and Linux disk classes that vary by a single digit. I didn't include that in this commit since I'm not sure what that digit is and why it is different between the two.\n. The second commit looks good but the first is going to change the public API of the CLI gem which I'm not wild about. I know the backend team is going to have to change some scripts that use the underscores in the options.\n. Thought about this some more and I'm being too picky. The next BOSH release will be a major version anyway and it's always annoyed me that these are underscores. +1 from me\n. The latest micro bosh stemcell for vSphere can be downloaded here. It should work with the newer CLI and gems.\n. Looks awesome! You need to vendor the gems you added though otherwise CI will not like it.\n. Merged, thanks!\n. > You mean like this: bbatsov/rubocop#355\n:+1: \n. Are you using bosh diff to merge a deployment stub and a deployment template? If so, then you should use the #find function as shown here. BOSH does allow ERB directly in the manifest but it only gives the TOP_LEVEL_BINDING binding to the template (for any simple work). Your deployment manifest should be plain YAML most of the time.\n- Chris & Matt (@mmb)\n. The --deployment flag is a global flag and so it has to be before the sub command. The command bosh -d manifest.yml deploy should be used for this use case.\n. Merged manually, thanks!\n@ryantang / @xoebus\n. @drnic \u2014 Thanks for this bugfix! Could you add some tests that check that this bug remains squashed in the future? Cheers.\n. We're still working on this so feel free to let it rest here until the work is complete.\n. One day...\nOn Tue, Jul 1, 2014 at 7:07 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #537 https://github.com/cloudfoundry/bosh/pull/537.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/537#event-137090196.\n. This would be simpler if I could drop support for warning people if they have a target: element in their manifest rather than a director_uuid: element. I didn't even know target: was a thing. Any objections?\n. We (@aminjam and myself) just ran the tests again for this PR and everything is looking green. This is getting near being merged. We have a few questions about some of the changes that we will comment inline below.\n\nThanks!\n. After these things are addressed it would also be nice if the commits here could be squashed into a single commit.\n. Very much so.\nOn Tue, Oct 2, 2018 at 11:57 AM Morgan Fine notifications@github.com\nwrote:\n\n@xoebus https://github.com/xoebus is this still a thing?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1946#issuecomment-426390979,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AACXnmhTyKDFZ0irnIzthYGl1OFHjoIOks5ug7cUgaJpZM4Tweb1\n.\n. It's the first time I've heard it requested but it's worth exploring if it could be supported. I've opened an issue on bpm-release to dig into this: https://github.com/cloudfoundry-incubator/bpm-release/issues/68.. In that case can you log to stdout and stderr and use a log forwarding release?. Ok, I'm going to close the BPM issue for now. When we've decided what we want the BOSH behavior to be then we can open another one.. We do the same in Perm. /var/vcap/sys/log/perm/audit.log. I'm able to completely remove the implementation (marked with * below) that this test corresponds with and this test still passes. Are you sure it's testing what you want it to test?\n. *\n. I think there are tests missing for this conditional behavior.\n. \n",
    "cdavisafc": "I don't recall if it was exactly in this spot, but have often had things die during a bosh deploy, only to reissue the bosh deploy and succeed the next time around.  I assumed it to be some network glitch.\n. I've definitely seen this behavior several times, and have always just\nchalked it up to some type of network blip, or timeout.  Doing the bosh\ndeploy again allows things to continue.  I'm working in a vSphere\nenvironment.  Wouldn't know how to reproduce as it happens sporadically.\nOn Wed, Aug 7, 2013 at 12:15 PM, aram price notifications@github.comwrote:\n\n@msackman https://github.com/msackman / @cdavisafchttps://github.com/cdavisafc/\n@seansweda https://github.com/seansweda - do any of you have insight\ninto how to reproduce this, or capture more info when the error happens?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/290#issuecomment-22276198\n.\n. \n",
    "michaelklishin": "Any progress on this? Jobs debugging is really painful without logs.\n. ",
    "goehmen": "@msackman and @aramprice would have better input into whether this still needs to be fixed or if in the subsequent months, other ways of accomplishing the same desired outcome have been developed/found.\n. There are a few stories related to BOSH user management in the\nbacklog/icebox.  Mainly -\nhttps://www.pivotaltracker.com/story/show/68212726- Bosh User Admin:\nRole Development.   This fits in that theme for sure.\nNic - is what you are saying \"Precreate the HM user because creating it\nexactly when you need it is too painful.\"?\nOn Mon, Apr 21, 2014 at 3:53 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nI think it's still a good idea - create the HM user when it is configured\n- otherwise it is complex to setup a non-admin user as the HM stops\n  working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley notifications@github.com\nwrote:\n\n@drnic\nIs this still an issue? If not, we would like to close this since it\nlooks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986220\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. @drnic Closing this.  I think spiff replaces this.  Happy to revisit and reopen if we need to.\n. @drnic Closing this.  I think spiff replaces this.  Happy to revisit and reopen if we need to.\n. @drnic Please verify whether this is still an issue or has been resolved.  If an issue, I'll work toward resolution.  I created https://www.pivotaltracker.com/story/show/67129536 to track\n. @frodenas  Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67129724 to track\n. @frodenas - NVM.  The BOSH team will handle the refactoring before completing the PR\n. @frodenas Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67129974 to track\n. @frodenas Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67130142 to track\n. @frodenas Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67136620 to track.\n. @frodenas Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67130366\n. @grenzr @mkocher @frodenas  \nOK.  So, the bot didn't fire off a tracker feature (mostly because the PR pre-dates the bot) & Ferdy originally created the PR as [CLI] so it got past the BOSH team.  I edited the subject to [BOSH-CLI] and talked to the community pair to get a tracker feature into my icebox.  We are rolling now.  https://www.pivotaltracker.com/story/show/66875308\n;-)\n. @frodenas @drnic BOSH team will handle the changes requested above via https://www.pivotaltracker.com/story/show/68325962\n. @frodenas Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67130524\n. @thansmann @mikepatterson Ping me to discuss the status here, please.\n. I spoke with @mikepatterson .  We need access to openstack cluster that has self-signed certs so we can run BOSH against it and see if issue exists or was resolved by PR 461 as per Ferdy's comment above on 1/8/14.  Mike is going to take on the work of getting access to cluster & test.\n. @mikepatterson Any update on getting us access to that cluster?\n. @thansmann Any input on @mikepatterson's clarification request above?\n. @frodenas See Mike's comments above.  Does this resolve your issue?\n. @amhuber @grenzr Any input on @calebamiles/CF Community Pair comments from 03/12/2014? Thx\n. @Kaixiang @monkeyherder @drnic  Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67130640\n. I closed #65856978 and #71252782 - we don't need 2 stories & a bug on this.\nthe ongoing story will be https://www.pivotaltracker.com/story/show/73552140\n. @caseymct  Please verify whether this is still an issue or has been resolved. If an issue, I'll work toward resolution. I created https://www.pivotaltracker.com/story/show/67135670 to track.\n. Closing this PR.  @caseymct : pls let me know if you want to discuss. Thx.\n. @yudai @mkocher There was not a Tracker story created by gitbot.  I created the story and prioritized it in my backlog. https://www.pivotaltracker.com/story/show/67135740\n. @yudai @mkocher There was not a Tracker story created by gitbot.  I created the story and prioritized it in my backlog. https://www.pivotaltracker.com/story/show/67135740\n. @drnic Is this PR still valid? Or has the issue been otherwise handled?  I created BOSH tracker story https://www.pivotaltracker.com/story/show/67135950 for this.\n. @drnic Is this PR still valid? Or has the issue been otherwise handled?  I created BOSH tracker story https://www.pivotaltracker.com/story/show/67135950 for this.\n. @mmb @ytolsk  @tsaleh  - No, it's not OK to break backward compatibility.  @drnic - Can you resubmit code change that is backwards compatible?  Glad to have a conversation to get on the same page.\n. @mmb @ytolsk  @tsaleh  - No, it's not OK to break backward compatibility.  @drnic - Can you resubmit code change that is backwards compatible?  Glad to have a conversation to get on the same page.\n. Yes, please do close the issue.  backup & restore functionality will be\nrevisited in the very near future but is not currently a feature of Bosh.\nOn Wed, Feb 5, 2014 at 11:05 AM, Eric Malm notifications@github.com wrote:\n\n@goehmen https://github.com/goehmen Is this currently a priority for\nbosh? If not, we'd like to close this issue.\nThanks,\nCF Community Pair (@ematpl https://github.com/ematpl & @sclevinehttps://github.com/sclevine\n)\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/506#issuecomment-34225020\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. Yes, please do close the issue.  backup & restore functionality will be\nrevisited in the very near future but is not currently a feature of Bosh.\nOn Wed, Feb 5, 2014 at 11:05 AM, Eric Malm notifications@github.com wrote:\n\n@goehmen https://github.com/goehmen Is this currently a priority for\nbosh? If not, we'd like to close this issue.\nThanks,\nCF Community Pair (@ematpl https://github.com/ematpl & @sclevinehttps://github.com/sclevine\n)\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/506#issuecomment-34225020\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. @mrdavidlaing Can you provide some deeper context about the use case you have in mind here.  The Bosh team is thinking about how resurrector would handle spot instances. Thx.\n. @mrdavidlaing Can you provide some deeper context about the use case you have in mind here.  The Bosh team is thinking about how resurrector would handle spot instances. Thx.\n. Thanks David.  How would you expect monitoring and logging to react in a\nproduction/enterprise sense?  What is the resource implication of the\nresurrector's continuous attempts?\n-Greg\nOn Mon, Feb 3, 2014 at 10:29 PM, David Laing notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen, I have two BOSH deployments - CF\nand ElasticSearch. In both I run some of the \"redundant\" nodes (eg, CF-DEA\nand ElasticSearch non-master nodes) as spot instances.\nI have the resurrector enabled for the spot instances, and it works\nfantastically.\nTypically with AWS spot instances the price spikes for a short period, and\nthen drops back down.\nIn this scenario the resurrector notices; and attempts to start up a new\nnode. When the spot price is too high, the resurrection fails. (I set a\nshort validity time on the AWS spot request, so this gets cancelled at\nabout the same time as the resurrector gives up waiting)\nThe resurrector seems to keep trying, and when the spot price has dropped\nagain, the resurrection succeeds and the spot nodes join back to the\ncluster.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-34034200\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. Thanks David.  How would you expect monitoring and logging to react in a\nproduction/enterprise sense?  What is the resource implication of the\nresurrector's continuous attempts?\n-Greg\nOn Mon, Feb 3, 2014 at 10:29 PM, David Laing notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen, I have two BOSH deployments - CF\nand ElasticSearch. In both I run some of the \"redundant\" nodes (eg, CF-DEA\nand ElasticSearch non-master nodes) as spot instances.\nI have the resurrector enabled for the spot instances, and it works\nfantastically.\nTypically with AWS spot instances the price spikes for a short period, and\nthen drops back down.\nIn this scenario the resurrector notices; and attempts to start up a new\nnode. When the spot price is too high, the resurrection fails. (I set a\nshort validity time on the AWS spot request, so this gets cancelled at\nabout the same time as the resurrector gives up waiting)\nThe resurrector seems to keep trying, and when the spot price has dropped\nagain, the resurrection succeeds and the spot nodes join back to the\ncluster.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-34034200\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. @mrdavidlaing  Thanks-pretty cool.  We'll check it out. - https://www.pivotaltracker.com/story/show/65132950\n. @mrdavidlaing  Thanks-pretty cool.  We'll check it out. - https://www.pivotaltracker.com/story/show/65132950\n. THe BOSH team will be working on a story for this functionality this week.\n Check in with the public Tracker backlog at the end of the day PT for\nstatus.\nCheers!\nOn Sun, Feb 16, 2014 at 10:20 PM, David Laing notifications@github.comwrote:\n\n@jbayer https://github.com/jbayer, @goehmen https://github.com/goehmen- thanks for keeping this moving forward. Let me know if I can help out in\nany way.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35231427\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. THe BOSH team will be working on a story for this functionality this week.\n Check in with the public Tracker backlog at the end of the day PT for\nstatus.\nCheers!\nOn Sun, Feb 16, 2014 at 10:20 PM, David Laing notifications@github.comwrote:\n\n@jbayer https://github.com/jbayer, @goehmen https://github.com/goehmen- thanks for keeping this moving forward. Let me know if I can help out in\nany way.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35231427\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. That link doesn't resolve anymore because the story was trashed.  The BOSH\nteam originally opened that story to spike on spot instances but decided to\nkeep the work contained in the story for your PR.\nHope this helps,\nG\nOn Fri, Feb 21, 2014 at 3:50 PM, rboshman notifications@github.com wrote:\n\nOh, I see what you are referring to...I don't have access to that other\nstory (65132950) either. Letme figure it out.\nG\nOn Fri, Feb 21, 2014 at 3:47 PM, Greg Oehmen <goehmen@pivotallabs.com\n\nwrote:\nDavid:\nDo you have a PivotalTracker account? If so, are you logged in? You\nshould be able to access the story without an account:\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950\nG\nOn Fri, Feb 21, 2014 at 10:35 AM, David Laing <notifications@github.com\nwrote:\n\n@goehmen https://github.com/goehmen - BOSH Pivotal Tracker >\ncloudfoundry/bosh #509: Add aws spot support<\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950>says:\nwaiting for story #65132950 to complete before executing PR\nI don't have permission to see the status of #65132950 or what its\nabout.\nLet me know if you need any further info from me\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35759165>\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35785956\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. That link doesn't resolve anymore because the story was trashed.  The BOSH\nteam originally opened that story to spike on spot instances but decided to\nkeep the work contained in the story for your PR.\nHope this helps,\nG\nOn Fri, Feb 21, 2014 at 3:50 PM, rboshman notifications@github.com wrote:\n\nOh, I see what you are referring to...I don't have access to that other\nstory (65132950) either. Letme figure it out.\nG\nOn Fri, Feb 21, 2014 at 3:47 PM, Greg Oehmen <goehmen@pivotallabs.com\n\nwrote:\nDavid:\nDo you have a PivotalTracker account? If so, are you logged in? You\nshould be able to access the story without an account:\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950\nG\nOn Fri, Feb 21, 2014 at 10:35 AM, David Laing <notifications@github.com\nwrote:\n\n@goehmen https://github.com/goehmen - BOSH Pivotal Tracker >\ncloudfoundry/bosh #509: Add aws spot support<\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950>says:\nwaiting for story #65132950 to complete before executing PR\nI don't have permission to see the status of #65132950 or what its\nabout.\nLet me know if you need any further info from me\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35759165>\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35785956\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. @mrdavidlaing FYI - the story is actually slated for the 03 March iteration but was put at the top of the backlog for that iteration so that it could be easily pulled into the current iteration if the opportunity was to arise.  There is a strong likelihood that it is being pulled in today as there were a number of stories above it that have been delivered.  Stay tuned!\n. @mrdavidlaing FYI - the story is actually slated for the 03 March iteration but was put at the top of the backlog for that iteration so that it could be easily pulled into the current iteration if the opportunity was to arise.  There is a strong likelihood that it is being pulled in today as there were a number of stories above it that have been delivered.  Stay tuned!\n. @jbayer FYI\n. @jbayer FYI\n. @mrdavidlaing   The PR hit master today.  I was able to deploy a dummy app and see BOSH create the spot instance when price < 0.07,  tear it down when price >0.07 and them create a new instance when price fell again.  Good work.  @jbayer and I were talking about asking you to write a blog post about this feature (since you authored it).  Great work!\n. @mrdavidlaing   The PR hit master today.  I was able to deploy a dummy app and see BOSH create the spot instance when price < 0.07,  tear it down when price >0.07 and them create a new instance when price fell again.  Good work.  @jbayer and I were talking about asking you to write a blog post about this feature (since you authored it).  Great work!\n. BTW we looked at BOSH/director performance during the spot instance cycling and did not see any significant or alarming degradation.\n. BTW we looked at BOSH/director performance during the spot instance cycling and did not see any significant or alarming degradation.\n. @mrdavidlaing Did you get time to draft a blog post about spot instances?  I'm compiling info for a \"release notes\" type of document and would like to include an announcement about your work. Cheers/\n. @mrdavidlaing Did you get time to draft a blog post about spot instances?  I'm compiling info for a \"release notes\" type of document and would like to include an announcement about your work. Cheers/\n. @matjohn2 - We'll go ahead and close the issue.  Please resubmit if you do want to pursue the recommendation that @mmb and @dsabeti suggested.  Thx\n. @matjohn2 - We'll go ahead and close the issue.  Please resubmit if you do want to pursue the recommendation that @mmb and @dsabeti suggested.  Thx\n. @gberche-orange Using this implementation seems to require an environment variable to be set on the director VM. Can you provide details about how that would work? Thx\n. @gberche-orange Using this implementation seems to require an environment variable to be set on the director VM. Can you provide details about how that would work? Thx\n. @tlawrence Is this still an issue or have you been able to work it out?  Thanks\n. @tlawrence Is this still an issue or have you been able to work it out?  Thanks\n. OK.  Will do.  Thanks\n. OK.  Will do.  Thanks\n. @khwang1 @chou I pulled it into my backlog.  thanks.\n. @khwang1 @chou I pulled it into my backlog.  thanks.\n. bosh team will respond\n. bosh team will respond\n. bosh team will respond\n. bosh team will respond\n. bosh team will respond\n. bosh team will respond\n. This is covered by CLA. I asked author to close PR #554  and reopen against develop rather than master. This is the reopen\n. This is covered by CLA. I asked author to close PR #554  and reopen against develop rather than master. This is the reopen\n. This is covered by CLA.  I asked author to close PR #550 and reopen against develop rather than master.  This is the reopen\n. This is covered by CLA.  I asked author to close PR #550 and reopen against develop rather than master.  This is the reopen\n. This PR is a reissue of #552.  Submitter is under CLA.  I asked for a resubmission against develop instead of master (#552 was issued against master)\n. This PR is a reissue of #552.  Submitter is under CLA.  I asked for a resubmission against develop instead of master (#552 was issued against master)\n. @cppforlife and @monkeyherder to review this PR\n. @cppforlife and @monkeyherder to review this PR\n. @singerdmx Why was this closed?  Last week, we had worked through the vdiskmanager licensing issue.  Thanks\n. @singerdmx Why was this closed?  Last week, we had worked through the vdiskmanager licensing issue.  Thanks\n. @monkeyherder is looking at this and will comment\n. @monkeyherder is looking at this and will comment\n. @akranga BOSH team was able to discuss this PR.  We're going to have a pair from the team take a deep dive look at the code.  Just wanted you to know we were working on this.  Thanks\n. @akranga BOSH team was able to discuss this PR.  We're going to have a pair from the team take a deep dive look at the code.  Just wanted you to know we were working on this.  Thanks\n. @stupakov @mbhave When ready, please go ahead and put the issue in my icebox.  We'll get this rolling.\n. @stupakov @mbhave When ready, please go ahead and put the issue in my icebox.  We'll get this rolling.\n. @mrdavidlaing I dont' have a clear path to acceptance for this.  I'm going to close the BOSH tracker story & depend on you to inform me if the implementation is not acceptable.  Thanks\nGreg\n. Yes please.\nOn Wednesday, May 14, 2014, stupakov notifications@github.com wrote:\n\n@goehmen https://github.com/goehmen @monkeyherderhttps://github.com/monkeyherderShould we move this to your icebox?\nThanks,\nCF Community Pair (@stupakov https://github.com/stupakov, @mbhavehttps://github.com/mbhave\n)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/580#issuecomment-43137467\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. @mrdavidlaing It's pretty high in the backlog & should get worked on soon.  I moved #594 up to accompany #585 \n. @jhellan Can you provide this info:\nStemcell version:\nIAAS: \nThanks\nGreg\n. Yes. Thanks.\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, Jun 24, 2014 at 9:27 AM, Ted Young notifications@github.com wrote:\n\nThis looks legitimate, assuming no backwards compatibility issues.\n@goehmen https://github.com/goehmen, is this something the bosh team is\ninterested in?\n@tedsuo https://github.com/tedsuo & @leoRoss\nhttps://github.com/leoRoss\nCommunity Pair\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/595#issuecomment-46994625.\n. @jerenkrantz The community pair was over talking to the BOSH team as Ferdy was replying.  What we came up with is this:  I'm going to reject the PR but keep the associated tracker story open and use that to improve retry logic as Ferdy suggested.\n. Closing after this thread: https://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-users/qO2kOvCvZBw\n. \n",
    "mariash": "Looks good, could you please rebase it?  Looks like some files have moved around since you opened this request.\nThanks\n@mariash & @monkeyherder \n. Code already in master\n. Merged manually https://github.com/cloudfoundry/bosh/commit/a57a8bd34e537f3655abc8f3db1789bf439cf7d2, closing.\n. Could you please add tests?\nThanks,\n@monkeyherder & @mariash\n. merged https://github.com/cloudfoundry/bosh/commit/63374213bd3fd3fb0f167ce9d9695618681af9e9, we were waiting for our (long) jenkins builds to go green\n. Merged in 92486c5d95c220e76522ea61ef12233c15b167fa\n. Merged in 75a60bef8468b2c6d2a8ad2c79f5562653e93bc7.\n. Merged in 1239842ddadc701a689efb157280663337229bb1.\n. Hi @gberche-orange, did you try changing environment variables on a director (in director_ctl)? Changing it locally won't have any effect since download_remote_file is being called on a director itself. Net:HTTP should use HTTP_PROXY if it is set, so there is no need to read it explicitly. The best approach would be to add director configuration for http_proxy that will be used in director_ctl.\n. merged in f112f8806e95cdfe7a166681e97b9042457b622f\n. PR was merged: https://github.com/cloudfoundry/bosh/commit/2bc6c56dd27db0f79ac7f50a7835522f699ec229 Probably it was rebased that's why it was not closed automatically.\n. @omarreiss Hi, that looks good. Could you rebase and squash your commits into one? Thanks.\n. hi @singerdmx,\nHave found the following error when running lifecycle_spec:\n1) VSphereCloud::Cloud lifecycle without existing disks should exercise the vm lifecycle\n     Failure/Error: @vm_id = @cpi.create_vm(\n     RuntimeError:\n       Unable to find vmware-vdiskmanager in either /vdiskmanager/bin or system PATH\n     # ./lib/cloud/vsphere/agent_env.rb:111:in `find_bin'\n     # ./lib/cloud/vsphere/agent_env.rb:183:in `convert_vmdk_to_esx_type'\n     # ./lib/cloud/vsphere/agent_env.rb:203:in `set_vmdk_content'\n     # ./lib/cloud/vsphere/agent_env.rb:21:in `set_env'\n     # ./lib/cloud/vsphere/vm_creator.rb:93:in `create'\n     # ./lib/cloud/vsphere/cloud.rb:183:in `block in create_vm'\n     # /Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_formatter.rb:46:in `with_thread_name'\n     # ./lib/cloud/vsphere/cloud.rb:174:in `create_vm'\n     # ./spec/integration/lifecycle_spec.rb:74:in `vm_lifecycle'\n     # ./spec/integration/lifecycle_spec.rb:148:in `block (4 levels) in <top (required)>'\nDo the tests assume that user have vmware-vdiskmanager installed locally? We are running these tests on MacOS and Linux environments.\n@phanle & @mariash\n. @singerdmx, thanks for explanation. That means that running micro deploy to vsphere would require users to install qemu-img and vmware-vdiskmanager or it can be done as part of micro deploy. \nAlso these packages need to be included in bosh-release the same way we do with genisoimage.\n. @jhellan do you any idea why logical and physical sizes are different? On our environment it is the same for trusty 512.\nAlso we haven't seen the mkswap warnings in our logs.\n@phanle & @mariash\n. @jhellan we don't think this warning would be a serious issue though. As we can't reproduce it and we see this issue as not actionable we decided to close this issue. If you have ideas on how to improve this feel free to reopen this issue. \n@phanle & @mariash\n. @jhellan interesting. Yeah that would make sense. Thanks!\n. Hi @mrdavidlaing, we pulled in #585 and verified that it works by running your lifecycle test. We updated unit tests to emulate responses we were getting from AWS. We decided to not pull in #593 as the expanded unit tests are good enough to cover different AWS responses.\n@krishicks & @mariash\n. @DanLavine \nWe reviewed the code and it looks pretty good. The messaging needs some minor fixes and the constructor could use a refactor. If you want to handle the refactor we'll pull it in and fix the messaging.\nMaria & Karl\nBOSH Team\n. We merged PR into develop branch: https://github.com/cloudfoundry/bosh/commit/d973995e606e7c5f540deb49ceac33532703bb5b\nClosing this issue.\nThanks for contribution,\n@mariash & @maximilien\n. Hi, thanks for pull request. It looks good. Please address the comments, rebase against develop, squash all commits into one and force-push to your branch.\nThanks,\n@maximilien & @mariash\n. After building stemcell we are getting the following error:\n```\nvCloud Stemcell\n  installed by system_parameters\n    File \"/var/vcap/bosh/etc/infrastructure\"\n/dev/loop0\nadd map loop0p1 (252:2): 0 5997984 linear /dev/loop0 63\ndel devmap : loop0p1\n      should contain \"vcloud\" (FAILED - 1)\nPending:\n  Ubuntu 14.04 stemcell installed by image_install_grub File \"/boot/grub/menu.lst\"\n    # until aws/openstack stop clobbering the symlink with \"update-grub\"\n    # ./spec/stemcells/ubuntu_trusty_spec.rb:19\nFailures:\n1) vCloud Stemcell installed by system_parameters File \"/var/vcap/bosh/etc/infrastructure\" should contain \"vcloud\"\n     Failure/Error: it { should contain('vcloud') }\n       expected File \"/var/vcap/bosh/etc/infrastructure\" to contain \"vcloud\"\n     # ./spec/stemcells/vcloud_spec.rb:6:in block (4 levels) in <top (required)>'\n     # ./spec/support/stemcell_image.rb:8:inblock (3 levels) in '\n     # ./lib/bosh/stemcell/disk_image.rb:35:in while_mounted'\n     # ./spec/support/stemcell_image.rb:6:inblock (2 levels) in '\nFinished in 11.56 seconds (files took 0.78337 seconds to load)\n38 examples, 1 failure, 1 pending\nFailed examples:\nrspec ./spec/stemcells/vcloud_spec.rb:6 # vCloud Stemcell installed by system_parameters File \"/var/vcap/bosh/etc/infrastructure\" should contain \"vcloud\"\n```\nPlease fix the test above.\nThanks,\n@maximilien & @mariash\n. We merged this into develop with the tests fix. https://github.com/cloudfoundry/bosh/commit/aae89378483a1e7aa3d02dcfb6b758f6deabc00d\n. Please see the spec file: release_tarball_spec.rb\nAs part of tests you can create a directory test: with colon at the end and run unpack with the tarball_path that will be located in that directory. The idea is that test should fail without --force-local option.\nThanks,\n@maximilien & @mariash\n. Hi @rkoster,\nThanks for reporting this issue. What Openstack infrastructure are you using?\n. @rkoster we were testing the config drive implementation on infrastructures that are configured to use config drive as cdrom device. We were able to reproduce this on environment that is attaching config drive as disk. We are going to work on fix soon. see story: https://www.pivotaltracker.com/story/show/79887754\n@mariash & @zaksoup \n. We merged this PR with support of heartbeats only since we don't use graphite internally and we do not test it. If community is interested in alerts support for graphite please let us know or use this as opportunity to contribute to bosh.\nThanks for PR,\n@zhang-hua & @mariash\n. this was merged to develop\n. Hi @cyrille-leclerc, thanks for PR. We squashed 2 commits and merged them into develop branch: https://github.com/cloudfoundry/bosh/commit/fb822dc87709b11acfc0cd54330b58cff5bafb6b\n. Hey @pmenglund, good to see a PR from you! :)\nMerged into develop https://github.com/cloudfoundry/bosh/commit/265be55e3d42c56f83e46b573c51ff7a53f631e3.\nThanks!\n. @xingzhou we merged PR with small refactor here: https://github.com/cloudfoundry/bosh/commit/e0e1e05e780021affbcc115c73a6bfdcaa65455d \nPlease note, when doing PRs they need to be done against develop branch, not master.\nThanks.\n. merged\n. merged into develop\n. merged with rebase\n. @guoger could you look at unit test failures on travis, they seem to be related. \nCould you please also add a functional tests in blobstore_client/spec/functional/?\n. @xingzhou should be fixed now, sorry about that :)\n. @zhang-hua could you provide an integration test for CLI output? \n. merged here: https://github.com/cloudfoundry/bosh/commit/43b2ab5848ef0cf614604fee1a49363205559e5b Changed :all to '*' to be consistent with other commands.\n. Closing PR, since we have unit coverage for this.\n. Hey Danny, this is an old style diff. Are you using old cli? The new style does not differentiate version types.\nMaria\n. Hi @mbhave, sounds like pre-start script is what you want https://bosh.io/docs/pre-start.html\n. you can run it with --no-color and bosh will add +/-\n. @XenoPhex do you guys still see this issue?. We are trying to provide default values in a job spec file to have a consistent way of providing the defaults in release.\n. @mrdavidlaing thanks for pull request! It looks really good! Would be nice to explicitly state the intention at the end of the test. For example, wrap the call in expectation that no errors were raised. \nexpect {\n  instance_manager.wait_for_spot_instance_request_to_be_active(spot_instance_requests)\n}.to_not raise_error\n. This method is really hard to follow, due to it's length and complexity. It would probably be better to abstract out a aws response manager class or maybe just refactor out some methods with self explanatory method names. This might make it easier to test too.\n. Thanks for finding the cause of this problem. The main issue with the PR is that it is missing tests. We should test that normalize_url method is idempotent. \noriginal_uri = 'https://localhost'\nfirst_url = base_command.normalize_url(original_uri)\nexpect(first_url).to eq('https://localhost:25555')\nsecond_url = base_command.normalize_url(first_url)\nexpect(second_url).to eq(first_url)\nOther suggestions would be:\n1. We should strip trailing slash at the beginning since had_port is looking for the port at the end of URL without slash.\n2. We can be consistent with the port matching and just set the default port if it is missing instead of introducing uri_to_string method which is not easy to follow. We can still use URI.parse for validation.\n```\ndef normalize_url(url)\n  URI.parse(url)\nurl.gsub!(/\\/$/, '')\n  url = \"https://#{url}\" unless url =~ /^http:?/\nunless url.to_s =~ /:\\d+$/\n    url += \":#{DEFAULT_DIRECTOR_PORT}\"\n  end\nurl\nend\n```\n. This method is a little too long, especially for a constructor. Is there an obvious way you can abstract this or extract some methods to make it more readable?\n. Please fix indentation\n. Maybe it makes sense to check the number of dynamic networks outside of loop before doing computation.\n. Net:HTTP should use http_proxy environment variable by default. So it seems like we don't need to explicitly pass it from environment. http://ruby-doc.org/stdlib-2.1.2/libdoc/net/http/rdoc/Net/HTTP.html#class-Net::HTTP-label-Proxies\n. please remove extra lines\n. director, scheduler and workers\n. List of comma-separated hosts which are proxy-free.\n. we can probably define HelpersTester above and use in both tests\n. This comment can be updated: centos > rhel\n. @xingzhou could you please put comments why we configure password for vcap user?\n. @xingzhou and also here:\n1) why we need to check if grub-md5-crypt is present, can we just call it?\n2) lets put password line when we writing template above (line  150,162)\n3) always chmod/chown grub.conf\n. @xingzhou we should update agent settings anyway, lets move the check below it.\n. We should update agent settings anyway at the end. Something like: \nif volume.nil?\n  @logger.info(\"Disk `#{disk_id}' not found while trying to detach it from vm `#{server_id}'...\")\nelse\n   detach_volume(server, volume)\nend\n. Lets instead update settings after detach to be consistent with openstack.\n```\nif volume.exists?\n  detach_ebs_volume(instance, volume)\nelse\n  @logger...\nend\nupdate_agent_settings...\n```\n. why escaping was removed? that seems like it might break this script\n. based on the comment above, it might break Ubuntu Trusty stemcells\n. Seems like we are downloading blob for sha verification anyway, so what is the point of checking this (instead of just uploading every package)?\n@loewenstein & @mariash\n. unneeded or\n. does not validate if release already uploaded\n. why was this removed?\n. not defaulting to static anymore?\n. ",
    "yuanyangen": "thanks for your reply\ni try to ssh into this machine but i got a authentication error:\nubuntu@ubuntu:~$ ssh -i microbosh.pem ubuntu@50.50.10.9\nPermission denied (publickey).\nubuntu@ubuntu:~$ \non the other hand,i got the information of that instance,it only has a memory with 1M:\n\u540d\u79f0\nvm-2ca3626c-ee71-4030-85c1-a47ba908cced\nID\nddea466b-e39f-4bab-abd6-d0870d929f21\n\u72b6\u6001\nActive\nSpecs\n\u7c7b\u578b\ntiny\nRAM\n1MB\nVCPUs\n1 VCPU\nDisk\n1GB\nIP Addresses\nCloudfoundry\n50.50.10.9\n\u5b89\u5168\u7ec4\ndefault\n\u5141\u8bb8 -1:-1 \u6765\u81ea 0.0.0.0/0\n\u5141\u8bb8 22:22 \u6765\u81ea 0.0.0.0/0\n\u5141\u8bb8 1:65534 \u6765\u81ea 0.0.0.0/0\nmicrobosh\n\u5141\u8bb8 1:65535 \u6765\u81ea microbosh\n\u5141\u8bb8 1:65535 \u6765\u81ea 0.0.0.0/0\n\u5141\u8bb8 22:22 \u6765\u81ea 0.0.0.0/0\nMeta\nKey Name\nmicrobosh\n\u955c\u50cf\u540d\u79f0\nBOSH-9ee77c1f-cf49-4131-9960-76d7ef103ca3\nName\nmicrobosh-openstack\nVolumes Attached\nAttached To\nvolume-069693a3-6c2f-4b76-b06a-13e969e44ad0 on /dev/vdc\nhow could this happens?\nRegards\nyuanyangen\n. hi all:\nI success log into the VM, but I didn't find  /var/vcap/sys/log/director/director.log:\nroot@bm-eb559845-c2a0-4dde-9ce2-fc3ff83512b4:/var/vcap/sys/log/director# ls\naccess.log           error.log           nginx.stdout.log      worker_1.debug.log   worker_2.stderr.log  worker_3.stdout.log\ndirector.debug.log   migrate.stderr.log  scheduler.debug.log   worker_1.stderr.log  worker_2.stdout.log\ndirector.stderr.log  migrate.stdout.log  scheduler.stderr.log  worker_1.stdout.log  worker_3.debug.log\ndirector.stdout.log  nginx.stderr.log    scheduler.stdout.log  worker_2.debug.log   worker_3.stderr.log\nthe  information in /var/vcap/bosh/log/current are in \nhttps://gist.github.com/yuanyangen/5813366\nthey are too many ,i can't paste them here, seems that something wrong with meta data in openstack\nwhat is it?\nregards\nyuanyangen\n. @frodenas \n@TieWei \nthank you for your reply, I try to do this:\nroot@ubuntu:~/bosh-workspace/deployments# bosh target\nTarget not set\nIt seems that setting the target is part of the deploy of micro bosh:\nthe belows are from the correct deploy:\nWARNING! Your target has been changed to https://<microbosh_ip_address>:25555'!\nDeployment set to '~/bosh-workspace/deployments/microbosh-openstack/micro_bosh.yml'\nDeployedmicrobosh-openstack/micro_bosh.yml' to `https://:25555', took 00:04:19 to complete\ni think i need to set the target manually and deploy the micro_bosh.yml manually,\nhow can I implement this?\nBests regards\nyuanyangen \n. Hi all:\nCause I'm going out for some mission, I will continue my deployment  after it\uff0cIt will take me about four days.\nThank you for your reply\nyuanyangen\n. ",
    "coveralls": "\nChanges Unknown when pulling 5f61279dbaa0c16bd5446ebfdbf74859b72fddc9 on coveralls into * on master*.\n. \nChanges Unknown when pulling 9cb5a3c13e06b684ae5da788a10627f0d89ab6f1 on drnic:ntp-defaults into * on cloudfoundry:develop*.\n. \n\nChanges Unknown when pulling 208c974efc7312ff108407f9d0c6fa5fcc9b8d8c on zhang-hua:stig-V-38586 into  on cloudfoundry:develop.\n. ",
    "grenzr": "Just wanted to chime in and say I have this exact issue too during vsphere deployment tests of full BOSH. \nI cheated and blew away the deployment and redeployed it all - no problem.\n. Hi Ruben, changing those properties did seem to work around the problem for me, until a proper fix is put in place\n. I still believe this issue is a BOSH bug, as the exception is not yet properly caught and causes the bosh-agent to crash on the compilation vm. I had similar problem to @rkoster where the storage backend in our openstack environment was not performant enough, but I think for completeness at very least better handling of the exception should be in place? \n. I still believe this issue is a BOSH bug, as the exception is not yet properly caught and causes the bosh-agent to crash on the compilation vm. I had similar problem to @rkoster where the storage backend in our openstack environment was not performant enough, but I think for completeness at very least better handling of the exception should be in place? \n. Hi Mark - cool thanks. How would I go about getting access to the tracker? I've signed a CLA if that helps? \nThanks, Ryan\nOn 30 Sep 2013, at 22:43, Mark Rushakoff notifications@github.com wrote:\n\nOkay, I've added a story to the BOSH icebox to properly handle exceptions in the compilation VM, and I've asked that they updated this issue when they fix the bug. (It's #57952778 for those with access to that Tracker.)\n\u2014\nReply to this email directly or view it on GitHub.\n. Hi Mark - cool thanks. How would I go about getting access to the tracker? I've signed a CLA if that helps? \nThanks, Ryan\n\nOn 30 Sep 2013, at 22:43, Mark Rushakoff notifications@github.com wrote:\n\nOkay, I've added a story to the BOSH icebox to properly handle exceptions in the compilation VM, and I've asked that they updated this issue when they fix the bug. (It's #57952778 for those with access to that Tracker.)\n\u2014\nReply to this email directly or view it on GitHub.\n. wipes the dust off this ticket\nHey guys - I just found this as I'm trying to fix the same thing myself, and thought the default dev release name should at least be the same as final name. or specified by environment variable or something so that bosh -n create release uses a decent release name.\n\nWas there a reason this didnt get merged?\n. wipes the dust off this ticket\nHey guys - I just found this as I'm trying to fix the same thing myself, and thought the default dev release name should at least be the same as final name. or specified by environment variable or something so that bosh -n create release uses a decent release name.\nWas there a reason this didnt get merged?\n. Agreed - I'm happy with the commit as is. Would appreciate if it could get merged asap :)\n. Agreed - I'm happy with the commit as is. Would appreciate if it could get merged asap :)\n. Sooo.... any chance we can get this merged please?\n. Sooo.... any chance we can get this merged please?\n. Thanks Greg :)\n. Thanks Greg :)\n. Sigh, no - I did some fiddling with my forked BOSH repo and probably borked the commit. I'll have a look shortly. \n. The commit still exists - I moved it from master branch into blobstore branch (shouldn't've done it in master branch in first place!)\nhttps://github.com/grenzr/bosh/commit/648916484033ee60070225fdbea914263f4c01e5\n. Would it be easier if i made a new PR? I cant see an easy way to bring that commit into this PR now (unless I'm missing something?)\n. Im not sure that did anything? \n. Im not sure that did anything? \n. I'm trying to test this out; you're missing the spec file property openstack.boot_from_volume:\nhttps://github.com/grenzr/bosh/commit/1a04407a4800928c9aee23e7d0b9487a3abc37d1\n. It'd be really nice if we could somehow just push a hash from a BOSH manifest directly into aws_options in https://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb#L34 but blobstore_client doesn't currently facilitate that.\nI am finding I need to use :ssl_verify_peer and :s3_force_path_style to use our corporate S3 compliant object store product, which I've hacked into aws_options to test on a temporary basis. (see http://docs.aws.amazon.com/AWSRubySDK/latest/AWS/Core/Configuration.html)\nI still haven't got blobstore create file working yet, but the point is it'd be nice to not have to specify every configuration option under the sun in a bosh manifest/spec/config template. I guess the same argument was presented when the Swift blobstore development happened in cc_ng? \n. Hi Tammer, thats a good point, but would it really be a major issue? Other parts of BOSH manifests change through time because of changes made to components and their interfaces.\nI can count quite a few times I've had to adjust BOSH manifests when moving through CF releases. Some of those changes are subtle, others are a bit more major, but I've accepted it as necessary because things are moving fast in the development of CF/BOSH. \nIf you chose to rewrite in golang, and change the AWS library then wouldn't it just be a simple case of adjusting the options hash in the BOSH manifest to suit that new interface? \nThese are the specific aws_options I'm using currently:\n-      ssl_verify_peer: false\n-      s3_force_path_style: true\n-      s3_multipart_threshold: 5368709120\nPR #508 does cover some of my connection requirements, but not all - maybe for the interim I'll just add in the ones I need to #508?\n. Hi @ematpl ,\nI still need s3_multipart_threshold and ssl_verify_peer as configurable aws_options for the blobstore client to work with my specific s3 compatible service.\nWould you mind if I rebased, added those options, and support for compiled_package_cache in this PR?\nThanks,\nRyan\n. My PR was originally forked from master, before the revised PR policy of forking from develop branch. Should I create a new PR to correct this?\n. done!\n. Trying this again since the original one appears to have failed: https://github.com/cloudfoundry/bosh/pull/439\n. You might also be interested in the conversation I started here about the same issue: https://github.com/cloudfoundry/bosh/pull/495\nRather than add specific properties like we both started doing, it'd be nicer to allow aws_options to be defined as a complete block in a BOSH manifest, giving complete flexibility to tweak the S3 interface without lots of individual code modifications. \n. Hi David - I dont think we're asking for specific options for specific libraries (see https://github.com/cloudfoundry/bosh/pull/495#issuecomment-33575031). Personally I would prefer, as per Swift blobstore PR in cc_ng, the ability to pass in an arbitrary hash of switches .. not specific ones which should be individually defined in a BOSH spec file.\nThat way a hash of any supported option of any library can be passed through, regardless of language or component used.\nI understand you are dual-supporting go and ruby versions of the BOSH agent currently, however surely allowing arbitrary options through just means the blobstore code setting/using those options, be it ruby or go, has to ensure they are valid (or just ignore the invalid ones)?\nDoes the BOSH team have a view on what they would prefer to implement to resolve this issue? Are you trying to ensure the same blobstore options will work on ruby/go agent versions?\n. Perhaps the class that wraps the downstream AWS library usage could validate the options before passing it down, but I feel that the AWS library itself should be testing the validity of those switches, not the class trying to instantiate with them.\nAlso, the BOSH manifest creator will no doubt be testing to ensure that the switches being passed through are working correctly in conjunction with their S3 service before committing them to a production deployment.\n. Yay.. I'm glad, thanks.\n. FYI - working on this now I've got a spare minute. Currently rebasing against develop as its been a couple of months since it was brought up to date. Please bear with me.\n. Ok its fully rebased against develop now, and I've added the defaults in for s3_multipart_threshold as requested earlier.\n. I did wonder about whether to put that default in - the only reason I didnt was because the code itself does this:\nuse_ssl: @options.fetch(:use_ssl, true),\n        s3_port: @options.fetch(:port, 443),\n        s3_endpoint: @options.fetch(:host, URI.parse(S3BlobstoreClient::ENDPOINT).host),\n        ssl_verify_peer: @options.fetch(:ssl_verify_peer, true),\n        s3_multipart_threshold: @options.fetch(:s3_multipart_threshold, 16_777_216),\nI wasn't totally clear why 2 sets of default provisioning are occuring? I guess one is from a BOSH manifest perspective, and another is from the code in isolation (if used outside of a BOSH manifest).\nI dont suppose its great practice to have defaults being applied at 2 layers, but the BOSH manifest default will take precidence over the codes own one, so its all cool. \nI'll update the PR shortly.\nCheers, Ryan\n. ",
    "emcee21": "I think we need this, for instance to add nagios hooks into our deployment.  Or is there a better way to do that sort of thing?\n. ",
    "soudmaijer": "We are wondering too if this works... any updates?\nbtw I really like dr nics proposal:\njobs:\n- name: api\n  template:\n  - name: sslproxy\n    release: sslproxy\n  - name: gorouter\n    release: cf\n. ",
    "borovsky": "As I know my host company (Altoros) already signed agreement.\n. Hello. Is it possible to merge this pull request? I'm working for Altoros.\n. It was typo\n. Hello Tammer,\nThis pull request can be reduced a bit, but not so much: most of the changes is moving AWS objects initialization logic to separate class (AWSProvider). It required big changes in tests.\n. Commits except first don't change behaviour.\nFor first commit I've checked AWS deploy.\nI haven't run BAT suite. Trying to do this now.\n. Hello.\n1. I've rebased branch\n2. Runned RSpec and BAT tests\nAs mentioned in first comments, my company (Altoros) already signed CLA.\n. ",
    "monkeyherder": "Please also update cf-docs/source/docs/running/deploying-cf/ec2/index.html.md after pull request is merged to reflect new functionality.\n. Please also update cf-docs/source/docs/running/deploying-cf/ec2/index.html.md after pull request is merged to reflect new functionality.\n. Please also sign the appropriate Contributor License Agreement (CLA) (individual or corporate).\nThanks!\n. Please also sign the appropriate Contributor License Agreement (CLA) (individual or corporate).\nThanks!\n. Spoke with @frodenas, and we're going to implement this functionality ourselves while removing support for user-injected files.  @frodenas - Feel free to reopen this if I misunderstood somehow.\n. Spoke with @frodenas, and we're going to implement this functionality ourselves while removing support for user-injected files.  @frodenas - Feel free to reopen this if I misunderstood somehow.\n. It looks like this has been resolved via other work, so I'm going to go ahead and close this PR for now.  Feel free to reopen if I misunderstood.\n. It looks like this has been resolved via other work, so I'm going to go ahead and close this PR for now.  Feel free to reopen if I misunderstood.\n. @tader Merged in, then mistakenly did a pull --rebase before running tests to push.  New commit sha on bosh/develop: 5ac0b78333c37736fbb809e3d7987d51c444e732.\nSorry about the rebase rather than a standard merge.  This should be in master tonight or tomorrow.\n. A couple of things:\ndirector.yml.erb.erb changes should be tested in release/spec/director.yml.erb.erb_spec.rb to confirm that the logic is being properly parsed.\nAlso, it probably isn't necessary to include an entire example if only the blobstore section has changed from other OpenStack examples.  Perhaps only include the section required to change, and a comment at the top of the snippet.\n@monkeyherder & @cppforlife\n. A couple of things:\ndirector.yml.erb.erb changes should be tested in release/spec/director.yml.erb.erb_spec.rb to confirm that the logic is being properly parsed.\nAlso, it probably isn't necessary to include an entire example if only the blobstore section has changed from other OpenStack examples.  Perhaps only include the section required to change, and a comment at the top of the snippet.\n@monkeyherder & @cppforlife\n. Merged into develop as revision e4a9ee7c189787e9351260989a3614d92cdb33be\n. Merged into develop as revision e4a9ee7c189787e9351260989a3614d92cdb33be\n. Reverted the anonymous merge, and re-merged the request.\n. Reverted the anonymous merge, and re-merged the request.\n. Stand by, looks like the local mirror stemcell building VM is missing a packages that are now required.  Reverted commit - will look into what packages are required and update/add them to the mirror.\n. Stand by, looks like the local mirror stemcell building VM is missing a packages that are now required.  Reverted commit - will look into what packages are required and update/add them to the mirror.\n. This is NOT actually in the code base at the moment.  It was merged, but then the changes were reverted due to a failure in stemcell creation.  The changes will have to be manually merged.\n. This is NOT actually in the code base at the moment.  It was merged, but then the changes were reverted due to a failure in stemcell creation.  The changes will have to be manually merged.\n. Is this causing failures when running BATs for BOSH lite?\n. Is this causing failures when running BATs for BOSH lite?\n. A couple of things we noticed:\n- Looks like the changes to release/jobs/director/templates/director.yml.erb.erb are not tested.  We have a spec at release/spec/director.yml.erb.erb_spec.rb .  Could you add a spec to test the logic in your change?\n- Looks like bosh-registry/spec/unit/bosh/registry/openstack/instance_manager_spec.rb has some leftover instance variable references (e.g. @compute) that becomes nil after your change. Did you mean to replace those with the new let called compute?\n- (very minor) Although we are single-quoters, in files that consistently have double quotes, we might opt for keeping consistency by doing double quotes.\n/@monkeyherder + @d\n. @drnic: @thansmann is on the community pair this week, so I'm taking a look at this with @ytolsk today.  If the problem is that you need to ensure that state.vm_cid has a value, does that include an empty string case?  If so, it seems that the change you've proposed won't actually catch that case properly, as an empty string would be considered true. \nAlso, it appears that your tests pass with the old code as well as the new code, so they don't seem to actually be testing the change.  Do you have a spec that shows the problem you are trying to fix here?\n. @drnic: @thansmann is on the community pair this week, so I'm taking a look at this with @ytolsk today.  If the problem is that you need to ensure that state.vm_cid has a value, does that include an empty string case?  If so, it seems that the change you've proposed won't actually catch that case properly, as an empty string would be considered true. \nAlso, it appears that your tests pass with the old code as well as the new code, so they don't seem to actually be testing the change.  Do you have a spec that shows the problem you are trying to fix here?\n. @rkoster Looks like the community pair rebased this commit when they were merging it in.  It's now in master as commit f36cae7402d90ef5ec70adcd612ea08027d19569\n. @rkoster Looks like the community pair rebased this commit when they were merging it in.  It's now in master as commit f36cae7402d90ef5ec70adcd612ea08027d19569\n. merged into develop. waiting for it to go through CI. \n- @cppforlife & @mariash \n. merged into develop. waiting for it to go through CI. \n- @cppforlife & @mariash \n. @karlkfi & @khwang1 - Go ahead and throw this into the BOSH project.  I'll chat with @goehmen about prioritization on Monday.\n. @karlkfi & @khwang1 - Go ahead and throw this into the BOSH project.  I'll chat with @goehmen about prioritization on Monday.\n. http://bosh-jenkins-artifacts.s3.amazonaws.com/micro-bosh-stemcell/vsphere/latest-micro-bosh-stemcell-vsphere.tgz is definitely an old stemcell.  We stopped creating separate stemcells for microbosh deployment last summer.  To give you an idea of how old, that stemcell is version 912, and the most recent stemcell is version 2347.\nTo get more up-to-date stemcells, you can head to http://bosh-artifacts.cfapps.io/, or run bosh public stemcells --full to get download URLs for the most recent stable stemcells.\nI'll also chat with @goehmen about looking into how and when we broke backwards compatibility.\n. http://bosh-jenkins-artifacts.s3.amazonaws.com/micro-bosh-stemcell/vsphere/latest-micro-bosh-stemcell-vsphere.tgz is definitely an old stemcell.  We stopped creating separate stemcells for microbosh deployment last summer.  To give you an idea of how old, that stemcell is version 912, and the most recent stemcell is version 2347.\nTo get more up-to-date stemcells, you can head to http://bosh-artifacts.cfapps.io/, or run bosh public stemcells --full to get download URLs for the most recent stable stemcells.\nI'll also chat with @goehmen about looking into how and when we broke backwards compatibility.\n. @goehmen - I'd like this to come into BOSH.  I'd like to have a discussion with the team to determine how we want to proceed with this.\n. @goehmen - I'd like this to come into BOSH.  I'd like to have a discussion with the team to determine how we want to proceed with this.\n. @bsiravara Is the new pull request going to have agent changes in the ruby agent, or the go agent?  We are currently moving towards having the go agent be the standard supported agent.\n. @bsiravara Is the new pull request going to have agent changes in the ruby agent, or the go agent?  We are currently moving towards having the go agent be the standard supported agent.\n. @bsiravara Just know that we have stopped adding functionality to the ruby agent, so it will be pretty much feature frozen (other than your pull request) going forward.\nOn the go agent, what kind of help would you be looking for?  Would you want to come in and pair with folks here, or just regular communication back and forth?\ncc @adamstegman, who is taking over as BOSH anchor\n. @bsiravara Just know that we have stopped adding functionality to the ruby agent, so it will be pretty much feature frozen (other than your pull request) going forward.\nOn the go agent, what kind of help would you be looking for?  Would you want to come in and pair with folks here, or just regular communication back and forth?\ncc @adamstegman, who is taking over as BOSH anchor\n. Cherry-picked, and resolved conflicts in Gemfile.lock with commit 6f9627e4d865a4f5d16c521da3e2d63dca07188a.\n. Cherry-picked, and resolved conflicts in Gemfile.lock with commit 6f9627e4d865a4f5d16c521da3e2d63dca07188a.\n. Great @AbelHu, thanks for making your address public. It should help @cfdreddbot realize that you've signed a CLA. Did you also make your membership in the appropriate Microsoft Github Orgs public?\nAlso, I don't think that the spec change cango into bosh-stemcell/spec/stemcells/ubuntu_trusty_spec.rb, as this change will only happen for Azure stemcells. So, if the additional test was there, we would end up failing the tests for stemcells on other infrastructures. It would probably be better to have the tests in bosh-stemcell/spec/stemcells/azure_spec.rb, which is infrastructure specific, and will only run when we're building Azure stemcells.\nBut if you can get the tests moved over to bosh-stemcell/spec/stemcells/azure_spec.rb, we should be able to pull this change in.\nThanks!\n. Great @AbelHu, thanks for making your address public. It should help @cfdreddbot realize that you've signed a CLA. Did you also make your membership in the appropriate Microsoft Github Orgs public?\nAlso, I don't think that the spec change cango into bosh-stemcell/spec/stemcells/ubuntu_trusty_spec.rb, as this change will only happen for Azure stemcells. So, if the additional test was there, we would end up failing the tests for stemcells on other infrastructures. It would probably be better to have the tests in bosh-stemcell/spec/stemcells/azure_spec.rb, which is infrastructure specific, and will only run when we're building Azure stemcells.\nBut if you can get the tests moved over to bosh-stemcell/spec/stemcells/azure_spec.rb, we should be able to pull this change in.\nThanks!\n. ",
    "narendrachoudhary": "Could you please let me know how to do that \n. ",
    "bosh-ci-push-pull": "...Screwed either way right now.  Obviously, bosh user mgt is in need of\nmaturation.\nMy preference is to leave it as it is now and fix it once - the right way -\nwhen we can rather than spending cycles on it now and then cycles again in\nthe future.  That is, of course, unless compelling reasons arise for\nbumping bosh user mgt up in prioritization.\nGreg\nOn Mon, Apr 21, 2014 at 4:06 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nIf a bosh/microbosh config says that it should use a specific hm user then\nit might as well create that user if it's missing.\nHaving said that, this would immediately disable the admin/admin default.\nSo perhaps it's screwed either way.\nGreg, what is you're preference for onboarding?\nOn Mon, Apr 21, 2014 at 4:02 PM, greg oehmen notifications@github.com\nwrote:\n\nThere are a few stories related to BOSH user management in the\nbacklog/icebox. Mainly -\nhttps://www.pivotaltracker.com/story/show/68212726- Bosh User Admin:\nRole Development. This fits in that theme for sure.\nNic - is what you are saying \"Precreate the HM user because creating it\nexactly when you need it is too painful.\"?\nOn Mon, Apr 21, 2014 at 3:53 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nI think it's still a good idea - create the HM user when it is\nconfigured\n- otherwise it is complex to setup a non-admin user as the HM stops\n  working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley notifications@github.com\nwrote:\n\n@drnic\nIs this still an issue? If not, we would like to close this since it\nlooks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986220>\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986896\n\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40987161\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Noted.  Thanks.  Want to write it?\nOn Mon, Apr 21, 2014 at 10:12 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nMy primary concern is that the moment a new bosh user is created say\ndrnic/password, then now the health monitor can't function (I believe it\nassumes admin/admin is its credentials). And I don't think any notice is\ngiven.\nPerhaps some documentation on health manager credentials might do the\ntrick in the meantime.\nOn Mon, Apr 21, 2014 at 10:08 PM, rboshman notifications@github.com\nwrote:\n\n...Screwed either way right now. Obviously, bosh user mgt is in need of\nmaturation.\nMy preference is to leave it as it is now and fix it once - the right\nway -\nwhen we can rather than spending cycles on it now and then cycles again\nin\nthe future. That is, of course, unless compelling reasons arise for\nbumping bosh user mgt up in prioritization.\nGreg\nOn Mon, Apr 21, 2014 at 4:06 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nIf a bosh/microbosh config says that it should use a specific hm user\nthen\nit might as well create that user if it's missing.\nHaving said that, this would immediately disable the admin/admin\ndefault.\nSo perhaps it's screwed either way.\nGreg, what is you're preference for onboarding?\nOn Mon, Apr 21, 2014 at 4:02 PM, greg oehmen notifications@github.com\nwrote:\n\nThere are a few stories related to BOSH user management in the\nbacklog/icebox. Mainly -\nhttps://www.pivotaltracker.com/story/show/68212726- Bosh User Admin:\nRole Development. This fits in that theme for sure.\nNic - is what you are saying \"Precreate the HM user because creating\nit\nexactly when you need it is too painful.\"?\nOn Mon, Apr 21, 2014 at 3:53 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nI think it's still a good idea - create the HM user when it is\nconfigured\n- otherwise it is complex to setup a non-admin user as the HM stops\n  working.\nOn Mon, Apr 21, 2014 at 3:48 PM, John Foley \nnotifications@github.com\nwrote:\n\n@drnic\nIs this still an issue? If not, we would like to close this since\nit\nlooks pretty stale.\nCF Community Pair (@jfoley & @jtuchscherer)\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40985924\n\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986220>\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40986896\n\n\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-40987161>\n.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-41004402\n\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/338#issuecomment-41004514\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. David:\nDo you have a PivotalTracker account?  If so, are you logged in?  You\nshould be able to access the story without an account:\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950\nG\nOn Fri, Feb 21, 2014 at 10:35 AM, David Laing notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen - BOSH Pivotal Tracker >\ncloudfoundry/bosh #509: Add aws spot supporthttps://www.pivotaltracker.com/s/projects/956238/stories/64444950says:\nwaiting for story #65132950 to complete before executing PR\nI don't have permission to see the status of #65132950 or what its about.\nLet me know if you need any further info from me\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35759165\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. Oh, I see what you are referring to...I don't have access to that other\nstory (65132950) either.  Letme figure it out.\nG\nOn Fri, Feb 21, 2014 at 3:47 PM, Greg Oehmen goehmen@pivotallabs.comwrote:\n\nDavid:\nDo you have a PivotalTracker account?  If so, are you logged in?  You\nshould be able to access the story without an account:\nhttps://www.pivotaltracker.com/s/projects/956238/stories/64444950\nG\nOn Fri, Feb 21, 2014 at 10:35 AM, David Laing notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen - BOSH Pivotal Tracker >\ncloudfoundry/bosh #509: Add aws spot supporthttps://www.pivotaltracker.com/s/projects/956238/stories/64444950says:\nwaiting for story #65132950 to complete before executing PR\nI don't have permission to see the status of #65132950 or what its about.\nLet me know if you need any further info from me\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-35759165\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. We started on this PR this morning.  We are going to release this as\n\"experimental functionality\" and there will be a warning message to this\neffect at deploy time.  David - if you want to have input on the message\ncontent, you are welcome to check it out after we merge.  I didn't want to\npush this back on you yet again so we are crafting the verbiage and moving\nforward.\nAs we see adoption and gain confidence in the feature, we will phase out of\n\"experimental\".  I promise it will happen faster than Google moved gmail\nout of beta.  :-)\n@jbayer \n-Greg\nOn Wed, Feb 26, 2014 at 2:31 AM, David Laing notifications@github.comwrote:\n\n Yay!\nI'm around to answer any questions\nOn 25 February 2014 19:12, goehmen notifications@github.com wrote:\n\n@mrdavidlaing https://github.com/mrdavidlaing FYI - the story is\nactually slated for the 03 March iteration but was put at the top of the\nbacklog for that iteration so that it could be easily pulled into the\ncurrent iteration if the opportunity was to arise. There is a strong\nlikelihood that it is being pulled in today as there were a number of\nstories above it that have been delivered. Stay tuned!\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36045623>\n.\n\n\nDavid Laing\nOpen source @ City Index - github.com/cityindex\nhttp://davidlaing.com\nTwitter: @davidlaing\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36111576\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. Hey Dr Nic - FYI - https://www.pivotaltracker.com/story/show/67369232\nOn Tue, Mar 11, 2014 at 3:32 AM, Dr Nic Williams\nnotifications@github.comwrote:\n\n\"as intended\" isn't corrector helpful  - ruby 2.1.0 is the current ruby\npeople are using on their machines. Only for bosh CLI do I need to\nconstantly switch back to legacy rubies.\nOn Tue, Mar 11, 2014 at 3:41 AM, Jesse Zhang notifications@github.com\nwrote:\n\nThis is working as intended: the BOSH CLI is not designed to run on Ruby\n2.1 (yet). We do have a little future-proofing in our CI that runs the same builds under\nRuby 2.0 but they are currently all broken.\nFWIW I am running the BOSH CLI in Ruby 2.0 (this is unsupported\nbehavior) and my production environment is not blown up (yet).\nBefore we follow Vagrant's precedence to ship our own Ruby, is this\nstill an issue for you, @matjohn2 ? Can we close this?\nJesse\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/511#issuecomment-37270082\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/511#issuecomment-37281660\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Rene - what's the advantage of providing 2 different ways to be pass the\nsame configuration options (either using endpoint OR scheme+host+port\nseparately)?  Isn't having 1 way to specify these options good enough?\nOn Fri, Feb 14, 2014 at 2:15 PM, Eric Malm notifications@github.com wrote:\n\nThanks for the additional info. Could you be more specific about what\ndoesn't work when using the S3 client for these blobstores so we can better\nunderstand the issues?\n-CF Community Pair (@ematpl https://github.com/ematpl & @mmbhttps://github.com/mmb\n)\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/521#issuecomment-35130309\n.\n\n\n-David Stevenson\nDirector of Engineering, CF Services\n. @drnic Good catch.  Thanks!\nOn Sat, Feb 15, 2014 at 8:30 AM, cf-gitbot notifications@github.com wrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view\nthe current status of your issue at:\nhttp://www.pivotaltracker.com/story/show/65847874\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/522#issuecomment-35160300\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n. We are actively working on the PR right now.  We expect the merge to\ncomplete today and be available after the next green CI run after that.\nCheers\nOn Mon, Mar 3, 2014 at 6:25 AM, tlawrence notifications@github.com wrote:\n\nHi, any news on when this will be merged?\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/526#issuecomment-36514094\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Ryan\nThat's great. Thank you!\nGreg\nOn Thursday, May 8, 2014, Ryan Grenz notifications@github.com wrote:\n\nFYI - working on this now I've got a spare minute. Currently rebasing\nagainst develop as its been a couple of months since it was brought up to\ndate. Please bear with me.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/539#issuecomment-42538248\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. sure.  glad to.\nOn Mon, Mar 31, 2014 at 2:47 PM, Karl Isenberg notifications@github.comwrote:\n\nMy understanding is that pre-packaging has internet access because it's\ndone locally, but packaging is not guaranteed to have internet access.\n@goehmen https://github.com/goehmen Do you want to address this?\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/547#issuecomment-39146380\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Yes.  You can put it in the icebox. Thanks\nOn Mon, Mar 31, 2014 at 3:02 PM, Karl Isenberg notifications@github.comwrote:\n\nWe re-ran the CI build and it succeeded. So it looks like it was a flakey\ntest failing the first time.\nThe change looks innocuous, and the spec already covers centos. Should we\nmake similar changes in other places where centos is explicitly called out\nin this file?\n@goehmen https://github.com/goehmen Would you like this in the Bosh\nbacklog?\nCF Community Pair (@karlkfi https://github.com/karlkfi & @khwang1https://github.com/khwang1\n)\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/549#issuecomment-39147936\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. @karlkfi & @shwang1 - Yes, pls drop it in the BOSH backlog\nOn Fri, Apr 4, 2014 at 5:57 PM, Rob Day-Reynolds\nnotifications@github.comwrote:\n\n@goehmen https://github.com/goehmen - I'd like this to come into BOSH.\nI'd like to have a discussion with the team to determine how we want to\nproceed with this.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/552#issuecomment-39624637\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. This PR is covered by CLA.  I asked @molteanu to close PR #553 and reopen\nagainst develop.  This is the reopen & the bosh team will merge this in.\nGreg\nOn Thu, Apr 10, 2014 at 2:07 PM, chou notifications@github.com wrote:\n\nHi @molteanu https://github.com/molteanu,\nWe are investigating this PR.\nWe have noticed that the dreddbot is asking you to file a CLA in each PR\nyou submit. Since your company has signed a CLA with us, we will email you\nwith instructions in order to suppress these CLA requests.\nCF Community Pair (@chou https://github.com/chou & @khwang1https://github.com/khwang1\n)\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/561#issuecomment-40143053\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Hi Doug:\nBecause commands via the CLI are dynamic over time, we have replaced an\nactual listing of commands  in documentation with bosh help --all\nreferences so that documentation does not get stale and/or increasingly\ndifficult to maintain.  We also recently finished a story (\nhttps://www.pivotaltracker.com/n/projects/956238/stories/67136620) to\nconverge these three commands:\nbosh --help\nbosh help\nbosh help --all\nSo that there is a single output that is useful.  After completing that\nstory and reviewing the results, I think that we actually need either an\noption with less verbosity or the ability to page through the output (like\npaging through man pages by hitting space bar and killing out of paging\nwith q).  I've actually created a story for the latter option.\nBut ultimately, there is only value in doing this type of work if it\ntotally serves the customer in a focussed way.  So, what do you think we\nshould do?\nBest,\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Fri, Apr 11, 2014 at 10:14 AM, cf-gitbot notifications@github.comwrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view\nthe current status of your issue at:\nhttp://www.pivotaltracker.com/story/show/69357758. This repo is managed\nby the 'BOSH' team.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/563#issuecomment-40227096\n.\n. OK.  Gotcha.  Perfect.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Wed, May 14, 2014 at 10:07 AM, Bharath Siravara <notifications@github.com\n\nwrote:\n@goehmen https://github.com/goehmen per the discussion last week we\ndecided to go with an alternate solution right? This PR uses the canned\nVMDK file and we want to use the vdiskmanager solution instead. A PR for\nthat change is on its way in the next few days.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/564#issuecomment-43108559\n.\n. OK.  Thanks works.  Looking forward to the PRs & getting this done!\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Wed, May 14, 2014 at 10:48 AM, Bharath Siravara <notifications@github.com\n\nwrote:\n@monkeyherder https://github.com/monkeyherder we'd like to get the ruby\nchanges in and then do a separate PR for the go agent. Also any chance we\ncan get some help from you guys for the go agent changes?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/564#issuecomment-43113838\n.\n. yes.  is there a new story?  i'm not sure which to grab.  Can you move it\nfor me?\nthx\ng\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Fri, May 9, 2014 at 1:43 PM, Adam Stegman \u269b notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen Should we move this into the\nicebox? (See #558 https://github.com/cloudfoundry/bosh/issues/558 for\nmore discussion)\nThanks,\nCF Community Pair (@adamstegman https://github.com/adamstegman & @mbhavehttps://github.com/mbhave\n)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/575#issuecomment-42711628\n.\n. Thanks David. This is in the BOSH tracker backlog now.\n\nBTW - if you want to write that spot instance blog post, let me know.  I\nhave a few other new feature posts I'm going to post soon (errands,\nmulti-networks, colocation)\nthanks\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 13, 2014 at 3:38 AM, David Laing notifications@github.comwrote:\n\nOk, build is fixed; this is ready for review / merging\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/577#issuecomment-42939982\n.\n. OK.  Great.  Let me know if I can help.\nG\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 13, 2014 at 2:33 PM, David Laing notifications@github.comwrote:\n\n@goehmen https://github.com/goehmen, I'm just in the process of getting\nready to write the spot instance blog post; currently finalising a spot\nonly CF deployment -\nhttps://github.com/mrdavidlaing/multi-az-spot-cloudfoundry-bosh-deployment- so that the blog post can be \"this is how you can use spot\", and \"this is\nwhy you should consider using many / all spot instances in your deployments\"\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/577#issuecomment-43016036\n.\n. did that blog get published yet?  I thought it had but don't see it on\nblog.gopivotal\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Wed, May 21, 2014 at 7:59 AM, James Bayer notifications@github.com\nwrote:\n\n@mrdavidlaing https://github.com/mrdavidlaing this looks great! thanks\nfor following up on this. we have recently moved the cloudfoundry.com\nblog content over to blog.gopivotal.com/cloud-foundry-pivotal/ in the\ntime we first gave you and account and when this draft has been finished.\nlet me get you a guest account there and publish it there. i think we can\ndo that as early as today.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/577#issuecomment-43765615.\n. Hi Doug:\n\nThat's perfect (bring up a discussion before pouring effort into a PR).\n The BOSH team and I have actually been giving a lot of thought to\nreleases, dev/final, versioning, etc.  I'd encourage you to first have a\nlook at the stories in this epic:\nhttps://www.pivotaltracker.com/epic/show/1187352 - \"Release Versioning\".\n Then, let's have a conversation about where you see gaps, whether we are\ngoing in the direction that works for you, etc.  We've sought a lot of\ncustomer/downstream input.\nLooking forward to next steps\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 13, 2014 at 5:13 PM, cf-gitbot notifications@github.com wrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view\nthe current status of your issue at:\nhttps://www.pivotaltracker.com/story/show/71283492. This repo is managed\nby the 'BOSH' team.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-43028796\n.\n. That link should be public but let me verify.\n\nOn Tuesday, May 13, 2014, Doug Davis notifications@github.com wrote:\n\nWhile I don't get a 404 for that link, I only see the title, no content.\nDo I need more ACLs?\nMy Pivotal ID is \"dougdavis1\".\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-43030178\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n. Yes.  See this story for details:\nhttps://www.pivotaltracker.com/story/show/62085266\nNote that the story was created Dec 6, 2013.  Best to pick up the\ncommentary starting on May 1, 2014.  There is a lot of detail there.  I'd\nlove to hear from folks whether this story (and the epic that it is part\nof) are solving release development/deployment issues and making releases\nbetter and easier.\nBest\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 27, 2014 at 10:37 AM, maxbrunsfeld notifications@github.comwrote:\n\nHi Doug,\nAh you cannot specify a name (say with a flag) without hacking dev.yml.\n@goehmen https://github.com/goehmen Do we have such a feature planned?\nThanks,\nCF Community Pair\nJesse and Max (@d https://github.com/d, @maxbrunsfeldhttps://github.com/maxbrunsfeld\n)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-44308597\n.\n. I pulled that bug into the backlog.  Thanks Karl.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Thu, Jun 12, 2014 at 6:08 PM, Karl Isenberg notifications@github.com\nwrote:\n\n@duglin https://github.com/duglin The release versioning changes are\nlive. That code should be calmed down enough now for you to work on a PR to\nadd a --name feature to create release.\nThere is an outstanding bug, however, where renaming a release without\nother changes seems to mess up version auto-incrementing, due to how the\nrelease index stores by fingerprint and not by release name. But you'll\nprobably be using --version for your case, with other changes, so it might\nnot matter.\nhttps://www.pivotaltracker.com/story/show/72994716\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-45966195.\n\nYou received this message because you are subscribed to the Google Groups\n\"pivotal-bosh-eng\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to pivotal-bosh-eng+unsubscribe@pivotallabs.com.\nTo post to this group, send email to pivotal-bosh-eng@pivotallabs.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/pivotallabs.com/d/msgid/pivotal-bosh-eng/cloudfoundry/bosh/issues/578/45966195%40github.com\nhttps://groups.google.com/a/pivotallabs.com/d/msgid/pivotal-bosh-eng/cloudfoundry/bosh/issues/578/45966195%40github.com?utm_medium=email&utm_source=footer\n.\n. I moved it to bosh tracker\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Fri, May 23, 2014 at 10:50 AM, stupakov notifications@github.com wrote:\n\n@goehmen https://github.com/goehmen @adamstegmanhttps://github.com/adamstegmanCan we move the corresponding Tracker story to your team's icebox?\nThanks,\nCF Community Pair (@d https://github.com/d, @stupakovhttps://github.com/stupakov\n)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/579#issuecomment-44040754\n.\n. @drnic  -  Actually, there are two large variables here.  You've changed\nboth agent (ruby to go) and OS (lucid/10.04 to trusty/14.04).\n\n\u00b1  |master \u2717| \u2192 diff diff1.yml diff2.yml\n59c59\n<     name: bosh-aws-xen-ubuntu-trusty-go_agent\n\nname: bosh-aws-xen-ubuntu\n\n@CF Community Pair - I'll pull this in to the BOSH Agent backlog and have\nthe team explore which change caused this.\nBest\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 20, 2014 at 5:15 AM, cf-gitbot notifications@github.com wrote:\n\nWe have created an issue in Pivotal Tracker to manage this. You can view\nthe current status of your issue at:\nhttps://www.pivotaltracker.com/story/show/71660302. This repo is managed\nby the 'BOSH' team.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/583#issuecomment-43617626\n.\n\nYou received this message because you are subscribed to the Google Groups\n\"pivotal-bosh-eng\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to pivotal-bosh-eng+unsubscribe@pivotallabs.com.\nTo post to this group, send email to pivotal-bosh-eng@pivotallabs.com.\nTo view this discussion on the web visit\nhttps://groups.google.com/a/pivotallabs.com/d/msgid/pivotal-bosh-eng/cloudfoundry/bosh/issues/583/43617626%40github.comhttps://groups.google.com/a/pivotallabs.com/d/msgid/pivotal-bosh-eng/cloudfoundry/bosh/issues/583/43617626%40github.com?utm_medium=email&utm_source=footer\n.\n. OK.  Thanks for the input.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, May 20, 2014 at 1:01 PM, Dr Nic Williams\nnotifications@github.comwrote:\n\nIf it helps with diagnosis - the error came from the goagent code, and I'm\nusing AWS in a way that Pivotal doesn't use (nor perhaps test for) which is\npure AWS EC2.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/583#issuecomment-43675466\n.\n. done\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Thu, Jun 19, 2014 at 12:42 PM, John Tuley notifications@github.com\nwrote:\n\n@goehmen https://github.com/goehmen,\nCan we move this to your icebox?\nThanks,\n@jmtuley https://github.com/jmtuley and @tedsuo\nhttps://github.com/tedsuo\nCF Community Pair\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/590#issuecomment-46607245.\n. Agreed - Yes please.\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Mon, Jun 30, 2014 at 9:48 AM, Leo Rossignac-Milon \nnotifications@github.com wrote:\n\n@goehmen https://github.com/goehmen this seems like a reasonable\nrequest. Should we bring this into your Tracker?\nThanks @tnaoto https://github.com/tnaoto\nleoRoss & danIavine\nCF Community Pair\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/598#issuecomment-47555679.\n. Yes.  Thanks!\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Tue, Jul 1, 2014 at 3:20 PM, Leo Rossignac-Milon \nnotifications@github.com wrote:\n\n@drnic https://github.com/drnic We reproduced both scenarios (normal\nand URL uploads) and achieved the same results.\n@goehmen https://github.com/goehmen Should we move this issue to the\nBosh Tracker?\nCF Community Pair\n@leoRoss https://github.com/leoRoss & @DanLavine\nhttps://github.com/DanLavine\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/600#issuecomment-47716393.\n. \"An app\u2019s config is everything that is likely to vary between deploys ... Apps sometimes store config as constants in the code. This is a violation of twelve-factor, which requires strict separation of config from code. Config varies substantially across deploys, code does not.\"\n\nIt seems pretty clear to me that the above text from the top of the 12-factor config page clearly states the primary motivation for choosing env vars. \nIt's unfortunate that 12-factor does not even mention command line arguments. The closest argument would be that they don't scale very well as the number increases, but I would argue that providing files to group settings/config is a perfectly valid option that not only satisfies all their stated requirements but also avoids some of the drawbacks of using env vars. \nThat Go lib is a nifty piece of reflection. It basically fakes a namespace with a prefix (making for longer names, like I mentioned) and auto generates a config object for you. But it's still the equivalent of constructing an object by passing in a hash of strings. It still doesn't solve the structure problem, provide type safety, explicitly define the requirements to aid usability or discoverability, nor does it handle early validation. \nYes, you can be principled about it and still use env vars well, adding abstractions to hide the medium of input transport from your code, but more often than not you see the lazy approach being used where OS environment reads litter the code with no abstraction at all. \nWhy doesn't 12-factor consider CLI args? I have no idea. Env Vars aren't always terrible, they're just not always the best option. \nIt's worth noting that there are other people who disagree with 12factor on this. Some easily found examples:\nhttp://grimoire.ca/12factor/3-config\nhttp://devopsblog.wordpress.com/2012/12/19/12-factor-saas-apps/\nJust because it's formally declared, has a pretty website, and is from a respectable source doesn't mean it's flawless and unimpeachable. \n-Karl\n\nOn Jul 10, 2014, at 2:04 AM, weilanwu notifications@github.com wrote:\nThe primary reason 12-factor recommends env vars is simply to expose deployment-specific details as inputs rather than hard coding them.\nI dont know if this is true.. Reading the 12-factor site it states clearly it is the design principals for 'modern application' (that is, not only for 'deployment-specific' details).\nAlso, there is blog on using this practice to build application in Go: http://blog.gopheracademy.com/day-03-building-a-twelve-factor-app-in-go\nHaving said that, I also dislike environment variables, from past experience, mostly from the list you mentioned: \"hidden switches\", \"hard to manage\", \"environment pollution\"...\nBut it begs the question, if environment variables are so bad (because global variables are \"really bad\" in programming), why it is listed as the bullet point of one of the 'factors' in 12-factor ?\nAnyways, I am open either way and stand corrected... :-)\n\u2014\nReply to this email directly or view it on GitHub.\n\nYou received this message because you are subscribed to the Google Groups \"pivotal-bosh-eng\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to pivotal-bosh-eng+unsubscribe@pivotallabs.com.\nTo post to this group, send email to pivotal-bosh-eng@pivotallabs.com.\nTo view this discussion on the web visit https://groups.google.com/a/pivotallabs.com/d/msgid/pivotal-bosh-eng/cloudfoundry/bosh/pull/608/c48580967%40github.com.\n. We had to postpone it few times. I'm going to prioritize it to be worked on\nin about two weeks.\n\nDo not worry about rebasing against removed go_agent directory. We will\ntake care of that.\nOn Tue, Aug 12, 2014 at 4:13 PM, Weian Deng notifications@github.com\nwrote:\n\n@adamstegman https://github.com/adamstegman Thank you for your good\nsuggestion! When will this PR be prioritized. I has been there for more\nthen a month now. The problem I have with making more changes, even adding\ntest code is that, BOSH code based has been change quickly. Instead of\nmaking the changes and then we have to doing rebase again and again. Would\nyou let us know when the PR is prioritized. We will add the test code.\nBTW, recently, the go_agent directory is replace with the go submodule.\nFor this PR, we used to have changes in go agent area (two *.go files), two\nfiles. With go_agent directory is gone. What is the process of rebasing\nthose changes? Thank you!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/613#issuecomment-51990557.\n. We'll do rebasing to keep it fair =) -dk\n\nOn Tue, Aug 12, 2014 at 4:22 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nThanks for the update. I will get back to this at some point :)\nOn Tue, Aug 12, 2014 at 3:28 PM, Adam Stegman \u269b notifications@github.com\nwrote:\n\n@drnic The director controllers have been refactored; you will need to\nrebase your commits onto develop before we can accept this pull request.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51986670\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/625#issuecomment-51991258.\n. That feature has been there for a bit. It should be in 94.\n\nOn Tue, Aug 12, 2014 at 5:12 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nOk will wait for next bosh final release (am running 94)\nOn Tue, Aug 12, 2014 at 5:11 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nClosed #633.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/633#event-152160408\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/633#issuecomment-51995039.\n. I've put a story (https://www.pivotaltracker.com/story/show/79038492) to\nstop locking during deletion because there is no point in removing drs lock\nattribute column in most cases.\n\nThis story is at the top of the backlog since this does make bosh deploy\nworkflow more fragile for certain cases.\nOn Wed, Sep 17, 2014 at 1:21 PM, Simon Johansson notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife we are currently deploying a\nstandard vsphere env from the generate_deployment_manifest, so, 33.\nThis only happens when deleting VMs though, never when creating(at least\nso far, we've done may fresh deployments today). Only solution we've found\nis to do bosh cck and delete the references(to the already deleted VMs).\nNo biggie, but it breaks automation. :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/647#issuecomment-55953797.\n. You can debug any cf command by setting CF_TRACE environment variable to\n'true'.\n\nWhat does CF_TRACE=true cf login show?\nOn Mon, Oct 27, 2014 at 2:48 AM, sreelathakoye notifications@github.com\nwrote:\n\nHello cppforlife,\nSorry to trouble once again by opening this issue.\nBut I was able to deploy cloudfoundry on AWS, and even able to set target\nand push one sample app.\nBut when today again I try to target again that using\ncf target http://api.x.x.x.x.xip.io\nSetting target to http://api.x.x.x.x.xip.io... FAILED\nTarget refused connection.\nThis I have tried from machine on AWS.\nBut when I try the same thing from another machine, it was able to target\nlike,\nC:\\Users\\sreelatha_koye>cf api http://api.x.x.x.x.xip.io\nSetting api endpoint to http://api.x.x.x.x.xip.io...\nWarning: Insecure http API endpoint detected: secure https API endpoints\nare rec\nommended\nOK\nAPI endpoint: http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nC:\\Users\\sreelatha_koye>cf login\nAPI endpoint: http://api.x.x.x.x.xip.io\nWarning: Insecure http API endpoint detected: secure https API endpoints\nare rec\nommended\nFAILED\nServer error, status code: 503, error code: , message:\nAPI endpoint: http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nIam unable to figure out if this is a network problem, or some problem\nwith my deployment.\nHelp on this would be really useful for me.\nThanks & regards,\nSrilatha.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/659#issuecomment-60569177.\n. I noticed from the stacktrace that you are using cf ruby cli. It was\ndeprecated some time ago. Could you try to use\nhttps://github.com/cloudfoundry/cli? CF_TRACE=true should work there as\nwell.\n\nOn Mon, Oct 27, 2014 at 4:42 AM, sreelathakoye notifications@github.com\nwrote:\n\nHello cppforlife,\nThank you for your reply.\nI see following when I tried CF_TRACE\nCF_TRACE=true cf target http://api.x.x.x.x.xip.io\nTarget refused connection.\nCF_TRACE=true cf login show\nTime of crash:\n2014-10-27 11:06:31 +0000\nCFoundry::TargetRefused: target refused connection (getaddrinfo: Name or\nservice not known)\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:172:in rescue in\nrequest_uri'\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:90:inrequest_uri'\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:60:in request'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:93:inrequest_raw'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:88:in request'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:66:inget'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:62:in info'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:29:inuaa'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:97:in refresh_token!'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:84:inrequest'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:66:in get'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:62:ininfo'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:29:in uaa'\ncfoundry-4.7.2.rc1/lib/cfoundry/concerns/login_helpers.rb:6:in\nlogin_prompts'\ncf-5.4.7/lib/cf/cli/start/login.rb:35:in login'\nmothership-0.5.1/lib/mothership/base.rb:66:inrun'\nmothership-0.5.1/lib/mothership/command.rb:72:in block in invoke'\nmothership-0.5.1/lib/mothership/command.rb:86:ininstance_exec'\nmothership-0.5.1/lib/mothership/command.rb:86:in invoke'\nmothership-0.5.1/lib/mothership/base.rb:55:inexecute'\ncf-5.4.7/lib/cf/cli.rb:195:in block (2 levels) in execute'\ncf-5.4.7/lib/cf/cli.rb:206:insave_token_if_it_changes'\ncf-5.4.7/lib/cf/cli.rb:194:in block in execute'\ncf-5.4.7/lib/cf/cli.rb:123:inwrap_errors'\ncf-5.4.7/lib/cf/cli.rb:190:in execute'\nmothership-0.5.1/lib/mothership.rb:45:instart'\ncf-5.4.7/bin/cf:18:in '\nruby-1.9.3-p547/bin/cf:23:inload'\nruby-1.9.3-p547/bin/cf:23:in '\nruby-1.9.3-p547/bin/ruby_executable_hooks:15:ineval'\nruby-1.9.3-p547/bin/ruby_executable_hooks:15:in `'\nI couldn't understand the exact problem.\nWhen I try the same command on other machine where I was able to set\ntarget,\nIt says\nC:\\Users\\sreelatha_koye>cf login show\nVERSION:\n6.6.2-0c953cf\nAPI endpoint: http://api.x.x.x.x.xip.io\nREQUEST: [2014-10-27T16:46:45+05:30]\nGET /v2/info HTTP/1.1\nHost: api.x.x.x.x.xip.io\nAccept: application/json\nContent-Type: application/json\nUser-Agent: go-cli 6.6.2-0c953cf / windows\nRESPONSE: [2014-10-27T16:47:17+05:30]\nHTTP/1.1 200 OK\nContent-Length: 219\nAge: 89\nConnection: Keep-Alive\nContent-Type: application/json;charset=utf-8\nDate: Tue, 07 Oct 2014 09:01:35 GMT\nProxy-Connection: Keep-Alive\nServer: nginx\nX-Content-Type-Options: nosniff\nX-Vcap-Request-Id: 434d795f-58a2-4eac-8e21-2f5008bc6a95\n{\"name\":\"vcap\",\"build\":\"2222\",\"support\":\"\nhttp://support.cloudfoundry.com\",\"version\":2,\"description\":\"Cloud Foundry\nsponsored by Pivotal\",\"authorization_endpoint\":\"\nhttp://uaa.x.x.x.x.xip.io\",\"api_version\":\"2.0\n.0\"}\nWarning: Insecure http API endpoint detected: secure https API endpoints\nare recommended\nREQUEST: [2014-10-27T16:47:17+05:30]\nGET /login HTTP/1.1\nHost: uaa.54.210.241.175.xip.io\nAccept: application/json\nContent-Type: application/json\nUser-Agent: go-cli 6.6.2-0c953cf / windows\nRESPONSE: [2014-10-27T16:48:11+05:30]\nHTTP/1.1 503 Service Unavailable\nConnection: close\nContent-Length: 1030\nCache-Control: no-cache\nContent-Type: text/html; charset=utf-8\nPragma: no-cache\nProxy-Connection: close\nNetwork Error\nNetwork Error (dns_server_failure)\nYour request could not be processed because an error occurred contacting\nthe DNS server.  The DNS server may be temporarily unavailable, or there\ncould be a network problem.\nFor assistance, contact your network support team.\nYour request was categorized by Blue Coat Web Filter as 'Dynamic DNS\nHost'.\nIf you wish to question or dispute this result, please click here\nhttp://%0Asitereview.bluecoat.com/sitereview.jsp?referrer=136&url=http://uaa.54.210.241.175.xip.io/login.\nFAILED\nServer error, status code: 503, error code: , message:\nFAILED\nServer error, status code: 503, error code: , message:\nAPI endpoint: http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nNow iam totally confused, like, on the first machine in which iam even\nunable to set target that is a instance of AWS. So no proxy issue.\nFor second machine it says proxy issue. and able to set target.\nHelp in this would be great for me to go ahead.\nThanks & Regards,\nSrilatha.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/659#issuecomment-60580473.\n. It is CPI specific (AWS) in its current form because not all CPIs take in\ntype as a disk cloud property. Other CPIs might want to pass in\nadditional configuration which would not work.\n\nWhat's not tested is microbosh's ability to specify cloud properties. If\ncode you have added is changed and does not pass in cloud properties into\nCPI anymore, there would be no tests that fails.\nI can settle on the following for now since we are rewriting micro cli from\nscratch:\nresources:\n  persistent_disk: 20_480\n  persistent_disk_cloud_properties:\n    type: gp2\n  cloud_properties:\n    instance_type: flavor\n    availability_zone: az1\nOn Wed, Oct 1, 2014 at 11:08 AM, Aaron Huber notifications@github.com\nwrote:\n\nThis change is already CPI independent, it's just passing the variable to\nthe existing CPI code so this will work with any CPI that has support for\ncloud_properties in the create_disk function. What you are requesting is\njust changing the syntax/formatting of how the variables are stored in the\nMicroBOSH manifest to match what BOSH is now using but there would be no\nfunctional change.\nAs it would take a much larger modification to the MicroBOSH code to\nsupport that as well as maintain backwards compatibility and changing all\nthe specs I'd prefer not to make that significant a change.\nI'll take a look at the specs to see if I can figure something out, but\nthere is already test coverage in the CPIs to handle the parameter when\nit's passed in so I didn't think it was necessary here.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/663#issuecomment-57509348.\n. It does not look like we have tests for that (part of the reason for the\nrewrite =). Do not worry about the tests in this case, BOSH team member\nwill add them when we pull this in, but do change the implementation to use\npersistent_disk_cloud_properties and confirm that it works manually.\n\nThanks.\nOn Wed, Oct 1, 2014 at 11:28 AM, Aaron Huber notifications@github.com\nwrote:\n\nOK, I think I can figure that change out (I'm far from proficient on Ruby\nso just trying to do what I can to get the functionality we need).\nAre there any similar specs to what we'd need to test here? I'm not sure\nwhere to start on that since this is optional and especially if I make the\nchange you're requesting anything could be passed. Is there a spec already\nthat tests that properties being set are actually passed into the CPI?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/663#issuecomment-57512495.\n. Was not picked up yet (https://www.pivotaltracker.com/n/projects/956238).\nWe have a smaller team last and this week and poodle took priority.\n\nOn Mon, Oct 20, 2014 at 11:30 AM, Aaron Huber notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife, is this still underway?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/663#issuecomment-59816577.\n. Thanks for contributing an awesome PR!\n\nOn Wed, Dec 3, 2014 at 5:58 PM, Aaron Huber notifications@github.com\nwrote:\n\nJust checked this out on the 2788 release and everything is working as\nexpected:\n- MicroBOSH volume type for boot disk and persistent disk getting\n  passed correctly to the volume API from the manifest\n- Settings from the manifest getting correctly pushed to the\n  director.yml\n- VMs created by MicroBOSH are built with the correct boot disk\n  properties and persistent disk properties\nThanks for getting this wrapped up.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/667#issuecomment-65526104.\n. It's almost at the top of the backlog. Very soon...\n\nOn Fri, Dec 12, 2014 at 10:47 AM, Alex Suraci notifications@github.com\nwrote:\n\npls\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/684#issuecomment-66816299.\n. Error claims that there were 2 partitions already. bosh-agent only supports\nroot disk with a single partition (root partition). it will create\nadditional ones for swap and ephemeral data.\n\nMay be there is some other error earlier on? Could you provide full log?\nOn Thu, Nov 13, 2014 at 8:55 AM, John McTeague notifications@github.com\nwrote:\n\nPrior to increasing the disk to 100GB I had experimented with different\nRAM sizes. 28GB seemed to be the upper limit before I had to increase the\ndisk\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/694#issuecomment-62925366.\n. We'll try this out. Which OpenStack version are you using and what is your cinder backend? \n\nI believe last time we investigated using by-id symlinks they were also not reliable.\n-dmitriy\n\nOn Dec 4, 2014, at 1:45 AM, boebu notifications@github.com wrote:\nWe are using openstack with config_drive:disk for our bosh deployments.\nI figured out, that the device order changes when the vm is rebooted trough nova (e.g. /dev/vdb becomes /dev/vdc)\nThis results in that bosh mounts the config drive as its persistent disk.\nBosh should use /dev/disk/by-id to be aware of this.\nto reproduce the issue:\ndeploy microbosh (config_drive:disk) (trusty-go_agent)\nnova reboot {microbosh-cid}\n\u2014\nReply to this email directly or view it on GitHub.\n. Not sure about this specific story, but in the past we've locked vcloud to\na version we know passes the lifecycle/acceptance tests. It's kind of a\nspecial case because Pivotal doesn't maintain it, have commit access, or\nhave auth to publish a new gems. We don't want new versions of bosh\nto accedently be published with new versions of the CPIs that they weren't\ntested with.\n\nRecently we've added vcloud testing to our CI, to block deployment of bosh\nif vcloud doesn't work. When it breaks or the vcloud cpi authors ask us to\nbump it, we will.\nEventually we hope to externalize all the CPIs so that they are distributed\nas bosh releases. Then it'd be up to the deployment operator (or some\nmanifest generating scripts) to decide what versions go together.\n- Karl Isenberg\n  Pivotal BOSH Team\nOn Sunday, December 21, 2014, Dr Nic Williams notifications@github.com\nwrote:\n\nIn the gemspec it is locked to 0.7.2 with a link to\nhttps://www.pivotaltracker.com/n/projects/956238/stories/84406944\nBut https://www.pivotaltracker.com/n/projects/956238/stories/84406944\ndoesn't explain at all why.\nIn #719 https://github.com/cloudfoundry/bosh/issues/719 we'll need to\nbump vcloud cpi to get newer nokogiri.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/721.\n. I'll think more about it but it sounds like a thing we can definitely fix.\n\nOn Fri, Jan 23, 2015 at 10:17 AM, Dr Nic Williams notifications@github.com\nwrote:\n\nThe user request is a maximum number. The reality might be I can't have\nthat (quotas hit us all)\nOn Fri, Jan 23, 2015 at 1:15 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nI meant something like this:\n- user requests 6 compilation VMs to compile CF (workers: 6)\n- Director only gets 1 compilation VM\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237500\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/738#issuecomment-71237906.\n. I've actually already prioritized this PR in the backlog:\nhttps://www.pivotaltracker.com/story/show/88572280\n\nOn Tue, Feb 17, 2015 at 8:49 PM, Ronak Banka notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife Hi should I open a issue\nrequest for the same to add it to tracker story or just this PR is enough?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/748#issuecomment-74811763.\n. The failure reported is in rendering a director job template:\nhttps://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/director.yml.erb.erb\n\nIt looks like you need to provide additional properties, namely\nvcenter.datacenters[0].clusters. Your yaml indentation might be wrong. I\ncan't tell.\n-Karl\nOn Sunday, February 8, 2015, fckbo notifications@github.com\n<javascript:_e(%7B%7D,'cvml','notifications@github.com');> wrote:\n\nHi me again,\nI did change my manifest file to remove the warning about the \"template\"\nstatement being deprecated when having several components deployed within\nthe same vm by replacing with a \"templateS\" statement (in the job section\nof the manifest) thinking that it might fixed the problem but unfortunately\nit did not & I still have the same error as before when Binding\nconfiguration takes place:\nStarted preparing configuration > Binding configuration. Failed: Error\nfilling in template director.yml.erb.erb' forbosh_api/0' (line 332:\nundefined method `each' for nil:NilClass) (00:00:00)\nError 100: Error filling in template director.yml.erb.erb' forbosh_api/0'\n(line 332: undefined method `each' for nil:NilClass)\nAny idea ?\nSEE BELOW THE NEW MANIFEST FILE I'M USING\nname: bosh2\nlogging:\n-level: DEBUG\ndirector_uuid: 35b5ea35-213b-4494-8fb7-c1c191178824\nrelease: {name: bosh, version: 139}\nnetworks:\n- name: default type: manual subnets:\n  - range: 10.105.144.224/28 gateway: 10.105.144.225 static:\n    - 10.105.144.237\n    - 10.105.144.238 reserved: # .224 is special, not sure if I need\n      to put it in the reserved range\n    - 10.105.144.235 - 10.105.144.236 # .239 is special, not sure if\n      I need to put it in the reserved range dns:\n      - 10.0.80.11\n      - 10.0.80.12 cloud_properties: name: Private Network - vmnic0\n        vmnic2\nresource_pools:\n- name: default stemcell: name:\n  bosh-vsphere-esxi-ubuntu-trusty-go_agent version: 2830 network: default\n  size: 5 cloud_properties: cpu: 2 ram: 512 disk: 2000\ncompilation:\nreuse_compilation_vms: true\nworkers: 3\nnetwork: default\ncloud_properties:\nram: 512\ndisk: 6000\ncpu: 2\nupdate:\ncanaries: 1\ncanary_watch_time: 30000-90000\nupdate_watch_time: 30000-90000\nmax_in_flight: 1\njobs:\n-\nname: bosh_data\n   templates:\n    - name: blobstore\n      - name: postgres\n      - name: redis instances: 1 resource_pool: default persistent_disk:\n      8_000 networks:\n      - name: default static_ips: [10.105.144.238]\n    -\nname: bosh_api\n   templates:\n    - name: nats\n      - name: director\n      - name: health_monitor\n      - name: powerdns instances: 1 resource_pool: default networks:\n      - name: default static_ips: [10.105.144.237]\nproperties:\nenv:\nntp:\n- servertime.service.softlayer.com\nnats:\nuser: nats\npassword: nats-password\naddress: 10.105.144.237\nport: 4222\nblobstore:\naddress: 10.105.144.238\nport: 25251\nbackend_port: 25552\nagent:\nuser: agent\npassword: agent\ndirector:\nuser: director\npassword: director\npostgres: &bosh_db\nuser: bosh\npassword: bosh\nhost: 10.105.144.238\nport: 5432\ndatabase: bosh\nredis:\npassword: redis\naddress: 10.105.144.238\nport: 25255\ndirector:\nname: bosh2\naddress: 10.105.144.237\nport: 25555\nencryption: false\nCheck if the CPI for your IaaS supports snapshots, otherwise disable it.\nAs an example vCloud CPI 0.5.2 does not support snapshots\nenable_snapshots: false\nmax_tasks: 100\ndb: *bosh_db\nIf needed, limit the number of threads used to concurrently instantiate\nnew vms (32 by default)\nmax_threads: 1\nhm:\nhttp:\nport: 25923\nuser: admin\npassword: admin\ndirector_account:\nuser: admin\npassword: admin\nintervals:\npoll_director: 60\npoll_grace_period: 30\nlog_stats: 300\nanalyze_agents: 60\nagent_timeout: 180\nrogue_agent_alert: 180\nloglevel: info\nemail_notifications: false\ntsdb_enabled: false\ncloud_watch_enabled: false\nresurrector_enabled: true\ndns:\naddress: 10.105.144.237\nrecursor: 10.0.80.11\ndb: *bosh_db\nvcenter:\naddress: 10.105.220.39\nuser: Administrator\npassword: a5pattes\ndatacenters:\n- name: bschmcSL1\n  vm_folder: mycfvm\n  template_folder: mycftemplate\n  disk_path: /cfdir\n  datastore_pattern: datastore1\n  persistent_datastore_pattern: datastore1\n  allow_mixed_datastores: true\n  clusters:\n- cfcluster:\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/750#issuecomment-73433277.\n. It'd be nice to know which version of ruby fixes the issue so we can be\nmore explicit.\n\nIIRC our tests in docker are running on 1.9.3 and 2.1.3. We'll probably\nwant to upgrade our CI before we say we officially support 2.1.5.\n-Karl\nOn Thursday, February 19, 2015, Marco Voelz notifications@github.com\nwrote:\n\nI was also confused that I didn't have to install the pg gem and specify\nthe architecture manually anymore - when I wrote the initial version of\nthis setup guide I needed to do this. My current ruby version is 2.1.5 as\nwell, so there this seems not to be an issue.\nSo shall we keep this in with the hint for people on ruby <2.1.5?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/757#issuecomment-75190337.\n. We had a hiccup in our pipeline but I expect it to be resolved\ntoday/tomorrow.\n\nOn Tue, Apr 21, 2015 at 6:02 AM, Long Nguyen notifications@github.com\nwrote:\n\nAny status branch wip-cm-89044786-light-stemcells-for-everyone ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/762#issuecomment-94784727.\n. We had a hiccup in our pipeline but I expect it to be resolved\ntoday/tomorrow.\n\nOn Tue, Apr 21, 2015 at 6:02 AM, Long Nguyen notifications@github.com\nwrote:\n\nAny status branch wip-cm-89044786-light-stemcells-for-everyone ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/762#issuecomment-94784727.\n. What's the output of which bosh?\n\nOn Tue, Apr 21, 2015 at 4:19 AM, Fabio Berchtold notifications@github.com\nwrote:\n\nHi,\nI'm having the same issue, but with different error codes. I'm not using\ntraveling-bosh though, but have it installed as a ruby gem.\nIt seems bosh deploy always returns with exitcode 0 even if there were\nerrors during the deployment.\nFor example:\n$ bosh deploy\nGenerating deployment manifest\nProcessing deployment manifest\nGetting deployment properties from director...\nCompiling deployment manifest...\nDeploying\nDeployment name: 'cf.yml'\nDirector name: 'bosh'\nDirector task 11\n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done (00:00:00)\n  Started preparing deployment > Binding releases. Done (00:00:00)\n  Started preparing deployment > Binding existing deployment. Done (00:00:00)\n  Started preparing deployment > Binding resource pools. Done (00:00:00)\n  Started preparing deployment > Binding stemcells. Done (00:00:00)\n  Started preparing deployment > Binding templates. Done (00:00:00)\n  Started preparing deployment > Binding properties. Done (00:00:00)\n  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)\n  Started preparing deployment > Binding instance networks. Failed: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use (00:00:00)\n   Failed preparing deployment (00:00:00)\nError 130008: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use\nTask 11 error\nFor a more detailed error report, run: bosh task 11 --debug\n$ echo $?\n0\n$ bosh version\nBOSH 1.2922.0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/767#issuecomment-94751705.\n. What's the output of which bosh?\n\nOn Tue, Apr 21, 2015 at 4:19 AM, Fabio Berchtold notifications@github.com\nwrote:\n\nHi,\nI'm having the same issue, but with different error codes. I'm not using\ntraveling-bosh though, but have it installed as a ruby gem.\nIt seems bosh deploy always returns with exitcode 0 even if there were\nerrors during the deployment.\nFor example:\n$ bosh deploy\nGenerating deployment manifest\nProcessing deployment manifest\nGetting deployment properties from director...\nCompiling deployment manifest...\nDeploying\nDeployment name: 'cf.yml'\nDirector name: 'bosh'\nDirector task 11\n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done (00:00:00)\n  Started preparing deployment > Binding releases. Done (00:00:00)\n  Started preparing deployment > Binding existing deployment. Done (00:00:00)\n  Started preparing deployment > Binding resource pools. Done (00:00:00)\n  Started preparing deployment > Binding stemcells. Done (00:00:00)\n  Started preparing deployment > Binding templates. Done (00:00:00)\n  Started preparing deployment > Binding properties. Done (00:00:00)\n  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)\n  Started preparing deployment > Binding instance networks. Failed: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use (00:00:00)\n   Failed preparing deployment (00:00:00)\nError 130008: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use\nTask 11 error\nFor a more detailed error report, run: bosh task 11 --debug\n$ echo $?\n0\n$ bosh version\nBOSH 1.2922.0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/767#issuecomment-94751705.\n. Noticed that property again in some spiff templates. I wonder if it was\nadded to the plugin so templates could reuse it to avoid having to know the\nresource pool name that has it in the cloud properties...\n\nOn Monday, March 16, 2015, Dmitriy Kalinin notifications@github.com wrote:\n\nyeah looks like dead property.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/773#issuecomment-81874621.\n. Noticed that property again in some spiff templates. I wonder if it was\nadded to the plugin so templates could reuse it to avoid having to know the\nresource pool name that has it in the cloud properties...\n\nOn Monday, March 16, 2015, Dmitriy Kalinin notifications@github.com wrote:\n\nyeah looks like dead property.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/773#issuecomment-81874621.\n. - ruby installed on the stemcell will be pulled out (and is currently /\n  should not be used by anything outside of stemcell building)\n- cpi releases will be updated as part of the \"External CPIs\" work\n\nThanks for looking out!\nOn Mon, Mar 23, 2015 at 9:36 AM, Jon K\u00e5re Hellan notifications@github.com\nwrote:\n\nOn 23. mars 2015 17:25, Dmitriy Kalinin wrote:\n\nIs there a particular area of BOSH you are concerned about that uses\n1.9? The Director runs on Ruby 2. The CLI currently supports 1.9, 2.x.\nstemcell_builder/stages/bosh_ruby/apply.sh installs ruby 1.9.3-p545 in\nthe stemcell. Which you can\nverify by ssh-ing in and typing 'ruby --version'.\nvsphere_cpi_release also refers to ruby 1.9\n\nJon\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/780#issuecomment-85075547.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/780#issuecomment-85083859.\n. - ruby installed on the stemcell will be pulled out (and is currently /\n  should not be used by anything outside of stemcell building)\n- cpi releases will be updated as part of the \"External CPIs\" work\n\nThanks for looking out!\nOn Mon, Mar 23, 2015 at 9:36 AM, Jon K\u00e5re Hellan notifications@github.com\nwrote:\n\nOn 23. mars 2015 17:25, Dmitriy Kalinin wrote:\n\nIs there a particular area of BOSH you are concerned about that uses\n1.9? The Director runs on Ruby 2. The CLI currently supports 1.9, 2.x.\nstemcell_builder/stages/bosh_ruby/apply.sh installs ruby 1.9.3-p545 in\nthe stemcell. Which you can\nverify by ssh-ing in and typing 'ruby --version'.\nvsphere_cpi_release also refers to ruby 1.9\n\nJon\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/780#issuecomment-85075547.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/780#issuecomment-85083859.\n. Yeah there is a known bug about bosh cck deleting compilation VMs (undefined\nmethod `fetch' for \"package_compiler\"). Here is the story to fix that:\nhttps://www.pivotaltracker.com/story/show/69672460\n\nOn Wed, Apr 29, 2015 at 1:38 AM, Kamil Burzynski notifications@github.com\nwrote:\n\nFYI, I didn't managed to get that vm in order. I did bosh delete\ndeployment concourse and then bosh deploy again and it worked.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/813#issuecomment-97352618.\n. Yeah there is a known bug about bosh cck deleting compilation VMs (undefined\nmethod `fetch' for \"package_compiler\"). Here is the story to fix that:\nhttps://www.pivotaltracker.com/story/show/69672460\n\nOn Wed, Apr 29, 2015 at 1:38 AM, Kamil Burzynski notifications@github.com\nwrote:\n\nFYI, I didn't managed to get that vm in order. I did bosh delete\ndeployment concourse and then bosh deploy again and it worked.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/813#issuecomment-97352618.\n. There is a bosh cleanup command that removes unused stemcells and\nreleases. Does that solve your use case?\n\nOn Mon, Sep 21, 2015 at 2:00 AM, Jonty Wareing notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife We are automating the removal\nof old stemcells from storage. Currently parsing bosh stemcells involves\nsome horrendous grep and awk and we'd like to make this more robust.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/947#issuecomment-141916356.\n. There is a bosh cleanup command that removes unused stemcells and\nreleases. Does that solve your use case?\n\nOn Mon, Sep 21, 2015 at 2:00 AM, Jonty Wareing notifications@github.com\nwrote:\n\n@cppforlife https://github.com/cppforlife We are automating the removal\nof old stemcells from storage. Currently parsing bosh stemcells involves\nsome horrendous grep and awk and we'd like to make this more robust.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/947#issuecomment-141916356.\n. I'm planning to switch to using GitHub releases to better keep track of that. Already added release notes for 3074. \n\nSent from my iPhone\n\nOn Sep 24, 2015, at 2:19 AM, mtekel notifications@github.com wrote:\nHi,\nI see that CHANGELOG.md file was not updated since 2941 and I also don't see any release notes with many of the releases (ever since v141, with recent v206 being exception). On some other web side I found a link to BOSH changelog, but which just pointed to git diff between the given and previous release - this is not so useful and human readable as the usual release notes.\nIs there any reason you have stopped updating CHANGELOG? Release notes are very useful for BOSH operators to help them understand what changes have been made, which helps them decide when and how to upgrade...\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm planning to switch to using GitHub releases to better keep track of that. Already added release notes for 3074. \n\nSent from my iPhone\n\nOn Sep 24, 2015, at 2:19 AM, mtekel notifications@github.com wrote:\nHi,\nI see that CHANGELOG.md file was not updated since 2941 and I also don't see any release notes with many of the releases (ever since v141, with recent v206 being exception). On some other web side I found a link to BOSH changelog, but which just pointed to git diff between the given and previous release - this is not so useful and human readable as the usual release notes.\nIs there any reason you have stopped updating CHANGELOG? Release notes are very useful for BOSH operators to help them understand what changes have been made, which helps them decide when and how to upgrade...\n\u2014\nReply to this email directly or view it on GitHub.\n. That's an interesting use case. Thanks for sharing.\n\nWe are actually adding some features to the CLI to make recovering Director easier and it may help here. We have a PR coming in to add: bosh upload stemcell X --fix. Typically it would be used for cases when stemcell is somehow corrupted and needs to be replaced in the IaaS with the same version.\nSent from my iPhone\n\nOn Sep 24, 2015, at 2:32 AM, mtekel notifications@github.com wrote:\nHi,\nit would be useful to have option in bosh-cli to skip deletion of compiled packages on stemcell removal. The use case is when you realize some issue in a stemcell and want to quickly replace it. You can then selectively rebuild/re-create the affected jobs. This saves a lot of time - which can be quite critical in case of downtime. The current process of increasing stemcell version number and applying that to a release would cause a complete rolling update which takes quite some time...\n\u2014\nReply to this email directly or view it on GitHub.\n. That's an interesting use case. Thanks for sharing.\n\nWe are actually adding some features to the CLI to make recovering Director easier and it may help here. We have a PR coming in to add: bosh upload stemcell X --fix. Typically it would be used for cases when stemcell is somehow corrupted and needs to be replaced in the IaaS with the same version.\nSent from my iPhone\n\nOn Sep 24, 2015, at 2:32 AM, mtekel notifications@github.com wrote:\nHi,\nit would be useful to have option in bosh-cli to skip deletion of compiled packages on stemcell removal. The use case is when you realize some issue in a stemcell and want to quickly replace it. You can then selectively rebuild/re-create the affected jobs. This saves a lot of time - which can be quite critical in case of downtime. The current process of increasing stemcell version number and applying that to a release would cause a complete rolling update which takes quite some time...\n\u2014\nReply to this email directly or view it on GitHub.\n. We have not officially released this feature yet as it's still going\nthrough our CI pipelines.\n\nOn Thu, Sep 24, 2015 at 1:49 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nThanks for spotting that. My use cases have been via settings.yml only I\nmissed it.\nDo you think there's a simple PR you can do to fix?\nOn Thu, Sep 24, 2015 at 1:21 PM, berniedurfee-ge <notifications@github.com\n\nwrote:\nIt appears that the 'bosh bootstrap' command requires that the AWS key\nbe hardcoded in the settings.yml file. Instead, we would like to run the\n'bosh bootstrap' command on an EC2 instance that is assigned an EC2 role,\nso we don't have to hardcode credentials.\nAdditionally, 'bosh bootstrap' should support the standard\nAWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\nLooks like there is a 'credentials_source' option that should work, just\ndoesn't seem to be recognized in settings.yml.\nBosh 1.3074.0\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/962\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/962#issuecomment-143046706.\n. We have not officially released this feature yet as it's still going\nthrough our CI pipelines.\n\nOn Thu, Sep 24, 2015 at 1:49 PM, Dr Nic Williams notifications@github.com\nwrote:\n\nThanks for spotting that. My use cases have been via settings.yml only I\nmissed it.\nDo you think there's a simple PR you can do to fix?\nOn Thu, Sep 24, 2015 at 1:21 PM, berniedurfee-ge <notifications@github.com\n\nwrote:\nIt appears that the 'bosh bootstrap' command requires that the AWS key\nbe hardcoded in the settings.yml file. Instead, we would like to run the\n'bosh bootstrap' command on an EC2 instance that is assigned an EC2 role,\nso we don't have to hardcode credentials.\nAdditionally, 'bosh bootstrap' should support the standard\nAWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.\nLooks like there is a 'credentials_source' option that should work, just\ndoesn't seem to be recognized in settings.yml.\nBosh 1.3074.0\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/962\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/962#issuecomment-143046706.\n. As soon as we reach following marker:\nhttps://www.pivotaltracker.com/story/show/100170490. This is a top priority\nfor the BOSH team.\n\nOn Wed, Oct 14, 2015 at 8:48 AM, mtekel notifications@github.com wrote:\n\nWhen is that expected to happen?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/979#issuecomment-148092548.\n. As soon as we reach following marker:\nhttps://www.pivotaltracker.com/story/show/100170490. This is a top priority\nfor the BOSH team.\n\nOn Wed, Oct 14, 2015 at 8:48 AM, mtekel notifications@github.com wrote:\n\nWhen is that expected to happen?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/979#issuecomment-148092548.\n. Just to make sure, in your real manifest is subnets value an array?\n\nOn Mon, Oct 26, 2015 at 7:21 PM, jcarrothers-sap notifications@github.com\nwrote:\n\nI've been experimenting with mult-homed VMs in my deployments and I\nbelieve I've found a problem where bosh is not correctly configuring the\nnetworking. Here is the relevent excerpt from my job config:\n- instances: 1\n  networks:\n  - name: internal\n    default: [ dns ]\n  - name: external\n    default: [ gateway ]\n    static_ips:\n    - 10.160.191.254\n      networks:\n- name: internal\n  subnets:\n      range: 192.168.0.0/16\n      reserved: [\"192.168.0.2-192.168.4.255\", \"192.168.6.0-192.168.255.254\"]\n      dns: [192.168.0.5]\n      gateway: 192.168.0.1\n- name: external\n  subnets:\n      range: 10.160.160.0/19\n      static: \n      dns: \n      gateway: 10.160.160.1\nHowever, when the VM is deployed, the /etc/resolve.conf is empty and the\ndefault gateway is configured for eth0 and a gateway of 192.168.0.1 instead\nof eth1 with a gateway of 10.160.160.1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/996.\n. Just to make sure, in your real manifest is subnets value an array?\n\nOn Mon, Oct 26, 2015 at 7:21 PM, jcarrothers-sap notifications@github.com\nwrote:\n\nI've been experimenting with mult-homed VMs in my deployments and I\nbelieve I've found a problem where bosh is not correctly configuring the\nnetworking. Here is the relevent excerpt from my job config:\n- instances: 1\n  networks:\n  - name: internal\n    default: [ dns ]\n  - name: external\n    default: [ gateway ]\n    static_ips:\n    - 10.160.191.254\n      networks:\n- name: internal\n  subnets:\n      range: 192.168.0.0/16\n      reserved: [\"192.168.0.2-192.168.4.255\", \"192.168.6.0-192.168.255.254\"]\n      dns: [192.168.0.5]\n      gateway: 192.168.0.1\n- name: external\n  subnets:\n      range: 10.160.160.0/19\n      static: \n      dns: \n      gateway: 10.160.160.1\nHowever, when the VM is deployed, the /etc/resolve.conf is empty and the\ndefault gateway is configured for eth0 and a gateway of 192.168.0.1 instead\nof eth1 with a gateway of 10.160.160.1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/996.\n. do you have more details how cpi failed (stacktrace) and lost track of the vm?\n\nSent from my iPhone\n\nOn Nov 9, 2015, at 2:13 AM, Marco Voelz notifications@github.com wrote:\nWhile looking more thorough through the logs I found the issue in this case:\nThe previous deploy failed because the compilation VM with that specific IP couldn't be brought up (IaaS timed out)\nIn the next deployment that IP was already taken. So it seems the IaaS brought up the VM after all.\nI'll re-open this when I encounter it again and have made sure that it is not an IaaS issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. do you have more details how cpi failed (stacktrace) and lost track of the vm?\n\nSent from my iPhone\n\nOn Nov 9, 2015, at 2:13 AM, Marco Voelz notifications@github.com wrote:\nWhile looking more thorough through the logs I found the issue in this case:\nThe previous deploy failed because the compilation VM with that specific IP couldn't be brought up (IaaS timed out)\nIn the next deployment that IP was already taken. So it seems the IaaS brought up the VM after all.\nI'll re-open this when I encounter it again and have made sure that it is not an IaaS issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. that doesnt look like a bosh error. it seems that you are using some kind of a plugin?\n\n@drnic ideas?\nSent from my iPhone\n\nOn Nov 18, 2015, at 11:09 AM, ChandraNarayanasamy notifications@github.com wrote:\nWe were using bosh earlier to install cloudfoundry. It was working as expected and we installed cloudfoundry locally. \nToday, we updated the bosh after that bosh stopped working. Below is the error message we get when we type bosh command.\nFailed to load plugin /usr/local/share/gems/gems/bosh-workspace-0.9.4/lib/bosh/cli/commands/deployment_patch.rb: cannot load such file -- bosh/workspace\nCan somebody help to resolve this error.\nThank you in advance.\n\u2014\nReply to this email directly or view it on GitHub.\n. that doesnt look like a bosh error. it seems that you are using some kind of a plugin?\n\n@drnic ideas?\nSent from my iPhone\n\nOn Nov 18, 2015, at 11:09 AM, ChandraNarayanasamy notifications@github.com wrote:\nWe were using bosh earlier to install cloudfoundry. It was working as expected and we installed cloudfoundry locally. \nToday, we updated the bosh after that bosh stopped working. Below is the error message we get when we type bosh command.\nFailed to load plugin /usr/local/share/gems/gems/bosh-workspace-0.9.4/lib/bosh/cli/commands/deployment_patch.rb: cannot load such file -- bosh/workspace\nCan somebody help to resolve this error.\nThank you in advance.\n\u2014\nReply to this email directly or view it on GitHub.\n. not sure what is installed in your envirobment but if you just install bosh cli without other cli plugins it should work. it works on my env.\n\nSent from my iPhone\n\nOn Nov 18, 2015, at 12:25 PM, Saravanan Renganathan notifications@github.com wrote:\nWe tried bundle install from cf-boshworkspace repo\nFailed to load plugin /usr/local/share/gems/gems/bosh_cli_plugin_micro-1.3130.0/lib/bosh/cli/commands/micro.rb: cannot load such file -- bosh/deployer\n\u2014\nReply to this email directly or view it on GitHub.\n. not sure what is installed in your envirobment but if you just install bosh cli without other cli plugins it should work. it works on my env.\n\nSent from my iPhone\n\nOn Nov 18, 2015, at 12:25 PM, Saravanan Renganathan notifications@github.com wrote:\nWe tried bundle install from cf-boshworkspace repo\nFailed to load plugin /usr/local/share/gems/gems/bosh_cli_plugin_micro-1.3130.0/lib/bosh/cli/commands/micro.rb: cannot load such file -- bosh/deployer\n\u2014\nReply to this email directly or view it on GitHub.\n. It looks like you have included agent.log from the Director VM. That wont\nhelp us here since there is nothing wrong with the Director VM. You want to\nssh into the VMs that are not reachable (and get eventually deleted after\n11 mins) to get their agent log.\n\nOn Fri, Dec 4, 2015 at 1:40 AM, smokingfly notifications@github.com wrote:\n\nPlease ignore the previous post about failing with execution expired, I\nmade progress from that point and am not stuck at the same point as\noriginally mentioned in this ticket above.\nAttached is the manifest file cloudfoundry.yml, debug log of \"bosh deploy\"\ncommand and agent log from /var/vcap/bosh/log/current. I confirm that I am\nable to ping NAT instance from the bosh director as well as from my local\nPC.\nSecurity Group on both - bosh director and NAT, are set to open to ALL\nTRAFFIC and for \"Anywhere\".\nHere is the error message again:\nStarted compiling packages\nStarted compiling packages >\nrootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea\nStarted compiling packages >\nhaproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171\nStarted compiling packages >\nbuildpack_binary/e0c8736b073d83c2459519851b5736c288311d92\nStarted compiling packages >\nbuildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3\nStarted compiling packages >\nbuildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b\nStarted compiling packages >\nbuildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3\nFailed compiling packages >\nbuildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3: Timed out\npinging to 907c1235-e411-45b6-8da6-73f95f295c48 after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3: Timed out\npinging to 3b815203-41b6-417a-9f1f-547cd93c718e after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b: Timed out pinging\nto 33a5bf4a-35b3-4d71-8915-136a29556adf after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_binary/e0c8736b073d83c2459519851b5736c288311d92: Timed out\npinging to 7bbf7e3f-d29c-4a81-bedd-14ed9a0c186c after 600 seconds (00:11:09)\nFailed compiling packages >\nrootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea: Timed out\npinging to a2172311-4594-4069-a9fe-e86b197619e0 after 600 seconds (00:11:09)\nFailed compiling packages >\nhaproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171: Timed out pinging to\n49a5b905-341c-49f0-b27c-82c9fe3bf7ad after 600 seconds (00:11:09)\nError 450002: Timed out pinging to 907c1235-e411-45b6-8da6-73f95f295c48\nafter 600 seconds\nTask 7 error\ncloudfoundry.txt\nhttps://github.com/cloudfoundry/bosh/files/51833/cloudfoundry.txt\ncurrent.txt https://github.com/cloudfoundry/bosh/files/51834/current.txt\ndebug7.txt https://github.com/cloudfoundry/bosh/files/51835/debug7.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1037#issuecomment-161920780.\n. It looks like you have included agent.log from the Director VM. That wont\nhelp us here since there is nothing wrong with the Director VM. You want to\nssh into the VMs that are not reachable (and get eventually deleted after\n11 mins) to get their agent log.\n\nOn Fri, Dec 4, 2015 at 1:40 AM, smokingfly notifications@github.com wrote:\n\nPlease ignore the previous post about failing with execution expired, I\nmade progress from that point and am not stuck at the same point as\noriginally mentioned in this ticket above.\nAttached is the manifest file cloudfoundry.yml, debug log of \"bosh deploy\"\ncommand and agent log from /var/vcap/bosh/log/current. I confirm that I am\nable to ping NAT instance from the bosh director as well as from my local\nPC.\nSecurity Group on both - bosh director and NAT, are set to open to ALL\nTRAFFIC and for \"Anywhere\".\nHere is the error message again:\nStarted compiling packages\nStarted compiling packages >\nrootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea\nStarted compiling packages >\nhaproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171\nStarted compiling packages >\nbuildpack_binary/e0c8736b073d83c2459519851b5736c288311d92\nStarted compiling packages >\nbuildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3\nStarted compiling packages >\nbuildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b\nStarted compiling packages >\nbuildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3\nFailed compiling packages >\nbuildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3: Timed out\npinging to 907c1235-e411-45b6-8da6-73f95f295c48 after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3: Timed out\npinging to 3b815203-41b6-417a-9f1f-547cd93c718e after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b: Timed out pinging\nto 33a5bf4a-35b3-4d71-8915-136a29556adf after 600 seconds (00:11:09)\nFailed compiling packages >\nbuildpack_binary/e0c8736b073d83c2459519851b5736c288311d92: Timed out\npinging to 7bbf7e3f-d29c-4a81-bedd-14ed9a0c186c after 600 seconds (00:11:09)\nFailed compiling packages >\nrootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea: Timed out\npinging to a2172311-4594-4069-a9fe-e86b197619e0 after 600 seconds (00:11:09)\nFailed compiling packages >\nhaproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171: Timed out pinging to\n49a5b905-341c-49f0-b27c-82c9fe3bf7ad after 600 seconds (00:11:09)\nError 450002: Timed out pinging to 907c1235-e411-45b6-8da6-73f95f295c48\nafter 600 seconds\nTask 7 error\ncloudfoundry.txt\nhttps://github.com/cloudfoundry/bosh/files/51833/cloudfoundry.txt\ncurrent.txt https://github.com/cloudfoundry/bosh/files/51834/current.txt\ndebug7.txt https://github.com/cloudfoundry/bosh/files/51835/debug7.txt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1037#issuecomment-161920780.\n. i think at this point its a cf manifest configuration problem. not sure how you built your manifest. cc @amit-pivotallabs\n\nSent from my iPhone\n\nOn Dec 4, 2015, at 9:14 AM, smokingfly notifications@github.com wrote:\nI did multiple \"bosh deploy\" and got rid of the errors and have now run into another one:\nStarted preparing configuration > Binding configuration. Failed: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]') (00:00:00)\nError 100: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]')\nAny idea?\n\u2014\nReply to this email directly or view it on GitHub.\n. i think at this point its a cf manifest configuration problem. not sure how you built your manifest. cc @amit-pivotallabs\n\nSent from my iPhone\n\nOn Dec 4, 2015, at 9:14 AM, smokingfly notifications@github.com wrote:\nI did multiple \"bosh deploy\" and got rid of the errors and have now run into another one:\nStarted preparing configuration > Binding configuration. Failed: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]') (00:00:00)\nError 100: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]')\nAny idea?\n\u2014\nReply to this email directly or view it on GitHub.\n. it would be great to also include a stronger assertion in integration tests on the output to see if instance was updated or not in the task output. currently these tests dont check that director did anything.\n\nSent from my iPhone\n\nOn Nov 25, 2015, at 4:15 AM, cfdreddbot notifications@github.com wrote:\nHey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n\u2014\nReply to this email directly or view it on GitHub.\n. it would be great to also include a stronger assertion in integration tests on the output to see if instance was updated or not in the task output. currently these tests dont check that director did anything.\n\nSent from my iPhone\n\nOn Nov 25, 2015, at 4:15 AM, cfdreddbot notifications@github.com wrote:\nHey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n\u2014\nReply to this email directly or view it on GitHub.\n. well missed the checks for vm cid and state changes. may be then just add a check that only one cid changed when doing recreate of a single vm? and check that starting a single vm doesnt start them all?\n\nSent from my iPhone\n\nOn Nov 25, 2015, at 4:15 AM, cfdreddbot notifications@github.com wrote:\nHey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n\u2014\nReply to this email directly or view it on GitHub.\n. well missed the checks for vm cid and state changes. may be then just add a check that only one cid changed when doing recreate of a single vm? and check that starting a single vm doesnt start them all?\n\nSent from my iPhone\n\nOn Nov 25, 2015, at 4:15 AM, cfdreddbot notifications@github.com wrote:\nHey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n\u2014\nReply to this email directly or view it on GitHub.\n. This appears to be a duplicate of:\n\nhttps://www.pivotaltracker.com/story/show/108826704\nOn Sat, Nov 28, 2015 at 6:43 PM, Amit Gupta notifications@github.com\nwrote:\n\nIf I have the following in my manifest\nrelease:\n- name: hello\n  version: latest\n  url: file:///path/to/release/dir\nAnd I bosh deploy, I get an error like:\nCannot find file `/var/folders/3z/st02pqt10q7fx_s715rxxwqr0000gn/T/d20151128-13417-1bojzdd/d20151128-13417-1i5h4o3/release.MF'\nI think it should either use latest and ignore the url information, or\nfail with an informative error message, saying that you can't specify\n\"latest\" version and also specify a URL simultaneously for a release. I\nthink the latter is better UX, but I'm not sure what is easier to implement\nin the CLI codebase. What would be the best solution here? Happy to try and\nsubmit a PR.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1044.\n. This appears to be a duplicate of:\n\nhttps://www.pivotaltracker.com/story/show/108826704\nOn Sat, Nov 28, 2015 at 6:43 PM, Amit Gupta notifications@github.com\nwrote:\n\nIf I have the following in my manifest\nrelease:\n- name: hello\n  version: latest\n  url: file:///path/to/release/dir\nAnd I bosh deploy, I get an error like:\nCannot find file `/var/folders/3z/st02pqt10q7fx_s715rxxwqr0000gn/T/d20151128-13417-1bojzdd/d20151128-13417-1i5h4o3/release.MF'\nI think it should either use latest and ignore the url information, or\nfail with an informative error message, saying that you can't specify\n\"latest\" version and also specify a URL simultaneously for a release. I\nthink the latter is better UX, but I'm not sure what is easier to implement\nin the CLI codebase. What would be the best solution here? Happy to try and\nsubmit a PR.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1044.\n. Ah that would be it. I just sent a response about this to the mailing list.\n\nOn Thu, Jan 7, 2016 at 10:32 AM, Ferran Rodenas notifications@github.com\nwrote:\n\nAny release that uses the encryption_key feature (like the networking\nhttps://github.com/cf-platform-eng/networking-boshrelease/blob/master/config/final.yml#L9\nrelease) will hit this problem if using a bosh cli version > 3144. See this\npost\nhttps://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/thread/H365YFWROOHGOGBGLPBNWT7LLBGBIQEB/\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1079#issuecomment-169765003.\n. Ah that would be it. I just sent a response about this to the mailing list.\n\nOn Thu, Jan 7, 2016 at 10:32 AM, Ferran Rodenas notifications@github.com\nwrote:\n\nAny release that uses the encryption_key feature (like the networking\nhttps://github.com/cf-platform-eng/networking-boshrelease/blob/master/config/final.yml#L9\nrelease) will hit this problem if using a bosh cli version > 3144. See this\npost\nhttps://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/thread/H365YFWROOHGOGBGLPBNWT7LLBGBIQEB/\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1079#issuecomment-169765003.\n. That is expected at this point. All processes on a machine are restarted.\n\nOn Fri, Jan 8, 2016 at 9:30 AM, Amulya Sharma notifications@github.com\nwrote:\n\nWe have redis cluster and a custom sensu release along with redis for\nmonitoring when we update our sensu release it restart all processes on VM\nincluding redis expected behavior is it should restart only sensu not redis\nor restarting everything for even single release change is expected ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1087.\n. That is expected at this point. All processes on a machine are restarted.\n\nOn Fri, Jan 8, 2016 at 9:30 AM, Amulya Sharma notifications@github.com\nwrote:\n\nWe have redis cluster and a custom sensu release along with redis for\nmonitoring when we update our sensu release it restart all processes on VM\nincluding redis expected behavior is it should restart only sensu not redis\nor restarting everything for even single release change is expected ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1087.\n. BOSH currently does not have an audit history that's nicely presentable via\nCLI, but it's something that we want to add as part of auditing feature.\nMajority of our deployments (bosh deploys) are from within <\nhttp://concourse.ci/> so we in most cases we look at the build history.\n\nOn Thu, Jan 21, 2016 at 10:17 AM, Peter G\u00f6tz notifications@github.com\nwrote:\n\nI think that's not exactly what I want (unless I misunderstood you).\nInstead, what I want to ask BOSH is e.g.:\n\"Which versions of MyComponent did you deploy in the last 10 days at what\ntime?\"\nIn the end, it's very similar to what bosh tasks recent gives me. But\nthis tells me only when a certain release (including version) was\ncreated, but not when this version was deployed. Instead it only\ngives me:\n| 995  | done | 2016-01-14 11:24:03 UTC | admin | create deployment  | /deployments/MyComponent          |\n| 994  | done | 2016-01-14 10:39:26 UTC | admin | create release     | Created release `MyComponent/26'  |\nTo map incidents that happened in the past to a certain code version,\nsomething like this seems like an essential feature for a deployment tool.\nIs it possible that BOSH does not support this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1103#issuecomment-173661761.\n. BOSH currently does not have an audit history that's nicely presentable via\nCLI, but it's something that we want to add as part of auditing feature.\nMajority of our deployments (bosh deploys) are from within <\nhttp://concourse.ci/> so we in most cases we look at the build history.\n\nOn Thu, Jan 21, 2016 at 10:17 AM, Peter G\u00f6tz notifications@github.com\nwrote:\n\nI think that's not exactly what I want (unless I misunderstood you).\nInstead, what I want to ask BOSH is e.g.:\n\"Which versions of MyComponent did you deploy in the last 10 days at what\ntime?\"\nIn the end, it's very similar to what bosh tasks recent gives me. But\nthis tells me only when a certain release (including version) was\ncreated, but not when this version was deployed. Instead it only\ngives me:\n| 995  | done | 2016-01-14 11:24:03 UTC | admin | create deployment  | /deployments/MyComponent          |\n| 994  | done | 2016-01-14 10:39:26 UTC | admin | create release     | Created release `MyComponent/26'  |\nTo map incidents that happened in the past to a certain code version,\nsomething like this seems like an essential feature for a deployment tool.\nIs it possible that BOSH does not support this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1103#issuecomment-173661761.\n. Don't worry about the Travis failures if it's not applicable. We'll run the\ntests on concourse before merging. I'll add a chore to the backlog to\ninvestigate these failures.\n\nOn Thu, Feb 18, 2016 at 12:58 AM, Alexander Lomov notifications@github.com\nwrote:\n\n@barthy1 https://github.com/barthy1 I had the same problem with CI. I\nthink you should ping someone from BOSH team to re-run tests.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1134#issuecomment-185606388.\n. you ll have create a new migration since once migration is published it cannot be modified. \n\nplease also add a test for the migration to make sure we dont override existing data (eg https://github.com/cloudfoundry/bosh/tree/master/bosh-director/spec/unit/db)\nSent from my iPhone\n\nOn Feb 17, 2016, at 7:28 PM, Matt Cui notifications@github.com wrote:\nSubmitted this PR to address issue 1133, I already verified this change, I can see the default value \"{}\" set in the newly added column cloud_properties_json.\nid | instance_id | disk_cid | size | active | cloud_properties_json\n----+-------------+----------+--------+--------+-----------------------\n1 | 2 | 8091253 | 524288 | t | {}\n2 | 4 | 8091423 | 153600 | t | {}\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/cloudfoundry/bosh/pull/1137\nCommit Summary\nSet default value {} to cloud_properties_json column\nFile Changes\nM bosh-director/db/migrations/director/20141204234517_add_cloud_properties_to_persistent_disk.rb (2)\nPatch Links:\nhttps://github.com/cloudfoundry/bosh/pull/1137.patch\nhttps://github.com/cloudfoundry/bosh/pull/1137.diff\n\u2014\nReply to this email directly or view it on GitHub.\n. you can currently recover vms via bosh cck with recreate option.\n\nwe do have planned enhancement to bosh deploy mechanism to forcefully continue on when bosh deploy cannot communicate with a vm.\nalso we are planning to evaluate our usage of nats and switch to a different mechanism for communication which may completely eliminate credential rotation problem.\nSent from my iPhone\n\nOn Feb 23, 2016, at 4:17 AM, Amin Chawki notifications@github.com wrote:\nUsing BOSH v253\nWe changed the property properties.nats.password https://github.com/cloudfoundry/bosh/blob/master/release/jobs/nats/spec#L33 and redeployed bosh with bosh-init.\nAfter that all vms are in state unresponsive agent. Running bosh stop --hard or bosh recreate is not possible, because bosh tries to get the state of the agent and times out Error 450002: Timed out sending 'get_state'.\nIt would be possible by deleting all deployments or manually fixing the agent on every VM. Is there any other solution for a running system?\n\u2014\nReply to this email directly or view it on GitHub.\n. you can currently recover vms via bosh cck with recreate option.\n\nwe do have planned enhancement to bosh deploy mechanism to forcefully continue on when bosh deploy cannot communicate with a vm.\nalso we are planning to evaluate our usage of nats and switch to a different mechanism for communication which may completely eliminate credential rotation problem.\nSent from my iPhone\n\nOn Feb 23, 2016, at 4:17 AM, Amin Chawki notifications@github.com wrote:\nUsing BOSH v253\nWe changed the property properties.nats.password https://github.com/cloudfoundry/bosh/blob/master/release/jobs/nats/spec#L33 and redeployed bosh with bosh-init.\nAfter that all vms are in state unresponsive agent. Running bosh stop --hard or bosh recreate is not possible, because bosh tries to get the state of the agent and times out Error 450002: Timed out sending 'get_state'.\nIt would be possible by deleting all deployments or manually fixing the agent on every VM. Is there any other solution for a running system?\n\u2014\nReply to this email directly or view it on GitHub.\n. Do you have the timeline when protocol may change?\n\nOn Wed, Mar 2, 2016 at 5:03 PM, Chou Hu notifications@github.com wrote:\n\nThe old stemcell still works now but in future it can not work after the\nprotocols are switched.\nBest Regards,\nAbel HU\n? 2016?3?3??08:59?Dmitriy Kalinin notifications@github.com<mailto:\nnotifications@github.com> ???\nDoes that mean older stemcells will stop working because they dont support\nnewer protocol?\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/1153#issuecomment-191520095>.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1153#issuecomment-191520825.\n. Do you have the timeline when protocol may change?\n\nOn Wed, Mar 2, 2016 at 5:03 PM, Chou Hu notifications@github.com wrote:\n\nThe old stemcell still works now but in future it can not work after the\nprotocols are switched.\nBest Regards,\nAbel HU\n? 2016?3?3??08:59?Dmitriy Kalinin notifications@github.com<mailto:\nnotifications@github.com> ???\nDoes that mean older stemcells will stop working because they dont support\nnewer protocol?\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/1153#issuecomment-191520095>.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1153#issuecomment-191520825.\n. does bosh deploy --recreate not work for you?\n\nSent from my iPhone\n\nOn Mar 4, 2016, at 7:15 AM, Marco Voelz notifications@github.com wrote:\nThe agent writes the file /var/vcap/bosh/etc/blobstore-.json once and never checks if the settings changed afterwards. So rolling blobstore passwords or even changing the blobstore is not possible.\n\u2014\nReply to this email directly or view it on GitHub.\n. does bosh deploy --recreate not work for you?\n\nSent from my iPhone\n\nOn Mar 4, 2016, at 7:15 AM, Marco Voelz notifications@github.com wrote:\nThe agent writes the file /var/vcap/bosh/etc/blobstore-.json once and never checks if the settings changed afterwards. So rolling blobstore passwords or even changing the blobstore is not possible.\n\u2014\nReply to this email directly or view it on GitHub.\n. On Mon, Nov 30, 2015 at 9:57 AM, Dmitriy Kalinin notifications@github.com\nwrote:\nIn stemcell_builder/stages/bosh_ntpd/assets/ntp.conf\nhttps://github.com/cloudfoundry/bosh/pull/1045#discussion_r46177934:\n\n@@ -0,0 +1,13 @@\n+driftfile /var/lib/ntp/ntp.drift\n+\n+statistics loopstats peerstats clockstats\n+filegen loopstats file loopstats type day enable\n+filegen peerstats file peerstats type day enable\n+filegen clockstats file clockstats type day enable\n+\n+restrict -4 default kod notrap nomodify nopeer noquery\n+restrict -6 default kod notrap nomodify nopeer noquery\n+\n+restrict 127.0.0.1\n\ni thought you guys mentioned that restrict 127... causes ntpd to fail\nactually fetching time from remote servers?\nI think restrict 127... should not cause failures when fetching time from\nremote servers.\n\nI have two servers in the NTP pool responding to ~2k NTP queries/sec, and\nmy VM's clock is able to fetch time from its upstream servers w/out\ndrifting more than ~20ms.  Here's the config I use:\n# Our upstream timekeepers; thank you Virginia Tech\n  server ntp-1.vt.edu\n  server ntp-2.vt.edu\n  server ntp-3.vt.edu\n  server time-b.nist.gov\n  # \"Batten down the hatches!\"\n  # see http://support.ntp.org/bin/view/Support/AccessRestrictions\n  restrict default limited kod nomodify notrap nopeer\n  # Amazon AWS doesn't have IPv6, but I restrict it anyway\n  restrict -6 default limited kod nomodify notrap nopeer\n  restrict 127.0.0.0 mask 255.0.0.0\n  restrict -6 ::1\n  # avoid 'out of memory' crash\n  rlimit memlock 128\n  # require at least 1s between packets from an address, rather than\nthe default of 2s\n      # http://lists.ntp.org/pipermail/questions/2010-April/026306.html\n      discard minimum 0\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1045/files#r46177934.\n. \n",
    "andreasmaier": "@drnic have you had a chance to add some tests to this PR?\n. @drnic have you had a chance to add some tests to this PR?\n. Closing this for now. Happy to re-open once the tests are added.\n. Closing this for now. Happy to re-open once the tests are added.\n. @drnic the travis build is erroring out. The issue seems related to https://github.com/eventmachine/eventmachine/issues/248. Apparently the solution to the eventmachine error is to upgrade it to version 1.0.1\n. @drnic the travis build is erroring out. The issue seems related to https://github.com/eventmachine/eventmachine/issues/248. Apparently the solution to the eventmachine error is to upgrade it to version 1.0.1\n. @drnic  We created a story in the bosh icebox for a ruby 2.0 bosh version. Closing this one for now until we have a passing 2.0 build.\n@AndreasMaier / @mmb \n. @drnic  We created a story in the bosh icebox for a ruby 2.0 bosh version. Closing this one for now until we have a passing 2.0 build.\n@AndreasMaier / @mmb \n. @blueboxjesse  We suggest adding an environment variable OPENSTACK_SSL_VERIFY_PEER to the OpenStack CPI that can be used to turn of SSL verification.\n@frodenas Any thoughts?\n@andreasmaier / @mmb \n. @blueboxjesse  We suggest adding an environment variable OPENSTACK_SSL_VERIFY_PEER to the OpenStack CPI that can be used to turn of SSL verification.\n@frodenas Any thoughts?\n@andreasmaier / @mmb \n. @frodenas Agreed a config option is a better solution than an environment variable. If not present it should leave ssl verification on.\n@mmb / @AndreasMaier\n. @frodenas Agreed a config option is a better solution than an environment variable. If not present it should leave ssl verification on.\n@mmb / @AndreasMaier\n. ",
    "jai11": "HI , I am using bosh micro to create bosh VM on AWS.\nI done all the re-search and change allot stem cell . each one got stuck at same point .\nI also tried with bootstrap and it also stuck at same point-\nPlease find the console output-\nroot@2d62e5510169:/microbosh/deployments# bosh micro deploy --update ami-979dc6fe\nUpdating mybosh/micro_bosh.yml' tohttps://54.210.142.162:25555' (type 'yes' to continue): yes\nWill deploy due to stemcell changes\n  Started prepare for update\n  Started prepare for update > Preserving stemcell. Done (00:00:00)\nStarted deploy micro bosh\n  Started deploy micro bosh > Using existing stemcell. Done (00:00:00)\n  Started deploy micro bosh > Creating VM from ami-979dc6fe\n. Done (00:00:49)\n  Started deploy micro bosh > Waiting for the agent\nUnable to connect to Bosh agent. Check logs for more details.\nOutput from bootstrap:-\nUse bundle show [gemname] to see where a bundled gem is installed.\nbundle exec bosh micro deployment firstbosh\nfatal: Not a git repository (or any of the parent directories): .git\nDeployment set to '/.microbosh/deployments/firstbosh/micro_bosh.yml'\nbundle exec bosh -n micro deploy --update-if-exists ami-7017b018\nfatal: Not a git repository (or any of the parent directories): .git\nfatal: Not a git repository (or any of the parent directories): .git\n  Started deploy micro bosh\n  Started deploy micro bosh > Using existing stemcell. Done (00:00:00)\n  Started deploy micro bosh > Creating VM from ami-7017b018. Done (00:00:30)\n  Started deploy micro bosh > Waiting for the agentUnable to connect to Bosh agent. Check logs for more details.\n/usr/local/lib/ruby/1.9.1/rake/file_utils.rb:53:in block in create_shell_runner': Command failed with status (1): [bundle exec bosh -n micro deploy --update-...] (RuntimeError)\n        from /usr/local/lib/ruby/1.9.1/rake/file_utils.rb:45:incall'\n        from /usr/local/lib/ruby/1.9.1/rake/file_utils.rb:45:in sh'\n        from /usr/local/lib/ruby/gems/1.9.1/gems/bosh-bootstrap-0.13.2/lib/bosh-bootstrap/cli/helpers/bundle.rb:11:inblock in bundle'\n        from /usr/local/lib/ruby/gems/1.9.1/gems/bundler-1.6.5/lib/bundler.rb:235:in block in with_clean_env'\n        from /usr/local/lib/ruby/gems/1.9.1/gems/bundler-1.6.5/lib/bundler.rb:222:inwith_original_env'\n        from /usr/local/lib/ruby/gems/1.9.1/gems/bundler-1.6.5/lib/bundler.rb:228:in `with_clean_env'\n        from /usr/local/lib/ruby/gems/1.9.1/gems/bosh-bootstrap-0.13.2/lib/bosh-bootstrap/cli/helpers/bun\n\nlogs:-\nI, [2014-09-27T03:14:52.975455 #2252]  INFO -- : HTTP server is starting on port 25888...\nE, [2014-09-27T03:14:53.420720 #2252] ERROR -- : Sinatra::NotFound\nD, [2014-09-27T03:15:42.000862 #2252] DEBUG -- : (0.000234s) PRAGMA foreign_keys = 1\nD, [2014-09-27T03:15:42.001000 #2252] DEBUG -- : (0.000032s) PRAGMA case_sensitive_like = 1\nD, [2014-09-27T03:15:42.001398 #2252] DEBUG -- : (0.000296s) PRAGMA table_info('registry_instances')\nD, [2014-09-27T03:15:42.002487 #2252] DEBUG -- : (0.000207s) SELECT * FROM registry_instances WHERE (instance_id = 'i-54d84bb9') LIMIT 1\nD, [2014-09-27T03:15:42.003390 #2252] DEBUG -- : (0.000125s) SELECT COUNT(*) AS 'count' FROM registry_instances WHERE (instance_id = 'i-54d84bb9') LIMIT 1\nD, [2014-09-27T03:15:42.003879 #2252] DEBUG -- : (0.000072s) SELECT sqlite_version() LIMIT 1\nD, [2014-09-27T03:15:42.004034 #2252] DEBUG -- : (0.000060s) BEGIN\nD, [2014-09-27T03:15:42.004526 #2252] DEBUG -- : (0.000238s) INSERT INTO registry_instances (instance_id, settings) VALUES ('i-54d84bb9', '{\"vm\":{\"name\":\"vm-d1930349-1b16-4c38-aee0-3f667247420f\"},\"agent_id\":\"bm-b92fb0f6-a42a-4a2e-bae7-0c2c59e2c519\",\"networks\":{\"bosh\":{\"cloud_properties\":{},\"netmask\":null,\"gateway\":null,\"ip\":null,\"dns\":null,\"type\":\"dynamic\",\"default\":[\"dns\",\"gateway\"]},\"vip\":{\"ip\":\"54.210.142.162\",\"type\":\"vip\",\"cloud_properties\":{}}},\"disks\":{\"system\":\"/dev/sda1\",\"ephemeral\":\"/dev/sdb\",\"persistent\":{}},\"env\":{\"bosh\":{\"password\":null}},\"ntp\":[],\"blobstore\":{\"provider\":\"local\",\"options\":{\"blobstore_path\":\"/var/vcap/micro_bosh/data/cache\"}},\"mbus\":\"https://vcap:b00tstrap@0.0.0.0:6868\"}')\nD, [2014-09-27T03:15:42.004971 #2252] DEBUG -- : (0.000121s) SELECT * FROM registry_instances WHERE (id = 1) LIMIT 1\nD, [2014-09-27T03:15:42.007517 #2252] DEBUG -- : (0.002390s) COMMIT\nD, [2014-09-27T03:18:57.866806 #2252] DEBUG -- : (0.000314s) SELECT * FROM registry_instances WHERE (instance_id = 'i-54d84bb9') LIMIT 1\nI, [2014-09-27T03:32:36.222613 #2252]  INFO -- : BOSH Registry shutting down...\nCan you please help me.\nAnd as per the documents they selected non-vpc but now AWS by default gives VPC only no option for non-vpc.\nIs this for i am getting stuck every time at same point \"waiting for the agent\"\n. I, [2014-09-29T13:00:05.486111 #1596] [0x605068]  INFO -- : discovered bosh ip=54.210.142.162\nD, [2014-09-29T13:02:12.844449 #1596] [0x605068] DEBUG -- : tcp socket 54.210.142.162:22 #\nD, [2014-09-29T13:02:14.285618 #1596] [0x605068] DEBUG -- : tcp socket 54.210.142.162:22 is readable\nI, [2014-09-29T13:03:14.286249 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:17.131385 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:18.132972 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:20.340142 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:21.340856 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:23.677199 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:24.677681 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:26.942301 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:27.942747 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:30.086336 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:31.086835 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:33.515320 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:34.515754 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:36.863151 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:37.863632 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:40.089921 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:41.090339 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:43.456335 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:44.460076 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:46.685346 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:47.685778 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:49.878479 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\nI, [2014-09-29T13:03:50.884089 #1596] [0x605068]  INFO -- : Starting SSH session for port forwarding to vcap@54.210.142.162...\nD, [2014-09-29T13:03:53.209519 #1596] [0x605068] DEBUG -- : ssh start vcap@54.210.142.162 failed: #\n. hi i am using the latest stable stemcell.I can ssh vcap user .\n. actually if I do the same steps from my aws VM to deploy Micro bosh to aws it successfully deploy.\nBut same steps through my mcahine always stuck on step \"waiting for agent\" after creating VM on aws.\nused latest old stemcell , done allot r&d but still always stuck at same step \"waiting for agent\"\nIt will be great if you can please help me out.\n. and one more thing I also used bosh-bootstrap tool to deploy micro bosh. but the tool also stuck at step \"waiting for agent\" and time out\n. Any update please\n. ",
    "cf-zapier": "You're a rockstar!\nOn Friday, July 12, 2013, Martin Englund wrote:\n\nYou mean like this: bbatsov/rubocop#355https://github.com/bbatsov/rubocop/issues/355\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/359#issuecomment-20888432\n.\n\nYou received this message because you are subscribed to the Google Groups\n\"Cloud Foundry Accounts\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an\nemail to cfaccounts+unsubscribe@pivotallabs.com .\nFor more options, visit\nhttps://groups.google.com/a/pivotallabs.com/groups/opt_out.\n. \n",
    "mmatuska": "I currently don't have resources to continue on this. We have figured this out during our failing tests with trying to deploy micro bosh on vCloud. The problem can actually be read from the code:\nin bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager/ the files aws.rb and openstack.rb define the following variables: @ssh_user @ssh_port @ssh_key\nvsphere.rb and vcloud.rb don't\nInstance manager allways calls wait_until_agent_ready():\nbosh/blob/master/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb line 431:\n``` ruby\n    def wait_until_agent_ready #XXX >> agent_client\n      remote_tunnel(@registry_port)\n  wait_until_ready(\"agent\") { agent.ping }\nend\n\n```\nThis function needs these variables and is located in:\nbosh/blob/master/bosh_cli_plugin_micro/lib/deployer/helpers.rb line 57\nThere are 2 possible solutions to this:\na) bypass remote_tunnel()\nb) implement the functionality to vsphere.rb and vcloud.rb\n. I currently don't have resources to continue on this. We have figured this out during our failing tests with trying to deploy micro bosh on vCloud. The problem can actually be read from the code:\nin bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager/ the files aws.rb and openstack.rb define the following variables: @ssh_user @ssh_port @ssh_key\nvsphere.rb and vcloud.rb don't\nInstance manager allways calls wait_until_agent_ready():\nbosh/blob/master/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb line 431:\n``` ruby\n    def wait_until_agent_ready #XXX >> agent_client\n      remote_tunnel(@registry_port)\n  wait_until_ready(\"agent\") { agent.ping }\nend\n\n```\nThis function needs these variables and is located in:\nbosh/blob/master/bosh_cli_plugin_micro/lib/deployer/helpers.rb line 57\nThere are 2 possible solutions to this:\na) bypass remote_tunnel()\nb) implement the functionality to vsphere.rb and vcloud.rb\n. ",
    "hgadgil": "@mmatuska & @tsaleh I've made this change locally, added a unit test (as requested) and I'm submitting a new all-in-one pull request for the same (in the interest of getting this fix in)... Can you cancel this pull request?\nHope thats fine.\nThanks,\n. New pull request # 385\n. ",
    "lukebakken": "\"Based on the diff in this PR, I'm not certain if you wanted to avoid the AWS key issue, or if you actually wanted to force the stemcell:micro task to use a locally created Micro Bosh release.\"\nBoth ... glad to hear things are getting simplified.\n. \"Based on the diff in this PR, I'm not certain if you wanted to avoid the AWS key issue, or if you actually wanted to force the stemcell:micro task to use a locally created Micro Bosh release.\"\nBoth ... glad to hear things are getting simplified.\n. ",
    "abic": "The change in test does not reflect the change in behavior. For instance if I take your test changes without changing the implementation it still passes. Please update the test, including the title of the test to demonstrate the new desired effect. The test should fail if we have a regression in this behavior.\n. It appears that this change will allow for a deployment to silently skip creating unallocated idle VMs that are unable to reserve a dynamic IP. This change is definitely addressing an implementation bug that we would like to resolve and we thank you for digging into this. What I'd rather see would be to hold off on doing any network reservations for both allocated VMs and unallocated VMs until the 'Binding Instance Networks' stage in the Preparer and maybe rename it to just 'Binding Networks Reservations'. This would be a bigger change though and will require us to take a closer look at how we construct the DeploymentPlan.\nJeff and Matt (@mmb)\n. ",
    "cfdreddbot": "Hey kei-yamazaki!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey hgadgil!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey oppegard!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey elliterate!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey ryantang!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey chrisblackburn!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey drogus!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey oppegard!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey kokuboyuichi!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey allomov!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey duritong!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey reneedv!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey dingyin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey goonzoid!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey Justin-Yu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey singerdmx!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey molteanu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey molteanu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey zoujin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey akranga!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey akranga!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey peterellisjones!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey dkoper!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey wdneto!\nWe have confirmed this pull request is covered by an existing CLA on file with the project.\nThank you for the pull request.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrdavidlaing!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrdavidlaing!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey shinji62!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey shinji62!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey molteanu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dkoper!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey dkoper!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey dkoper!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey thecadams!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey thecadams!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey liuhewei!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey liuhewei!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xoebus!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey molteanu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey molteanu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey liuhewei!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey liuhewei!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey sra!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey sra!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey flavorjones!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey amhuber!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey amhuber!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey thecadams!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey thecadams!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ipoddar-ibm!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ipoddar-ibm!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey amhuber!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey amhuber!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey danhigham!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey danhigham!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey skibum55!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey shinji62!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey shinji62!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey nterry!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey nterry!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey nterry!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey nterry!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey nterry!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey luan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey adamstegman!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey petardp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey petardp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey petardp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey petardp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey skibum55!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey amhuber!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xtreme-andrei-dinin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey xtreme-andrei-dinin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey xtreme-andrei-dinin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey Kaixiang!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey krumts!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey krumts!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey xtreme-andrei-dinin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey xtreme-andrei-dinin!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mikedillion!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dkoper!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dkoper!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dkoper!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey petarz!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate.\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey datianshi!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey uzzz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey uzzz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey adamstegman!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey adamstegman!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmc2004jp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mmc2004jp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mmc2004jp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mmc2004jp!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mariash!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mariash!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ronakbanka!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ronakbanka!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey cyrille-leclerc!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey cyrille-leclerc!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey voelzmo!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey jfuerth!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey Amit-PivotalLabs!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrwangxc!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mrwangxc!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mrwangxc!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey jfuerth!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey jfuerth!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey jfuerth!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey jfuerth!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey bonzofenix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey bonzofenix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dsboulder!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey dsboulder!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey mrwangxc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrwangxc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmb!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmb!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmb!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmb!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey julweber!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey julweber!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\n. Hey vito!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey vito!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey pivotal-graeme-davison!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey pivotal-graeme-davison!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey vito!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey vito!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey monkeyherder!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey monkeyherder!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey jrbudnack!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey rkoster!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey schwarzmx!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey schwarzmx!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey JamesClonk!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey JamesClonk!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mmc2004jp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey pmenglund!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey pmenglund!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey raespinosa!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey raespinosa!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey vlerenc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey vlerenc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey wayneeseguin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey wayneeseguin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey uzzz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey uzzz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey benmoss!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey benmoss!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drich10!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey drich10!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey evanfarrar!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey evanfarrar!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey jamiemonserrate!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey jamiemonserrate!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey jamiemonserrate!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dthaluru!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrdavidlaing!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mrdavidlaing!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey Jonty!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey Jonty!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey voelzmo!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey guoger!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey njbennett!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey guoger!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zaksoup!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey AbelHu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey kaleo211!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey AbelHu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey hashmap!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey loewenstein!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey leizhu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey aaronshurley!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey aaronshurley!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey Kaixiang!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey Kaixiang!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey calebamiles!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey calebamiles!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey calebamiles!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey calebamiles!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey byllc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dingyin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey jianqiu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey jianqiu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey zhang-hua!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey fabianschwarzfritz!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey fabianschwarzfritz!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey fabianschwarzfritz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey fabianschwarzfritz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey hashmap!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey lwoydziak!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey lwoydziak!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey xingzhou!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey barthy1!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey syslxg!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey syslxg!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey filefrog!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey filefrog!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey leizhu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey leizhu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey leizhu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey leizhu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey premist!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey premist!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey guoger!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey guoger!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey dpb587-pivotal!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ronakbanka!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ronakbanka!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey tomoe!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey mattcui!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey flawedmatrix!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey yuanzhao!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey yuanzhao!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey AbelHu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey AbelHu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey liuweichu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey liuweichu!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey geofffranks!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey geofffranks!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ChaosEternal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey ChaosEternal!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\n. Hey omazhary!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey omazhary!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey omazhary!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey omazhary!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey omazhary!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey omazhary!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey evanphx!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you've already signed the CLA.\n. Hey simonjjones!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jianqiu!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey kalambet!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey allomov!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jianqiu!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey jianqiu!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey evandbrown!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dpb587!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ronakbanka!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dlapiduz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey AbelHu!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey krishnanrs!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey Petahhh!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey alext!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dpb587!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey forrestsill!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey coreyti!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey krishnanrs!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dickeyf!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey forrestsill!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey forrestsill!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey sharms!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey sharms!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey mattcui!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey aduffeck!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey cunnie!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey loewenstein!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey coreyti!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dcarley!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey evandbrown!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey evandbrown!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey combor!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey friegger!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jianqiu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jianqiu!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey thardeck!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey gossion!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey evandbrown!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey loewenstein!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey loewenstein!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cunnie!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cunnie!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey markfussell!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey gossion!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey geofffranks!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey johnsonj!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey MatthiasWinzeler!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey lnguyen!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey holgero!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey voelzmo!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cunnie!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey aduffeck!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey istvanballok!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey istvanballok!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey manno!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey alext!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey alext!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cdutra!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey Samze!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ljfranklin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey xiujiao!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey paolostivanin!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey paolostivanin!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey paolostivanin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey voelzmo!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey paolostivanin!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey robertjsullivan!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey robertjsullivan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey robertjsullivan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey pivotal-rmierzwiak!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey frodenas!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey frodenas!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey camelpunch!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey camelpunch!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey loewenstein!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey christopherclark!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey drnic!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Lafunamor!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey bpicode!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey bpicode!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey simonkey007!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey simonkey007!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey simonkey007!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey coreyti!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey coreyti!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dpb587!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey coreyti!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dpb587!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey manno!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Everlag!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey domdom82!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey domdom82!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey domdom82!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey wfernandes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey friegger!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey KaiHofstetter!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey aramprice!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey dannysullivan!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey deniseirvin!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey deniseirvin!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey petergtz!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey beyhan!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey achawki!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey achawki!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey Kiemes!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ansd!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey peterellisjones!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey manno!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey BenChapman!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ansd!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jmcarp!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey knm3000!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey pivotal-jamil-shamy!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey jamesjoshuahill!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ebeer!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey cppforlife!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey friegger!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey ansd!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey s4heid!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey barthy1!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey KaiHofstetter!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey muralisc!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey cunnie!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey hoegaarden!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey pc-jedi!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate).\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey pc-jedi!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey manno!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey jaresty!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey bstick12!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey pivotal-jamil-shamy!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please sign the appropriate CLA (individual or corporate), and send to: contributors@cloudfoundry.org.\nWhen sending signed CLA please provide your github username in case of individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nIf you are confident that you're covered under a Corporate CLA, please make sure you've publicized your membership in the appropriate Github Org, per these instructions.\nOnce you've publicized your membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey pivotal-jamil-shamy!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey andyliuliming!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey muralisc!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey voelzmo!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey kreamyx!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey degaurab!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey keymon!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. Hey rosenhouse!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. \nHey flyinprogrammer!\nThanks for submitting this pull request!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please have everyone sign the appropriate CLA (individual or corporate), and send to: contributors@cloudfoundry.org.\nWhen sending a signed CLA please provide your github username in case of an individual CLA or the list of github usernames that can make pull requests on behalf of your organization.\nThe following github user @flyinprogrammer has not signed the appropriate CLA.\nIf you are confident that everyone listed is covered under a Corporate CLA, please make sure everyone has publicized their membership in the appropriate Github Org, per these instructions.\nOnce everyone has publicized their membership, one of the owners of this repository can close and reopen this pull request, and dreddbot will take another look.\n. Hey flyinprogrammer!\nThanks for submitting this pull request! I'm here to inform the recipients of the pull request that you and the commit authors have already signed the CLA.\n. :white_check_mark: Hey MatthiasWinzeler! The comit authors and yourself have already signed the CLA.. :white_check_mark: Hey kieron-pivotal! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey jaresty! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey jaresty! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey luan! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey KaiHofstetter! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey ansd! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey kreamyx! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey aclevername! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey tylerschultz! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey voelzmo! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey miguelverissimo! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey beyhan! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey beyhan! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey ansd! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey ansd! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey ansd! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey s4heid! The commit authors and yourself have already signed the CLA.\n. \n:x: Hey zts-pash!\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA). Click here for details on the CLA process.\nThe following github user @zts-pash is not covered by a CLA.\nAfter the CLA process is complete, this pull request will need to be closed & reopened. DreddBot will then validate the CLA(s).\n. :white_check_mark: Hey zts-pash! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey BeckerMax! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey beyhan! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey ansd! The commit authors and yourself have already signed the CLA.\n. :white_check_mark: Hey charleshansen! The commit authors and yourself have already signed the CLA.\n. ",
    "kei-yamazaki": "I finished the CLA (maybe..\nAnd fixed #install_dependencies issue.\n. ",
    "Rikbruggink": "Hello John,\nI am using the latest bosh version now where khadgils commit is merged and now using the latest microbosh stemcell the agent is responding but it now fails on mounting the persistence disk.\ncli output:\nroot@ubuntu:~/vchsbosh/bosh/deployments# bosh micro deploy micro-bosh-stemcell-latest-vsphere-esxi-ubuntu.tgz\nDeploying new micro BOSH instance vcloud/micro_bosh.yml' tohttps://192.168.1.10:25555' (type 'yes' to continue): yes\nVerifying stemcell...\nFile exists and readable\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OK\nManifest not found in cache, verifying tarball...\nRead tarball\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OK\nManifest exists\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OK\nStemcell image file\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OK\nWriting manifest to cache...\nStemcell properties\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OK\nStemcell info\nName:\u00a0\u00a0\u00a0 micro-bosh-stemcell\nVersion: 896\nDeploy Micro BOSH\n\u00a0 unpacking stemcell (00:00:03)\n\u00a0 uploading stemcell (00:02:39)\n\u00a0 creating VM from urn:vcloud:catalogitem:8e470bfe-329c-48bb-96cb-5f570aa26759 (00:01:20)\n\u00a0 waiting for the agent (00:00:21)\nCreate disk\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |oooooooo\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 | 4/11 00:04:30\u00a0 ETA: 00:07:38/usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/abstract_response.rb:48:in return!': 400 Bad Request (RestClient::BadRequest)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/vcd_client.rb:135:inblock in invoke'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:228:in call'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:228:inprocess_result'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:178:in block in transmit'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/rubies/ruby-1.9.3-p448/lib/ruby/1.9.1/net/http.rb:746:instart'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:172:in transmit'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:64:inexecute'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rest-client-1.6.7/lib/restclient/request.rb:33:in execute'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/vcd_client.rb:133:ininvoke'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/vcd_client.rb:151:in invoke_and_wait'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps/attach_detach_disk.rb:7:inperform'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps.rb:37:in next'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/cloud.rb:209:inblock in attach_disk'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps.rb:44:in call'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps.rb:44:inblock in perform'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_common/lib/common/thread_formatter.rb:46:in with_thread_name'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps.rb:41:inperform'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/steps.rb:58:in perform'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/cloud.rb:269:insteps'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/gems/bosh_vcloud_cpi-0.5.0/lib/cloud/vcloud/cloud.rb:202:in attach_disk'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:308:inattach_disk'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:345:in update_persistent_disk'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:156:inblock in create'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:92:in step'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:155:increate'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:112:in block in create_deployment'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:105:inwith_lifecycle'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/deployer/instance_manager.rb:111:in create_deployment'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli_plugin_micro/lib/bosh/cli/commands/micro.rb:172:inperform'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli/lib/cli/command_handler.rb:57:in run'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli/lib/cli/runner.rb:59:inrun'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli/lib/cli/runner.rb:18:in run'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /root/vchsbosh/bosh/bosh_cli/bin/bosh:7:in'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/bin/bosh:19:in load'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/bin/bosh:19:in'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/bin/ruby_noexec_wrapper:14:in eval'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from /usr/local/rvm/gems/ruby-1.9.3-p448/bin/ruby_noexec_wrapper:14:in'\ntail of debug log output from the new job:\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ovfenv:PlatformSection\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ovfenv:KindVMware ESXi/ovfenv:Kind\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ovfenv:Version5.1.0/ovfenv:Version\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ovfenv:VendorVMware, Inc./ovfenv:Vendor\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ovfenv:Localeen/ovfenv:Locale\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 /ovfenv:PlatformSection\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n/ve:EthernetAdapterSection\n\u00a0\u00a0\u00a0 /ovfenv:Environment\n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 false\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 false\n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\nD, [2013-08-16T12:29:54.007956 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST REQ GET https://31.200.210.84/api/entity/urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8 {:Accept=>\"application/+xml;version=5.1\", :content_type=>\"/*\", :x_vcloud_authorization=>\"5f5eABNkYYuMYmXC7/R1V6w5vjNZL7jRzv1MUTp4Qh0=\"} {\"vcloud-token\"=>\"5f5eABNkYYuMYmXC7%2FR1V6w5vjNZL7jRzv1MUTp4Qh0%3D\", \"Path\"=>\"%2F\"}\nD, [2013-08-16T12:29:54.038232 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST RES 200 {:date=>\"Fri, 16 Aug 2013 10:29:57 GMT\", :content_type=>\"application/vnd.vmware.vcloud.entity+xml;version=5.1\", :content_encoding=>\"gzip\", :content_length=>\"311\"} <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\u00a0\u00a0\u00a0 \n\nD, [2013-08-16T12:29:54.039279 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST REQ GET https://31.200.210.84/api/disk/43eadca3-7dea-4f39-a1ec-0087a79e8df8 {:Accept=>\"application/+xml;version=5.1\", :content_type=>\"/*\", :x_vcloud_authorization=>\"5f5eABNkYYuMYmXC7/R1V6w5vjNZL7jRzv1MUTp4Qh0=\"} {\"vcloud-token\"=>\"5f5eABNkYYuMYmXC7%2FR1V6w5vjNZL7jRzv1MUTp4Qh0%3D\", \"Path\"=>\"%2F\"}\nD, [2013-08-16T12:29:54.080474 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST RES 200 {:date=>\"Fri, 16 Aug 2013 10:29:57 GMT\", :content_type=>\"application/vnd.vmware.vcloud.disk+xml;version=5.1\", :content_encoding=>\"gzip\", :content_length=>\"592\"} <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 \n\nD, [2013-08-16T12:29:54.081155 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : STEP VCloudCloud::Steps::AttachDetachDisk\nD, [2013-08-16T12:29:54.082632 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST REQ POST https://31.200.210.84/api/vApp/vm-a97d898e-1756-4602-8380-393372b7bb2c/disk/action/attach {:Accept=>\"application/*+xml;version=5.1\", :content_type=>\"application/vnd.vmware.vcloud.diskAttachOrDetachParams+xml\", :x_vcloud_authorization=>\"5f5eABNkYYuMYmXC7/R1V6w5vjNZL7jRzv1MUTp4Qh0=\"} {\"vcloud-token\"=>\"5f5eABNkYYuMYmXC7%2FR1V6w5vjNZL7jRzv1MUTp4Qh0%3D\", \"Path\"=>\"%2F\"} \n\u00a0  \n\nD, [2013-08-16T12:29:54.174318 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REST RES 400 {:date=>\"Fri, 16 Aug 2013 10:29:57 GMT\", :content_type=>\"application/vnd.vmware.vcloud.error+xml;version=5.1\", :content_length=>\"446\"} <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\nE, [2013-08-16T12:29:54.174909 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] ERROR -- : FAIL attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8) 400 Bad Request\nD, [2013-08-16T12:29:54.175056 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REV ROLLBACK VCloudCloud::Steps::AttachDetachDisk\nD, [2013-08-16T12:29:54.175156 #18376] [attach_disk(urn:vcloud:vm:a97d898e-1756-4602-8380-393372b7bb2c, urn:vcloud:disk:43eadca3-7dea-4f39-a1ec-0087a79e8df8)] DEBUG -- : REV CLEANUP VCloudCloud::Steps::AttachDetachDisk\n. ",
    "thansmann": "Reading through the comments, it looks like everyone thinks there should be accidental deploy protection. It's clear the CLI alone is not sufficient to prevent it. \nI agree with Sean and Ryan - the director should be configurable to reject (or warn) on deploys without UUID. Ops is often done under pressure, having safety interlocks help prevent career ending mistakes. \n. @mikepatterson @blueboxjesse @tsaleh, we'd like to get this pull request going again. Because @dajulia3 and I are new to the issue we'd like to plan some time today or tomorrow for a Hangout to talk about what we'll need to get going?\n@thansmann && @dajulia3 \n. @drnic @jbayer LGTU - waiting for an answer the 'rackspace' intentions - we're good either way, just want to clarify. \nJZ && TH\n. @drnic, @frodenas comments makes us leery, we've got a request out to @tsaleh for a product ruling.\n. Hi @drnic,\nWe're looking at this and bosh/bosh_cli_plugin_micro/lib/bosh/deployer/instance_manager.rb defines exists?:\n    def exists?\n      [state.vm_cid, state.stemcell_cid, state.disk_cid].any?\n    end\nIt checks that state.vm_cid is defined. Your PR is the same behavior we had, but narrows the cases where we'll try to update. Is there an issue checking the other two?\n@thansmann && @ytolsk \n. @drnic \n'exists?' is old, but checking all three is sorta new - added in Oct. The pervious behavior was identical to this pull request. \n@thansmann && @ytolsk \n. @drnic we did some digging on why we made this change, there are two stories related to the issue. Unfortunately they are in our old, non-public tracker. Here's a gist with the reasons for the change: \nhttps://gist.github.com/thansmann/e56d329645dd5f83b3dc\n(tl;dr this fixed an issue that caused Pivotal CF a lot of headache).\nWe might have a bug here instead of a pull request. Do you have a test that replicates the issue? \nAlso re: testing, we ran your new tests and they pass with the old code and new code - that makes us a bit uneasy as there should be red case to go with the green case. \n@thansmann @ytolsk \n. @drnic - you (and your fix) are right. In AWS (but not vSphere) we are testing for too many parameters. This has been converted to a bosh bug (https://www.pivotaltracker.com/story/show/62192362) and is prioritized for this week.\n@thansmann && @dajulia3 \n. Is 'rackspace' intentionally left out here? We saw mentions of rackspace in the specs, but we'll raise here if it's selected. @drnic \n. ",
    "stevenbzb": "hi drnic, thank for your comment.  Here is some text of  my deployments/cf.yml:\nname: cf\ndirector_uuid: 9522af13-558b-43a2-90a9-cf788a40e94a\nreleases:\n- name: appcloud\n  version: latest\ncompilation:\n  workers: 6\n  network: default\n  reuse_compilation_vms: true\n  cloud_properties:\n    instance_type: m1.medium\nupdate:\n  canaries: 1\n  canary_watch_time: 30000-600000\n  update_watch_time: 30000-600000\n  max_in_flight: 4\n  max_errors: 1\nnetworks:\n- name: floating\n  type: vip\n  cloud_properties: {}\n- name: default\n  type: dynamic\n  cloud_properties:\n    security_groups:\n  - default\n  - cf\nresource_pools:\n- name: dea\n  network: default\n  size: 1\n  stemcell:\n    name: bosh-stemcell\n    version: latest\n  cloud_properties:\n    instance_type: m1.small\n- name: medium\n  network: default\n  size: 2\n  stemcell:\n    name: bosh-stemcell\n    version: latest\n  cloud_properties:\n    instance_type: m1.medium\njobs:\n- name: core\n  template:\n  - syslog_aggregator\n  - nats\n  - postgres\n  - health_manager_next\n  - collector\n  - debian_nfs_server\n  - uaa\n  - login\n    instances: 1\n    resource_pool: medium\n    persistent_disk: 8192\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    properties:\n    db: databases\n- name: api\n  template:\n  - cloud_controller_ng\n  - gorouter\n    instances: 1\n    resource_pool: medium\n    networks:\n    - name: default\n      default:\n  - dns\n  - gateway\n    - name: floating\n      static_ips:\n  - 10.141.123.245\n    properties:\n    db: databases\n- name: dea\n  template:\n  - dea_next\n    instances: 1\n    resource_pool: dea\n    networks:\n  - name: default\n    default: [dns, gateway]\nproperties:\n  domain: huawei.com\n  system_domain: huawei.com\n  system_domain_organization: \"mycloud\"\n  app_domains:\n    - huawei.com\nnetworks:\n    apps: default\n    management: default\nnats:\n    address: 0.core.default.cf.microbosh\n    port: 4222\n    user: nats\n    password: \"c1oudc0w\"\n    authorization_timeout: 5\nrouter:\n    port: 8081\n    status:\n      port: 8080\n      user: gorouter\n      password: \"c1oudc0w\"\ndea: &dea\n    max_memory: 4096\n    memory_mb: 4084\n    memory_overcommit_factor: 4\n    disk_mb: 4096\n    disk_overcommit_factor: 4\ndea_next: *dea\nservice_lifecycle:\n    serialization_data_server:\n    - 0.core.default.cf.microbosh\nsyslog_aggregator:\n    address: 0.core.default.cf.microbosh\n    port: 54321\nserialization_data_server:\n    port: 8080\n    logging_level: debug\n    upload_token: 8f7COGvThwlmulIzAgOHxMXurBrG364k\n    upload_timeout: 10\ncollector:\n    deployment_name: cf\n    use_tsdb: false\n    use_aws_cloudwatch: false\n    use_datadog: false\nnfs_server:\n    address: 0.core.default.cf.microbosh\n    #network: \"*.cf.microbosh\"\n    #idmapd_domain: iad1\ndebian_nfs_server:\n    no_root_squash: true\ndatabases: &databases\n    db_scheme: postgres\n    address: 0.core.default.cf.microbosh\n    port: 5524\n    roles:\n      - tag: admin\n        name: ccadmin\n        password: \"c1oudc0w\"\n      - tag: admin\n        name: uaaadmin\n        password: \"c1oudc0w\"\n    databases:\n      - tag: cc\n        name: ccdb\n        citext: true\n      - tag: uaa\n        name: uaadb\n        citext: true\n.................\n.................\nCommand \"grep '<%= p(\"' jobs//templates/ | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\nccdb_postgres\ncloud_controller_ng\ncollector\ndashboard\ndea_logging_agent\ndea_next\ndebian_nfs_server\ngorouter\nhealth_manager_next\nloggregator\nlogin\nsaml_login\nsyslog_aggregator\nuaa\nCommand \"grep 'properties:' jobs/*/spec | awk '{print $1}' | cut -d / -f 2 | uniq\" return:\nccdb_postgres\ncloud_controller_ng\ncollector\ndashboard\ndea_logging_agent\ndea_next\ndebian_nfs_server\ngorouter\nhealth_manager_next\nloggregator\nlogin\nnats\npostgres\nsaml_login\nsyslog_aggregator\nuaa\n. Hi drnic, thank you very much, after move postgres & nfs into a shared job (say data), Error 80010 doesn't appear again. I got a new error say \"OpenStack API Request Entity Too Large error\" when compiling packages libpq/5 & rootfs_lucid64/1 & debian_nfs_server/3. I will continue to work on it. It's a tough work to deploy cf on openstack. Thanks a lot for your help.\n. ",
    "calebamiles": "Hi @drnic,\nThanks for submitting this PR. Could you break out the #color_value method into a separate PR. If possible it would be nice to avoid using default arguments despite the fact that the code is more concise since it is much easier to reason about. Could you please also replace your uses of stub with allow and your uses of should with expect. If you could also rebase against the develop branch that would be great. Once those changes are made we should be happy to merge in your pull request.\nCF Community Pair\n@calebamiles, @drich10 \n. Hi @frodenas, \nThanks for submitting this pull request. Would it be possible for you to refactor your specs a bit to use instance_double rather than double, allow rather than stub, and expect().to receive or expect().to have_recieved rather than the should versions. With those changes we would be happy to merge this in.\nCF Community Pair\n@calebamiles, @drich10 \n. Merged in 8e255c757bcbaa0c1ff4e0f2434b29cb259c98d8, closing.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Merged in aa5ab1f10d6f2bf62858e0edecdffe503b2e37d5, closing.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hello @frodenas,\nThanks for submitting this PR. We believe that this functionality should also be tested with an additional CPI lifecycle test. Additionally, if you could also update your PR to use Rspec 3 style\nexpectations that would be fantastic.\nCF Community Pair\n@calebamiles, @jfmyers9 \n. Merged in e0b430d7b4ed5852036483c3d903b2c33a7dff0f, closing.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hi @frodenas,\nThanks so much for this pull request. Could you add the tests requested in the inline notes. Also if possible could you switch from using double to instance_double and stub to allow as per our new spec conventions.\nCF Community Pair\n@calebamiles, @drich10 \n. Hello @amhuber,\nThanks for submitting this pull request! There are few issues that need to be resolved. We try to use instance_double or class_double rather than double when creating specs. Also given the type of functionality this PR adds, a test would probably need to be added to the BATs and perhaps the CPI lifecycle tests as well. It also seems that there is no default set for boot_from_volume, was that intentional? Could you also rebase your pull request against the develop branch? Thanks again for the request!\nCF Community Pair,\n@calebamiles @drich10 \n. Hello @drnic,\nWe're still waiting for the additional test coverage...without it we cannot merge your PR.\nThanks,\n@calebamiles  and @maximilien\n. Merged in 69c0cee17dac719cfa338bd97636c1264c6f9cd7, closing.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hello @rkoster,\nThanks for submitting your pull request! Could you split out the RedactedParams and the ExconLoggingInstrumentor classes into two separate files, then we should be happy to merge this feature in.\nCF Community Pair\n@calebamiles @drich10 \n. Hello @rkoster,\nWe are currently waiting for a green CI run before we can merge in your changes. Hopefully we will be able to merge in this PR soon.\nCF Community Pair,\n@calebamiles, @jfmyers9 \n. Hello @yudai,\nIt seems as if this problem has been resolved in master. If the current implementation does not address the bug we will be happy to open a new GitHub issue.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hi @drnic,\nThank you for addressing this issue. Unfortunately, we will be unable to merge in your pull request without appropriate test coverage. Once you have added some tests we would be happy to merge your work in.\nCF Community Pair\n@calebamiles, @drich10 \n. Hi @drnic,\nThanks for your willingness to improve the test coverage; that this is a bug fix only adds weight to the argument that the previous test coverage was insufficient.\nCF Community Pair\n@calebamiles, @drich10 \n. Hello @grenzr,\nThank you for submitting this pull request. As previously mentioned, this pull request is essentially identical to pull request 521. Since test coverage would have to be added before this pull request could be merged we are closing it. Please don't hesitate to create a new pull request if 521 does not provide the functionality you desire and please include appropriate test coverage.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hello @cf-blobstore-eng, we had a few questions about this pull request:\n- Is there any reason why #fetch was not used to perform hash lookups with default values?\n- Is there any reason why the agent will not inherit <%= use_ssl %> from the director section?\nOther than those minor issues everything else looks pretty good.\n- @calebamiles and @maximilien\n. Hello @drnic,\nWe made some small inline notes on your PR. Would you also be so kind as to change it 'has API call that returns a list of stemcells in JSON with existing deployments' and it 'has API call that returns a list of stemcells in JSON with no existing deployments' into contexts\nThanks!\n@calebamiles and @maximilien\n. Hi @drnic,\nWe have created a story here to address this bug. Closing this issue.\nCF Community Pair\n@calebamiles, @drich10 \n. Hello @reneedv,\nThank you again for submitting your pull request; this functionality has been available since 52f1ae5adbbc62c2ed6ae4d683a894ad21893560, so we are closing this pull request.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Hi @matjohn2,\nThanks again for submitting this pull request. Would you be so kind as to update your spec to use the expect syntax rather than should. Also it would be wonderful if your spec covered the case where the parameter was not specified by using contexts. Once those changes have been made we should be able to pull your changes in.\nCF Community Team\n@calebamiles, @drich10 \n. Merged in d97c579441f5bc7edbbf24015046bc5a91480672, closing.\nCF Community Pair\n@calebamiles @jfmyers9 \n. Merged in 5788ec8d8cf57003d4785d5ba2d5490bd27edb21\n. Hello @sfielder,\nAs your manifest contains private information please sanitize it and send it, along with your bosh_environment file to cf-community-pair@pivotallabs.com where we can examine it away from prying eyes :)\nCF Community\n@calebamiles\n. Hi @drnic,\nWe switched to syncing only the bare git repository for a few reasons. First, we avoid copying up stemcells with tend to end up in bosh/tmp. Syncing the bare repository also helps force us to make a commit before bringing up a VM for testing.\n. Hello @dingyin,\nThank you for submitting this pull request, however, these packages are installed in the updated system_kernel stage as of 9c9333cea4a2f2d6b5564978ec1da317c8458724. When the develop branch has been promoted by CI this issue will be resolved so we are closing this pull request. We also notice that there is a fair bit of additional functionally added which should be submitted in a separate pull request. \nCF Community Pair,\n@calebamiles @jfmyers9 \n. Hello @amhuber,\nThanks for submitting this pull request. We have a couple of issues that we hope you'll be able to address:\n- We prefer the new Rspec style for test expectations and stubs, so that should becomes expect or allow depending on the context. \n- We are wondering if you might investigate using Fog.mock! in cloud_spec.rb to avoid stubbing :new. We understand that the current tests do this as well, but as yet we have not been able to refactor them.\n- As your test changes the VM lifecycle, by modifying how a VM is created, a test should probably be added to the CPI lifecycle specs in lifecycle_spec.rb\nThanks again!\nCF BOSH Team\n@calebamiles, @phanle \n. Hi @amhuber,\nWe are able to update the Rspec style used in your tests, however, a CPI lifecycle test will still be required before this feature can be merged in. When you are able to add the additional test coverage for the feature, we can then work on the styling changes after the PR is merged.\nCF BOSH Team\n@calebamiles, @phanle\n. Hi @amhuber,\nWe spoke with @goehmen, this morning and the BOSH Team will create the lifecycle test as well as cleaning up the CPI specs in their entirety so there is no further action required on your part at this point.\nCF BOSH Team\n@calebamiles, @phanle \n. Merged 3eb4cbfdfa80536be95882e1bf10ad5ae47def54.\n. Merged in 60882bf3c991b05dde201356fb4de0de75ca883c\n. Doug,\nUnless I'm mistaken, all work required to support named releases was done\nin the CLI so you should be able to do a 'gem update bosh_cli' after the\nnext release to pick up the changes. Please let the team know if you\nencounter any issues using the new CLI with your BOSH Lite environment.\nCaleb\nOn Aug 5, 2014 7:03 AM, \"Dr Nic Williams\" notifications@github.com wrote:\n\nbosh-lite uses the same bosh repo but it's on a slower release cycle; and\nmcf (which trycf.starkandwayne.com uses) is on a slower cycle that that\n(months old)\n@ccpforlife we'd be happy to help with bosh-lite/mcf releases if you'd\nlike\nOn Tue, Aug 5, 2014 at 6:07 AM, Doug Davis notifications@github.com\nwrote:\n\ngreat! thanks!\nHow does bosh-lite pickup features like this?\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-51194335\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/578#issuecomment-51201327.\n. Hello @drnic,\n\nWe are closing this issue due to lack of activity. Please don't hesitate to (re)open an issue if the bug you are reporting has not been addressed.\nCF BOSH Team\n@calebamiles\n. Hi @mrdavidlaing,\nDo you think you'll be able to do the refactoring that you discussed with @mariash? Merging this PR is currently awaiting your feedback. Additionally, since the spot instance creation fundamentally changes how a VM is created, would it be possible for you to add an additional test (in a separate PR) to the AWS lifecycle tests to reflect the additional functionality? Thanks again for your work on adding spot instance support!\nCF BOSH Team\n@calebamiles\n. Hello @jhellan,\nThank you for submitting this issue. The BOSH Team will investigate and respond.\nCF Community Pair\nCaleb and Max\n@calebamiles @maxbrunsfeld \n. Hello @james-masson,\nIt seems as if you were using the director endpoints without using the BOSH CLI which is currently unsupported. If you see an issue with improper redirects when using the BOSH CLI or have a PR to submit, please do not hesitate to open a new issue.\nCF BOSH Team\n@calebamiles\n. Hello @tnaoto,\nWe are reviewing your PR and it seems a bit strange that we would have to move the network configuration before the wait_resource() call had completed since we won't actually know if the VM had been successfully created until wait_resource exits. Could you provide some more information about your setup, as well as providing any relevant, sanitized logs and open a new issue about possible networking problems on Icehouse?\nCF BOSH Team\n@calebamiles\n. Hello @drnic,\nThe warning message was to alert you that your director does not support the semantic versioning format of new dev releases. Unfortunately, there is no way to address this issue when using a remote release because the director handles downloading and by the time the release has been downloaded by the director a known bug will cause old BOSH directors to misread the release format. Using a new CLI with a local release should have converted the version internally for you. Please open a new issue, which would be a bug, if that did not happen.\nCF BOSH Team,\n@calebamiles\n. Hello @fthamura,\nAre you asking for a progress indicator during the upload release process? There is currently no indication as to how many total packages and jobs will need to be copied or where BOSH is in that process. As adding this additional information would be new feature work I have created a story about displaying progress for package and job processing and one story about showing the percent downloaded when a remote blob is used. Both these stories can be prioritized by @goehmen, the project manager for BOSH so I am closing this issue. Please feel free to reopen or submit a new issue if you believe these stories do not address your concerns.\nCF BOSH Team\n@calebamiles\n. accidentally opened against master\n. Could we use \nruby\ndeployment_names =  stemcell.deployments.map(&:name) if stemcell.deployments\njust for the clarity to future maintainers. Also should there be a default value here besides nil?\n. We have been trying to standardize on expect.to eq() rather than should ==\n. Please change should to expect e.g. expect(@cloud.instance_variable_defined?(\"@wait_resource_poll_interval\")).to  eq(true)\n. If at all possible please don't use instance_variable_defined? or instance_variable_get.\n. We would prefer the use of instance_double rather than double.\n. If at all possible, try to avoid stubbing :new\n. There should be no linux-image-virtual on recently produced stemcells. The transition package is still available in Ubuntu Trusty but is not installed.\n. If the vm is migrated to another datastore env.json won't be migrated. In case of ISO image we can determine location of env.json based on cdrom information about ISO since ISO is not being migrated with vm. In the case that a CDROM drive was never attached to a VM we will have no way of determining where the env.iso or env.json files are located. This will break get_current_env method and will also leave old env.json around after VM is deleted. Do you think there is a way you can determine env.json location after vm was migrated?\n. ",
    "swat30": "Is this available when using the ruby gem yet?\n. ",
    "drich10": "Hi @frodenas,\nThanks for submitting this pull request. If possible, we would like you to change your uses of double to instance_double or class_double and your uses of stub to allow. Please also change your expectations to use the expect style rather than should. When those changes are made we would be happy to merge in your changes.\nCF Community Pair\n@calebamiles, @drich10\n. ",
    "jesseproudman": "I can do that.  Any suggestions / requirements on where in the code stack you would prefer that variable be checked for an the Excon settings modified?\n. Happy to implement this... Sounds like a config option would be the best bet.  Any preferences on where to read that config option and force Excon preference?  This seems to touch so much of the codebase...\n. Submitted PR for this.\n. Thanks.  Will take a look at this this week.\n. This was reverted here:\nhttps://github.com/cloudfoundry/bosh/commit/01eb08905d7c44fb0f67d0d766bb505b843bb559\nSubmitted a new PR to resolve those issues here:\nhttps://github.com/cloudfoundry/bosh/pull/430\n. Close and Reopen to see if we can get Travis to build: https://twitter.com/blueboxjesse/status/383336015252815872\n. Trying to get Travis to run by opening an entirely new PR.\n. https://github.com/cloudfoundry/bosh/pull/432\n. So while I'm working to get Travis fixed, I spun up a second VM to run the spec suite.  I'm getting the following failures there.  Ever seen this before?\n777) Bosh::Director::ProblemHandlers::InactiveDisk registers under inactive_disk type\n     Failure/Error: Unable to find matching line from backtrace\n     Errno::ENOENT:\n       No such file or directory - redis-server\n     # /home/ubuntu/source/bosh/director/spec/spec_helper.rb:168:in `spawn'\n     # /home/ubuntu/source/bosh/director/spec/spec_helper.rb:168:in `block (2 levels) in <top (required)>'\n     # /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rspec-core-2.13.1/lib/rspec/core/example.rb:237:in `instance_eval'\n     # /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rspec-core-2.13.1/lib/rspec/core/example.rb:237:in `instance_eval'\n     # /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rspec-core-2.13.1/lib/rspec/core/hooks.rb:21:in `run'\n     # /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rspec-core-2.13.1/lib/rspec/core/hooks.rb:66:in `block in run'\n     # /usr/local/rvm/gems/ruby-1.9.3-p448/gems/rspec-core-2.13.1/lib/rspec/core/hooks.rb:66:in `each'\nThese seem to get triggered with these two edits:\nhttps://github.com/cloudfoundry/bosh/pull/434/files#L3L92\nhttps://github.com/cloudfoundry/bosh/pull/434/files#L1R53\nAny thoughts?\n. Cool.  Thanks Josh!  Let me know if there's anything else I can do to assist and I greatly appreciate the attention!\n. Done!\n. Figured out all my issues and this code now runs a spec suite locally.  Newb issue - redis-server was missing from the environment.  Should have been obvious from the spec errors, but commenting out the code I had been working on changed that.\nThis should be ready to adopt, and should have resolved the issues previously raised.\n. Jesse and I talked over hangout and he's got the info he needs to continue to process this.\n. To confirm, bundle exec bosh within the bosh directory itself will run the local bosh version (and not the installed versions)?\n. Great!  Then ignore this!  :)\n. ",
    "matthewmcnew": "Merged.\n@matthewmcnew / @mmb\n. Merged.\n@matthewmcnew / @mmb\n. Hello Yulia,\nThe reason that we chose not to send deployment tags for alerts was because they don't have a deployment attribute. (https://github.com/cloudfoundry/bosh/blob/master/bosh-monitor/lib/bosh/monitor/events/alert.rb#L14)\n@matthewmcnew / @jmtuley \n. ",
    "fraenkel": "Going with #426 \nRemoving the instance in configuration.rb\n. Yes, I had some debug. The failing line was line 9.\n/Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/resources/job.rb:9:in `block in discover': undefined method `<<' for nil:NilClass (NoMethodError)\n    from /Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/resources/job.rb:7:in `each'\n    from /Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/resources/job.rb:7:in `inject'\n    from /Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/resources/job.rb:7:in `discover'\n    from /Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/commands/release/create_release.rb:194:in `build_jobs'\n    from /Users/fraenkel/.gem/ruby/2.1.7/gems/bosh_cli-1.3093.0/lib/cli/commands/release/create_release.rb:98:in `create_from_spec'\n. I am more than happy to give you a debug log. Where is it on my local machine?\n. There is no task since it didn't get that far.\n. It was the develop branch of cf-release with updated loggregator code.\nI am on a Mac. I was using 3115 but dropped to 3100 to get things working.\n. I can no longer recreate the issue.\n. I located the source of the problem.\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/release_tarball.rb#L47\nOn my machine, I have gnu tar installed and --fast-read is a mac specific option which fails.\n. ",
    "drogus": "For some reason we're getting different payload for this repo. I will check it with Github's support,\n. For some reason we're getting different payload for this repo. I will check it with Github's support,\n. It seems that the problem fixed by itself. We're still not sure what happend, but we added a code, which will allow us to catch such situations earlier and we have better logging, which will give us more details in case we need to bother @github guys to check the event payloads.\n. It seems that the problem fixed by itself. We're still not sure what happend, but we added a code, which will allow us to catch such situations earlier and we have better logging, which will give us more details in case we need to bother @github guys to check the event payloads.\n. ",
    "joshk": "Sorry for the issues, this seems to be related to the GitHub payloads we are getting. We are diving in deeper!\n. can you do another commit please, just tracking it down in the logs\n. ",
    "ruthie": "We're considering this blocked until someone from BlueBox can come and pair with us on it.\n@ruthie @charliebevis \n. ",
    "mikepatterson": "Hi @ruthie and @charliebevis I can pair with you on this. Do you want to contact me at mpatterson@bluebox.net and lmk how/when would be best to pair with you on this? Thanks!\n. Hi @ruthie and @charliebevis I can pair with you on this. Do you want to contact me at mpatterson@bluebox.net and lmk how/when would be best to pair with you on this? Thanks!\n. @thansmann, tomorrow works for me. I'm also fairly new to the issue and don't have any additional information outside of what's been posted in this thread. In any event, please feel free to shoot me an invite and I can attend a hangout tomorrow at your convenience and we can hopefully get this sorted out. Thanks! \n. @thansmann, tomorrow works for me. I'm also fairly new to the issue and don't have any additional information outside of what's been posted in this thread. In any event, please feel free to shoot me an invite and I can attend a hangout tomorrow at your convenience and we can hopefully get this sorted out. Thanks! \n. @frodenas, good to know, thank you. I will verify using 1744 on our end and report back. \n. @frodenas, good to know, thank you. I will verify using 1744 on our end and report back. \n. @goehmen my apologies for the delay. I obtained access to a stack as of this week. I'm still getting this in a state ready for testing and should have confirmation on this specific self-signed cert issue by tomorrow.\nJust to clarify, I will be testing to confirm whether or not bosh can communicate with an Openstack endpoint that is facilitating a self-signed SSL cert. If there is anything else that I need to be testing for that I am missing, please lmk. Thank you!\n. @goehmen my apologies for the delay. I obtained access to a stack as of this week. I'm still getting this in a state ready for testing and should have confirmation on this specific self-signed cert issue by tomorrow.\nJust to clarify, I will be testing to confirm whether or not bosh can communicate with an Openstack endpoint that is facilitating a self-signed SSL cert. If there is anything else that I need to be testing for that I am missing, please lmk. Thank you!\n. @goehmen, I've been able to successfully test deploying microbosh on an openstack cluster using a self-signed certificate for its encrypted endpoint.\nThis was tested using (currently) the most recent versions of bosh_cli (1.2291.0) and bosh_cli_plugin_micro (1.2291.0).\nI've verified that toggling the value of the 'ssl_verify_peer' directive in my manifest will cause the deploy to succeed or fail as expected.\nPlease let me know if there is any additional information I can provide regarding this. Thank you.\n. @goehmen, I've been able to successfully test deploying microbosh on an openstack cluster using a self-signed certificate for its encrypted endpoint.\nThis was tested using (currently) the most recent versions of bosh_cli (1.2291.0) and bosh_cli_plugin_micro (1.2291.0).\nI've verified that toggling the value of the 'ssl_verify_peer' directive in my manifest will cause the deploy to succeed or fail as expected.\nPlease let me know if there is any additional information I can provide regarding this. Thank you.\n. ",
    "kkbankol-ibm": "Hello @d ,\nThis issue can be reproduced by attempting to deploy to a Openstack Grizzly environment using a custom stemcell. In our case, we are building our custom stemcell using the command \"bundle exec rake ci:build_stemcell[openstack,ubuntu]\". \nI've also seen this issue mentioned at the forums below\nhttp://grokbase.com/t/cloudfoundry.org/bosh-users/137qyw8mtd/bosh-on-openstack-grizzly\nhttps://bugs.launchpad.net/nova/+bug/1221985\nI am attempting to redeploy BOSH so I can run the BAT tests now.\n. Hi Jesse,\nI think the problem can only be reproduced by deploying to Openstack Grizzly. We have not seen this issue in Openstack Folsom. Also, I'm having a few issues with the BAT tests, could you point me to any documentation that might assist me? \nThanks, \nKalonji\n. Hey Jesse,\nYes, exactly. When attempting to deploy microbosh to a Grizzly environment, we get the errors below\nBOSH CLI\nCreating VM from...              |oooo                  | 2/11 00:05:42  ETA: --cal/lib/ruby/gems/1.9.1/gems/bosh_openstack_cpi-1.5.0.pre.1076/lib/cloud/openstack47:in `rescue in block in create_vm': Bosh::Clouds::VMCreationFailed \nOpenstack API Log\n2013-09-04 18:00:20.942 ERROR nova.virt.libvirt.driver [req-f39fd530-764c-4601-9954-dbdb2d4c6e8b 973093ddd3db4b7a8dcaf670b32b7b41 616e857a78f24206ad13b4e84652c997] [instance: e562b310-d5db-4833-992a-3647a4808bb2] Error injecting data into image 9924e399-d53f-495f-a640-ad5c1734fe8e (Error mounting /var/lib/nova/instances/e562b310-d5db-4833-992a-3647a4808bb2/disk with libguestfs (could not parse /etc/fstab or empty file))\nWhen applying our fix, these errors seem to go away.\nThanks,\nKalonji\n. Hello,\nBelow is the output of our BAT tests, executed on the modified stemcell. Is there anything I can do to improve the results?\n```\nFinished in 80 minutes 22.88 seconds\n91 examples, 7 failures, 3 pending\nFailed examples:\nrspec ./spec/system/with_release_stemcell_deployment_spec.rb:20 # with release, stemcell and deployment agent should survive agent dying\nrspec ./spec/system/with_release_stemcell_deployment_spec.rb:30 # with release, stemcell and deployment ssh can bosh ssh into a vm\nrspec ./spec/system/with_release_stemcell_deployment_spec.rb:69 # with release, stemcell and deployment dns internal should be able to look up its own name\nrspec ./spec/system/with_release_stemcell_spec.rb:42 # with release and stemcell and two deployments first deployment should set vcap password\nrspec ./spec/system/with_release_stemcell_spec.rb:72 # with release and stemcell and two deployments first deployment should use job colocation\nrspec ./spec/system/with_release_stemcell_spec.rb:95 # with release and stemcell and two deployments first deployment second deployment should migrate disk contents\nrspec ./spec/system/with_release_stemcell_spec.rb:100 # with release and stemcell and two deployments first deployment second deployment should rename a job\n```\n. Hello Jesse,\nAh, I see. Perhaps it's an issue with our particular version of Openstack. Which version of Grizzly are you using, and what server are you using? We're encountering this error on Openstack Grizzly 2013.1.1-2, hosted on Redhat 6.4.\nThanks,\nKalonji \n. Hi Sean,\nThank you. Is there any documentation I can reference to get a better understanding of exactly what is supported? @frodenas , which settings do I have to modify to use the http metadata endpoint?\nThanks,\nKalonji\n. Hello,\n@aramprice , thank you for the documentation. @sbrady and @d , would you mind sending me a copy of your nova.conf file from your OpenStack Grizzly controller & compute node?\nThanks,\nKalonji \n. ",
    "sbrady": "Hi Kalonji,\n  Using libguestfs to inject instance metadata files into the guest image is not a set-up we support. Please use the http instance metadata endpoint instead. @frodenas has more context on that.\n  Can you try changing that setting in your cluster and try again?\nThanks\nJesse and Sean\nTeam Cloud Foundry\n. Hi Kalonji,\n  Using libguestfs to inject instance metadata files into the guest image is not a set-up we support. Please use the http instance metadata endpoint instead. @frodenas has more context on that.\n  Can you try changing that setting in your cluster and try again?\nThanks\nJesse and Sean\nTeam Cloud Foundry\n. You need to enable the metada_ip option at your /etc/quantum/l3_agent.ini file.\nThanks\n. You need to enable the metada_ip option at your /etc/quantum/l3_agent.ini file.\nThanks\n. ",
    "dajulia3": "@grenzr we noticed that there are zero commits in this pull request... Seems a little strange! Could you shed some light on this?\n. @tader could you push up the amended commit to your branch so that we can review the changes? \nThanks!\n. @drnic - That looks cool/useful! Do you have any interest in adding the tests for this so that we can accept the PR? If not I'd like to close the PR\n@dajulia3 && @dbailey (Cloud Foundry Community Pair)\n. @joshuamckenty, there hasn't been much activity on this pull request. We're closing this PR for now, but we'd love a new pull request with test coverage if you feel so inclined :-).\nThank you!\n. @tsaleh What do you think about making the CLI command configurable with a timeout as @m1093782566 suggested above?\n. @m1093782566 we'd like to see a PR with the above changes. We will close this PR, and hope that you submit a new one!\n. @m1093782566 we'd like to see a PR with the above changes. We will close this PR, and hope that you submit a new one!\n. @m1093782566, since it seems like there's no more conversation happening on this issue, I am closing it.\n. @m1093782566, since it seems like there's no more conversation happening on this issue, I am closing it.\n. closing this issue due to lack of discussion.\nIf this is still a problem for you, please open a new issue.\n. closing this issue due to lack of discussion.\nIf this is still a problem for you, please open a new issue.\n. So that we can help identify the issue, can you please provide use with the following information about your environment:\n- IaaS and version and basic setup (e.g vCenter 5.1 with 6x96GB hosts running 300 VMs)\n- Stemcell name (e.g. bosh-vsphere-esxi-ubuntu). If you are using a stemcell named \u2018bosh-stemcell\u2019 it is pre Oct 2013 and is out of date. You\u2019ll need to switch to the current stemcells listed via: \u2018bosh public stemcells\u2019\n  output of \u2018bosh --version\u2019\n- In a gist - clean (passwords and sensitive info removed) deployment manifest\n- In a gist - cleaned \u2018bosh task xxxx --debug\u2019 log\n. @rkoster we asked around, and we are no longer actively supporting cf-services-contrib-release. However, we are still maintaining the v1 services API for the moment so as not to break backwards compatibility. That said, if you need any more help please join the Cloudfoundry-Dev mailing list and ask there.\n@dajulia3 and @dbailey (Cloud Foundry Community Pair)\n. @rkoster we asked around, and we are no longer actively supporting cf-services-contrib-release. However, we are still maintaining the v1 services API for the moment so as not to break backwards compatibility. That said, if you need any more help please join the Cloudfoundry-Dev mailing list and ask there.\n@dajulia3 and @dbailey (Cloud Foundry Community Pair)\n. @rkoster, on second thought, we might have been a little hasty in closing this. Is this a repeatable/consistent error for you? If it's actually a problem with bosh and not the release then we want to make sure to fix it.\n. @rkoster, on second thought, we might have been a little hasty in closing this. Is this a repeatable/consistent error for you? If it's actually a problem with bosh and not the release then we want to make sure to fix it.\n. @rkoster @monkeyherder suggested that you might not have write access to that s3 bucket. Can you confirm that you have write access to cf-contrib?\n. @rkoster @monkeyherder suggested that you might not have write access to that s3 bucket. Can you confirm that you have write access to cf-contrib?\n. @rkoster We also just confirmed that Cloud Foundry as an organization no longer supports cf-contrib or manages the associated AWS accounts- @drnic will probably be able to check your privileges on the cf-contrib bucket for you.\n@dajulia3 & @dbailey (Community Pair)\n. @rkoster We also just confirmed that Cloud Foundry as an organization no longer supports cf-contrib or manages the associated AWS accounts- @drnic will probably be able to check your privileges on the cf-contrib bucket for you.\n@dajulia3 & @dbailey (Community Pair)\n. @grenzr we don't see anywhere in your PR where this config is actually being used... Are we missing something here? Also, please describe your use case for this addition.\n. This is a dup of issue #500 - closing this one. You just wanted to snag issue 500, didn't you ;) ? \n. This is a dup of issue #500 - closing this one. You just wanted to snag issue 500, didn't you ;) ? \n. @drnic could you please clarify this request?\n. @drnic could you please clarify this request?\n. @drnic, nevermind, @tsaleh has got it and will follow up with you if need be.\n. @drnic, nevermind, @tsaleh has got it and will follow up with you if need be.\n. ",
    "nsnt": "It's OK now. Thanks!\n. ",
    "amhuber": "I'm working on the test cases and I'll submit a new request once I've got them working.\n. Thanks!\n. Yes, same concept.  Booting to a persistent volume so that the VM can be live migrated between hosts, etc.\n. Yes, same concept.  Booting to a persistent volume so that the VM can be live migrated between hosts, etc.\n. Working on this code was my first experience with Ruby and about the limit of my development abilities.  I can spend some additional time on this if needed but you'd need to be a bit more explicit about what is needed as I didn't understand any of the comments above.  My intent with the data types was to just match what was already in there and I did add some test cases in for BFV so I'm not sure what else would be needed.  And I have no idea what would be required to rebase the code.  Does that mean start over from the develop branch vs. what I already put in and tested?\n. Working on this code was my first experience with Ruby and about the limit of my development abilities.  I can spend some additional time on this if needed but you'd need to be a bit more explicit about what is needed as I didn't understand any of the comments above.  My intent with the data types was to just match what was already in there and I did add some test cases in for BFV so I'm not sure what else would be needed.  And I have no idea what would be required to rebase the code.  Does that mean start over from the develop branch vs. what I already put in and tested?\n. @calebamiles @adamstegman @jtuchscherer \nI'm working on a new pull request to resolve these issues and had a few clarifying questions:\n1) When you say to create the pull request against the develop branch, do you mean to actually make the changes to the develop branch in my forked repo, or to create a new branch as the contributing guide indicates and then target the develop branch when submitting the pull request?  I'm assuming the latter.\n2) It appears that the BAT tests are just to test a BOSH environment once it's deployed. Since my changes only impact how the bosh VM is deployed (whether it uses an ephemeral or persistent disk the disk contents are identical) I'm not sure any BAT changes are needed here.  Can you please clarify what additional tests I would need?\n3) I'll add a default value for the new property in the spec, that was just an oversight.\n4) Regarding changing double to instance or class, I've read up on the changes and it doesn't look like that would be difficult, but I'm just matching the existing functionality in the specs.  Are you asking me to just use the class or instance on the doubles I'm adding, or do I need to refactor the existing stuff to match?  My intent was just to match what's there and I'd think it would be better to refactor all of it at the same time once my changes are integrated but please let me know.\n5) It appears that CPI lifecycle changes are not needed, is that correct?\nThanks for your time and help.\n. @calebamiles @adamstegman @jtuchscherer \nI'm working on a new pull request to resolve these issues and had a few clarifying questions:\n1) When you say to create the pull request against the develop branch, do you mean to actually make the changes to the develop branch in my forked repo, or to create a new branch as the contributing guide indicates and then target the develop branch when submitting the pull request?  I'm assuming the latter.\n2) It appears that the BAT tests are just to test a BOSH environment once it's deployed. Since my changes only impact how the bosh VM is deployed (whether it uses an ephemeral or persistent disk the disk contents are identical) I'm not sure any BAT changes are needed here.  Can you please clarify what additional tests I would need?\n3) I'll add a default value for the new property in the spec, that was just an oversight.\n4) Regarding changing double to instance or class, I've read up on the changes and it doesn't look like that would be difficult, but I'm just matching the existing functionality in the specs.  Are you asking me to just use the class or instance on the doubles I'm adding, or do I need to refactor the existing stuff to match?  My intent was just to match what's there and I'd think it would be better to refactor all of it at the same time once my changes are integrated but please let me know.\n5) It appears that CPI lifecycle changes are not needed, is that correct?\nThanks for your time and help.\n. @adamstegman @mbhave \nOK, it appears my assumption that changing from double to instance_double or class_double may have been optimistic.  I've been playing with it all day and can't get it to work.  Since my changes are integrated pretty closely with what's already there anything I change starts to cascade and cause issues in the other tests already there.  Given my level of experience with Ruby I'm able to copy what's already there and tweak it slightly, but making it better than it already is might be more than I can handle.\nHere is a diff of my current code:\nhttps://github.com/amhuber/bosh/compare/boot_from_volume\nI'd ask that you accept a pull request based on it for the following reasons:\n1) I have full unit test coverage for the pieces that I've added using double like all the rest of the code.\n2) I have fully tested this and we've been using in production for 6 months.\n3) It seems like a very low risk that I'm going to break anything that's already there.\n4) Other enterprises have expressed interest in this functionality.\n5) I'm hoping it's possible to contribute without being an uber developer.  :-)\nShould I go ahead and submit a pull request based on the current code or will you be unable to accept it based on the limitations?  If not, I understand and I'll speak to James and Ferdy about how to move forward as Intel needs this functionality in BOSH.\n. @adamstegman @mbhave \nOK, it appears my assumption that changing from double to instance_double or class_double may have been optimistic.  I've been playing with it all day and can't get it to work.  Since my changes are integrated pretty closely with what's already there anything I change starts to cascade and cause issues in the other tests already there.  Given my level of experience with Ruby I'm able to copy what's already there and tweak it slightly, but making it better than it already is might be more than I can handle.\nHere is a diff of my current code:\nhttps://github.com/amhuber/bosh/compare/boot_from_volume\nI'd ask that you accept a pull request based on it for the following reasons:\n1) I have full unit test coverage for the pieces that I've added using double like all the rest of the code.\n2) I have fully tested this and we've been using in production for 6 months.\n3) It seems like a very low risk that I'm going to break anything that's already there.\n4) Other enterprises have expressed interest in this functionality.\n5) I'm hoping it's possible to contribute without being an uber developer.  :-)\nShould I go ahead and submit a pull request based on the current code or will you be unable to accept it based on the limitations?  If not, I understand and I'll speak to James and Ferdy about how to move forward as Intel needs this functionality in BOSH.\n. Hey @calebamiles, I think this is more or less the same topics that were discussed in the last pull request for this (https://github.com/cloudfoundry/bosh/pull/448).  I definitely understand the desire to have the new code meet the newer standards, but the bottom line is that after spending some time trying to make those changes I wasn't able to get them working.  I'm just good enough with Ruby to copy what was there and add some tweaks but there were some interactions with the rest of the code that I wasn't able to figure out.  My request is to let this code in based on the old standards and save any larger refactoring for someone who actually knows what they are doing. :-)\nAaron\n. Hey @calebamiles, I think this is more or less the same topics that were discussed in the last pull request for this (https://github.com/cloudfoundry/bosh/pull/448).  I definitely understand the desire to have the new code meet the newer standards, but the bottom line is that after spending some time trying to make those changes I wasn't able to get them working.  I'm just good enough with Ruby to copy what was there and add some tweaks but there were some interactions with the rest of the code that I wasn't able to figure out.  My request is to let this code in based on the old standards and save any larger refactoring for someone who actually knows what they are doing. :-)\nAaron\n. @calebamiles, is the \"lifecycle test\" just the stuff in https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/spec/integration/lifecycle_spec.rb?  I'm not entirely sure I understand how this works - is this set up to work against an OpenStack environment that is already configured somewhere?\n@phanie\n. @calebamiles, is the \"lifecycle test\" just the stuff in https://github.com/cloudfoundry/bosh/blob/master/bosh_openstack_cpi/spec/integration/lifecycle_spec.rb?  I'm not entirely sure I understand how this works - is this set up to work against an OpenStack environment that is already configured somewhere?\n@phanie\n. @calebamiles, I've been discussing this request via email with @goehmen and he's indicated that he's added some tasks in the tracker for the BOSH team to address these additional requirements.  I just wanted to make sure there wasn't a gap in communications.\n. Excellent, thank you.  If there is any additional information required for the team please let me know.  Testing this does require some special configuration in OpenStack:\n\u2022 Converting the stemcell from QCOW2 format to RAW (require for boot from volume to work)\n\u2022 Configuring the flavor to specify a disk size (we're also configuring our stemcell with growpart to use the space)\nI\u2019m not sure how those changes can be integrated into the lifecycle tests but I\u2019m happy to help in any way that I'm capable of.\n. This change is already CPI independent, it's just passing the variable to the existing CPI code so this will work with any CPI that has support for cloud_properties in the create_disk function.  What you are requesting is just changing the syntax/formatting of how the variables are stored in the MicroBOSH manifest to match what BOSH is now using but there would be no functional change.\nAs it would take a much larger modification to the MicroBOSH code to support that as well as maintain backwards compatibility and changing all the specs I'd prefer not to make that significant a change.\nI'll take a look at the specs to see if I can figure something out, but there is already test coverage in the CPIs to handle the parameter when it's passed in so I didn't think it was necessary here.\n. OK, I think I can figure that change out (I'm far from proficient on Ruby so just trying to do what I can to get the functionality we need).\nAre there any similar specs to what we'd need to test here?  I'm not sure where to start on that since this is optional and especially if I make the change you're requesting anything could be passed.  Is there a spec already that tests that properties being set are actually passed into the CPI?\n. Updated to the new property as suggested.  This is much more generic.  :-)  Let me know if you see anything else needed before you can accept this.\n. @cppforlife, is this still underway?\n. It's shown above, but basically:\nresources:\n  persistent_disk: 20480\n  persistent_disk_cloud_properties:\n    type: volume_type\n  cloud_properties:\n    instance_type: flavor\n    availability_zone: zone\nThis depends on support in the CPI for disk pool configuration, which is present only in the AWS CPI at the moment (and will be in OpenStack once they accept my pull request for that).\n. Yes, according to the Cinder wiki (https://wiki.openstack.org/wiki/Cinder):\nWhat is Cinder ?\nCinder provides an infrastructure for managing volumes in OpenStack. It was originally a Nova component called nova-volume, but has become an independent project since the Folsom release.\n. @cppforlife, Is there a tracker item to get this pulled in?  We're dependent on this functionality for our next environment launch and I'm hoping this can be completed soon.\n. That is for a different pull request (663).  Do you have a story link for this one?\n. @cppforlife, can you comment on a timeline for pulling this in?  I'm hoping it will be in place for our next release where we're dependent on the volume type.  :-)\n. The change to boot_volume_cloud_properties looks great but you'll need to change the spec and erb to match:\nrelease/jobs/director/spec\nrelease/jobs/director/templates/director.yml.erb.erb\nRight now they still refer to the old property name from my changes.\n. Just checked this out on the 2778 release and everything is working as expected:\n- MicroBOSH volume type for boot disk and persistent disk getting passed correctly to the volume API from the manifest\n- Settings from the manifest getting correctly pushed to the director.yml\n- VMs created by MicroBOSH are built with the correct boot disk properties and persistent disk properties\nThanks for getting this wrapped up.\n. We are looking at testing vRA and would be very interested in a CPI as well.\n. Definitely agreed.  When doing manual deployments we use the diff as a final \"make sure I didn't screw anything up\" check and expect the diff to be there by default. Having the option to turn it off is fine for folks that need it, as it was before.\n. This is due to the differences in fog between the compute/volume model (https://github.com/fog/fog/blob/master/lib/fog/openstack/models/compute/volume.rb) and the volume/volume model (https://github.com/fog/fog/blob/master/lib/fog/openstack/models/volume/volume.rb).  I'm not sure why they used names in the compute model that don't match up to the API, but you can see in the request that they changed it back before submitting it to the API (https://github.com/fog/fog/blob/master/lib/fog/openstack/requests/compute/create_volume.rb).\nThe code change I submitted included switching to the volume model to get support for setting the volume_type, so I needed to change the attributes being passed to fog to line up.\n. ",
    "shashankmjain": "Does the boot from volume survive restart, If some data exists on boot volume(say some packages), will Bosh boot from the same volume in case of say VM restart or recreation?. Thanks.\nYes for persistent data we use store.\nQuestion is about packages and then also the advantage of booting from a\nvolume. Openstack provides a reserve feature in case we want to get the\nsame volume for the rootdisk.\nThanks\nShashank\nOn Fri, May 26, 2017 at 8:03 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nvm restart is typically not bosh involved so thats up to openstack. vm\nrecreation gurantees that root and ephemeral disks are new. packages are\nreinstalled everytime vm is created.\nif you need to store some data i would recommend placing it on\n/var/vcap/store which is for persistent data.\nSent from my iPhone\n\nOn May 26, 2017, at 6:41 AM, shashankmjain notifications@github.com\nwrote:\nDoes the boot from volume survive restart, If some data exists on boot\nvolume(say some packages), will Bosh boot from the same volume in case of\nsay VM restart or recreation?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/445#issuecomment-304298777,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADzK1lyjZhT7sJI8psHBaWgN0ust9NPDks5r9uKngaJpZM4BIyqf\n.\n. \n",
    "agenticarus": "Thanks - this change let me create subnets in the eu-west-1 region!\n. ",
    "tader": "@hiremaga I just squashed the two commits, and \u2013 since I was rewriting history anyway \u2013 I also rebased it on top of the current master.\n. Comment on commit GiMiScale@6af4bc28df78b3978901b01b08aac03ca4a55c16:\n\n@tader \nWe looked at .../bosh/bosh_cli_plugin_aws/lib/bosh_cli_plugin_aws/aws_config.rb, where it got an option to set the region via a bash ENV var 'BOSH_AWS_REGION'. Have you tried this to set your deployment to another region?\nIf that does not suit your use case could you please explain your use case to us in a bit more detail?\nRegarding the actual pull request- We have some concerns about the test: \nWe were unable to get the test to go red by reverting your change to AwsProvider. To test that the correct ec2_endpoint is merged in, you could delete line 19 of aws_provider spec and then add another assertion that the AWS::EC2 object has the correct ec2_endpoint as generated from the region. If, after filling us in on your use case, it becomes clear that we need this pull request, then we'd like you to please update the tests as suggested.\nThanks!\n@dajulia3 && @thansmann\n\n@dajulia3, @thansmann \nWe do indeed set the BOSH_AWS_REGION environment variable, I think this is where the region variable, which is used to construct the EC2 endpoint, actually gets its value from.\nIt seems that (at least for the used version) the AWS::EC2 class does not set the endpoint correctly if you only provide the region to the constructor, and that you are probably limited to the us-east-1 region. With this proposed change however, we have been able to deploy to the eu-west-1 and ap-northeast-1 regions.\nI have just amended the commit, and now the spec fails if ec2_endpoint was not provided correctly as an agument to the AWS::EC2 constructor.\n. ",
    "jtuchscherer": "@amhuber @calebamiles \nSorry for the long silence. Here are some thoughts around this PR:\n- Please recreate this PR against the develop branch. \n- In regards to @calebamiles 's comments, you can read about the BATs here.\n- It would be great if you could address the question around setting a default value for boot_from_volume (you can set the default in the director job spec)\n- If you need more info about changing the double to instance_double or class_double let us know. We can help you with that.\n- @calebamiles we are not sure if this is a good candidate for a CPI lifecycle test.\nWe are going to go ahead and close this PR and are waiting for a new one against the develop branch.\n\\cc @goehmen \nThanks again for the contribution,\nCF Community Pair (@adamstegman & @jtuchscherer)\n. ",
    "emalm": "This PR has a lot of files changed that are not related to swift blobstore support. #496 makes the same change as this PR with the addition of rackspace. Any objections to closing this and merging #496 when tests are added?\nCF Community Pair (@ematpl & @mmb)\n. It looks like these changes have been superseded by the changes in #508 and #521. Closing.\n-CF Community Pair (@ematpl & @mmb)\n. Hi @rkoster,\nThe whitespace cleanup is helpful but could it be moved to a separate pull request to make this one smaller?\n-CF Community Pair (@ematpl & @mmb)\n. Hi, @grenzr,\nIt looks like the configuration changes in #508 supersede most of these configuration changes, except for the configurability of the compiled_package_cache. Would you be interested in rebasing and amending this PR to provide that in the style of #508?\nThanks,\nCF Community Pair (@ematpl & @mmb)\n. Hi, @grenzr,\nSure, that sounds reasonable to me. Also, @mmb, @goehmen, or someone else on the BOSH team might have more context on what would be most appropriate, especially as the team tries to address all of the recent pull requests around these configuration options.\nBest,\nEric \n. This looks good to merge but it needs specs added to https://github.com/cloudfoundry/bosh/blob/master/release/spec/director.yml.erb.erb_spec.rb\nWe have specs for this template because the logic is so complex.\n-CF Community Pair (@ematpl & @mmb)\n. #508 was recently merged to support S3 compatible blobstores. Would you be able to support swift as an S3 provider with the new blobstore.use_ssl, blobstore.port and blobstore.host properties?\n-CF Community Pair (@ematpl & @mmb)\n. @goehmen Is this currently a priority for bosh? If not, we'd like to close this issue.\nThanks,\nCF Community Pair (@ematpl & @sclevine)\n. @goehmen Does this issue need to be addressed? Should we create a tracker story for it?\n@drnic If so, would you be willing to submit a pull request for this?\n-CF Community Pair (@ematpl & @sclevine)\n. Moving story to the BOSH tracker for discussion.\n-CF Community Pair (@ematpl & @mmb)\n. Merged to develop branch. Thanks.\n-CF Community Pair (@ematpl & @mmb)\n. Hi, @vmadired ,\nIt looks like you're getting assistance with this already as part of this bosh-users thread. We're closing this issue to prevent having the conversation in two places.\nThanks,\nCF Community Pair (@ematpl & @sclevine)\n. Hi @byllc,\nWould you be able to provide the result of monit status and /var/vcap/monit/monit.log for these jobs? How did you determine that these jobs were not running?\nThanks,\nCF Community Pair (@ematpl & @mmb)\n. Moving this story to the Bosh icebox for a larger discussion about job state reporting.\n-CF Community Pair (@ematpl & @mmb)\n. The bosh_deployer gem has not been updated for about a year. The bosh_cli_plugin_micro gem is the currently supported way to deploy microbosh.\nCF Community Pair (@ematpl & @mmb)\n. #450 has a lot of files changed that are not related to swift blobstore support. #496 makes the same change as this PR with the addition of rackspace. Any objections to closing this and merging #496 when tests are added?\nCF Community Pair (@ematpl & @mmb)\n. #508 was recently merged to support S3 compatible blobstores. Would you be able to support swift as an S3 provider with the new blobstore.use_ssl, blobstore.port and blobstore.host properties?\n-CF Community Pair (@ematpl & @mmb)\n. Thanks for the additional info. Could you be more specific about what doesn't work when using the S3 client for these blobstores so we can better understand the issues?\n-CF Community Pair (@ematpl & @mmb)\n. @rboshman (@dsboulder?): Did you mean to make your endpoint comment on the #516 thread? If so, we definitely agree that the extra option is potentially confusing and we have some reservations about the logic in parsing out the host, port, and scheme. In particular, it could end up mixing and matching fields, and it doesn't provide a correct default if the endpoint URI is malformed.\nThanks,\nCF Community Pair (@ematpl & @mmb)\n. Hi, @DonBower, I'm assuming you're using bosh-deployment to deploy the Director, based on the variable names you're referring to. That cert comes from the mbus_bootstrap_ssl variable in the bosh.yml base manifest, and this line configures the VM's agent to present it. So if you delete that mbus_bootstrap_ssl variable from the vars store and let the BOSH CLI regenerate it with the new internal_ip value, that should resolve the IP SAN mismatch error.. Thanks, @dpb587-pivotal, my pleasure!. Hi, @ywysuibian, if you still see an issue with memory usage on a Diego cell after reviewing the processes on it and checking these memory accounting details, could you please file an issue on diego-release about it?\nThanks,\nEric, CF Diego PM. The Diego team likewise has several different BOSH directors sending VM metrics for deployments named cf to the same Datadog team, and so would also be interested in some mechanism to distinguish those deployments. I also expect we don't particularly care what the tag is so long as it's stable and distinct across directors. /cc @jvshahid. This is actually testing the inner RedactedParams class. Should that be tested separately? -CF CP\n. We didn't see any specs for this method. The call to the bosh logger and the yield could be tested. -CF CP\n. ",
    "armatures": "@drnic is right: we would like to see a passing test suite with new tests to cover the new functionality you added. Once we see this we will review this pull request more thoroughly, but it looks good from a quick glance.\nThanks,\n@charliebevis @zrob \n. ",
    "dbailey": "@rkoster Would you mind updating this PR to eliminate the merge conflicts?\n. @tsaleh @goehmen Is it possible to get this pulled in?\n. @tsaleh @goehmen Is it possible to get this pulled in?\n. @grenzr It looks like the BOSH team is already working on PR#450\nhttps://www.pivotaltracker.com/story/show/59889342\nhttps://github.com/cloudfoundry/bosh/pull/450\nWould that PR cover these changes or is this separate? \n. @duritong Thanks for the pull request, we're unable to review it until you sign the CLA. Feel free to reopen this if you decide to sign the CLA, following the directions @cfdreddbot mentioned in the above comment. \n. @mrdavidlaing The team is interested in pulling this feature in. It would help if you could update your PR to fix the current merge conflict. \n. @mrdavidlaing It looks like the Gemfile.lock references 2 different versions of aws-sdk. I think if you update the old 1.8.5 to 1.32.0 it should allow the travis build to run. \n. @mrdavidlaing It looks like the Gemfile.lock references 2 different versions of aws-sdk. I think if you update the old 1.8.5 to 1.32.0 it should allow the travis build to run. \n. @tsaleh Any idea why rubocop is freaking out on this build?\n. @tsaleh Any idea why rubocop is freaking out on this build?\n. @mrdavidlaing Looks like you managed to get a newer version of Rubocop, which renamed AvoidPerlisms to SpecialGlobalVars. Could you roll back all of the vendor/cache changes except the aws-sdk?\n. @mrdavidlaing Looks like you managed to get a newer version of Rubocop, which renamed AvoidPerlisms to SpecialGlobalVars. Could you roll back all of the vendor/cache changes except the aws-sdk?\n. ",
    "krishicks": "@rkoster This has been merged in as 55ef66d.\nCheers,\n@krishicks and @adamstegman \n. @rkoster This has been merged in as 55ef66d.\nCheers,\n@krishicks and @adamstegman \n. Hi @dims,\nJust so you know, we ended up re-implementing this based on the changes @mariash suggested above. We'll close this pull request.\nCheers,\n@krishicks && @maximilien \n. Hi @dims,\nJust so you know, we ended up re-implementing this based on the changes @mariash suggested above. We'll close this pull request.\nCheers,\n@krishicks && @maximilien \n. @dengwa Thanks for the pull request.\nThere are tests for build_environment.rb which you should add this to in bosh/bosh-stemcell/spec/bosh/stemcell/build_environment_spec.rb\nDon't forget to add it to both tests, to account for case-insensitivity.\nPlease amend your commit and do a force-push to HuaweiTech:hw-issue-38, which will update this pull request accordingly.\nCheers,\n@krishicks && @xingzhou\nBOSH Team\n. @dengwa Thanks for the pull request.\nThere are tests for build_environment.rb which you should add this to in bosh/bosh-stemcell/spec/bosh/stemcell/build_environment_spec.rb\nDon't forget to add it to both tests, to account for case-insensitivity.\nPlease amend your commit and do a force-push to HuaweiTech:hw-issue-38, which will update this pull request accordingly.\nCheers,\n@krishicks && @xingzhou\nBOSH Team\n. This can be tracked in https://www.pivotaltracker.com/story/show/62500104.\n. This can be tracked in https://www.pivotaltracker.com/story/show/62500104.\n. @dkoper Did you have a chance to add tests per @mariash above?\n. @dkoper Did you have a chance to add tests per @mariash above?\n. @liuhewei This was pulled in as part of 4b3c6f0de7fb4ad452cef08d991f7b363355b563.\nCheers,\nKH\n. @liuhewei This was pulled in as part of 4b3c6f0de7fb4ad452cef08d991f7b363355b563.\nCheers,\nKH\n. @drnic This has been pulled in as 6cd5c8c along with added test coverage.\n. @drnic This has been pulled in as 6cd5c8c along with added test coverage.\n. There were some conflicts when we pulled this in, which were pretty easy to resolve. This has been pulled in as 43aba09.\nAlso note that we changed the usage of this from a string to a hash in 56b6cad, to boot_volume_cloud_properties, which takes a type key:\nyaml\ncloud:\n  plugin: openstack\n  properties:\n    openstack:\n      # ...\n      boot_volume_cloud_properties:\n        type: SSD\nThis will allow us to add any further boot volume properties more easily.\n. There were some conflicts when we pulled this in, which were pretty easy to resolve. This has been pulled in as 43aba09.\nAlso note that we changed the usage of this from a string to a hash in 56b6cad, to boot_volume_cloud_properties, which takes a type key:\nyaml\ncloud:\n  plugin: openstack\n  properties:\n    openstack:\n      # ...\n      boot_volume_cloud_properties:\n        type: SSD\nThis will allow us to add any further boot volume properties more easily.\n. Thanks for the catch. Updated in 8c1b363.\n. Thanks for the catch. Updated in 8c1b363.\n. @skibum55: We looked at this PR, added tests and squashed some commits (put tests with production code changes), which we pushed to pivotalservices. Please take a look and let me know if you're OK with us pushing the squashed branch to develop.\n. @skibum55: We looked at this PR, added tests and squashed some commits (put tests with production code changes), which we pushed to pivotalservices. Please take a look and let me know if you're OK with us pushing the squashed branch to develop.\n. Pushed to develop as 768c12e..4967ad0.\nThanks!\n. Pushed to develop as 768c12e..4967ad0.\nThanks!\n. Pulled into develop. This PR is incorrectly pointing at master.\n. Pulled into develop. This PR is incorrectly pointing at master.\n. Pulled in as f8490c2.\n. Pulled in as f8490c2.\n. Extra space:\ndiff\n-Context(\"when we have one eth interface  and one virtual interface on the system\", func() {\n+Context(\"when we have one eth interface and one virtual interface on the system\", func() {\n. Is there documentation that supports this change? We were looking at the Block Storage API reference docs and see that the 'name' and 'description' fields are present, but the display_* ones are not.\n. I see. It looks then like Fog is only set up to support the V1 Block Storage API, as that version does support those parameters.\n. ",
    "zrob": "Hey Chris,\nWe tried to reproduce your issue and came up with something similar, but not quite the same.  It is expected that rebooting the vagrant vm will cause your containers to be destroyed.  See: https://github.com/cloudfoundry/bosh-lite#restore-your-deployment\nWe used a fusion vm and did a 'vagrant reload'.  This basically broke our deployment, although we didn't see the same problem you did ('bosh vms' returned \"unresponsive agent\" for each container).  We then ran the 'bosh cck' command as instructed in the above link, and this was able to restore our deployment to a happy state.\nIf 'bosh cck' does not work for you, there are also instructions on upgrading vagrant and bosh-lite in the README linked  above that may help.\nIf loading bosh-lite from the latest Vagrantfile doesn't work, can you give more info as to what's happening and how you're troubleshooting?\nThanks,\n@zrob and @oppegard \n. Closing this issue in github.  Please follow progress in tracker: https://www.pivotaltracker.com/story/show/61442960\n. ",
    "cf-gitbot": "We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65856978\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/64490442\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/64583842\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/64982690\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/64998084\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65007460\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65191894\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65315234\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65740546\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65779916\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65790902\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65820924\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65847874\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65855940\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65889848\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65896612\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/65990852\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66112578\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66112770\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66141966\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66173118\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66210238\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66270114\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66516882\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66574238\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66732894\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66732932\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66843462\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/66986818\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/67599732\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/67765836\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/67792452\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/67848756\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/67854812\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68132206\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68139174\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68249220\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68277434\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68485390\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68578390\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68787914. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68839682. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68844394. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/68917294. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69290344. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69080790. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69094898. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69162954. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69198726. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71180904. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69290650. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69290916. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69291240. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69357758. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69461266. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69796898. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69881380. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69988722. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/69990454. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/70012502. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/70035530. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/70136990. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: http://www.pivotaltracker.com/story/show/70542458. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/70923242. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/70985394. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71066352. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71109702. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71283492. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71314594. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71347542. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71439818. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71652556. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71660302. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71735974. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/71827594. This repo is managed by the 'BOSH' team.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/72316878.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/72758866.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73271108.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73457484.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73554150.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73635400.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73762894.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73783816.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73783876.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73790388.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73849582.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/73896184.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74092696.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74162606.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74193220.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74199066.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74247042.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74284732.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74787082.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/74917008.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75304942.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75384616.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75549768.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75569436.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75688222.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75795848.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/75859534.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76004864.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76105416.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76107078.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76312500.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76382492.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76506246.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76541154.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76568244.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76654580.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76839526.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76840308.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76894510.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76986232.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/76994852.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/77117064.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/77185600.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/77276322.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/77413848.\n. We have created an issue in Pivotal Tracker to manage this. You can view the current status of your issue at: https://www.pivotaltracker.com/story/show/77604474.\n. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159942867 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160468766 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160492588 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160493175 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160493351 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160495498 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160499152 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160499921 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160186667 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160599052 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160598349 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160598517 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160598870 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160598984 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160599285 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160686730 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160726653 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160738866 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160935765 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160935883 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160936070 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160936297 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160936387 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160936662 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160498662 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160936935 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161725760 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160390130 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162202236 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162202331 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480138 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480224 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480286 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480286 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480584 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480584 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480689 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162480689 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159492735 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159492735 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159575320 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159575320 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159589407 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159589407 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159595986 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159595986 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159613558 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159613558 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159644733 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159644733 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159656302 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159656302 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159663377 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159663377 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727483 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727483 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727857 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727857 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727960 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159727960 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159728533 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159728533 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159754825 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159754825 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159809609 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159809609 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159820814 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159820814 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159826426 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159826426 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159827262 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159827262 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159884264 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159891421 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159908878 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159932717 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159966928 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159966928 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159968440 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159968440 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159999631 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/159999631 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160052838 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160052838 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160070506 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160070506 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160123244 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160123244 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160159391 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160170240 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160260950 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160268119 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160284704 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160286305 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160291370 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160295535 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160306147 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160506674 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160615386 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160685209 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160722477 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160763363 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160780561 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160850781 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160877964 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160943365 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/160978242 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161109402 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161140285 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161243098 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161333009 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161336781 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161351307 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161396940 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161402252 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161403047 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161542542 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161611131 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161638046 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161661430 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161683593 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161691883 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161699301 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161722692 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161728332 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161759632 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161816582 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161954418 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162238356 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161975698 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/161986529 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162017592 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162151983 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162164647 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162165967 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162189118 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162227465 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162257458 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162261244 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162334592 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162388966 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162445358 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162457784 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162479258 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162489352 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162507648 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162528927 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162549352 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162628002 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162636339 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162703159 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162739681 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162777322 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162818386 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162853711 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/162905267 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163047698 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163085473 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163129966 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163174248 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163194991 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163197005 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163242148 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163299711 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163327081 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163608025 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163344144 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163385787 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163388498 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163595513 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163601527 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163637046 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163642242 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163655209 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163766547 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163806729 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163837813 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163838535 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163954750 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163981815 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/163982235 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164008640 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164132395 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164145998 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164164857 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164173407 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164273968 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164280232 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164309446 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164320080 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164322482 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164325756 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164429910 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164468465 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164495579 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164576311 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164576738 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164600574 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164604056 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164645309 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164649194 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164734511 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164735288 \nThe labels on this github issue will be updated when the story is started.. We have created an issue in Pivotal Tracker to manage this: \nhttps://www.pivotaltracker.com/story/show/164745631 \nThe labels on this github issue will be updated when the story is started.. ",
    "shalako": "I would accept downtime for the option of moving the IP in one deployment. Any deployment that wants to horizontally scale by adding a HAProxy tier will likely want this feature. \n. I would accept downtime for the option of moving the IP in one deployment. Any deployment that wants to horizontally scale by adding a HAProxy tier will likely want this feature. \n. closed in favor of issue #57 opened with Diego\n@cppforlife suggests this problem is due to the Diego manifest not including enough IP addresses to support the number of compilation workers. @ematpl \n. closed in favor of issue #57 opened with Diego\n@cppforlife suggests this problem is due to the Diego manifest not including enough IP addresses to support the number of compilation workers. @ematpl \n. cc @dsboulder @richardsalloum . cc @dsboulder @richardsalloum . ",
    "jfmyers9": "Closing this due to inactivity. We have since largely refactored the way networks are managed by BOSH, please feel free to reopen if this is still an issue.. @goonzoid Do you have specific questions that are preventing you from finishing this PR that we can pass along to the BOSH team? \nCF Community Pair (@karlkfi & @jfmyers9)\n. Closing due to inactivity. We are moving away from PowerDNS and have since implemented https://github.com/cloudfoundry/bosh-dns-release which should address many of these concerns.. Closing this due to inactivity. We have since implemented https://github.com/cloudfoundry/bosh-dns-release which should solve some of these concerns.. Closing due to inactivity. If this is still an issue on recent stemcells please open a new issue in https://github.com/cloudfoundry/bosh-linux-stemcell-builder. Closing this due to inactivity. Feel free to open a new issue if you are still experiencing this issue.. Closing this due to inactivity. The bosh cli has been rewritten in golang and has changed the scp/ssh behavior quite a lot. Feel free to create a new issue on the repository if you are still seeing this issue.. Closing this due to inactivity.\nWhile we don't have any plans to implement a CPI for vRealize, we do have documentation on how to build a CPI. We are always open to community contributions / implementations and will be available to answer any questions that you may have.. Closing this due to inactivity. We have improved documentation at bosh.io which hopefully explains a good amount of networking in BOSH. Feel free to create a new issue if you are still looking for more information.. Closing this due to inactivity. \n@frodenas The manifest not being persisted until a successful deploy is a known limitation. We have considered tackling this issue but it has not been a priority. Feel free to create a new issue if this is still blocking you.. Confirmed this is still an issue on recent version of the director/cli:\n```\n[jfmyers9@ronaldo vbox]$ bosh -d bpm take-snapshot bpm/0\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'bpm'\nTask 8. Done\nSucceeded\n[jfmyers9@ronaldo vbox]$ bosh -d bpm snapshots\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'bpm'\nInstance  CID  Created At  Clean\n0 snapshots\nSucceeded\n```\nCreated a story to track this: https://www.pivotaltracker.com/story/show/159629201. Closing this due to inactivity.\nCompiled releases are currently in use in various deployment strategies at the moment. A good example is bosh deployment.\nAt this point, I don't think tackling this issue through a global package cache is on our roadmap. Feel free to re-open this issue if this is still a concern.. Hi @omazhary,\nSorry that we haven't tackled this yet. You can find the corresponding story here. Hopefully we will get to it soon.\nBest,\n@jfmyers9. As bosh-init is not longer supported and bosh create-env exists in the cli repository, I have made another issue to properly track this. I'm going to close this issue as a result.\nNew Issue: https://github.com/cloudfoundry/bosh-cli/issues/463. Moved this issue to the bosh-cli: https://github.com/cloudfoundry/bosh-cli/issues/464. Hey all,\nI'm gonna go ahead an close this issue due to inactivity. If you run into this issue on the latest director version, please feel free to reopen / create a new issue.\n@gerhard You issue seems to be slightly different than the originally reported issue. If you are still running into this problem on the latest version of the director, could you create a new issue with updated recreation steps?\nBest,\n@jfmyers9. I just recreated this issue on my local bosh-lite and it appears the error message has improved:\n```\n[jfmyers9@ronaldo vbox]$ bosh -d bpm2 deploy ~/workspace/bpm-release/example-manifests/bosh-lite-ci.yml -v stemcell_name=ubuntu-trusty\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'bpm2'\n\nstemcells:\n\n\nalias: default\n\n\nos: ubuntu-trusty\n\nversion: '3541.9'\n\n\nreleases:\n\n\n\nname: bpm\n\n\n\nversion: 0.9.0+dev.16\n\n\nupdate:\n\ncanaries: 1\ncanary_watch_time: 5000-120000\nmax_in_flight: 1\nserial: false\n\nupdate_watch_time: 5000-120000\n\n\ninstance_groups:\n\n\n\nazs:\n\n\n\n\nz1\n\n\n\n\nz2\n\n\n\n\nz3\n\n\ninstances: 1\njobs:\n\n\nname: bpm\n\n\nrelease: bpm\n\n\nname: test-server\n\n\nrelease: bpm\nname: bpm\nnetworks:\n\n\nname: default\n\n\nstatic_ips:\n\n\n10.244.0.34\n\n\nstemcell: default\nvm_type: default\n\n\nazs:\n\n\n\n\nz1\n\n\n\n\nz2\n\n\n\n\nz3\n\n\ninstances: 1\njobs: []\nname: bpm2\nnetworks:\n\n\nname: default\n\n\nstatic_ips:\n\n\n10.244.0.34\n\n\nstemcell: default\n\nvm_type: default\n\n\nname: bpm2\n\n\nContinue? [yN]: y\nTask 60\nTask 60 | 18:54:06 | Preparing deployment: Preparing deployment (00:00:01)\n                   L Error: Failed to reserve IP '10.244.0.34' for instance 'bpm2/d8d30f51-4bdb-4d58-b8de-f304388be837 (0)': already reserved by instance 'bpm/9357b1a9-a48b-458f-91e3-5cb0d92dc91a' from deployment 'bpm2'\nTask 60 | 18:54:07 | Error: Failed to reserve IP '10.244.0.34' for instance 'bpm2/d8d30f51-4bdb-4d58-b8de-f304388be837 (0)': already reserved by instance 'bpm/9357b1a9-a48b-458f-91e3-5cb0d92dc91a' from deployment 'bpm2'\nTask 60 Started  Wed Aug 15 18:54:06 UTC 2018\nTask 60 Finished Wed Aug 15 18:54:07 UTC 2018\nTask 60 Duration 00:00:01\nTask 60 error\nUpdating deployment:\n  Expected task '60' to succeed but state is 'error'\nExit code 1\n```\nFeel free to create a new issue if this error messaging is still not clear enough.\nBest,\n@jfmyers9. At this time we are unable to update the version of monit due to a licensing change made by monit after v.5.2.5.\nFrom other threads, our recommendation is to not rely heavily on monit features and hopefully we will be moving away from monit in the stemcell as a longer term goal.\nHopefully this helps.. Closing this due to inactivity.\nTo @dpb587-pivotal 's point, we highly discourage the use of pre_packaging. Ideally we will be removing pre_packaging at some point in the future.. Hi all,\nThis appears to have been implemented. If you include a url: in the release block, the release will be uploaded to the director when running bosh update-runtime-config.\nBest,\n@jfmyers9. This can now be achieved by running bosh delete-config.. This has been fixed in more recent releases.. Closing this due to inactivity. The code has since been refactored quite a bit from its original state. If this is still an issue, please feel free to reopen this issue.. Closing this due to inactivity.\nAlso it appears that in recent versions of BOSH, we require the user to specify these certificates when deploying. These certificates are also able to be generated by using the bosh-cli + variables: block of the manifest.. Validated that this is an issue. Created a story to track this.. Hey all,\nAs @dpb587-pivotal pointed out, this should currently wait for up to 15 minutes before timing out on the compilation lock.\nAt this point in time, we consider this to be the expected behavior. We don't have a strong signal of what we should change the timeout to, and leaving the timeout unbounded could put us into situations where tasks are un-cancellable. Also, at this point we recommend that users use compiled to avoid unnecessary compilation of packages and improve the overall deployment experience.\nPlease feel free to reopen this issue if you have any more concerns or would like to discuss this further.\nBest,\n@jfmyers9 && @langered, CF BOSH. @henryaj,\nIt appears that you issue is slightly different as it deals with the release lock rather than the compiled package lock. It appears that we already have an issue open for this at https://github.com/cloudfoundry/bosh/issues/1995. Please feel free to continue the conversation there.\nBest,\n@jfmyers9 && @langered, CF BOSH. Validated that this is still an issue. Closing and reopening this story so that gitbot will create a corresponding tracker story.. Closing this as we have unblocked the pinning of rsyslog. Please feel free to reopen if the memory leak is still an issue.. As discussed, this change in behavior was somewhat intentional.\nGoing forward, we have introduced the migrated_from directive in the CPI config so that assets associated with the default CPI \"\" can be associated with a CPI specified in the CPI config. An example of this looks like:\n- name: cpi\n  migrated_from:\n  - name: \"\"\nAs of now, we don't expect CPI configs to work with v1 manifests.\nWe are going to close this issue, feel free to reopen if you have more concerns.\nBest,\n@jfmyers9 && @belinda-liu, CF BOSH. This has been implemented as of https://github.com/cloudfoundry/bosh/pull/1902.. Closing and reopening to generate a github story.. Hi @AbelHu,\nAs we are actively removing the BOSH registry, this isn't something that we are going to fix. Please feel free to reopen/open a new issue if you have further questions.\nBest,\n@jfmyers9, CF BOSH. Closing this due to inactivity. Please feel free to reopen this issue if this is still a persistent problem.. Seems like this has probably been addressed as of this story. Feel free to reopen if this is still an issue. . After a few attempts, I was not able to recreate this issue with cloud configs. There is a chance that this has been fixed at some point in the last year.\nHowever this doesn't appear to be done for runtime configs. If this is still an issue, feel free to create a new issue.\nThanks!. I wasn't able to reproduce this issue.\nClosing this due to inactivity. Feel free to reopen if you are still running into this issue.. This is a duplicate of https://github.com/cloudfoundry/bosh/issues/2033. We have fixed this in the latest BOSH release.\nFeel free to reopen / create a new issue if you are still seeing issues.. This appears to have been fixed. I am going to close this issue. Feel free to reopen if you have further questions.. Going to close this due to inactivity. Feel free to reopen / create a new issue if you have further questions.. Closing and reopening so that a tracker story gets created.. Just tested this on the new xenial stemcells on bosh-lite and it appears to work!\nbpm/b8d1c785-7915-41e9-aff7-6dff4560952b:~# tcpdump\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on wktiahlqvvh0-1, link-type EN10MB (Ethernet), capture size 262144 bytes\n02:58:05.116772 IP b8d1c785-7915-41e9-aff7-6dff4560952b.bpm.bpm.bpm.bosh.ssh > 192.168.50.1.46434: Flags [P.], seq 2630760462:2630760658, ack 2799067615, win 293, options [nop,nop,TS val 1968243993 ecr 3513736840], length 196\n02:58:05.117411 IP 192.168.50.1.46434 > b8d1c785-7915-41e9-aff7-6dff4560952b.bpm.bpm.bpm.bosh.ssh: Flags [.], ack 196, win 1444, options [nop,nop,TS val 3513737085 ecr 1968243993], length 0\nGoing to close this issue as we likely will not be addressing this on trusty stemcells due to the upcoming EOL.\n. Going to close and reopen this issue to generate a tracker story.. Closing and reopening to generate a github story.. I tried to reproduce this and was unsuccessful:\n```\n[jfmyers9@ronaldo vbox]$ bosh -d zookeeper deploy zookeeper.yml --recreate\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'zookeeper'\nRelease 'zookeeper/0.0.9' already exists.\ninstance_groups:\n  - name: zookeeper\n    azs:\n+   - z3\nContinue? [yN]: y\nTask 2769\nTask 2769 | 03:33:17 | Preparing deployment: Preparing deployment (00:00:00)\nTask 2769 | 03:33:18 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 2769 | 03:33:18 | Updating instance zookeeper: zookeeper/3396a3e3-8988-4bb8-b0d6-670014da467d (0) (canary) (00:00:38)\nTask 2769 | 03:33:56 | Updating instance zookeeper: zookeeper/89f1143c-2c4b-4304-bfc9-e965afc86eb4 (2) (canary) (00:00:39)\nTask 2769 | 03:34:35 | Updating instance zookeeper: zookeeper/e375c21b-9229-4110-b48a-2672ca4959ca (4) (00:00:38)\nTask 2769 | 03:35:13 | Updating instance zookeeper: zookeeper/4c42c86a-4f74-414f-995f-4aa30453f9f5 (3) (00:00:39)\nTask 2769 | 03:35:52 | Updating instance zookeeper: zookeeper/7ada26c3-f812-48d4-8512-b893ea60317c (1) (00:00:37)\nTask 2769 Started  Thu Sep 13 03:33:17 UTC 2018\nTask 2769 Finished Thu Sep 13 03:36:29 UTC 2018\nTask 2769 Duration 00:03:12\nTask 2769 done\nSucceeded\n```\nGoing to close this issue out due to inactivity. Feel free to reopen if this is still a problem.. Closing and reopening this to create a github story.. This seems to be documented by following the template accessor link on the link properties documentation. Going to close this issue. Feel free to reopen if you have further questions.. The story linked above has been accepted and released in current versions of BOSH. Going to close this issue. Feel free to reopen if you have further questions.. This is something that we are unlikely to support for the various reasons mentioned above.\nI'm going to close this issue due to inactivity. Feel free to reopen if you have more questions.. Hi @CAFxX,\nThanks for the issue. At the moment we aren't planning to implement a \"pre/post-instance\" lifecycle hook. In the future, we are discussing the ability to allow operators to have more fine grained control over when errands and services execute during a deploy. We hope that this future feature will provide the ability to have a more complex deploy process.\nAs a result, I am going to close this issue. Feel free to reopen / create a new issue if you have further questions.\nBest,\n@jfmyers9, CF BOSH. I'm going to close this issue as the original question has been answered.\nThe secondary feature request is currently being tracked by this issue: https://github.com/cloudfoundry/bosh/issues/1958. As the original question has been answered and there isn't much activity on this issue, I'm going to close it.\nPlease feel free to create a new issue to address trusted certificates on compilation VMs if that is still an issue.. The resurrection config has been released in the latest BOSH releases. Closing this issue.. While I think this would be a nice feature, this isn't something that is a priority in our future roadmap. I'm going to close this issue as a result. If you have further questions, feel free to reopen this issue.. The above story has been accepted and released in the latest BOSH director releases. I'm going to close this issue due to inactivity. If this above solution hasn't fixed your problem, please feel free to reopen this issue.\nThanks,\n@jfmyers9, CF BOSH. The only two values that are able to have white spaces in them are the director name and the credential name.\nFor the director name, we could probably do some amount of validation in the director itself to error out if users provide a name that contains invalid characters.\nFor the credential name, I think the manner in which it fails (credhub variable generation failure) seems appropriate for the time being.\nFor deployment names, there is currently validation to ensure that they do not contain spaces at the moment.\nI'm going to close and reopen this story to generate a github story.. Closing and reopening so that gitbot makes a story.. This appears to be a duplicate issue of https://github.com/cloudfoundry/bosh/issues/1632. As mentioned, this has hopefully been fixed as of this story. Feel free to reopen this issue if you have further questions or find the new output insufficient.. Closing this issue as it has been fixed in v266+.. Closing and reopening to generate a tracker story.. Closing due to inactivity. There are a large number of complications when it comes to migrating instance groups across AZs (i.e. persistent disks, ip addresses, etc). Therefore it is often a case by case basis on what the best course of action is for a deployment. Please feel free to reopen if you have further questions.. Closing this issue as the referenced story has been accepted and released in recent versions. Please feel free to reopen if you have more questions.. Hi all,\nAs of the latest BOSH director releases, we should be supporting TLS connections on both postgres and mysql for the director and the registry. I'm going to close this issue. Feel free to reopen it if you have any further questions.\nBest,\n@jfmyers9, CF BOSH. Hi @dmlemos,\nmonit reload is used for reloading the configuration for the monit daemon which lives in /var/vcap/monit. This should not be tied to changing persistent storage. If you want to restart the processes that monit is managing, you can use bosh restart to do so.\nI'm going to close this issue. Please feel free to reopen this issue if you have any further questions.\nBest,\n@jfmyers9, CF BOSH. As mentioned, post-start should run on every deploy in which BOSH updates an instance. This includes initial deploys. If a lifecycle step that runs before post-start fails (i.e. pre-start, monit start, etc), then the deploy will exit at that point and post-start will not run.\nI'm going to close this due to inactivity. Please feel free to reopen if you have further questions.. Going to close and reopen this issue to create a tracker story.. Going to close this issue due to inactivity.\nAs mentioned, bosh-cli functionality on Windows is not fully supported as of the moment.\nAlso this appears to be related to the create-env command, if you have further questions or would like to continue this conversation, could you please open a new issue on the https://github.com/cloudfoundry/bosh-cli repository.\nThanks.. Closing due to inactivity. \nThe CLI has gone through extensive work and multiple golang updates. Testing this with bosh int appears that commas are not stripped:\n```\n[jfmyers9@ronaldo vbox]$ bosh int creds.yml --path /value\n1,2,3,4,5\nSucceeded\n```\nIt's also worth mentioning that we are following the official YAML spec. So the behavior of the golang yaml parser should be up to date with the correct behavior.. This has been implemented as of https://github.com/cloudfoundry/bosh/pull/1902. I'm going to close this issue and reopen it to generate a tracker story.\nThis does not appear to be an immediate priority for the BOSH team to fix, but it is something that we would be open to accepting contributions for.. Going to close this issue as we are unlikely to fix this issue in the short term.\nWe do have long term features on the product roadmap to refactor how we deal with manifests and failed deploys which we are hoping will prevent this occurring in the future.\nPlease feel free to reopen this issue if you have further questions.. This is something that we are unlikely to fix in the near future. Even with the changes to the update strategy, this would still stop an instances work loads before starting them on other instances.\nOur current recommendation would be for users to manually plan for these situations in their instance counts of a cluster.. Hi all,\nWe've recently made the switch to using parted by default on all xenial stemcells. As this is the case, it is unlikely that we will be focusing on fixing this migration path in the future.\nDue to this, I'm going to close this issue. If you have further questions, or if this use case is a major issue, please feel free to reopen this issue to continue the conversation.\nBest,\n@jfmyers9, CF BOSH. If the partition table does not exist (i.e. new disks) we use gpt as the label type.\nDisks that are currently using MSDOS will need to be migrated, but moving forward we should be using gpt for all new disks on the xenial stemcell lines.. @xtreme-andrew-su I don't believe the documentation has been updated. That is the same documentation that was referenced in the original comment:\nThe bosh.io docs say:\n\nYou must ensure that your drain script exits in one of following ways:\n- exit with a non-0 exit code to indicate drain script failed\n- exit with 0 exit code and also print an integer followed by a newline to stdout (nothing else must be printed to stdout)\n\nThe issue here is that when you exit with a non-0 exit code, if you also output an integer to stdout it will not properly fail. There are 2 potential solutions:\n\nWe always respect the exit code and fail when it is non-0 (preferable)\nWe update the documentation to say something along the lines of: exit with a non-0 exit code to indicate drain script failed and do not output any numeric characters to stdout\n\nI would much prefer us to tackle this issue by following the first solution, but it hasn't been a priority in the past.. I think that the concerns brought up are still theoretically valid and might force us to reconsider this approach if we end up introducing logic that brings dependent services down in drain. Also, I think if we ended up in this situation, the entire process of draining BOSH workers would be questionable as most BOSH tasks would fail if it's dependent services were down.\nThat being said, we chose to just progress with this approach as its a small quality of life improvement for most operators and doesn't have any negative impacts. I believe we also fixed some issues with the current drain implementation as well.. Closing and reopening to generate a tracker story.. Closing this as a duplicate of #1169. Please feel free to reopen if you have further questions.\nBest,\n@jfmyers9 && @jaresty, CF BOSH. Hi @beyhan,\nIs this still an issue on the latest version of the BOSH director? If so, could you give us some specific steps to reproduce this issue? \nWe've tried recreating this on initial deploys, upgrade deploys, etc and still haven't been able to see the HM create a large number of alerts.\nLet us know if you have any other questions.\nBest,\n@jfmyers9 && @jaresty, CF BOSH. Hi @beyhan,\nWe tried recreating this behavior on our deployment and were unsuccessful. First we tried deleting a deployment with an unresponsive agent, and we saw that the VM was successfully deleted from the IAAS when using bosh delete-deployment --force. Then we tried setting the lifecycle: nil for the instance in question and once again saw that the VM was deleted from the IAAS when using the --force flag.\nWe are going to close this issue. If this is still a problem, please feel free to reopen the issue. If you could provide more specific steps to reproduce, that would also be helpful.\nBest,\n@jfmyers9 && @jaresty, CF BOSH . Closing as a duplicate of https://github.com/cloudfoundry/bosh/issues/1652. Please redirect any further comments to that issue.\nBest,\n@jfmyers9 && @jaresty, CF BOSH. Closing and reopening to generate a github story.. Hi @BooleanCat,\nThis is something that we are unlikely to implement in the future. We haven't seen a lot of feedback on this sort of use case, so we are hesitant to expand the interface of properties defined in the spec.  For those reasons, we are going to close this issue. Please feel free to reopen if you have further questions.\nBest,\n@jfmyers9 && @jaresty, CF BOSH. Hey @mattcui,\nThis happens when you have added a CPI config after previously uploading a stemcell to a director without a CPI config.\nWhen a director does not have a CPI config, it's default cpi is \"\". After uploading a CPI config, the stemcell will get associated with the matching CPI in the config, which in this case is softlayer_ng. One thing you can do to avoid this is to have the following migrated_from: block in your CPI config for the softlayer_ng CPI:\n- name: sotflayer_ng\n  type: softlayer\n  migrated_from:\n  - name: \"\"\nAny subsequent uploads will correctly detect any assets previously associated with the default CPI of \"\".\nAt this point in time, we don't consider this behavior a bug. Please let us know if you have any other questions.\nBest,\n@jfmyers9 && @GarfieldIsAPhilosopher, CF BOSH. bosh upload-stemcell --fix is used for reuploading the bits of the stemcell in the IAAS/blobstore, not for updating it's database representation. Unfortunately, if you have already uploaded the stemcell without the CPI config, there is not currently a way to get the previous stemcell to show up with the updated CPI name.\nUsing migrated_from: will prevent there from being duplicate stemcell references. This also is only a temporary issue when migrating to a CPI config, as all subsequent stemcells that are uploaded will reference the correct CPI.. Hey @mattcui,\nCan you provide us with the output of bosh task <task-number> --debug | grep \"changes\"? This should give us a good idea of what is causing all of these VMs to update.\nBest,\n@jfmyers9 && @GarfieldIsAPhilosopher, CF BOSH. Hi @mattcui,\nYou can use the migrated_from: block in the CPI config to acheive what you are looking for. Specifically you need to add:\nmigrated_from:\n- name: \"\"\nto the CPI that you wish to be associated with the original default CPI.\nLet us know if that works for you.\nBest,\n@jfmyers9 && @GarfieldIsAPhilosopher, CF BOSH . Hi @techie20122018,\nIt looks like your (or a similar users) question was answered here: https://cloudfoundry.slack.com/archives/C02HPPYQ2/p1534305747000100. \nFeel free to reopen this issue and let us know if you have any other questions if it is still unclear.\nBest,\n@jfmyers9 && @mikexuu, CF BOSH\n. @voelzmo I noticed you merged the previous incarnation of this PR. Is this something that you plan on merging? Or should we review / merge this in SF?. @voelzmo That makes sense to me.\nFor informational purposes, we have started a \"community\" role for pairs which involves triaging github issues and pull requests from a certain section in the icebox. If there are any pull requests or issues that you plan on handling, it would be awesome if you could either tag them or indicate that this is the case. We can discuss this new work flow a bit more in our weekly sync up if you still have questions. Thanks.. Hi @sjolicoeur,\nCan you give us an example of something that would have made this situation clearer to you?\nFrom our perspective: \nCan't use release 'loggregator-agent/2.0'. It references packages without source code and are not compiled against intended stemcells:\n - 'golang1.9.6/492fce63ab236ec3d01949398623824641c8ffbd02a04d952e90b33b4359a62b' against stemcell 'bosh-google-kvm-windows2012R2-go_agent/1200.22'\nseems to indicate that you are using a compiled release that was not compiled against your desired stemcell. The solution in this case would be to use a source release so that these packages could be compiled.\nLet us know what you think.\nBest,\n@jfmyers9 && @jrussett, CF BOSH\n. Hey @gossion,\nSeems like the workers aren't picking up the queued delayed jobs. I believe I have seen this before, and just restarting the worker processes seems to unblock this.\nIs there any chance you could give us the contents of the delayed_jobs table? This might give us some more insight into what is currently queued. This can be done by running:\nclass J < ::Sequel::Model(:delayed_jobs) ; end\nJ.all\nin the director console, or by using the psql binary to list the contents of the delayed_jobs table.\nThanks,\n@jfmyers9 && @mikexuu, CF BOSH. Hey @ishustava,\nWe are having a tough time reproducing this. In our reproduction, we see that the Downloading packages line only shows up for the newly created VM, but every previously created VM that gets reused still gets updated packages / templates.\nIs there any chance you could provide us with the debug log for the task that resulted in the deploy failure (31003) and task which failed to create the VM (30998)? Have you seen this multiple times, or was this the only instance?\nBest,\n@jfmyers9 && @dpb587-pivotal, CF BOSH. Hi @mxplusb,\nThe create-swap-delete functionality isn't guaranteed to work if templates rely on the IP addresses of the instances using this update method. That is due to the fact that the instances IP address will change when recreating the instance as a VM is pre-created before deleting the original VM. We recommend that users rely on domain names to reference instances. This can often be accessed through spec.address or link(...).instances[...].address.\nThat being said, using static IPs is still supported. BOSH will detect if there are instances in which instances cannot be updated using the create-swap-delete method (instances using a static IP address), and will default to using the previous delete-create update method.\nLet me know if you have any further questions.\nBest,\n@jfmyers9. Hi @carlhejiayu,\nCould you clarify what you are seeing with addons? From the deployment task output it looks like it properly updates the fim instance which is using the ubuntu-trusty os.\nAlso we attempted to reproduce this on our local deployment on version 268.3.0 and were unable to see the behavior that you described. If you could also try updating your BOSH director to a more recent version that would be helpful.\nBest,\n@jfmyers9 && @belinda-liu, CF BOSH. We just validated that @voelzmo explanation matches the behavior of the BOSH director. At the moment this appears to be the expected behavior. We would encourage you to use the os field when specifying stemcells: in your deployment manifest.. @jaresty Do you think it's valuable to break this out into a chore or a bug to explore further so that we can get a pair to spend some time digging into it during working hours? Otherwise I'll remind the community pair to take a look at this tomorrow.. This seems beneficial. Since this change touches a large number of files and has the potential for merge conflicts, I'm going to merge this now.. As to the original question, I also don't have any concrete reasoning as to why this is the case today. Hopefully we can sort this out as we review this PR soon.\nTangentially, do we have any idea if this reboot_vm functionality works in a world in which the BOSH agent is configured to use tmpfs for it's configuration?. @beyhan We've scheduled both of these PRs to be reviewed soon. Thanks for the submission.. Can you add a newline at the end of the file?. Rather than keeping track of the @active_ids, can we keep track of the sha256 of the content of active configs? This can be done through Digest::SHA256.digest(resurrection_config['content']).. Can you add a newline here?. 5 days seems like quite a long time to leave around an orphaned network. Is there a reason that this value is so high or can we reduce it to 1 day?\nAlso, we noticed that orphaned VMs aren't left around for a period of time and are always cleaned up on the schedule. Is there a reason to keep orphaned networks around for longer than the cleanup schedule?. It doesn't appear that we are adding these properties in this location of the director.yml. Shouldn't we be validating that the ScheduledOrphanedNetworkCleanup job is added to the list of scheduled_jobs?. Since this controller is unused, could you please remove it? We understand that this is for consistency with disks, however we would like to move away from how the orphaned disks controllers were implemented and move towards a single controller for all networks.. It appears that we didn't implement an orphan query parameter. Can you move this assertion into the previous it on line 71?. If we are planning on ever listing active networks as well as orphaned networks, we should consider adding an orphaned query parameter to list only the orphaned networks. This would prevent us from making a backwards incompatible change in the future.. This variable name is someone incorrect. It isn't quite JSON at this point. Can you update this variable name to be more accurate?. Is there a reason to change this from using find_by_ids? From the definition on the class it seems to be doing a very similar thing.. ",
    "trastle": "+1 \n. ",
    "dsboulder": "Hmmm - we are just using the AWS ruby SDK to access S3.  You're saying that amazon's SDK doesn't follow 301s?\n. This is separate, it allows BOSH to use any s3-compatible blobstore that is not actually S3.  Swift, Ceph, RiakCS, and others would be enabled by this PR (though the swift-api from the other PR is probably the \"best\" way to talk to swift).\n. Can we get this merged soon?\n. In order the maintain backward compatibility, I think we can't just switch over to a hash of all AWS options easily.\n. Ruben - The BOSH team doesn't want the external face of their product to be any options that could be passed into the AWS SDK for ruby.  We also have a go implementation of the BOSH agent that speaks to the blobstore, so we have to reimplement any allowed options in that language as well.  Luckily we don't allow arbitrary options to be passed in.\n. 1.  We can use #fetch instead of [], no specific reason. \n2.  This blobstore system is external and might or might not support SSL independently of whether the director uses it.\n. Good news, sort of.  This looks like a duplicate of https://github.com/cloudfoundry/bosh/pull/508, which just got merged yesterday or today.  If 508 is good enough, can you please close 516?\n. Renee - we've tested using BOSH against piston cloud's Ceph S3 blobstore, now that PR 508 has been merged.  \nWhat features do you want in addition to what's added to PR 508?  I was looking through your commit, is it related to providing SSL verify cert options?\n. Can we get this merged soon?  @cppforlife \n. Thanks!\n. I also noticed that the blobstore nginx does not come back up when rebooted because the unix socket is still present (the pre-start to clean it out doesn't run). This used to work because the socket was on a tmpfs, but when we switched to BPM a year ago we moved the socket to /var/vcap/data which does survive a reboot.. ",
    "jchesterpivotal": "Buildpacks team just got bitten by this. We consider it a bug.\nIf following redirects automatically will not be supported behaviour, it would be super duper helpful to get something other than an inscrutable stacktrace.\nFor example, something like \nLooked for blob name-of-binary in us-east-1, but was redirected to us-west-2. BOSH will not follow redirects. Either update your config/final.yml with your preferred endpoint, or ensure that your blob bucket is in us-east-1.\n. oh github, you so terrible\n. ",
    "caseymct": "Hi @drnic,\nthis has been delegated to the BOSH team; you can see the chore here. \n~ @thansmann & @caseymct\n. Hi @nicregez, \nHow's Zurich? \nTo start on this issue, let's see if we can get a manifest diff. Every 'bosh deploy' keeps the manifest that goes with it, from that might be able to tell is there was a config issue. \nstep 1 - find the 1471 and 1478 deployments\nbosh tasks recent | grep 'create deployment'\nstep 2 - Get the manifests from both of them and diff them\nbosh task <task_id_for_1471> --debug | perl -ne 'for(<ARGV>) { next if (not /DEBUG -- : Manifest:/ || $found) ; $found = 1; print ; exit if m{^$}s ;}' > /tmp/1471_manifest.yml\nrepeat for task_id_for_1478\nstep 3 - clear your passwds and other sensitive data from the manifests and post to a gist for review. \nstep 4 - in a separate gist, please give us the whole 'bosh task xxxx --debug' (again, clear your passwds and other sensitive data) output for the failing deploy.  \n~ @thansmann & @caseymct\n. Hi @nicregez \nSorry to miss this is a micro! There is another troubleshooting route. You can deploy the good micro, target it and use it to deploy a dummy release with the bad stemcell. You'll have to write a basic manifest for it, but once it's done it makes stemcell troubleshooting easier.\nWhen you were on-site we confirmed that stemcell worked as micro in our ci - I'd like to see if it works at all in your environment. \n~ @thansmann & @caseymct\n. Hi @nicregez, \nOur CI does a microbosh deploy for each stemcell we produce, however this stemcell was produced as we where converting to generating 'final' releases and likely had bits from multiple releases. The fix went in here. \nThe first build to be considered good is 1492.\nIt looks like what happened is that build 1478 was bad, but had good bits left over from 1471 due to a CI scripting issue. All the stemcells between 1471 and 1492 are deprecated.\nCan you try the latest build and let us know how that works for you?\n~ @thansmann & @caseymct\n. Hi Josh,\nLooks like we need to get you to sign one. We'll address this PR as soon as we receive the CLA. \nThanks!\n@julz and @caseymct\n. @joshuamckenty bump\n. Hi @rd7869,\nWe agree with @nicregez on this on one. The long, ugly names are overall best.\nHowever, if the issue is that you'd like a better PS1 when you're on the host (e.g. the job name and job index), you can get that with some state.yml parsing on the vm. If you're doing a custom release you can add these to the /etc/profile.d/ dir. In most cases, you're stuck with a vanilla CF release, so you can copy this from a file and paste it into the vm:\n```\nmac only\nalias good_ps1=''cat ~/etc/CF_good_PS1 | pbcopy'\n```\nContents of ~/etc/CF_good_PS1\n```\n. ~vcap/.bashrc\nJOB=$(ruby -ryaml -e 'y = YAML.load_file(\"/var/vcap//bosh/state.yml\"); puts y[\"job\"][\"name\"]')\nINDEX=$(ruby -ryaml -e 'y = YAML.load_file(\"/var/vcap//bosh/state.yml\"); puts y[\"index\"]')\nDEPLOYMENT=$(ruby -ryaml -e 'y = YAML.load_file(\"/var/vcap//bosh/state.yml\"); puts y[\"deployment\"]')\nif [[ -d \"/var/vcap/jobs/$JOB/config\" ]] ; then\n  C=\"/var/vcap/jobs/$JOB/config\"\nelse\n  C=$(find  /var/vcap/jobs/*/config -type d|head -1)\n  JOB=$(perl -e '@F=split(q{/}, $ENV{C}); print $F[-2]')\nfi\nPS1=\"$DEPLOYMENT//${JOB}:${INDEX}> \" # SIMPLE\nPS1='[\\033[01;35m]${DEPLOYMENT}[\\033[00m] on \\u@[\\033[01;32m]${JOB}[\\033[00m]:[\\033[01;32m]${INDEX}[\\033[00m]\\n[\\033[01;34m]\\w[\\033[00m]\\$ ' # FANCY\n```\n~ @thansmann & @caseymct\n. Hi @rd7869,\nThe BOSH Blobstore can be any one of S3, Swift, NFS, or a testing-only local blobstore. S3 is pretty hard to bring down. It may have avaliablity issues, but it's got 11 nines of data protection (S3 info). Swift can be configured to the nines you need. A clustered NFS like Isilon will easily give you 9-nines. \nThe local blobstore is a single-point of failure. We don't recommend it.\nCan you tell us more about your use case?\n~ @thansmann & @caseymct\n. Hi @rd7869,\nThe behavior you're describing is expected - bosh is an imperative system, it will do you what you tell it in the manifest. Can you tell us more about your use case and we'll see if we can recommend a work flow?\nIt'll help to see your deploy history. For each bosh deploy a 'bosh task xxxxx --debug | less' will show the manifest for the deploy as the first thing. WARNING it will have your passwds and secrets in it - best to clear those before you post. Please send us examples before and after stealing.\n~ @thansmann & @caseymct \n. ",
    "nicregez": "Today, the changes contained in the present pull request one more time saved me from a corrupted state of my microbosh.\nMay I kindly ask you to release this soon?\n. Hello Casey & Tony.\nThanks for taking care of this issue.\nStep 1: As you can see from my post, I wanted to bootstrap micro bosh from stemcell 1478, so there is no director present which I could query for any recent tasks.\nStep 2: Again, no bosh tasks to query and grep from.\nStep 3: https://gist.github.com/nicregez/7994561\nStep 4: Again, all I have is already contained in the original issue post.\nRegards,\nNic\n. Dear Casey.\nWould you mind doing an OpenStack micro deploy yourself? You probably gonna setup an CI for OpenStack anyway, so wouldn't this be a perfect opportunity do do it now?\nThanks & regards,\nNic\n. Yes, of course, I will do a bosh micro deploy with 1492 asap and report.\n. Ask Ruben about this topic as we raised this question in Bern two weeks ago and Ruben would give very convincing answer.\nIn short, as I understood him, using a uuid-like name for each vm is the only possible way to properly handle the references to the vms held by bosh in an infrastructure agnostic way.\n. bosh cck solved my problem.\n. ",
    "m1093782566": "Hi @Tammer,\nI think you can't image the painful experience of uploading the compiled packages of cf-release especially for roorfs and buildpack_cache when the network speed is not very fast. I have tested it for many times and I can't upload cf-release successfully without updating timeout values. In my env, 7200s works well. I wonder wy I can't pass  BAT? BTW, what's the best fit timeout_value do you think? Thanks! \n. @tsaleh and @resouer. Yeah, I only tested it for dav blobstore client, but I assume this issue happens in any blobstore using httpclient. I agree that high timeouts will slow things down in other occasions such as updating jobs like dea. In fact, I think bosh lock time is also affected by htttpclient timeouts:\nError 100: Bosh::Director::Lock::TimeoutError\nBTW, I have two solutions:\n1) add some parameters for bosh cli command. Like bosh deploy --timeout [timeout_value]\n2)Test network speed by uploading a small file before adding set_timeouts method for blobstore client . After uploading cf release packages, reset timeout values to default. In this way, timeout values would not affect other things.\n@tsaleh , what do you think about it?\n. @dajulia3 Okay, I will send a PR achieving --blobstore-timeout option for bosh deploy command. Thanks!\n. ",
    "tammer": "Hi @m1093782566 \nAre you sure you meant this message for me?\n. ",
    "rd7869": "Since this has been identified as a long term goal, closing the issue.\n. Any ETA?\n. ",
    "ytolsk": "@dbailey @goehmen see comment above.\n. This whole section is not tested. Can you add a few tests for this part too? Thank you :)\n. Could you add a test for the raising exception part? Thanks!\n. any reason we would not want to send deployment tag for alerts too? that if conditional is a bit odd to have here.\n. ",
    "jmtuley": "Hello @tsaleh,\nThere actually is a reason. Bosh::Monitor::Events::Alert instances don't actually have metrics (see https://github.com/cloudfoundry/bosh/blob/master/bosh-monitor/lib/bosh/monitor/events/alert.rb#L87) and so this code doesn't actually need to apply to them.\nWe'll refactor the code to be clearer on this point and add the new commit to this PR.\n@matthewmcnew / @jmtuley\n. @ryantang any update on this?\n@jmtuley and @jfoley (CF Community Pair)\n. @goehmen should I move the Tracker story for this into your inbox?\n. @goehmen,\nCan we move this to your icebox?\nThanks,\n@jmtuley and @tedsuo\nCF Community Pair\n. Hello @tsaleh,\nThere actually is a reason. Bosh::Monitor::Events::Alert instances don't actually have metrics (see https://github.com/cloudfoundry/bosh/blob/master/bosh-monitor/lib/bosh/monitor/events/alert.rb#L87) and so this code doesn't actually need to apply to them.\nWe'll refactor the code to be clearer on this point and add the new commit to this PR.\n@matthewmcnew / @jmtuley\n. ",
    "mrdavidlaing": "Dear future S3 apprentice,\nIf you get stuck at this stage (like me), wondering how to grant the correct permissions on your newly created bosh S3 bucket, you need to add the following $BUCKET_NAME > Properties > Permissions > Add bucket policy via the S3 console\nNB: make sure you replace the $NAME placeholders\n{\n    \"Version\": \"2008-10-17\",\n    \"Id\": \"Policy1391410203403\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Stmt-private-yml-user\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"$IAM_PRIVATE_USER_ARN\"\n            },\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::$BUCKET_NAME/*\"\n        },\n        {\n            \"Sid\": \"Stmt-final-yml-user\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"$IAM_PUBLIC_USER_ARN\"\n            },\n            \"Action\": [\n                \"s3:List*\",\n                \"s3:Get*\"\n            ],\n            \"Resource\": \"arn:aws:s3:::$BUCKET_NAME/*\"\n        }\n    ]\n}\n. Ok.  Gem dependancies fixed (must have mis-merged).\nBut now I get a host of Rubocop failures in code I haven't changed.\nHelp!\n. Advice followed, Rubocop happy.  Thanks!\nBUT; it looks like the aws-sdk update has made bosh_cli_plugin_aws unhappy.  Bummer.\n. All the errors are: AWS::RDS::Client does not implement: describe_db_subnet_groups.\nThis is strange; because that method definitely exists; although it is dynamically generated.\n- lib/aws/rds/client.rb \n- lib/aws/api_config/RDS-2013-09-09.yml#L1196\nMy guess is that let(:fake_aws_rds_client) { instance_double('AWS::RDS::Client') } is getting confused by the dynamic method generation.\nChanging this to let(:fake_aws_rds_client) { double('AWS::RDS::Client') } fixes the problem.\nIs this an acceptable fix?\n. Hurrah!  Green build!\n:green_apple: for me!\n. @dbailey @tsaleh, @atomanyih - Bump.\nAny further feedback on this PR?\nTests are passing; so I propose it is ready for merging.\n. @goehmen, I have two BOSH deployments - CF and ElasticSearch.  In both I run some of the \"redundant\" nodes (eg, CF-DEA and ElasticSearch non-master nodes) as spot instances.\nI have the resurrector enabled for the spot instances, and it works fantastically.  \nTypically with AWS spot instances the price spikes for a short period, and then drops back down.\nIn this scenario the resurrector notices; and attempts to start up a new node.  When the spot price is too high, the resurrection fails.  (I set a short validity time on the AWS spot request, so this gets cancelled at about the same time as the resurrector gives up waiting)\nThe resurrector seems to keep trying, and when the spot price has dropped again, the resurrection succeeds and the spot nodes join back to the cluster.\n. Just to clarify; just because these are spot instances, it doesn't mean that they get started and stopped frequently.  I've had spot instances that have run for 3 months before a price spike put them out of action for a day.  I'm choosing to take that very occasional \"server down\" risk in exchange for the huge (90%) cost savings.\nI'm really after the simplest possible functionality that lets me start a \"spot instance\" resource pool.  The fact that the BOSH resurrector happens to make that resource pool \"persistent\" is just icing on the cake.\nAnyway, onto your questions\n\n\nHow would you expect monitoring and logging to react in a production/enterprise sense?\n\n\nNo differently to any other VM started by BOSH.  I expect the spot instances to run just like normal on demand instances, but \"fail\" slightly more frequently.\n\n\nWhat is the resource implication of the resurrector's continuous attempts?\n\n\nI'd imagine pretty low; since spot instances hardly get terminated.\n. @goehmen - I'd be happy to share data on my BOSH releases that are using spot instances; just let me know what data you'd like.\nI currently have the following running in AWS eu-west-1 VPC:\n``\n$ bosh vms\nDeploymentmeta-logsearch-io'\nDirector task 1513\nTask 1513 done\n+----------------------------+--------------------+-------------------------+-------------------------+\n| Job/index                  | State              | Resource Pool           | IPs                     |\n+----------------------------+--------------------+-------------------------+-------------------------+\n| api/0                      | running            | vpc-public-small        | 10.0.0.51, 54.xxxxxxxxx |\n| elasticsearch_autoscale/0  | running            | vpc-private-large-spot  | 10.0.1.64               |\n| elasticsearch_persistent/0 | running            | vpc-private-large       | 10.0.1.52               |\n| ingestor/0                 | running            | vpc-public-small        | 10.0.0.52, 54.xxxxxxx |\n| log_parser/0               | running            | vpc-private-medium-spot | 10.0.1.62               |\n| log_parser/1               | running            | vpc-private-medium-spot | 10.0.1.63               |\n| queue/0                    | running            | vpc-private-medium      | 10.0.1.51               |\n+----------------------------+--------------------+-------------------------+-------------------------+\nVMs total: 7\nDeployment `monitor-cloud'\nDirector task 1514\nTask 1514 done\n+--------------------+---------+-------------------------+--------------------------+\n| Job/index          | State   | Resource Pool           | IPs                      |\n+--------------------+---------+-------------------------+--------------------------+\n| api/0              | running | vpc-public-small        | 10.0.0.12, 54.xxxxxxxx |\n| cloud_controller/0 | running | vpc-private-small       | 10.0.1.14                |\n| core/0             | running | vpc-private-small       | 10.0.1.13                |\n| data/0             | running | vpc-private-small       | 10.0.1.12                |\n| dea/0              | running | vpc-private-medium      | 10.0.1.54                |\n| dea_spot/0         | running | vpc-private-medium-spot | 10.0.1.53                |\n+--------------------+---------+-------------------------+--------------------------+\nVMs total: 6\nDeployment `monitor-cloud-jenkins'\nDirector task 1515\nTask 1515 done\n+------------------+---------+-----------------+---------------------------+\n| Job/index        | State   | Resource Pool   | IPs                       |\n+------------------+---------+-----------------+---------------------------+\n| jenkins_master/0 | running | vpc-public-spot | 10.0.0.70, 54.xxxxxxxx |\n+------------------+---------+-----------------+---------------------------+\n```\n. @jbayer, @goehmen - thanks for keeping this moving forward.  Let me know if I can help out in any way.\n. @goehmen - BOSH Pivotal Tracker > cloudfoundry/bosh #509: Add aws spot support says:\n\nwaiting for story #65132950 to complete before executing PR\n\nI don't have permission to see the status of #65132950 or what its about.\nLet me know if you need any further info from me\n. @goehmen, @rboshman; thanks for the clarification.  \nI'm keen to keep pushing this forward; and responding to the new waves of objections that seem to come every week.  Grr :smile: \nI can/have always been able to see https://www.pivotaltracker.com/s/projects/956238/stories/64444950.  However, it hasn't had any new info posted to it since Feb 8th beyond the (trashed) link to 65132950.\n@goehmen, could you give me an update on the status of this PR please.\n- Do I need to convince people that starting spot instances from BOSH is useful?\n- Do I need to refactor the code / add more tests?\n- Do I need to find a way to focus the AWS-SDK update?\n. @jbayer - thanks for the quick update.  \nI can see that #64444950 is scheduled for the iteration starting 10 March 2014; so I'll make sure I'm around to answer any questions then.\n.   Yay!\nI'm around to answer any questions\nOn 25 February 2014 19:12, goehmen notifications@github.com wrote:\n\n@mrdavidlaing https://github.com/mrdavidlaing FYI - the story is\nactually slated for the 03 March iteration but was put at the top of the\nbacklog for that iteration so that it could be easily pulled into the\ncurrent iteration if the opportunity was to arise. There is a strong\nlikelihood that it is being pulled in today as there were a number of\nstories above it that have been delivered. Stay tuned!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36045623\n.\n\n\nDavid Laing\nOpen source @ City Index - github.com/cityindex\nhttp://davidlaing.com\nTwitter: @davidlaing\n. That sounds fine to me.\nOn 26 February 2014 18:11, rboshman notifications@github.com wrote:\n\nWe stated on this PR this morning. We are going to release this as\n\"experimental functionality\" and there will be a warning message to this\neffect at deploy time. David - if you want to have input on the message\ncontent, you are welcome to check it out after we merge. I didn't want to\npush this back on you yet again so we are crafting the verbiage and moving\nforward.\nAs we see adoption and gain confidence in the feature, we will phase out of\n\"experimental\". I promise it will happen faster than Google moved gmail\nout of beta. :-)\n-Greg\nOn Wed, Feb 26, 2014 at 2:31 AM, David Laing <notifications@github.com\n\nwrote:\n Yay!\nI'm around to answer any questions\nOn 25 February 2014 19:12, goehmen notifications@github.com wrote:\n\n@mrdavidlaing https://github.com/mrdavidlaing FYI - the story is\nactually slated for the 03 March iteration but was put at the top of\nthe\nbacklog for that iteration so that it could be easily pulled into the\ncurrent iteration if the opportunity was to arise. There is a strong\nlikelihood that it is being pulled in today as there were a number of\nstories above it that have been delivered. Stay tuned!\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36045623>\n.\n\n\nDavid Laing\nOpen source @ City Index - github.com/cityindex\nhttp://davidlaing.com\nTwitter: @davidlaing\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36111576>\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manger - Bosh\nPivotal, Inc.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-36156809\n.\n\n\nDavid Laing\nOpen source @ City Index - github.com/cityindex\nhttp://davidlaing.com\nTwitter: @davidlaing\n. Yay!\n. @goehmen, @jbayer - I'd be happy to write a blog post.\nCan you tell me for which blog; and point to something similar in terms of length, audience & tone.\n. @jbayer - thanks for the advise.  I'll have a crack at a draft article whilst at QCon next week & then run it past you and @goehmen for comments prior to publishing.\n. @goehmen,\nSorry; in the flurry to release logsearch.io I haven't had a chance to\nwrite this post.\nWill hopefully get to it next week.\nD\nOn 10 April 2014 01:01, goehmen notifications@github.com wrote:\n\n@mrdavidlaing https://github.com/mrdavidlaing Did you get time to draft\na blog post about spot instances? I'm compiling info for a \"release notes\"\ntype of document and would like to include an announcement about your work.\nCheers/\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/509#issuecomment-40030491\n.\n\n\nDavid Laing\nlogsearch.io - build your own open source cloud logging cluster\nhttp://davidlaing.com\n. Looks like the build on develop is also failing.  Will wait 'til develop is fixed, and then merge fix into this branch.\n. Ok, build is fixed; this is ready for review / merging\n. @goehmen, I'm just in the process of getting ready to write the spot instance blog post; currently finalising a spot only CF deployment - https://github.com/mrdavidlaing/multi-az-spot-cloudfoundry-bosh-deployment - so that the blog post can be \"this is how you can use spot\", and \"this is why you should consider using many / all spot instances in your deployments\"\n. @goehmen - any chance you could share the size (eg, type & number of instances) of the run.pivotal.io CF deployment?\n. @mariash Test intentions updated as suggested.\n@goehmen A screencast of deploying a CF cluster to AWS using spot instances\n. @goehmen - here is a preview of the draft CF blog article.  Much editing still required.\n. Super - looks like this has made it into BOSH build 2559.  Thanks!\n. @goehmen,\n\na clear path to acceptance for this\n\nI'm not sure I understand what that means.  However, I'm currently happy :)\n. :+1:\n. @goehmen, @jbayer - I've submitted an article for blog.cloudfoundry.com on using AWS spot instances.   What is the process for receiving feedback & getting it published?\n(just want to make sure my mailbox isn't spamming emails from some automated service :worried: )\n. Yep - on the .org blog - http://blog.cloudfoundry.org/2014/05/22/using-aws-spot-instances-to-cut-the-cost-of-your-bosh-deployments/\n. @goehmen - This is a refinement of and adds to my previous spot PR #577 \n. Sorry about the extra commits; picked up this additional error scenario around price-too-low errors during a spot price spike on one my live clusters, and thought the fix belonged in this PR\n. @calebamiles, I'm done with the refactoring.\nFrom my perspective this is ready to merge.\nI will try wrap my head around the AWS lifecycle tests, and submit these as a separate PR.\n. Hmm.  I don't understand why the build failed - seems to be something to do with the go agent; which is unrelated to my changes.  Going to try merge latest changes from develop\n. Grr.  I see that upstream develop branch builds are currently failing - https://travis-ci.org/cloudfoundry/bosh/builds/27364097\nI'm going to wait until these get fixed and are passing and then try again\n. @calebamiles, Any idea when the build for the develop branch will be passing again?\n. @calebamiles, sigh - I just can't seem to find a develop commit that actually passes the Travis-CI tests. :(\nAny ideas?\n. As requested, a separate PR with some AWS lifecycle tests - https://github.com/cloudfoundry/bosh/pull/594\n. @goehmen, Finally, a green build in develop that also works in this branch.  \nPlease could you schedule this PR for merging, along with the related  AWS lifecycle tests - #594\nThanks!\n. @goehmen - Brilliant; thanks.  Was just checking that you knew that the build was finally green :)\n. Oops - wrong branch :(\n. Please see https://github.com/cloudfoundry/bosh/pull/594\n. @krishicks & @mariash - Great!  Thanks for the update & merge\n. @krishicks & @mariash - question - should #585 be closed?\n. @cppforlife - @dpb587 is doing this work on behalf of City Index (https://github.com/cityindex/), who you should have a Corporate CLA from.  Please send an email to david.laing@cityindex.com if you don't; and I'll dig out the paperwork and resubmit.  Thanks\n. @dpb587 Just out of interest, in your testing is it possible to capture any data about the improvements in network performance that this brings?\n. Dern it, wrong merge branch target\n. Using spot instances required an upgrade of AWS-SDK.  Unfortunately, this caused these (unrelated) tests to start failing:\n1) Bosh::Aws::RDS subnet_group_exists? should return false if the db subnet group does not exist\n     Failure/Error: fake_aws_rds_client.should_receive(:describe_db_subnet_groups).\n       AWS::RDS::Client does not implement:\n         describe_db_subnet_groups\n     # ./spec/unit/rds_spec.rb:17:in `block (3 levels) in <top (required)>'\nAs discussed in the comment above, I think  let(:fake_aws_rds_client) { instance_double('AWS::RDS::Client') } is getting confused by the dynamic method generation used by the AWS-SDK.  \nSwitching to double seemed the smallest change to fix the confused tests, allowing all tests to start passing.\nCan you think of a better way?\n. @d - Are you asking me to take an action here?\nAs I mentioned in the original comment; I can't think of \"lower impact\" way to make the tests pass; and am very open to better options.\nIn order to use spot instances, we must update the AWS-SDK for bosh_aws_cpi. (the spot api calls in 1.8.5 fail with the current AWS EC2 API).\nPerhaps we could update the AWS-SDK for just  bosh_aws_cpi?  \nIs it possible / desirable to have different BOSH components use different AWS-SDK versions? \n. @mariash - I've been thinking that its nearly time to refactor out the spot logic into a separate class.  Would you like me to do that as part of this PR, or as a separate PR (I'm nervous of including too many changes in a single PR)\n. Apologies for being late to the party; but I've just noticed that the heartbeat events are not going to be forwarded over syslog.\nI'd really like to be able to forward heartbeat data to my log aggregation system (ELK).\nWould it be possible to also send heartbeat data to syslog?  (Or add a setting that enables it, with it switched off by default)\n. ",
    "grenzr-bskyb": "Sounds good. I think spurious adding of XYZ options for XYZ blobstores as is being done presently is not the best approach, but will do for now. I have already started prepping the PR as I mentioned above. Should I take your message as a notice to hold off for now? I'd still like to see changes added sooner so I selfishly dont have to keep operating in my own fork of BOSH, but can do if need be.\nI am still a fan of the use of fog and allowing the BOSH manifest to set as many options as required to connect with your blobstore of choice, as was eventually allowed in cc_ng.\nI look forward to seeing what ideas come out of your conversations internally.\nRyan\n. Thanks - working on this now.\n. thanks :)\n. ",
    "dsabeti": "@grenzr \nGo ahead with the approach you mentioned previously: rebasing, adding the parameters you specified above (s3_multipart_threshold and ssl_verify_peer), and adding support for the compiled_package_cache.\nOur only requirement is that the host, port and SSL fields be configured separately, as opposed to the other approach of specifying an endpoint as the single parameter. \nThe bosh team is still figuring out how they want to handle the bigger picture of passing the parameters - especially in light of the agent rewrite - which is underway.\nCF Community Pair (@dsabeti & @thecadams )\n. @grenzr \nGo ahead with the approach you mentioned previously: rebasing, adding the parameters you specified above (s3_multipart_threshold and ssl_verify_peer), and adding support for the compiled_package_cache.\nOur only requirement is that the host, port and SSL fields be configured separately, as opposed to the other approach of specifying an endpoint as the single parameter. \nThe bosh team is still figuring out how they want to handle the bigger picture of passing the parameters - especially in light of the agent rewrite - which is underway.\nCF Community Pair (@dsabeti & @thecadams )\n. @grenzr \nYou may want to look at PR #521. It looks like in includes your changes, but also contains the necessary specs. Does that patch include all of the changes to the configuration that you need? If so, we'd like to close this pull request.\nCF Community Pair (@dsabeti & @thecadams )\n. @grenzr \nYou may want to look at PR #521. It looks like in includes your changes, but also contains the necessary specs. Does that patch include all of the changes to the configuration that you need? If so, we'd like to close this pull request.\nCF Community Pair (@dsabeti & @thecadams )\n. @reneedv \nWe're hesitant to move forward with this pull request because we intend only to use the host-port-ssl configuration (instead of the endpoint format). \nAlso, from my understanding, the s3 client wasn't even using the endpoint configuration properly, because it was only using the endpoint to fallback to a simple blobstore client. I imagine that @monkeyherder has more to say about (and a greater understanding of) this.\nCF Community Pair (@dsabeti & @thecadams )\n. @reneedv \nWe're hesitant to move forward with this pull request because we intend only to use the host-port-ssl configuration (instead of the endpoint format). \nAlso, from my understanding, the s3 client wasn't even using the endpoint configuration properly, because it was only using the endpoint to fallback to a simple blobstore client. I imagine that @monkeyherder has more to say about (and a greater understanding of) this.\nCF Community Pair (@dsabeti & @thecadams )\n. @matjohn2, any word on this? If not, we'll file a story with the BOSH team and close this issue.\nCF Community Pair (@dsabeti & @thecadams)\n. @matjohn2, any word on this? If not, we'll file a story with the BOSH team and close this issue.\nCF Community Pair (@dsabeti & @thecadams)\n. Hi @gberche-orange ,\nAs you noted, previous commits to this file didn't include appropriate testing -- that's our mistake, but we don't want to let bad habits persist if we can avoid it. After talking with the BOSH team, we've decided that we need to add these missing tests.\nWe'd like to give some guidance on how to write the kind of test we'd like to see. As you mentioned above, there are several ways we could test this (unit tests with mocks as the 'lowest' level, and BATS as the 'highest' level). We definitely would like to see at least a unit test associated with this kind of change, although a BAT might also be valuable (as you discussed in bosh-dev).\nRather than verifying that CONNECTION_EXCEPTIONS contains HTTPClient::BadResponseError, one option is to verify retryable receives HTTPClient::BadResponseError as a parameter. Something akin to:\nallow(Bosh::Common).to receive(:retryable)\nBosh::Deployer::InstanceManager.create(config)\nexpect(Bosh::Common).to have_received(:retryable) do |args|\n  expect(args[:on]).to include HTTPClient::BadResponseError\nend\nWe understand that there aren't many tests to use as an example, so let us know if you need more help.\nCF Community Pair (@dsabeti & @thecadams)\n. Hi @gberche-orange ,\nAs you noted, previous commits to this file didn't include appropriate testing -- that's our mistake, but we don't want to let bad habits persist if we can avoid it. After talking with the BOSH team, we've decided that we need to add these missing tests.\nWe'd like to give some guidance on how to write the kind of test we'd like to see. As you mentioned above, there are several ways we could test this (unit tests with mocks as the 'lowest' level, and BATS as the 'highest' level). We definitely would like to see at least a unit test associated with this kind of change, although a BAT might also be valuable (as you discussed in bosh-dev).\nRather than verifying that CONNECTION_EXCEPTIONS contains HTTPClient::BadResponseError, one option is to verify retryable receives HTTPClient::BadResponseError as a parameter. Something akin to:\nallow(Bosh::Common).to receive(:retryable)\nBosh::Deployer::InstanceManager.create(config)\nexpect(Bosh::Common).to have_received(:retryable) do |args|\n  expect(args[:on]).to include HTTPClient::BadResponseError\nend\nWe understand that there aren't many tests to use as an example, so let us know if you need more help.\nCF Community Pair (@dsabeti & @thecadams)\n. @gberche-orange \nFirst of all, good work trying to work around the that file, especially considering the poor existing test coverage. The test you provided isn't ideal, but we've been struggling to write a test that looks much better -- a smell that the code is gnarly and needs more work.\nWe've talked with the BOSH team about this PR, and they brought up a few concerns and updated us on the direction they're taking with that part of the code. \nFirst, they mentioned that HTTPClient::BadResponseError catches too many unrelated errors. Take a look at this test in the HTTPClient codebase: BadResponseError is raised as a result of invalid auth. They've asked that you find a way to specifically handle only 502 and/or 503. They mentioned that this might be more complicated than adding a parameter to the retryable block; you might need to add something to BoshRetryable that can delve into the error and check the status. Some other tests might change too as a result of this work and that should be fine.\nSecond, this PR seems to include two distinct changes: one to allow the micro-deployer to work with a proxy, and another to allow the downloader to use a proxy. They asked that these be split into two separate PRs so they can be reviewed independently.\nAs soon as you open the new pull requests, we'll put them into the BOSH icebox straightaway and the BOSH team will be working with you directly to get the changes merged in.\nCF Community Pair (@dsabeti and @thecadams)\n. @gberche-orange \nFirst of all, good work trying to work around the that file, especially considering the poor existing test coverage. The test you provided isn't ideal, but we've been struggling to write a test that looks much better -- a smell that the code is gnarly and needs more work.\nWe've talked with the BOSH team about this PR, and they brought up a few concerns and updated us on the direction they're taking with that part of the code. \nFirst, they mentioned that HTTPClient::BadResponseError catches too many unrelated errors. Take a look at this test in the HTTPClient codebase: BadResponseError is raised as a result of invalid auth. They've asked that you find a way to specifically handle only 502 and/or 503. They mentioned that this might be more complicated than adding a parameter to the retryable block; you might need to add something to BoshRetryable that can delve into the error and check the status. Some other tests might change too as a result of this work and that should be fine.\nSecond, this PR seems to include two distinct changes: one to allow the micro-deployer to work with a proxy, and another to allow the downloader to use a proxy. They asked that these be split into two separate PRs so they can be reviewed independently.\nAs soon as you open the new pull requests, we'll put them into the BOSH icebox straightaway and the BOSH team will be working with you directly to get the changes merged in.\nCF Community Pair (@dsabeti and @thecadams)\n. We're hesitant to base the logic around matching against the error messages. Also, it looks like BadResponseError has a field called res -- the HTTP response object associated with the error. So you could check on error.res.status to see if the HTTP error was 502 or 503.\nThe BOSH team also suggested that your new on_matching parameter could be a hash that uses rror types (e.g. BadResponseError) ask keys and lambda functions as values. These lambda functions would return true/false to say that the retryable object should retry or not. So the lambda function might say something like\nlambda { |error| error.res && [502, 503].include?(error.res.status) }\nCF Community Pair (@dsabeti & @thecadams)\n. We're hesitant to base the logic around matching against the error messages. Also, it looks like BadResponseError has a field called res -- the HTTP response object associated with the error. So you could check on error.res.status to see if the HTTP error was 502 or 503.\nThe BOSH team also suggested that your new on_matching parameter could be a hash that uses rror types (e.g. BadResponseError) ask keys and lambda functions as values. These lambda functions would return true/false to say that the retryable object should retry or not. So the lambda function might say something like\nlambda { |error| error.res && [502, 503].include?(error.res.status) }\nCF Community Pair (@dsabeti & @thecadams)\n. Hi @matjohn2, is this PR still in progress?\nCF Community Pair (@dsabeti & @thecadams)\n. Hi @matjohn2, is this PR still in progress?\nCF Community Pair (@dsabeti & @thecadams)\n. Sweet. We're moving this into the BOSH icebox for prioritization. Someone from that team will merge it in.\nCF Community Pair (@dsabeti & @thecadams )\n. Sweet. We're moving this into the BOSH icebox for prioritization. Someone from that team will merge it in.\nCF Community Pair (@dsabeti & @thecadams )\n. Great, thanks for the PR! We've put this in the BOSH team's icebox.\nCF Community Pair (@dsabeti & @thecadams)\n. Great, thanks for the PR! We've put this in the BOSH team's icebox.\nCF Community Pair (@dsabeti & @thecadams)\n. @cppforlife We just hit this issue today. \nWe had capi-release v1.29 compiled against stemcell v3363.24. Then, the stemcell version got bumped to version 3421. We recompiled the release against the new stemcell -- capi-release v1.29 with stemcell v3421. When we try to deploy, the BOSH director gives an error:\n```\n...\nRelease 'capi/1.29.0' already exists.\n...\nCan't use release 'capi/1.29.0'. It references packages without source code and are not compiled against stemcell 'bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3421':\n```\nOne way to understand this problem is that it impacts the ability to continuously deliver. As valuable as it is for bosh to compile releases as part of its deploy workflow, the vast majority of users use one of two or three stemcells and so would much rather use compiled releases. Especially with larger deployments like CF, we've gotten a lot of feedback that people prefer compiled releases. If we manage to solve this problem and ship compiled releases for everything, deploying a full Cloud Foundry becomes much more reasonable. \nWe could definitely settle for some kind of flag, like @voelzmo suggested, that tells the CLI and the Director to force an update of the release even if the version numbers are the same. (My bet is that everyone would start using this flag all of the time, but you could do some validation for that.)  Another thing I noticed is that the SHA1 for the release changed when we compiled against a new stemcell: maybe the Director could (should?) notice that the SHA1 is different and insist on uploading the release.\ncc @staylor14 @anEXPer . @cppforlife We just hit this issue today. \nWe had capi-release v1.29 compiled against stemcell v3363.24. Then, the stemcell version got bumped to version 3421. We recompiled the release against the new stemcell -- capi-release v1.29 with stemcell v3421. When we try to deploy, the BOSH director gives an error:\n```\n...\nRelease 'capi/1.29.0' already exists.\n...\nCan't use release 'capi/1.29.0'. It references packages without source code and are not compiled against stemcell 'bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3421':\n```\nOne way to understand this problem is that it impacts the ability to continuously deliver. As valuable as it is for bosh to compile releases as part of its deploy workflow, the vast majority of users use one of two or three stemcells and so would much rather use compiled releases. Especially with larger deployments like CF, we've gotten a lot of feedback that people prefer compiled releases. If we manage to solve this problem and ship compiled releases for everything, deploying a full Cloud Foundry becomes much more reasonable. \nWe could definitely settle for some kind of flag, like @voelzmo suggested, that tells the CLI and the Director to force an update of the release even if the version numbers are the same. (My bet is that everyone would start using this flag all of the time, but you could do some validation for that.)  Another thing I noticed is that the SHA1 for the release changed when we compiled against a new stemcell: maybe the Director could (should?) notice that the SHA1 is different and insist on uploading the release.\ncc @staylor14 @anEXPer . +1 to a field like compiled-for that users can add in their manifest. This would certainly solve the issue for us in cf-deployment.. +1 to a field like compiled-for that users can add in their manifest. This would certainly solve the issue for us in cf-deployment.. @spunky11, you can follow these directions to deploy Cloud Foundry to bosh-lite. You'll probably want to start at step 4, since the first few steps have to do with setting up a bosh-lite on a public cloud like AWS or GCP, and you're using you're local VirtualBox instead.. ",
    "thecadams": "@grenzr Yes please. Thanks so much.\nCF Community Pair (@drich10 and @thecadams)\n. Thanks! Closing this PR and focusing on the new one!\nCF Community Pair (@drich10 and @thecadams)\n. @gberche-orange there is nothing to worry about, thank you for your patience as we are working out what to do to try to solve your problem.\n/cc @mmb, @cppforlife; do you guys have an opinion on whether @gberche-orange should continue working to support http proxy over flows A and F in the first diagram above?\nRegards,\nCF Community Team (@thecadams)\n. Hey @matjohn2! \nThanks for getting back to us and just let us know when you're done!\nRegards,\nCF Community Pair (@drich10 and @thecadams)\n. @matjohn2 Thanks!\nWe've just chatted to @goehmen and he's agreed to have the BOSH team look at it in due course. Here's the story and you can follow its progress on the BOSH tracker here.\nRegards,\nCF Community Pair (@drich10 and @thecadams)\n. Hi guys!\nThanks for the pull request, and \"props\" for getting this worked out while airborne - or should I say jets? :airplane:\nI've asked the BOSH team to take a look and get back to you.\nCF Community Team (@thecadams)\n. @monkeyherder when you get a chance can you please take a look and give these guys some feedback?\nThanks,\nCF Community Pair (@drich10 and @thecadams)\n. Hi,\nThanks for raising this issue. In lieu of the obvious solution of improving your connection, we've just chatted to the PM of BOSH and we'd welcome a pull request to add that retry feature.\nDo other large uploads also timeout? If so, you may want to check whether your MTU is set correctly on your connection; perhaps path MTU discovery is not working correctly.\nHowever the most likely situation is you were affected by degraded S3 service which occurred yesterday. You might want to just try again today, it might have gotten better.\nLet us know if there's any more we can do to help.\nThanks,\nCF Community Pair (@drich10 and @thecadams)\n. Thanks for resubmitting this PR against the develop branch! We have moved this story into the BOSH team's icebox to complete. You can follow the progress here.\nRegards,\nCF Community Pair (@drich10 and @thecadams)\n. ",
    "Zouuup": "@tsaleh any news on this?\n. ",
    "tjvman": "Closing - Microbosh has been deprecated for a while. If you're looking for a way to spin up initial directors, bosh-deployment uses v2 CLI functionality to spin up a director, and has templates for AWS, vSphere, OpenStack (and more).. Closing - The v2 CLI has support for both single-instance recreation with bosh recreate, as well as a --dry-run flag for deploy that will show what instances will be affected without actually making any changes. . Closing - Ruby CLI has been frozen in favor of the new go CLI. If this issue crops up when using the new CLI, feel free to open an issue in that repo.. @mmb Ruby CLI is frozen and won't have new features added - if you'd like to have this feature in the v2 Go CLI, please feel free to open an issue there. . There is a PR for 16.04 stemcells: https://github.com/cloudfoundry/bosh-linux-stemcell-builder/pull/20, we aim to get in the next supported stemcell line.\nTracker story: https://www.pivotaltracker.com/story/show/151744820. Closing, the behavior of -d and deployment has changed in the new cli. Closing - Ruby CLI is frozen we don't intend to publish a new version of the gem. The v2 CLI shouldn't have this issue (of course if it does, definitely open an issue). @MatthiasWinzeler In the cpi_config you used for manually testing the feature, were your properties for each CPI constructed like this?\n- name: blah\n  type: aws\n  properties:\n    openstack:\n      some-prop: foo\n      some-other-prop: bar\n...\nIf you could provide an example cpi_config based on the one you used, that would be great too. \n. @MatthiasWinzeler @voelzmo Thanks for this PR! We've been looking over the code and doing local testing and it's looking good so far. There are a couple issues that we'd like to bring to your attention before we finalize the review, though.\n1. When using cpi_config, if you run bosh task x --debug then the logs will contain any credentials that were passed in the request['context'] to the CPI; redact creds in the context  before sending them to debug logs. See bosh_cpi/lib/cloud/external_cpi.rb:84.\n2. Always using the newest cloud_config when performing CPI calls breaks bosh recreate functionality (may break other things). We would like to preserve the versioning functionality for the CloudConfig, but from your initial PR comments it seemed like always using the latest CloudConfig was necessary. Can you give us some more context on why the newest CloudConfig has to be used so we can work together and figure out a solution that allows both the new PR and desired pre-existing functionality to work? \n. @MatthiasWinzeler Thanks for making those changes. We looked into how the change to when the deployment model gets updated, and we believe we've identified the source of the bug as well as a way to fix it. \nWith that in mind, we intend to merge this PR as it currently stands and make our change to how deployment.cloud_config works on top of that. \n. Closing: As part of https://www.pivotaltracker.com/story/show/137738153, errand instances will never be deleted (VMs will, however, unless keep-alive is specified).. Closing based on @ljfranklin's comment.. @liuweichu Out of curiosity, what commands is the CLI crashing on? For commands whose primary purpose is to store information about the director/deployment (eg. target or alias-env), at least, failing seems like the correct behavior to me. If commands that only need read access are also failing that'd be concerning.. This was investigated in https://www.pivotaltracker.com/story/show/140452723 and concluded to not be caused by 3363 stemcell (the issue was reproduced in earlier lines). . Closing - above story has been accepted and is available in 262.0 and up. https://github.com/cloudfoundry/bosh/issues/1637 remains open for tracking bad error handling (unhelpful 500) when uploading malformed cloud configs.. Closing - Config diffing is available with CLI v2, starting with v.2.0.30. Closing. The postgres version has been bumped to 9.4.12 for major versions 260 and up, as well as master.. @domdom82 Thanks for the PR! \"with cloud config enabled on your director\" still feels a bit misleading because the issue isn't one of enablement (there's no property for cloud configs) but rather that no cloud config has been uploaded. What would you think of tweaking it to \"with a cloud config uploaded to your director\" or something similar? . Closing, bosh.io now shows the correct command. For reference, https://github.com/bosh-io/ org is the new place to look for bosh.io related code.. Thanks for the contribution! \nWe have merged this locally (https://github.com/cloudfoundry/bosh/commit/3e5daa7ef15569ecaef76bd8f6148c12aec74fb5), and added some additional changes in a followup commit - we added very simple unit tests and removed the schema namespace. We also added another rake task which outputs the table descriptions for a human-readable way to see the current tables/columns.. Seems like more evidence to switch the stemcells over to parted en masse. Would be curious if making that switch will cause problems with existing disks, though.\n@cppforlife wdyt?. So, the problem is that the mysql-server assumes that Upstart is accessible so that it can enable the mysql daemon. However, since bosh-lite is using containers, it won't know about Upstart because that's not the init process. \nYou'd need to find a way to install the server that doesn't use apt, or see if there's a workaround.. @ramonskie Is there a reason why you want Concourse and Director's postgres versions to agree? It's fine to have Concourse running against 9.6 and Director running against 9.4, just need to make sure that the manifest is configured correctly and that the databases run on different ports.. For AWS, you can set the root_disk cloud property in your resource_pools or vm_types section. See docs. . @cppforlife If this is the now-canonical implementation of this behavior, would you mind closing the previous PR?. If context is always going to be initialized in this way, then the CPIs (Openstack CPI currently does this) should not test if context && context[<CPI TYPE>] before merging in the context CPI properties because context will always contain a value (holds true for 3146.x, 256.x, 257.x - backwards compatibility)\n. Would be nice to have a more detailed error/specific error type here.\n. Since CloudFactoryHelper is the only place that this method is called and it doesn't override the default_cloud,  remove the default_cloud method parameter. (See comment in CloudFactoryHelper as well)\n. Already the default for this parameter. Remove, also see comment on cloud_factory.rb:15\n. Tighten access to this endpoint; cpi_configs can have sensitive credentials so we don't want to be as permissive as we are with CloudConfig.\n. This can be moved above the loop and use instance.deployment and instance.availability_zone; shouldn't differ between an instance's PDs\n. Remove method this until it's needed.\n. When CPIs aren't configured this log message should say Checking if this stemcell already exists\n. This can no longer be hard-coded because it's dependent on number of configured CLIs. Currently update_stemcell output with multiple CPIs looks like\nStarted update stemcell\n  Started update stemcell > Extracting stemcell archive. Done (00:00:00)\n  Started update stemcell > Verifying stemcell manifest. Done (00:00:00)\n  Started update stemcell > Checking if this stemcell already exists on cloud foo. Done (00:00:00)\n  Started update stemcell > Uploading some-name/some-version to the cloud foo. Done (00:00:12)\n  Started update stemcell > Save some-name/some-version (some-ami) for cloud foo. Done (00:00:00)\n     Done update stemcell (00:00:12)\n  Started update stemcell > Checking if this stemcell already exists on cloud bar. Done (00:00:00)\n  Started update stemcell > Uploading stemcell some-name/some-version to the cloud bar. Done (00:00:07)\n  Started update stemcell > Save stemcell some-name/some-version (some-ami) for cloud bar. Done (00:00:00)\nThe Done update message should print after all stemcells have been updated successfully. \n. Since cpi_manifest_parser doesn't allow CPIs with duplicate names, is there value in testing that the ParsedCpiConfig returns the first cpi?\n. This line can be deleted; cloud was only needed for initializing the subject.\n. Seems like cpi could be a separate argument and default to nil. Then params arg could be removed; doesn't seem necessary to have a params hash when only one param is ever used.\n. Why was this test removed? Don't see how it relates to this PR.\n. That's a fair point, this will be fine for now.\n. Ah, didn't catch the cid. I don't have strong feelings about params vs separate args in this case, so this seems fine as is.\n. The --forest isn't needed here, and this command relies on nginx always creating its children in such a way that they have the same session id as the master process. \nSince the stemcells install the psmisc package, we have access to the pstree utility. we could instead use something like pstree -p 679 | grep -E \"\\([0-9]+\\)\" --only-matching | tr --delete \"()\", which would ensure that we cover both the nested children case and the differing sid/gid case.. @cppforlife @dpb587 wdyt. ",
    "shinji62": "@d and @jfoley , Hey httpclient have been bump since a long time on rubygems, can you update the bosh dependencies ?!\nThanks.\n. @gberche-orange Anynews on this ?\n. Hi @gberche-orange I have exactly the same problem with bosh and proxy.\nThe first things I did, was to bump the version of httpClient and I use your branch Orange-OpenSource:downloader_http_proxy\n. Hi I made a PR for this proxy setting https://github.com/cloudfoundry/bosh/pull/628\n. About CLA my company rakuten have already sign an agreement since a long time.\n. @mariash  My colleague adress all issue. So please take a look\n. Modification  done. @adamstegman  @mariash @xingzhou\n. Yes, another tips in the spec file you need to put empty array for package too :+1: \n packages: {} \n. Hi made this PR https://github.com/cloudfoundry/bosh/pull/680\n. ok @cppforlife you can close this PR, if your are not agree with it :)\n. Really want this :) :+1: \n. Yes basicaly we created a pgpool release, to make postgres HA and so on.\n. @frodenas ah ah ah ! Yes we will OSS it, just need some cleanup and more testing ! \n. @cppforlife Any thought on that  ?\n. PCF 1.9 so I guess BOSH Director: 260.7. A ticket is also open in Pivotal support.. @cppforlife @dpb587-pivotal Got the same issue again, when I try to create 100 routers bosh 264.11.0 (OpsManager 2.0.18) \n```\nTask 3069 | 09:58:04 | Preparing deployment: Preparing deployment\nTask 3069 | 09:58:18 | Error: stack level too deep\nTask 3069 Started  Wed Aug  1 09:58:04 UTC 2018\nTask 3069 Finished Wed Aug  1 09:58:18 UTC 2018\nTask 3069 Duration 00:00:14\nTask 3069 error\nUpdating deployment:\n  Expected task '3069' to succeed but state is 'error'\nExit code 1\n===== 2018-08-01 09:58:19 UTC Finished \"/usr/local/bin/bosh --no-color --non-interactive --tty --environment=100.64.58.0 --deployment=cf-bcc597313e7483a1d686 deploy /var/tempest/workspaces/default/deployments/cf-bcc597313e7483a1d686.yml\"; Duration: 17s; Exit Status: 1\nExited with 1.\n```\nSo I am stuck I need to create 100 routers for heavy perf test.. ",
    "atomanyih": "@tsaleh @goehmen Does this need to be addressed?\n-CF Community Pair (@atomanyih & @sclevine)\n. @mrdavidlaing We were able to get apparent success by using bundle update --source aws-sdk, which should update only that gem and none of its dependencies\n. @drnic Thanks for the report. This seems like it could require a large fix. Were you planning on submitting a pull request?\n-CF Community Pair (@atomanyih & @sclevine)\n. ",
    "jhellan": "I was able to do a successful backup which resulted in 2115147250 bytes. \nWith one more release known to bosh, backup failed, and a 2155401732 bytes /var/vcap/store/director/backup.tgz was left behind on the bosh vm. \n2**31-1 is 2147483647.\n. Stemcell version: bosh build 2583 on ubuntu trusty.\nIAAS: Microsoft Virtual Machine Manager Service Provder Framework\nIt's our inhouse stemcell for this IAAS.\nBlock sizes on trusty are reported as 512 bytes logical, 4096 physical, according to /sys/block/sdb/queue/*_block_size. On precise, both are reported as 512. \nApparently, this is a well known problem with 4 k disk blocks. But I have no idea if it makes any practical difference. That's why I asked :-)\n. @mariash Different logical and physical block sizes - I suspect that is just the way the Microsoft Virtualization environment (Hyper-V + VMM + SPF)  behaves, i.e. what kind of physical disk it chooses to emulate. I understand that in the physical machine world, this is how 4k disks are supposed to behave on Linux.\nI can also only speculate about why we see this behavior on trusty, but not on older versions of ubuntu. Apparently, Linux has been getting better about detecting 4k disks.\nI can understand why you choose to close the bug. Particularly as Hyper-V doesn't officially support trusty yet. If it turns out to be a realy world problem, we'll come back with code to make aligned partitions.\n@jhellan\n. On 23. mars 2015 17:25, Dmitriy Kalinin wrote:\n\nIs there a particular area of BOSH you are concerned about that uses \n1.9? The Director runs on Ruby 2. The CLI currently supports 1.9, 2.x.\nstemcell_builder/stages/bosh_ruby/apply.sh installs ruby 1.9.3-p545 in \nthe stemcell. Which you can\nverify by ssh-ing in and typing 'ruby --version'.\nvsphere_cpi_release also refers to ruby 1.9\n\nJon\n\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/cloudfoundry/bosh/issues/780#issuecomment-85075547.\n. \n",
    "cf-blobstore-eng": "updated the PR with the requested change.\n. ",
    "matjohn2": "Ran into this also! Thanks for the headsup drnic!\n. For those less familiar with BOSH/MicroBOSH finding this. Workaround is to install and use a ruby < 2.1\nFor example;\nrvm install ruby-2.0\nrvm use ruby-2.0\ngem install bosh_cli_plugin_micro --pre #Which will install all the other BOSH gem dep's\n. Hi @mmb \nI have added to the director spec and updated the erb template. If i'm missing anything from test coverage, please give me some pointers as this is my first pull-req against bosh.\nThanks\n. Hey, apologies for the delay, very much so! just had other priorities. Making the changes now, watch this space :)\n- PS, @mmb thanks for all the pointers, using a default is much cleaner!\n. @thecadams Done and Travis has passed!\n. Hi @calebamiles, I'm afraid coding isn't my strong point, so i'm a little stuck to see how i'd test the @wait_resource_poll_interval  without using @cloud.instance_variable_get because the variable has no public interface.\nAlso, i'm looking through the test suite and cannot see any similar checks for the other 'with defaults' mandatory openstack params? (otherwise i'd see how i should be testing internal vars such as mine), so is it possible i'm just adding a test for something which doesn't need testing? \nA little confused!\nThanks,\nMatt \n. +1. However, BOSH install docs do say to use 'a non-system ruby'.\nSo as a workaround:\nbrew install ruby\nbash\ngem install bosh_cli --no-ri --no-rdoc\nbosh target x.y.z.a\nWorked for me after facing the same issue w/system ruby.\n/M\n. ",
    "reneedv": "These are the tests requested in PR #469 \n. These are the tests requested in PR #469 \n. I sent the CLA on February 5th and received a note back saying it was on\nfile.\nThanks!\n**Ren\u00e9e\nPresident, NIRD LLC\nhttp://nird.us\n@NIRDLLC\n707.266.NIRD\nOn Wed, Feb 5, 2014 at 4:56 PM, cfdreddbot notifications@github.com wrote:\n\nHey reneedv!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA)\non-file with us. Please sign the appropriate CLA (individualhttp://www.cloudfoundry.org/assets/individualcontribution.pdfor\ncorporate http://www.cloudfoundry.org/assets/corpcontribution.pdf).\nWhen sending signed CLA please provide your github username in case of\nindividual CLA or the list of github usernames that can make pull requests\non behalf of your organization.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/516#issuecomment-34281218\n.\n. I sent the CLA on February 5th and received a note back saying it was on\nfile.\n\nThanks!\n**Ren\u00e9e\nPresident, NIRD LLC\nhttp://nird.us\n@NIRDLLC\n707.266.NIRD\nOn Wed, Feb 5, 2014 at 4:56 PM, cfdreddbot notifications@github.com wrote:\n\nHey reneedv!\nThanks for submitting this pull request!\nAll pull request authors must have a Contributor License Agreement (CLA)\non-file with us. Please sign the appropriate CLA (individualhttp://www.cloudfoundry.org/assets/individualcontribution.pdfor\ncorporate http://www.cloudfoundry.org/assets/corpcontribution.pdf).\nWhen sending signed CLA please provide your github username in case of\nindividual CLA or the list of github usernames that can make pull requests\non behalf of your organization.\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/516#issuecomment-34281218\n.\n. Any word on this?\n. Any word on this?\n. I've incorporated that change, and re-factored / added some tests. Would this addition be acceptable? So you could either specify a single endpoint or the host, ssl, and port.\n. I've incorporated that change, and re-factored / added some tests. Would this addition be acceptable? So you could either specify a single endpoint or the host, ssl, and port.\n. Hi David,\n  My code is giving people the option of specifying just the endpoint instead of all 3 (the host, port, and ssl). Also, I updated the Readme and re-factored the tests to cover the different scenarios. \n. Hi David,\n  My code is giving people the option of specifying just the endpoint instead of all 3 (the host, port, and ssl). Also, I updated the Readme and re-factored the tests to cover the different scenarios. \n. The tests are passing on this as can be seen here: https://travis-ci.org/reneedv/bosh/builds/18788938 Does this need to be rebased instead of merged?\n. The tests are passing on this as can be seen here: https://travis-ci.org/reneedv/bosh/builds/18788938 Does this need to be rebased instead of merged?\n. @ematpl & @mmb I'll reply to your comment on PR #521 here since it has to do this with request. The original s3 client code supported the endpoint option, and the simple blobstore client supports the endpoint option. The comment in the code says the s3 blobstore becomes a simple blobstore client when there is no secret and access keys, so having the endpoint option possible would make the options consistent with that. Also, users could have existing configurations specifying the endpoint, and this code makes the change backwards compatible. \n. @ematpl & @mmb I'll reply to your comment on PR #521 here since it has to do this with request. The original s3 client code supported the endpoint option, and the simple blobstore client supports the endpoint option. The comment in the code says the s3 blobstore becomes a simple blobstore client when there is no secret and access keys, so having the endpoint option possible would make the options consistent with that. Also, users could have existing configurations specifying the endpoint, and this code makes the change backwards compatible. \n. Sent to the wrong repo. Plz ignore this.\n. Sent to the wrong repo. Plz ignore this.\n. #496 Doesn't have an example file or tests. The \"additions\" this PR has are a refactor to the director tests to reduce duplication (which I can take out) and a change to the default security group name that Dr. Nic made that could probably also be removed and put in a separate PR. Is there something else that's \"extra\" that you are seeing? Should I just rebase against #496 and add the tests there? \n. #496 Doesn't have an example file or tests. The \"additions\" this PR has are a refactor to the director tests to reduce duplication (which I can take out) and a change to the default security group name that Dr. Nic made that could probably also be removed and put in a separate PR. Is there something else that's \"extra\" that you are seeing? Should I just rebase against #496 and add the tests there? \n. Also, per the comments on #496 it's indicated that this is a better way to talk to swift.\n. as per @d I've made a new PR #521 with the changes squashed.\n. As per the conversation on #508 the additional configuration options that are supported by S3 and compatible blobstores are not being passed through (only host, port, ssl, or endpoint if you look at PR #516 ) so the additional options for swift or hp that are in this PR are not currently supported. If there are plans in the works to refactor so generic blobstore options could be passed in and properly added to the director config I would be happy to help - would you be open to such a pull request is there a PT ticket or issue for that?\n. \n",
    "byllc": "@d At the time what had happened was that all three vms had spun up from a previous deployment. But none of the jobs had been started. The subsequent deployment was failing during the compile phase and none of the vm's were able to start the job yet  but it was only flagging the first VM as failing. I think the process to get in to the state was basically \n- Create a new release\n- Create a package that passes compilation\n- Deploy to test package\n- Now adjust the package so that it fails. \n- Only the first vm will show as failing when you run bosh vms\nI believe this is what caused the state we reported.  @drnic  was pairing with me at the time and the question he asked above is related to this problem, it should be the same thing. I'm sorry but I had been fighting with an issue for several hours at the time and it could have been something else in the mix that got us in to this state. I have not had a chance to try to reproduce it because the release is no longer broken.\n. @cppforlife the idea is that they can be used to help us decide how to act and if a response is needed when something is not in the right state, in general with bosh resurrection on we shouldn't need to intervene  but  the heartbeat events especially have a great deal of information about job nodes (mem,cpu, disk) which is useful upstream of the bosh because it provides a lot of color to our sense and response framework. \n. Should I have synched with the develop branch and PR'd this there? \n. @cppforlife I'm finding that when I move those defaults out,  my tests now fail because the spec file isn't really in play when I run tests but I  want an assurance that defaults are working. Do you have any suggestions for this situation?  The other plugins don't have very many non trivial defaults so I don't see a good example of how to ensure my defaults from spec.yml are present when I run my tests. \n. +1. ",
    "heei3k": "Thanks for your quickly response. I will try it in ruby 2.0.0. \n. I have tried 2.0.0, it gave the same error:\nbosh micro deployment micro01\nFailed to load plugin /root/.rbenv/versions/2.0.0-p0/lib/ruby/gems/2.0.0/gems/bosh_deployer-1.4.1/lib/bosh/cli/commands/micro.rb: Unable to activate bosh_aws_registry-0.2.3, because aws-sdk-1.8.5 conflicts with aws-sdk (~> 1.6.9)\n. When I uninstall bosh_deployer and install  bosh_cli_plugin_micro. It's OK. Thanks for your help!\n. Sorry ,I'm a novice at vsphere as well as bosh.  I'm not clear about the procedures of micro bosh deployments. For example, for the IP to install micro bosh-172.16.71.172, should it be able to connect or not?Now i can't connect this ip ,because I think  the bosh_cli will do this thing , including create a VM and some of middleware ,at last it will install micro bosh. Is it correct?\n. the whole error printed in bosh_cli \n$ bosh micro deploy ~/stemcells/latest-micro-bosh-stemcell-vsphere.tgz\nDeploying new micro BOSH instance micro01/micro_bosh.yml' tohttps://172.16.71.133:25555' (type 'yes' to continue): yes\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    micro-bosh-stemcell\nVersion: 912\nDeploy Micro BOSH\nUnpacking stemcell                  |                        | 0/11 00:00:05  ETA: --:--:--at depth 0 - 18: self signed certificate\n  unpacking stemcell (00:00:05)\nUploading stemcell                  |oo                      | 1/11 00:00:12  ETA: 00:00:52create stemcell failed: Field entity is not optional:\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/serializer.rb:39:in serialize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:116:inserialize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:92:in block in serialize_request'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:91:ineach'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:91:in serialize_request'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/soap/stub_adapter.rb:28:ininvoke_method'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:13:in invoke_method'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/ruby_vim_sdk/vmodl/managed_object.rb:45:inblock (3 levels) in finalize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/client.rb:418:in fetch_perf_metric_names'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/client.rb:405:inblock in find_perf_metric_names'\ninternal:prelude:10:in synchronize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/client.rb:403:infind_perf_metric_names'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/client.rb:370:in get_perf_counters'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/cluster.rb:216:infetch_cluster_utilization'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/cluster.rb:70:in initialize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/datacenter.rb:69:innew'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/datacenter.rb:69:in block in clusters'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/datacenter.rb:62:ineach'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources/datacenter.rb:62:in clusters'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources.rb:118:inblock in place'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/monitor.rb:211:in mon_synchronize'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/resources.rb:96:inplace'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/cloud.rb:86:in block (2 levels) in create_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/tmpdir.rb:83:inmktmpdir'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/cloud.rb:73:in block in create_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_common-1.1975.0/lib/common/thread_formatter.rb:46:inwith_thread_name'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.1975.0/lib/cloud/vsphere/cloud.rb:71:in create_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:233:inblock (2 levels) in create_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:89:in step'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:232:inblock in create_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/1.9.1/tmpdir.rb:83:in mktmpdir'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:218:increate_stemcell'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:122:in create'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:102:inblock in create_deployment'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:96:in with_lifecycle'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/deployer/instance_manager.rb:102:increate_deployment'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.1975.0/lib/bosh/cli/commands/micro.rb:180:in perform'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.1975.0/lib/cli/command_handler.rb:57:inrun'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.1975.0/lib/cli/runner.rb:56:in run'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.1975.0/lib/cli/runner.rb:16:inrun'\n/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.1975.0/bin/bosh:7:in <top (required)>'\n/root/.rbenv/versions/1.9.3-p448/bin/bosh:23:inload'\nUploading stemcell                  |oo                      | 1/11 00:00:16  ETA: 00:00:47\n. Hi, @d  , could you help me? I'm almost crazy about this problem for there is nothing call help me by google.\nFrom the error message , it failed when the stemcell is uploading, is it? Where does it upload to?If it's upload to where the micro bosh's ip which is configured as 172.16.71.133 , and it's not available, Would not bosh_cli invoke vm to create a VM first and then uploading micro bosh package. If the VM which ip is 172.16.71.133 should be available at first, why there is some configuration such as resources in micro_bosh.yml?\n. Hi,@cppforlife, I check the disk path is present in the datastore1 and datastore2 which are both local storages in two physical machines.\nI changed it to /bosh_deployer as that you said the disk_path specified should be a full path.But it still failed.  Below is the log in  bosh_micro_deploy.log\nLogfile created on 2014-02-18 09:44:31 +0800 by logger.rb/31641\nI, [2014-02-18T09:44:31.872944 #17108] [0xf4f2004]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-18T09:45:15.183860 #17457] [0x77b7814]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-18T09:45:51.430122 #17457] [0x77b7814]  INFO -- : Loading yaml from /tmp/d20140218-17457-jh5v9m/sc-20140218-17457-lb80j8/stemcell.MF\nI, [2014-02-18T09:45:51.602575 #17457] [create_stemcell(/tmp/d20140218-17457-jh5v9m/sc-20140218-17457-lb80j8/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140218-17457-jh5v9m/d20140218-17457-19jwaq\nI, [2014-02-18T09:46:02.779806 #17457] [create_stemcell(/tmp/d20140218-17457-jh5v9m/sc-20140218-17457-lb80j8/image, )]  INFO -- : Generated name: sc-c1f3d7bb-0335-42c9-ab3b-7f586c0bb195\nI, [2014-02-18T09:50:27.609892 #17864] [0x5298010]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-18T09:50:47.323596 #18213] [0xad6b80c]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-18T09:51:22.801685 #18213] [0xad6b80c]  INFO -- : Loading yaml from /tmp/d20140218-18213-9djdzq/sc-20140218-18213-6sanu2/stemcell.MF\nI, [2014-02-18T09:51:22.966420 #18213] [create_stemcell(/tmp/d20140218-18213-9djdzq/sc-20140218-18213-6sanu2/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140218-18213-9djdzq/d20140218-18213-2n1a4s\nI, [2014-02-18T09:51:34.235169 #18213] [create_stemcell(/tmp/d20140218-18213-9djdzq/sc-20140218-18213-6sanu2/image, )]  INFO -- : Generated name: sc-89d1d980-7f6b-49c7-a3b0-79997c47c186\nI, [2014-02-18T09:53:35.524141 #18609] [0xe8ae004]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-18T09:54:09.826630 #18609] [0xe8ae004]  INFO -- : Loading yaml from /tmp/d20140218-18609-w99o4c/sc-20140218-18609-xx9rv/stemcell.MF\nI, [2014-02-18T09:54:09.984216 #18609] [create_stemcell(/tmp/d20140218-18609-w99o4c/sc-20140218-18609-xx9rv/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140218-18609-w99o4c/d20140218-18609-txd0b0\nI, [2014-02-18T09:54:23.473040 #18609] [create_stemcell(/tmp/d20140218-18609-w99o4c/sc-20140218-18609-xx9rv/image, )]  INFO -- : Generated name: sc-01fff395-ce39-4ebe-aee7-d493ecd1ffdc\n. Hi @cppforlife Only bosh_micro_deploy.log can be found.\nI added some lines in micro_bosh.yml : \n. Hi @cppforlife ,thanks for working on this with me, i really appreciate this help.\nI submited previous comment by mistake. This  continue  with previous comment: \nI added some lines in micro_bosh.yml :\nlogging:\nfile: /root/deployments/micro_bosh.log\nlevel: DEBUG\nBut when i rum deployment , there is not any log file in the directory.\nAnd I get some info by bosh commands that may be useful.\nbosh micro status\nStemcell CID   n/a\nStemcell name  n/a\nVM CID         n/a\nDisk CID       n/a\nMicro BOSH CID bm-a4131d62-8b9c-40d9-ba43-7aebf8b6ddb5\nDeployment     /root/deployments/micro01/micro_bosh.yml\nTarget         https://172.16.71.143:25555\nbosh micro deployments\nNo deployments\nroot@icity-centos55-147 deployments]# bosh status\nConfig\n             /root/.bosh_config\nDirector\n  not set\nDeployment\n  not set\nmore /root/.bosh_config\n--- {}\n. Any help appreciated on solving this.These are screenshots on vsphere:\n\n. Hi,@mmb, I'm sure cluster named \"test\" and ctemplate_folder\nare there  . this is the screenshots of vsphere configuration. \n. Sorry , the web page is changed , I can't upload any images.\n. Hi,@mmb, Thank you very much for your response.\nI can't upload the image in the website. So  I send it by email. Could you check whether something wrong in yaml?\nAny help  appreciated on this issue.\nDate: Tue, 18 Feb 2014 15:22:21 -0800\nFrom: notifications@github.com\nTo: bosh@noreply.github.com\nCC: heei3k@hotmail.com\nSubject: Re: [bosh] bosh micro deploy failed :create stemcell failed: Field entity is not optional: (#523)\nHi @heei3k,\nCan you confirm that you have a cluster named \"test\" in the vCenter Hosts and Clusters view?\nCan you also confirm that there a folder called \"template_folder\" in the datacenter in the vCenter Inventory view?\nCF Community Pair (@dsabeti & @mmb)\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.                  \n. Below is the cluster and host configuration in vsphere:\n\nBelow is the vm and  template configuration in vsphere:\n\nI have tried it in both centos and ubuntu using different stemcells. The messages are same.\nI wonder the principle of uploading stemcells from bosh_cli vm to local storage. May I upload the stemcells  by manual to local storage directory which named 'bosh_deployer'?  I tried it ,but still failed.\n. My target is to deploy cloud foundry, which is recommanded with BOSH. Bosh is deployed by micro BOSH.  Any help appreciated to take the first step for me. \n. Hi, @mmb, thanks for your help. \nThis attachment is the log \nDate: Wed, 19 Feb 2014 13:51:15 -0800\nFrom: notifications@github.com\nTo: bosh@noreply.github.com\nCC: heei3k@hotmail.com\nSubject: Re: [bosh] bosh micro deploy failed :create stemcell failed: Field entity is not optional: (#523)\nIt looks like you may have found a bug (we still need to look into it more). But we do have a work-around that you can use in the meantime. In your micro-bosh configuration file, replace this section:\nclusters:\n- test\nand use this instead:\nclusters:\n- test:\n    resource_pool: test_pool\nBefore you try to deploy this, you also need to add a resource pool to that cluster.\nCF Community Pair (@dsabeti & @mmb)\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.                    \nLogfile created on 2014-02-19 19:24:08 +0800 by logger.rb/31641\nI, [2014-02-19T19:24:08.329001 #22620] [0xf9e7f0]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-19T19:24:56.276759 #22964] [0x11d1ff4]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-19T19:25:34.890148 #22964] [0x11d1ff4]  INFO -- : Loading yaml from /tmp/d20140219-22964-1reff3m/sc-20140219-22964-yflphj/stemcell.MF\nI, [2014-02-19T19:25:40.940456 #22964] [create_stemcell(/tmp/d20140219-22964-1reff3m/sc-20140219-22964-yflphj/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140219-22964-1reff3m/d20140219-22964-6vjqd\nI, [2014-02-19T19:26:04.870205 #22964] [create_stemcell(/tmp/d20140219-22964-1reff3m/sc-20140219-22964-yflphj/image, )]  INFO -- : Generated name: sc-dadd0bf7-a2c9-4512-a5c0-c04a078d0951\nD, [2014-02-19T19:26:05.074091 #22964] [create_stemcell(/tmp/d20140219-22964-1reff3m/sc-20140219-22964-yflphj/image, )] DEBUG -- : Datastores - ephemeral: [], persistent: [], shared: [\"datastore1\", \"datastore2\"].\nI, [2014-02-20T09:03:23.788537 #23815] [0xc14800]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-20T09:03:35.759049 #24158] [0x1408ffc]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-20T09:04:11.109168 #24158] [0x1408ffc]  INFO -- : Loading yaml from /tmp/d20140220-24158-1rb9j1m/sc-20140220-24158-yty2ht/stemcell.MF\nI, [2014-02-20T09:10:26.355419 #24510] [0xc41800]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-20T09:11:02.106078 #24510] [0xc41800]  INFO -- : Loading yaml from /tmp/d20140220-24510-ji9j5/sc-20140220-24510-1u6nas8/stemcell.MF\nI, [2014-02-20T09:43:09.212610 #25549] [0xfc27f4]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-20T09:43:39.874749 #25892] [0xd247f4]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-02-20T09:44:14.865350 #25892] [0xd247f4]  INFO -- : Loading yaml from /tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/stemcell.MF\nI, [2014-02-20T09:44:15.129652 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140220-25892-n7ixaz/d20140220-25892-1pxwzek\nI, [2014-02-20T09:44:42.428803 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Generated name: sc-bb388f00-1637-415f-9921-72a61f478e9b\nD, [2014-02-20T09:44:42.778461 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-52>\nD, [2014-02-20T09:44:42.784894 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Datastores - ephemeral: [], persistent: [], shared: [\"datastore1\", \"datastore2\"].\nD, [2014-02-20T09:44:42.796732 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : test ephemeral disk bound\nD, [2014-02-20T09:44:42.796864 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Score: test: 1962\nI, [2014-02-20T09:44:42.796982 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Deploying to:  / <[Vim.Datastore] datastore-10>\nD, [2014-02-20T09:44:42.978350 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Search for folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:44:42.981878 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Creating folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:44:43.024705 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )] DEBUG -- : Found folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v53>\nI, [2014-02-20T09:44:43.024863 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Importing VApp\nI, [2014-02-20T09:44:43.071724 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Waiting for NFC lease to become ready\nI, [2014-02-20T09:44:46.094793 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Uploading\nI, [2014-02-20T09:44:46.102155 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Uploading disk to: https://172.16.71.132/nfc/52a0e83e-f56f-8fe9-abe6-da229c02642a/disk-0.vmdk\nI, [2014-02-20T09:45:17.095714 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, )]  INFO -- : Removing NICs\nI, [2014-02-20T09:45:18.180583 #25892] [create_stemcell(/tmp/d20140220-25892-n7ixaz/sc-20140220-25892-58e9ja/image, _)]  INFO -- : Taking initial snapshot\nD, [2014-02-20T09:45:21.335340 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : VM creator initialized with memory: 2048, disk: 16384, cpu: 1, placer: #VSphereCloud::Resources:0x00000003c25490\nD, [2014-02-20T09:45:21.340851 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Search for folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:45:21.347001 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v53>\nD, [2014-02-20T09:45:21.385343 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-52>\nD, [2014-02-20T09:45:21.390930 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Datastores - ephemeral: [], persistent: [], shared: [\"datastore1\", \"datastore2\"].\nD, [2014-02-20T09:45:21.398376 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : test memory bound\nD, [2014-02-20T09:45:21.398465 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Score: test: 9\nI, [2014-02-20T09:45:21.398594 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Creating vm: vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70 on  stored in <[Vim.Datastore] datastore-16>\nD, [2014-02-20T09:45:21.401665 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Search for folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:45:21.404482 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v53>\nI, [2014-02-20T09:45:21.410944 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-bb388f00-1637-415f-9921-72a61f478e9b.\nD, [2014-02-20T09:45:21.413651 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Search for folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:45:21.416306 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v53>\nI, [2014-02-20T09:45:21.418834 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Cluster doesn't have stemcell sc-bb388f00-1637-415f-9921-72a61f478e9b, replicating\nI, [2014-02-20T09:45:21.421447 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Replicating sc-bb388f00-1637-415f-9921-72a61f478e9b (<[Vim.VirtualMachine] vm-54>) to sc-bb388f00-1637-415f-9921-72a61f478e9b %2f datastore-16\nD, [2014-02-20T09:45:21.424717 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Search for folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:45:21.427634 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found folder template_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v53>\nI, [2014-02-20T09:46:34.639178 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Replicated sc-bb388f00-1637-415f-9921-72a61f478e9b (<[Vim.VirtualMachine] vm-54>) to sc-bb388f00-1637-415f-9921-72a61f478e9b %2f datastore-16 (<[Vim.VirtualMachine] vm-56>)\nI, [2014-02-20T09:46:34.639346 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Creating initial snapshot for linked clones on <[Vim.VirtualMachine] vm-56>\nI, [2014-02-20T09:46:36.671368 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Created initial snapshot for linked clones on <[Vim.VirtualMachine] vm-56>\nI, [2014-02-20T09:46:36.671504 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-56>\nI, [2014-02-20T09:46:36.700995 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Cloning vm: <[Vim.VirtualMachine] vm-56> to vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70\nD, [2014-02-20T09:46:36.704049 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Search for folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:46:36.707031 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Creating folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:46:36.733301 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)] DEBUG -- : Found folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v58>\nI, [2014-02-20T09:46:43.252736 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70\", \"id\"=>\"vm-59\"},\n \"agent_id\"=>\"bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\",\n \"networks\"=>\n  {\"bosh\"=>\n    {\"cloud_properties\"=>{\"name\"=>\"VM Network\"},\n     \"netmask\"=>\"255.255.255.128\",\n     \"gateway\"=>\"172.16.71.254\",\n     \"ip\"=>\"172.16.71.143\",\n     \"dns\"=>[\"172.16.1.90\", \"10.135.12.101\"],\n     \"type\"=>nil,\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"mac\"=>\"00:50:56:a5:32:5f\"}},\n \"disks\"=>{\"system\"=>\"0\", \"ephemeral\"=>\"1\", \"persistent\"=>{}},\n \"ntp\"=>[\"time.windows.com\", \"ntp02.las01.emcatmos.com\"],\n \"blobstore\"=>\n  {\"provider\"=>\"local\",\n   \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}},\n \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\",\n \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}\nI, [2014-02-20T09:46:45.239311 #25892] [create_vm(bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496, ...)]  INFO -- : Powering on VM: <[Vim.VirtualMachine] vm-59> (vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70)\nD, [2014-02-20T09:46:47.396864 #25892] [set_vm_metadata(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, ...)] DEBUG -- : Search for folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:46:47.401325 #25892] [set_vm_metadata(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, ...)] DEBUG -- : Found folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v58>\nD, [2014-02-20T09:46:47.423262 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: nil\nD, [2014-02-20T09:46:51.445073 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:46:54.445115 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:46:57.445013 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:00.445153 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:03.445068 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:05.892848 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:06.894687 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:07.896571 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:08.898467 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:09.900384 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:10.920353 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:11.922343 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:12.924297 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:13.926312 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:14.996143 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:15.998177 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:17.000278 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:18.002278 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:19.004283 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:20.042045 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:21.044112 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nD, [2014-02-20T09:47:22.045946 #25892] [0xd247f4] DEBUG -- : Waiting for agent to be ready: #\nI, [2014-02-20T09:47:22.103441 #25892] [create_disk(16384, )]  INFO -- : Creating disk with size: 16384\nI, [2014-02-20T09:47:22.105407 #25892] [create_disk(16384, )]  INFO -- : Created disk: #1, :path=>nil, :datacenter=>nil, :datastore=>nil, :size=>16384, :uuid=>\"disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164\"}>\nI, [2014-02-20T09:47:22.108549 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)]  INFO -- : Attaching disk: disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164 on vm: vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70\nD, [2014-02-20T09:47:22.114969 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)] DEBUG -- : Search for folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496\nD, [2014-02-20T09:47:22.119800 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)] DEBUG -- : Found folder vm_folder/bm-cd9d23a2-2688-4f1a-9a4e-28f74e77b496: <[Vim.Folder] group-v58>\nI, [2014-02-20T09:47:22.191031 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)]  INFO -- : Need to create disk\nD, [2014-02-20T09:47:22.217813 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-52>\nD, [2014-02-20T09:47:22.223671 #25892] [attach_disk(vm-5c8969f7-57e9-42b0-a86a-a454e8f21e70, disk-70fd40ad-9ccc-4c22-8230-0eb537c8a164)] DEBUG -- : Datastores - ephemeral: [], persistent: [], shared: [\"datastore1\", \"datastore2\"].\n. Hi,@mmb, the yml file is as below:\n\nname: micro01\nlogging:\n  file: /root/deployments/micro_bosh.log\n  level: DEBUG\nnetwork:\n  ip: 172.16.71.143\n  netmask: 255.255.255.128\n  gateway: 172.16.71.254\n  dns:\n- 172.16.1.90\n- 10.135.12.101\n  cloud_properties:\n    name: \"VM Network\"\n  resources:\n  persistent_disk: 16384\n  cloud_properties:\n    ram: 2048\n    disk: 16384\n    cpu: 1\n  cloud:\n  plugin: vsphere\n  properties:\n    agent:\n      ntp:\n       - time.windows.com\n       - ntp02.las01.emcatmos.com\n    vcenters:\n      - host: 172.16.71.197\n        user: root\n        password: 1234qwer\n        datacenters:\n          - name: datacenter\n            vm_folder: vm_folder\n            template_folder: template_folder\n            disk_path: bosh_deployer\n            datastore_pattern: datastore?\n            persistent_datastore_pattern: datastore?\n            allow_mixed_datastores: true\n            clusters:\n              - test:\n            resource_pool: test_pool\n. Hi,@mmb, I paste the yml in comment but it looks like another format, so I send it by email with attachment. Could you help to check it. Thanks!\nDate: Wed, 19 Feb 2014 17:30:59 -0800\nFrom: notifications@github.com\nTo: bosh@noreply.github.com\nCC: heei3k@hotmail.com\nSubject: Re: [bosh] micro bosh deploy faild:create stemcell failed:extend from issue 523 (#528)\nCan you paste your (sanitized) .yml configuration in a gist? The yaml parser is very picky about whitespace, so if that's wrong validation of the configuration could fail. Make sure the resource_pool property is indented one additional level from the line that reads - test:\nCF Community Pair (@dsabeti & @mmb)\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.                  \n. Hi, @mmb, now it's working for uploading stemcell and creating VM but when it goto waiting for the agent, it failed , the output in bosh cli is:\nDeploy Micro BOSH\n  unpacking stemcell (00:00:19)\n                                    |oo                      | 1/11 00:00:20  ETA: 00:03:18at depth 0 - 18: self signed certificate\nUploading stemcell                  |oo                      | 1/11 00:00:48  ETA: 00:02:50at depth 0 - 18: self signed certificate\nUploading stemcell                  |oo                      | 1/11 00:00:51  ETA: 00:02:47at depth 0 - 20: unable to get local issuer certificate\n  uploading stemcell (00:01:06)\nCreating VM from...                 |oooo                    | 2/11 00:02:46  ETA: 00:05:07at depth 0 - 18: self signed certificate\nCreating VM from...                 |oooo                    | 2/11 00:02:48  ETA: 00:05:05at depth 0 - 18: self signed certificate\nCreating VM from...                 |oooo                    | 2/11 00:02:49  ETA: 00:05:04at depth 0 - 18: self signed certificate\n  creating VM from sc-bb388f00-1637-415f-9921-72a61f478e9b (00:01:26)\nWaiting for the agent               |oooooo                  | 3/11 00:03:20  ETA: 00:07:11at depth 0 - 18: self signed certificate\nWaiting for the agent               |oooooo                  | 3/11 00:03:27  ETA: 00:07:04/root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.2005.0/lib/cloud/vsphere/cloud.rb:403:in find_persistent_datastore': Datastore not accessible to host, datastore1, [\"datastore2\"] (RuntimeError)\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.2005.0/lib/cloud/vsphere/cloud.rb:465:inblock in attach_disk'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_common-1.2005.0/lib/common/thread_formatter.rb:46:in with_thread_name'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_vsphere_cpi-1.2005.0/lib/cloud/vsphere/cloud.rb:409:inattach_disk'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:317:in attach_disk'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:354:inupdate_persistent_disk'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:142:in block in create'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:89:instep'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:141:in create'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:102:inblock in create_deployment'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:96:in with_lifecycle'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/deployer/instance_manager.rb:102:increate_deployment'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli_plugin_micro-1.2005.0/lib/bosh/cli/commands/micro.rb:180:in perform'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2005.0/lib/cli/command_handler.rb:57:inrun'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2005.0/lib/cli/runner.rb:56:in run'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2005.0/lib/cli/runner.rb:16:inrun'\n        from /root/.rbenv/versions/1.9.3-p448/lib/ruby/gems/1.9.1/gems/bosh_cli-1.2005.0/bin/bosh:7:in <top (required)>'\n        from /root/.rbenv/versions/1.9.3-p448/bin/bosh:23:inload'\n        from /root/.rbenv/versions/1.9.3-p448/bin/bosh:23:in `'\nI send the log file of micro_log to you by email as it's too many lines.\n. Hi, @mmb,  I notice that  we have two physical machines datastore, one is datastore1 , the other  is datastore2, and datastore1 have only 1.4G memery remained, datatore2 have enough , would it fail by this cause?\n. Yeah! Now I succeed in deploying micro bosh. This is my first step. it's so difficult for me , I feel very exciting. Thanks very much for your helps @mmb, I will forward to my target of deploying Cloud Foundry..^-^\n. Now it's OK. I just reduce the configuration for compilation :\ncompilation:\n  workers: 2      # before is 4\n  network: default\n  cloud_properties:\n    ram: 1024     # before is 2048\n    disk: 4096\n    cpu: 2            # before is 4\nI have 2 questions: \n1. The memory in each physical machine is only 12G .One is 1G left(datastore1), the other is 8G left(datastore2).    micro bosh is in physical machine of datastore1.  I configure datastore_pattern as datastore2. I want all the new VMs are created in physical machine of datastore2, but actually , all the 6 VMs are still in datastore1. How to configure yaml file to let all the new VMs are in the machine which resources are enough?  I have tried when I configure it in micro_bosh.yml, the micro bosh VM will be created in the defined datastore, why doesn't it  take affect in bosh.yml?\n1.  All of the 6 VMs are in test_pool but I configure in yaml two new resource pools , one is small, the other is director, why are they not used?\nIs there any advice on it ?Thanks in advance!\n. Hi ,@d,  I have tried it again. It's OK for download dea_jvm7, but  still not finish.  thanks very much!\n. Hi, thanks for your response! I already finish the downloading and cf deployment.\n@d, in the instruction, there is no creating release process. The uploading already include creating release process I think.\n@thecadams. I think only this function should be added for retry feature. Because for other downloading such as downloading stemcell, downloading cf-release.tgz,we can download them by some professional tools. Only this function we can't. \n. ",
    "gberche-orange": "@mbb I'm just beginning ruby development so I'm doing my best to extend existing tests to cover code I'm contributing :-)\nI've therefore added tests covering the http_proxy similar to your suggestion in the existing https://github.com/cloudfoundry/bosh/blob/master/bosh-director/spec/unit/download_helper_spec.rb\nRegarding the http retries I did not find existing tests on the current retries in https://github.com/cloudfoundry/bosh/blob/master/bosh_cli_plugin_micro/spec/unit/bosh/deployer/instance_manager_spec.rb, so I only added tests that merely verify the presence of newly added retries\nHope this can be sufficient for merging this PR, and that as I improve my ruby skills I'll be able to further enhance existing tests.\n. Hi @mmb ,\nThe rubocop failures should now be fixed. \nI added more comments regarding the behavior of the code when retrying through http-proxy triggered error. I understand the bosh micro cli is waiting for bosh agent to come up usually by trying to open a socket which either results into a cnx refused or a timeout. \nWhen accessed through a proxy, the observed failure is slightly different and the retries should be the same. Currently the lack of retries fails the bosh micro deploy command as reported above.\nThe retry mecanism was initially introduced in inital file commit https://github.com/cloudfoundry/bosh/commit/9266a28eaa116f3e8fa06c146f336cd4477f8cb3#diff-787ad9ef43f668585d427b5116035f62R316 as I understand without explicit coverage of the retried exception. I'm not adding any logic related to the retry of the connection errors, just fixing the fact that they're not detected as such when invoked behind a proxy.\nNote that the exception retries have since been augmented into https://github.com/cloudfoundry/bosh/commit/1edbb2a179bc790de5eb374d6661e974e9d22ca7 without any further tests nor specs.\nI believe the spec currently contributed clearly state the intent of the retried exception, and there will be more added value to have BAT tests use an HTTP proxy than with either\n- unit tests mocking HTTPClient to throw this exception, \n- or integration tests that would testing http client behavior in face of a not yet started agent behind an http proxy and check it indeed returns a 503 error.\nI propose we continue the discussion on test strategy for bosh when used behing an HTTP proxy in this bosh-dev@ thread https://groups.google.com/a/cloudfoundry.org/d/msg/bosh-dev/B_hGiw2fKgQ/ONQLqp_gB2sJ where I was asking for guidance prior to submitting this PR. There are other pending PRs to come to support HTTP proxies in both bosh and CF and I'd like to have a rational testing approach to them.\n. Thanks @dsabeti and @thecadams . Timezones are against us so I tried to progress with your suggestion without being able to get some additional help/guidance in between issues I was hitting.\nIn short, I understand that the InstanceManager is basically too large to effectively be unit tested and would need to be split into multiple smaller parts. A symptom of this is that its 552 lines long, and many deep private and relatively complex methods.\nThis refactoring is outside my reach and help would be required if this is made a prereq to merge this contrib. The best I could do is workaround this, and I realize this is not an ideal result. I however hope that this slightly improves the coverage for the http_proxy support and enables it to be merged.\nThis was hard to me as a ruby beginner to try to get around this, see my trial/failures attempt below for the records.\n\nI'm struggling to instanciate the instance_manager which tries to load the plugin class into the initialize code.\nErrno::ENOENT: No such file or directory - /home/gberche/code/bosh/bosh_cli_plugin_micro/config/fake-plugin_defaults.yml\n./lib/bosh/deployer/configuration.rb:147:in `load_defaults'\n./lib/bosh/deployer/configuration.rb:10:in `configure'\n./lib/bosh/deployer/instance_manager.rb:48:in `initialize'\n./lib/bosh/deployer/instance_manager.rb:44:in `new'\n./lib/bosh/deployer/instance_manager.rb:44:in `create'\n./spec/unit/bosh/deployer/instance_manager_spec.rb:54:in `block (3 levels) in <top (required)>'\nI tried stubbing some of methods called by initialize but it seems overkill given the number of calls made:\n```\n def initialize(config, config_sha1, ui_messager, plugin_name)\n      Config.configure(config)\n      @config = Config\n  plugin_class = InstanceManager.const_get(plugin_name.capitalize)\n  @infrastructure = plugin_class.new(self, logger)\n\n  @deployments_state = DeploymentsState.load_from_dir(config['dir'], logger)\n  load_state(config['name'])\n\n  Config.uuid = state.uuid\n\n  @config_sha1 = config_sha1\n  @ui_messager = ui_messager\n  @renderer = LoggerRenderer.new\nend\n\n```\nWithout stubbing collaborators would require to have a a test plugin class, a config file to laod in resources, a log renderer ...\nI considered modifying the wait_until_ready method to a class method but it uses the logger field.\nI tried stubbing the initialize method but this had no effect:\n```\nit 'retries on BadResponseError (503 bad proxy) when waiting for agent and director through an http proxy' do\n      described_class.stub(:require)\n      described_class.stub(:initialize) {}\n      allow(Bosh::Common).to receive(:retryable)\n      instance_manager = Bosh::Deployer::InstanceManager.create(config)\n      instance_manager.wait_until_ready('director') do\n  end\n  expect(Bosh::Common).to have_received(:retryable) do |args|\n    expect(args[:on]).to include HTTPClient::BadResponseError\n  end\nend\n\n```\n``\nTesting started at 11:10 AM ...\n/home/gberche/.rvm/gems/ruby-1.9.3-p484/gems/rspec-mocks-3.0.0.beta1/lib/rspec/mocks/method_double.rb:89: warning: removinginitialize' may cause serious problems\nErrno::ENOENT: No such file or directory - /home/gberche/code/bosh/bosh_cli_plugin_micro/config/fake-plugin_defaults.yml\n./lib/bosh/deployer/configuration.rb:147:in load_defaults'\n./lib/bosh/deployer/configuration.rb:10:inconfigure'\n./lib/bosh/deployer/instance_manager.rb:48:in initialize'\n./lib/bosh/deployer/instance_manager.rb:44:innew'\n./lib/bosh/deployer/instance_manager.rb:44:in create'\n./spec/unit/bosh/deployer/instance_manager_spec.rb:54:inblock (3 levels) in '\n4 examples, 1 failure, 3 passed\n```\nI tried stubbing the new method and instead returning a subclass that overrides the initialize method\nhttp://stackoverflow.com/questions/3722587/mocking-the-initialize-method-on-a-ruby-class\nhttp://stackoverflow.com/questions/6012952/what-is-going-on-here-rspec-stubnew-with\n```\n  it 'retries on BadResponseError (503 bad proxy) when waiting for agent and director through an http proxy' do\n      class InstanceManagerNoInit < Bosh::Deployer::InstanceManager\n        def initialize(config, config_sha1, ui_messager, plugin_name)\n    end\n  end\n\n  described_class.stub(:require)\n  described_class.stub(:initialize) {}\n  described_class.stub(:new) { InstanceManagerNoInit.new }\n  allow(Bosh::Common).to receive(:retryable)\n  instance_manager = Bosh::Deployer::InstanceManager.create(config)\n  instance_manager.wait_until_ready('director') do\n\n  end\n  expect(Bosh::Common).to have_received(:retryable) do |args|\n    expect(args[:on]).to include HTTPClient::BadResponseError\n  end\nend\n\n```\n``\nTesting started at 11:09 AM ...\n/home/gberche/.rvm/gems/ruby-1.9.3-p484/gems/rspec-mocks-3.0.0.beta1/lib/rspec/mocks/method_double.rb:89: warning: removinginitialize' may cause serious problems\nSystemStackError: stack level too deep\n/home/gberche/.rvm/gems/ruby-1.9.3-p484/gems/rspec-mocks-3.0.0.beta1/lib/rspec/mocks/method_double.rb:69\n```\nFinally got around using described_class.any_instance.stub(:initialize) {} suggested into \nhttp://stackoverflow.com/questions/316294/rspec-how-to-stub-an-instance-method-called-by-constructor\nThen the second issue is that the suggested expects will fail unless wait_until_retry is actually called. However, this is a private method, same for its callers wait_until_director_ready.  https://www.ruby-forum.com/topic/197346 suggest some alternatives to testing private methods \"The clue is in the names you gave them - you shouldn't\nhave complex methods, especially private ones.\"\nFor now I got around with using send to call the method.\n\nps: sorry if the multiple edits to this comment generated spam notifications, I accidentally triggered comment submission through unexpected keyboard shortcut during this long edit...\n. @dsabeti and @thecadams \nBesides a separate PR for the download, I started working on refining the exception filtering\nCurrently the BadResponseError is thrown at the following locations:\n- session.rb\n  - raise BadResponseError.new(\"connect to the server failed with status #{@status} #{@reason}\")\n  - raise BadResponseError.new(\"connect to ssl proxy failed with status #{@status} #{@reason}\", res)\n  - raise BadResponseError.new('unexpected EOF') (in parse_header)\n- httpclient.rb\n  - raise BadResponseError.new(\"redirecting to non-https resource\") (follow redirect\n  - raise BadResponseError.new(\"retry count exceeded\", res) (follow redirect)\n  - raise BadResponseError.new(\"unexpected response: #{res.header.inspect}\", res) success_content\nUnfortunately, there is no http status code or msg that can safely be accessed from the exception, nor tests that garantee that the message won't change in the future.\nI however tried to filter on the exception message to only retry on exception that contain \"connect\". Although retrying on other exceptions would'nt be that bad given that there is a max retry of 600, at worst operator would wait a bit more.\nAnyway, since the bosh team felt it was important, I made the investment, can you please review it at https://github.com/Orange-OpenSource/bosh/commit/47cf0c1d6d873eea19bd7991c5c114f84e46d27c and confirm this is the directing the bosh team wants to go. I have some minors steps left to do tomorrow before submitting a clean PR with this.\n. Sorry that I had missed the res accessor into BadResponseError. Thanks @dsabeti and the bosh team for you much cleaner and flexible solution with the lambda.\nThis PR started from me as a good will trying to avoid other users searching around when trying to use bosh to install CF behind an http proxy. \n\nThe symptom was that after having defined the http_proxy and https_proxy env for flows D, then flows A and F would implicitly go through the http proxy and and retry would fail.\nThe initial workaround for flows A and F looked like a quite simple 1 line fix so this initially felt it was a good idea to submit it. I'm sorry I underestimated the effort necessary for this both in the bosh team, community pairs and myself. \nThinking it through there might be a simpler workaround by leveraging http_client specific no_proxy env to no go through the http proxy for flows A and F. \nWhile theoretically there could have some use cases where bosh_cli is accessing the bosh_micro over an http proxy (e.g. having vcloud_cli BAT tests that use a public vchs vcloud instance on the internet + a micro-bosh instance over the internet), the workaround of using no_proxy seems acceptable, I'll further test it. \nLet me know if you share my analysis, if so I'll close this issue, submit a docs fix PR and rather focus my bosh/cf http-proxy effort on the remaining Cf related issues, namely flows 5 and 6 below. If you feel http_proxy for flows A, F deserve it, then I'm ok to spend the few more hours to complete it with the suggested lambda-based filtering approach.\n\nThanks again for your help, and sorry if I sometimes appeared grumbling during our interactions.\n. Thanks @adamstegman and @jtuchscherer for the follow up. As part of #535 I'll be retesting and closing this PR after validating the workaround. I haven't had yet a chance to work on it, hopefully I should be able to finish that in the coming 2 weeks.\n. @shinji62 sorry for delay on this, I was side-tracked for a while. I resumed this work this week, see https://github.com/vchs/bosh_vcloud_cpi/pull/4 I hope to be able to complete the bosh part in the next 2 weeks.\n. @goehmen sorry this PR is partial and is indeed dependent on more commits or another PR for assigning the http_proxy env. This first step is about using it in flow E in diagram below. \nI was asking guidance in the related bosh-dev@ thread about the way to assign the http_proxy environment variable, from a new proxy_uri property in the manifest with the following proposals\nwhere to make the ENV['http_proxy']  assignments:\na- at config parsing bosh/bosh-director/lib/bosh/director/config.rb ?\nb- in launchers  bosh\\bosh-director/bin/bosh-director-worker and bosh\\bosh-director/bin/bosh-director ?\nc- somewhere else ?\nPlease let me know if the bosh team has any preference, otherwise I'll go ahead with b) into bosh-director-worker line 53 and add it into this PR as new commit.\n\nThis http_proxy ENV would also be necessary for flow F (CPI to IaaS endpoints) initially reported into https://github.com/cloudfoundry/bosh/issues/77\nLet me know your thoughts about how to make this http_proxy contrib more efficient (phone call, background document, email discussion ?).\n. Thanks @monkeyherder and @cppforlife for your feedback, I'll try to progress on this in the coming days and update the PR with new commits.\n. @shinji62 good to hear this pr #535 is useful to you albeit not being complete. It lacks manifest support for configuring http_proxy and probably no_proxy env (to control which flow goes through a proxy w.r.t. going direct as this is likely to be very specific to each network design). It also lacks wider cpi support. I only started on vcloud support so far. Finally, it would probably deserve integration testing going through an http proxy in a similar way as https://github.com/cloudfoundry/cf-java-client/pull/90 ? It would increase test coverage to detect when some code does not honor the proxy settings, not sure if ruby meta programming can make this possible.\nAs of today, I have found a workaround in my organization's network that enables me to deploy CF without proxy. Given time constrainsts, I'm currently forced to delaying progress on this PR. If you or anyone wants to take over, I'd be happy to code review or be involved in related discussions.\n. Thanks @shinji62 for your PR ! \n@mariash I understand the implicit handling of http_proxy env var is not supported in ruby 1.9.3 Note that net/http does not use the HTTP_PROXY environment variable. If you want to use a proxy, you must set it explicitly., the version that I was using for testing.\nWith the recent ruby 2.0 support in bosh, this should work, I'll give it a try as well (the Bosh director version 1.2641.0 (00000000) I'm using against v176 seems to still be using ruby 1.9 from stack traces collected)\nI might be able to discard  PR made for vcloud cpi https://github.com/vchs/bosh_vcloud_cpi/pull/4 when used with a recent bosh version.\nNote that Bosh cli Documentation still mentions Note: Installing the BOSH CLI requires the latest patched version of Ruby 1.9.3, 2.0.x, or 2.1.x. and [bosh-cli ruby gems dependencies] still mentions Ruby Dependency >= 1.9.3. I'll try sending a docs PR mentionning preference to 2.1 to get proxy support in bosh-cli for deployign microbosh.\n. yes, PR #550 seems to address this issue (although I haven't tested it directly), thanks @karlkfi and @khwang1 for the notification.\n. Thanks Dmitry for your response. \nWhile bosh does not guarantee order of release jobs, relying on the specified order seems the only available workaround for fixing cf-release failed deploys (cf http://cf-dev.70369.x6.nabble.com/cf-dev-api-and-api-worker-jobs-fail-to-bosh-update-but-monit-start-OK-tp195p202.html ).\nIt would be interesting to understand whether the root cause is in bosh/monit or in cf-release, and hence if the bosh's guarantee on order of releases jobs would need to be made stronger.\n. Thanks @cppforlife for your response. A documented opt-out roll back steps would certainly help. \nIn our case we rolled back using regular SQL client. I was not familiar enough with the director_ctl console script to use it right away. Is it close to the CC console behavior (for which I had contributed initial pointers ) ?\n. > Another place I've seen this implemented is through health checks or telemetry. Services can report their cert lifetimes as metrics which allow monitors to be configured and watching.\nFYI, a related community contribution enables monitoring credhub entries in external systems (prometheus in this case) and trigger alarms when they are close to expiration. See https://github.com/bosh-prometheus/prometheus-boshrelease/pull/241\nSuch monitoring of credhub entries would however fail to detect that a bosh deployment isn't yet using the latest version of a given credential in credhub.\n/CC @psycofdj. thanks @mfine30 \n\nThere are a few potential ways we're considering addressing this\n\nOut of curiosity (as this isn't for short term), can you share some of your thoughts, or pointers to related story/bosh-notes if they exist already ?. ",
    "tlawrence": "Hi, any news on when this will be merged?\n. That's great! Will the build include a new stemcell?\nTim\nSent from Samsung Mobile\n-------- Original message --------\nFrom: rboshman notifications@github.com \nDate:\nTo: cloudfoundry/bosh bosh@noreply.github.com \nCc: tlawrence tim.lawrence1984@gmail.com \nSubject: Re: [bosh] update vCloud CPI gem and add vCloud infrastructure support to stemcell builder (#526) \nWe are actively working on the PR right now. We expect the merge to\ncomplete today and be available after the next green CI run after that.\nCheers\nOn Mon, Mar 3, 2014 at 6:25 AM, tlawrence notifications@github.com wrote:\n\nHi, any news on when this will be merged?\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/526#issuecomment-36514094\n.\n\n\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\n\u2014\nReply to this email directly or view it on GitHub.\n. Tried that, no difference. Are there any other escape characters?\nHaving to escape a password in a config file is pretty ugly. Are there any other ways we could workaround this?\nTim\nOn 24 Mar 2014, at 22:12, dingyin notifications@github.com wrote:\n\nCan you try escapting the \"!\" and \"$\" with \"\\\"? I believe these are special characters in YAML and will be treated as such by the parser.\n\u2014\nReply to this email directly or view it on GitHub.\n. I\u2019m afraid I haven\u2019t tried since. I have been using a password I know to work. Given I am the only one seeing it you may as well close it. \n\nThanks,\nTim\nOn 18 Jun 2014, at 07:14, greg oehmen notifications@github.com wrote:\n\n@tlawrence Is this still an issue or have you been able to work it out? Thanks\n\u2014\nReply to this email directly or view it on GitHub.\n. Does this affect max_threads at all? I am getting 500 DEPLOYMENT_LIMIT_EXCEEDED messages from vCloud despite having limited my requests. max_threads is set to 10 but my 11th VM always fails\n. \n",
    "allomov": "As for me this is serious question. Especially to build new CPIs. For instance you can check CPI for GCE by @frodenas, it uses last version of fog to support latest version of GCE API. As I see on this stage it didn't have crucial collisions with excon as is told on pivotaltracker discussion.\nI'm sure that fog as a layer that connects BOSH with cloud providers should be used only inside CPIs.\nI see the following components depend on fog:\n- bosh-director has fog only in its gemspec file, I'm not sure that it is actual requirement for director and it can/should be removed from there.\n- bosh-registry uses fog in InstanceManager (link), and I guess the better way is to move instance_ips method to CPI interface.\n- bosh_openstack_cpi take advantage of fog and it is completely ok.\nI guess it is quite possible to perform this update in order to develop better CPIs. \nBest wishes,\nAlex L.\n. cast my vote for @drnic's proposal \n. This is really cool thing and will allow to avoid building hacks around to render bosh templates (I know, we've made it once). This will really help.\nOne thing I am looking for is getting list of required_properties for concrete template. I guess it can be done by combing information from template properties and job's spec properties that does not have default value (this for instance). Are you planning to add such functionality to gem? \n. Ok, I see that you've already fixed it in develop branch. Cool.\n. I have rebased this changes to the latest BOSH stable tag (stable-3033). You can find diff with develop branch here: https://github.com/cloudfoundry/bosh/compare/develop...Altoros:power-3033-stemcell.\nStill this changes are not completely ready for PR because of some dev artifacts. Also there are some things to be done:\n1. I needed to remove rsyslog-mmjsonparse package, because it was absent in package sources.\n2. I had problem installing system_ixgbevf.\n3. I made changes that rely on this PRs: https://github.com/cloudfoundry/bosh-agent/pull/24 and https://github.com/cloudfoundry/bosh-davcli/pull/1.\n. AWS provider is not always suitable option for building stemcells. There could be bunch of reasons, for instance company policy.\n. Thank you anyway for going through PRs. It is hard work to keep everything uptodate.\n. A good question. I used bosh 2858 (this commit b72546515fe71a0fe123a0ef688d93088d19aec8) to run stemcell:build_os_image task.\nCurrently I can't see what could fix this issue in the master for changelog and diff. Do you use the same base os image for tests? Is it possible that this test image has correct version of udev?\n. @cppforlife could you comment on your plans to support solution with registry?\nDo you plan to make it IaaS independent or remove from bosh at all?\n. did you change sources of lib/cli/resources/job.rb? I am confused because error message show that error happen on line 12 and there is no any code invocation there.\n. @fraenkel really \"lovely\" behaviour.\nI fixed it in this PR https://github.com/cloudfoundry/bosh/pull/983\n. Thank you @hashmap. Looks much more better for me. \nIt seems it will fix this issue https://github.com/cloudfoundry/bosh-init/pull/46 with bosh-init.\n. I think it would be better to add 1.9.3 ruby version to this list in .travis.yml to be sure nothing is broken by upcoming changes.\n. I was able to fix this problem by installing nokogiri 1.6.6.2:\ngem install nokogiri -v 1.6.6.2\ngem install bosh_cli --no-ri --no-rdoc\n. I was able to fix this problem by installing nokogiri 1.6.6.2:\ngem install nokogiri -v 1.6.6.2\ngem install bosh_cli --no-ri --no-rdoc\n. Yey!. Yey!. It does seem to be a strange error. I haven't heard about such error before. At the moment I run ruby 2.1.5 on my jumpbox with latest bosh_cli:\n```\n$ ruby -v\nruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]\n$ gem list bosh_cli\n LOCAL GEMS \nbosh_cli (1.3130.0, 1.3126.0, 1.3100.0)\n```\nCould you tell more about the error? How do you install bosh cli? What is the error message? Is there any stack trace?\n. It does seem to be a strange error. I haven't heard about such error before. At the moment I run ruby 2.1.5 on my jumpbox with latest bosh_cli:\n```\n$ ruby -v\nruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]\n$ gem list bosh_cli\n LOCAL GEMS \nbosh_cli (1.3130.0, 1.3126.0, 1.3100.0)\n```\nCould you tell more about the error? How do you install bosh cli? What is the error message? Is there any stack trace?\n. Sorry for late response. As far as I understand you need following things:\n- install Ruby with the version higher than 1.9.3\n- install BOSH CLI gem\nThe first task is easy to do with RVM (ruby version management tool). I believe you'll find useful this docs. You haven't mentioned a system on your host, still RVM is possible to use with Windows. Despite of this fact my advice is to use Linux.\nAfter you install RVM you'll need to run following commands:\nbash\nrvm install 2.1.5                       #  choose the version you need here\nrvm use 2.1.5@bosh --create             #   create and use a separate ruby environment for your work\ngem install bosh_cli --no-ri --no-rdoc  #   install BOSH CLI\nWish you luck with this.\n. Sorry for late response. As far as I understand you need following things:\n- install Ruby with the version higher than 1.9.3\n- install BOSH CLI gem\nThe first task is easy to do with RVM (ruby version management tool). I believe you'll find useful this docs. You haven't mentioned a system on your host, still RVM is possible to use with Windows. Despite of this fact my advice is to use Linux.\nAfter you install RVM you'll need to run following commands:\nbash\nrvm install 2.1.5                       #  choose the version you need here\nrvm use 2.1.5@bosh --create             #   create and use a separate ruby environment for your work\ngem install bosh_cli --no-ri --no-rdoc  #   install BOSH CLI\nWish you luck with this.\n. As far as I understand this issue is duplicate of this one https://github.com/cloudfoundry/bosh/issues/1024 and can be removed \n. As far as I understand this issue is duplicate of this one https://github.com/cloudfoundry/bosh/issues/1024 and can be removed \n. Could you tell what plans do you have on supporting autocomplete in bosh_cli?\n. Could you tell what plans do you have on supporting autocomplete in bosh_cli?\n. I found an old chunk of bosh code that seems to implement bash completion, still can't see how I can use it.\n. I found an old chunk of bosh code that seems to implement bash completion, still can't see how I can use it.\n. @barthy1 I had the same problem with CI. I think you should ping someone from BOSH team to re-run tests.\n. bosh_cli_plugin_micro is outdated way to install BOSH to your IaaS.\nPossibly the best way to start with CF will be using bosh-lite. It will create a VM on your localhost (or AWS) with BOSH, after that you'll be able to deploy CF easily within this VM.\n. bosh_cli_plugin_micro is outdated way to install BOSH to your IaaS.\nPossibly the best way to start with CF will be using bosh-lite. It will create a VM on your localhost (or AWS) with BOSH, after that you'll be able to deploy CF easily within this VM.\n. You should follow this docs to deploy BOSH to IaaS of your choice.\n. You should follow this docs to deploy BOSH to IaaS of your choice.\n. It seems to be CPI dependent feature. I think possible solution can be providing CPI with additional information about reason of VM removal to avoid bosh-director from having CPI dependent options.\n. It seems to be CPI dependent feature. I think possible solution can be providing CPI with additional information about reason of VM removal to avoid bosh-director from having CPI dependent options.\n. @jianqiu the idea is that bosh-director should provide context to CPI with the reason of VM removal.\n. @jianqiu the idea is that bosh-director should provide context to CPI with the reason of VM removal.\n. @cppforlife sorry for delay in response.\nAbout the idea of how CPI can get context of operation that is currently happen: I can think of 2 ways, and both of them are debatable:\n1. CPI can make request to BOSH director API and get the latest event that occurred with disk which has specific CID. This will require extend events endpoint to be able to filter events by resources like disks. Also this will require CPI developers to implement simple bosh director client. Both options don't seem to be hard.\n2. CPI interface can be extended. In order to extend CPI interface easier, the way to get version of supported CPI interface should be implemented. This seems to be trickier and less at this moment, not so required (I mean there won't be lots of CPI interface changes).\nI would vote for 1st option. What do you think?\n. @cppforlife sorry for delay in response.\nAbout the idea of how CPI can get context of operation that is currently happen: I can think of 2 ways, and both of them are debatable:\n1. CPI can make request to BOSH director API and get the latest event that occurred with disk which has specific CID. This will require extend events endpoint to be able to filter events by resources like disks. Also this will require CPI developers to implement simple bosh director client. Both options don't seem to be hard.\n2. CPI interface can be extended. In order to extend CPI interface easier, the way to get version of supported CPI interface should be implemented. This seems to be trickier and less at this moment, not so required (I mean there won't be lots of CPI interface changes).\nI would vote for 1st option. What do you think?\n. Originally, this error takes place in loggregator template.\nIt looks like dynamic network is not intended to provide any ip property to networks spec: see this test case.\nThis could mean you'll need to switch to manual networks. With the latests changes to bosh project you can do it easily, because bosh director begun to provide jobs with ips automatically, so you don't need to specify static_ips.\n. Originally, this error takes place in loggregator template.\nIt looks like dynamic network is not intended to provide any ip property to networks spec: see this test case.\nThis could mean you'll need to switch to manual networks. With the latests changes to bosh project you can do it easily, because bosh director begun to provide jobs with ips automatically, so you don't need to specify static_ips.\n. @cppforlife @loewenstein did I get it right? should it be added to docs or fixed somehow? \nBecause lots of cf-release templates use spec.networks to get ip addresses of the job. This fact makes it impossible to use it with dynamic networks.\n. @cppforlife @loewenstein did I get it right? should it be added to docs or fixed somehow? \nBecause lots of cf-release templates use spec.networks to get ip addresses of the job. This fact makes it impossible to use it with dynamic networks.\n. The error in CI does not seem to be connected with this PR.\n. The error in CI does not seem to be connected with this PR.\n. Hi @holgerkoser. I share your confusion on this case. If you have access to list of all tasks (even if from removed deployments), you should be able to filter this tasks by deployment. \nAs I see, this is expected behaviour of bosh-director, because of this specs.\nThink it can be fixed rather quickly.\n. Hi @holgerkoser. I share your confusion on this case. If you have access to list of all tasks (even if from removed deployments), you should be able to filter this tasks by deployment. \nAs I see, this is expected behaviour of bosh-director, because of this specs.\nThink it can be fixed rather quickly.\n. @holgerkoser I've created a PR #1212 to address this issue.\n. @holgerkoser I've created a PR #1212 to address this issue.\n. @dpb587-pivotal thank you for the note. It seems that it will not be hard to follow this changes in this PR. \nOne thing that bothered me was that tasks from removed deployment can be seen by everyone who has read access to director (even if this user was not in the scope of this project). The possible solution here could be soft deletion, which seems to lead to a lot of code changes.\n. @dpb587-pivotal thank you for the note. It seems that it will not be hard to follow this changes in this PR. \nOne thing that bothered me was that tasks from removed deployment can be seen by everyone who has read access to director (even if this user was not in the scope of this project). The possible solution here could be soft deletion, which seems to lead to a lot of code changes.\n. Hey.\nI've reproduced this error on my environment. The interesting thing is that postgres database already has registered migration from 20160329201256_set_instances_with_nil_serial_to_false.rb file that doesn't exist in stable-3125.4:\nroot@b2485b68-2948-4565-5b8e-5aaeaebc2c4c:~# /var/vcap/packages/postgres/bin/psql -U postgres -d bosh -c \"select * from schema_migrations;\" | grep 20160329201256\n 20160329201256_set_instances_with_nil_serial_to_false.rb\n. Hey.\nI've reproduced this error on my environment. The interesting thing is that postgres database already has registered migration from 20160329201256_set_instances_with_nil_serial_to_false.rb file that doesn't exist in stable-3125.4:\nroot@b2485b68-2948-4565-5b8e-5aaeaebc2c4c:~# /var/vcap/packages/postgres/bin/psql -U postgres -d bosh -c \"select * from schema_migrations;\" | grep 20160329201256\n 20160329201256_set_instances_with_nil_serial_to_false.rb\n. I've migrated back to 255.8 (must be tag stable-3125.3) and it appears to contain missing migrations:\nroot@b881c74d-78d2-43c9-5b3d-5cc9a7eabf5c:/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/db/migrations/director# ls -al | grep 20160329201256\n-rw-r--r-- 1 root root  446 Apr 16 14:31 20160329201256_set_instances_with_nil_serial_to_false.rb\n. I've migrated back to 255.8 (must be tag stable-3125.3) and it appears to contain missing migrations:\nroot@b881c74d-78d2-43c9-5b3d-5cc9a7eabf5c:/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/db/migrations/director# ls -al | grep 20160329201256\n-rw-r--r-- 1 root root  446 Apr 16 14:31 20160329201256_set_instances_with_nil_serial_to_false.rb\n. Looks like bosh.io points to commit 8f47e6a7 for bosh version 255.8, this commit contains missing migration files.\nAt the same time bosh release version 255.9 in bosh.io points to 47cafe56, which doesn't have this files.\n. Looks like bosh.io points to commit 8f47e6a7 for bosh version 255.8, this commit contains missing migration files.\nAt the same time bosh release version 255.9 in bosh.io points to 47cafe56, which doesn't have this files.\n. @cppforlife thank you for quick update, it worked nicely for me.\n@Infra-Red I think this issue can be closed.\n. @cppforlife thank you for quick update, it worked nicely for me.\n@Infra-Red I think this issue can be closed.\n. Hey @tinygrasshopper! Could you tell what version of BOSH do you use?\nI am sure that the latest version of BOSH got rid of using Redis for locks already. As far as I can see no-redis-no-cry branch is merged to master.\n. Hey @tinygrasshopper! Could you tell what version of BOSH do you use?\nI am sure that the latest version of BOSH got rid of using Redis for locks already. As far as I can see no-redis-no-cry branch is merged to master.\n. right, thank you for notice. seems like latest changes can fix this problem.\n. right, thank you for notice. seems like latest changes can fix this problem.\n. @tinygrasshopper actually, current changes to deployments controller are not going to change behavior: you'll get redirect (302 http status) to task object every time you try to create deployment.\nThis is a common pattern to provide asynchronous calls (create_deployment method just queues a job to create deployment) and I don't think it should be or will be changed.\n. @tinygrasshopper actually, current changes to deployments controller are not going to change behavior: you'll get redirect (302 http status) to task object every time you try to create deployment.\nThis is a common pattern to provide asynchronous calls (create_deployment method just queues a job to create deployment) and I don't think it should be or will be changed.\n. Hey, @craigfurman! Yes, this is a behaviour that is currently implemented in the bosh-director code. I can't say if it is going to be changed in future, I just put my thoughts on this point.\nIt is good for REST API to follow one way of processing the same requests on objects. Currently, most of requests that change state of object make redirects to task object. Doing it in such way allows to support client side easier. Also it allows to track every event that took place in a system (by saving tasks to database). I have seen such redirects on number of projects, so I my believe is that bosh-director can go with it.\n. Hey, @Freyert!\nI also have not seen such docs, but I can address your questions.\n1. What are the rules for name creation?\nThe short answer is that the DNS records for job instances are created in the following way:\n<instance-index>.<job-name>.<network-name>.<deployment-name>.<bosh-dns-domain>\nYou set bosh-dns-domain in your deployment manifest with dns.domain_name option. Other options (such as instance index and name) are taken from instance model.\nIf you want to understand better how generating of DNS records works, you need to look into DnsManager implementation (link) in BOSH director. DNS name is built with DnsManager#dns_record_name method. \nAs a tip: you can run bosh vms --dns command to see what DNS records are created for your job/instances by BOSH.\n2. What if I want multiple IPs?\nOriginally BOSH director create only one DNS record per instance. If you want to have multiple DNS records you'll need to write script to add it automatically or create it manually.\n3. Can I select one IP to be \"private\"?\nAFIK all DNS records, that are created by BOSH, work only for private network, so you can't access them from outside.\nI hope it helped you! If I missed something or you have any further questions, you are welcome to ask them.\n. Hey, @Freyert!\nI also have not seen such docs, but I can address your questions.\n1. What are the rules for name creation?\nThe short answer is that the DNS records for job instances are created in the following way:\n<instance-index>.<job-name>.<network-name>.<deployment-name>.<bosh-dns-domain>\nYou set bosh-dns-domain in your deployment manifest with dns.domain_name option. Other options (such as instance index and name) are taken from instance model.\nIf you want to understand better how generating of DNS records works, you need to look into DnsManager implementation (link) in BOSH director. DNS name is built with DnsManager#dns_record_name method. \nAs a tip: you can run bosh vms --dns command to see what DNS records are created for your job/instances by BOSH.\n2. What if I want multiple IPs?\nOriginally BOSH director create only one DNS record per instance. If you want to have multiple DNS records you'll need to write script to add it automatically or create it manually.\n3. Can I select one IP to be \"private\"?\nAFIK all DNS records, that are created by BOSH, work only for private network, so you can't access them from outside.\nI hope it helped you! If I missed something or you have any further questions, you are welcome to ask them.\n. @jianqiu I afraid, that the team can be aware of the some technical debt for this work around. This can be my personal feeling, but maybe such kind of feature should be re-thinked in some way.\n. @jianqiu I afraid, that the team can be aware of the some technical debt for this work around. This can be my personal feeling, but maybe such kind of feature should be re-thinked in some way.\n. Hey, @krishnanrs.\nThis changes seem to be very cloud specific, so the best way to provide them will be developing your  external CPI for specifically for Bracket Computing infrastructure.\nYou can move delay for 30 seconds to attach_disk method of CPI and it will be executed before agent will get command to mount the disk.\nTalking about changes in stemcell builder, they also should be decoupled in the other way, because this changes can fail on other IaaSes. For instance, you can build you pipeline that builds BRKT stemcell on base of current pipelines for stemcell building (which are here), you can patch stemcell builder inside this pipeline.\n. Hey, @krishnanrs.\nThis changes seem to be very cloud specific, so the best way to provide them will be developing your  external CPI for specifically for Bracket Computing infrastructure.\nYou can move delay for 30 seconds to attach_disk method of CPI and it will be executed before agent will get command to mount the disk.\nTalking about changes in stemcell builder, they also should be decoupled in the other way, because this changes can fail on other IaaSes. For instance, you can build you pipeline that builds BRKT stemcell on base of current pipelines for stemcell building (which are here), you can patch stemcell builder inside this pipeline.\n. Hey, @mmessmore.\nI believe this issue should be moved to cloudfoundry-incubator/bosh-vsphere-cpi-release repo. Since the problem is in these lines. This is definitely not problem of BOSH project.\n. @dpb587-pivotal hey. could you also check this PR https://github.com/cloudfoundry/bosh-init/pull/73 for additional validations in bosh-init? it covers error type that happens rather frequently.\n. Hey @philippthun!\nIt appeared that in current release of BOSH postgresql server has version 9.4.6, but client libpq library still has version 9.0.23. I am sure that BOSH team will fix it in upcoming releases.\nIf you have a strict requirement to use backup functionality, you can downgrade to stable-3196 version (bosh release v254).\nThank you for making report for this error.\n. This is expected behaviour, if you take a look on the code. format=full triggers tasks that gathers actual information from all instances in the deployment. This can require too much time to wait.\n. You can fetch the result by making the following request:\ncurl -v -s -k -H \"Authorization: bearer $UAA_TOKEN\" \"https://$BOSH_HOST:25555/tasks/<task-id>/output?type=result\"\nAlso you can simply run bosh-cli command:\nbosh task <task-id> --result. I had the same issue trying to update stemcell with an old version of bosh-cli.\n@cppforlife do you think it is possible to create a way to have correspondence between bosh-cli version and the version of stemcell? It worked well with ruby bosh cli.. @dpb587-pivotal thank you for your comment. Using the latest version of CLI might be a good solution here.\nFrom my understanding there is a weak dependence between bosh-agent and bosh-cli, because bosh-cli performs requests to bosh-agent through http mbus. Changing interface of http mbus communication is going to break possibility to deploy/update bosh using bosh-cli.. Hey, @ivandavidov.\nBOSH uses a special environment (that is provided by stemcell image) to run its jobs. It is quite uncommon task to run BOSH jobs out of cloud/container environment. You can run all BOSH components on VM, but it is going to be hard to handle everything. To do so you need to run all components listed in manifest generated by bosh-deployment for warden iaas.\nThere is a project that helps to make any VM to behave like BOSH vm, but I am not sure it is suitable in your case.\nAlso you can try using docker image of BOSH, it may probably satisfy your needs to avoid using Virtualbox.\n. I think you can simply remove this line.\n. ",
    "goonzoid": "@drnic Opened against the wrong branch, have re-opened here: #537 \n. ",
    "karlkfi": "@goehmen Do you want this in your icebox so your team can follow up later?\nCF Community Pair (@karlkfi & @khwang1)\n. @drnic Is this the same problem as #547?\nCF Community Pair (@karlkfi & @khwang1)\n. My understanding is that pre-packaging has internet access because it's done locally, but packaging is not guaranteed to have internet access.\n@goehmen Do you want to address this?\nCF Community Pair (@karlkfi & @khwang1)\n. #543 is a duplicate of this.\n. Is your concern handled by #550 ?\nCF Community Pair (@karlkfi & @khwang1)\n. Closing as resolved. If you have a new issue after #550 is merged, let us know.\n. We re-ran the CI build and it succeeded. So it looks like it was a flakey test failing the first time.\nThe change looks innocuous, and the spec already covers centos. Should we make similar changes in other places where centos is explicitly called out in this file?\n@goehmen Would you like this in the Bosh backlog?\nCF Community Pair (@karlkfi & @khwang1)\n. @goehmen Would you like this in the Bosh team backlog? \nIt seems to make more director options configurable via template.\nOn a related note, it appears the template config is nearing the complexity of the configuration it's generating. At what point does it become easier to just have example configs, instead of a template?\n. Oops, got a little ahead of myself there. Still need the CLA signed by @Justin-Yu.\n. Verified vmware email address. \nIt sounds like you added the vmware email to your github account after making the PR. Future PRs shouldn't ask for a CLA due to email domain matching.\n@goehmen You want this one?\nCF Community Pair (@karlkfi & @khwang1)\n. @goehmen Can someone from BOSH take a look at this issue? \nCF Community Pair (@karlkfi & @khwang1)\n. @goehmen Would you like this in the BOSH team backlog?\nCF Community Pair (@karlkfi & @khwang1)\n. @goehmen Would you like this in the BOSH team backlog?\n. @goehmen Do you want this in the BOSH backlog?\n. My understanding of the current cli version behavior:\n- by default the manifest version is used\n- the --version flag allows setting a new version to use\n- the --final flag either uses the specified final version (via manifest or version flag) or truncates the \"#-dev\" suffix and auto-increments\n- the --rebase ignores the specified \"#-dev\" suffix and replaces it with a new one auto-incremented from the latest (with the same final version) from the director db.\nMy current work on user supplied versions should allow all of the above to continue working in a reverse compatible way for single number final versions, but operate subtly different if a more complex user-specified version is used. \nIt could be argued that --final behavior is confusing in the context of user-specified behavior, but I'm hesitant to remove it after a single pass. I'd rather get more feedback on the usability of the new feature before breaking old features.\nThat said, with the new user-specified versions you're no longer restricted to making dev releases. You can append additional dot separated components easily using the --version flag. If you don't want auto-incrementing, just don't use --final or --rebase. \n. After checking the code, it looks like my assumption about the default behavior (without --version, --rebase, or --final) was wrong. It looks like in that case the dev version gets auto-incremented, rather than just using the one in the manifest. So it looks like you have to use --version if you want to skip auto-incrementing.\n. For the record, specifying the version on the command line (via --version) is currently limited to final releases. @goehmen Would it make sense to loosen this restriction to allow specifying new custom pre-release or post-release versions via cli?\n. @duglin With the new semi-semantic version support (https://www.pivotaltracker.com/story/show/62085266) I'd recommend taking advantage of the new the new pre-release version support, going from a pivotal release of \"172\" to an IBM release of \"172.1-ibm.1\".  Then for dev releases it would add \"172.1-ibm.1+dev.1\". This would retain logical sorting with pivotal releases: \"172\" < \"172.1-ibm.1\" <  \"172.1-ibm.1+dev.1\" < \"172+dev.1\" < \"172.1\" < \"172.1+dev.1\" < \"173\"\nIf you stuck with \"ibm-v172.1\" it would parse it thinking \"ibm\" was the primary segment and \"v172.1\" was the pre-release segment, and the \"v172\" component would sort alphanumerically. It should work otherwise, but sorting would be off (which would affect bosh cleanup or version: latest).\n. The new user-defined version support is not yet released. There are several related stories in progress. When it is done and release, there will be a new bosh cli gem and new bosh stemcells (with the new director). \nThat said, bosh-lite uses its own custom stemcell with the old director. So bosh-lite will also have to be updated in order to use new-format versions. We just made a story to update it: https://www.pivotaltracker.com/story/show/72572098\nOnce user-defined versions are supported, if your custom release version doesn't sort well, you wont be able to use \"latest\" release, bosh cleanup may delete the wrong versions\", and auto-incrementing in create release and upload release --rebase may not work, but most other things may still work for you, especially if you create release with a specified manifest or final version.\nAs it turns out, the user-defined version release also adds the \"--version\" flag I've been mentioning. Sorry for the confusion.\n. @duglin, @sykesm mentioned today that you probably meant that you wanted to use \"ibm\" as the release name, not in the release version as I had mistakenly inferred.\nWe discussed this morning that specifying a name would actually resolve most of the issue, and that custom versioning is somewhat tangential to your issues. It sounds like custom versioning might be usable to work around the issue, but custom naming would be more useful, and having both would be ideal.\nIs that accurate?\n. Interesting...\nSo you're already creating custom versions by modifying the manifest before creating a release?\nIf so, then yes, that's the version string. It's just an unexpected usage. \nIs there a technical or business reason why you don't want to name it something like \"cf-ibm\", other than having to modify another line in the release manifest?\n. I think BOSH already allows you to supply a manifest as a CLI parameter to bosh create release, in which case the name and version from the manifest are used and not modified or incremented, allowing you to specify them beforehand. Have you tried that? \nI can't speak to \"why\", being new to the team, other than to guess that it doesn't yet allow you to specify them on the CLI itself, because it wasn't a known required use case, not that it was deliberately disallowed. The existing simple versioning strikes me as a product of implementing the simplist approach to solve the known requirements, which is even now our general approach to new features. \nI do think it's probably reasonable tho to add a --name parameter, but I delegate those product decisions and prioretization to our @goehmen. A PR is probably the best way to go, but the code around versioning is in flux ATM. It might be better to hold off until the new versioning changes are complete before attempting it. \n. Looks like you've discovered an interesting discrepancy in how the help info prints...\n$ bosh version\nBOSH 1.2579.0\n$ bosh help create release\ncreate release [<manifest_file>] [--force] [--final] [--with-tarball]\n[--dry-run]\n    Create release (assumes current directory to be a release repository)\n    --force                       bypass git dirty state check\n    --final                       create final release\n    --with-tarball                create release tarball\n    --dry-run                     stop before writing release manifest\n$ bosh create release --help\nUsage: bosh [options]\n        --force                      bypass git dirty state check\n        --final                      create final release\n        --with-tarball               create release tarball\n        --dry-run                    stop before writing release manifest\nTBH, I don't actually know why there's an option to specify a manifest. The only useful scenario I can think of would be to re-create a release after tweaking it manually, or for reproducing bug behavior given specific input at a later stage. I was just thinking it might work for your needs if you already have a manifest.\n. I created a bug for the help discrepancy: https://www.pivotaltracker.com/story/show/72714220\n. @duglin The release versioning changes are live. That code should be calmed down enough now for you to work on a PR to add a --name feature to create release. \nThere is an outstanding bug, however, where renaming a release without other changes seems to mess up version auto-incrementing, due to how the release index stores by fingerprint and not by release name. But you'll probably be using --version for your case, with other changes, so it might not matter.\nhttps://www.pivotaltracker.com/story/show/72994716\n. @drnic We fixed the bug in 2596 stemcell. ed9ea1c1297c69e9531f7ae154753ad72466b35e commit has more details. please confirm if it solved your problem.\n. @drnic Thanks for the fix.\nWe'll merge this in as soon as we have a green CI build.\nCF BOSH Team\n@karlkfi & @calebamiles \n. merged\n. Not sure why those last emails didn't make it back into github.\nIn summary, the status on this is that we're not going to pull this pull request in as written. If you change this pull request to use the config file pass in method to ginkgo via the bin/test script we'll review it again.\nCF BOSH Team\n@karlkfi & @calebamiles \n. Understood. Thanks for trying to stick to CF's documented best-practices, and thanks for providing a useful fix.\nUnfortunately, I haven't talked to anyone here who actually prefers env vars for this kind of use case. We have lots of apps doing it and it's just lead to frustration. There's even some anecdotes of left-over env vars accidentally targeting prod instead of dev for subsequent operations, leading to various disasters. Much of the rest of 12factor's recommendations make sense, but we haven't really taken the time to write our own similar best practices book that expresses our reservations in as authoritative a way. \nMigrating existing app's configs also just hasn't been our highest priority.  But in this case, since it's a new config, it's worth trying something new.\n. After further review of the test it occurs to me that it's rather ridiculous that these tests require your computer to be connected to a network using IPv4 in order to pass. \nAs was mentioned in the initial issue, this is error prone and buggy. Furthermore, this PR doesn't actually resolve that underlying restriction. It just allows you to specify some other interface (like WLAN) that needs to be connected to an IPv4 network. \nA better fix would be to create the ipResolver with a mock InterfaceToAddrsFunc that returns a known set of addresses, not all of which are IPv4.\nIt's unfortunate, but there's no good way to test the real NetworkInterfaceToAddrsFunc impl that delegates to gonet.InterfaceByName. Thankfully it has no real logic, just a method proxy. This is a fairly common problem with static utility methods, tho. There's no way to test things that call them without calling them. \n. Hi @liuhewei,\nThank you for submitting your pull request. You have highlighted a place where the BATs assumed that they would only be used in the context of the microbosh, so that the DNS host would necessarily be the director host. \nWe'll pull this in as soon as we have a green CI pipeline.\nCF BOSH Team\n@karlkfi & @calebamiles \n. merged\n. @dengwa looks better, thanks!\n. Re-merged to develop. \nWrapped the NATS calls in NatsRpc.send_message that lazy loads NATS.connect\n. merged to develop\n. There's a lot of other places where stemcell:build is called in bosh-dev (mostly specs) that will cause this to fail. Did you try to run the unit/integration tests against this?\nTry the containerized tests: https://github.com/cloudfoundry/bosh/tree/master/gocd\n. Fixed in develop. Going through the CI now.\ndc98ab8b6d4e267b0a8b9d82b1912770f47b2b22\n. Hmm.. that's odd...\nCreating partition 0 with start 3071000064B and end 33747754751B\nRunning command: parted -s /dev/vda unit B mkpart primary 3071000064 33747754751\nSuccessful: true (0)\nStdout: Warning: The resulting partition is not properly aligned for best performance.\nCreating partition 1 with start 33747754752B and end 64424509439B\nRunning command: parted -s /dev/vda unit B mkpart primary 33747754752 64424509439\nStdout: Error: You requested a partition from 33747754496B to 64424508928B.\nThe closest location we can manage is 33747755008B to 64424508928B.\nThe parted command asked for 33747754752 as the start of the ephemeral partition, but parted says we requested 33747754496... Weird.\nWell, hopefully my proposed alignment (see linked story for details) fixes this. I'll be testing it today.\n. We fixed this for GoCD by adding the git config to the Dockerfile without realizing it was also happening on Travis. We probably should just configure them for the local repo in the test itself...\n. Made a story you can follow, but I'm not sure what kind of prioritization it will get: https://www.pivotaltracker.com/story/show/83247218\n. Apparently the updated image was not pushed to docker hub. We'll get on that.\n. The cpi cli aren't currently designed to be used by humans, but we should definitely have the expected API documented somewhere. IIRC, most of the commands take a JSON string as stdin. \nFor the vcloud cpi release we actually put the ruby script (like the one you modified) in the release itself, rather than in the cpi gem. We mostly did it because it wraps a gem we don't have access to, but it has the side effect of not installing something humans can't use when you gem install the bosh cli. \n. That's correct. The bosh director assumes all templates are erb templates and renders them with ruby. IIRC, the bosh agent, when installing the rendered templates, makes any under 'bin/' executable (+x).\n. It looks like the vsphere cpi is more demanding than the configuration processing in the director job templates. See the full validation here: https://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/lib/cloud/vsphere/config.rb\nAt some point in the near future the cpi config will go directly to the cpi, rather than going through the director. Hopefully this will make debugging CPI config simpler. \nI can't validate your issue arm, but it possible that your yaml was invalid. Did you try one of these?\nclusters:\n- cfcluster\nclusters: [cfcluster]\nclusters:\n- cfcluster: {}\nAs for release changes: microbosh is deployed using a bosh release precompiled into the stemcell, which is non-trivial to modify. We're working on a new bosh-micro cli that handles release compilation itself, but it's not quite ready for public consumption. \nThe new docs might have more info on resource pools: http://bosh.io/docs/vsphere-cpi.html\n. It should be fine to refactor, as long as it's a new commit on the PR branch. If it ends up being too much then it's easy enough to split it into two PRs that way by cherry picking to a new branch. \n. ",
    "rohmann": "@drnic Are you just looking for a way to install https://github.com/berkshelf/vagrant-berkshelf/pull/158 as a vagrant plugin?\nDefinitely a workaround, but you can do this:\ngit clone https://github.com/chulkilee/vagrant-berkshelf.git\ncd vagrant-berkshelf\ngit checkout vagrant-1.5\ngem build vagrant-berkshelf.gemspec\nvagrant plugin install --plugin-version 1.4.0.dev1 vagrant-berkshelf-1.4.0.dev1.gem\n. Hmm, this didn't happen for me. Maybe try purging your vagrant plugins first?\nrm -r ~/.vagrant.d/plugins.json ~/.vagrant.d/gems\n. @mgarciap glad that worked for you. One more thing I noticed is that I couldn't install vagrant-omnibus after doing this. I needed start over and install that before vagrant-berkshelf.\n. ",
    "mgarciap": "I was really sad it was failing but I followed your instructions. Before doing so I\nvagrant plugin uninstall berkshelf\nAnd it worked.\n10:59 AM $ git clone https://github.com/chulkilee/vagrant-berkshelf.git\nCloning into 'vagrant-berkshelf'...\nremote: Counting objects: 1038, done.\nremote: Compressing objects: 100% (607/607), done.\nremote: Total 1038 (delta 412), reused 1038 (delta 412)\nReceiving objects: 100% (1038/1038), 145.33 KiB | 174.00 KiB/s, done.\nResolving deltas: 100% (412/412), done.\nChecking connectivity... done\n~/dev/cf/vagrant-installer \n10:59 AM $ cd vagrant-berkshelf/\n~/dev/cf/vagrant-installer/vagrant-berkshelf [master|\u2714] \n10:59 AM $ git checkout vagrant-1.5\nBranch vagrant-1.5 set up to track remote branch vagrant-1.5 from origin.\nSwitched to a new branch 'vagrant-1.5'\n~/dev/cf/vagrant-installer/vagrant-berkshelf [vagrant-1.5|\u2714] \n10:59 AM $ gem build vagrant-berkshelf.gemspec\nWARNING:  description and summary are identical\nWARNING:  prerelease dependency on berkshelf (>= 3.0.0.beta7) is not recommended\n  Successfully built RubyGem\n  Name: vagrant-berkshelf\n  Version: 1.4.0.dev1\n  File: vagrant-berkshelf-1.4.0.dev1.gem\n~/dev/cf/vagrant-installer/vagrant-berkshelf [vagrant-1.5|\u2714] \n11:00 AM $ vagrant plugin install --plugin-version 1.4.0.dev1 vagrant-berkshelf-1.4.0.dev1.gem\nInstalling the 'vagrant-berkshelf-1.4.0.dev1.gem --version '1.4.0.dev1'' plugin. This can take a few minutes...\nInstalled the plugin 'vagrant-berkshelf (1.4.0.dev1)'!\n. ha ha. I had to do the very same thing in order to install an old project of mine (cf-vagrant-installer).\nBerkshelf went well... and then\n```\n11:20 AM $ vagrant plugin install omnibus\nInstalling the 'omnibus' plugin. This can take a few minutes...\nThe plugin(s) can't be installed due to the version conflicts below.\nThis means that the plugins depend on a library version that conflicts\nwith other plugins or Vagrant itself, creating an impossible situation\nwhere Vagrant wouldn't be able to load the plugins.\nYou can fix the issue by either removing a conflicting plugin or\nby contacting a plugin author to see if they can address the conflict.\nVagrant could not find compatible versions for gem \"celluloid\":\n  In Gemfile:\n    vagrant-berkshelf (>= 0) ruby depends on\n      celluloid (~> 0.13.0) ruby\nvagrant (= 1.5.1) ruby depends on\n  celluloid (0.15.2)\n\n```\n. ",
    "samuelololol": "I have same issue with @mgarciap \n```\nvagrant plugin install vagrant-berkshelf\nInstalling the 'vagrant-berkshelf' plugin. This can take a few minutes...\nThe plugin(s) can't be installed due to the version conflicts below.\nThis means that the plugins depend on a library version that conflicts\nwith other plugins or Vagrant itself, creating an impossible situation\nwhere Vagrant wouldn't be able to load the plugins.\nYou can fix the issue by either removing a conflicting plugin or\nby contacting a plugin author to see if they can address the conflict.\nVagrant could not find compatible versions for gem \"celluloid\":\n  In Gemfile:\n    vagrant-berkshelf (>= 0) ruby depends on\n      celluloid (~> 0.13.0) ruby\nvagrant (= 1.5.1) ruby depends on\n  celluloid (0.15.2)\n\n```\n. ",
    "dingyin": "Duplicate pull request\n. Can you try escapting the \"!\" and \"$\" with \"\\\"? I believe these are special characters in YAML and will be treated as such by the parser.\n. Will reopen in to Dev branch.\n. ",
    "Justin-Yu": "Hi Tim,\nI tried the password ending with \"!\" and \"$\" using the current vCloud CPI code to do micro bosh deployment on vCloud. It seems working fine for me. What weird things were added in the auth header? Can you share your micro bosh deployment log file(bosh_micro_deploy.log) with me?\n. @karlkfi @cfdreddbot I updated my git profile to use VMware email as primary. It's not necessary to sign CLA for BOSH code as VMware employee, right? \n. @karlkfi any update on this pull request?\n. @tlawrence This change doesn't affect max threads. It affects the following parameters:  wait_max, wait_delay, cookie_timeout, retry_max and retry_delay in bosh_vcloud_cpi. \n. ",
    "yuhuan00": "I can log in the VM created by bosh with \"vcap/c1oudc0w\".\nand here is the log file of bosh_micro_deploy.log\nroot@ubuntu251:~/deployments/micro01# cat bosh_micro_deploy.log \nI, [2014-04-04T00:07:17.398113 #5148] [0xded7f8]  INFO -- : No existing deployments found (will save to /root/deployments/bosh-deployments.yml)\nI, [2014-04-04T00:07:30.732662 #5148] [0xded7f8]  INFO -- : Loading yaml from /tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/stemcell.MF\nI, [2014-04-04T00:07:31.078039 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Extracting stemcell to: /tmp/d20140404-5148-g9gyi/d20140404-5148-16blna2\nI, [2014-04-04T00:07:47.081156 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Generated name: sc-ec59204a-4417-402f-b3c9-453258cd54c3\nI, [2014-04-04T00:07:47.324374 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Deploying to: <[Vim.ClusterComputeResource] domain-c553> / <[Vim.Datastore] datastore-684>\nI, [2014-04-04T00:07:47.352863 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Importing VApp\nI, [2014-04-04T00:07:47.374115 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Waiting for NFC lease to become ready\nI, [2014-04-04T00:07:54.455074 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Uploading\nI, [2014-04-04T00:07:54.464649 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Uploading disk to: https://10.137.47.205/nfc/52fccc25-cd28-01f5-9c87-8b7528909ed8/disk-0.vmdk\nI, [2014-04-04T00:08:19.387111 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, )]  INFO -- : Removing NICs\nI, [2014-04-04T00:08:21.445650 #5148] [create_stemcell(/tmp/d20140404-5148-g9gyi/sc-20140404-5148-1g0us54/image, _)]  INFO -- : Taking initial snapshot\nI, [2014-04-04T00:08:23.652895 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Creating vm: vm-f856a212-e640-4ed4-85d3-2f7272084974 on <[Vim.ClusterComputeResource] domain-c553> stored in <[Vim.Datastore] datastore-625>\nI, [2014-04-04T00:08:23.661767 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-ec59204a-4417-402f-b3c9-453258cd54c3.\nI, [2014-04-04T00:08:23.664867 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Cluster doesn't have stemcell sc-ec59204a-4417-402f-b3c9-453258cd54c3, replicating\nI, [2014-04-04T00:08:23.667512 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Replicating sc-ec59204a-4417-402f-b3c9-453258cd54c3 (<[Vim.VirtualMachine] vm-732>) to sc-ec59204a-4417-402f-b3c9-453258cd54c3 %2f datastore-625\nI, [2014-04-04T00:10:07.200724 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Replicated sc-ec59204a-4417-402f-b3c9-453258cd54c3 (<[Vim.VirtualMachine] vm-732>) to sc-ec59204a-4417-402f-b3c9-453258cd54c3 %2f datastore-625 (<[Vim.VirtualMachine] vm-734>)\nI, [2014-04-04T00:10:07.200947 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Creating initial snapshot for linked clones on <[Vim.VirtualMachine] vm-734>\nI, [2014-04-04T00:10:13.287027 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Created initial snapshot for linked clones on <[Vim.VirtualMachine] vm-734>\nI, [2014-04-04T00:10:13.287240 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-734>\nI, [2014-04-04T00:10:13.321921 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Cloning vm: <[Vim.VirtualMachine] vm-734> to vm-f856a212-e640-4ed4-85d3-2f7272084974\nI, [2014-04-04T00:10:34.053021 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"},\n \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\",\n \"networks\"=>\n  {\"bosh\"=>\n    {\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"},\n     \"netmask\"=>\"255.255.254.0\",\n     \"gateway\"=>\"10.137.46.1\",\n     \"ip\"=>\"10.137.47.252\",\n     \"dns\"=>nil,\n     \"type\"=>nil,\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"mac\"=>\"00:50:56:84:a5:63\"}},\n \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}},\n \"ntp\"=>nil,\n \"blobstore\"=>\n  {\"provider\"=>\"local\",\n   \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}},\n \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\",\n \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}\nI, [2014-04-04T00:10:38.318520 #5148] [create_vm(bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5, ...)]  INFO -- : Powering on VM: <[Vim.VirtualMachine] vm-736> (vm-f856a212-e640-4ed4-85d3-2f7272084974)\n. Log in the VM created by bosh with \"root/c1oudc0w\",and run these commands :\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/bosh/bin# netstat -an |grep 6868   //the port 6868 is not  listening\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/bosh/bin# ./monit \n/var/vcap/bosh/etc/monitrc:8: Warning: include files not found '/var/vcap/monit/job/*.monitrc'\n/var/vcap/bosh/etc/monitrc:5: Error: cannot read htpasswd '/var/vcap/monit/monit.user'\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/bosh/bin# cd /var/vcap/monit\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/monit# ls\nempty.monitrc\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/monit# \nIs this caused the failure of micro deploy?And what can i do ?\n. There is something wrong with the log .\nroot@bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5:/var/vcap/bosh/log# vi current \n2014-04-04_15:52:44.81362 #[30353] INFO: failed to load infrastructure settings: No bosh cdrom env: #\n2014-04-04_15:52:44.83332 #[30353] INFO: loaded cached settings: {\"vm\"=>{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}\n2014-04-04_15:52:44.83392 #[30353] INFO: Loaded settings: #{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}, @cache_file=\"/var/vcap/bosh/settings.json\">\n2014-04-04_15:52:44.84459 /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:144:in update_time': undefined methodjoin' for nil:NilClass (NoMethodError)\n2014-04-04_15:52:44.84480   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:47:in configure'\n2014-04-04_15:52:44.84484   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:105:instart'\n2014-04-04_15:52:44.84487   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:84:in run'\n2014-04-04_15:52:44.84490   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/bin/bosh_agent:102:in'\n2014-04-04_15:52:44.84493   from /var/vcap/bosh/bin/bosh_agent:23:in load'\n2014-04-04_15:52:44.84495   from /var/vcap/bosh/bin/bosh_agent:23:in'\n2014-04-04_15:52:45.24884 #[30361] INFO: Starting agent 1.5.0.pre.3...\n2014-04-04_15:52:45.24890 #[30361] INFO: Configuring agent...\n2014-04-04_15:52:45.25053 #[30361] INFO: Configuring instance\n2014-04-04_15:52:45.25273 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:45.75576 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:46.25878 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:46.76296 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:47.26618 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:47.77330 #[30361] INFO: udevadm: #Bosh::Exec::Result:0x00000002d1bfa8\n2014-04-04_15:52:47.77605 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:48.27912 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:48.78217 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:49.28526 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:49.78839 #[30361] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:50.29155 #[30361] INFO: failed to load infrastructure settings: No bosh cdrom env: #\n2014-04-04_15:52:50.29203 #[30361] INFO: loaded cached settings: {\"vm\"=>{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}\n2014-04-04_15:52:50.29266 #[30361] INFO: Loaded settings: #{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}, @cache_file=\"/var/vcap/bosh/settings.json\">\n2014-04-04_15:52:50.30071 /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:144:in update_time': undefined methodjoin' for nil:NilClass (NoMethodError)\n2014-04-04_15:52:50.30083   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:47:in configure'\n2014-04-04_15:52:50.30085   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:105:instart'\n2014-04-04_15:52:50.30087   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:84:in run'\n2014-04-04_15:52:50.30089   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/bin/bosh_agent:102:in'\n2014-04-04_15:52:50.30090   from /var/vcap/bosh/bin/bosh_agent:23:in load'\n2014-04-04_15:52:50.30092   from /var/vcap/bosh/bin/bosh_agent:23:in'\n2014-04-04_15:52:50.70086 #[30382] INFO: Starting agent 1.5.0.pre.3...\n2014-04-04_15:52:50.70092 #[30382] INFO: Configuring agent...\n2014-04-04_15:52:50.70259 #[30382] INFO: Configuring instance\n2014-04-04_15:52:50.70479 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:51.20793 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:51.71098 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:52.21409 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:52.71713 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:53.22455 #[30382] INFO: udevadm: #Bosh::Exec::Result:0x00000002e6fdc8\n2014-04-04_15:52:53.22681 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:53.73011 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:54.23321 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:54.73636 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:55.23951 #[30382] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:55.74267 #[30382] INFO: failed to load infrastructure settings: No bosh cdrom env: #\n2014-04-04_15:52:55.76320 #[30382] INFO: loaded cached settings: {\"vm\"=>{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}\n2014-04-04_15:52:55.76381 #[30382] INFO: Loaded settings: #{\"name\"=>\"vm-f856a212-e640-4ed4-85d3-2f7272084974\", \"id\"=>\"vm-736\"}, \"agent_id\"=>\"bm-0ce7b9ae-9a21-4524-a14f-96cd81f979c5\", \"networks\"=>{\"bosh\"=>{\"cloud_properties\"=>{\"name\"=>\"VM-Fabric\"}, \"netmask\"=>\"255.255.254.0\", \"gateway\"=>\"10.137.46.1\", \"ip\"=>\"10.137.47.252\", \"dns\"=>nil, \"type\"=>nil, \"default\"=>[\"dns\", \"gateway\"], \"mac\"=>\"00:50:56:84:a5:63\"}}, \"disks\"=>{\"system\"=>0, \"ephemeral\"=>1, \"persistent\"=>{}}, \"ntp\"=>nil, \"blobstore\"=>{\"provider\"=>\"local\", \"options\"=>{\"blobstore_path\"=>\"/var/vcap/micro_bosh/data/cache\"}}, \"mbus\"=>\"https://vcap:b00tstrap@0.0.0.0:6868\", \"env\"=>{\"bosh\"=>{\"password\"=>nil}}}, @cache_file=\"/var/vcap/bosh/settings.json\">\n2014-04-04_15:52:55.77415 /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:144:in update_time': undefined methodjoin' for nil:NilClass (NoMethodError)\n2014-04-04_15:52:55.77437   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent/bootstrap.rb:47:in configure'\n2014-04-04_15:52:55.77441   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:105:instart'\n2014-04-04_15:52:55.77444   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/lib/bosh_agent.rb:84:in run'\n2014-04-04_15:52:55.77447   from /var/vcap/bosh/lib/ruby/gems/1.9.1/gems/bosh_agent-1.5.0.pre.3/bin/bosh_agent:102:in'\n2014-04-04_15:52:55.77450   from /var/vcap/bosh/bin/bosh_agent:23:in load'\n2014-04-04_15:52:55.77453   from /var/vcap/bosh/bin/bosh_agent:23:in'\n2014-04-04_15:52:56.18469 #[30390] INFO: Starting agent 1.5.0.pre.3...\n2014-04-04_15:52:56.18475 #[30390] INFO: Configuring agent...\n2014-04-04_15:52:56.18638 #[30390] INFO: Configuring instance\n2014-04-04_15:52:56.18861 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:56.69172 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:57.19474 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:57.69791 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:58.20103 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM): #\n2014-04-04_15:52:58.70843 #[30390] INFO: udevadm: #Bosh::Exec::Result:0x00000001fa6ad0\n2014-04-04_15:52:58.71119 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:59.21422 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:52:59.71753 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n2014-04-04_15:53:00.22073 #[30390] INFO: Waiting for /dev/sr0 (ENOMEDIUM or ENOTBLK)\n. Thanks a lot.\nThere is something wrong in micro_bosh.yml. \nin the original file :\ncloud:\n  plugin: vsphere\n  properties:\n    agent:\n    ntp:\nin the new file, add the value of NTP:\ncloud:\n  plugin: vsphere\n  properties:\n    agent:\n      ntp:\n       - 10.137.47.221\n. ",
    "singerdmx": "Per conversation with the BOSH team,  going to reissue pull request against develop\n. Hi,\nIt looks like travis:install_go spec:unit failed with the following error:\nFailure [1.487 seconds]\nawsDevicePathResolver\n(/home/travis/build/cloudfoundry/bosh/go_agent/src/bosh/infrastructure/devicepathresolver/aws_device_path_resolver_test.go:94)\nwhen no matching device is found the first time\n(/home/travis/build/cloudfoundry/bosh/go_agent/src/bosh/infrastructure/devicepathresolver/aws_device_path_resolver_test.go:85)\nwhen the timeout has not expired\n(/home/travis/build/cloudfoundry/bosh/go_agent/src/bosh/infrastructure/devicepathresolver/aws_device_path_resolver_test.go:76)\nreturns the match [It]\n(/home/travis/build/cloudfoundry/bosh/go_agent/src/bosh/infrastructure/devicepathresolver/aws_device_path_resolver_test.go:75)\nExpected error:\n<*errors.errorString>: &errors.errorString{s:\"Timed out getting real device path for /dev/sda\"}\nMessage: Timed out getting real device path for /dev/sda\nnot to have occurred\n/home/travis/build/cloudfoundry/bosh/go_agent/src/bosh/infrastructure/devicepathresolver/aws_device_path_resolver_test.go:73\n. Hi Rob,\nI am starting to work on go agent.\nI wonder if there is a dev that I can talk to in order to get some help with go. Currently I have some trouble updating submodule package \"github.com/onsi/ginkgo\".\nThanks!\nXin\n----- Original Message -----\nFrom: \"Rob Day-Reynolds\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Wednesday, May 14, 2014 11:08:05 AM\nSubject: Re: [bosh] Read agent settings from vmdk instead of cdrom iso to enable SRM on vsphere (#564)\n@bsiravara Just know that we have stopped adding functionality to the ruby agent, so it will be pretty much feature frozen (other than your pull request) going forward. \nOn the go agent, what kind of help would you be looking for? Would you want to come in and pair with folks here, or just regular communication back and forth? \ncc @adamstegman , who is taking over as BOSH anchor \n\u2014 \nReply to this email directly or view it on GitHub\n. Hi Greg,\nI am trying to test my go agent change, but I am unable to build stemcell when setting agent to go instead of ruby.\nThe command I ran is: bundle exec rake stemcell:build_with_local_os_image[vsphere,ubuntu,lucid,ruby,\"/vagrant/osimage/image.tgz\"]\nThe stemcell build is failing on 'aws_cli' stage. Error as follows:\n=== Configuring 'aws_cli' stage ===\n== Started Fri May 16 16:21:27 UTC 2014 ==\nsudo env  /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash 2>&1\nrake aborted!\nFailed: 'sudo env  /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash 2>&1' from /bosh, with exit status 512\nbash: warning: setlocale: LC_ALL: cannot change locale (en_US)\n++++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh\n+++ dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli\n+++ assets_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/../..\n+++ base_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib\n+++ lib_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton\n+++ skeleton_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton\n+++ settings_file=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ source /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash\n+++ bosh_users_password=c1oudc0w\n+++ stemcell_name=bosh-vsphere-esxi-ubuntu-lucid-go_agent\n+++ stemcell_tgz=bosh-stemcell-0000-vsphere-esxi-ubuntu-lucid-go_agent.tgz\n+++ stemcell_image_name=vsphere-esxi-ubuntu.raw\n+++ stemcell_version=0000\n+++ stemcell_hypervisor=esxi\n+++ stemcell_infrastructure=vsphere\n+++ stemcell_operating_system=ubuntu\n+++ stemcell_operating_system_version=lucid\n+++ bosh_protocol_version=1\n+++ ruby_bin=/usr/local/rbenv/versions/1.9.3-p448/bin/ruby\n+++ bosh_release_src_dir=/bosh/release/src/bosh\n+++ bosh_agent_src_dir=/bosh/bosh_agent\n+++ go_agent_src_dir=/bosh/go_agent\n+++ image_create_disk_size=3072\n+++ os_image_tgz=/vagrant/osimage/image.tgz\n+++ bosh_micro_enabled=yes\n+++ bosh_micro_package_compiler_path=/bosh/bosh-release\n+++ bosh_micro_manifest_yml_path=/bosh/release/micro/vsphere.yml\n+++ bosh_micro_release_tgz_path=/bosh/release/dev_releases/bosh-73.1-dev.tgz\n+++ UBUNTU_ISO=\n+++ UBUNTU_MIRROR=\n+++ image_ovftool_path=\n+++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ mkdir -p /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n++ rm -f /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ echo '# THIS FILE IS GENERATED; DO NOT EDIT OR COMMIT'\n- cd /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n- rm -rf s3cli\n- curl -L -o s3cli.tar.gz https://api.github.com/repos/pivotal/s3cli/tarball\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n  100    87  100    87    0     0     15      0  0:00:05  0:00:05 --:--:--   112\n- mkdir s3cli\n- tar -xzf s3cli.tar.gz -C s3cli/ --strip-components 1\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now/bosh/bosh-core/lib/bosh/core/shell.rb:43:in report'\n/bosh/bosh-core/lib/bosh/core/shell.rb:12:inrun'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:49:in run_sudo_with_command_env'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:24:inblock in configure'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:18:in each'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:18:inconfigure'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:13:in configure_and_apply'\n/bosh/bosh-stemcell/lib/bosh/stemcell/stemcell_builder.rb:16:inbuild'\n/bosh/bosh-dev/lib/bosh/dev/tasks/stemcell.rake:112:in `block (2 levels) in '\nTasks: TOP => stemcell:build_with_local_os_image\n(See full trace by running task with --trace)\nbash: warning: setlocale: LC_ALL: cannot change locale (en_US)\n++++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh\n+++ dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli\n+++ assets_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/../..\n+++ base_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib\n+++ lib_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib\n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton\n+++ skeleton_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton\n+++ settings_file=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ source /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash\n+++ bosh_users_password=c1oudc0w\n+++ stemcell_name=bosh-vsphere-esxi-ubuntu-lucid-go_agent\n+++ stemcell_tgz=bosh-stemcell-0000-vsphere-esxi-ubuntu-lucid-go_agent.tgz\n+++ stemcell_image_name=vsphere-esxi-ubuntu.raw\n+++ stemcell_version=0000\n+++ stemcell_hypervisor=esxi\n+++ stemcell_infrastructure=vsphere\n+++ stemcell_operating_system=ubuntu\n+++ stemcell_operating_system_version=lucid\n+++ bosh_protocol_version=1\n+++ ruby_bin=/usr/local/rbenv/versions/1.9.3-p448/bin/ruby\n+++ bosh_release_src_dir=/bosh/release/src/bosh\n+++ bosh_agent_src_dir=/bosh/bosh_agent\n+++ go_agent_src_dir=/bosh/go_agent\n+++ image_create_disk_size=3072\n+++ os_image_tgz=/vagrant/osimage/image.tgz\n+++ bosh_micro_enabled=yes\n+++ bosh_micro_package_compiler_path=/bosh/bosh-release\n+++ bosh_micro_manifest_yml_path=/bosh/release/micro/vsphere.yml\n+++ bosh_micro_release_tgz_path=/bosh/release/dev_releases/bosh-73.1-dev.tgz\n+++ UBUNTU_ISO=\n+++ UBUNTU_MIRROR=\n+++ image_ovftool_path=\n+++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ mkdir -p /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n++ rm -f /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash\n++ echo '# THIS FILE IS GENERATED; DO NOT EDIT OR COMMIT'\n- cd /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets\n- rm -rf s3cli\n- curl -L -o s3cli.tar.gz https://api.github.com/repos/pivotal/s3cli/tarball\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n  100    87  100    87    0     0     15      0  0:00:05  0:00:05 --:--:--   112\n- mkdir s3cli\n- tar -xzf s3cli.tar.gz -C s3cli/ --strip-components 1\ngzip: stdin: not in gzip format\ntar: Child returned status 1\ntar: Error is not recoverable: exiting now\nBuild step 'Execute shell' marked build as failure\nArchiving artifacts\nPerforming Post build task...\nMatch found for :marked build as failure : True\n----- Original Message -----\nFrom: \"Rob Day-Reynolds\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Wednesday, May 14, 2014 11:08:05 AM\nSubject: Re: [bosh] Read agent settings from vmdk instead of cdrom iso to enable SRM on vsphere (#564)\n@bsiravara Just know that we have stopped adding functionality to the ruby agent, so it will be pretty much feature frozen (other than your pull request) going forward. \nOn the go agent, what kind of help would you be looking for? Would you want to come in and pair with folks here, or just regular communication back and forth? \ncc @adamstegman , who is taking over as BOSH anchor \n\u2014 \nReply to this email directly or view it on GitHub\n. Correction hightlighted \n----- Original Message -----\nFrom: \"Xin Yao\" xyao@vmware.com \nTo: \"cloudfoundry/bosh\" reply@reply.github.com, \"greg oehmen\" greg.oehmen@gmail.com \nCc: \"Bharath Siravara\" bsiravara@vmware.com \nSent: Friday, May 16, 2014 10:52:16 AM \nSubject: Re: [bosh] Read agent settings from vmdk instead of cdrom iso to enable SRM on vsphere (#564) \nHi Greg, \nI am trying to test my go agent change, but I am unable to build stemcell when setting agent to go instead of ruby. \nThe command I ran is: bundle exec rake stemcell:build_with_local_os_image[vsphere,ubuntu,lucid, go ,\"/vagrant/osimage/image.tgz\"] \nThe stemcell build is failing on 'aws_cli' stage. Error as follows: \n=== Configuring 'aws_cli' stage === \n== Started Fri May 16 16:21:27 UTC 2014 == \nsudo env /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash 2>&1 \nrake aborted! \nFailed: 'sudo env /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash 2>&1' from /bosh, with exit status 512 \nbash: warning: setlocale: LC_ALL: cannot change locale (en_US) \n++++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh \n+++ dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli \n+++ assets_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/../.. \n+++ base_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib \n+++ lib_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton \n+++ skeleton_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton \n+++ settings_file=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ source /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash \n+++ bosh_users_password=c1oudc0w \n+++ stemcell_name=bosh-vsphere-esxi-ubuntu-lucid-go_agent \n+++ stemcell_tgz=bosh-stemcell-0000-vsphere-esxi-ubuntu-lucid-go_agent.tgz \n+++ stemcell_image_name=vsphere-esxi-ubuntu.raw \n+++ stemcell_version=0000 \n+++ stemcell_hypervisor=esxi \n+++ stemcell_infrastructure=vsphere \n+++ stemcell_operating_system=ubuntu \n+++ stemcell_operating_system_version=lucid \n+++ bosh_protocol_version=1 \n+++ ruby_bin=/usr/local/rbenv/versions/1.9.3-p448/bin/ruby \n+++ bosh_release_src_dir=/bosh/release/src/bosh \n+++ bosh_agent_src_dir=/bosh/bosh_agent \n+++ go_agent_src_dir=/bosh/go_agent \n+++ image_create_disk_size=3072 \n+++ os_image_tgz=/vagrant/osimage/image.tgz \n+++ bosh_micro_enabled=yes \n+++ bosh_micro_package_compiler_path=/bosh/bosh-release \n+++ bosh_micro_manifest_yml_path=/bosh/release/micro/vsphere.yml \n+++ bosh_micro_release_tgz_path=/bosh/release/dev_releases/bosh-73.1-dev.tgz \n+++ UBUNTU_ISO= \n+++ UBUNTU_MIRROR= \n+++ image_ovftool_path= \n+++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ mkdir -p /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n++ rm -f /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ echo '# THIS FILE IS GENERATED; DO NOT EDIT OR COMMIT' \n- cd /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n- rm -rf s3cli \n- curl -L -o s3cli.tar.gz https://api.github.com/repos/pivotal/s3cli/tarball \n  % Total % Received % Xferd Average Speed Time Time Time Current \n  Dload Upload Total Spent Left Speed \n0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0 \n0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0 \n100 87 100 87 0 0 15 0 0:00:05 0:00:05 --:--:-- 112 \n- mkdir s3cli \n- tar -xzf s3cli.tar.gz -C s3cli/ --strip-components 1 \ngzip: stdin: not in gzip format \ntar: Child returned status 1 \ntar: Error is not recoverable: exiting now/bosh/bosh-core/lib/bosh/core/shell.rb:43:in report' \n/bosh/bosh-core/lib/bosh/core/shell.rb:12:inrun' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:49:in run_sudo_with_command_env' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:24:inblock in configure' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:18:in each' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:18:inconfigure' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stage_runner.rb:13:in configure_and_apply' \n/bosh/bosh-stemcell/lib/bosh/stemcell/stemcell_builder.rb:16:inbuild' \n/bosh/bosh-dev/lib/bosh/dev/tasks/stemcell.rake:112:in `block (2 levels) in ' \nTasks: TOP => stemcell:build_with_local_os_image \n(See full trace by running task with --trace) \nbash: warning: setlocale: LC_ALL: cannot change locale (en_US) \n++++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/config.sh \n+++ dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli \n+++ assets_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/../.. \n+++ base_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib \n+++ lib_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/lib \n++++ readlink -nf /mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton \n+++ skeleton_dir=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/skeleton \n+++ settings_file=/mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ source /mnt/stemcells/vsphere/esxi/ubuntu/build/build/etc/settings.bash \n+++ bosh_users_password=c1oudc0w \n+++ stemcell_name=bosh-vsphere-esxi-ubuntu-lucid-go_agent \n+++ stemcell_tgz=bosh-stemcell-0000-vsphere-esxi-ubuntu-lucid-go_agent.tgz \n+++ stemcell_image_name=vsphere-esxi-ubuntu.raw \n+++ stemcell_version=0000 \n+++ stemcell_hypervisor=esxi \n+++ stemcell_infrastructure=vsphere \n+++ stemcell_operating_system=ubuntu \n+++ stemcell_operating_system_version=lucid \n+++ bosh_protocol_version=1 \n+++ ruby_bin=/usr/local/rbenv/versions/1.9.3-p448/bin/ruby \n+++ bosh_release_src_dir=/bosh/release/src/bosh \n+++ bosh_agent_src_dir=/bosh/bosh_agent \n+++ go_agent_src_dir=/bosh/go_agent \n+++ image_create_disk_size=3072 \n+++ os_image_tgz=/vagrant/osimage/image.tgz \n+++ bosh_micro_enabled=yes \n+++ bosh_micro_package_compiler_path=/bosh/bosh-release \n+++ bosh_micro_manifest_yml_path=/bosh/release/micro/vsphere.yml \n+++ bosh_micro_release_tgz_path=/bosh/release/dev_releases/bosh-73.1-dev.tgz \n+++ UBUNTU_ISO= \n+++ UBUNTU_MIRROR= \n+++ image_ovftool_path= \n+++ dirname /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ mkdir -p /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n++ rm -f /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets/settings.bash \n++ echo '# THIS FILE IS GENERATED; DO NOT EDIT OR COMMIT' \n- cd /mnt/stemcells/vsphere/esxi/ubuntu/build/build/stages/aws_cli/assets \n- rm -rf s3cli \n- curl -L -o s3cli.tar.gz https://api.github.com/repos/pivotal/s3cli/tarball \n  % Total % Received % Xferd Average Speed Time Time Time Current \n  Dload Upload Total Spent Left Speed \n0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0 \n0 0 0 0 0 0 0 0 --:--:-- 0:00:05 --:--:-- 0 \n100 87 100 87 0 0 15 0 0:00:05 0:00:05 --:--:-- 112 \n- mkdir s3cli \n- tar -xzf s3cli.tar.gz -C s3cli/ --strip-components 1 \ngzip: stdin: not in gzip format \ntar: Child returned status 1 \ntar: Error is not recoverable: exiting now \nBuild step 'Execute shell' marked build as failure \nArchiving artifacts \nPerforming Post build task... \nMatch found for :marked build as failure : True \n----- Original Message -----\nFrom: \"Rob Day-Reynolds\" notifications@github.com \nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com \nCc: \"Xin Yao\" xyao@vmware.com \nSent: Wednesday, May 14, 2014 11:08:05 AM \nSubject: Re: [bosh] Read agent settings from vmdk instead of cdrom iso to enable SRM on vsphere (#564) \n@bsiravara Just know that we have stopped adding functionality to the ruby agent, so it will be pretty much feature frozen (other than your pull request) going forward. \nOn the go agent, what kind of help would you be looking for? Would you want to come in and pair with folks here, or just regular communication back and forth? \ncc @adamstegman , who is taking over as BOSH anchor \n\u2014 \nReply to this email directly or view it on GitHub . \n. Thanks. It is fixed.\nBest Regards,\nXin Yao\n----- Original Message -----\nFrom: \"Adam Stegman \u269b\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Friday, May 16, 2014 11:26:08 AM\nSubject: Re: [bosh] Read agent settings from vmdk instead of cdrom iso to enable SRM on vsphere (#564)\n@singerdmx when you pull the latest bosh source, that will also fix the stemcell building issue you're having. That repository no longer exists, so we vendor the s3cli. \n\u2014 \nReply to this email directly or view it on GitHub\n. I am not sure what the test failure is about. Could I get some help, please? Thanks!\n. I opened new one https://github.com/cloudfoundry/bosh/pull/581\nIt failed immediately with the same errors.\nXin\n----- Original Message -----\nFrom: \"Adam Stegman \u269b\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Thursday, May 15, 2014 3:19:17 PM\nSubject: Re: [bosh] Enable SRM on vsphere by replacing the workflow of reading settings from cdrom with reading from attached independent disk (#580)\n@singerdmx Can you close this PR and reopen against the develop branch instead? \nThe test failures look unrelated to your change. We think they might be transient. When you reopen your PR they will run again and we can see what happens. \n\u2014 \nReply to this email directly or view it on GitHub\n. Yes,\nThere is the note in my pull request:\nNote:\nYou need to install qemu and vdiskmanager and make sure vdiskmanager is in your system path\nInstruction on how to install vdiskmanager:\nDownload vdiskmanager.tar from link https://github.com/vchs/bosh/blob/wip_remove_iso_f1/stemcell_builder/stages/system_vdiskmanager/assets/vdiskmanager.tar\nThen run the following commands:\ntar -xvf vdiskmanager.tar\nmodule_dir=ls -d /lib/modules/3.*-virtual | tail -1\ninstall_dir=\"${module_dir}/vdiskmanager\"\nperl vmware-vix-disklib-distrib/vmware-install.pl \"$install_dir\"\n----- Original Message -----\nFrom: \"Maria Shaldibina\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Tuesday, June 24, 2014 2:42:08 PM\nSubject: Re: [bosh] Enable SRM on vsphere by replacing the workflow of reading settings from cdrom with reading from attached independent disk (#581)\nhi @singerdmx , \nHave found the following error when running lifecycle_spec : 1) VSphereCloud::Cloud lifecycle without existing disks should exercise the vm lifecycle\n     Failure/Error: @vm_id = @cpi.create_vm(\n     RuntimeError:\n       Unable to find vmware-vdiskmanager in either /vdiskmanager/bin or system PATH\n     # ./lib/cloud/vsphere/agent_env.rb:111:in find_bin'\n     # ./lib/cloud/vsphere/agent_env.rb:183:inconvert_vmdk_to_esx_type'\n     # ./lib/cloud/vsphere/agent_env.rb:203:in set_vmdk_content'\n     # ./lib/cloud/vsphere/agent_env.rb:21:inset_env'\n     # ./lib/cloud/vsphere/vm_creator.rb:93:in create'\n     # ./lib/cloud/vsphere/cloud.rb:183:inblock in create_vm'\n     # /Users/pivotal/workspace/bosh/bosh_common/lib/common/thread_formatter.rb:46:in with_thread_name'\n     # ./lib/cloud/vsphere/cloud.rb:174:increate_vm'\n     # ./spec/integration/lifecycle_spec.rb:74:in vm_lifecycle'\n     # ./spec/integration/lifecycle_spec.rb:148:inblock (4 levels) in ' \nDo the tests assume that user have vmware-vdiskmanager installed locally? We are running these tests on MacOS and Linux environments. \n@phanle & @mariash \n\u2014 \nReply to this email directly or view it on GitHub\n. These two methods look similar but if you take a close look, it's different inside the each  loop. Hence I don't think reusing which function is feasible.\n. OK, will update it. Thanks!\n. Are you saying convert_iso_to_vmdk should return File.join(tmp_dir, 'env_source.vmdk') and convert_vmdk_to_esx_typ should return File.join(tmp_dir, 'env.vmdk') ?\n. Kind of. So are you suggesting something as follows:\n```\ndef find_bin(bin_path, bin)\n  exe = File.join(bin_path, bin)\n  return exe if File.exists?(exe)\n  program = which([bin])\n  return program unless program == bin\nfail \"Unable to find #{bin} in either #{bin_path} or system PATH\"\nend\n```\n. Ok, will update it.\n. OK, will delete it.\n. OK, will update it.\n. This seems not to be my change. Because I added an overall context and every line gets pushed two spaces right. And I am not sure what you are suggesting. Could we ignore the code here since I did not touch it and you guys do not care about ruby agent that much (I assume you care about go), please? Otherwise please clarify what needs to be changed. Thanks!\n. I see. Will change them\n. Hi Adam,\nAll comments are addressed and the pull request is refreshed with two more changes. Please let us know if anything is needed.\nThanks!\nBest Regards,\nXin Yao\n----- Original Message -----\nFrom: \"Adam Stegman \u269b\" notifications@github.com\nTo: \"cloudfoundry/bosh\" bosh@noreply.github.com\nCc: \"Xin Yao\" xyao@vmware.com\nSent: Thursday, May 29, 2014 1:36:52 PM\nSubject: Re: [bosh] Enable SRM on vsphere by replacing the workflow of reading settings from cdrom with reading from attached independent disk (#581)\nIn bosh_agent/spec/unit/infrastructure/vsphere/settings_spec.rb: >  \n\n\nFileUtils.mkdir_p(cdrom_dir)\nFile.open(env, 'w') { |f| f.write(settings_json) }\nit \"should fail when there is no cdrom\" do\nsettings.stub(:read_cdrom_byte).and_raise(Errno::ENOMEDIUM)\nlambda {\nFile.should_receive(:read).with('/proc/sys/dev/cdrom/info').and_return(@proc_contents)\nsettings.send(:check_cdrom) \n\n\nYou switched this method from public to private. I'm suggesting that you test via the public interface rather than doing private-method-specific tests. \n\u2014 \nReply to this email directly or view it on GitHub\n. Hi Caleb,\nI don't fully understand the scenario. But when one deletes a VM through CPI, the associated folder in the datastore is deleted, including env.json and the vmdk file. I am unsure about the migration issue though.\nBest Regards,\nXin Yao\nFrom: caleb miles notifications@github.com<mailto:notifications@github.com>\nReply-To: cloudfoundry/bosh reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Tuesday, July 8, 2014 3:11 PM\nTo: cloudfoundry/bosh bosh@noreply.github.com<mailto:bosh@noreply.github.com>\nCc: Xin Yao xyao@vmware.com<mailto:xyao@vmware.com>\nSubject: Re: [bosh] Enable SRM on vsphere by replacing the workflow of reading settings from cdrom with reading from attached independent disk (#581)\nIn bosh_vsphere_cpi/lib/cloud/vsphere/agent_env.rb:\n\n+\n-      module_dir = (ls -d /lib/modules/3.*-virtual | tail -1).strip\n-      vdiskmanager = find_bin(\"#{module_dir}/vdiskmanager/bin\", 'vmware-vdiskmanager')\n-      output = #{vdiskmanager} -r #{env_source_vmdk} -t 4 #{target_vmdk_file} 2>&1\n-      fail \"#{$?.exitstatus} -#{output}\" if $?.exitstatus != 0\n-      target_files\n-    end\n  +\n-    def set_vmdk_content(vm, location, env)\n-      @logger.info('Setting env from vmdk')\n-      env_json = JSON.dump(env)\n  +\n-      vmdk_path = \"[#{location[:datastore]}] #{location[:vm]}/env.vmdk\"\n-      @cpi.detach_independent_disk(vm, vmdk_path, location)\n  +\n-      @file_provider.upload_file(location[:datacenter],\n\nWhen vm will be migrated to another datastore env.json won't be migrated. In case of ISO image we can determine location of env.json based on cdrom information about ISO since ISO is not being migrated with vm. In the case that a CDROM drive was never attached to a VM we will have no way of determining where the env.iso or env.json files are located. This will break get_current_env method and will also leave old env.json around after VM is deleted. Do you think there is a way you can determine env.json location after vm was migrated?\n\nReply to this email directly or view it on GitHubhttps://urldefense.proofpoint.com/v1/url?u=https://github.com/cloudfoundry/bosh/pull/581/files%23r14683217&k=oIvRg1%2BdGAgOoM1BIlLLqw%3D%3D%0A&r=L3LUbYIHReE4mdemHaEAVA%3D%3D%0A&m=ToRxE%2FVC9v9yOgJ4hoKtGx27KgaSmd6j9nmM1eTGJbY%3D%0A&s=10f4a06848911e3d34ae073dfeffc8923fe9b78b7b7b29fb6ba94041a40cb7c0.\n. ",
    "molteanu": "Will reopen against dev branch.\n. Thanks. Sorry for missing the spec\nFrom: Maria Shaldibina notifications@github.com<mailto:notifications@github.com>\nReply-To: cloudfoundry/bosh reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Friday, August 8, 2014 2:01 PM\nTo: cloudfoundry/bosh bosh@noreply.github.com<mailto:bosh@noreply.github.com>\nCc: \"VMware Inc.\" molteanu@vmware.com<mailto:molteanu@vmware.com>\nSubject: Re: [bosh] stemcell_builder: set infrastructure to vsphere for vcloud stemcell (#629)\nClosed #629https://urldefense.proofpoint.com/v1/url?u=https://github.com/cloudfoundry/bosh/pull/629&k=oIvRg1%2BdGAgOoM1BIlLLqw%3D%3D%0A&r=ibkhKUQwO4ZlGCaWWNJYFqI455ez6c6%2FHcnvOKY7UBQ%3D%0A&m=hK2dS6%2Bf60fBP%2BKo1P01kzskySxz4KJE1bqDbl3rTAE%3D%0A&s=669912664488a68807e30fe6a6c77e7a7050bc5d85aba73ec86668a9b8389bb9.\n\nReply to this email directly or view it on GitHubhttps://urldefense.proofpoint.com/v1/url?u=https://github.com/cloudfoundry/bosh/pull/629%23event-150939761&k=oIvRg1%2BdGAgOoM1BIlLLqw%3D%3D%0A&r=ibkhKUQwO4ZlGCaWWNJYFqI455ez6c6%2FHcnvOKY7UBQ%3D%0A&m=hK2dS6%2Bf60fBP%2BKo1P01kzskySxz4KJE1bqDbl3rTAE%3D%0A&s=e05641c7095356ea8d7ead51c648507c872ec1e1d8c7922d547d39013e4a5514.\n. @adamstegman I don't thin the current behavior is a regression in the vCloud stemcell. When the ruby agent was used the behavior was the same, the difference was that these tests were disabled. This change is meant to unblock the stemcell publishing pipeline while we get to the bottom of what is going on.\nSo i would rather not change the CPI until we looked at the issue in more detail. The goal would be to just make this work.\n. the let \"infrastructure_name\" is needed since it is used by another let at a higher level: line 13 and AWS and OpenStack use it to set the infrastructure name appropriately.\n. Sadly there is no \"vapp_exists?\" in the SDK.\n. Will do.\n. Fixing indentation. Will leave the break the way it is to stay consistent rest of the file.\nI also think breaking on the before the .to makes it easier to read the code.\n. Fixing.\n. Will do.\n. ",
    "chou": "@goehmen, do you want this in your backlog?\nCF Community Pair (@khwang1 & @chou)\n. @goehmen, can your team address this issue?\nCF Community Pair (@khwang1 & @chou)\n. @goehmen, would the BOSH team like to take a look at this?\nCF Community Pair (@khwang1 & @chou)\n. @goehmen, can your team comment on this issue?\nCF Community Pair (@khwang1 & @chou)\n. Hi @molteanu,\nWe are investigating this PR.\nWe have noticed that the dreddbot is asking you to file a CLA in each PR you submit.  Since your company has signed a CLA with us, we will email you with instructions in order to suppress these CLA requests.\nCF Community Pair (@chou & @khwang1)\n. Hi @duglin,\nWe'll pass your comments along to the BOSH docs team. Thanks for the feedback!\nCF Community Pair (@khwang1 & @chou)\n. Hi @duglin,\nThe docs team has actually put bosh help --all in the Glossary, in the IaaS Setup page, and in the Deploying MicroBOSH page. Based on your suggestion, they will put it in the landing page as well.\nCF Community Pair (@khwang1 & @chou)\n. ",
    "omarreiss": "@cppforlife @adamstegman Thanks for reminding me! I have been so busy finishing a project at Innovation Factory that I had totally forgotten about this PR. I will add specs and 'quote-ify' the other properties.\n. I stringified all the options except for the connection_options and the default_security_groups, since they can be something else then a string (hash or array).\nI also added a spec.\n. @adamstegman Thanks for letting me know. Would you like me to create a PR on that branch instead?\n. ",
    "khwang1": "@goehmen this seems reasonable, would you like to review this PR?\nCF Community Pair (@khwang1 & @chou)\n. @goehmen this seems reasonable, would you like to review this PR?\nCF Community Pair (@khwang1 & @chou)\n. ",
    "dims": "normalize_url is causing this problem (https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/base_command.rb#L209). to_s method does not output \":443\" since 443 is the default port for SSL (see https://github.com/ruby/ruby/blob/trunk/lib/uri/generic.rb#L1478)\nirb(main):044:0* require 'uri'\n=> false\nirb(main):045:0> url = 'https://localhost:8443'\n=> \"https://localhost:8443\"\nirb(main):046:0> uri = URI.parse(url)\n=> #\nirb(main):047:0> print uri.to_s\nhttps://localhost:8443=> nil\nirb(main):048:0> url = 'https://localhost:443'\n=> \"https://localhost:443\"\nirb(main):049:0> uri = URI.parse(url)\n=> #\nirb(main):050:0> print uri.to_s\nhttps://localhost=> nil\nIf we fix/special case this, this scenario should work just fine.\n. Here's a pull request that works for me : https://github.com/cloudfoundry/bosh/pull/588\n. @maximilien - travelling this week. will try to get this done soon.\n. @krishicks && @maximilien - My apologies for not respinning quickly. Yes, this sounds awesome! \n. ",
    "akranga": "I will put my 5 cents. First of all see impl for :https://github.com/cloudfoundry/bosh/pull/575 \nThere is possibility to request a public IP address for eth0 device in VPC 2.0. \n- if network is manual\n- if elastic IP is not assigned\n- if not overridden by props in deployment manifest\nThis will make micro_bosh deployment behave exactly as deployment to \"default\" network. See here: http://docs.aws.amazon.com/sdkfornet1/latest/apidocs/html/P_Amazon_EC2_Model_InstanceNetworkInterfaceSpecification_AssociatePublicIpAddress.htm\n. A side question. Why micro_bosh needs a public IP in case of VPC deployment? I believe micro_bosh must be reachable by other VMs within VPC. Is there any other limitation?\n. @adamstegman & @mbhave  thanks for commenting. \nI understand that EIP is the easiest way to implement to make Bosh available for the user. However my concern is exactly a requirement to use \"elastic ip\". For our deployments of CF we want to spare as much as possible elastic-ips for the user. \nI would suggest to the following: \n- implement what @drnic suggests (this will make: https://www.pivotaltracker.com/s/projects/956238 not to fail), \n- #575 (PR already submitted). \n- or both\nIf first is preferable then I can create PR shortly.\n. Well, my initial problem have been described  #573 \nYou start the deployment but not specify any public IP. Then it will fail in VPC deployment (but work in \"default\" sub-network, because it allocates public ip). As the outcome there are two choices:\n- make deployment possible without any form of public ip (as @drnic suggest)\n- or make bosh deployment consistent inside or outside of VPC. \nWithout knowing proper reasoning behind current bosh deployment logic I was assuming 2nd will be less risky change. So, as result #575 will also fix the problem because micro_bosh instance will have a public IP address.\n. Yeah looks the same. Submitted pull request: https://github.com/cloudfoundry/bosh/pull/574\n. Thanks... working on it\n. I belive it is better to send a completely new PR (clean and fresh) and just \"close\" this one\n. Rebased against develop branch\n. Please advice what is going next with current PR? Hope it will not be frozen until end of life.\n. ",
    "stupakov": "Closing the Pivotal Tracker story that cf-gitbot just created (https://www.pivotaltracker.com/story/show/71180904).\nThe story that contains progress for the issue is the one originally assigned by gitbot:\nhttp://www.pivotaltracker.com/story/show/69198726\n. Closing the Pivotal Tracker story that cf-gitbot just created (https://www.pivotaltracker.com/story/show/71180904).\nThe story that contains progress for the issue is the one originally assigned by gitbot:\nhttp://www.pivotaltracker.com/story/show/69198726\n. @goehmen Could you please address this when you have a chance?\nThanks,\nCF Community Pair (@d, @stupakov)\n. @goehmen Could you please address this when you have a chance?\nThanks,\nCF Community Pair (@d, @stupakov)\n. @goehmen @adamstegman Can we move the corresponding Tracker story to your team's icebox?\nThanks,\nCF Community Pair (@d, @stupakov)\n. @goehmen @adamstegman Can we move the corresponding Tracker story to your team's icebox?\nThanks,\nCF Community Pair (@d, @stupakov)\n. @goehmen @monkeyherder Should we move this to your icebox?\nThanks,\nCF Community Pair (@stupakov, @mbhave)\n. @goehmen @monkeyherder Should we move this to your icebox?\nThanks,\nCF Community Pair (@stupakov, @mbhave)\n. @goehmen We moved this to the BOSH tracker.\nThanks,\nCF Community Pair (@stupakov, @mbhave)\n. @goehmen We moved this to the BOSH tracker.\nThanks,\nCF Community Pair (@stupakov, @mbhave)\n. @goehmen Moving this story to the Bosh team's Tracker Project.\nThanks,\nCF Community Pair (@d, @stupakov)\n. @goehmen Moving this story to the Bosh team's Tracker Project.\nThanks,\nCF Community Pair (@d, @stupakov)\n. ",
    "aristotelesneto": "I've modified PowerDNS conf directly, injecting a zone+records into the db, and set it up to do zone transfers and it all works, but figure I ask if there was a compelling reason for bosh to be performing that before poking around the source code as my Ruby isn't that great.\n. I've modified PowerDNS conf directly, injecting a zone+records into the db, and set it up to do zone transfers and it all works, but figure I ask if there was a compelling reason for bosh to be performing that before poking around the source code as my Ruby isn't that great.\n. If we really want to keep the if statement, either need to change to /bin/bash, or change the comparison operator to simply '='.\n. Apologies, it appears that I was meant to be looking at the develop branch for the most up to date code base, and commit a6c52754cc2b4c8478582ddb0f341e331b14044f has already made it there, removing the ifdown/ifup statements altogether.\n. Doesn't I'm afraid - fails here: https://github.com/cloudfoundry/bosh/blob/93a2522b3a6a58cc3d24f6ea10a9dba0a9a421bb/bosh-registry/lib/bosh/registry/instance_manager/aws.rb#L67\n. Sorry wrong file (i just did a quick search on github for the env_of_profile and picked the first error) - fails with a similar error. IIRC it was on the blobstore, so here: https://github.com/cloudfoundry/bosh/blob/master/blobstore_client/lib/blobstore_client/s3_blobstore_client.rb#L172\n. Sorry, my bad.\nLooks like it uses a property outside compiled_package_cache block, which was unexpected:\nhttps://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/director.yml.erb.erb#L216\n. ",
    "duglin": "I agree with Dr Nic.  However, can we at least get something on the main page that says something like:\n  Use \"bosh help --all\" to see the full list of BOSH commands.\nEven that, which to be honest I didn't know about, would have helped me a ton.\n. Looks good - thanks\n. While I don't get a 404 for that link, I only see the title, no content.  Do I need more ACLs?\nMy Pivotal ID is \"dougdavis1\".\n. ok my bad, I see now that the \"epic\" is supposed to be empty, I didn't notice the link on the right to the stories under it.\nFrom the list, it appears that 62085266 might be one that's closest to what I'm looking for, but it seems to only focus on \"final\" releases.  And, I need to apologize first, but its been a while since I put in this hack to tweak the release name and so my memory of dev vs final releases is probably a bit blurred.  But, I seem to recall that \"final\" releases of CF should really only be done by Pivotal. And that would then leave the rest of the community with just \"dev\" releases.  I seem to recall this having to do with version numbering and such. \nHowever, perhaps with 62085266 we can pretty much ignore that rule and simply  give our final release any version we want, even one that is totally different than the CF version we're basing it on. Is that the intent?  If so, that's good, however, it would then be nice to also allow us to set the name of the release.\nLet me give some details of our naming scheme/process:\n- we grab cf v170 from github/cf-release\n- we add our tweaks and extensions to the src\n- bosh create release --with-tarball --force\n- we rename the release to be:  ibm-v170.x where \"x\" is our Xth version of 170 that we're building/deloying\nSo, I think the --version flag in 62085266 will cover the 170.x part, but not sure how we'd get the ibm-v part.\nI know this is probably off topic, but let me ask, it seems to me that BOSH is pushing people towards a particular build model.  By that I mean it has knowledge of previous builds and version numbers (as mentioned in 62085266 when it talks about comparing version numbers numerically) and tries to use that particular build patten in its processing, and I can understand that to some extent.  However, that makes it harder for folks (like us) who do not necessarily have those same requirements.  For example, every build we do is basically 100% independent of any previous build so the version number, or even the entire version string, is just a string to us.  We don't really care about the split between dev build and final build, they're all just builds and whether its a 'dev' or 'final' is really more dictated by what we choose to do with it.  For example, a 'dev' build might end up being a 'final' build if it passes all of our test. And it would be up to our CI/CD pipeline to decide when to increment the major version number instead of the minor one.  So, I guess my question is whether you guys have considered modifying things such that there's just the option of just having a \"build\" w/o preset notion of dev vs final?  Or is that totally weird given how things are designed?\n. hmm, are you able to give it a name on the command line so it can be automated and can I give it a version number - like 171.1 ?\n. I think Adam's comment:\n\nRegarding auto-incrementing of the \"minor\" version, I think that's a mistake. Bosh can't enforce semantic versioning because it can only guess at the semantics of the release. I would prefer the release developer always specifies a version number but in the absence of that I'd rather not have bosh apply semantics when it doesn't know how.\n\ncomes closest to my thinking on this. \nAnd, I'm sure I'm just being dense here, but I'm still confused as to why there is a distinction between the various types of releases instead of just \"build it\" and let the release manager decide what to call it and how to use it (dev vs final vs hotfix vs just-because).  If there are functional differences (meaning, certain actions are taken during a \"final\" build but not during a \"dev\" build) then why not just expose those as flags (--do-xxx) and let the release manager decide when to invoke those steps.  I'm not sure why those actions have to be tightly linked with a version string.  Just feels like BOSH is trying to be \"too smart\" and I'd prefer a \"dumber\" make-like feature that I can script around based on my CI/CD process needs rather than trying to make my process fit into BOSH's notion of how I should work. Or at least, make those \"smarts\" optional so I can do my own \"smarts\" when needed and BOSH doesn't get in my way, but rather just does what I tell it to do.\n. Well, let's go back to my original scenario, I pick up cf-release v172, make some modifications to the code in some of its submodules and then want to build a \"ibm-v172.1\" tarball of cf-release.\nI'm doing this using bosh-lite, so maybe that's an issue but \"bosh create release\" 's help text doesn't seem to mention passing in a version name/number.  So, what's the command line I put into my automation scripts to generate a tarball where in the tarball's release.MF file I have \"ibm-v172.1\" for the top-level \"version\" string and the phrase \"ibm-v172.1\" in the \"version\" string for the rebuilt jobs - e.g. hm9000 has a \"version\" string of \"15-ibm-v172.1\"?  It's been a while since I wrote my script, but I think I needed to modify the version string of changed submodules otherwise BOSH would complain about a mismatch.\n. I'm not sure I really care too much about the parsing side of things since each new build is totally independent in my case - eg. brand new cf-release dir, no history in the dev_releases dir.  The exception to that is the blobstore (I think) since, as I mentioned I needed to tweak the version string of the changed submodules to avoid a bosh conflict error message.\nSo, in the end, I'm not clear on how to get what I'm looking for.  What is the bosh command line I would issue to build what I described above? (tarball with the correct ibm-.... release name) Or is it not possible?  \nAlso, my \"bosh create release --help\" doesn't seem to mention a \"--version\" flag, I'm on BOSH 1.2200.0 via bosh-lite, do I need to wait for bosh-lite to catch-up?\n. When I do a bosh release I see this:\n+---------------------+-------------+-------------+\n| Name                | Versions    | Commit Hash |\n+---------------------+-------------+-------------+\n| admin-ui            | ibm-v172.1* | 2f111e65+   |\n| cf                  | ibm-v172.1* | 9b05799a+   |\n| cf-services         | ibm-v172.1* | 746d1f0c+   |\n| cf-services-contrib | ibm-v172.1* | da3925e4+   |\n+---------------------+-------------+-------------+\nand the release.MF in cf-release.tgz has:\nname: cf\nversion: ibm-v172.1\nwhich I think means I want to tweak the version string not the release name, right?  Its important to me to keep the original release names (e.g. \"cf\") so that people who see our setup can easily map it to other CF installs. It also makes it easier if I pick-up a vanilla cf-release.tgz because the release name will still be \"cf\" and its less changes in my BOSH manifest if I want to easily move between CF's 172 and the ibm-v172.1.  To me, that's all the same \"release\" just different versions of it.\n. Just so you have the complete picture, here's the snippet of bash we use to fix-up our release tgz,after we create the release, to make sure it has the version string we want:\n  gzip -f -d $project_name.tgz\n  tar -xvf $project_name.tar ./release.MF\n  sed -i \"s/.[0-9]-dev/-$RELEASE_VERSION/g\" release.MF\n  sed -i \"s/^name:./name: $release_name/\" release.MF\n  sed -i \"s/^version:.*/version: $RELEASE_VERSION/\" release.MF\n  tar -rf $project_name.tar ./release.MF\n  gzip $project_name.tar\n  rm release.MF\n  mv $project_name.tar.gz $project_name.tgz\nwhere $project_name is \"cf-release\", $release_name=\"cf\", $RELEASE_VERSION is \"ibm-v172.1\".\nWe would have done this before we created the release tgz but it wasn't clear, at that time, how to stop \"bosh create release\" from doing things like using \"-dev\" in the version string. So, it was just easier to fix it up afterwards.\nAs for not wanting to name it \"cf-ibm\".... to be honest, at the time I never even considered it.\nProbably because the help text of \"bosh create release\" never mentioned being able to specify a new name, but more because I don't consider it to be a different release, its a different version of the same release.  The same way a dev build is just a new version of the same release, and bosh doesn't force those to be named \"cf-dev\".\nBut I still go back to the same question... why does bosh care what I call it? What is the reason for preventing someone from naming a release whatever they want?\n. Nope, didn't try that, is it an undocumented CLI option?\n\nbosh create release --help\nUsage: bosh [options]\n        --force                      bypass git dirty state check\n        --final                      create final release\n        --with-tarball               create release tarball\n        --dry-run                    stop before writing release manifest\n\nUsing BOSH 1.2334.0 via bosh-lite.\nWith such an option, is there a generic algorithm I could use to generate a modified input manifest?  From my limited knowledge of bosh it seems like there is no requirement for a manifest file to be present prior to the \"bosh create release\", so I don't always have something to copy and modify with a new version string.  I do see one for cf-release but for other releases that we build I don't see one. And I'm not sure it makes sense to require an entire manifest file just to specify a version string. Also, isn't this usually a generated file rather than a hand-crafted one?\nFor example, we have an admin-ui-release that gets built and it ends up initially with a version string of 0.1-dev, I assume because its the first bosh build and there's no existing manifest file to tell bosh otherwise. And this works ok for us because we then blindly modify the release.MF in the resulting tgz to have an IBM-specific version string.\n. that help text discrepancy is kind of funky  :-)   gotta wonder why there are two ways of doing (almost) the same thing.\n. great! thanks!\nHow does bosh-lite pickup features like this?\n. ",
    "sparameswaran": "A mind-map sort of picture in public docs would help users follow the\nvarious related commands (like initial setup, deployments,\nmonitoring, troubleshooting etc. as its hard to remember or relate various\noptions available within Bosh. I would suggest similar mind-map for CF cli\nalso.\nthanks,\nSabha\nOn Fri, Apr 11, 2014 at 10:31 AM, rboshman notifications@github.com wrote:\n\nHi Doug:\nBecause commands via the CLI are dynamic over time, we have replaced an\nactual listing of commands in documentation with bosh help --all\nreferences so that documentation does not get stale and/or increasingly\ndifficult to maintain. We also recently finished a story (\nhttps://www.pivotaltracker.com/n/projects/956238/stories/67136620) to\nconverge these three commands:\nbosh --help\nbosh help\nbosh help --all\nSo that there is a single output that is useful. After completing that\nstory and reviewing the results, I think that we actually need either an\noption with less verbosity or the ability to page through the output (like\npaging through man pages by hitting space bar and killing out of paging\nwith q). I've actually created a story for the latter option.\nBut ultimately, there is only value in doing this type of work if it\ntotally serves the customer in a focussed way. So, what do you think we\nshould do?\nBest,\nGreg\nGreg Oehmen\nCloud Foundry Product Manager - Bosh\nPivotal\nOn Fri, Apr 11, 2014 at 10:14 AM, cf-gitbot <notifications@github.com\n\nwrote:\nWe have created an issue in Pivotal Tracker to manage this. You can view\nthe current status of your issue at:\nhttp://www.pivotaltracker.com/story/show/69357758. This repo is managed\nby the 'BOSH' team.\n\nReply to this email directly or view it on GitHub<\nhttps://github.com/cloudfoundry/bosh/issues/563#issuecomment-40227096>\n.\n\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/563#issuecomment-40228806\n.\n\n\nSabha Parameswaran | Platform Engineer, Cloud Foundry | Pivotal\n415-244-5554 | Skype: sabha_mp | sparameswaran@goPivotal.com\nhttp://gopivotal.com/\ngoPivotal.com http://gopivotal.com/\n. ",
    "bsiravara": "@goehmen per the discussion last week we decided to go with an alternate solution right? This PR uses the canned VMDK file and we  want to use the vdiskmanager solution instead. A PR for that change is on its way in the next few days.\n. @monkeyherder we'd like to get the ruby changes in and then do a separate PR for the go agent. Also any chance we can get some help from you guys for the go agent changes?\n. ",
    "zoujin": "Thank you.\n. @jfoley @cfdreddbot \nThank you. I see.\n. ",
    "metadave": "no problem, I'll resubmit against develop.\nCheers-\nDave\n. no problem, I'll resubmit against develop.\nCheers-\nDave\n. ",
    "maxbrunsfeld": "Hi Doug,\nAh you cannot specify a name (say with a flag) without hacking dev.yml. @goehmen Do we have such a feature planned?\nThanks,\nCF Community Pair\nJesse and Max (@d, @maxbrunsfeld)\n. Hi Doug,\nAh you cannot specify a name (say with a flag) without hacking dev.yml. @goehmen Do we have such a feature planned?\nThanks,\nCF Community Pair\nJesse and Max (@d, @maxbrunsfeld)\n. Hi @duglin,\nAs to your question regarding the differences between dev and final releases, a final release currently requires access to some blobstore and a dev release can be rebased by the director such that the version of a release will be changed at upload to not conflict with previously uploaded releases. \nTo the question about who should make final releases, we were hoping that the epic would encompass enough possible workflows to enable people other than Pivotal to create final releases such that you at IBM could take cf-release v100 a final release with an arbitrary version. If when checking out the original cf-release from GitHub you should be able to change your blobstore configuration and create your own final releases with IBM bits. You may want to lobby @goehmen for the relaxation of the blobstore requirement when creating final releases by providing a --without-blobstore option or similar. \nRegarding the auto-incrementation of the release version, you may have a point that BOSH is trying to be 'too smart' in order to add a little convenience, and perhaps it may make sense to deprecate the creation of final releases without specifying a version explicitly but given the magnitude of such a change it would certainly require a long sunset window if @goehmen, was interested in the feature. \nWith the introduction of user-specified semantic versioning in BOSH, we have come a long way in removing the specialness of final releases, such that dev is essentially now a builtin post release version specifier. We would welcome any additional input you have on the issue as additional stories are completed. Are there any specific additional features beyond the stories which currently exist that you require in order to close this issue?\nThanks,\nCF Community Pair\nCaleb and Max (@calebamiles, @maxbrunsfeld)\n. Hi @duglin,\nAs to your question regarding the differences between dev and final releases, a final release currently requires access to some blobstore and a dev release can be rebased by the director such that the version of a release will be changed at upload to not conflict with previously uploaded releases. \nTo the question about who should make final releases, we were hoping that the epic would encompass enough possible workflows to enable people other than Pivotal to create final releases such that you at IBM could take cf-release v100 a final release with an arbitrary version. If when checking out the original cf-release from GitHub you should be able to change your blobstore configuration and create your own final releases with IBM bits. You may want to lobby @goehmen for the relaxation of the blobstore requirement when creating final releases by providing a --without-blobstore option or similar. \nRegarding the auto-incrementation of the release version, you may have a point that BOSH is trying to be 'too smart' in order to add a little convenience, and perhaps it may make sense to deprecate the creation of final releases without specifying a version explicitly but given the magnitude of such a change it would certainly require a long sunset window if @goehmen, was interested in the feature. \nWith the introduction of user-specified semantic versioning in BOSH, we have come a long way in removing the specialness of final releases, such that dev is essentially now a builtin post release version specifier. We would welcome any additional input you have on the issue as additional stories are completed. Are there any specific additional features beyond the stories which currently exist that you require in order to close this issue?\nThanks,\nCF Community Pair\nCaleb and Max (@calebamiles, @maxbrunsfeld)\n. ",
    "peterellisjones": "We tried that with the cloud controller (see here), but it didn't seem to be respecting the set limits \u2014 ie, we were still getting errors when trying to create threads.\n. postgres should support this too (https://www.postgresql.org/docs/9.6/static/libpq-connect.html) using the options sslcert, sslkey, and sslrootcert (for mysql this last one is sslca). Unfortunately although the director spec allows passing in arbitrary connection_options to Sequel, the ssl keys all need to be paths to files rather than the contents of files. It would be nice to have a translation step that detected sslcert/sslkey/sslrootcert/sslca in the connection_options hash, wrote the values to a file, and replaced the values with their filepaths.\nNote that this feature is currently blocking anyone from using Google Cloud SQL as a Bosh Director DB using SSL. > @peterellisjones , btw have you tried connecting to a postgres instance over TLS? In our development we needed to bump the sequel gem version to be able to connect.\n@pivotal-jamil-shamy I'm afraid I didn't try it with postgres, just MySQL\n. hi @pivotal-jamil-shamy,\nI closed this since it sounds like you're already working on it and it looks like it's an inconsistent implementation with how you've done other certificates. Is there a public tracker where I can follow the progress of the Bosh team's implementation of this? (I couldn't find a story in https://www.pivotaltracker.com/n/projects/956238)\nthanks,\nPete. ",
    "gerhard": "Don't have access to the Bosh tracker, please add me if you want to move the discussion there. I'm gerhardlazu3.\n. I think @peterellisjones is working on a project where this might still be relevant, I no longer have the context unfortunately.\n. I think @peterellisjones is working on a project where this might still be relevant, I no longer have the context unfortunately.\n. OK, no worries, will close this.\n. OK, no worries, will close this.\n. ## What happened?\nJust ran into this today during bosh deploy:\n11:31:51 | Error: PG::Error: ERROR:  update or delete on table \"variable_sets\" violates foreign key constraint \"instance_table_variable_set_fkey\" on table \"instances\"\nDETAIL:  Key (id)=(527) is still referenced from table \"instances\".\nAnd again, during bosh clean-up --all\n```\n11:32:17 | Deleting jobs: rabbitmq-server/0a9605e8cfb147bf9d3e135484f923c8994b1164 (00:00:00)\n            L Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n11:32:17 | Deleting releases: rabbitmq-server/0.11.0+dev.1509711606 (00:00:00)\n            L Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n11:32:17 | Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n```\nWhich BOSH director version & CPI is this?\nsh\nName      bosh-director-20170606\nUUID      6767783b-4ca4-4e33-90e5-cbf8c6a7bfd0\nVersion   262.0.0 (00000000)\nCPI       google_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\nWhat did bosh cck say?\n11:41:06 | Scanning 3 VMs: Checking VM states (00:00:06)\n11:41:12 | Scanning 3 VMs: 3 OK, 0 unresponsive, 0 missing, 0 unbound (00:00:00)\n11:41:12 | Scanning 3 persistent disks: Looking for inactive disks (00:00:01)\n11:41:13 | Scanning 3 persistent disks: 3 OK, 0 missing, 0 inactive, 0 mount-info mismatch (00:00:00)\nHow did I fix it?\n\n[x] delete problematic1 deployment\n[x] bosh clean-up --all\n[x] bosh deploy to redeploy\n\n\n1 12 days ago, variable interpolation started failing. The deploy would succeed, but BOSH would fail to template files correctly. \u00af\\_(\u30c4)_/\u00af rabbitmq/rabbitmq-server-boshrelease#38. ## What happened?\nJust ran into this today during bosh deploy:\n11:31:51 | Error: PG::Error: ERROR:  update or delete on table \"variable_sets\" violates foreign key constraint \"instance_table_variable_set_fkey\" on table \"instances\"\nDETAIL:  Key (id)=(527) is still referenced from table \"instances\".\nAnd again, during bosh clean-up --all\n```\n11:32:17 | Deleting jobs: rabbitmq-server/0a9605e8cfb147bf9d3e135484f923c8994b1164 (00:00:00)\n            L Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n11:32:17 | Deleting releases: rabbitmq-server/0.11.0+dev.1509711606 (00:00:00)\n            L Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n11:32:17 | Error: PG::Error: ERROR:  update or delete on table \"templates\" violates foreign key constraint \"instances_templates_template_id_fkey\" on table \"instances_templates\"\nDETAIL:  Key (id)=(377) is still referenced from table \"instances_templates\".\n```\nWhich BOSH director version & CPI is this?\nsh\nName      bosh-director-20170606\nUUID      6767783b-4ca4-4e33-90e5-cbf8c6a7bfd0\nVersion   262.0.0 (00000000)\nCPI       google_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\nWhat did bosh cck say?\n11:41:06 | Scanning 3 VMs: Checking VM states (00:00:06)\n11:41:12 | Scanning 3 VMs: 3 OK, 0 unresponsive, 0 missing, 0 unbound (00:00:00)\n11:41:12 | Scanning 3 persistent disks: Looking for inactive disks (00:00:01)\n11:41:13 | Scanning 3 persistent disks: 3 OK, 0 missing, 0 inactive, 0 mount-info mismatch (00:00:00)\nHow did I fix it?\n\n[x] delete problematic1 deployment\n[x] bosh clean-up --all\n[x] bosh deploy to redeploy\n\n\n1 12 days ago, variable interpolation started failing. The deploy would succeed, but BOSH would fail to template files correctly. \u00af\\_(\u30c4)_/\u00af rabbitmq/rabbitmq-server-boshrelease#38. I've just hit this today:\nError: Action Failed get_task: Task c74caa06-8c76-4e8e-55d2-6193524ecaac result: Mounting persistent disk: Formatting partition with xfs: Shelling out to mkfs.xfs: Running command: 'mkfs.xfs /dev/sdb1', stdout: '', stderr: 'warning: device is not properly aligned /dev/sdb1\nUse -f to force usage of a misaligned device\n': exit status 1\nRunning 264.7.0 with stemcell bosh-google-kvm-ubuntu-trusty-go_agent/3541.5. I've just hit this today:\nError: Action Failed get_task: Task c74caa06-8c76-4e8e-55d2-6193524ecaac result: Mounting persistent disk: Formatting partition with xfs: Shelling out to mkfs.xfs: Running command: 'mkfs.xfs /dev/sdb1', stdout: '', stderr: 'warning: device is not properly aligned /dev/sdb1\nUse -f to force usage of a misaligned device\n': exit status 1\nRunning 264.7.0 with stemcell bosh-google-kvm-ubuntu-trusty-go_agent/3541.5. I believe that this is already addressed via the bosh update-resurrection command: https://bosh.io/docs/resurrector.html. I believe that this is already addressed via the bosh update-resurrection command: https://bosh.io/docs/resurrector.html. When the deployment is created, I would expect post-start to run. We deployed something for the first time and post-start didn't run. We no longer have the deployment to get the instance logs, but I am wondering whether you've seen this before.\nPart of the same deployment, we have observed pre-start failing - could this prevent post-start from running?. When the deployment is created, I would expect post-start to run. We deployed something for the first time and post-start didn't run. We no longer have the deployment to get the instance logs, but I am wondering whether you've seen this before.\nPart of the same deployment, we have observed pre-start failing - could this prevent post-start from running?. This just hit us as well, we're on 262.0.0 & bosh-google-kvm-ubuntu-trusty-go_agent/3421.11 .\nWhich stemcell / director versions do we need to upgrade to?. This just hit us as well, we're on 262.0.0 & bosh-google-kvm-ubuntu-trusty-go_agent/3421.11 .\nWhich stemcell / director versions do we need to upgrade to?. Experiencing the same issue on 3468 & 3445, testing with 3431.13 next.. Experiencing the same issue on 3468 & 3445, testing with 3431.13 next.. I've observed the same issue with 3431.13.\nI've just confirmed the same behaviour in bosh-google-kvm-ubuntu-trusty-go_agent/3541.12 & route_registrar v0.175.0, so it doesn't seem to be an issue with this specific BOSH release.\nI'll take you up on the pairing offer, reaching out via DM.. If drain runs and stops the process, monit will not run the stop script. Running monit in debug mode (append -v to /etc/sv/monit/run & restart with sv restart /etc/sv/monit) shows us the following:\n```\n[UTC Apr 25 16:29:41] info     : monit daemon at 6654 awakened\n[UTC Apr 25 16:29:41] info     : Awakened by User defined signal 1\n[UTC Apr 25 16:29:41] debug    : Monitoring disabled -- service rabbitmq-server\n[UTC Apr 25 16:29:41] info     : 'rabbitmq-server' unmonitor action done\ndrain runs and succeeds...\n[UTC Apr 25 16:30:04] info     : stop service 'rabbitmq-server' on user request\n[UTC Apr 25 16:30:04] info     : monit daemon at 6654 awakened\n[UTC Apr 25 16:30:04] info     : Awakened by User defined signal 1\n[UTC Apr 25 16:30:04] debug    : 'rabbitmq-server' Error testing process id [18235] -- No such process\n[UTC Apr 25 16:30:04] info     : 'rabbitmq-server' stop action done\n```\nThe solution is to call the stop script from the drain script. Be careful with output from the stop script, drain will fail if anything other than an integer is returned on stdout (see http://bosh.io/docs/drain/).. Works as expected, just a gotcha.. Thanks! Dupe, closing.. Thanks! Dupe, closing.. These rlimits will be passed to Warden::Protocol::SpawnRequest where beefcake expects symbols.\n. Pretty sure these were the warden container limits. The way we used to handle this before was to set ulimits within each job monit starts, but that meant making changes in many places. The real problem was with products that we didn't own, e.g. Cloud Foundry. The above addition means that we can increase the global container ulimit which will be respected by all processes started within those containers.\n. ",
    "DanielCloudCredo": "@cppforlife This is a blocking bug for us. The patch above is no longer applicable after the change to external CPIs.\nWhen we deploy all of our releases we run out of resources (open files and processes). We see errors such as:\n\nThis error does not occur when we deploy Cloud Foundry by itself. \nOur workaround at the moment is to run the following steps on the BOSH-lite VM.\n1. Deploy everything.\n2. We downloaded and compiled util-linux 2.21 in order to get the prlimit command.\n3. Define the following functions:\n```\nEcho out PID and all child PIDs thereof, each on a newline\npidlist ()\n{\n  local thispid=$1;\n  local fulllist=;\n  local childlist=;\n  childlist=$(ps --ppid $thispid -o pid h);\n  for pid in $childlist;\n  do\n    fulllist=\"$(pidlist $pid) $fulllist\";\n  done;\n  echo \"$thispid $fulllist\"\n}\n```\n```\nTake a PID, use pidlist to get it and all its children, strip whitespace and set ulimits\nsetulimits()\n{\n   pidlist $1 | xargs echo | tr ' ' '\\n' | xargs -I {} prlimit --nofile=999999 --nproc=99999 --pid={}\n}\n``\n1. We then finally run:pgrep wshd | while read pid; do setulimits $pid; done. This sets the limits sky high for all of the containers and child processes in them.\n2. Rejoice as everything now works (until you runbosh recreate`).\nCould this be fixed or, as above, configurable somewhere?\n. Would love to see this implemented, it would remove a constant source of irritation in our working day.\n. ",
    "simonjohansson": "Hey. We are also seing missaligned disks in our installation.\nbosh-stemcell-2624-vsphere-esxi-ubuntu-lucid.\nThis is output from microbosh, but would be similar for bosh deployed VMs.\n```\nDisk /dev/sdb: 107.4 GB, 107374182400 bytes\n255 heads, 63 sectors/track, 13054 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\nDevice Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1        1021     8201182   82  Linux swap / Solaris\n/dev/sdb2            1022       13054    96655072+  83  Linux\nDisk /dev/sda: 3221 MB, 3221225472 bytes\n4 heads, 32 sectors/track, 49152 cylinders\nUnits = cylinders of 128 * 512 = 65536 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0000e876\nDevice Boot      Start         End      Blocks   Id  System\n/dev/sda1               1       46860     2998992   83  Linux\nDisk /dev/sdc: 107.4 GB, 107374182400 bytes\n255 heads, 63 sectors/track, 13054 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x00000000\nDevice Boot      Start         End      Blocks   Id  System\n/dev/sdc1               1       13054   104856254+  83  Linux\n```\nAs the graph shows in this tread,\nhttps://communities.vmware.com/thread/425590\nmissaligned disks are something that you do not want. We are using a netapp that is shared with other projects, and we do not want to be a busy neighbour. :)\nPlease open again.\n. We are also seeing the same issue. Happens a bit randomly, I have attached a gist task x --debug from a \"bosh run errand smoke_tests\"\nhttps://gist.github.com/simonjohansson/cb64088d2f5ca4b4c0c4\nDoing a subsequent bosh cck finds an issue,\n```\n\u279c  bosh-workspace git:(master) bosh cck\nPerforming cloud check...\nDirector task 37\n  Started scanning 33 vms\n  Started scanning 33 vms > Checking VM states. Done (00:00:10)\n  Started scanning 33 vms > 32 OK, 0 unresponsive, 1 missing, 0 unbound, 0 out of sync. Done (00:00:00)\n     Done scanning 33 vms (00:00:10)\nStarted scanning 7 persistent disks\n  Started scanning 7 persistent disks > Looking for inactive disks. Done (00:00:00)\n  Started scanning 7 persistent disks > 7 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 7 persistent disks (00:00:00)\nTask 37 done\nStarted     2014-09-16 18:59:04 UTC\nFinished    2014-09-16 18:59:14 UTC\nDuration    00:00:10\nScan is complete, checking if any problems found...\nFound 1 problem\nProblem 1 of 1: VM with cloud ID `vm-ad8ff33b-bab7-47f5-9d1c-7b058ba9b234' missing.\n```\n. We are also seeing the same issue. Happens a bit randomly, I have attached a gist task x --debug from a \"bosh run errand smoke_tests\"\nhttps://gist.github.com/simonjohansson/cb64088d2f5ca4b4c0c4\nDoing a subsequent bosh cck finds an issue,\n```\n\u279c  bosh-workspace git:(master) bosh cck\nPerforming cloud check...\nDirector task 37\n  Started scanning 33 vms\n  Started scanning 33 vms > Checking VM states. Done (00:00:10)\n  Started scanning 33 vms > 32 OK, 0 unresponsive, 1 missing, 0 unbound, 0 out of sync. Done (00:00:00)\n     Done scanning 33 vms (00:00:10)\nStarted scanning 7 persistent disks\n  Started scanning 7 persistent disks > Looking for inactive disks. Done (00:00:00)\n  Started scanning 7 persistent disks > 7 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 7 persistent disks (00:00:00)\nTask 37 done\nStarted     2014-09-16 18:59:04 UTC\nFinished    2014-09-16 18:59:14 UTC\nDuration    00:00:10\nScan is complete, checking if any problems found...\nFound 1 problem\nProblem 1 of 1: VM with cloud ID `vm-ad8ff33b-bab7-47f5-9d1c-7b058ba9b234' missing.\n```\n. So we tried to delete our deployment today which failed with DRS Lock issues.\nThe VMs get deleted but it takes some time after that the \"bosh delete ...\" has failed.\nBosh still thinks the VMs are there so we need to do a bosh cck and delete the references in DB.\nIt looks like our vSphere environment is to slow to handle the MAX_LOCK_TIMEOUT_IN_SECONDS=30[1] or that this value is just a bit to optimistic.\nAny way we can have this be overridable? Either in the bosh manifest or in the ~/.bosh_config?\n[1] https://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/lib/cloud/vsphere/drs_rules/drs_lock.rb#L8\n. So we tried to delete our deployment today which failed with DRS Lock issues.\nThe VMs get deleted but it takes some time after that the \"bosh delete ...\" has failed.\nBosh still thinks the VMs are there so we need to do a bosh cck and delete the references in DB.\nIt looks like our vSphere environment is to slow to handle the MAX_LOCK_TIMEOUT_IN_SECONDS=30[1] or that this value is just a bit to optimistic.\nAny way we can have this be overridable? Either in the bosh manifest or in the ~/.bosh_config?\n[1] https://github.com/cloudfoundry/bosh/blob/master/bosh_vsphere_cpi/lib/cloud/vsphere/drs_rules/drs_lock.rb#L8\n. Nah, changing the timeout(to 300) doesn't make any difference.\nLooks like there might be a bug with the newly introduced drs_rules.\n. Nah, changing the timeout(to 300) doesn't make any difference.\nLooks like there might be a bug with the newly introduced drs_rules.\n. Hi.\nI have not looked to much into the code so sorry if Im asking stupid questions :)\n1. Why does the DRS Lock issue happen in the first place, looking at the debug log it seems like \"Creating DRS rule attribute: drs_lock\" gets executed over and over again until the timeout.\n2. Why was DRS locking introduced in the first place?\n. Hi.\nI have not looked to much into the code so sorry if Im asking stupid questions :)\n1. Why does the DRS Lock issue happen in the first place, looking at the debug log it seems like \"Creating DRS rule attribute: drs_lock\" gets executed over and over again until the timeout.\n2. Why was DRS locking introduced in the first place?\n. @cppforlife we are currently deploying a standard vsphere env from the generate_deployment_manifest, so, 33.\nThis only happens when deleting VMs though, never when creating(at least so far, we've done may fresh deployments today). Only solution we've found is to do bosh cck and delete the references(to the already deleted VMs).\nNo biggie, but it breaks automation. :)\n. @cppforlife we are currently deploying a standard vsphere env from the generate_deployment_manifest, so, 33.\nThis only happens when deleting VMs though, never when creating(at least so far, we've done may fresh deployments today). Only solution we've found is to do bosh cck and delete the references(to the already deleted VMs).\nNo biggie, but it breaks automation. :)\n. Great, thanks! :)\n. Great, thanks! :)\n. Hazzah, neato!\n. Hazzah, neato!\n. I'm hitting the same issue when trying to deploy in Frankfurt.\n```\nubuntu@vm-in-us-east-1:~$ rvm list gemsets\nrvm gemsets\nruby-2.0.0-p598 [ x86_64 ]\n=> ruby-2.0.0-p598@bosh [ x86_64 ]\n   ruby-2.0.0-p598@global [ x86_64 ]\n```\n```\nubuntu@vm-in-us-east-1:~$ gem list\n LOCAL GEMS \nagent_client (1.2858.0)\naws-sdk (1.60.2)\naws-sdk-v1 (1.60.2)\nbigdecimal (1.2.0)\nblobstore_client (1.2858.0)\nbosh-bootstrap (0.16.1)\nbosh-director-core (1.2858.0)\nbosh-registry (1.2858.0)\nbosh-stemcell (1.2858.0)\nbosh-template (1.2858.0)\nbosh_aws_cpi (1.2858.0)\nbosh_cli (1.2858.0)\nbosh_cli_plugin_micro (1.2858.0)\nbosh_common (1.2858.0)\nbosh_cpi (1.2858.0)\nbosh_openstack_cpi (1.2858.0)\nbosh_vcloud_cpi (0.7.2)\nbosh_vsphere_cpi (1.2858.0)\nbuilder (3.1.4)\nbundler (1.7.6)\nbundler-unload (1.0.2)\nCFPropertyList (2.3.0)\ncyoi (0.11.3)\ndaemons (1.1.9)\neventmachine (1.0.7)\nexcon (0.44.3)\nexecutable-hooks (1.3.2)\nfission (0.5.0)\nfog (1.27.0)\nfog-atmos (0.1.0)\nfog-aws (0.1.1)\nfog-brightbox (0.7.1)\nfog-core (1.29.0)\nfog-ecloud (0.0.2)\nfog-json (1.0.0)\nfog-profitbricks (0.0.1)\nfog-radosgw (0.0.3)\nfog-sakuracloud (1.0.0)\nfog-serverlove (0.1.1)\nfog-softlayer (0.4.1)\nfog-storm_on_demand (0.1.0)\nfog-terremark (0.0.4)\nfog-vmfusion (0.0.1)\nfog-voxel (0.0.2)\nfog-xml (0.1.1)\nformatador (0.2.5)\ngem-wrappers (1.2.7)\nhighline (1.6.21)\nhttpclient (2.4.0)\ninflecto (0.0.2)\nio-console (0.4.2)\nipaddress (0.8.0)\njson (1.7.7)\njson_pure (1.8.2)\nlittle-plugger (1.1.3)\nlog4r (1.1.10)\nlogging (1.8.2)\nmembrane (1.1.0)\nmime-types (1.25.1)\nminitar (0.5.4)\nminitest (4.3.2)\nmono_logger (1.1.0)\nmulti_json (1.10.1)\nmysql2 (0.3.18)\nnet-scp (1.1.2)\nnet-ssh (2.9.2)\nnet-ssh-gateway (1.2.0)\nnetaddr (1.5.0)\nnokogiri (1.5.11)\npg (0.15.1)\nprogressbar (0.9.2)\npsych (2.0.0)\nrack (1.6.0)\nrack-protection (1.5.3)\nrake (0.9.6)\nrbvmomi (1.8.2)\nrdoc (4.0.0)\nreadwritesettings (3.0.1)\nredcard (1.1.0)\nrest-client (1.6.8)\nruby-atmos-pure (1.0.5)\nruby-hmac (0.4.0)\nrubygems-bundler (1.4.4)\nrvm (1.11.3.9)\nsemi_semantic (1.1.0)\nsequel (3.43.0)\nsinatra (1.4.5)\nsqlite3 (1.3.10)\nterminal-table (1.4.5)\ntest-unit (2.0.0.0)\nthin (1.5.1)\nthor (0.19.1)\ntilt (1.4.1)\ntrollop (2.1.1)\nyajl-ruby (1.2.1)\n```\n```\nubuntu@vm-in-us-east-1:~$ bosh-bootstrap deploy\n1. AWS\n2. OpenStack\n3. vSphere\nChoose your infrastructure: 1\nUsing provider AWS\nAccess key: snippety-snip\nSecret key: snippety-snip\n\n*US East (Northern Virginia) Region (us-east-1)\nUS West (Oregon) Region (us-west-2)\nUS West (Northern California) Region (us-west-1)\nEU (Ireland) Region (eu-west-1)\nEU (Frankfurt) Region (eu-central-1)\nAsia Pacific (Singapore) Region (ap-southeast-1)\nAsia Pacific (Sydney) Region (ap-southeast-2)\nAsia Pacific (Tokyo) Region (ap-northeast-1)\nSouth America (Sao Paulo) Region (sa-east-1)\nChina (Beijing) Region (cn-north-1)\nChoose AWS region: 5\n\nConfirming: Using AWS EC2/eu-central-1\n1. vpc-cfba5ba6 (172.31.0.0/16)\n2. bosh (10.0.0.0/16)\n3. EC2 only\nChoose a VPC: 3\nUsing EC2...\nAcquiring a public IP address... aaa.bbb.ccc.ddd\nConfirming: Using address aaa.bbb.ccc.ddd\nReusing security group ssh\n -> no additional ports opened\nReusing security group dns-server\n -> no additional ports opened\nReusing security group bosh\n -> no additional ports opened\nDestroying key pair firstbosh... done\nAcquiring a key pair firstbosh... done\nConfirming: Using key pair firstbosh\nDetermining stemcell image/file to use... curl -O 'https://bosh-jenkins-artifacts.s3.amazonaws.com/bosh-stemcell/aws/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  442M  100  442M    0     0  68.5M      0  0:00:06  0:00:06 --:--:-- 70.5M\n/home/ubuntu/deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz\nbosh micro deployment firstbosh\nWARNING! Your target has been changed to `https://aaa.bbb.ccc.ddd:25555'!\nDeployment set to '/home/ubuntu/deployments/firstbosh/micro_bosh.yml'\nbosh -n micro deploy --update-if-exists /home/ubuntu/deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    bosh-aws-xen-ubuntu-trusty-go_agent\nVersion: 2858\nStarted deploy micro bosh\n  Started deploy micro bosh > Unpacking stemcell. Done (00:00:06)\n  Started deploy micro bosh > Uploading stemcelllog writing failed. can't be called from trap context\ncreate stemcell failed: The instance ID 'i-af97f555' does not exist:\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:375:in return_or_raise'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:476:inclient_request'\n(eval):3:in describe_instances'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/resource.rb:72:indescribe_call'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/instance.rb:787:in get_resource'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/core/resource.rb:235:inblock (2 levels) in define_attribute_getter'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/core/cacheable.rb:63:in retrieve_attribute'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/resource.rb:66:inretrieve_attribute'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/core/resource.rb:235:in block in define_attribute_getter'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/aws-sdk-v1-1.60.2/lib/aws/ec2/instance.rb:527:inavailability_zone'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_aws_cpi-1.2858.0/lib/cloud/aws/availability_zone_selector.rb:19:in select_availability_zone'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_aws_cpi-1.2858.0/lib/cloud/aws/cloud.rb:172:inblock in create_disk'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_common-1.2858.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_aws_cpi-1.2858.0/lib/cloud/aws/cloud.rb:166:increate_disk'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_aws_cpi-1.2858.0/lib/cloud/aws/cloud.rb:417:in block in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_common-1.2858.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_aws_cpi-1.2858.0/lib/cloud/aws/cloud.rb:405:in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:228:inblock (2 levels) in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:85:in step'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:227:inblock in create_stemcell'\n/home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/tmpdir.rb:88:in mktmpdir'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:213:increate_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:118:in create'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:98:inblock in create_deployment'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:92:in with_lifecycle'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/deployer/instance_manager.rb:98:increate_deployment'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli_plugin_micro-1.2858.0/lib/bosh/cli/commands/micro.rb:179:in perform'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli-1.2858.0/lib/cli/command_handler.rb:57:inrun'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli-1.2858.0/lib/cli/runner.rb:56:in run'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh_cli-1.2858.0/bin/bosh:14:in'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/bosh:23:in load'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/bosh:23:in'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/ruby_executable_hooks:15:in eval'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/ruby_executable_hooks:15:in'\n/home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/rake/file_utils.rb:53:in block in create_shell_runner': Command failed with status (1): [bosh -n micro deploy --update-if-exists /h...] (RuntimeError)\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/rake/file_utils.rb:45:incall'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/rake/file_utils.rb:45:in sh'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/helpers/bundle.rb:18:inblock in run'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@global/gems/bundler-1.7.6/lib/bundler.rb:236:in block in with_clean_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@global/gems/bundler-1.7.6/lib/bundler.rb:223:inwith_original_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@global/gems/bundler-1.7.6/lib/bundler.rb:229:in with_clean_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/helpers/bundle.rb:16:inrun'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:54:in block in deploy_or_update'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/fileutils.rb:125:inchdir'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/fileutils.rb:125:in cd'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:52:indeploy_or_update'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:42:in block in deploy'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/fileutils.rb:125:inchdir'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p598/lib/ruby/2.0.0/fileutils.rb:125:in cd'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:40:indeploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/commands/deploy.rb:77:in perform_microbosh_deploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/commands/deploy.rb:24:inperform'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/thor_cli.rb:11:in deploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/thor-0.19.1/lib/thor/command.rb:27:inrun'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/thor-0.19.1/lib/thor/invocation.rb:126:in invoke_command'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/thor-0.19.1/lib/thor.rb:359:indispatch'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/thor-0.19.1/lib/thor/base.rb:440:in start'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/gems/bosh-bootstrap-0.16.1/bin/bosh-bootstrap:13:in'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/bosh-bootstrap:23:in load'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/bosh-bootstrap:23:in'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/ruby_executable_hooks:15:in eval'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p598@bosh/bin/ruby_executable_hooks:15:in'\n```\n```\nubuntu@vm-in-us-east-1:~$ du -sch deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz \n443M    deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz\n443M    total\nubuntu@vm-in-us-east-1:~$ tar -tvf deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz \n-rw-r--r-- root/root      7911 2015-02-24 21:40 apply_spec.yml\n-rw-r--r-- root/root 468107204 2015-02-24 21:43 image\n-rw-rw-r-- ubuntu/ubuntu   380 2015-02-24 21:43 stemcell.MF\nubuntu@vm-in-us-east-1:~$ tar zxf deployments/firstbosh/bosh-stemcell-2858-aws-xen-ubuntu-trusty-go_agent.tgz stemcell.MF\nubuntu@vm-in-us-east-1:~$ cat stemcell.MF \n\nname: bosh-aws-xen-ubuntu-trusty-go_agent\nversion: '2858'\nbosh_protocol: 1\nsha1: 1d1d26b06127dc8248211f7fb5be95987b49d36c\ncloud_properties:\n  name: bosh-aws-xen-ubuntu-trusty-go_agent\n  version: '2858'\n  infrastructure: aws\n  hypervisor: xen\n  disk_format: raw\n  container_format: bare\n  os_type: linux\n  os_distro: ubuntu\n  architecture: x86_64\n  root_device_name: /dev/sda1\n```\n. Anyone know what might be missing for this to work with the Frankfurt region? \nThe close proximity to our DCs in Netherland make it much more viable than Ireland from a connection standpoint.\n. For my own reference, \nhere is a thread in the bosh-dev list about producing light stemcells for all regions.\nhttps://groups.google.com/a/cloudfoundry.org/forum/#!topic/bosh-dev/eS-ZKJaFiJw\n. Hi. Any updates on this? \n. Hi. Any updates on this? \n. @cppforlife good catch!\nI will try with a jumpbox in Frankfurt and see if I have better luck :)\n. @cppforlife good catch!\nI will try with a jumpbox in Frankfurt and see if I have better luck :)\n. Still an issue(seems to be the same as @arekkas now)\n```\nubuntu@machine-in-frankfurt:~$ bosh-bootstrap deploy\n1. AWS\n2. OpenStack\n3. vSphere\nChoose your infrastructure: 1\nUsing provider AWS\nAccess key: asdf\nSecret key: asdf\n\n*US East (Northern Virginia) Region (us-east-1)\nUS West (Oregon) Region (us-west-2)\nUS West (Northern California) Region (us-west-1)\nEU (Ireland) Region (eu-west-1)\nEU (Frankfurt) Region (eu-central-1)\nAsia Pacific (Singapore) Region (ap-southeast-1)\nAsia Pacific (Sydney) Region (ap-southeast-2)\nAsia Pacific (Tokyo) Region (ap-northeast-1)\nSouth America (Sao Paulo) Region (sa-east-1)\nChina (Beijing) Region (cn-north-1)\nChoose AWS region: 5\n\nConfirming: Using AWS EC2/eu-central-1\n1. vpc-cfba5ba6 (172.31.0.0/16)\n2. bosh (10.0.0.0/16)\n3. EC2 only\nChoose a VPC: 2\n1. public (10.0.0.0/24)\n2. subnet-61b6b619 (172.31.0.0/20)\n3. subnet-779d7d1e (172.31.16.0/20)\nChoose a subnet: 1\nChoose IP |10.0.0.4| \nConfirming: Using address 10.0.0.4\nReusing security group ssh-vpc-0ff20a66 for the VPC\n -> no additional ports opened\nReusing security group dns-server-vpc-0ff20a66 for the VPC\n -> no additional ports opened\nReusing security group bosh-vpc-0ff20a66 for the VPC\n -> no additional ports opened\nDestroying key pair firstbosh... done\nAcquiring a key pair firstbosh... done\nConfirming: Using key pair firstbosh\nDetermining stemcell image/file to use... curl -O 'https://bosh-jenkins-artifacts.s3.amazonaws.com/bosh-stemcell/aws/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz'\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  445M  100  445M    0     0  12.7M      0  0:00:34  0:00:34 --:--:-- 17.0M\n/home/ubuntu/deployments/firstbosh/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz\nbosh micro deployment firstbosh\nWARNING! Your target has been changed to `https://10.0.0.4:25555'!\nDeployment set to '/home/ubuntu/deployments/firstbosh/micro_bosh.yml'\nbosh -n micro deploy --update-if-exists /home/ubuntu/deployments/firstbosh/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    bosh-aws-xen-ubuntu-trusty-go_agent\nVersion: 2881\nStarted deploy micro bosh\n  Started deploy micro bosh > Unpacking stemcell. Done (00:00:07)\n  Started deploy micro bosh > Uploading stemcelllog writing failed. can't be called from trap context\ncreate stemcell failed: unable to find AKI:\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_aws_cpi-1.2881.0/lib/cloud/aws/aki_picker.rb:15:in pick'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_aws_cpi-1.2881.0/lib/cloud/aws/stemcell_creator.rb:100:inimage_params'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_aws_cpi-1.2881.0/lib/cloud/aws/stemcell_creator.rb:24:in create'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_aws_cpi-1.2881.0/lib/cloud/aws/cloud.rb:425:inblock in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_common-1.2881.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_aws_cpi-1.2881.0/lib/cloud/aws/cloud.rb:405:increate_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:228:in block (2 levels) in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:85:instep'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:227:in block in create_stemcell'\n/home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/tmpdir.rb:88:inmktmpdir'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:213:in create_stemcell'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:118:increate'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:98:in block in create_deployment'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:92:inwith_lifecycle'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/deployer/instance_manager.rb:98:in create_deployment'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli_plugin_micro-1.2881.0/lib/bosh/cli/commands/micro.rb:179:inperform'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli-1.2881.0/lib/cli/command_handler.rb:57:in run'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli-1.2881.0/lib/cli/runner.rb:56:inrun'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh_cli-1.2881.0/bin/bosh:16:in <top (required)>'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/bosh:23:inload'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/bosh:23:in <main>'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/ruby_executable_hooks:15:ineval'\n/home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/ruby_executable_hooks:15:in <main>'\n/home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/rake/file_utils.rb:53:inblock in create_shell_runner': Command failed with status (1): [bosh -n micro deploy --update-if-exists /h...] (RuntimeError)\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/rake/file_utils.rb:45:in call'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/rake/file_utils.rb:45:insh'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/helpers/bundle.rb:18:in block in run'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@global/gems/bundler-1.8.5/lib/bundler.rb:247:inblock in with_clean_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@global/gems/bundler-1.8.5/lib/bundler.rb:234:in with_original_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@global/gems/bundler-1.8.5/lib/bundler.rb:240:inwith_clean_env'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/helpers/bundle.rb:16:in run'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:54:inblock in deploy_or_update'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/fileutils.rb:125:in chdir'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/fileutils.rb:125:incd'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:52:in deploy_or_update'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:42:inblock in deploy'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/fileutils.rb:125:in chdir'\n    from /home/ubuntu/.rvm/rubies/ruby-2.0.0-p643/lib/ruby/2.0.0/fileutils.rb:125:incd'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/microbosh.rb:40:in deploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/commands/deploy.rb:77:inperform_microbosh_deploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/cli/commands/deploy.rb:24:in perform'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/lib/bosh-bootstrap/thor_cli.rb:11:indeploy'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/thor-0.19.1/lib/thor/command.rb:27:in run'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/thor-0.19.1/lib/thor/invocation.rb:126:ininvoke_command'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/thor-0.19.1/lib/thor.rb:359:in dispatch'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/thor-0.19.1/lib/thor/base.rb:440:instart'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/gems/bosh-bootstrap-0.16.1/bin/bosh-bootstrap:13:in <top (required)>'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/bosh-bootstrap:23:inload'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/bosh-bootstrap:23:in <main>'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/ruby_executable_hooks:15:ineval'\n    from /home/ubuntu/.rvm/gems/ruby-2.0.0-p643@bosh/bin/ruby_executable_hooks:15:in `'\n```\n```\nubuntu@asdf:~$ du -sch deployments/firstbosh/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz \n446M    deployments/firstbosh/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz\n446M    total\nubuntu@asdf:~$ tar zxf deployments/firstbosh/bosh-stemcell-2881-aws-xen-ubuntu-trusty-go_agent.tgz stemcell.MF\nubuntu@asdf:~$ cat stemcell.MF\n\nname: bosh-aws-xen-ubuntu-trusty-go_agent\nversion: '2881'\nbosh_protocol: 1\nsha1: d6e9c2aa7b1df7ea2d4b314c0f57f49fb13f855a\ncloud_properties:\n  name: bosh-aws-xen-ubuntu-trusty-go_agent\n  version: '2881'\n  infrastructure: aws\n  hypervisor: xen\n  disk_format: raw\n  container_format: bare\n  os_type: linux\n  os_distro: ubuntu\n  architecture: x86_64\n  root_device_name: /dev/sda1\n```\n. @Jonty \nThere is a client that we use for these kind of tasks. (Uploading releases, stemcells, deleting releases, stemcells, deploying etc.)\n```\nrequire 'cli' # Super shitty name for a client :D\nbosh_client = Bosh::Cli::Client::Director.new(\"https://hostname.com:25555\")\nbosh_client.login(\"admin\", \"password\")\nstemcells = bosh_client.list_stemcells\nunused = stemcells.select {|stemcell| stemcell[\"deployments\"] == []}\nunused.sort_by {|stemcell| stemcell[\"version\"].to_f}.each do |stemcell|\n  bosh_client.delete_stemcell(stemcell['name'], stemcell['version'])\nend\n```\nhttps://rubygems.org/gems/bosh_cli/versions/1.3071.0\n. @cppforlife \nI agree that functionality with the bosh CLI for most normal tasks is great, but sometimes there is a need to go outside of the \"normal\", and then --terse would be really nifty so one does not have to write awk's/sed's of doom.\nSurely there is room for both?\n. ",
    "maximilien": "Hi, @dims,\n@mariash and the BOSH team discussed this today. Overall the agreement, as was mentioned before, this is a good feature to add, except that the implementation approach could be improved.\nCould you please take a look at her previous comments on how to improve and resubmit?\nThanks much,\n@maximilien\n. Hi, @valeriap, please edit the issue and use quote and \nLong quotes\nTo make this and future issues more readable. Also, please include what BOSH version you are using as well as stemcell. Thanks.\nPS: see (Markdown supported)[https://guides.github.com/features/mastering-markdown/] for info on using ` and other such formatting.\n. @mattcui seems like @cppforlife says it's already done.\nSeparately, yes, you would still need a unit tests but that's a moot point now.\n. @mattcui seems like @cppforlife says it's already done.\nSeparately, yes, you would still need a unit tests but that's a moot point now.\n. @dpb587 are you saying this was never pulled in?. @mattcui can you chime in here and confirm we are good on this front. Thx. Any update on this @cppforlife?\n. Any update on this @cppforlife?\n. Yeah, what about tonight 6:30p if no resolution.\n. Yeah, what about tonight 6:30p if no resolution.\n. @gu-bin if you and @mattcui work on a PR, please pass it by me and I can review. Update me tonight when we talk. Thx.\n. @gu-bin if you and @mattcui work on a PR, please pass it by me and I can review. Update me tonight when we talk. Thx.\n. So if understand this correctly. When this condition happens:\n\"If bosh deploy fails/canceled before the current vm's spec.json created, bosh director\" \nthe normal behavior would be the director cleaning up the VM. However, due to adding:\ndebug:\n        keep_unreachable_vms: true\nthe director keeps VM around. And next usage of that fails since VM is not reachable.\nCorrect?\nAlso, why is this issue happening now? Can you elaborate a bit. Thx\n. So if understand this correctly. When this condition happens:\n\"If bosh deploy fails/canceled before the current vm's spec.json created, bosh director\" \nthe normal behavior would be the director cleaning up the VM. However, due to adding:\ndebug:\n        keep_unreachable_vms: true\nthe director keeps VM around. And next usage of that fails since VM is not reachable.\nCorrect?\nAlso, why is this issue happening now? Can you elaborate a bit. Thx\n. @gu-bin I redacted comment to remove VM info.\n. Do you see the same issue with the community release? I ask since you listed version: ibm-v235.12 as version?\n. Do you see the same issue with the community release? I ask since you listed version: ibm-v235.12 as version?\n. @gu-bin cool. Closing this issue then.\n. @gu-bin can you also report this to Golang? If you already did, can you share here responses links. Thanks.\n. @gu-bin can you also report this to Golang? If you already did, can you share here responses links. Thanks.\n. @mattcui DK had some customer thingie today. We'll try him tomorrow. @mattcui DK had some customer thingie today. We'll try him tomorrow. Hey @cppforlife can we chat about this today? Thx\n/cc @jianqiu . Hey @cppforlife can we chat about this today? Thx\n/cc @jianqiu . ",
    "tedsuo": "Hi @james-masson \nWe've forwarded this request to the bosh team. @goehmen, can you comment on how difficult this will be?\n@tedsuo & @jmtuley\nCommunity Pair\n. Hi @jerenkrantz,\nIt should be possible to write a unit test in create_vm_spec that proves that the :personality param is not generated and sent to servers.create when skip_personality is set.  That should be sufficient.\nThanks,\n@tedsuo & @leoRoss\nCommunity Pair\n. Ok it looks like a lot of Fog is mixed into the Openstack::Cloud object, so a personality method may exist, though it's not clear when it would be called.  But that's okay, we don't need to test Fog.\nOnce you've got that one spec removed (line 312), we can pull it in.  The Bosh team will then review the lifecycle tests to ensure we're testing rdb in CI, and create an integration test for this.  I've moved this story to the Bosh tracker, so they'll take it from here.\nCheers,\n@tedsuo & @leoRoss \nCommunity Pair\n. @goehmen - this is a continuation of @mrdavidlaing 's work #585 \nWe will again move this story to the Bosh team's Tracker Project.\nCommunity Pair\n@tedsuo @leoRoss\n. This looks legitimate, assuming no backwards compatibility issues.  @goehmen, is this something the bosh team is interested in?\n@tedsuo & @leoRoss\nCommunity Pair\n. Moved to Bosh Tracker\nCF Community Pair\n. The Travis failure was just flakey tests, your PR looks good. We've moved this story to the Bosh team's tracker to be merged.\n. Sorry that we have taken so long to reply.  Monit is out of date.  But because we plan to move away from monit to a better cross platform interface, we want to keep the monit version stable to enable a smoother transition for Bosh community.  As a result, monit upgrade requests have to-date been declined.\nPlease see the closing of this pull request for more details: https://github.com/cloudfoundry/bosh/pull/590\nCurrently we are scheduled for an upgrade to monit 5.2.5.  You can track the progress of that story here: https://www.pivotaltracker.com/story/show/88572424\nAs an alternative, I suggest you enquire on the bosh or monit mailing list about the best way to emulate the behavior you need with the current version of monit.\nThank you,\nTed\n@tedsuo\n. We plan to fully support migrating from monit 5.2 to the new interface, so as long as you are targeting that as your api, you should be OK. @cppforlife can probably provide more details about the new interface. I don't believe there is a concrete proposal yet.  We just know that we don't want direct access to monit to be our permanent interface for process monitoring.  Possibly a pull request against https://github.com/cloudfoundry/bosh-notes would get things moving?  The necessary shleps are researching all of the api's we expect to support; deriving a universal api; and deriving a migration strategy from monit 5.2.\nBTW, here is the pull-request for the current move to monit 5.2.5: https://github.com/cloudfoundry/bosh/pull/743\n. ",
    "jerenkrantz": "Of course!  Testing this requires a custom OpenStack that rejects file\ninjection, so this is not trivial to test unless we have pretty thorough\nmocks.\nOn Jun 23, 2014 8:40 PM, \"Dr Nic Williams\" notifications@github.com wrote:\n\nYou may be asked to write a little unit test before this is merged. Let me\nknow if you'd like help with that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/592#issuecomment-46919691.\n. Of course!  Testing this requires a custom OpenStack that rejects file\ninjection, so this is not trivial to test unless we have pretty thorough\nmocks.\nOn Jun 23, 2014 8:40 PM, \"Dr Nic Williams\" notifications@github.com wrote:\nYou may be asked to write a little unit test before this is merged. Let me\nknow if you'd like help with that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/592#issuecomment-46919691.\n. I pushed a commit that should add a unit test as above.\n\nIdeally, it'd also may make sense to add another unit test to confirm that the personality field is present when not set, but my rspec-foo isn't good enough to figure this out.  I tried:\nopenstack.should_receive(:personality)\naround line 89 - but, that syntax isn't correct and fails.  But, this commit should handle the absence of the personality field and passes here.\n. I pushed a commit that should add a unit test as above.\nIdeally, it'd also may make sense to add another unit test to confirm that the personality field is present when not set, but my rspec-foo isn't good enough to figure this out.  I tried:\nopenstack.should_receive(:personality)\naround line 89 - but, that syntax isn't correct and fails.  But, this commit should handle the absence of the personality field and passes here.\n. Thanks - I will update the PR later.\nNote that when I set the skip_personality to true on line 302, the test at line 312 fails - so, it does seem that the openstack object has something called personality in this case.  But, I'm only grasping at straws here...so, I'll go ahead and remove the check at line 312 and you can submit for approval.\n. Thanks - I will update the PR later.\nNote that when I set the skip_personality to true on line 302, the test at line 312 fails - so, it does seem that the openstack object has something called personality in this case.  But, I'm only grasping at straws here...so, I'll go ahead and remove the check at line 312 and you can submit for approval.\n. The branch is now updated per comments above.  Please let me know if there's anything else required!\n. The branch is now updated per comments above.  Please let me know if there's anything else required!\n. I'll create a new PR to just remove file injection entirely.  Thanks.\n. I'll create a new PR to just remove file injection entirely.  Thanks.\n. See PR #615.  Closing this PR.\n. See PR #615.  Closing this PR.\n. As a followup, increasing MAX_RETRIES is not sufficient in and of itself - as the retries every six seconds appears to count against the rate limit.  In my case, I had three concurrent VMs trying to be created every six seconds - which is 180 retries in 60 seconds which goes against a limit of 10 per minute by default.  Oops.  =)\n. As a followup, increasing MAX_RETRIES is not sufficient in and of itself - as the retries every six seconds appears to count against the rate limit.  In my case, I had three concurrent VMs trying to be created every six seconds - which is 180 retries in 60 seconds which goes against a limit of 10 per minute by default.  Oops.  =)\n. Sure!  I have no real opinion how to solve this problem other than we\nshould work out of the box when the defaults are admittedly lame.  =)\nOn Jun 25, 2014 6:37 PM, \"greg oehmen\" notifications@github.com wrote:\n\n@jerenkrantz https://github.com/jerenkrantz The community pair was over\ntalking to the BOSH team as Ferdy was replying. What we came up with is\nthis: I'm going to reject the PR but keep the associated tracker story open\nand use that to improve retry logic as Ferdy suggested.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/597#issuecomment-47168020.\n. Sure!  I have no real opinion how to solve this problem other than we\nshould work out of the box when the defaults are admittedly lame.  =)\nOn Jun 25, 2014 6:37 PM, \"greg oehmen\" notifications@github.com wrote:\n@jerenkrantz https://github.com/jerenkrantz The community pair was over\ntalking to the BOSH team as Ferdy was replying. What we came up with is\nthis: I'm going to reject the PR but keep the associated tracker story open\nand use that to improve retry logic as Ferdy suggested.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/597#issuecomment-47168020.\n. I wonder if the bosh registry is failing to accept the connections from the BOSH agent due to a mismatch in the actual connecting IP address. @frodenas and I encountered this.\n\nThe hack that we came up with is:\n```\ndiff --git a/bosh-registry/lib/bosh/registry/instance_manager.rb b/bosh-registry/lib/bosh/registry/instance_manager.rb\nindex 8e9d165..aebc86b 100644\n--- a/bosh-registry/lib/bosh/registry/instance_manager.rb\n+++ b/bosh-registry/lib/bosh/registry/instance_manager.rb\n@@ -26,7 +26,7 @@ module Bosh::Registry\n     #        check will be performed to see if it instance id\n     #        actually has this IP address according to the IaaS.\n     def read_settings(instance_id, remote_ip = nil)\n-      check_instance_ips(remote_ip, instance_id) if remote_ip\n+      #check_instance_ips(remote_ip, instance_id) if remote_ip\n   get_instance(instance_id).settings\n end\n\n```\nIdeally, there should be some type of override to this.  But, my BOSH-fu is limited.\n. ",
    "leoRoss": "Hi @jerenkrantz,\nI don't believe the Openstack Cloud object has a personality method on it, personality is just a parameter on server_params which is passed to the servers.create method.  So unless I'm missing something, on line 312, openstack.should_not_receive(:personality) will always be true. This is also why the line you are trying to add always fails.\nI think your test on line 305 - confirming that server.create receives the expected server parameters when skip personality is set - is sufficient for a unit test.  I don't think we need to test that server.create returns a properly configured object, as it's a different subsystem and we are mocking it out.  \nIf that makes sense, can you delete line 312, and we'll send this along to the bosh team for approval?\nThanks,\n@tedsuo & @leoRoss\nCommunity Pair\n. Hi @jerenkrantz,\nI don't believe the Openstack Cloud object has a personality method on it, personality is just a parameter on server_params which is passed to the servers.create method.  So unless I'm missing something, on line 312, openstack.should_not_receive(:personality) will always be true. This is also why the line you are trying to add always fails.\nI think your test on line 305 - confirming that server.create receives the expected server parameters when skip personality is set - is sufficient for a unit test.  I don't think we need to test that server.create returns a properly configured object, as it's a different subsystem and we are mocking it out.  \nIf that makes sense, can you delete line 312, and we'll send this along to the bosh team for approval?\nThanks,\n@tedsuo & @leoRoss\nCommunity Pair\n. @goehmen this seems like a reasonable request. Should we bring this into your Tracker?\nThanks @tnaoto \nleoRoss & danIavine\nCF Community Pair\n. @goehmen this seems like a reasonable request. Should we bring this into your Tracker?\nThanks @tnaoto \nleoRoss & danIavine\nCF Community Pair\n. @goehmen This looks like it may be a breaking change, since blobstore.max_upload_size is currently not a field in the spec file. However, this seems like a useful feature. Let us know what you think?\nThanks @tinateng \n@leoRoss & @danIavine\nCF Community Pair\n. @goehmen This looks like it may be a breaking change, since blobstore.max_upload_size is currently not a field in the spec file. However, this seems like a useful feature. Let us know what you think?\nThanks @tinateng \n@leoRoss & @danIavine\nCF Community Pair\n. @tinateng Perhaps you forgot to push your latest change to this commit?\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng Perhaps you forgot to push your latest change to this commit?\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng You need to make the new code changes on the patch-1 branch, commit, and push. Git will take care of updating the pull request for you.\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng You need to make the new code changes on the patch-1 branch, commit, and push. Git will take care of updating the pull request for you.\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng All good! Both commits are in this pull request :)\n@goehmen We've moved this to your icebox\nThanks,\nCommunity Pair\n@leoRoss & @DanLavine\n. @drnic We reproduced both scenarios (normal and URL uploads) and achieved the same results. \n@goehmen Should we move this issue to the Bosh Tracker?\nCF Community Pair\n@leoRoss & @DanLavine\n. @drnic We reproduced both scenarios (normal and URL uploads) and achieved the same results. \n@goehmen Should we move this issue to the Bosh Tracker?\nCF Community Pair\n@leoRoss & @DanLavine\n. Hi @tinateng, please append this commit to #599 and close this one. We try to ensure that each pull request is stand alone.\nCF Community Pair\n@leoRoss & @DanLavine\n. Hi @tinateng, please append this commit to #599 and close this one. We try to ensure that each pull request is stand alone.\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng You need to make the new code changes on the #599 branch, commit, and push. Git will take care of updating the pull request for you.\nCF Community Pair\n@leoRoss & @DanLavine\n. @tinateng You need to make the new code changes on the #599 branch, commit, and push. Git will take care of updating the pull request for you.\nCF Community Pair\n@leoRoss & @DanLavine\n. @goehmen This pull requests seems reasonable and matches a pre-existing pattern.\nWe've moved it to the Bosh Tracker.\nThanks @dengwa \n@leoRoss & @DanLavine\nCF Community Pair\n. ",
    "tnaoto": "Hello @calebamiles\nThis is My Openstack.\n\nMyMac and pub-net are same subnet.\nI thought it openstack environment of a standard. \nIn this environment, the deployment will fail to complete startup is not able to confirm \nthat it does not put the ip a ip-reachable in micro Bosh you have deployed.\nI think it is possible to avoid this problem by setting the vip before.\n. ",
    "abylaw": "\nstartup is not able to confirm that it does not put the ip a ip-reachable in micro Bosh you have deployed.\n\nCan you elaborate on this a bit more? What exactly is going wrong? Do you have any logs from your deployment that we can see?\n@abylaw and @adamstegman \n. @dengwa all but one test in that file stub out network addresses finding to test ip selection logic. test in question tries to test IPResolver at a higher level (an integration test) to make sure that class works when injected with real network addr finding function. we do not have a separate integration test suite for go agent (only bats test go agent at such level, but bat tests are expensive), so that's why this test is in the unit test suite. in future if we have more integration tests like this we will split them into a separate test suite (which will take a config file).\nusing dynamic search approach in the test itself is not very useful since that's what implementation does, so proposed config file approach is the best way to go for now.\n- @cppforlife & @abylaw\n. The agent has already been bumped on develop to the same commit (a807749b848bfafd595ae607ce0506a8a2a8d820). \nThanks,\n@abylaw & @xtreme-andrei-dinin \n. @voelzmo: This has been pulled into develop as f3d5434de887a2fd700343e96a112885b383cf53. Thanks!\n. Tracked by story https://www.pivotaltracker.com/story/show/79805910. Waiting for CI to go green before merging in.\n. @voelzmo, this has been merged into develop as 329511bf4b82df33e59ea5b3701ae5c9c0ad3663. Thanks!\n. Hey, it seems like I was too hasty in making this pull request. The new fix should work; we have tested it on aws.\n. Yes, that's fine.\n. It is.\n. ",
    "tinateng": "default value of blobstore.max_upload_size is added to release/jobs/blobstore/spec.\n. how to append the commit in #601 to #599?\n. Hi @leoRoss & @DanLavine,\nI\u2019ve closed #601.\nHow to modify #599 to include this commit?\n@tinateng\nFrom: Leo Rossignac-Milon [mailto:notifications@github.com]\nSent: Tuesday, July 01, 2014 2:33 PM\nTo: cloudfoundry/bosh\nCc: Teng, Ying\nSubject: Re: [bosh] Update spec (#601)\nHi @tinatenghttps://github.com/tinateng, please append this commit to #599https://github.com/cloudfoundry/bosh/pull/599 and close this one. We try to ensure that each pull request is stand alone.\nCF Community Pair\n@leoRosshttps://github.com/leoRoss & @DanLavinehttps://github.com/DanLavine\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/601#issuecomment-47712521.\n. ",
    "johnmcteague": "On a fresh microbosh with no previous releases I am also unable to upload cf-173. I'm not sure why cf-173 is so much larger, assuming its not a one-off issue with that release I am faced with having to manually adjust the microbosh vmto perform a cf release.\n. Prior to increasing the disk to 100GB I had experimented with different RAM sizes. 28GB seemed to be the upper limit before I had to increase the disk\n. The /var/vcap/bosh/log/current file is as follows:\n2014-11-21_11:09:03.86493 [main] 2014/11/21 11:09:03 DEBUG - Starting agent\n2014-11-21_11:09:03.86498 [File System] 2014/11/21 11:09:03 DEBUG - Reading file /var/vcap/bosh/agent.json\n2014-11-21_11:09:03.86582 [File System] 2014/11/21 11:09:03 DEBUG - Read content\n2014-11-21_11:09:03.86584 ********************\n2014-11-21_11:09:03.86584 {\n2014-11-21_11:09:03.86585   \"Platform\": {\n2014-11-21_11:09:03.86586     \"Linux\": {\n2014-11-21_11:09:03.86586       \"CreatePartitionIfNoEphemeralDisk\": true\n2014-11-21_11:09:03.86587     }\n2014-11-21_11:09:03.86587   },\n2014-11-21_11:09:03.86588   \"Infrastructure\" : {\n2014-11-21_11:09:03.86588     \"MetadataService\": {\n2014-11-21_11:09:03.86589       \"UseConfigDrive\": true\n2014-11-21_11:09:03.86589     }\n2014-11-21_11:09:03.86590   }\n2014-11-21_11:09:03.86590 }\n2014-11-21_11:09:03.86591\n2014-11-21_11:09:03.86591 ********************\n2014-11-21_11:09:03.86624 [Cmd Runner] 2014/11/21 11:09:03 DEBUG - Running command: bosh-agent-rc\n2014-11-21_11:09:03.87627 [Cmd Runner] 2014/11/21 11:09:03 DEBUG - Stdout:\n2014-11-21_11:09:03.87629 [Cmd Runner] 2014/11/21 11:09:03 DEBUG - Stderr:\n2014-11-21_11:09:03.87630 [Cmd Runner] 2014/11/21 11:09:03 DEBUG - Successful: true (0)\n2014-11-21_11:09:03.87643 [ConfigDriveMetadataService] 2014/11/21 11:09:03 DEBUG - Loading config drive metadata service\n2014-11-21_11:09:03.87644 [File System] 2014/11/21 11:09:03 DEBUG - Checking if file exists /dev/disk/by-label/CONFIG-2\n2014-11-21_11:09:03.87674 [ConfigDriveMetadataService] 2014/11/21 11:09:03 WARN - Failed to load config from /dev/disk/by-label/CONFIG-2 - Reading files on config drive: Failed to get file contents, disk path '/dev/disk/by-label/CONFIG-2' does not exist\n2014-11-21_11:09:03.87678 [File System] 2014/11/21 11:09:03 DEBUG - Checking if file exists /dev/disk/by-label/config-2\n2014-11-21_11:09:03.87679 [ConfigDriveMetadataService] 2014/11/21 11:09:03 WARN - Failed to load config from /dev/disk/by-label/config-2 - Reading files on config drive: Failed to get file contents, disk path '/dev/disk/by-label/config-2' does not exist\n2014-11-21_11:09:03.87680 [File System] 2014/11/21 11:09:03 DEBUG - Checking if file exists /dev/disk/by-label/METADATA\n2014-11-21_11:09:03.87684 [ConfigDriveMetadataService] 2014/11/21 11:09:03 WARN - Failed to load config from /dev/disk/by-label/METADATA - Reading files on config drive: Failed to get file contents, disk path '/dev/disk/by-label/METADATA' does not exist\n2014-11-21_11:09:04.54353 [File System] 2014/11/21 11:09:04 DEBUG - Getting HomeDir for vcap\n2014-11-21_11:09:04.54636 [File System] 2014/11/21 11:09:04 DEBUG - HomeDir is /home/vcap\n2014-11-21_11:09:04.54638 [File System] 2014/11/21 11:09:04 DEBUG - Making dir /home/vcap/.ssh with perm 448\n2014-11-21_11:09:04.55012 [File System] 2014/11/21 11:09:04 DEBUG - Chown /home/vcap/.ssh to user vcap\n2014-11-21_11:09:04.55014 [File System] 2014/11/21 11:09:04 DEBUG - Writing /home/vcap/.ssh/authorized_keys\n2014-11-21_11:09:04.55015 [File System] 2014/11/21 11:09:04 DEBUG - Making dir /home/vcap/.ssh with perm 511\n2014-11-21_11:09:04.55015 [File System] 2014/11/21 11:09:04 DEBUG - Write content\n2014-11-21_11:09:04.55016 ********************\n2014-11-21_11:09:04.55016 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAxYLQpxbMo8o5Kb6nXw7cwzptJscKZ1WcNleY2tspTkL9BBi72jbBvdCv0xvuDcsPmOrETFp5frv3wBjqLSj1kWATkelIKuMMSAwCxTtVajyyLPxstRf7Z5RI2g9pU2NIj1xtgpMcPvfYdTgrAmKp0/TL7feQfTXwOxnADCnwVrC0yXHX7Eny7l+PqZYHNyJwIzGvZfLbm+CAhFq39GACuddHTp1UxKEHelSaWfNksquxso5PRJGfhv9kxKtsHr562ba9EgaVbKRYW16GdTdE9tYD/psCWJq/MC3M8GIdyGYTeukC6aIfSonYLRHvquxjEz6WjJZicwTEsj5ex34B8Q== cmp-lab-01\n2014-11-21_11:09:04.55020 ********************\n2014-11-21_11:09:04.55020 [File System] 2014/11/21 11:09:04 DEBUG - Chown /home/vcap/.ssh/authorized_keys to user vcap\n2014-11-21_11:09:04.55021 [File System] 2014/11/21 11:09:04 DEBUG - Chmod /home/vcap/.ssh/authorized_keys to 384\n2014-11-21_11:09:04.55021 [concreteService] 2014/11/21 11:09:04 DEBUG - Loading settings from fetcher\n2014-11-21_11:09:08.07379 [concreteService] 2014/11/21 11:09:08 DEBUG - Successfully received settings from fetcher\n2014-11-21_11:09:08.07389 [File System] 2014/11/21 11:09:08 DEBUG - Writing /var/vcap/bosh/settings.json\n2014-11-21_11:09:08.07390 [File System] 2014/11/21 11:09:08 DEBUG - Making dir /var/vcap/bosh with perm 511\n2014-11-21_11:09:08.07391 [File System] 2014/11/21 11:09:08 DEBUG - Write content\n2014-11-21_11:09:08.07392 ********************\n2014-11-21_11:09:08.07392 {\"agent_id\":\"2c88dc0b-45f1-44f9-a802-37fec272ea1a\",\"blobstore\":{\"provider\":\"dav\",\"options\":{\"endpoint\":\"http://172.16.1.4:25250\",\"password\":\"agent\",\"user\":\"agent\"}},\"disks\":{\"system\":\"/dev/sda\",\"ephemeral\":\"\",\"persistent\":{}},\"env\":{\"bosh\":{\"password\":\"\"}},\"networks\":{\"cf-core\":{\"type\":\"\",\"ip\":\"172.16.3.51\",\"netmask\":\"255.255.255.0\",\"gateway\":\"172.16.3.1\",\"default\":[\"dns\",\"gateway\"],\"dns\":[\"172.16.1.4\"],\"mac\":\"\"}},\"ntp\":[\"0.north-america.pool.ntp.org\",\"1.north-america.pool.ntp.org\"],\"mbus\":\"nats://nats:nats@172.16.1.4:4222\",\"vm\":{\"name\":\"vm-e44c06d4-c5e1-4761-86c3-3c0ba1674c0e\"}}\n2014-11-21_11:09:08.07397 ********************\n2014-11-21_11:09:08.07397 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Running command: hostname 2c88dc0b-45f1-44f9-a802-37fec272ea1a\n2014-11-21_11:09:08.07527 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stdout:\n2014-11-21_11:09:08.07529 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stderr:\n2014-11-21_11:09:08.07531 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Successful: true (0)\n2014-11-21_11:09:08.07531 [File System] 2014/11/21 11:09:08 DEBUG - Writing /etc/hostname\n2014-11-21_11:09:08.07533 [File System] 2014/11/21 11:09:08 DEBUG - Making dir /etc with perm 511\n2014-11-21_11:09:08.07664 [File System] 2014/11/21 11:09:08 DEBUG - Write content\n2014-11-21_11:09:08.07667 ********************\n2014-11-21_11:09:08.07668 2c88dc0b-45f1-44f9-a802-37fec272ea1a\n2014-11-21_11:09:08.07668 ********************\n2014-11-21_11:09:08.07669 [File System] 2014/11/21 11:09:08 DEBUG - Writing /etc/hosts\n2014-11-21_11:09:08.07669 [File System] 2014/11/21 11:09:08 DEBUG - Making dir /etc with perm 511\n2014-11-21_11:09:08.07670 [File System] 2014/11/21 11:09:08 DEBUG - Write content\n2014-11-21_11:09:08.07670 ********************\n2014-11-21_11:09:08.07671 127.0.0.1 localhost 2c88dc0b-45f1-44f9-a802-37fec272ea1a\n2014-11-21_11:09:08.07672\n2014-11-21_11:09:08.07672 # The following lines are desirable for IPv6 capable hosts\n2014-11-21_11:09:08.07673 ::1 localhost ip6-localhost ip6-loopback 2c88dc0b-45f1-44f9-a802-37fec272ea1a\n2014-11-21_11:09:08.07673 fe00::0 ip6-localnet\n2014-11-21_11:09:08.07674 ff00::0 ip6-mcastprefix\n2014-11-21_11:09:08.07674 ff02::1 ip6-allnodes\n2014-11-21_11:09:08.07677 ff02::2 ip6-allrouters\n2014-11-21_11:09:08.07678 ff02::3 ip6-allhosts\n2014-11-21_11:09:08.07678\n2014-11-21_11:09:08.07679 ********************\n2014-11-21_11:09:08.07679 [ubuntuNetManager] 2014/11/21 11:09:08 DEBUG - Configuring DHCP networking\n2014-11-21_11:09:08.07680 [File System] 2014/11/21 11:09:08 DEBUG - Glob '/sys/class/net/*'\n2014-11-21_11:09:08.07680 [File System] 2014/11/21 11:09:08 DEBUG - Checking if file exists /sys/class/net/eth0/device\n2014-11-21_11:09:08.07681 [File System] 2014/11/21 11:09:08 DEBUG - Checking if file exists /sys/class/net/lo/device\n2014-11-21_11:09:08.07682 [ubuntuNetManager] 2014/11/21 11:09:08 INFO - Ignoring virtual network device: /sys/class/net/lo\n2014-11-21_11:09:08.07683 [File System] 2014/11/21 11:09:08 DEBUG - File /etc/network/interfaces will be overwritten\n2014-11-21_11:09:08.07684 [File System] 2014/11/21 11:09:08 DEBUG - Making dir /etc/network with perm 511\n2014-11-21_11:09:08.07774 [File System] 2014/11/21 11:09:08 DEBUG - File /etc/dhcp/dhclient.conf will be overwritten\n2014-11-21_11:09:08.07778 [File System] 2014/11/21 11:09:08 DEBUG - Making dir /etc/dhcp with perm 511\n2014-11-21_11:09:08.07779 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Running command: ifup --version\n2014-11-21_11:09:08.07957 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stdout: ifup version 0.7.47.2ubuntu4.1\n2014-11-21_11:09:08.07959 Copyright (c) 1999-2009 Anthony Towns\n2014-11-21_11:09:08.07959 Copyright (c) 2010-2013 Andrew Shadura\n2014-11-21_11:09:08.07960\n2014-11-21_11:09:08.07962 This program is free software; you can redistribute it and/or modify\n2014-11-21_11:09:08.07962 it under the terms of the GNU General Public License as published by\n2014-11-21_11:09:08.07963 the Free Software Foundation; either version 2 of the License, or (at\n2014-11-21_11:09:08.07964 your option) any later version.\n2014-11-21_11:09:08.07964 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stderr:\n2014-11-21_11:09:08.07965 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Successful: true (0)\n2014-11-21_11:09:08.07965 [ubuntuNetManager] 2014/11/21 11:09:08 DEBUG - Restarting network interfaces\n2014-11-21_11:09:08.07966 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Running command: ifdown -a --no-loopback\n2014-11-21_11:09:08.33873 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stdout:\n2014-11-21_11:09:08.33880 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Stderr: Internet Systems Consortium DHCP Client 4.2.4\n2014-11-21_11:09:08.33881 Copyright 2004-2012 Internet Systems Consortium.\n2014-11-21_11:09:08.33882 All rights reserved.\n2014-11-21_11:09:08.33882 For info, please visit https://www.isc.org/software/dhcp/\n2014-11-21_11:09:08.33883\n2014-11-21_11:09:08.33883 Listening on LPF/eth0/fa:16:3e:58:d3:ff\n2014-11-21_11:09:08.33884 Sending on   LPF/eth0/fa:16:3e:58:d3:ff\n2014-11-21_11:09:08.33885 Sending on   Socket/fallback\n2014-11-21_11:09:08.33885 DHCPRELEASE on eth0 to 172.16.3.2 port 67 (xid=0x6fe0e085)\n2014-11-21_11:09:08.33886 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Successful: true (0)\n2014-11-21_11:09:08.33887 [Cmd Runner] 2014/11/21 11:09:08 DEBUG - Running command: ifup -a --no-loopback\n2014-11-21_11:09:32.20705 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Stdout:\n2014-11-21_11:09:32.20709 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Stderr: Internet Systems Consortium DHCP Client 4.2.4\n2014-11-21_11:09:32.20710 Copyright 2004-2012 Internet Systems Consortium.\n2014-11-21_11:09:32.20711 All rights reserved.\n2014-11-21_11:09:32.20711 For info, please visit https://www.isc.org/software/dhcp/\n2014-11-21_11:09:32.20712\n2014-11-21_11:09:32.20713 Listening on LPF/eth0/fa:16:3e:58:d3:ff\n2014-11-21_11:09:32.20714 Sending on   LPF/eth0/fa:16:3e:58:d3:ff\n2014-11-21_11:09:32.20714 Sending on   Socket/fallback\n2014-11-21_11:09:32.20715 DHCPDISCOVER on eth0 to 255.255.255.255 port 67 interval 3 (xid=0x7e3d45a8)\n2014-11-21_11:09:32.20716 DHCPREQUEST of 172.16.3.51 on eth0 to 255.255.255.255 port 67 (xid=0x7e3d45a8)\n2014-11-21_11:09:32.20716 DHCPOFFER of 172.16.3.51 from 172.16.3.2\n2014-11-21_11:09:32.20717 DHCPACK of 172.16.3.51 from 172.16.3.2\n2014-11-21_11:09:32.20718 bound to 172.16.3.51 -- renewal in 39440 seconds.\n2014-11-21_11:09:32.20719 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Successful: true (0)\n2014-11-21_11:09:32.20748 [File System] 2014/11/21 11:09:32 DEBUG - Writing /var/vcap/bosh/etc/ntpserver\n2014-11-21_11:09:32.20749 [File System] 2014/11/21 11:09:32 DEBUG - Making dir /var/vcap/bosh/etc with perm 511\n2014-11-21_11:09:32.20750 [File System] 2014/11/21 11:09:32 DEBUG - Write content\n2014-11-21_11:09:32.20750 ********************\n2014-11-21_11:09:32.20751 0.north-america.pool.ntp.org 1.north-america.pool.ntp.org\n2014-11-21_11:09:32.20753 ********************\n2014-11-21_11:09:32.20754 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Running command: ntpdate\n2014-11-21_11:09:32.20769 [arping] 2014/11/21 11:09:32 DEBUG - Broadcasting MAC addresses\n2014-11-21_11:09:32.20771 [File System] 2014/11/21 11:09:32 DEBUG - Checking if file exists /sys/class/net/eth0\n2014-11-21_11:09:32.20871 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Running command: arping -c 1 -U -I eth0 172.16.3.51\n2014-11-21_11:09:32.22901 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Stdout: ARPING 172.16.3.51 from 172.16.3.51 eth0\n2014-11-21_11:09:32.22920 Sent 1 probes (1 broadcast(s))\n2014-11-21_11:09:32.22930 Received 0 response(s)\n2014-11-21_11:09:32.22944 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Stderr:\n2014-11-21_11:09:32.22952 [Cmd Runner] 2014/11/21 11:09:32 DEBUG - Successful: true (0)\n2014-11-21_11:09:37.22956 [Cmd Runner] 2014/11/21 11:09:37 DEBUG - Running command: arping -c 1 -U -I eth0 172.16.3.51\n2014-11-21_11:09:37.24517 [Cmd Runner] 2014/11/21 11:09:37 DEBUG - Stdout: ARPING 172.16.3.51 from 172.16.3.51 eth0\n2014-11-21_11:09:37.24521 Sent 1 probes (1 broadcast(s))\n2014-11-21_11:09:37.24522 Received 0 response(s)\n2014-11-21_11:09:37.24527 [Cmd Runner] 2014/11/21 11:09:37 DEBUG - Stderr:\n2014-11-21_11:09:37.24531 [Cmd Runner] 2014/11/21 11:09:37 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.16911 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout:\n2014-11-21_11:09:42.16924 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.16926 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: false (1)\n2014-11-21_11:09:42.16932 [openstackInfrastructure] 2014/11/21 11:09:42 INFO - Ephemeral disk path is empty\n2014-11-21_11:09:42.16934 [linuxPlatform] 2014/11/21 11:09:42 INFO - Setting up ephemeral disk...\n2014-11-21_11:09:42.16935 [File System] 2014/11/21 11:09:42 DEBUG - Glob '/var/vcap/data/*'\n2014-11-21_11:09:42.17148 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Existing ephemeral mount `/var/vcap/data' is not empty. Contents: [/var/vcap/data/sys]\n2014-11-21_11:09:42.17309 [File System] 2014/11/21 11:09:42 DEBUG - Making dir /var/vcap/data with perm 488\n2014-11-21_11:09:42.17313 [linuxPlatform] 2014/11/21 11:09:42 INFO - Creating swap & ephemeral partitions on root disk...\n2014-11-21_11:09:42.17314 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Determining root device\n2014-11-21_11:09:42.17315 [File System] 2014/11/21 11:09:42 DEBUG - Reading file /proc/mounts\n2014-11-21_11:09:42.17317 [File System] 2014/11/21 11:09:42 DEBUG - Read content\n2014-11-21_11:09:42.17319 ********************\n2014-11-21_11:09:42.17320 rootfs / rootfs rw 0 0\n2014-11-21_11:09:42.17320 sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0\n2014-11-21_11:09:42.17321 proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0\n2014-11-21_11:09:42.17322 udev /dev devtmpfs rw,relatime,size=16463824k,nr_inodes=4115956,mode=755 0 0\n2014-11-21_11:09:42.17324 devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0\n2014-11-21_11:09:42.17325 tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3294896k,mode=755 0 0\n2014-11-21_11:09:42.17326 /dev/disk/by-uuid/a29d7e60-3549-4f44-bf59-2e858d6d806a / ext4 rw,relatime,data=ordered 0 0\n2014-11-21_11:09:42.17327 none /var/lib/ureadahead/debugfs debugfs rw,relatime 0 0\n2014-11-21_11:09:42.17331 none /sys/fs/cgroup tmpfs rw,relatime,size=4k,mode=755 0 0\n2014-11-21_11:09:42.17332 none /sys/fs/fuse/connections fusectl rw,relatime 0 0\n2014-11-21_11:09:42.17334 none /sys/kernel/debug debugfs rw,relatime 0 0\n2014-11-21_11:09:42.17335 none /sys/kernel/security securityfs rw,relatime 0 0\n2014-11-21_11:09:42.17336 none /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0\n2014-11-21_11:09:42.17337 none /run/shm tmpfs rw,nosuid,nodev,relatime 0 0\n2014-11-21_11:09:42.17338 none /run/user tmpfs rw,nosuid,nodev,noexec,relatime,size=102400k,mode=755 0 0\n2014-11-21_11:09:42.17339 none /sys/fs/pstore pstore rw,relatime 0 0\n2014-11-21_11:09:42.17340 rpc_pipefs /run/rpc_pipefs rpc_pipefs rw,relatime 0 0\n2014-11-21_11:09:42.17341\n2014-11-21_11:09:42.17342 ********************\n2014-11-21_11:09:42.17343 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Found root partition: `/dev/disk/by-uuid/a29d7e60-3549-4f44-bf59-2e858d6d806a'\n2014-11-21_11:09:42.17344 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: readlink -f /dev/disk/by-uuid/a29d7e60-3549-4f44-bf59-2e858d6d806a\n2014-11-21_11:09:42.17345 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: /dev/vda1\n2014-11-21_11:09:42.17347 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.17348 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.17350 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Symlink is: `/dev/vda1'\n2014-11-21_11:09:42.17352 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Found root device `/dev/vda'\n2014-11-21_11:09:42.17365 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Getting remaining size of `/dev/vda'\n2014-11-21_11:09:42.17369 [RootDevicePartitioner] 2014/11/21 11:09:42 DEBUG - Getting size of disk remaining after first partition\n2014-11-21_11:09:42.17389 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: parted -m /dev/vda unit B print\n2014-11-21_11:09:42.21088 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: BYT;\n2014-11-21_11:09:42.21098 /dev/vda:64424509440B:virtblk:512:512:msdos:Virtio Block Device;\n2014-11-21_11:09:42.21099 1:32256B:3071000063B:3070967808B:ext4::;\n2014-11-21_11:09:42.21100 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.21101 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.21110 [linuxPlatform] 2014/11/21 11:09:42 DEBUG - Calculating partition sizes of `/dev/vda', remaining size: 61353509376B\n2014-11-21_11:09:42.21211 [linuxPlatform] 2014/11/21 11:09:42 INFO - Partitioning root device `/dev/vda': [Type: swap, SizeInBytes: 30676754688]\n2014-11-21_11:09:42.21225 [linuxPlatform] 2014/11/21 11:09:42 INFO - Partitioning root device `/dev/vda': [Type: linux, SizeInBytes: 30676754688]\n2014-11-21_11:09:42.21235 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: parted -m /dev/vda unit B print\n2014-11-21_11:09:42.25167 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: BYT;\n2014-11-21_11:09:42.25180 /dev/vda:64424509440B:virtblk:512:512:msdos:Virtio Block Device;\n2014-11-21_11:09:42.25185 1:32256B:3071000063B:3070967808B:ext4::;\n2014-11-21_11:09:42.25187 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.25188 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.25189 [RootDevicePartitioner] 2014/11/21 11:09:42 DEBUG - Found partitions BYT;\n2014-11-21_11:09:42.25190 /dev/vda:64424509440B:virtblk:512:512:msdos:Virtio Block Device;\n2014-11-21_11:09:42.25191 1:32256B:3071000063B:3070967808B:ext4::;\n2014-11-21_11:09:42.25192 [RootDevicePartitioner] 2014/11/21 11:09:42 DEBUG - Current partitions: []disk.existingPartition{disk.existingPartition{Index:1, SizeInBytes:0xb70b4000, StartInBytes:0x7e00, EndInBytes:0xb70bbdff}}\n2014-11-21_11:09:42.25194 [RootDevicePartitioner] 2014/11/21 11:09:42 INFO - Creating partition 0 with start 3071000064B and end 33747754751B\n2014-11-21_11:09:42.25551 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: parted -s /dev/vda unit B mkpart primary 3071000064 33747754751\n2014-11-21_11:09:42.25666 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: arping -c 1 -U -I eth0 172.16.3.51\n2014-11-21_11:09:42.28513 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: ARPING 172.16.3.51 from 172.16.3.51 eth0\n2014-11-21_11:09:42.28518 Sent 1 probes (1 broadcast(s))\n2014-11-21_11:09:42.28519 Received 0 response(s)\n2014-11-21_11:09:42.28523 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.28527 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.30824 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: Warning: The resulting partition is not properly aligned for best performance.\n2014-11-21_11:09:42.30834 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.30838 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: true (0)\n2014-11-21_11:09:42.30844 [RootDevicePartitioner] 2014/11/21 11:09:42 INFO - Creating partition 1 with start 33747754752B and end 64424509439B\n2014-11-21_11:09:42.30866 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Running command: parted -s /dev/vda unit B mkpart primary 33747754752 64424509439\n2014-11-21_11:09:42.33058 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stdout: Error: You requested a partition from 33747754496B to 64424508928B.\n2014-11-21_11:09:42.33063 The closest location we can manage is 33747755008B to 64424508928B.\n2014-11-21_11:09:42.33064 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Stderr:\n2014-11-21_11:09:42.33066 [Cmd Runner] 2014/11/21 11:09:42 DEBUG - Successful: false (1)\n2014-11-21_11:09:42.33110 [main] 2014/11/21 11:09:42 ERROR - App setup Running bootstrap: Setting up ephemeral disk: Creating ephemeral partitions on root device: Partitioning root device `/dev/vda': Partitioning disk `/dev/vda': Running command: 'parted -s /dev/vda unit B mkpart primary 33747754752 64424509439', stdout: 'Error: You requested a partition from 33747754496B to 64424508928B.\n2014-11-21_11:09:42.33131 The closest location we can manage is 33747755008B to 64424508928B.\n2014-11-21_11:09:42.33142 ', stderr: '': exit status 1\n2014-11-21_11:09:42.35125 [main] 2014/11/21 11:09:42 DEBUG - Starting agent\nThe log then repeats the same issue once the agent restarts.\n. Im just about to kick off a rebuild of CF, will use 2870 and let you know how it goes.\n. Everything worked as expected, thanks.\n. Not perfect but will help my problem for now. Thanks.\n. Deployed BOSH on top of Microbosh using v2827. All works as expected, servers and storage are in different zones when I set ignore_server_availability_zone: true\nRunning with ignore_server_availability_zone: false gave me the desired outcome of an Openstack error.\n. ",
    "weilanwu": "I rebased the changes - please review and let me know?\nThanks\nWeilan\n. Due to complication caused by rebase and commit history. I have created a brand new branch and re-submit the patch (PR#607). Could we move the discussion to PR#607?\nThanks for the review.\n. To add on this patch - it doesn't look like go_agent/bin/test is part of travis CI build. After this patch, go_agent/bin/test should be included in the CI build (the more unit tests, the better). \n. travis build passed: https://travis-ci.org/HuaweiTech/bosh/builds/29107909\n. @DanLavine I updated the README.md. Could you please review? \nAlso the Travis-ci builds fail with some issues not related to the code patch - is there a way to restart the build, without a further check-in?\n. About the question: ' is there a way to restart the build, without a further check-in?'\nNever mind - this is a known issue on Travis, an enhancement request has been filed:\nhttps://github.com/travis-ci/travis-ci/issues/887\n. @DanLavine : travis build passed. should this PR be merged? (not sure what you mentioned 'bosh tracker' means?)\n. @cppforlife That's fair. I would avoid environment variables if there are alternatives. Look around the go_agent source tree, i see a similar JSON config file is already in use. How about we use the similar approach in the 'fixtures/dav-cli-config.json' - like creating another JSON config as 'fixtures/ip-config.json' and let the individual test code read this config ? (therefore avoid changing bin/test and passing arguments to ginkgo)\nThanks..\n. @cppforlife Regarding storing configuration in env vars, from research, I learned that 'twelve factors' actually mandate storing configuration in environment variables: http://12factor.net/config.\nSince this is test configuration, which used by unit test code, won't it be OK to store this information in environment, based on the above 'best practice'  (Although to avoid name confusion, we can change the name of the environment variable to 'TEST_PRIMARY_IPv4_INTERFACE' )?\n. When I refer to \u201cbest practices\u201d, I am referring the CloudFoundry documentation:\nThe following guidelines represent best practices for developing modern applications for cloud platforms. For more detailed reading about good app design for the cloud, see The Twelve-Factor Apphttp://www.12factor.net/.\nhttp://docs.cloudfoundry.org/devguide/deploy-apps/prepare-to-deploy.html\nI was trying to solve the problem at hand (fixing a broken unit tests), but didn\u2019t expect this exploration introduced so much controversial. \u263a\nThanks,\nWeilan\nFrom: Karl Isenberg [mailto:notifications@github.com]\nSent: Wednesday, July 09, 2014 9:50 PM\nTo: cloudfoundry/bosh\nCc: Weilan W Wu\nSubject: Re: [bosh] Added code to check environment variable defining primary nic (#608)\nThat was me, above. (Didn't know replying via email to the list was gonna post as rboshman.)\nEnvironment Variables are the globals of execution environments. Sure they're better than hard coding magic numbers, but they're still child's play compared to dependency injection.\nSorry to get on a soapbox, but hearing \"best-practices\" refereed to as mandates ruffles my feathers. They're not rules, they're recommendations based on shared experiences. Unfortunately they're heavy on the what, and light on the why. Give a man rules and he can build what you built. Teach a man to think and you hand him the tools to build the future.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/608#issuecomment-48564987.\n. When I refer to \u201cbest practices\u201d, I am referring the CloudFoundry documentation:\nThe following guidelines represent best practices for developing modern applications for cloud platforms. For more detailed reading about good app design for the cloud, see The Twelve-Factor Apphttp://www.12factor.net/.\nhttp://docs.cloudfoundry.org/devguide/deploy-apps/prepare-to-deploy.html\nI was trying to solve the problem at hand (fixing a broken unit tests), but didn\u2019t expect this exploration introduced so much controversial. \u263a\nThanks,\nWeilan\nFrom: Karl Isenberg [mailto:notifications@github.com]\nSent: Wednesday, July 09, 2014 9:50 PM\nTo: cloudfoundry/bosh\nCc: Weilan W Wu\nSubject: Re: [bosh] Added code to check environment variable defining primary nic (#608)\nThat was me, above. (Didn't know replying via email to the list was gonna post as rboshman.)\nEnvironment Variables are the globals of execution environments. Sure they're better than hard coding magic numbers, but they're still child's play compared to dependency injection.\nSorry to get on a soapbox, but hearing \"best-practices\" refereed to as mandates ruffles my feathers. They're not rules, they're recommendations based on shared experiences. Unfortunately they're heavy on the what, and light on the why. Give a man rules and he can build what you built. Teach a man to think and you hand him the tools to build the future.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/608#issuecomment-48564987.\n. Hi, after review and experimenting, I realize we can resolve this without introducing the env var or even configuration. Basically we can fix the unit test code without user input.\nSo, we just need to return an interface name, with a resolvable IP address in the following function:\n```\nDescribe(\"GetPrimaryIPv4\", func() {\n                findInterfaceName := func() string {\n                         if , err := gonet.InterfaceByName(\"en0\"); err == nil {\n                                  return \"en0\"\n                 } else if , err := gonet.InterfaceByName(\"eth0\"); err == nil {\n                                 return \"eth0\"\n                  } else if _, err := gonet.InterfaceByName(\"venet0\"); err == nil {\n                                // Travis CI uses venet0 as primary network interface\n                              return \"venet0\"\n                  }\n            panic(\"Not sure which interface name to use: en0 and eth0 are not found\")\n    }\n\n```\nInstead of iterating hardcoded interface names ('en0', 'eth0', 'venet0'), we can call gonet.Interfaces(), and examine each interface returned, and pick one can be resolved to a valid IPv4 address. Here is the new implementation I come up with:\nDescribe(\"GetPrimaryIPv4\", func() {\n            findInterfaceName := func() string {\n                    interfaces, err := gonet.Interfaces()\n                    if err != nil {\n                            panic(\"Unable to get interfaces\")\n                    }\n                    for _, inter := range interfaces {\n                            addrs, err := inter.Addrs()\n                            if err != nil || len(addrs) == 0 {\n                                    continue\n                            }\n                            return inter.Name\n                    }\n                    panic(\"No interface with Ipv4 address(es) can be found\")\n            }\nAnd I have tested locally and it works. \nSo, if this looks OK, i can resubmit a new PR, and see how we can merge the code? (I believe this new unit test is robust, since it doesnt rely on hardcoded interface names any more).\n. Hi, after review and experimenting, I realize we can resolve this without introducing the env var or even configuration. Basically we can fix the unit test code without user input.\nSo, we just need to return an interface name, with a resolvable IP address in the following function:\n```\nDescribe(\"GetPrimaryIPv4\", func() {\n                findInterfaceName := func() string {\n                         if , err := gonet.InterfaceByName(\"en0\"); err == nil {\n                                  return \"en0\"\n                 } else if , err := gonet.InterfaceByName(\"eth0\"); err == nil {\n                                 return \"eth0\"\n                  } else if _, err := gonet.InterfaceByName(\"venet0\"); err == nil {\n                                // Travis CI uses venet0 as primary network interface\n                              return \"venet0\"\n                  }\n            panic(\"Not sure which interface name to use: en0 and eth0 are not found\")\n    }\n\n```\nInstead of iterating hardcoded interface names ('en0', 'eth0', 'venet0'), we can call gonet.Interfaces(), and examine each interface returned, and pick one can be resolved to a valid IPv4 address. Here is the new implementation I come up with:\nDescribe(\"GetPrimaryIPv4\", func() {\n            findInterfaceName := func() string {\n                    interfaces, err := gonet.Interfaces()\n                    if err != nil {\n                            panic(\"Unable to get interfaces\")\n                    }\n                    for _, inter := range interfaces {\n                            addrs, err := inter.Addrs()\n                            if err != nil || len(addrs) == 0 {\n                                    continue\n                            }\n                            return inter.Name\n                    }\n                    panic(\"No interface with Ipv4 address(es) can be found\")\n            }\nAnd I have tested locally and it works. \nSo, if this looks OK, i can resubmit a new PR, and see how we can merge the code? (I believe this new unit test is robust, since it doesnt rely on hardcoded interface names any more).\n. ",
    "phanle": "Hey @weilanwu, @DanLavine and @leoRoss,\nI am pretty sure that PRs for BOSH are supposed to be rebased against developed, not master.\n. We think that it is better to change in our stemcell rather than having a ctl script to change our system wide settings.\n@abylaw && @phanle\n. We think that it is better to change in our stemcell rather than having a ctl script to change our system wide settings.\n@abylaw && @phanle\n. Pulled in to develop\n. Pulled in to develop\n. Pulled to develop\n. Pulled to develop\n. Pushed to develop branch.\n. Pushed to develop branch.\n. Pulled into develop.\n. Pulled into develop.\n. ",
    "dengwa": "@karlkfi does the solution proposed by Weilan ok to go \"call gonet.Interfaces(), and examine each interface returned, and pick one can be resolved to a valid IPv4 address\"?\n. @DanLavine  Good point!  we will add unit tests.\n. @DanLavine unit tests are added.\n. @DanLavine The failed travis-ci is not caused by the code change.  The travis-ci is not stable.\n. @mariash  We will take care of the refactoring.\n. @mariash refactoring is done.\n. @cppforlife code changes based on your review suggestions are checked in.\n. @krishicks code message changes are updated and checked in.  test run locally is fine.  But travis build has its own issue, not these message changes related.\n. @adamstegman  Thank you for your good suggestion!  When will this PR be prioritized.  I has been there for more then a month now.  The problem I have with making more changes, even adding test code is that, BOSH code based has been change quickly.  Instead of making the changes and then we have to doing rebase again and again.  Would you let us know when the PR is prioritized.  We will add the test code.\nBTW,  recently,  the go_agent directory is replace with the go submodule.  For this PR,  we used to have changes in go agent area (two *.go files), two files.  With go_agent directory is gone.  What is the process of rebasing those changes?   Thank you!\n. @rboshman Ok, I will leave the two go files under go_agent in hw-issue-31 branch.\n. @mariash No, this can not be moved out of the loop because only two of the cases doing this check. other cases checks differnt thing.\n. ",
    "liuhewei": "@karlkfi Thanks for your works. Finally :-) We also hope BOSH versioning can be easier in the future. \n. @krishicks Tests are added as suggested, thanks.\n@dengwa \n. ",
    "wallyqs": "+1\n. ",
    "dkoper": "tests are passing now. Are there any tests in this area that you want me to modify/add to? Where? @cppforlife \n. I discussed this change with @cppforlife a few days ago. We need a different change:\ntar on Mac's doesn't support the --force-local option. He suggested I change the logic to only inject the option if on Windows. I plan to work on that next week.\n. that looks better, thanks.\n. Thanks for the feedback!\nAs you've noticed I was very conservative in my patch: no change of box, only propagate http_proxy if set, only propagate https_proxy if set, etc. I wanted to make it future-proof by not hard-coding the /etc/default/docker path (which is ubuntu specific?).\nSo in your patch, the docker config file gets created before docker is installed, and as long as the docker installation process doesn't overwrite it and merges in possible settings it deems required, it should work fine, right?\nThe BOSH team can better judge, but if no OS change is on the roadmap, the alternatives you described may be preferable. I noticed there are a number of ubuntu docker images (e.g. williamyeh/ubuntu-trusty64-docker), not sure if we want to rely on a non-official image that has no track record yet of being continuously maintained and updated - as you can see from Travis, there are enough things already that can go wrong in the current test process.\n. @xian, I have confirmed 216a1c3 works too.\nNot sure why you bothered - common unix commands such as curl don't support uppercase HTTP_PROXY so most people specify the proxy vars in lowercase as that's accepted by all tools.\n. I should add, that although in your patch you pass in the proxy vars only in uppercase to the docker run, this works because curl is currently only used to download using HTTPS (curl supports HTTPS_PROXY, it's just HTTP_PROXY that it doesn't).\nIf someone ever adds a curl statement that downloads using HTTP, this will break.\n. Why not use pool.ntp.org (0.pool.ntp.org, 1.pool.ntp.org, etc)? They \"will usually return IP addresses for servers in or close to your country. For most users this will give the best results. \" according to http://www.pool.ntp.org/en/use.html. We can still include the comment about editing it for your region.\n. ",
    "afritzler": "Issue was due to an error on our side. \n. ",
    "ronakbanka": "@neilAitken  Just curious if the user in microbosh deployment cloud property is member or admin of both the tenants (Projects)?? \n. @cppforlife In our Environment we are using much lower timeout of 150 . I used 150 just keeping in mind some release operation on db side. \nBut my main reason to put 3600 as default is most people don't face a db connection break if they are using a dedicated database. We are using a Postgres HA cluster. So there can be a session break on pooling side during operation.\n. @frodenas sure We will do it  :)\n. @cppforlife Hi should I open a issue request for the same to add it to tracker story or just this PR is enough?\n. @liuxiaoxi2237 Yeah David by default there are no provisioned services, you can either use contrib services or even docker bosh release to provision services for your cloudfoundry deployment.\nThanks\nRonak\n. @liuxiaoxi2237 I don't think there should be any hard requirement of bosh version to deploy the services. You can try with this version itself . If any issues feel free to put it out here.\n. @voelzmo : We are using omkafka package to forward syslog stream to Kafka brokers , so instead of setting a syslog endpoint we add the custom rule with configuration.\nReference: http://www.rsyslog.com/doc/master/configuration/modules/omkafka.html\n. @voelzmo we are facing similar issue , agent spec on director inside director db is correct but bosh agent confuses while reporting back to the director . \nIf you restart the agent on vm itself , ip out of 2 nics it randomly reports duplicated ip.\nFor now we are using kind of old version of stemcell 2978 (works fine ),but when tried with version 3033 we saw this thing. trying to narrow down what can be the issue.\n. @cppforlife @voelzmo Any update when development on above added story will start or is it already under development?\n. @geofffranks we are a facing similar issue, any proper way to tackle this error?. @geofffranks That worked! Thanks. Maybe you can close this ticket. \nJust adding this link as a reference for others https://bosh.io/docs/migrated-from.html#migrate. How about logging the cloud_error here, in case volume is not found?\n. Logging before volume check ?? \n. Fixed\n. ",
    "wolfgangkirchler": "first of all, sorry for de delayed response, but I was on vacation :)\nI think the idea of making the timeout overridable is a good start, but does not solve the underlying problem. \nThe problem is that after a 'failed to accuire DRS lock' Bosh remains in an unconsistent state. The VM is already deleted in vCenter but a reference remains in the bosh database. The deletion of a VM should be done in one (logical) transaction so that eighter\n- the VM will not be deleted in case of a DRS lock error\n- or the reference to the VM will also be deleted in the Bosh database (even if a DRS lock error occured)\n. It works fine now, thanks a lot!!!\n. ",
    "techdragon": "The script has a bunch of other failures when you leave the default VPC in a region. But much of this seems to be related to the script not having a 'complete picture' of what it created and is working with vs the real state of the region, I have a suspicion that the script is being too 'naive' somewhere, ill give it another time tonight and record the trace result. But ill have to finish cleaning out everything in the region. I couldn't confirm or work out that the bosh AWS destroy only affects the one region configured in the bootstrap scripts.\n. Another update, after giving up on confirming the behaviour of the destroy command, and running bosh aws destroy then performing bosh aws create, I am sure the script is too naive about the content of an AWS account. After purging everything with the destroy command, the create worked The script is definitely unable to deal with having anything in the AWS account when it starts trying to create, and since all new AWS accounts start with default VPCs now, this is a problem. \n. ",
    "andyp1per": "So what's the solution? I see this too.\n. Thanks Dimitri. I was able to get it to work by creating a entirely new AWS account and starting from there. Somewhat inconvenient if you already have an account. I also found I could only create things is us-east, eu-west would not work.\n. ",
    "flavorjones": "Note that we ran this in staging and successfully saw the HM alerts in DataDog.\n. done, removed curlies as well\n. done\n. done and done\n. done\n. done\n. done\n. done\n. done\n. this should watch HM for a matching alert\n. ",
    "voelzmo": "@abylaw a green CI build makes sense ;) However, I cannot find the reason for the failing build in the logs - is it related to my content? Did I break anything?\n. Whoops, my bad. I'll have a look at it on Monday, sorry for that. I didn't\nrun the gocd tests so far, thanks for the hint.\nOn Sat, Oct 18, 2014 at 2:34 AM, Karl Isenberg notifications@github.com\nwrote:\n\nThere's a lot of other places where stemcell:build is called in bosh-dev\n(mostly specs) that will cause this to fail. Did you try to run the\nunit/integration tests against this?\nTry the containerized tests:\nhttps://github.com/cloudfoundry/bosh/tree/master/gocd\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/676#issuecomment-59592456.\n. @karlkfi So I've run the containerized tests and they pass without any additional changes. I don't know if there are additional modifications necessary for this change to pass.\n. Actually, for me even the Dockerized tests are broken. Did you try and create your vagrant&docker machines from scratch? If I follow the description at https://github.com/cloudfoundry/bosh/tree/master/gocd, the test fails with the exact same error as in Travis.\n\nMaybe I didn't understand the setup correctly, but after doing \ndocker run -t -i -v $(pwd):/opt/bosh bosh/integration /bin/bash\ncd /opt/bosh\ngit config --global -l\nI don't see user.name and user.email being configured.\nDid you update the image at https://registry.hub.docker.com/u/bosh/integration/ after adding the email and user to the Dockerfile?\n. Dear @dkoper, this looks really nice - I was actually doing the same thing over here. Just one more thing: I saw you're adding to run vagrant reload so that pulling the bosh/integration docker image works for you.\nThere are two alternatives to this, both require less manual effort: \n- changing /etc/default/docker, e.g. by using a shell provider. \n- Use a base image with docker already installed. Then vagrant-proxyconf will automatically configure the proxy for docker. Unfortunately, this doesn't work if one relies on the automatic installation of docker during provisioning.\nIf no vagrant reload is necessary, we could even pull the docker image automatically, instead of having to enter this every time, you can have a look at my take on this here: https://github.com/voelzmo/bosh/compare/vagrantfile_proxy\n. merged with commit https://github.com/cloudfoundry/bosh/commit/5a1a8dfc33543f95c59583b4226e454163575b1c\n. Fixed in bosh-v225, aka stable-3138\n. @poblin-orange the fix is applied in bosh CLI >=1.3138.0 so please update\n. This seems to happen when you use stemcell building machines created with https://github.com/cloudfoundry/bosh-packer-templates/tree/master/bosh-stemcell - is that what you are doing, @dwu-pivotal and @dthaluru? For now a workaround could be to build the stemcells on AWS with the images provided by pivotal as the readme in https://github.com/cloudfoundry/bosh/tree/master/bosh-stemcell describes.\n. I guess this should only be closed once the Virtualbox images have been re-built.\n. bump Can we re-build the virtualbox images please, @cppforlife?\n. bump Can we re-build the virtualbox images please, @cppforlife?\n. @dpb587-pivotal Haven't tried to build the stemcell from a docker image yet. From https://github.com/cloudfoundry/bosh/blob/master/bosh-stemcell/docker/Dockerfile#L10 it seems that the UID should be fine. The README could make it maybe clearer what the preferred way to build stemcells is and the Vagrantfile still contains the option to use the virtualbox image.\n. @cppforlife I knew this was how it would happen. ;)\n. Merged into develop with https://github.com/cloudfoundry/bosh/commit/ddfb6f13bec23aae099fb7d4ac9048b5478fb6a8\n. @wayneeseguin fixing that for all stemcells seems odd - one of the most common workflows I do is\nbosh ssh <machine>\nsudo su -\nmonit summary\nwhich works perfectly fine on ubuntu stemcells for me.\n. Yeah, sorry it is actually the other way around: The Openstack DHCP server configures the MTU to something like 1476 or even a bit less. That is necessary because tunneling over GRE tunnels adds an overhead of 24 bytes. \nAfter a restart, the machine defaults to an MTU of 1500 which results in non-functional overlay network.\nAbout those two changes you referred to:\nDoes that mean I can do manual networking, use static IPs, set use_dhcp: true and the DHCP server will assign the IP which I specified and additionally all other DHCP properties coming from Openstack, such as MTU and additional routes?\n. As far as I can tell there is no newer release than version 9 for the openstack-cpi so that functionality is not ready for consumption yet, right? I'll try to create my own final release and test if that fixes the problem\n. Found another bug in fog 1.34.0 which prevents me from actually deploying bosh with keystone V3 https://github.com/fog/fog/issues/3687\nEdit: Seems like this is actually expected behavior in fog: V2 'tenants' got renamed to 'projects' again in V3. Therefore, the parameters passed to fog calls should contain project_name rather than tenant_name if the URL is keystone V3.\nI'll prepare a PR to the openstack CPI for that tomorrow, so we can start the next round of testing :)\n. @rradecki are you trying to use bosh with a keystone V3 API? Then this currently doesn't work \u2013 even after updating fog \u2013 due to the mismatch of tenant and project parameters I mentioned in the above comment.\nOnce PR https://github.com/cloudfoundry-incubator/bosh-openstack-cpi-release/pull/10 gets pulled in, this might actually be fixed in the CPI coding. However, in order for this to be useable, we need additional changes in bosh itself, i.e. something like I proposed with PR https://github.com/cloudfoundry/bosh/pull/950.\nWe've added a stories to make the necessary changes in bosh-openstack-cpi-release (see https://www.pivotaltracker.com/story/show/104344048 and https://www.pivotaltracker.com/story/show/104344480). One those are done, we can do the remaining changes in bosh.\n. @jhiemer The last remaining item todo is currently https://www.pivotaltracker.com/story/show/105334890\nWe're currently setting up end-to-end integration tests to verify V3 support. We need to cut a new bosh release for this to be included, so stay tuned for one of the next releases.\n. @simonjohansson that looks pretty much like what bosh cleanup is actually doing: https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/maintenance.rb#L54-L81\n@Jonty I can see that having machine-parseable output for many bosh commands makes sense - especially for cases in which you are building functionality that bosh doesn't have and doesn't want to include.\nIn this case, however, I don't really see how bosh cleanup doesn't achieve what you want. Working with the output of a CLI is always error-prone and if you could rather switch to a native CLI command, I'd rather do that.\nThis is just my 2cents, @cppforlife feel free to jump in if you have a different opinion here.\n. @fabianschwarzfritz works nicely for me:\n``\n$ bosh ssh redis_leader_z1 0 --gateway_host <redacted>  --gateway_user vcap --gateway_identity_file <redacted> --default_password abc \"echo abc | sudo -S ls -l /\"\nActing as user 'admin' on deployment 'redis-aws-ec2' on 'my-bosh'\nExecutingecho abc | sudo -S ls -l /' on redis_leader_z1/0\nTarget deployment is `redis-aws-ec2'\nSetting up ssh artifacts\nDirector task 20\nTask 20 done\nredis_leader_z1/0\n[sudo] password for bosh_1aku9fikm: total 73\ndrwxr-xr-x   2 root root  4096 Oct  5 23:41 bin\ndrwxr-xr-x   3 root root  4096 Oct  5 23:44 boot\ndrwxr-xr-x  15 root root  4060 Oct 16 04:17 dev\ndrwxr-xr-x  83 root root  4096 Oct 16 04:26 etc\ndrwxr-xr-x   3 root root  4096 Oct  5 23:43 home\nlrwxrwxrwx   1 root root    33 Oct  5 23:42 initrd.img -> boot/initrd.img-3.19.0-30-generic\ndrwxr-xr-x  17 root root  4096 Oct  5 23:42 lib\ndrwxr-xr-x   2 root root  4096 Oct  5 23:40 lib64\ndrwx------   2 root root 16384 Oct 14 18:06 lost+found\ndrwxr-xr-x   2 root root  4096 Oct  5 23:39 media\ndrwxr-xr-x   2 root root  4096 Apr 10  2014 mnt\ndrwxr-xr-x   2 root root  4096 Oct  5 23:39 opt\ndr-xr-xr-x 100 root root     0 Oct 16 04:16 proc\ndrwx------   3 root root  4096 Oct 16 04:17 root\ndrwxr-xr-x  15 root root   620 Oct 16 04:26 run\ndrwxr-xr-x   2 root root  4096 Oct 14 18:07 sbin\ndrwxr-xr-x   2 root root  4096 Oct  5 23:39 srv\ndr-xr-xr-x  13 root root     0 Oct 16 04:16 sys\ndrwxrwx---   3 root vcap  1024 Oct 16 04:26 tmp\ndrwxr-xr-x  11 root root  4096 Oct 14 17:46 usr\ndrwxr-xr-x  12 root root  4096 Oct  5 23:42 var\nlrwxrwxrwx   1 root root    30 Oct  5 23:42 vmlinuz -> boot/vmlinuz-3.19.0-30-generic\nCleaning up ssh artifacts\nDirector task 21\nTask 21 done\n```\nIgnore the gateway arguments above \u2013 if you are not on AWS you most likely don't need a gateway to get to your job VM.\n. Fixed with openstack-cpi-release v16 and corresponding new versions of the other CPIs.\n. @cppforlife I agree that it doesn't make any sense to set dns.domain_name if rest of dns properties are not set. However, dns.domain_name has a default and is therefore always set: http://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh&version=218#p=dns.domain_name\nJust encountered the same thing when deploying my director with v218 using the default templates from bosh.io. I don't even co-locate the powerdns job.\ncc @bonzofenix \n. @hashmap using bosh-init on aws with the config from bosh.io, bosh v218, aws-cpi v35\nNote: \n- I don't use DNS, to I don't specify anything in the manifest\n- I don't co-locate the powerdns job\nAnd it probably doesn't fail with a dev build because of this change: https://github.com/cloudfoundry/bosh/commit/6b9ecf19d3b4dd0d54380fe5eda4f4e8c2312b07\n. bump any updates on this?\ncc @cppforlife \n. looking forward to this, it has been a while :). While looking more thorough through the logs I found the issue in this case:\n- The previous deploy failed because the compilation VM with that specific IP couldn't be brought up (IaaS timed out)\n- In the next deployment that IP was already taken. So it seems the IaaS brought up the VM after all.\nI'll re-open this when I encounter it again and have made sure that it is not an IaaS issue.\n. This is the run where the VM didn't come up. Some timeout, but seems like the VM did come up after all:\n``\nD, [2015-11-05T16:45:41.854837 #6623] DEBUG -- : excon.request {:chunk_size=>1048576, :ciphers=>\"HIGH:!SSLv2:!aNULL:!eNULL:!3DES\", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{\"User-Agent\"=>\"fog/1.34.0 fog-core/1.32.1\", \"Content-Type\"=>\"application/json\", \"Accept\"=>\"application/json\", \"X-Auth-Token\"=>\"bfc3ebe3500f4612ae55f735cc1d59c9\", \"Host\"=>\"<redacted>:<redacted>\"}, :idempotent=>false, :instrumentor_name=>\"excon\", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>false, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>\"excon/0.45.4 (x86_64-linux) ruby/2.1.7\", :write_timeout=>60, :host=>\"<redacted>\", :hostname=>\"<redacted>\", :path=>\"/v2/p-b1bd5299d/servers.json\", :port=><redacted>, :query=>nil, :scheme=>\"https\", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor, :body=>\"{\\\"server\\\":{\\\"flavorRef\\\":\\\"4\\\",\\\"imageRef\\\":\\\"03efa09d-7ab5-40e7-835c-278efbebbd38\\\",\\\"name\\\":\\\"vm-31448e72-1abd-41f3-9b2e-c00cb6d3c721\\\",\\\"availability_zone\\\":\\\"<redacted>\\\",\\\"user_data\\\":\\\"<redacted>\\\",\\\"key_name\\\":\\\"master\\\",\\\"security_groups\\\":[<redacted>],\\\"networks\\\":[{\\\"uuid\\\":\\\"c65e3621-26a4-4ecf-a7b7-9b626e2fcdc3\\\",\\\"fixed_ip\\\":\\\"10.3.4.42\\\"}]}}\", :expects=>[200, 202], :method=>\"POST\", :retries_remaining=>4, :connection=>#<Excon::Connection:7fb9740a0f70 @data={:chunk_size=>1048576, :ciphers=>\"HIGH:!SSLv2:!aNULL:!eNULL:!3DES\", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{\"User-Agent\"=>\"fog/1.34.0 fog-core/1.32.1\"}, :idempotent=>false, :instrumentor_name=>\"excon\", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>false, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>\"excon/0.45.4 (x86_64-linux) ruby/2.1.7\", :write_timeout=>60, :host=>\"<redacted>\", :hostname=>\"<redacted>\", :path=>\"\", :port=><redacted>, :query=>nil, :scheme=>\"https\", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor} @socket_key=\"https://<redacted>:<redacted>\">, :stack=>#<Excon::Middleware::ResponseParser:0x007fb9735bb6f0 @stack=#<Excon::Middleware::Expects:0x007fb9735bb718 @stack=#<Excon::Middleware::Idempotent:0x007fb9735bb7b8 @stack=#<Excon::Middleware::Instrumentor:0x007fb9735bb7e0 @stack=#<Excon::Middleware::Mock:0x007fb9735bb858 @stack=#<Excon::Connection:7fb9740a0f70 @data={:chunk_size=>1048576, :ciphers=>\"HIGH:!SSLv2:!aNULL:!eNULL:!3DES\", :connect_timeout=>60, :debug_request=>false, :debug_response=>true, :headers=>{\"User-Agent\"=>\"fog/1.34.0 fog-core/1.32.1\"}, :idempotent=>false, :instrumentor_name=>\"excon\", :middlewares=>[Excon::Middleware::ResponseParser, Excon::Middleware::Expects, Excon::Middleware::Idempotent, Excon::Middleware::Instrumentor, Excon::Middleware::Mock], :mock=>false, :nonblock=>true, :omit_default_port=>false, :persistent=>false, :read_timeout=>60, :retry_limit=>4, :ssl_verify_peer=>false, :tcp_nodelay=>false, :thread_safe_sockets=>true, :uri_parser=>URI, :versions=>\"excon/0.45.4 (x86_64-linux) ruby/2.1.7\", :write_timeout=>60, :host=>\"<redacted>\", :hostname=>\"<redacted>\", :path=>\"\", :port=><redacted>, :query=>nil, :scheme=>\"https\", :instrumentor=>Bosh::OpenStackCloud::ExconLoggingInstrumentor} @socket_key=\"https://<redacted>:<redacted>\">>>>>>}\nD, [2015-11-05T16:46:41.924745 #6623] DEBUG -- : excon.error {:error=>#<Excon::Errors::Timeout: read timeout reached>}\n, exit_status: pid 6614 exit 0\nE, [2015-11-05 16:46:41 #6591] [compile_package(ruby_2.1.5/a8612d8ed0e33940e9ae04dffd94edcbaa3de527, bosh-openstack-kvm-ubuntu-trusty-go_agent/3115)] ERROR -- DirectorJobRunner: error creating vm: Unknown CPI error 'Unknown' with message 'read timeout reached'\nD, [2015-11-05 16:46:41 #6591] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2015-11-05 16:46:41 #6591] [compile_package(ruby_2.1.5/a8612d8ed0e33940e9ae04dffd94edcbaa3de527, bosh-openstack-kvm-ubuntu-trusty-go_agent/3115)] DEBUG -- DirectorJobRunner: Deleting lock: lock:compile:90:1\nD, [2015-11-05 16:46:41 #6591] [compile_package(ruby_2.1.5/a8612d8ed0e33940e9ae04dffd94edcbaa3de527, bosh-openstack-kvm-ubuntu-trusty-go_agent/3115)] DEBUG -- DirectorJobRunner: Deleted lock: lock:compile:90:1\nD, [2015-11-05 16:46:41 #6591] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: Unknown CPI error 'Unknown' with message 'read timeout reached' - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:108:inhandle_error'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:89:in invoke_cpi_method'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:51:increate_vm'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/vm_creator.rb:41:in create'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/vm_creator.rb:9:increate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:153:in prepare_vm'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:80:inblock in compile_package'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock_helper.rb:48:in block in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock.rb:56:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock_helper.rb:48:in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:73:incompile_package'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:302:in block (2 levels) in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:50:intrack'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:300:in block in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:296:in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:256:inblock (4 levels) in compile_packages'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:77:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:77:inblock (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:63:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:63:inblock in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nD, [2015-11-05 16:46:41 #6591] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: Waiting for tasks to complete\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: Shutting down pool\nD, [2015-11-05 16:46:41 #6591] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:logsearch-apps\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:logsearch-apps\nI, [2015-11-05 16:46:41 #6591] [task:24]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"8b596f8b-4648-45ac-8118-d372aa7bd389\",\"severity\":3,\"title\":\"director - error during update deployment\",\"summary\":\"Error during update deployment for 'logsearch-apps' against Director '188bab23-4c96-4800-977e-8cafb93b2926': #\",\"created_at\":1446742001}\nE, [2015-11-05 16:46:41 #6591] [task:24] ERROR -- DirectorJobRunner: Unknown CPI error 'Unknown' with message 'read timeout reached'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:108:in handle_error'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:89:ininvoke_cpi_method'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_cpi-1.3123.0/lib/cloud/external_cpi.rb:51:in create_vm'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/vm_creator.rb:41:increate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/vm_creator.rb:9:in create'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:153:inprepare_vm'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:80:in block in compile_package'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock_helper.rb:48:inblock in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock.rb:56:in lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/lock_helper.rb:48:inwith_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:73:in compile_package'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:302:inblock (2 levels) in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:97:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:97:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/event_log.rb:50:in track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:300:inblock in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:296:inprocess_task'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3123.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:256:in block (4 levels) in compile_packages'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3123.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: (0.000477s) SELECT NULL\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: (0.000136s) BEGIN\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: (0.000700s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2015-11-05 16:46:41.981592+0000', \"description\" = 'create deployment', \"result\" = 'Unknown CPI error ''Unknown'' with message ''read timeout reached''', \"output\" = '/var/vcap/store/director/tasks/24', \"checkpoint_time\" = '2015-11-05 16:46:30.638838+0000', \"type\" = 'update_deployment', \"username\" = 'admin' WHERE (\"id\" = 24)\nD, [2015-11-05 16:46:41 #6591] [task:24] DEBUG -- DirectorJobRunner: (0.001995s) COMMIT\nI, [2015-11-05 16:46:41 #6591] []  INFO -- DirectorJobRunner: Task took 1 minute 11.362545248999993 seconds to process.\nTask 24 error\n```\nOpenstack cpi trace is\nD, [2015-11-05 16:46:41 #6591] [compile_package(common/9498cfd1def046e3e188d34bb67d284b8b60c4ea, bosh-openstack-kvm-ubuntu-trusty-go_agent/3115)] DEBUG -- DirectorJobRunner: External CPI got response: {\"result\":null,\"error\":{\"type\":\"Unknown\",\"message\":\"read timeout reached\",\"ok_to_retry\":false},\"log\":\"Rescued Unknown: read timeout reached. backtrace: /var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/socket.rb:299:in `raise_timeout_error'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/socket.rb:49:in `rescue in readline'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/socket.rb:44:in `readline'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/response.rb:53:in `parse'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/middlewares/response_parser.rb:6:in `response_call'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/connection.rb:372:in `response'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/excon-0.45.4/lib/excon/connection.rb:236:in `request'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/fog-core-1.32.1/lib/fog/core/connection.rb:81:in `request'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/fog-1.34.0/lib/fog/openstack/compute.rb:355:in `request'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/fog-1.34.0/lib/fog/openstack/requests/compute/create_server.rb:85:in `create_server'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/fog-1.34.0/lib/fog/openstack/models/compute/server.rb:368:in `save'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/fog-core-1.32.1/lib/fog/core/collection.rb:51:in `create'\n/var/vcap/packages/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb:311:in `block (2 levels) in create_vm'\n/var/vcap/packages/bosh_openstack_cpi/lib/cloud/openstack/helpers.rb:26:in `with_openstack'\n/var/vcap/packages/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb:311:in `block in create_vm'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/bosh_common-1.3093.0/lib/common/thread_formatter.rb:49:in `with_thread_name'\n/var/vcap/packages/bosh_openstack_cpi/lib/cloud/openstack/cloud.rb:216:in `create_vm'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/bosh_cpi-1.3093.0/lib/bosh/cpi/cli.rb:71:in `public_send'\n/var/vcap/packages/bosh_openstack_cpi/vendor/bundle/ruby/2.1.0/gems/bosh_cpi-1.3093.0/lib/bosh/cpi/cli.rb:71:in `run'\n/var/vcap/packages/bosh_openstack_cpi/bin/openstack_cpi:31:in `<main>'\"}\n. A more generic approach could also be to set export PKG_CONFIG_PATH=$(brew --prefix openssl)/lib/pkgconfig instead of setting bundler options. This should also install everything else that requires a link to openssl.\n. Has been fixed with eventmachine 1.0.9.1. Closing this issue.. @cppforlife @barthy1 haven't heard back from out security guys yet, can we wait until this is pulled in?\n. Any reason why you're not using light stemcells here? That seems to work for me.\n. btw: your manifest contains bosh-aws-xen-hvm-ubuntu-trusty-go_agent as name but the name in the db seems to be bosh-aws-xen-ubuntu-trusty-go_agent. That's why the first test doesn't find the stemcell but the second says it is already there. \nFrom the donwload URL it seems like you're using the xen variant \u2013 I don't even think there are full stemcells for the xen-hvm variant. Remove the -hvm from the name in the manifest and then it should work.\n@cppforlife seems like the CLI needs to check if the name in the manifest matches the one the user is trying to upload?\n. @mavenraven see https://github.com/cloudfoundry/bosh-init/issues/41 for the md5/sha1 mismatch.\n. @cunnie @yuzhangcmu Seems like this is fixed in the newer Bosh version. Tried to reproduce it with v250, but couldn't. So let's close this one, I'll let you know if I encounter it again. Thanks for investigating!\n. @mattcui out of curiosity: What are you putting on the root disk that you need a bigger one?\n. @cppforlife Just tried with bosh v255.3 and can confirm this is fixed and works out. Thanks!\n. @wcamarao yup, the rename seems weird an unexpected to me. Not sure why this happens, though. Fun fact: once you deployed successfully with the key being changed from on to true, it is impossible to update this deployment. The director tries to calculate a diff on the boolean true, which leads to a stacktrace.\nSo yes, I'd say the rename should not happen. Maybe this is too much of an edge-case to invest much effort in it, so depending on how hard it is to fix, I would decide. :)\n. @wcamarao making sure that the keys are strings seems to make sense to me.\nSome steps to reproduce what I described above:\n- deploy the above manifest with global properties looking like this:\nproperties:\n  what:\n    is:\n      going:\n        on: here\nNote: I saw the deploy fail with recent stemcells with this error\nError 100: undefined method `to_sym' for true:TrueClass\nHowever, I was experimenting with older stemcells at that point, and e.g. with v3140 I could actually deploy it. Maybe due to another ruby version? \n. @dpb587-pivotal seems like this works out now. Thanks for the reminder.\n. Hey @ronakbanka thanks for the PR. Out of curiosity: What is it that you want to forward but isn't covered by the current settings? \n. Hi @CAFxX thanks for the suggestion. We're currently implementing a feature in the OpenStack CPI that allows to switch to human-readable VM names (i.e. jobname_zone/index), as the AWS CPI currently offers. See stories tagged with better_vm_names in our backlog. Due to bosh internals, this is unfortunately not a trivial task. \nOnce this is done, we can think about additional features, such as a user-defined template for naming. To get a better idea about what you are suggesting, a few questions:\n- Why would you want the IP in the VM name? Don't basically all tools show you name and IP side-by-side?\n- What does role in your above example mean? Are you referring to a jobname, such as nats or runner, or is it something different?\n- Would your naming scheme be the same across all deployments deployed with the same BOSH Director? In your example you include cf- as a prefix, so I assume this would be different for each deployment?\n- BOSH uses different names/identifiers for the VMs, so if you execute bosh vms or bosh instances in the commandline, you would see an entirely different list than you would see in your IaaS management UI. Is this an issue for your operators? How do they find the VM in the IaaS given an identifier from the BOSH commandline?\n. @CAFxX I'd be happy to help out with that feature on OpenStack, so could you provide some more input on the above questions? We can also continue the conversion via mail or slack, if that is more convenient for you.\n. Hallo Tyler, mir geht es gut, danke. Wie geht es dir? Ich hoffe, du \u00fcbst flei\u00dfig dein Deutsch?\nI guess we can adapt the CPI to put MACs in the agent settings. However, I still have the feeling that your description of GetSettings() seems a little weird. Having two networks using DHCP without defining a MAC address works (because the agent doesn't have to configure them) on the VM, as the output of ip address show shows. The agent is just confused what to report, which seems weird.\nSo are you looking into fixing what the agent reports? I can take care of the OpenStack CPI network side of things.\n. I guess the problem in the agent is right here https://github.com/cloudfoundry/bosh-agent/blob/1dca3244702c18bf2c36483c529d4e7b3fb92b2e/settings/service.go#L107\nI'll have a look and figure out what we can do.\nAdded story: https://www.pivotaltracker.com/story/show/114099719\n. @poblin-orange there is no fix available. On OpenStack, your only options to use multiple NICs is the way described here: http://bosh.io/docs/openstack-multiple-networks.html. Alright, I added a story for this: https://www.pivotaltracker.com/story/show/114092983\nWe'll probably start it next week.\n. Thanks for the reminder @tomoe. Change has been merged, closing the issue. Released with BOSH version v255.4\n. Additionally: Changing blobstore password is not possible. Shall we open something in addition or can you create an item in tracker for this @cppforlife?\n. New PR against the develop branch: https://github.com/cloudfoundry/bosh/pull/1152\n. Please rebase on recent develop.\n. See https://github.com/cloudfoundry/bosh-init/issues/41 feel free to chime in there.\n. +1 seems helpful. The diff on the Director side is now done only after authentication, so anybody who gets a diff also can do bosh download manifest. Therefore, this is no additional security measure, but only helpful for CIs where the logs are publicly visible. \nYou could even argue that the default should be redact-diff turned off, but I'll leave you to that.\n. @cppforlife why not having the default vice versa? Typing is annoying for humans, but easy for machines. So if you don't want properties to be on your CI, add --redact-properties to the deployment statement?\n. Ah awesome, thanks!\n. Sure, but re-creating 150+ VMs when I'm changing the blobstore password seems a bit much.\n. Hey @StanleyShen \nI've already asked some questions in the issue linked by @cppforlife, but I'll just paste the same questions here. I'd be great if you could try to help us understand more about your use-case and its implications.\nTo get a better idea about what you are suggesting, a few questions:\n- What exactly are those names derived from? Where is reserved and CF 3.0 coming from? Is that the deployment name and some internal versioning? What does (Creator) refer to in your example? The bosh user which started the deployment that created this VM? What would that mean when the resurrector re-created a VM then?\n- Would your naming scheme be the same across all deployments deployed with the same BOSH Director? Would the names be different for each deployment because the deployment name is part of the VM name?\n- BOSH uses different names/identifiers for the VMs, so if you execute bosh vms or bosh instances in the commandline, you would see an entirely different list than you would see in your IaaS management UI. Is this an issue for your operators? How do they find the VM in the IaaS given an identifier from the BOSH commandline?\n. @cholick We have a story for returning the IP address from the DB instead of relying on what the agent returns: https://www.pivotaltracker.com/story/show/115972549\nWe might want to do the same things for jobs/instance groups that should be running on VMS as well, not sure what @cppforlife thinks about this.\n. @kalambet If it is two unrelated changes, can we have those as two separate PRs? This makes reasoning and discussing easier. Thanks!\n. @tomoe We are aware of the problem that BOSH will 'lose' a VM when either cck or automatic resurrection triggered by the HM fails. The problem is basically, that the Director removes the VM from the IaaS and DB and then tries to create a new one. If that fails, the VM is gone. BOSH doesn't realize this, because it uses the VM table to figure out if there is a problem that needs to be fixed.\nWithin the OpenStack CPI team we've started with an epic to fix this, see https://www.pivotaltracker.com/epic/show/2345443 and the linked stories in that epic.\nWith the next BOSH release >255, you'll already get an endpoint /deployments/<deployment name>/instances that can tell you if a VM is missing for an instance. As a workaround for now, you might want to build a script that checks all your deployments regularly for missing VMs that BOSH won't find. In the next steps, we would make bosh cck and the HM use this endpoint, so it would try to re-create the VM in the next consecutive calls as well.\ncc @cppforlife \n. @tomoe no plan from my side to fix this for any specific version. I'm just on the open-source side so I don't necessarily care which versions are used in commercial products of any vendor.\n. Fixed in BOSH v258\n. @evandbrown out of curiosity: That means the GCE metadata service lets you change settings during runtime of a VM and not just once during boot time? That sounds awesome!\n. @joshnn you can try to follow http://bosh.io/docs/tips.html#failed-job to figure out some more information why the consul job is failing. \n. On a side note: this might get more and more relevant when service brokers deploy automatically without a human interacting. Right now, we work around this with a manual script checking running tasks before updating, but this is far from optimal.\n. I've created https://www.pivotaltracker.com/story/show/119363951 to bump fog in bosh-registry.\n. Fixed with bosh 257.14\n. I've seen that also recently on one of our Directors after upgrading to a new version. Unfortunately not sure about the concrete versions involved, couldn't figure out the root-cause there :(\n. @suhlig If I understand you correctly, you are looking for a way to separate setting the target and logging in? Does bosh -n target <target> help? You can call bosh login <user> <pass> afterwards to login.\n. We've added a flag on the instance model called expects_vm which we use as indicator in the HM to resurrect or not.\n. monit restart all or monit restart director also fixed the problem. However, the question remains why the director start took more than 30s?\n. Current suspect: creation of certificates in the director_nginx takes too long due to too little entropy: https://github.com/cloudfoundry/bosh/blob/master/release/jobs/director/templates/nginx_ctl#L31-L36\n. Just verified that this has nothing to do with entropy: Even with user-provided certificates the Director job fails. monit reload works, afterwards the Director job is shown as running.\nIncreasing the VM size didn't work, decreasing the amount of workers also didn't work. Any further ideas, @cppforlife?. Ah, interesting, didn't know this was fixed. Thanks for the pointer, @dpb587-pivotal, I'll update and re-open if that still exists. \n. Should this be fixed in 1.3262.0 already?\n```\nturbulence-0.:  96% |ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo     |  79.3MB  14.5MB/s Time: 00:00:05\nRelease uploaded\nvcap@jumpbox:~/workspace/turbulence-release$ bosh -v\nBOSH 1.3262.0\n```\n. Thanks, @DanielJonesEB for maintaining the workaround for this.\nI've created and linked https://github.com/cloudfoundry/bosh-cli/issues/509 as requested by @mfine30 . @cunnie I recently saw https://github.com/cloudfoundry/bosh/pull/1389\nIs the stemcell you're testing this with already having that fix? Supposedly that could solve some problems around it...\n. Wait, how is the linked story fixing this? It is even making it worse \u2013 currently most users of the ruby CLI won't pass in their own certs. So wouldn't this change even break things for many users who haven't switched to the go cli yet?. The idea was to have something working out-of-the-box. I can't \"just switch to the go-cli\", I also have to change my process of deploying the Director. That's what people will do eventually, but forcing this from the start is a problem that hinders adoption.\nAdding a SAN with the IP to the cert generated on startup would have been an easy win, I guess.. @dpb587-pivotal :)\n. thanks!\n. @cppforlife Thanks for the explanation. Still, current behavior is unexpected. What about my above suggestions in the Expected section?\n. Unfortunately, DEAs drain can hang in some situations. For example, some time ago, I opened https://github.com/cloudfoundry/dea_ng/issues/178 with a similar problem. It might be worth investing some time to figure out the underlying issue, if you still have the DEA around.\n. This should be already possible. We're using Dreamhost in a few test runs. Make sure your blobstore property in the bosh-init manifest looks like this:\nblobstore:\n  provider: s3\n  access_key_id: <redacted>\n  secret_access_key: <redacted>\n  bucket_name: <redacted>\n  host: objects.dreamhost.com\n. @kitsirota storing the compiled packages in S3 is also possible using compiled_package_cache:\ncompiled_package_cache:\n      options:\n        access_key_id: <redacted>\n        bucket_name: <redacted>\n        s3_force_path_style: true\n        secret_access_key: <redacted>\n      provider: s3\nAs a better option, you could start using compiled releases which allow to import releases that do not need to be compiled anymore.\n. @poblin-orange you can, with s3_force_path_style: true as posted in the second example above.\n. @xyloman Where would your full stemcell come from in this case?\nOn AWS, we have light and full stemcells, because:\n- there is a global image store where people can get public images from\n- you can import images to use into your own account by referencing those global images\n- you can then start a VM with the image in your account\nIn OpenStack you can only start a VM referencing an image from the glance image store. So, whatever you do, somebody has to upload the stemcell into the glance store. Would this be taken care of by your IT operations people? Is this just something which is not allowed for regular users? Please note that stemcells might be updated very frequently, i.e. at least once per week. I don't know if the setup you're working in is a good fit for that.\nOut of curiosity: Are you using a publicly available OpenStack, or is this some on-premise installation by your company?\n. In OpenStack you reference images by their UUID when booting a VM. Therefore, if the image is public, you don't need a light stemcell, you just include the image's UUID in your deployment manifest. A light stemcell can't help you there, OpenStack doesn't have the concept to copy over images from somewhere else.\nPlease note that bosh-init doesn't support using an existing stemcell, on the first run it will always need to upload one. \n. Hm, I see. So after giving this some thought: If you want to re-use existing public images in your OpenStack instead of uploading your own, light stemcells sound like a possible option.\nHowever, you would have to produce those yourself \u2013 in contrast to AWS this is not something we can provide on bosh.io, as the UUIDs would be different on every installation.\n@cppforlife How would you imagine those light stemcells being produced? Some bosh command to create it from a stemcell UUID which is later uploaded to the Director with bosh upload-stemcell? Could we teach upload-stemcell to take a UUID instead? I guess we'd have to extract the metadata which is normally present in a stemcell.MF from the IaaS then?\n. @cppforlife that script will need the same configuration as the CPI, as it talks to OpenStack to fetch image metadata. Certainly doable, but users would now need to clone the OpenStack CPI repo just for a small script...\n. What do you think about taking https://github.com/cloudfoundry-incubator/aws-light-stemcell-builder and making an openstack-light-stemcell-builder which takes your OpenStack credentials and the UUID of the image to produce a light stemcell?\n. created epic: https://www.pivotaltracker.com/epic/show/3095067\n. @dennisjbell Latest bosh-lite works well with links. Make sure you upgrade your vagrant VM after pulling the most recent state from the repo.\nvagrant destroy -f\nvagrant box update\nvagrant up --provider virtualbox\n. @dpb587 @barthy1 I'm not sure if checking for vm_cid == nil is what you want to do. That means you now got different behavior depending on whether an errand is running when you call bosh ssh or not. What about checking the expects_vm flag on an instances as suggested in https://github.com/cloudfoundry/bosh-cli/issues/18#issue-180256086?\n. @cppforlife I'm not saying you shouldn't be able to ssh into errand VMs. I'm saying the automatic selection of an instance, if you didn't provide an instance to the command should not be different based on if an errand is running. That sounds weird to me.\n. @cppforlife I'm not talking about running a command on all VMs, I'm talking about the automatic selection of an instance for interactive ssh in the case that the user didn't provide the instance.\nThis selection should not be different if by coincidence an errand is running? It probably shouldn't exist at all, if there is more than one instance.\n. seems like you renamed the network and made it a static one instead of a dynamic? If you didn't change that in the manifest, I'm wondering what happened inside the Director here...\n. @cppforlife @ljfranklin We decided for merge instead of a simple replacement because of properties like use_dhcp. Users mostly don't really care about those and don't set them in their manifest, because \"it just works\".\nI'd like to keep it this way without compromising the idea that defaults in a boshrelase are exactly in one place: the spec file. The current way isn't ideal, I know, because now CPI configuration is coming from two places instead of just one (if you're using this new feature). Seemed like the lesser evil to me, though.\n. @ljfranklin @cppforlife Furthermore, cpi-config contains credentials and should potentially handled differently by admins and also the director than the rest of the cloud-config.\n. It was probably related to the ruby cli requiring a manifest for commands such as bosh stop, so I guess we can close this one.. Same problem here. Multiple workers compiling the same packages, together with global cache leads to that error. Was that introduced some time recently or why is this showing up right now?\n. @dpb587-pivotal Can we merge this? ;)\n. I can also hit 'merge' if that's alright with you \u2013 just wanted to check if something is missing. Don't feel stressed because of me checking in ;)\n. Hey Tom, thanks for following up on this and closing appropriately!. @paolostivanin @cppforlife We'll look into it today: https://www.pivotaltracker.com/story/show/133419353\n. @zhutao11 Any specifics on where that is happening? Which OpenStack installation/distribution are you using?\nNova even has a test ensuring that this never happens: https://github.com/openstack/nova/blob/5158ca7dcffd583dc8dd7c29b0a9a595d7de7dcf/nova/tests/unit/api/openstack/test_wsgi.py#L63-L67\n. This is a duplicate of https://github.com/cloudfoundry/bosh/issues/1495 \u2013 I already asked the same questions.\n@zhutao11 If you have a webserver in the middle between OpenStack and your client make sure that it doesn't add charset to the content-type header. As already commented in https://github.com/cloudfoundry/bosh/issues/1495: nova has a test that there won't be a charset in the content-type answer: https://github.com/openstack/nova/blob/5158ca7dcffd583dc8dd7c29b0a9a595d7de7dcf/nova/tests/unit/api/openstack/test_wsgi.py#L63-L67\n. Fixed in stemcell 3309. Thanks!\n. @cppforlife still, that's a breaking change \u2013 I don't think you should just do it like that.. Then I maybe misunderstood @ljfranklin's statement. Lyle, did this work before? . Let's just bump rsyslogd in the stemcells. We've recently had another outage, also related to an rsyslogd memory leak on version 8.22\nSo I guess it should be fine to just update and see what's happening. It most likely won't get worse than it already is.. /cc @mfine30 Can you create a story to bump rsyslogd in the stemcells to latest stable?. Given that there hasn't been much feedback around rsyslogd in the past (except from our side, afaik), I'd say just bump it in a minor version. However, this is entirely your call. If you feel more comfortable with a major bump, go with that.. cc @jsievers . @paolostivanin is it still the case that v260.1 didn't fix it after our debugging last week?. @Adamjmb you might want to rotate those AWS credentials as an email has been sent to anybody who is subscribed to the issues in this repository.. Yup, that seems like an issue with the registry. @StBurcher I remember you had some proxy issues earlier on. Could it be that 2017-01-16_15:13:49.90153 [settingsService] 2017/01/16 15:13:49 ERROR - Failed loading settings via fetcher: Unmarshalling settings wrapper: invalid character '<' looking for beginning of value is reading some answer from the proxy?\nYou can try the following:\n Access a VM that got 'timed out pinging...' with the OpenStack console\n execute curl http://169.254.169.254/latest/user-data -vv and look for registry.endpoint and server.name in the json returned\n* execute http://<registry.endpoint>/instances/<server.name>/settings -vv\nYou should see a json, but the above error indicates that you might see something else.. Closing this as conversation is already happening in https://github.com/cloudfoundry-incubator/bosh-openstack-cpi-release/issues/59. @harshalk91 please refrain from cross-posting your issue without linking it in the future.. error It is not allowed to create an interface on external network dc8385e3-3771-481d-ba98-4abf1f21c30b' indicates you have put the external network's UUID into your manifest instead of the private network UUID \u2013 might that be the case?. +1, also thought about doing this on OpenStack before. current state:\n OpenStack CPI supports this with https://github.com/cloudfoundry/bosh-deployment/blob/master/misc/cpi-resize-disk.yml\n we deliberately didn't do this for AWS, as there's a ridiculous rate limit of once every 6 hours\n* not sure about other IaaS layers and their options/rate limits on native disk resizing. Yes @frodenas you're right: We changed the IP addresses to be returned from the DB instead of from the agents some time ago. Moving them to the non-full response seems to make sense.\nThanks!. yup, that's from master. sorry about the missing info there.\nFrom: Dmitriy Kalinin notifications@github.com\nReply-To: cloudfoundry/bosh reply@reply.github.com\nDate: Friday, 3 March 2017 at 15:59\nTo: cloudfoundry/bosh bosh@noreply.github.com\nCc: \"Voelz, Marco\" marco.voelz@sap.com, Author author@noreply.github.com\nSubject: Re: [cloudfoundry/bosh] panic during parsing of sha1 during upload-stemcell (#1607)\nis this built from master, not 261.x?\nSent from my iPhone\n\nOn Mar 3, 2017, at 12:35 AM, Marco Voelz notifications@github.com wrote:\nuploading 3363.9 with information from http://bosh.io/stemcells/bosh-openstack-kvm-ubuntu-trusty-go_agent\n$ bosh us https://s3.amazonaws.com/bosh-core-stemcells/openstack/bosh-stemcell-3363.9-openstack-kvm-ubuntu-trusty-go\n_agent.tgz --sha1=1cddb531c96cc4022920b169a37eda71069c87dd Using environment '192.168.1.12' as client 'admin'\nTask 351\n08:28:01 | Update stemcell: Downloading remote stemcell (00:00:29)\n08:28:30 | Update stemcell: Verifying remote stemcell (00:00:00)\nL Error: panic: Parsing multiple digest: Unable to parse digest string. Digest and algorithm key can only contain alpha-numeric characters.\ngoroutine 1 [running]:\npanic(0x51cc80, 0xc420016580)\n/usr/local/go/src/runtime/panic.go:500 +0x1a1\ngithub.com/cloudfoundry/bosh-utils/crypto.MustParseMultipleDigest(0x7ffc0b44bbee, 0x2a, 0x10000c420039c70, 0x5ed126dd0000005d, 0x2)\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/crypto/multiple_digest.go:29 +0x12e\nmain.MultiDigestCommand.Execute(0x7ffc0b44bba4, 0x49, 0x7ffc0b44bbee, 0x2a, 0xc42000e570, 0x0, 0x3, 0x488a01, 0x7f5ed7f47000)\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/main/verify_multidigest.go:52 +0x39\nmain.(MultiDigestCommand).Execute(0xc420010140, 0xc42000e570, 0x0, 0x3, 0x1, 0x3)\n:1 +0x8b\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.(Parser).ParseArgs(0xc4200180c0, 0xc42000a190, 0x3, 0x3, 0xc420066000, 0xc4200180c0, 0x7ffc0b44bb7\nd, 0x0, 0x0)\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:316 +0x8e6\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.(*Parser).Parse(0xc4200180c0, 0xc420010140, 0x16, 0xc4200180c0, 0x4a3b01, 0xc420010140)\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:186 +0x74\ngithub.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags.Parse(0x516100, 0xc420010140, 0x54f8bc, 0x28, 0x5dd1a0, 0xc420016360, 0xc4200001a0)\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/vendor/github.com/jessevdk/go-flags/parser.go:133 +0x4c\nmain.main()\n/tmp/build/020a9e9e/gopath/src/github.com/cloudfoundry/bosh-utils/main/verify_multidigest.go:29 +0x77\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1607#issuecomment-283975622, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ACJv93EDMeY9JCbuojoRHmbdS-aeqGTWks5riCrHgaJpZM4MR_nR.\n. Any ideas here? Uploading the same version of a release for a new major stemcell version should be possible without human interaction (i.e. --fix or leaving out --name and --version), right?\nWhat's your workaround here?. I actually had the same thought as @dpb587 when I saw this change. Having it in the ctl scripts for every job makes it crystal clear when what happens. I guess is might be a matter of perspective: When you're coming from the BOSH world, it makes sense to do it in the script, when you're coming from the software you're installing as a release, you want to do it like you would do without using BOSH.. I keep arguing about this since forever. @cppforlife, can we have this now? Would solve so many problems, even compiled releases for cpis in create-env would only be half of a problem then.... @Amit-PivotalLabs Are there any details written down on the CredHub plugins? That sounds like an interesting concept I'd like to learn more about.. also hit this today. pretty unfortunate :(. Should be fixed with 266.2.0 and 262.8.0 according to https://www.pivotaltracker.com/n/projects/2132440/stories/155257235. Still open though: problems with the diff for global addons which aren't used on a specific deployment: https://github.com/cloudfoundry/bosh-cli/issues/360. Updated to reflect that this also happens when doing bosh ssh. Validated this is fixed in 3586.x, tested on 3586.36.. Didn't hear any complaints from operations so far. Need to check back and see how many snapshots we currently have and if that is an issue.. sure, we can do that. For now, I wanted to keep it in sync with the tags on VMs. We probably should  change both occurrences, right?. nice work!. I think currently name and version are used to check if a release is already present on the Director. I we wouldn't have this, the release would be downloaded on every deploy.\nWe have a somehow related problem in https://github.com/cloudfoundry/bosh/issues/1620 where this leads to not downloading a compiled release \u2013 because a release with the same name and version is already there.\nSeems like we should start also looking at sha1 to figure out if we should download a release and maybe in turn stop requiring version? Wdyt, @cppforlife @dpb587  @drnic?. Are you referring to something like bosh instances --ps?. @huangered Out of curiosity: Which IaaS are you running on and what is your use-case for this?. Seems like there is a connection to pool_timeout  on the director db connections options \u2013 not really sure why that seems to only be relevant in the cloud-config case and otherwise works fine?\nWhen increasing the timeout from 10 to 60 seconds the deployments seems to work fine.. Still seeing issues now, although timeout for the postgres connection pool is set to 80. @cppforlife @dpb587-pivotal how do you feel about exposing the checkpoint timeout multiplier as a property with default 3 (so it stays at 3*30=90 seconds)? Seems like for some reason we currently need to adapt this \u2013 not sure how long digging into root causes might take.. @dpb587-pivotal I feel like commit https://github.com/cloudfoundry/bosh/commit/61a6991e5812a33fec3839379122462c1ec47029 tried to solve an issue with similar symptoms, as described in https://github.com/cloudfoundry/bosh/issues/1778\nUnfortunately, release 264.2.0 didn't solve this for us, we still see the issue of tasks being cancelled due to state 'timeout'. Any other ideas/assumptions we should investigate here?. Saw the same some time ago. Director error handling isn't very good in these situations, so we definitely should fix this.. In case others stumble upon this: \n Finding the blocking VMs worked nicely with bosh ssh -d <DEPLOYMENT> -c 'http://vcap:random-password@127.0.0.1:2822/_status2?format=xml' -r --tty > /tmp/ssh-out\n root-cause was in our case that the VM was overloaded in terms of CPU. Killing the CPU-heavy process made monit responsive again\nStill todo: Make the Director able to cope with that message and show the unresponsive VM to the user.. Fixed with https://github.com/cloudfoundry/bosh/commit/c9cc3ff80774d00a52be1be3f9d8bad6c6674591. Proposed fix: https://github.com/cloudfoundry/bosh-agent/pull/140. Agent that fixes this is included in stemcell 3541.x: https://github.com/cloudfoundry/bosh-linux-stemcell-builder/releases/tag/stable-3541.2. +1 we also have a few automated scripts which SSH regularly into VMs. bosh events is pretty much useless for us right now.. on a side note: events are also logged to syslog by default, without specifying a property: https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/api/event_manager.rb#L55\nSo if you want a stream that gives you everything: syslog forward from the director to an auditlog endpoint.\nEdit: all of which still doesn't make bosh events useful on the commandline. External auditing tools serve a different purpose for a different audience.. It seems that a change in tags in the manifest does show up in the diff in the meanwhile. However, a change in tags doesn't trigger any action for the CPI \u2013 it says done, but didn't change anything.\n```\n  tags:\n-   really: false\n+   some: changes\nContinue? [yN]: y\nTask 13896\nTask 13896 | 13:22:56 | Preparing deployment: Preparing deployment (00:00:00)\nTask 13896 | 13:22:56 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 13896 Started  Fri Sep 21 13:22:56 UTC 2018\nTask 13896 Finished Fri Sep 21 13:22:56 UTC 2018\nTask 13896 Duration 00:00:00\nTask 13896 done\nSucceeded\n```. @cppforlife @dpb587-pivotal any objections merging this? If not, we can just merge this in.. @cppforlife @dpb587-pivotal any objections merging this? If not, we can just merge this in.. Ah, sorry, I completely missed this, thanks for the reminder.\n\ncompilation VMs:\ne.g. Virus scan. I only want virus scanning for sources and their dependencies, don't need Virus scanning during runtime (be is long-lived service or short-lived errands)\nI'm also thinking about some license or CVE scan that could enforce that nobody installs stuff that doesn't match company policy. There are also other means to achieve this, if you e.g. limit the location from which you can install bosh-releases (i.e. only company-vetted releases are available)\nService VMs: \nSomething like Turbulence or log-forwarding agents. I don't need those (or in a very different configuration) for compilation VMs or errands\nErrand VMs:\nNot sure what those VMs specifically need. It seems to me, that they would not need most of the stuff I'd like to install on the other two types of VMs.. @cppforlife @dpb587-pivotal opinions about the use-cases?. Ah, good point. I guess you\u2019d also come up with cases where you want to exclude those scanning tools from compilation VMs then.\n\nI guess what I was looking for with the compilation cases above was something similar to multi buildpacks where I can include some scanning tool before my actual code is compiled. Thanks for the race condition reminder.\nOn 21. Dec 2017, at 07:49, Danny Berger notifications@github.com<mailto:notifications@github.com> wrote:\nThe include-compilation use cases sound a bit weird - both of those use cases sound racy to me. There's no guarantee the addons may have finished scanning by the time compilation was finished. For completeness though, I think it might make sense regardless.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1833#issuecomment-353272155, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ACJv99sR5GMd7QE6Wv3XCADbTzALCoKmks5tCf-IgaJpZM4QbAio.\n. Awesome, thanks!\nFrom: Danny Berger notifications@github.com\nReply-To: cloudfoundry/bosh reply@reply.github.com\nDate: Thursday, 8. March 2018 at 21:29\nTo: cloudfoundry/bosh bosh@noreply.github.com\nCc: \"Voelz, Marco\" marco.voelz@sap.com, Mention mention@noreply.github.com\nSubject: Re: [cloudfoundry/bosh] As a bosh user, I'd like to filter addon installation based on the type of a VM: errand, service, compilation (#1833)\nPull request for lifecycle filtering: #1902https://github.com/cloudfoundry/bosh/pull/1902\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1833#issuecomment-371614270, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ACJv91E_OO2ELmJNq3h6dq7ReeCmIMO5ks5tcZSqgaJpZM4QbAio.\n. Yes, we're still interested in making it easier to follow-up on rogue agent alerts for operators or automation.. our performance improvements work towards making /deployments faster, thus and issue like this more unlikely, yes. Switching to puma certainly helps in terms of being able to process more requests.\nThe underlying issue here, which potentially needs to be tackled in the HM, isn't touched by anything we do.. does bosh delete-config cpi --name=<config_name> work for you? This should be possible with bosh >=264 to get rid of all kinds of configuration now.. Already fixed in the agent code with https://github.com/cloudfoundry/bosh-agent/pull/140, was documented in https://github.com/cloudfoundry/bosh/issues/1758 previously (although way less thorough ;)).\nWe're waiting for agent version 2.34.0 to be included in some stemcell to fix this.\nping @cppforlife we recently chatted about this, remember?. @dpb587-pivotal We would be happy with an officially released stemcell with this fix, either new series or a backport to an older one. We're not married to a specific line of stemcells, so we could switch to a newer one, if that's easier to build. Backporting to 3468.x is also fine.. Updated stemcell hasn\u2019t been released yet. @cppforlifecan we get a fix released, please?\nOn 20. Dec 2017, at 18:54, Gerhard Lazu notifications@github.com<mailto:notifications@github.com> wrote:\nThis just hit us as well, we're on 262.0.0 & bosh-google-kvm-ubuntu-trusty-go_agent/3421.11 .\nWhich stemcell / director versions do we need to upgrade to?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1840#issuecomment-353135981, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ACJv9y-NSrxb4x2ialGhIoFDAlebKrvpks5tCUnrgaJpZM4Qn6Je.\n. Updated agent that fixes this should be released with stemcell 3541.x: https://github.com/cloudfoundry/bosh-linux-stemcell-builder/releases/tag/stable-3541.2. edit: it's only changing the _spec for the migrations, which I missed, all is well ;). @jfmyers9 Are you saying the way that @beyhan proposed above would be the one you'd like to see a contribution for?\n\nPossible Solution: Director is able to identify duplicate tasks and doesn't add them to the queue. This could be just a solution for scan and fix tasks or a general solution for all tasks.\n\nIf so: should this be a generic mechanism for all kinds of tasks or just scoped towards scan_and_fix?. This is my Director's bosh env output, which shows now that local_dns: enabled and power_dns: disabled\n```\n$ bosh env\nUsing environment '10.0.1.10' as client 'admin'\nName               bosh\nUUID               70332b59-24b0-4a11-95b2-1cc0ed83c240\nVersion            268.4.0 (00000000)\nDirector Stemcell  ubuntu-xenial/170.9\nCPI                openstack_cpi\nFeatures           compiled_package_cache: disabled\n                   config_server: enabled\n                   local_dns: enabled\n                   power_dns: disabled\n                   snapshots: disabled\nUser               admin\nSucceeded\n``. @damzog I think we default to callingbosh delete-vmfollowed bybosh deployto fix these issues.. I'm not sure all aspects of this bug are related to the registry. The first part, whereattach_disk` can fail to update the registry, should be gone, correct.\nThe second part though, where a failure in the mount_disk call to the agent errors out, might still produce the same mismatch between agent and director. I think this case isn't handled properly on the director side.. You could make the switch by forcing data migration, I guess ;). story: https://www.pivotaltracker.com/story/show/155320835. Probably was fixed with https://github.com/cloudfoundry/bosh/pull/1874? . See also this discussion on the mailing list https://lists.cloudfoundry.org/g/cf-bosh/topic/6331977#2438\nI think currently the only fail to make your drain fail is echoing a non-integer: https://github.com/cloudfoundry/bosh-agent/blob/7d965883bd85c5297dda9015cfe63d67937327e1/agent/script/drain/concrete_script.go#L157. Closing in favor of #2015. Hey @ywysuibian \nIt seems you're trying to use multiple networks on a single VM, which is only supported in a very specific case: http://bosh.io/docs/openstack-multiple-networks.html\nDue to missing functionality in the bosh-agent, other cases involving dynamic networking or DHCP are not supported: https://github.com/cloudfoundry/bosh/issues/1129#issuecomment-186286247. Hey @ywysuibian \nIt seems you're trying to use multiple networks on a single VM, which is only supported in a very specific case: http://bosh.io/docs/openstack-multiple-networks.html\nDue to missing functionality in the bosh-agent, other cases involving dynamic networking or DHCP are not supported: https://github.com/cloudfoundry/bosh/issues/1129#issuecomment-186286247. Hey @mfine30, I'm surprised this got prioritized suddenly and you already explored the solution space. What about @cppforlife's thoughts on the risk of implicitly depending on other components with this approach? How did you end up dealing with this?. Hey @mfine30, I'm surprised this got prioritized suddenly and you already explored the solution space. What about @cppforlife's thoughts on the risk of implicitly depending on other components with this approach? How did you end up dealing with this?. Thanks, @jfmyers9 for the insights, that certainly makes sense.. Thanks, @jfmyers9 for the insights, that certainly makes sense.. See also: https://github.com/cloudfoundry/bosh-agent/issues/89 and https://github.com/cloudfoundry/bosh-agent/issues/112. See also: https://github.com/cloudfoundry/bosh-agent/issues/89 and https://github.com/cloudfoundry/bosh-agent/issues/112. @cppforlife  Hasn't this been an error ever since? I remember we encountered https://github.com/cloudfoundry/bosh/issues/1169 quite a while ago (and even had some proposed fix up for discussion, which was never merged). @cppforlife  Hasn't this been an error ever since? I remember we encountered https://github.com/cloudfoundry/bosh/issues/1169 quite a while ago (and even had some proposed fix up for discussion, which was never merged). delete-deployment fails, because the agent can't execute drain and stop successfully. delete-deployment fails, because the agent can't execute drain and stop successfully. Hey Christian,\nif I'm not mistaken, it depends on the fingerprints of the packages being the same. This means, if your content in the two packages is the exact same, up to the last bit, this should totally work.\nIf, however, you do have a difference between the two, this fails with the error you're describing above.\nHope that helps!. Hey Christian,\nif I'm not mistaken, it depends on the fingerprints of the packages being the same. This means, if your content in the two packages is the exact same, up to the last bit, this should totally work.\nIf, however, you do have a difference between the two, this fails with the error you're describing above.\nHope that helps!. Well, the solution for not duplicating this is to re-use the same package ;)\nIIRC, there should be a story which allows packages of the same name from different releases even if fingerprints aren't the same. Does that sound like a story in your backlog @cppforlife? . Well, the solution for not duplicating this is to re-use the same package ;)\nIIRC, there should be a story which allows packages of the same name from different releases even if fingerprints aren't the same. Does that sound like a story in your backlog @cppforlife? . @cppforlife is correct. IIRC, this was even researched by the BOSH team by looking at the nginx code some time ago. All log messages make it to the correctly configured error.log, it is just that nginx tries to open the default error log before reading the configuration about a error.log location different from the default.\nSo this has always been there, bpm or no-bpm.\nEdit: found the story: https://www.pivotaltracker.com/story/show/140424061. /cc @coreyti\nWe talked about more granular health metrics at some point, do you remember?. somewhat related to https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/deployment-steps.md. Seems like nginx has been updated to 1.14 with https://github.com/cloudfoundry/bosh/commit/37156d3851824509069872eb0209f74d4606cc1f. Seems like nginx has been updated to 1.14 with https://github.com/cloudfoundry/bosh/commit/37156d3851824509069872eb0209f74d4606cc1f. Closing in favor of #2004 as it had tests. Thanks!. Closing in favor of #2004 as it had tests. Thanks!. You are correct, this is the current behavior. We could try to implement option 1 (removing or re-using the port if it is currently not attached to a vm) in https://github.com/cloudfoundry-incubator/bosh-openstack-cpi-release. I think the current implementation is still from back in the days, when ports were not a separate entity managed by neutron.\nCreating a new port could be an option when dynamic networking relying on OpenStack DHCP is used. In the cases of manual networking, I'd rather not get into picking a new IP address during cck/resurrection.\n/cc @cppforlife wdyt?. You are correct, this is the current behavior. We could try to implement option 1 (removing or re-using the port if it is currently not attached to a vm) in https://github.com/cloudfoundry-incubator/bosh-openstack-cpi-release. I think the current implementation is still from back in the days, when ports were not a separate entity managed by neutron.\nCreating a new port could be an option when dynamic networking relying on OpenStack DHCP is used. In the cases of manual networking, I'd rather not get into picking a new IP address during cck/resurrection.\n/cc @cppforlife wdyt?. Story to make the openstack CPI more resilient against already existing ports: https://www.pivotaltracker.com/story/show/159498851. Story to make the openstack CPI more resilient against already existing ports: https://www.pivotaltracker.com/story/show/159498851. Thanks for the reminder, @freddesbiens! The re-use existing ports feature has been released with v40 of the openstack-cpi-release: https://github.com/cloudfoundry/bosh-openstack-cpi-release/releases/tag/v40\nPlease give the release a try and let us know when this doesn't work for you, @kayrus . Thanks for the reminder, @freddesbiens! The re-use existing ports feature has been released with v40 of the openstack-cpi-release: https://github.com/cloudfoundry/bosh-openstack-cpi-release/releases/tag/v40\nPlease give the release a try and let us know when this doesn't work for you, @kayrus . Just to clarify this: I'm not sure if we want the messages to show up on the host. Forwarding them to a remote syslog endpoint is what ultimately should happen. If this can work without having the messages on the host VM, this would be totally fine.. Just to clarify this: I'm not sure if we want the messages to show up on the host. Forwarding them to a remote syslog endpoint is what ultimately should happen. If this can work without having the messages on the host VM, this would be totally fine.. I just wanted to state the goal without saying how to get there. If we need the logs to show up on the host to forward them, that\u2019s fine with me. If there would be another means for forwarding without the logs showing up in the host VM, also good ;)\n. I just wanted to state the goal without saying how to get there. If we need the logs to show up on the host to forward them, that\u2019s fine with me. If there would be another means for forwarding without the logs showing up in the host VM, also good ;)\n. In the pre-bpm times (when also syslog-forwarder wasn't an easily available choice) the director opted for logging to syslog directly for a few audit-relevant log statements. Those include all events and, optionally, also all API accesses in CEF format.\nYou might be right: given the availability of the syslog-forwarder we might want to reconsider the choice to log to syslog directly from ruby code.\n/cc @cppforlife @freddesbiens @mfine30 wdyt?. In the pre-bpm times (when also syslog-forwarder wasn't an easily available choice) the director opted for logging to syslog directly for a few audit-relevant log statements. Those include all events and, optionally, also all API accesses in CEF format.\nYou might be right: given the availability of the syslog-forwarder we might want to reconsider the choice to log to syslog directly from ruby code.\n/cc @cppforlife @freddesbiens @mfine30 wdyt?. Thanks Danny for the context! I'm not sure if directly logging to syslog was part of the requirements or part of the solution that was picked back then. @cppforlife do you still have some context on the decisions that were taken back then?. Thanks Danny for the context! I'm not sure if directly logging to syslog was part of the requirements or part of the solution that was picked back then. @cppforlife do you still have some context on the decisions that were taken back then?. Sounds like a reasonable plan to me. Should we do the changes, given that we might currently have the biggest interest in fixing it soon?\nDirector, workers, and HM would get their own file audit.log. Potentially a file per worker or a single file for all workers? Not sure what\u2019s easier. \nI can throw in a story to do this for master and backport to 266 \u2013 or at least we add to the release notes that this is broken in 266.x\nWdyt?\n. Sounds like a reasonable plan to me. Should we do the changes, given that we might currently have the biggest interest in fixing it soon?\nDirector, workers, and HM would get their own file audit.log. Potentially a file per worker or a single file for all workers? Not sure what\u2019s easier. \nI can throw in a story to do this for master and backport to 266 \u2013 or at least we add to the release notes that this is broken in 266.x\nWdyt?\n. I've created https://www.pivotaltracker.com/story/show/159191961 for now, let me know what your think.. I've created https://www.pivotaltracker.com/story/show/159191961 for now, let me know what your think.. Sounds reasonable, let's go for one file per worker. Thanks for the feedback!. Sounds reasonable, let's go for one file per worker. Thanks for the feedback!. Fixed with BOSH v268.0.1. Fixed with BOSH v268.0.1. Hey @degaurab, thanks for raising this issue!\nFor my understanding: When you're talking about \"multi-layer\" deployment, you're referring to a deployment with multiple, named cloud-configs?\nIf that is the case, there might be a few different issues hidden in your report, let me try to unpack this:\n There should be validation when uploading a cloud-config (probably runtime- and cpi-config as well, I guess), because any errors currently show only up during deployment when the config is used.\n When you're using multiple cloud-configs, there is some confusion around what bosh cloud-config shows: Is it a merged version? Is it just one of them?\nIs my understanding of those issues correct?. Hey @degaurab, thanks for raising this issue!\nFor my understanding: When you're talking about \"multi-layer\" deployment, you're referring to a deployment with multiple, named cloud-configs?\nIf that is the case, there might be a few different issues hidden in your report, let me try to unpack this:\n There should be validation when uploading a cloud-config (probably runtime- and cpi-config as well, I guess), because any errors currently show only up during deployment when the config is used.\n When you're using multiple cloud-configs, there is some confusion around what bosh cloud-config shows: Is it a merged version? Is it just one of them?\nIs my understanding of those issues correct?. ping @degaurab any comments on my try to put this into separately understandable issues above?. ping @degaurab any comments on my try to put this into separately understandable issues above?. Hm, I thought this was working already. Given that after #1985 we now have the second PR open for this, it seems my impression was wrong. Thanks for the PRs, we should merge this soon!\n. Hm, I thought this was working already. Given that after #1985 we now have the second PR open for this, it seems my impression was wrong. Thanks for the PRs, we should merge this soon!\n. I guess this is most likely the same issue as described in https://github.com/cloudfoundry/bosh-cli/issues/360. I guess this is most likely the same issue as described in https://github.com/cloudfoundry/bosh-cli/issues/360. Seems like it, thanks! This was observed on a colleague's installation who just used bosh-deployment to set it up. Can we update bosh-deployment with the latest release?. Seems like it, thanks! This was observed on a colleague's installation who just used bosh-deployment to set it up. Can we update bosh-deployment with the latest release?. Great, @muralisc, thanks. Could you also update https://github.com/cloudfoundry/bosh-cli/pull/459 to reflect that the passed in string is no longer default, but the parameter is not passed in at all?. Great, @muralisc, thanks. Could you also update https://github.com/cloudfoundry/bosh-cli/pull/459 to reflect that the passed in string is no longer default, but the parameter is not passed in at all?. Edit: forget what I wrote below, after reading it again two times, I understand that you're calling rsync a second time just to deal with the situation I described below. Thanks for proposing this.\n~~This might be stupid question, but looking at your description this immediately popped up in my mind: If you're not stopping the job before calling rsync, how are you avoiding that you're ending up with a stale copy?~~\n~~I understand that you're specifically targeting jobs with a huge persistent disk, so my assumption is rsync --bwlimit <reasonable amount here> will take a non-zero amount of time to complete. The files that have been rsynced first might already be out of date when the entire operation finishes.~~ \n~~Is there something I'm missing or are you saying this would be a risk you are willing to take?~~. Edit: forget what I wrote below, after reading it again two times, I understand that you're calling rsync a second time just to deal with the situation I described below. Thanks for proposing this.\n~~This might be stupid question, but looking at your description this immediately popped up in my mind: If you're not stopping the job before calling rsync, how are you avoiding that you're ending up with a stale copy?~~\n~~I understand that you're specifically targeting jobs with a huge persistent disk, so my assumption is rsync --bwlimit <reasonable amount here> will take a non-zero amount of time to complete. The files that have been rsynced first might already be out of date when the entire operation finishes.~~ \n~~Is there something I'm missing or are you saying this would be a risk you are willing to take?~~. Sorry for the confusion: We ended up creating a new story instead of moving the auto-generated one to our backlog. Story is already pointed and in the backlog, not sure if we get to it next week, though: https://www.pivotaltracker.com/story/show/162165109. Sorry for the confusion: We ended up creating a new story instead of moving the auto-generated one to our backlog. Story is already pointed and in the backlog, not sure if we get to it next week, though: https://www.pivotaltracker.com/story/show/162165109. related: I guess that might be why bosh-deployment specifies a 40GB root disk on GCP: https://github.com/cloudfoundry/bosh-deployment/blob/master/gcp/cpi.yml#L18. related: I guess that might be why bosh-deployment specifies a 40GB root disk on GCP: https://github.com/cloudfoundry/bosh-deployment/blob/master/gcp/cpi.yml#L18. Is this a product feature?\n\nBOSH leaves any created intents behind during a failed deployment, to assist in debugging.\n\nTo me it seemed like something should have been rolled back, but wasn't.\nThis happened on Director Version: 266.6.0, I think this should also occur on the 267.x series, right?. Is this a product feature?\n\nBOSH leaves any created intents behind during a failed deployment, to assist in debugging.\n\nTo me it seemed like something should have been rolled back, but wasn't.\nThis happened on Director Version: 266.6.0, I think this should also occur on the 267.x series, right?. Hey @pulpham \nIn your repack-stemcell command, is src.tgz the stemcell you downloaded, or is it the stemcell you modified somehow above? As you're using --empty-image it should be safe to use the downloaded one, as all your previous modifications only apply to the image itself.. Hey @pulpham \nIn your repack-stemcell command, is src.tgz the stemcell you downloaded, or is it the stemcell you modified somehow above? As you're using --empty-image it should be safe to use the downloaded one, as all your previous modifications only apply to the image itself.. Hey @pulpham \ncould unpack your dst.tgz and paste the contents of stemcell.MF? After applying a similar set of commands, mine looks like this \nname: bosh-openstack-kvm-ubuntu-trusty-go_agent\nversion: 1.1.1\noperating_system: ubuntu-trusty\nsha1: 6ca278dacb71641d3b7ab5db6d340cf88dfbefeb\nbosh_protocol: \"1\"\nstemcell_formats:\n- openstack-light\ncloud_properties:\n  architecture: x86_64\n  auto_disk_config: true\n  container_format: bare\n  disk: 3072\n  disk_format: qcow2\n  hypervisor: kvm\n  image_id: my-uuid-here\n  infrastructure: openstack\n  name: bosh-openstack-kvm-ubuntu-trusty-go_agent\n  os_distro: ubuntu\n  os_type: linux\n  version: \"3586.36\". Hey @pulpham \ncould unpack your dst.tgz and paste the contents of stemcell.MF? After applying a similar set of commands, mine looks like this \nname: bosh-openstack-kvm-ubuntu-trusty-go_agent\nversion: 1.1.1\noperating_system: ubuntu-trusty\nsha1: 6ca278dacb71641d3b7ab5db6d340cf88dfbefeb\nbosh_protocol: \"1\"\nstemcell_formats:\n- openstack-light\ncloud_properties:\n  architecture: x86_64\n  auto_disk_config: true\n  container_format: bare\n  disk: 3072\n  disk_format: qcow2\n  hypervisor: kvm\n  image_id: my-uuid-here\n  infrastructure: openstack\n  name: bosh-openstack-kvm-ubuntu-trusty-go_agent\n  os_distro: ubuntu\n  os_type: linux\n  version: \"3586.36\". Oh, and which CLI and Director versions are you using?. Oh, and which CLI and Director versions are you using?. Hey Danny, thanks for sharing your thoughts! I completely agree that having it explicitly in bbr is the desired way, that's why I ended up asking them first. From what I've heard in the conversations, they don't plan to add any work to provide a generic way to not run bbr for certain jobs, even though a script is present.\nGiven the current situation, what should we do here? Currently, this is blocking teams from adopting bbr and using hand-crafted backup/restore scripts, which is what we'd like to avoid.\nHow would you feel about a marker file in the artifact directory for the blobstore, indicating that the backup is disabled? We could empty the script, but have https://github.com/cloudfoundry/bosh/blob/master/jobs/blobstore/templates/bbr_backup#L16 write a file like backup_disabled.marker, if you want to have it more explicitly. Also potentially interesting: can bbr scripts stream some logs to the client? This would be a different option.. Hey Danny, thanks for sharing your thoughts! I completely agree that having it explicitly in bbr is the desired way, that's why I ended up asking them first. From what I've heard in the conversations, they don't plan to add any work to provide a generic way to not run bbr for certain jobs, even though a script is present.\nGiven the current situation, what should we do here? Currently, this is blocking teams from adopting bbr and using hand-crafted backup/restore scripts, which is what we'd like to avoid.\nHow would you feel about a marker file in the artifact directory for the blobstore, indicating that the backup is disabled? We could empty the script, but have https://github.com/cloudfoundry/bosh/blob/master/jobs/blobstore/templates/bbr_backup#L16 write a file like backup_disabled.marker, if you want to have it more explicitly. Also potentially interesting: can bbr scripts stream some logs to the client? This would be a different option.. > As we understand it, it's just an ephemeral cache that does not belong to the backup altogether.\nBOSH's blobstore isn't really an 'ephemeral cache' \u2013 you do need it for resurrection and when re-deploying already uploaded releases. All of this would fail, if you restore the DB, but not the blobstore contents. If by 'ephemeral' you mean that you can re-upload everything with bosh upload-release --fix and don't really need a backup to get back the data: that is correct.\nFurthermore, the times when blobstore contents change are pretty easy to predict: every time when a new release is uploaded or compiled for a new stemcell. If you wanted to use bbr to backup your blobstore, this would be the times to do it. In all other cases, it is just copying the same old data you already have.\nI'd again ask what I've asked above: Given the current situation, what should we do here? Currently, this is blocking teams from adopting bbr and using hand-crafted backup/restore scripts, which is what we'd like to avoid.\nPossible options:\n try to convince the bbr team to introduce a switch to not backup a blobstore, or ideally a more generic switch to not backup an arbitrary job\n option to disable blobstore backup with a property at director deployment time, as proposed in the original ticket above\n* ???\nwdyt?\n. > As we understand it, it's just an ephemeral cache that does not belong to the backup altogether.\nBOSH's blobstore isn't really an 'ephemeral cache' \u2013 you do need it for resurrection and when re-deploying already uploaded releases. All of this would fail, if you restore the DB, but not the blobstore contents. If by 'ephemeral' you mean that you can re-upload everything with bosh upload-release --fix and don't really need a backup to get back the data: that is correct.\nFurthermore, the times when blobstore contents change are pretty easy to predict: every time when a new release is uploaded or compiled for a new stemcell. If you wanted to use bbr to backup your blobstore, this would be the times to do it. In all other cases, it is just copying the same old data you already have.\nI'd again ask what I've asked above: Given the current situation, what should we do here? Currently, this is blocking teams from adopting bbr and using hand-crafted backup/restore scripts, which is what we'd like to avoid.\nPossible options:\n try to convince the bbr team to introduce a switch to not backup a blobstore, or ideally a more generic switch to not backup an arbitrary job\n option to disable blobstore backup with a property at director deployment time, as proposed in the original ticket above\n* ???\nwdyt?\n. Regarding who should be responsible for selecting what to update\n\nRevisiting this, I'm still of the opinion that \"what\" gets backed up should be the responsibility of the backup agent/tool/operator. In environments where business continuity and standard provisioning may be separate teams, I think it's too risky to introduce job-level properties to disable backup support \n\nThat's already what BOSH does now. If you are using an external blobstore, you would have to include an additional ops-file to also backup that one when you're doing a backup of your DB. If you don't do it, the assumption is that you have some process to restore the necessary blobstore contents. It already is in the responsibility of the BOSH operator to decide for backing up the external blobstore or not. What is even more problematic, in my opinion, is that there is a non-obvious difference between external and internal blobstores: One needs specific enablement, the other is always backed up.\nRegarding work and opinions of the BBR team: As documented in https://github.com/cloudfoundry-incubator/bosh-backup-and-restore/issues/18, I had a while ago a conversation on slack with @glestaris about that. I'd be curious to hear if and how their opinions changed since then. As mentioned in the initial issue message above, their recommendation was to use a similar approach like https://github.com/cloudfoundry-incubator/backup-and-restore-sdk-release/blob/develop/jobs/s3-versioned-blobstore-backup-restorer/spec#L30 and empty out the scripts based on a property.\nAdditionally, they had two epics: \n https://www.pivotaltracker.com/epic/show/4148046 to look at selective backup/restore for blobstores\n https://www.pivotaltracker.com/epic/show/4027472 for incremental backup/restore on S3 and compatible blobstores.\nWhile the BBR team seems to already investigate and implement solutions, I feel that we're still stuck at a conceptual level of whose responsibility it should be to make which decisions about what to backup.. Regarding who should be responsible for selecting what to update\n\nRevisiting this, I'm still of the opinion that \"what\" gets backed up should be the responsibility of the backup agent/tool/operator. In environments where business continuity and standard provisioning may be separate teams, I think it's too risky to introduce job-level properties to disable backup support \n\nThat's already what BOSH does now. If you are using an external blobstore, you would have to include an additional ops-file to also backup that one when you're doing a backup of your DB. If you don't do it, the assumption is that you have some process to restore the necessary blobstore contents. It already is in the responsibility of the BOSH operator to decide for backing up the external blobstore or not. What is even more problematic, in my opinion, is that there is a non-obvious difference between external and internal blobstores: One needs specific enablement, the other is always backed up.\nRegarding work and opinions of the BBR team: As documented in https://github.com/cloudfoundry-incubator/bosh-backup-and-restore/issues/18, I had a while ago a conversation on slack with @glestaris about that. I'd be curious to hear if and how their opinions changed since then. As mentioned in the initial issue message above, their recommendation was to use a similar approach like https://github.com/cloudfoundry-incubator/backup-and-restore-sdk-release/blob/develop/jobs/s3-versioned-blobstore-backup-restorer/spec#L30 and empty out the scripts based on a property.\nAdditionally, they had two epics: \n https://www.pivotaltracker.com/epic/show/4148046 to look at selective backup/restore for blobstores\n https://www.pivotaltracker.com/epic/show/4027472 for incremental backup/restore on S3 and compatible blobstores.\nWhile the BBR team seems to already investigate and implement solutions, I feel that we're still stuck at a conceptual level of whose responsibility it should be to make which decisions about what to backup.. In the meanwhile, cf-deployment released this: https://github.com/cloudfoundry/cf-deployment/blob/master/operations/backup-and-restore/skip-backup-restore-droplets-and-packages.yml\nIt even removes the respective directories from the backup for the internal blobstore: https://github.com/cloudfoundry/cf-deployment/blob/4eb08d924b77dd0ce492ece5fc18d3afd4dec827/operations/backup-and-restore/skip-backup-restore-droplets-and-packages.yml#L26-L28 and the blobstore's backup script selectively does a backup of directories or not: https://github.com/cloudfoundry/capi-release/blob/c20a012416d4893972dcc58587a06fda0e3a323b/jobs/blobstore/templates/backup.erb#L10-L19\nWould this be an approach we'd be willing to take for BOSH? I feel we're now discussing this for quite a few months without a clear path forward. This is effectively stopping us from using bbr for regular backups, as the blobstore backups take too long and are too big on production directors.. In the meanwhile, cf-deployment released this: https://github.com/cloudfoundry/cf-deployment/blob/master/operations/backup-and-restore/skip-backup-restore-droplets-and-packages.yml\nIt even removes the respective directories from the backup for the internal blobstore: https://github.com/cloudfoundry/cf-deployment/blob/4eb08d924b77dd0ce492ece5fc18d3afd4dec827/operations/backup-and-restore/skip-backup-restore-droplets-and-packages.yml#L26-L28 and the blobstore's backup script selectively does a backup of directories or not: https://github.com/cloudfoundry/capi-release/blob/c20a012416d4893972dcc58587a06fda0e3a323b/jobs/blobstore/templates/backup.erb#L10-L19\nWould this be an approach we'd be willing to take for BOSH? I feel we're now discussing this for quite a few months without a clear path forward. This is effectively stopping us from using bbr for regular backups, as the blobstore backups take too long and are too big on production directors.. Related thread on slack: https://cloudfoundry.slack.com/archives/C02HPPYQ2/p1534375069000100. Related thread on slack: https://cloudfoundry.slack.com/archives/C02HPPYQ2/p1534375069000100. @jfmyers9 If it is alright with you, I can merge this one and the corresponding PR in the cli.. @jfmyers9 If it is alright with you, I can merge this one and the corresponding PR in the cli.. Cross-posted from https://github.com/cloudfoundry-incubator/kubo-deployment/issues/339. Cross-posted from https://github.com/cloudfoundry-incubator/kubo-deployment/issues/339. The contents of the agent log would be helpful, as described in https://bosh.io/docs/tips/#unreachable-agent\nAlso: could you ssh into one of the VMs and see which ntp servers are configured? The configuration lives in /var/vcap/bosh/etc/ntpserver. Others have reported issues on Xenial where the ntp servers couldn't be reached \u2013 I think this should have been fixed, but just to double-check.. The contents of the agent log would be helpful, as described in https://bosh.io/docs/tips/#unreachable-agent\nAlso: could you ssh into one of the VMs and see which ntp servers are configured? The configuration lives in /var/vcap/bosh/etc/ntpserver. Others have reported issues on Xenial where the ntp servers couldn't be reached \u2013 I think this should have been fixed, but just to double-check.. > However, I wanted to clarify how to gain sudo access on these machines. As far as I have tried, I'm unable to view the logs of a compilation machine or if the machine didn't start correctly (as in this case)\nGiven that your VMs error out with timout pinging, I guess you have two choices\n setting a password for the vcap user in the compilation block of your cloud config via env.bosh.password and try to access the VM with normal ssh using the configured default ssh key. Depending on where the agent fails, the password for obtaining superuser rights is either c1oudc0w or your configured password\n accessing the VM via the OpenStack VPN console in Horizon. The vcap password should be c1oudc0w or your configured password\nHope that helps.. > However, I wanted to clarify how to gain sudo access on these machines. As far as I have tried, I'm unable to view the logs of a compilation machine or if the machine didn't start correctly (as in this case)\nGiven that your VMs error out with timout pinging, I guess you have two choices\n setting a password for the vcap user in the compilation block of your cloud config via env.bosh.password and try to access the VM with normal ssh using the configured default ssh key. Depending on where the agent fails, the password for obtaining superuser rights is either c1oudc0w or your configured password\n accessing the VM via the OpenStack VPN console in Horizon. The vcap password should be c1oudc0w or your configured password\nHope that helps.. @bloeys Thanks, that helped! I was able to reproduce this on our OpenStack.\nFor now, this seems to happen reliably, if you are using a flavor which has an ephemeral device configured. If it is possible, I recommend for now the workaround to not configure an ephemeral device and increase the size of your root device accordingly. The agent will partition the root device into root and ephemeral \u2013 this should be transparent for your installed releases.\nWe will follow-up next week with https://www.pivotaltracker.com/story/show/160688556. @bloeys Thanks, that helped! I was able to reproduce this on our OpenStack.\nFor now, this seems to happen reliably, if you are using a flavor which has an ephemeral device configured. If it is possible, I recommend for now the workaround to not configure an ephemeral device and increase the size of your root device accordingly. The agent will partition the root device into root and ephemeral \u2013 this should be transparent for your installed releases.\nWe will follow-up next week with https://www.pivotaltracker.com/story/show/160688556. Seems like this is related to the way how OpenStack recycles ephemeral devices. Apparently, those are not completely nulled-out by default, therefore parted thinks there is still data on it and reports a partition table of type loop: https://www.pivotaltracker.com/story/show/160688556/comments/195477876\nThe linked commit in the agent makes it treat loop partition tables as if there was no partition table and fixes your issue. Now waiting for a stemcell to be released which contains the fix in the agent.. Seems like this is related to the way how OpenStack recycles ephemeral devices. Apparently, those are not completely nulled-out by default, therefore parted thinks there is still data on it and reports a partition table of type loop: https://www.pivotaltracker.com/story/show/160688556/comments/195477876\nThe linked commit in the agent makes it treat loop partition tables as if there was no partition table and fixes your issue. Now waiting for a stemcell to be released which contains the fix in the agent.. This is fixed with the most recent Xenial stemcells >= 170.3\nThanks again for raising this!. This is fixed with the most recent Xenial stemcells >= 170.3\nThanks again for raising this!. @mfine30 Are you also releasing a new 268 version with that fix?. @mfine30 Are you also releasing a new 268 version with that fix?. That's all good, thanks! I guess I got confused as I expected a commit to happen on some 268.x branch. I'm currently not clear what the policy for bringing fixes into the latest version is: master vs. release branch.. That's all good, thanks! I guess I got confused as I expected a commit to happen on some 268.x branch. I'm currently not clear what the policy for bringing fixes into the latest version is: master vs. release branch.. @pivotal-jamil-shamy We didn't open an issue yet, as we're still at the stage to figure out what the issue is.\nSymptoms are:\n bosh-dns is switched on\n we have >6000 VMs\n NATS message seem to take between 9 seconds and 60 seconds to get from Director to agent and vice versa. This is what we found comparing the sent message and received message timestamps in Director debug logs and agent logs\n This causes failed sending X after 45 seconds errors in arbitrary bosh tasks . @pivotal-jamil-shamy We didn't open an issue yet, as we're still at the stage to figure out what the issue is.\nSymptoms are:\n bosh-dns is switched on\n we have >6000 VMs\n NATS message seem to take between 9 seconds and 60 seconds to get from Director to agent and vice versa. This is what we found comparing the sent message and received message timestamps in Director debug logs and agent logs\n This causes failed sending X after 45 seconds errors in arbitrary bosh tasks . yeah, apologies for requesting two things at once ;)\nI think we can contribute the second part of enabling monitoring with https://www.pivotaltracker.com/story/show/162268374\nWe already had this on a branch to debug our production env a bit, so this shouldn't be too much effort.. yeah, apologies for requesting two things at once ;)\nI think we can contribute the second part of enabling monitoring with https://www.pivotaltracker.com/story/show/162268374\nWe already had this on a branch to debug our production env a bit, so this shouldn't be too much effort.. > For what it's worth, I think it should be exposed on a localhost-only bind.\nI think so as well, that's why I included the sample configuration above, which should achieve limiting access to localhost only:\nserver {\n    location /stats {\n        stub_status;\n        access_log off;\n        allow 127.0.0.1;\n        deny all;\n    }\n}\nIf we are agreeing this is something useful to do and have, I'd put this into our backlog and prioritize it.\n\nI don't know how similar this is, but I @freddesbiens and I have spent a bit of time talking about better monitoring for the director in general as something to focus on in the mid-term. It seems like this could fit well into that sort of effort?\n\nThis is one of the overarching goals for us for the next 6 months. Improving the director's observability, make it able to deal with more load in terms of deployments&instances&requests, and make it more resilient to big infrastructure-related like AZ failure or partial network outages. If you have some thoughts on this, I'd like to hear your current ideas, @mfine30 @freddesbiens! . > For what it's worth, I think it should be exposed on a localhost-only bind.\nI think so as well, that's why I included the sample configuration above, which should achieve limiting access to localhost only:\nserver {\n    location /stats {\n        stub_status;\n        access_log off;\n        allow 127.0.0.1;\n        deny all;\n    }\n}\nIf we are agreeing this is something useful to do and have, I'd put this into our backlog and prioritize it.\n\nI don't know how similar this is, but I @freddesbiens and I have spent a bit of time talking about better monitoring for the director in general as something to focus on in the mid-term. It seems like this could fit well into that sort of effort?\n\nThis is one of the overarching goals for us for the next 6 months. Improving the director's observability, make it able to deal with more load in terms of deployments&instances&requests, and make it more resilient to big infrastructure-related like AZ failure or partial network outages. If you have some thoughts on this, I'd like to hear your current ideas, @mfine30 @freddesbiens! . Turns out, once I made the properties in the CPI optional, template rendering went through smoothly, thanks @xtreme-andrew-su!\nNow: should we make the removal of the nats properties an additional ops-file in bosh-deployments such that you can use it when the CPI supports it?. Turns out, once I made the properties in the CPI optional, template rendering went through smoothly, thanks @xtreme-andrew-su!\nNow: should we make the removal of the nats properties an additional ops-file in bosh-deployments such that you can use it when the CPI supports it?. Some context from my POV: We're not using datadog, but graphite instead. What's working for us is setting a prefix for graphite metrics that's either the 'landscape name' or the 'director name' (which already is in your deployment manifest, anyways). Both of these can be populated during deployment time, do not change on re-deployment, and should be unique per BOSH director.\nLooking at your use-case and comparing it to the change, I'm not sure if we would need a generic list of key-value pairs to be forwarded to datadog, or if one, well-defined tag (like 'director-name' or 'environment-name', or similar) might suffice.\nwdyt?\n. Some context from my POV: We're not using datadog, but graphite instead. What's working for us is setting a prefix for graphite metrics that's either the 'landscape name' or the 'director name' (which already is in your deployment manifest, anyways). Both of these can be populated during deployment time, do not change on re-deployment, and should be unique per BOSH director.\nLooking at your use-case and comparing it to the change, I'm not sure if we would need a generic list of key-value pairs to be forwarded to datadog, or if one, well-defined tag (like 'director-name' or 'environment-name', or similar) might suffice.\nwdyt?\n. While you're closing this, @jfmyers9, is there any place we can follow the planned improvements for \n\nwe're going to look into improving the UX rather than investing in code optimization at this stage.\n\n/cc @deniseyu  . While you're closing this, @jfmyers9, is there any place we can follow the planned improvements for \n\nwe're going to look into improving the UX rather than investing in code optimization at this stage.\n\n/cc @deniseyu  . This might be related to the deployment specifying the stemcell with properties name and version, while the addon selects on property os.\nmanifest has\nstemcells:\n - alias: trusty\n    name: bosh-google-kvm-ubuntu-trusty-go_agent\n    version: '3586.56'\naddon selector has\naddons:\n - include:\n     stemcell:\n     - os: ubuntu-trusty\nCould you switch your stemcell definition in the manifest to \nstemcells:\n - alias: trusty\n    os: ubuntu-trusty\n    version: '3586.56'\nand see if this works as expected?. This might be related to the deployment specifying the stemcell with properties name and version, while the addon selects on property os.\nmanifest has\nstemcells:\n - alias: trusty\n    name: bosh-google-kvm-ubuntu-trusty-go_agent\n    version: '3586.56'\naddon selector has\naddons:\n - include:\n     stemcell:\n     - os: ubuntu-trusty\nCould you switch your stemcell definition in the manifest to \nstemcells:\n - alias: trusty\n    os: ubuntu-trusty\n    version: '3586.56'\nand see if this works as expected?. I'm happy to merge this to improve the execution time of pre-start! The question from above still remains:\n\nWe were also discussing whether we even need the recursion?\nWhy not just chown vcap:vcap $STORE_DIR?\n\nCan anyone remember a specific reason why we should need to chown recursively and not just the directory itself?. I'm happy to merge this to improve the execution time of pre-start! The question from above still remains:\n\nWe were also discussing whether we even need the recursion?\nWhy not just chown vcap:vcap $STORE_DIR?\n\nCan anyone remember a specific reason why we should need to chown recursively and not just the directory itself?. Incredible code archeology, thanks! I've merged this change for now, but we can just remove the recursive part, if we feel we don't need it anymore, I guess.. Incredible code archeology, thanks! I've merged this change for now, but we can just remove the recursive part, if we feel we don't need it anymore, I guess.. Hey @charleshansen and @dpb587-pivotal, thanks for following up on this!\nThe change looks great, I'll try to draw some graphs for NATS performance using a bigger setup we have here. We also try to do some even bigger scaling to 2k instances and see if this change makes it possible to deploy at all for us without disabling sync_dns after instance update.. ~~Should we move this functionality to bpm's pre-start instead to ensure it is always called before executing start?~~\nEdit: I just realized that's what the commit linked by Danny above does. I'm confused why the unix socket would cause problems as pointed out by @dsboulder above then?. @jhvhs thanks for your data and cross-linking issue #2032 here.\n@dpb587-pivotal thanks for the explicit mention of the floating releases complication. On deleting a stemcell, we could still have some delete mechanism for blobs based on major version matching, right?. @charleshansen \n\nDo you also expect bosh delete-stemcell to trigger this cleanup behavior?\n\nThanks for having a look at this! By reading my original issue above again, I realized that I should have phrased the Expected behavior section a bit differently to make it more clear:\n\nExpected behavior\nThe compiled bits for a release are also removed on bosh clean-up [--all] when the stemcell they have been compiled for is removed and there is no other stemcell with the same major version these packages could be applicable for on the director.\n\nI don't expect bosh delete-stemcell to delete blobs from the blobstore. Regularly calling bosh clean-up is necessary anyways, so it would be perfectly fine to do the work there. . side note: this is true for any error response code, regardless of the reason. Most likely because the response code isn't checked here: https://github.com/cloudfoundry/bosh-cli/blob/a18c72307483843b5e190c383ff5e6835b3a8bf2/installation/tarball/provider.go#L124 but the golang httpclient doesn't throw errors on no-2xx responses: https://golang.org/pkg/net/http/#Client.Get. We did this so we wouldn't need to make the ctl script an .erb which needs to be rendered. That was propagated some time ago by Dr Nic and his bosh-gen. If you're fine with making the ctl a template we can do that.\n. nice, thanks :)\n. This is literally taken from how all the other plugins are enabled - which seems to work. If we change it for this plugin we should probably do it for all of them in an additional change?\n. @mrdavidlaing Sure, we can add a setting with which you can also forward heartbeat events. We decided not to do this in that change, because were not sure if syslog is the right format for these types of events. Instead, we use the graphite-plugin to forward them to a stream processor and time-series database and actually use the data provided in the heartbeats for alerting and dashboards.\n. My guess at what you are trying to achieve here is to have bosh help scp print out the parameters that can be passed into that command. usage is probably not meant to be used this way. Other commands, such as bosh deployment or bosh login print their parameters, the printing even considers if the parameters are optional or not. To achieve this, they use some funky logic hidden in https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/help.rb#L62-L74\nThe parsing of method parameters happens in https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/command_handler.rb#L66-L74.\nSo the problem seems to be that bosh ssh and bosh scp accept *args instead of specifying what might go in there \u2013 if these methods were taking specific arguments, you'd get the documentation for free.\nNote that there is some funky comparisons going on with cmd.usage in https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/help.rb#L40 and https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/commands/help.rb#L51 so your change might add some unexpected sideeffects.\nThe configuration adds commands with usage as key: https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/config.rb#L39 and this is used in parsing the commandline to find out which command to run: https://github.com/cloudfoundry/bosh/blob/master/bosh_cli/lib/cli/runner.rb#L214-L218\nSo I'd rather not have the usage contain parameters and rather have some real arguments in the method itself.\n. instance_data should probably be deployment_data\n. Couldn't all of this agent. business go into Agent.new(name, instance)?\n. why is id special here?\n. maybe use the attr_reader instead of instance variables here?\n. at some point in the future, probably. Right now, we don't have a clear guidance on when to increase the number of workers and what that would mean for other things, such as your database connection pool. Therefore, I'd like to not expose this number for now.. This logic means: params[:disk_properties] can either be empty, or contain the string default to achieve the same thing. Can we just remove the string default, such that if you do want the default, you don't submit the parameter?. Is there a better way to attach additional columns to sequel models which doesn't involve turning it into a hash? . Good catch, this seems to have gone wrong during squashing the changes. I've pushed a new set of changes which fixes the squash misery.. This line is what I'm trying to do: specifically add the agent_id to an instance, without having to load the vm for each instance with a separate SELECT * FROM \"vms\" WHERE ((\"instance_id\" = 3683) AND (\"active\" IS TRUE)) LIMIT 1\nI'm not sure why the query in place is already using a lot of sequel internals like specific inner_join, qualify, and select_append, it seems pretty much all of it could be done with a single eager_graph like Artist.eager_graph(albums: proc{|ds| ds.where{year > 1990}}).all\nI'll give that a try and see what happens, thanks!. Thanks @charleshansen for the suggestion. I've added a second comment above where the change is maybe a bit clearer. I'll try to switch to eager_graph (I think that's necessary because we're filtering on instance.vm.active?) and see what happens.. @charleshansen ok, I give up. I tried wrestling sequel into what's necessary, but couldn't do it. Here is what I'm unsure how this would work in sequel\n instance has a one_to_many with vms\n we want to achieve that for an instance i.agent_id does not lazily do SELECT * FROM \"vms\" WHERE ((\"instance_id\" = 3683) AND (\"active\" IS TRUE)) LIMIT 1\n* At the same time, the above method needs to filter out instances based on fields in the associated vm. This is what I was unable to figure out in sequel. Any claused passed to eager_graph only filters on what to eagerly load, not what to keep from the list of instances.\n  * examples are: a vm.cid which gets passed into the method and VMs with vm.active != true \nNot sure how/if that is possible in sequel.\nI tried something like Models::Instance.where(compilation: false).eager_graph(vms: proc{|vm| vm.where{cid !~ vm_cid_to_exclude}}).all, but that still leaves all the instances, it doesn't filter out the ones where the vm.cid matches. for those instances, it just doesn't eager load the vm.. > It should actually be both faster and simpler to do the filtering in memory instead of using sql queries.\nThis might sound stupid, but how did you validate this?. ",
    "sreelathakoye": "Hello Cppforlife,\nI have tried again installing bosh_cli in another user in a vm. And now it is showing me the following issue\nDeploying new micro BOSH instance microbosh/microbosh.yml' tohttps://microbosh/microbosh.yml:25555' (type 'yes' to continue): yes\nDeploy Micro BOSH\n  using existing stemcell (00:00:00)\nCreating VM from ami-03223a6a       |oo                      | 1/11 00:00:52  ETA: --:--:--/usr/lib/ruby/1.9.1/net/http.rb:762:in initialize': execution expired (Timeout::Error)\n        from /usr/lib/ruby/1.9.1/net/http.rb:762:inopen'\n        from /usr/lib/ruby/1.9.1/net/http.rb:762:in block in connect'\n        from /usr/lib/ruby/1.9.1/net/http.rb:762:inconnect'\n        from /usr/lib/ruby/1.9.1/net/http.rb:755:in do_start'\n        from /usr/lib/ruby/1.9.1/net/http.rb:750:instart'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/net/http/connection_pool/session.rb:118:in start'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/net/http/connection_pool.rb:208:in_create_session'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/net/http/connection_pool.rb:193:in session_for'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/net/http/connection_pool.rb:171:inrequest'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/net/http/connection_pool/connection.rb:173:in request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/http/net_http_handler.rb:66:inhandle'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:231:in block in make_sync_request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:267:inretry_server_errors'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:227:in make_sync_request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:472:inblock (2 levels) in client_request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:355:in log_client_request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:441:inblock in client_request'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:337:in return_or_raise'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/core/client.rb:440:inclient_request'\n        from (eval):3:in describe_images'\n        from /var/lib/gems/1.9.1/gems/aws-sdk-1.8.5/lib/aws/ec2/image.rb:214:inexists?'\n        from /var/lib/gems/1.9.1/gems/bosh_aws_cpi-1.5.0.pre.1657/lib/cloud/aws/stemcell.rb:9:in find'\n        from /var/lib/gems/1.9.1/gems/bosh_aws_cpi-1.5.0.pre.1657/lib/cloud/aws/stemcell_finder.rb:10:infind_by_region_and_id'\n        from /var/lib/gems/1.9.1/gems/bosh_aws_cpi-1.5.0.pre.1657/lib/cloud/aws/cloud.rb:84:in block in create_vm'\n        from /var/lib/gems/1.9.1/gems/bosh_common-1.5.0.pre.1657/lib/common/thread_formatter.rb:46:inwith_thread_name'\n        from /var/lib/gems/1.9.1/gems/bosh_aws_cpi-1.5.0.pre.1657/lib/cloud/aws/cloud.rb:82:in create_vm'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:244:increate_vm'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:123:in block in create'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:79:instep'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:122:in create'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:98:inblock in create_deployment'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:92:in with_lifecycle'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/deployer/instance_manager.rb:98:increate_deployment'\n        from /var/lib/gems/1.9.1/gems/bosh_cli_plugin_micro-1.5.0.pre.1657/lib/bosh/cli/commands/micro.rb:182:in perform'\n        from /var/lib/gems/1.9.1/gems/bosh_cli-1.5.0.pre.1657/lib/cli/command_handler.rb:57:inrun'\n        from /var/lib/gems/1.9.1/gems/bosh_cli-1.5.0.pre.1657/lib/cli/runner.rb:56:in run'\n        from /var/lib/gems/1.9.1/gems/bosh_cli-1.5.0.pre.1657/lib/cli/runner.rb:16:inrun'\n        from /var/lib/gems/1.9.1/gems/bosh_cli-1.5.0.pre.1657/bin/bosh:7:in <top (required)>'\n        from /usr/local/bin/bosh:23:inload'\n        from /usr/local/bin/bosh:23:in `'\nPFB log file details,\nLogfile created on 2014-09-25 18:28:55 +0530 by logger.rb/31641\nI, [2014-09-25T18:28:57.744832 #6502]  INFO -- : BOSH Registry starting...\nI, [2014-09-25T18:28:57.744995 #6502]  INFO -- : HTTP server is starting on port 25888...\nI, [2014-09-25T18:29:48.114604 #6502]  INFO -- : BOSH Registry shutting down...\nSo is this something related to registry issue?\nAnd port 25888 is free and nothing else is starting at that port.\n. Hello ccpforlife,\nThank you for your help on this. But as ia facing lot many issues on my local environment. I thought of doing this whole activity of deploying microbosh on AWS , from an instance of AWS.\nSO i have manually created one instance of AWS, and have installed bosh-cli in it.\nAnd from there iam trying to deploy microbosh using same command,\n$ bosh micro deploy ami-2012bb48\nNow it gives me following error\nDeploying new micro BOSH instance microbosh/microbosh.yml' tohttps://54.165.210.67:25555' (type 'yes' to continue): yes\n/usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:128:in require': cannot load such file -- thin_parser (LoadError)\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:128:inrescue in require'\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:39:in require'\n        from /usr/local/share/ruby/gems/2.0/gems/thin-1.5.1/lib/thin.rb:39:in'\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:54:in require'\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:54:inrequire'\n        from /usr/local/share/ruby/gems/2.0/gems/bosh-registry-1.2719.0/lib/bosh/registry.rb:14:in <top (required)>'\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:54:inrequire'\n        from /usr/local/share/ruby/site_ruby/2.0/rubygems/core_ext/kernel_require.rb:54:in require'\n        from /usr/local/share/ruby/gems/2.0/gems/bosh-registry-1.2719.0/bin/bosh-registry:3:in'\n        from /usr/local/bin/bosh-registry:23:in load'\n        from /usr/local/bin/bosh-registry:23:in'\nbosh-registry -c /tmp/d20140926-2317-s6bura/bosh_registry_yml20140926-2317-e5lw02 failed, exit status=1\nPFB my gem env\ngem env\nRubyGems Environment:\n- RUBYGEMS VERSION: 2.4.1\n- RUBY VERSION: 2.0.0 (2014-05-08 patchlevel 481) [x86_64-linux]\n- INSTALLATION DIRECTORY: /usr/local/share/ruby/gems/2.0\n- RUBY EXECUTABLE: /usr/bin/ruby2.0\n- EXECUTABLE DIRECTORY: /usr/local/bin\n- SPEC CACHE DIRECTORY: /root/.gem/specs\n- SYSTEM CONFIGURATION DIRECTORY: /etc\n- RUBYGEMS PLATFORMS:\n  - ruby\n  - x86_64-linux\n- GEM PATHS:\n  - /usr/local/share/ruby/gems/2.0\n  - /root/.gem/ruby/2.0\n  - /usr/share/ruby/gems/2.0\n- GEM CONFIGURATION:\n  - :update_sources => true\n  - :verbose => true\n  - :backtrace => false\n  - :bulk_threshold => 1000\n- REMOTE SOURCES:\n  - https://rubygems.org/\n- SHELL PATH:\n  - /usr/local/sbin\n  - /usr/local/bin\n  - /sbin\n  - /bin\n  - /usr/sbin\n  - /usr/bin\n  - /opt/aws/bin\n  - /root/bin\n    So iam not sure what this thin_parser is all about, and which particular gem is giving this exception.\n    Help in this would be great to move further.\nThanks & Regards,\nSreelatha.\n. Hello Cppforlife,\nFinally with reinstalling ruby, my issues were solved to some extent, but still iam facing issue with Deploying Bosh to AWS,\nwhen i give this command it gives,\nbosh deployment bosh/bosh.yml\nIncorrect YAML structure in /root/bosh-workspace/deployments/bosh/bosh.yml': undefined methodvalue' for #Psych::Nodes::Mapping:0x000000020d23c8\nIam using , ruby -v\nruby 1.9.3p547 (2014-05-14 revision 45962) [x86_64-linux]\nI have seen in some threads like we should use ruby 1.9.3p5XX to solve this kind of error. But now iam facing same issue.\nAny help in this would be great.\nThanks&Regards,\nSreelatha K.\n. Hello cppforlife,\nSorry to trouble once again by opening this issue.\nBut I was able to deploy cloudfoundry on AWS, and even able to set target and push one sample app.\nBut when today again I try to target again  that using\ncf target http://api.x.x.x.x.xip.io\nSetting target to http://api.x.x.x.x.xip.io... FAILED\nTarget refused connection.\nThis I have tried from machine on AWS.\nBut when I try the same thing from another machine, it was able to target like,\nC:\\Users\\sreelatha_koye>cf api http://api.x.x.x.x.xip.io\nSetting api endpoint to http://api.x.x.x.x.xip.io...\nWarning: Insecure http API endpoint detected: secure https API endpoints are rec\nommended\nOK\nAPI endpoint:   http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nC:\\Users\\sreelatha_koye>cf login\nAPI endpoint: http://api.x.x.x.x.xip.io\nWarning: Insecure http API endpoint detected: secure https API endpoints are rec\nommended\nFAILED\nServer error, status code: 503, error code: , message:\nAPI endpoint:   http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nIam unable to figure out if this is a network problem, or some problem with my deployment.\nHelp on this would be really useful for me.\nThanks & regards,\nSrilatha.\n. Hello cppforlife,\nThank you for your reply.\nI see following when I tried CF_TRACE\nCF_TRACE=true cf target http://api.x.x.x.x.xip.io\nTarget refused connection.\nCF_TRACE=true cf login show\nTime of crash:\n  2014-10-27 11:06:31 +0000\nCFoundry::TargetRefused: target refused connection (getaddrinfo: Name or service not known)\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:172:in rescue in request_uri'\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:90:inrequest_uri'\ncfoundry-4.7.2.rc1/lib/cfoundry/rest_client.rb:60:in request'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:93:inrequest_raw'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:88:in request'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:66:inget'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:62:in info'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:29:inuaa'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:97:in refresh_token!'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:84:inrequest'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:66:in get'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:62:ininfo'\ncfoundry-4.7.2.rc1/lib/cfoundry/baseclient.rb:29:in uaa'\ncfoundry-4.7.2.rc1/lib/cfoundry/concerns/login_helpers.rb:6:inlogin_prompts'\ncf-5.4.7/lib/cf/cli/start/login.rb:35:in login'\nmothership-0.5.1/lib/mothership/base.rb:66:inrun'\nmothership-0.5.1/lib/mothership/command.rb:72:in block in invoke'\nmothership-0.5.1/lib/mothership/command.rb:86:ininstance_exec'\nmothership-0.5.1/lib/mothership/command.rb:86:in invoke'\nmothership-0.5.1/lib/mothership/base.rb:55:inexecute'\ncf-5.4.7/lib/cf/cli.rb:195:in block (2 levels) in execute'\ncf-5.4.7/lib/cf/cli.rb:206:insave_token_if_it_changes'\ncf-5.4.7/lib/cf/cli.rb:194:in block in execute'\ncf-5.4.7/lib/cf/cli.rb:123:inwrap_errors'\ncf-5.4.7/lib/cf/cli.rb:190:in execute'\nmothership-0.5.1/lib/mothership.rb:45:instart'\ncf-5.4.7/bin/cf:18:in <top (required)>'\nruby-1.9.3-p547/bin/cf:23:inload'\nruby-1.9.3-p547/bin/cf:23:in <main>'\nruby-1.9.3-p547/bin/ruby_executable_hooks:15:ineval'\nruby-1.9.3-p547/bin/ruby_executable_hooks:15:in `'\nI couldn't understand the exact problem.\nWhen I try the same command on other machine where I was able to set target,\nIt says \nC:\\Users\\sreelatha_koye>cf login show\nVERSION:\n6.6.2-0c953cf\nAPI endpoint: http://api.x.x.x.x.xip.io\nREQUEST: [2014-10-27T16:46:45+05:30]\nGET /v2/info HTTP/1.1\nHost: api.x.x.x.x.xip.io\nAccept: application/json\nContent-Type: application/json\nUser-Agent: go-cli 6.6.2-0c953cf / windows\nRESPONSE: [2014-10-27T16:47:17+05:30]\nHTTP/1.1 200 OK\nContent-Length: 219\nAge: 89\nConnection: Keep-Alive\nContent-Type: application/json;charset=utf-8\nDate: Tue, 07 Oct 2014 09:01:35 GMT\nProxy-Connection: Keep-Alive\nServer: nginx\nX-Content-Type-Options: nosniff\nX-Vcap-Request-Id: 434d795f-58a2-4eac-8e21-2f5008bc6a95\n{\"name\":\"vcap\",\"build\":\"2222\",\"support\":\"http://support.cloudfoundry.com\",\"version\":2,\"description\":\"Cloud Foundry sponsored by Pivotal\",\"authorization_endpoint\":\"http://uaa.x.x.x.x.xip.io\",\"api_version\":\"2.0\n.0\"}\nWarning: Insecure http API endpoint detected: secure https API endpoints are recommended\nREQUEST: [2014-10-27T16:47:17+05:30]\nGET /login HTTP/1.1\nHost: uaa.54.210.241.175.xip.io\nAccept: application/json\nContent-Type: application/json\nUser-Agent: go-cli 6.6.2-0c953cf / windows\nRESPONSE: [2014-10-27T16:48:11+05:30]\nHTTP/1.1 503 Service Unavailable\nConnection: close\nContent-Length: 1030\nCache-Control: no-cache\nContent-Type: text/html; charset=utf-8\nPragma: no-cache\nProxy-Connection: close\n\nNetwork Error\n\n\n\n\n\n\n\n\n\nNetwork Error (dns_server_failure)\n\n\n\n\n\n\nYour request could not be processed because an error occurred contacting the DNS server.\n\n\n\n\nThe DNS server may be temporarily unavailable, or there could be a network problem.\n\n\n\n\n\nFor assistance, contact your network support team.Your request was categorized by Blue Coat Web Filter as 'Dynamic DNS Host'. If you wish to question or dispute this result, please click here.\n\n\n\n\n\n\nFAILED\nServer error, status code: 503, error code: , message:\nFAILED\nServer error, status code: 503, error code: , message:\nAPI endpoint:   http://api.x.x.x.x.xip.io (API version: 2.0.0)\nNot logged in. Use 'cf login' to log in.\nNow iam totally confused, like, on the first machine in which iam even unable to set target that is a instance of AWS. So no proxy issue.\nFor second machine it says proxy issue. and able to set target.\nHelp in this would be great for me to go ahead.\nThanks & Regards,\nSrilatha.\n. Hello Bosh-ci-push-pull,\nThank you very much for your response, now I have tried installing cf v6.\nNow its working fine.\nThank you a ton , this worked.\nThanks & Regards,\nSrilatha.\n. ",
    "Dulanjalie": "Hi all,\nI am using MicroBOSH and i am hitting the same issue. I am using the latest stemcells and my security group configs are correct. \ncfoundry@c-foundry:~/micro-deployment$ bosh micro deploy test/light-bosh-stemcell-2986-aws-xen-hvm-ubuntu-trusty-go_agent.tgz\nNo bosh-deployments.yml file found in current directory.\nConventionally, bosh-deployments.yml should be saved in /home/cfoundry.\nIs /home/cfoundry/micro-deployment a directory where you can save state? (type 'yes' to continue): yes\nDeploying new micro BOSH instance manifest.yml' tohttps://[ip addr of vip]>:25555' (type 'yes' to continue): yes\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    bosh-aws-xen-hvm-ubuntu-trusty-go_agent\nVersion: 2986\nStarted deploy micro bosh\n  Started deploy micro bosh > Unpacking stemcell. Done (00:00:00)\n  Started deploy micro bosh > Uploading stemcell. Done (00:00:10)\n  Started deploy micro bosh > Creating VM from ami-3742b55c light. Done (00:00:35)\n  Started deploy micro bosh > Waiting for the agent\n. ",
    "alfredcs": "Changed diego: false --> diego: disabled resolved the same issue occurred to me.\n. ",
    "ghost": "I'm an employee of SAP which AFAIK has already signed a corporate CLA. Do I still need to sign and send a CLA?\n. I'm missing most of the HTTP client configuration, to be more specific: http_read_timeout, http_wire_trace, max_retries. We use SAP's own IaaS which is EC2 compatible and we need these properties so we can debug/trace compatibility issues between the AWS CPI and our implementation as well as allow for better fault tollerance (max_retries and timeout intervals). \nI understand that exposing these properties can lead to complications if you want to switch the CPI implementation but can you advise us on any other solution where we can control the behavior of the CPI in such manner?\n. OK, I've amended the commit to only allow for max_retires, http_read_timeout and http_wire_trace. I'll propose another change for redirecting the http traces to the CPI logs.\n. Done - added new properties to spec files for both director and registry.\n. Is your Director collocated with PowerDNS?\nOn Mon, May 4, 2015 at 5:43 PM, guanglinlv notifications@github.com wrote:\n\n@cppforlife https://github.com/cppforlife\nit's too large, the part of debug log is given here:\nbosh-155 is successful with the same cf(release and deployment file) and\nbosh(deployment file).\nthis failure is happened once each time when update jobs for each VM\ninstance of CF,bosh deploy again after once failure will be successful.\n{\"method\":\"get_task\",\"arguments\":[\"8f11a8df-c6fd-4e58-4afa-240954c82352\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4 {\"value\":\"prepared\"}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"drain\",\"arguments\":[\"update\",{\"deployment\":\"cf-warden\",\"job\":{\"name\":\"ha_proxy_z1\",\"templates\":[{\"name\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},{\"name\":\"metron_agent\",\"version\":\"8ad734ba7952b6184ac127a45608e64ebb1c4611\",\"sha1\":\"6f229f6f524dafc9ee4d46188c8cd7af1bcb8098\",\"blobstore_id\":\"3ccf6ca4-ff9d-4b4a-8b91-0d30f0dd11c8\"}],\"template\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},\"index\":0,\"networks\":{\"floating\":{\"type\":\"vip\",\"ip\":\"9.91.39.29\",\"cloud_properties\":{},\"dns_record_name\":\"0.ha-proxy-z1.floating.cf-warden.bosh\"},\"cf1\":{\"ip\":\"10.10.10.140\",\"netmask\":\"255.255.255.0\",\"cloud_properti\n es\":{\"net_id\":\"67a8ddc7-4d5f-432f-a154-660df2e8e69c\",\"security_groups\":[\"default\"]},\"default\":[\"dns\",\"gateway\"],\"dns\":[\"10.10.10.3\",\"10.10.10.64\"],\"gateway\":\"10.10.10.1\",\"dns_record_name\":\"0.ha-proxy-z1.cf1.cf-warden.bosh\"}},\"resource_pool\":{\"name\":\"router_z1\",\"cloud_properties\":{\"instance_type\":\"m1.medium\"},\"stemcell\":{\"name\":\"bosh-openstack-kvm-centos-7-go_agent-raw-m\",\"version\":\"2962\"}},\"packages\":{\"haproxy\":{\"name\":\"haproxy\",\"version\":\"630ad6d6e1d3cab4547ce104f3019b483f354613.1\",\"sha1\":\"33522a7aef69d54845e4dbb2e3706a6e19f1b87d\",\"blobstore_id\":\"148473d8-c738-4dce-5dc2-1e064a502354\"},\"common\":{\"name\":\"common\",\"version\":\"99c756b71550530632e393f5189220f170a69647.1\",\"sha1\":\"f4e3d8668a5cd7bbce48a5a600936e8fd1813e2b\",\"blobstore_id\":\"3f8c19b4-aaaf-4ef0-5228-688538998b59\"},\"metron_agent\":{\"name\":\"metron_agent\",\"version\":\"27c6122c3dcabaeac758bae7cb258910a5ac1e42.1\",\"sha1\":\"d50ec240046610a5a6e3493258cc269778baa82d\",\"blobstore_id\":\"1e91a9bd-8bb7-4f08-68ad-186fb62afef1\"}},\"configuration_hash\n \":\"5ddbb2da0fc22bb3790622806d0b3ebce5e44cfc\",\"properties\":{\"ha_proxy\":{\"ssl_pem\":\"-----BEGIN CERTIFICATE-----\\nMIIDETCCAfmgAwIBAgIJANZuykf1uh3LMA0GCSqGSIb3DQEBBQUAMB8xHTAbBgNV\\nBAMMFCouMTAuMjQ0LjAuMzQueGlwLmlvMB4XDTE0MTIyNDIzMTkxM1oXDTI0MTIy\\nMTIzMTkxM1owHzEdMBsGA1UEAwwUKi4xMC4yNDQuMC4zNC54aXAuaW8wggEiMA0G\\nCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCjq73Fgwfj2UT0/+wR9kVVsGAguMj\\npoA0opLCgE0yHStAhSvqq7YpO39dH3vBMWXyr2xIfDyaeZyhV86jWu/ZKswGjNGI\\nZKv/yUINe1bqukOBqd+SHVvkVhxSLJuD1MR83JQMONRjOPJp661/ABpVhnrNfBiA\\nAA6aaFv4/KbyGY/E1FHoUXqEdh4WxaJdfX6SbgG05ArWxhSD7PNj4CYvJWGCdvqP\\nKBsvWFDrkxBHn5h1JIDfZJB8FKP6vaHBr7MU4pIHM+qaZ1Y+8ja0wcgkHn4YHcp6\\nIOhqpck7LaH5Qq2ydYFNTcG4fTbG0jXqcit2WSUxRkXzWnrgo2E0SiHBAgMBAAGj\\nUDBOMB0GA1UdDgQWBBSpCtEDtEvMwaZzXN6Lvk5U7Eyn2zAfBgNVHSMEGDAWgBSp\\nCtEDtEvMwaZzXN6Lvk5U7Eyn2zAMBgNVHRMEBTADAQH/MA0GCSqGSIb3DQEBBQUA\\nA4IBAQB1YIHmw3gPiMn8WDR4yVxDvSVgFHY6ZE1iZb17vVs4N2/mhQZXWJ2nZV02\\ngoAivtgxHOj39sK5OBWsGvrQo5H8dt1t4XmbwB1C6xRerGc25dhDRq42RqhCN0RJ\\nzzjd9b8YSiwtAaZlW36l2jVDLfRa\n pb00tWToF9qYrDrmKy2sekS7g2hbiRStcue/\\nbpT4X/CHxb/lUbpL4m8BpDbkGiOJgl+SEHRx5tZ0Kob/RDQRCcN3p+71FRbDIEBj\\n+8sJl/yUUUPwQ6PNYx6cjtlICWJ1G0l0hRa141VXPqSNCmxYS4dp/8ifPCSoLc+k\\n9TXFkuGl+86CPTyyMJxyMhEcAGZT\\n-----END CERTIFICATE-----\\n-----BEGIN RSA PRIVATE KEY-----\\nMIIEpAIBAAKCAQEAwo6u9xYMH49lE9P/sEfZFVbBgILjI6aANKKSwoBNMh0rQIUr\\n6qu2KTt/XR97wTFl8q9sSHw8mnmcoVfOo1rv2SrMBozRiGSr/8lCDXtW6rpDganf\\nkh1b5FYcUiybg9TEfNyUDDjUYzjyaeutfwAaVYZ6zXwYgAAOmmhb+Pym8hmPxNRR\\n6FF6hHYeFsWiXX1+km4BtOQK1sYUg+zzY+AmLyVhgnb6jygbL1hQ65MQR5+YdSSA\\n32SQfBSj+r2hwa+zFOKSBzPqmmdWPvI2tMHIJB5+GB3KeiDoaqXJOy2h+UKtsnWB\\nTU3BuH02xtI16nIrdlklMUZF81p64KNhNEohwQIDAQABAoIBAC1U3YOIyY5Y9O4n\\nyT2jn/sO2cs9s/rMgrbA4n0bM+FnVnqUDOWC2NDGoihqe4VKIzzmjs5c1CoSB+K3\\n+NerCpOJGzyzdubWvhS9KfzGLjxG5g/CKut6l7yeK78h0aJn4thM9NncK/BqhmET\\nnrsmpPwkd1yFe5fna3+irTtYcvWZgxp8DK4JxIRB0QpJvEwbs9fUFE1E0DVw/uBV\\nCytTIrik2O+n1m8S5xzsFXHXDjXT/TVNi3jtN12Oaj1avYZRP2q45RmcODhtHQIy\\n4o0vqjyAmZb3jOcYysyXVwfyNreIH3Qn0/waflPkyaljMpa1OHVOCJFjh2Xl+aIc\\ndQsME3kCgYE\n A4aXU2xb9SHew84/3ow6ZmBeYLm+7B8GUjDTe7HqLldtTUhlrJSdB\\nSHZHZ/BXEQ126FnbfZ0IISkBjqVCQBw8MgjZAlaEIInDA1DRTLuOfrcyUD6PeSLi\\ngVZHNYbR9MDnmJ3/So5HXiRy2rBWtLKwPlciqbwY+l3xYr2kP/QdCisCgYEA3LpB\\nTBDTqp9j8QVvB/YjCakS+z+kgtO5HyMZO/uh6o9PRHrFQHwZA5y/MRrx9lxJ9zOd\\nfqysKfA7fXs4VzNnRSnOfBmuDHF0HhlIgAShX/RnB94p9xnsQaMae/o2nXcknQng\\n3qogvHWTo16GCJRE+YTmhA0QtvSlScJlTzHfqcMCgYAPRt/rWVoajufvBX85jeJ+\\nNpK6Chx6gPOirm2tSvqqUagJdekYIdk8o61f7xil8ehsALFohronLJSLaMrcdkzp\\nAkpW6y6U2V7XmaAh9szF7Xc9kY67H85//SxjBlauoGTNo1zGWm2ghQ01mxyzrSlb\\nfyC8pxx1zuhpy/cT0V4p8wKBgQDb+EJaq+pFf9L5v5CHPqRsXDKucR5hwt4aScA8\\nJumV+HvmovMw8Ht9PhjLty6rdg3AbY/nTe3FXcPrqYDcZj3kj2VYB7+MZwRxeoDm\\nE7c/CTIkhSMNPqhUQVeDdjg3dSTn25BeVu2I4yPfC7RHmHukru2La/ncWrLebvzH\\nj8x2QQKBgQDevjRDDTWbBkg8HdCRxxCvhfaHBntoSJdTHlr14Gce48NXGaXRJPQT\\n9dMsPsSHkFxEra7G2clGnhpe+pK9V+WTrD9Qnoc+tK808hX1YQ6mBlnR6w99jlaR\\nHVTi2pRhEhbWUkBv2kooXvD6ANb15PbPSF1FK7YyW1KHqcbm+lF22g==\\n-----END RSA PRIVATE KEY-----\\n\",\"disable_http\":false,\"ssl_ciphers\":\"ECDHE-RSA-AES128-GCM-SH\n A256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-RC4-SHA:AES128:AES256:RC4-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK\"},\"request_timeout_in_seconds\":900,\"router\":{\"servers\":{\"z1\":[\"10.10.10.141\"],\"z2\":[]},\"port\":80},\"networks\":{\"apps\":\"cf1\"},\"syslog_daemon_config\":{\"address\":null,\"port\":null,\"transport\":\"tcp\"},\"metron_agent\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457,\"statsd_incoming_port\":8125,\"debug\":false,\"status\":{\"user\":\"\",\"password\":\"\",\"port\":0},\"zone\":\"z1\",\"etcd_query_interval_mill\n iseconds\":5000,\"collector_registrar_interval_milliseconds\":60000},\"loggregator\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457},\"loggregator_endpoint\":{\"shared_secret\":\"loggregator-secret\"},\"nats\":{\"user\":\"nats\",\"password\":\"nats\",\"machines\":[\"10.10.10.142\"],\"port\":4222},\"etcd\":{\"machines\":[\"10.10.10.143\"],\"maxconcurrentrequests\":10}},\"dns_domain_name\":\"bosh\",\"persistent_disk\":0,\"template_hashes\":{\"haproxy\":\"66baf4bb051487714f5b6886a8fa27b645f783f2\",\"metron_agent\":\"aa6290cab96fd7e456be9c34299a42c1aa90a104\"},\"rendered_templates_archive\":{\"blobstore_id\":\"d7018513-3179-436d-85a3-b12f719f3528\",\"sha1\":\"0f48d34d5a0976dd906915472bbd8ffa3c9bb716\"}}],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258 {\"value\":0}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000893s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000891s) SELECT * FROM \"tasks\" WHERE \"id\" = 43\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"stop\",\"arguments\":[],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af {\"value\":{\"agent_task_id\":\"3d66d0dd-07dc-455b-64e5-29f8f7249525\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"3d66d0dd-07dc-455b-64e5-29f8f7249525\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e {\"value\":\"stopped\"}\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Snapshots are disabled; skipping\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping VM update\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping network re-configuration\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000803s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000555s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000515s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000464s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000735s) SELECT * FROM \"records\" WHERE ((\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'A') AND (\"content\" = '9.91.39.29')) LIMIT 1\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.floating.cf-warden.bosh to 9.91.39.29\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000345s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000472s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000347s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000942s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.floating.cf-warden.bosh', 'A', 300, '9.91.39.29', 1430826503) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001686s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000352s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000369s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000311s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000448s) SELECT * FROM \"domains\" WHERE ((\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000285s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000247s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000429s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('39.91.9.in-addr.arpa', 'NATIVE') RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001353s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000301s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000409s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000339s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000490s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000416s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000266s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000497s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (6, '39.91.9.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001004s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000492s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000363s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000323s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000817s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (6, '39.91.9.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001443s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000370s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000291s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000262s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000408s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000441s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000789s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (6, '29.39.91.9.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.floating.cf-warden.bosh', 1430826503) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001202s) COMMIT\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.cf1.cf-warden.bosh to 10.10.10.140\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000457s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000428s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000680s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.cf1.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000586s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.cf1.cf-warden.bosh', 'A', 300, '10.10.10.140', 1430826503) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000957s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000244s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000297s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT * FROM \"domains\" WHERE ((\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000223s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000430s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('10.10.10.in-addr.arpa', 'NATIVE') RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001081s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000357s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000336s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000324s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000488s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000361s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000509s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (7, '10.10.10.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001014s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000331s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000463s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000473s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000892s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (7, '10.10.10.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001340s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000479s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000447s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000382s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000606s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.cf1.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000801s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (7, '140.10.10.10.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.cf1.cf-warden.bosh', 1430826503) RETURNING \nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001254s) COMMIT\nE, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] ERROR -- DirectorJobRunner: Error updating canary instance: #\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in spawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inpopen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:in popen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:incapture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:in flush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:inupdate_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in block in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:instep'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:inblock (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:inblock in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:inblock (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:inblock (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inblock in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl - /var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in spawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inpopen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:in popen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:incapture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:in flush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:inupdate_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in block in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:instep'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:inblock (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:inblock in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:inblock (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:inblock (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inblock in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Shutting down pool\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.005891s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.001478s) SELECT \"stemcells\".* FROM \"stemcells\" INNER JOIN \"deployments_stemcells\" ON ((\"deployments_stemcells\".\"stemcell_id\" = \"stemcells\".\"id\") AND (\"deployments_stemcells\".\"deployment_id\" = 3))\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:cf-warden\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:cf-warden\nI, [2015-05-05 11:48:23 #20891] [task:43]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"2f9236d9-aa9a-464f-be68-2173793a47d4\",\"severity\":3,\"title\":\"director - error during update deployment\",\"summary\":\"Error during update deployment for 'cf-warden' against Director '609afeed-cd42-48dd-a3a6-08e5bf044992': #\",\"created_at\":1430826503}\nE, [2015-05-05 11:48:23 #20891] [task:43] ERROR -- DirectorJobRunner: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in spawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inpopen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:in popen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:incapture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:in flush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:inupdate_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in block in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:instep'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:inblock (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:inblock in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:inblock (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:inblock (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inblock in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000293s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000943s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2015-05-05 11:48:23.258668+0000', \"description\" = 'create deployment', \"result\" = 'No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl', \"output\" = '/var/vcap/store/director/tasks/43', \"checkpoint_time\" = '2015-05-05 11:48:20.863669+0000', \"type\" = 'update_deployment', \"username\" = 'admin' WHERE (\"id\" = 43)\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.002082s) COMMIT\nI, [2015-05-05 11:48:23 #20891] []  INFO -- DirectorJobRunner: Task took 1 minute 32.420035999999996 seconds to process.\nTask 43 error\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/817#issuecomment-98893974.\n. Your Director is not collocated with the PowerDNS since they are on the\ndifferent VMs. Collocated = on the same VM. Recently Director was changed\nto correctly flush DNS entries via\n/var/vcap/jobs/powerdns/bin/powerdns_ctl flush (\nhttps://github.com/cloudfoundry/bosh/commit/2137b55d5405ee8af59474d0764934777dd75006).\nIt looks like it does not properly account for the case when Director is on\na separate VM from the PowerDNS. Not sure what can be done in that case...\n\nOn Mon, May 4, 2015 at 7:20 PM, guanglinlv notifications@github.com wrote:\n\n@bosh-ci https://github.com/bosh-ci @cppforlife\nhttps://github.com/cppforlife ,\nI use microbosh to deploy bosh. then, i use bosh to deploy CF.\nso, the bosh director is collocated with powerDNS, is there some\ndifference between bosh-155 and later version on the deployment file?\nhere is my deployment file:\n---name: boshdirector_uuid: 11a6c6ab-8419-4db9-9730-7e15a9f42ed0 # CHANGE\nrelease:\n  name: bosh\n  version: latest\ncompilation:\n  workers: 3\n  network: default\n  reuse_compilation_vms: true\n  cloud_properties:\n    instance_type: m1.medium # CHANGE#    availability_zone: szxts01-az\nupdate:\n  canaries: 1\n  canary_watch_time: 3000-120000\n  update_watch_time: 3000-120000\n  max_in_flight: 50\n  max_errors: 1#  serial: true\nnetworks:\n- name: floating\n  type: vip\n  cloud_properties: {}\n- name: default\n  type: manual\n  subnets:\n  - name: private\n    range: 10.10.10.0/24 # CHANGE\n    gateway: 10.10.10.1 # CHANGE\n    dns: [10.10.10.3] #CHANGE\n    reserved:\n    - 10.10.10.2 - 10.10.10.60 # CHANGE,100-110 larger than resource_pools[\"size\"]\n      static:\n    - 10.10.10.61 - 10.10.10.70 # CHANGE\n      cloud_properties:\n      net_id: 67a8ddc7-4d5f-432f-a154-660df2e8e69c # CHANGE\n      resource_pools:\n- name: common\n  network: default\n  size: 8\n  stemcell:\n    name: bosh-openstack-kvm-centos-7-go_agent-raw-m\n    version: latest\n  cloud_properties:\n    instance_type: m1.medium # CHANGE#      availability_zone: szxts01-az\n  jobs:\n- name: nats\n  template: nats\n  instances: 1\n  resource_pool: common\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.61 # CHANGE\n- name: redis\n  template: redis\n  instances: 1\n  resource_pool: common\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.62 # CHANGE\n- name: postgres\n  template: postgres\n  instances: 1\n  resource_pool: common\n  persistent_disk: 16384\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.63 # CHANGE\n- name: powerdns\n  template: powerdns\n  instances: 1\n  resource_pool: common\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.64 # CHANGE\n  - name: floating\n    static_ips:\n    - 9.91.39.27 # CHANGE\n- name: blobstore\n  template: blobstore\n  instances: 1\n  resource_pool: common\n  persistent_disk: 51200\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.65 # CHANGE\n- name: director\n  template: director\n  instances: 1\n  resource_pool: common\n  persistent_disk: 16384\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.66 # CHANGE\n  - name: floating\n    static_ips:\n    - 9.91.39.28 # CHANGE\n- name: registry\n  template: registry\n  instances: 1\n  resource_pool: common\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.67 # CHANGE\n- name: health_monitor\n  template: health_monitor\n  instances: 1\n  resource_pool: common\n  networks:\n  - name: default\n    default: [dns, gateway]\n    static_ips:\n    - 10.10.10.68 # CHANGE\n      properties:\n      nats:\n      address: 10.10.10.61 # CHANGE\n      user: nats\n      password: nats\nredis:\n    address: 10.10.10.62 # CHANGE\n    password: redis\npostgres: &bosh_db\n    host: 10.10.10.63 # CHANGE\n    user: postgres\n    password: postgres\n    database: bosh\ndns:\n    address: 10.10.10.64 # CHANGE\n    db: *bosh_db\n    recursor: 9.91.39.27 # CHANGE\nblobstore:\n    address: 10.10.10.65 # CHANGE\n    agent:\n      user: agent\n      password: agent\n    director:\n      user: director\n      password: director\ndirector:\n    name: bosh\n    address: 10.10.10.66 # CHANGE\n    db: *bosh_db\nregistry:\n    address: 10.10.10.67 # CHANGE\n    db: *bosh_db\n    http:\n      user: registry\n      password: registry\nhm:\n    http:\n      user: hm\n      password: hm\n    director_account:\n      user: admin\n      password: admin\n    resurrector_enabled: true\nntp:\n- 0.north-america.pool.ntp.org\n- 1.north-america.pool.ntp.org\nopenstack:\n    auth_url: http://9.91.17.18:5000/v2.0 # CHANGE\n    username: admin # CHANGE\n    api_key: Huawei # CHANGE\n    tenant: admin # CHANGE#    region: RegionOne # CHANGE\n    default_security_groups: [\"default\", \"default\"] # CHANGE\n    default_key_name: lv_microbosh # CHANGE#    ignore_server_availability_zone: true\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/817#issuecomment-98908827.\n. Sorry seems like it is already in cloudfoundry-incubator/bosh-vsphere-cpi-release@3d3932020395a71199bbb506696ce8c06561a9ab\n. I'm not sure what bosh_deployer does. Please refer to the official bosh\ndocs: http://bosh.io/docs to install BOSH.\n\nOn Wed, Jun 10, 2015 at 10:39 AM, Robert Christian <notifications@github.com\n\nwrote:\nI am new to working with Ruby Gems.\nI'm trying to install Cloud Foundary on AWS using Bosh.\nWhen I run gem install -V bosh_deployer I see that the gem indirectly\nrelies on aws-sdk versions 1.60.2 and ~> 1.3.6.\nERROR:  While executing gem ... (Gem::DependencyResolutionError)\n    conflicting dependencies aws-sdk (= 1.60.2) and aws-sdk (~> 1.3.6)\n  Activated aws-sdk-1.3.6\n  which does not match conflicting dependency (= 1.60.2)\nConflicting dependency chains:\n    bosh_deployer (>= 0), 1.4.1 activated, depends on\n    bosh_aws_registry (~> 0.2.2), 0.2.2 activated, depends on\n    aws-sdk (~> 1.3.6), 1.3.6 activated\nversus:\n    bosh_deployer (>= 0), 1.4.1 activated, depends on\n    bosh_cli (>= 1.0.2), 1.2981.0 activated, depends on\n    blobstore_client (~> 1.2981.0), 1.2981.0 activated, depends on\n    aws-sdk (= 1.60.2)\n\u279c  ~\nIs this something I can fix myself with a workaround, or is this something\nthat should be submitted as a bug?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/843.\n. Please open this issue against cf-release since this is an Elastic Runtime issue.\n\nSent from my iPhone\n\nOn Aug 5, 2015, at 5:27 AM, liuxiaoxi2237 notifications@github.com wrote:\nHi Expert,\ncf login success with --skip-ssl-validation. Create user, space, assigning user role works fine.\nBut when run \"cf org-users\", get below error.\nFailed fetching org-users for role ORG MANAGER.\nServer error, status code: 500, error code: 10001, message: An unknown error occurred.\nCheck controller logs, find that\n{\"timestamp\":1438705617.9227388,\"message\":\"Request failed: 500: {\\\"code\\\"=>10001, \\\"description\\\"=>\\\"Invalid SSL Cert for https://uaa.10.62.70.82.xip.io/oauth/token. Use '--skip-ssl-validation' to continue with an insecure target\\\", \\\"error_code\\\"=>\\\"CF-SSLException\\\", \\\"backtrace\\\"=>[\\\"/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/cache/cf-uaa-lib-b1e11235dc6c/lib/uaa/http.rb:169:\nI use self signed certificate.\nMy questions is:\n1, Is there away to skip cert validation when run \"cf org-users\" ? Or some other solution?\n2, in cf-deployment.yml, there are 3 components need to input cert(or key pair) - haproxy, router, uaa token. Should I use same key pair for the 3 components? or I need to generate key pair(cert ) for each components?\nany experience and suggestions?\nthanks,\nDavid\n\u2014\nReply to this email directly or view it on GitHub.\n. Yeah. If you use recent enough version of cpi  and stemcells, use_dhcp=true is the default for openstack.\n\nSent from my iPhone\n\nOn Aug 7, 2015, at 12:05 AM, Marco Voelz notifications@github.com wrote:\nYeah, sorry it is actually the other way around: The Openstack DHCP server configures the MTU to something like 1476 or even a bit less. That is necessary because tunneling over GRE tunnels adds an overhead of 24 bytes.\nAfter a restart, the machine defaults to an MTU of 1500 which results in non-functional overlay network.\nAbout those two changes you referred to:\nDoes that mean I can do manual networking, use static IPs, set use_dhcp: true and the DHCP server will assign the IP which I specified and additionally all other DHCP properties coming from Openstack, such as MTU and additional routes?\n\u2014\nReply to this email directly or view it on GitHub.\n. Right. We haven't published OpenStack cpi release with that fix. We ll get that going today.\n\nSent from my iPhone\n\nOn Aug 7, 2015, at 1:00 AM, Marco Voelz notifications@github.com wrote:\nAs far as I can tell there is no newer release than version 9 for the openstack-cpi so that functionality is not ready for consumption yet, right? I'll try to create my own final release and test if that fixes the problem\n\u2014\nReply to this email directly or view it on GitHub.\n. Turns out there are newer links that we should use:\nhttps://github.com/cloudfoundry/bosh/commit/a96a14f3703d13eb6ec4944f1ad1817d265b341c.\nUpdated!\n\nThanks for noticing.\nOn Fri, Sep 18, 2015 at 9:46 AM, Dmitriy Kalinin notifications@github.com\nwrote:\n\n@Jonty https://github.com/Jonty to understand better, why do you parse\nthe stemcells output of the CLI?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/947#issuecomment-141504589.\n. I've updated docs to state that 2+ version is required.\n\nOn Wed, Sep 30, 2015 at 11:08 AM, Maciej Strzelecki \nnotifications@github.com wrote:\n\nThere's similar problem with fog-google gem:\nERROR:  Error installing bosh_cli:\n    fog-google requires Ruby version >= 2.0.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/965#issuecomment-144493973.\n. +1\n. Sorry, reposted in https://github.com/cloudfoundry/bosh-init\n. \n",
    "skibum55": "Looks good.  Go for it.\n. ",
    "cromega": "Thanks for the quick fix! Although most likely this error was not the actual cause of the queued tasks that never got to run. I will provide more info or just create a new issue if we track it down. Cheers.\n. ",
    "teancom": "The default timeout for ntpdate is 1 second - it should not be a big consumer of resources.\n. ",
    "zaksoup": "in develop\n. Sorry about all the opening and closing. I had a bit of a brain-fart on how to use github properly. This should be auto-closed once it passes CI and is merged to master\n. Merged by a4547a53783954c90e6d018fd344a154c4bc95da\n. Sorry about all the opening and closing. I had a bit of a brain-fart on how to use github properly. This should be auto-closed once it passes CI and is merged to master\n. Pushed to https://registry.hub.docker.com/u/bosh/integration/\nSee if this helps.\n. Thanks @mikedillion!\n. Updated. Tests should pass. Added ability to create/upload release from local release folder. @cppforlife \n. idk what happened I think I hit enter and this is totally wrong I'm so sorry.\n. Hi @barthy1 \nThanks for this pull request. This functionality is very useful and will improve a lot of operators lives, I'm sure!\nWe've noticed a few inconsistencies with return types in some methods that had functionality added. We've commented where applicable for this issue. We're also concerned that changing all of the relevant unit tests to be strings actually hides the additional complication of multiple return types for the canaries and max_in_flight methods.\nWe'd love to pull this in once you apply these changes\nThanks!\nZak and @monkeyherder \n. Thanks for the updates!\n.  \n. lgtm\n. lgtm\n. \n. hey @AbelHu while we know you're an azure CPI member, our automated systems won't know it unless you make your membership of the relevant microsoft github orgs public.\nCan you also please add a test for this change in bosh-stemcell/spec/stemcells/azure_spec.rb. You can find similar tests of the agent.json in bosh-stemcell/spec/stemcells/centos_7_spec.rb.\n. hey @AbelHu while we know you're an azure CPI member, our automated systems won't know it unless you make your membership of the relevant microsoft github orgs public.\nCan you also please add a test for this change in bosh-stemcell/spec/stemcells/azure_spec.rb. You can find similar tests of the agent.json in bosh-stemcell/spec/stemcells/centos_7_spec.rb.\n. Hi @beyhan, Thanks for this PR!\nWe've definitely recognized the need for this functionality and are really glad to see you contribute.\nAfter reviewing the code we would like to suggest that you refactor this pull request to avoid such heavy use of hashes. Instead we'd like you to fully implement model objects for Deployment, Agent, and Instance. Right now the relationships between these objects are unclear and difficult to discover. Hashes like agent_id_to_agent obfuscate the fact that the Agent as a \"concept\" has one ID and is generally referenced by the director via that ID. In other places, hashes like deployment_name_to_agent_ids obfuscate the relationship between Deployments and Instances. Conceptually a Deployment has many Instances, each which have one Agent. Every one of these concepts is backed by an ID (usually a UUID) in the Director/HM databases, but that's not always how we'd like to see them manipulated in code.\nInstead, if we have a full ruby object for Deployment, which had many Instances as data members, we can reason about the code and logic with more intuition as people who collectively understand the general concepts of bosh.\nOverall, this PR solves the problem well, and we're excited to pull it in. Many places in the BOSH codebase are riddled with what we jokingly refer to as \"hash oriented programming\". Moving forward, we, as a team, would like to focus on avoiding this pattern when writing new code and refactor old code when we come across it.\nThanks again,\nZak and @monkeyherder!\n. Hi @beyhan, Thanks for this PR!\nWe've definitely recognized the need for this functionality and are really glad to see you contribute.\nAfter reviewing the code we would like to suggest that you refactor this pull request to avoid such heavy use of hashes. Instead we'd like you to fully implement model objects for Deployment, Agent, and Instance. Right now the relationships between these objects are unclear and difficult to discover. Hashes like agent_id_to_agent obfuscate the fact that the Agent as a \"concept\" has one ID and is generally referenced by the director via that ID. In other places, hashes like deployment_name_to_agent_ids obfuscate the relationship between Deployments and Instances. Conceptually a Deployment has many Instances, each which have one Agent. Every one of these concepts is backed by an ID (usually a UUID) in the Director/HM databases, but that's not always how we'd like to see them manipulated in code.\nInstead, if we have a full ruby object for Deployment, which had many Instances as data members, we can reason about the code and logic with more intuition as people who collectively understand the general concepts of bosh.\nOverall, this PR solves the problem well, and we're excited to pull it in. Many places in the BOSH codebase are riddled with what we jokingly refer to as \"hash oriented programming\". Moving forward, we, as a team, would like to focus on avoiding this pattern when writing new code and refactor old code when we come across it.\nThanks again,\nZak and @monkeyherder!\n. @beyhan thanks! We definitely appreciate trying to keep PRs in readable chunks. We're looking forward to seeing your next few commits \ud83d\ude04 \n. Done\n. This method has two possible return types, string or int, depending on the context. This seems like a big recipe for possible future confusion. We noticed lower down that there are tests that explicitly call out this strange behavior, but we're unsure why it's necessary in the first place. Can we make sure that all return values from UpdateConfig.canaries(size) are a single type?\n. Why is this not an expect rather than allow? Looks like we are actually confirming that the fix param is passed into the DeploymentManager object.\n. Is this the right let for this context? job_state returning a value of 'running' doesn't seem like it implies that the instance is nil.\n. It looks like this method is public. Are there any existing unit tests for it? This functionality change looks like it would be good to have a unit test. This method also has two possible return types, string in the \"%\" case and int in the digit case. Can we refactor this to only have one return type and make sure that consumers of this method always properly handle strings as output?\n. ",
    "st3v": "We merged in commit 5198e72 but fixed healthz in a different way. Thanks @Kaixiang!\n. Correct me if I'm wrong but it looks like you are trying to block the /healthz request in case the EM thread pool has been exhausted. The problem is that EM.schedule executes on the reactor thread, which should not be blocked in that situation. So, Monit will still get a 200 right away even though the requests to Data Dog are queueing up.\nAssuming that was the intent of this change, it seems what you could do here is using EM.defer with some sort of async response handling [see example]. That way the response would be queued up with all the Data Dog requests and the request from Monit would timeout.\ncc @tylerschultz \n. @luan: Just wondering, where are the up! and down! methods defined?\n. @luan: I think the (@count == @max) condition here causes a deadlock under certain circumstances. Why would we need it at all? Why not always signal? \n. @luan: NP. Is the @dwait also a left-over from those times? I don't quite see why we would need it.\n. @luan: What do you think about something like this ...\n```\n   def wait\n      @mon.synchronize do\n        @uwait.wait_while do\n          @max > 0 and @count == @max\n        end\n        @count += 1\n      end\n    end\ndef signal\n  @mon.synchronize do\n    @count -= 1\n    @uwait.signal\n    count\n  end\nend\n\n```\nSeems cleaner to me.\n. Yep. It's a cool feature!\n. No worries. Enjoy your vacation (what's left of it).\n. ",
    "krumts": "I'm working at SAP and as far as know we have a corporate agreement.\nSome background about the proposed change - we are using our own iaas layer over aws compatible APIs. However the default CA certificate bundle of the aws ruby sdk doesn't trust our end-points. Therefore we need some more flexibility to configure them.\n. Thanks for accepting the change!\n. About the CLA: I am working at SAP and as far as I know there is a corporate agreement in place.\n. Thanks for accepting it!\n. ",
    "lexsys27": "no, the deployments are fine. it's just annoying message\nok, waiting for bosh-micro cli release\n. Now I use bosh-init to create bosh env and it works fine. See: https://github.com/cloudfoundry/bosh-init\n. I can't reproduce problem so let's assume it is gone.\nSurprisingly bosh deploy started to rebuild vms after extracting config. I expected to just bind new config\n. @dpb587-pivotal you are almost right. I added two azs to the cloud-config and allocate two instances of app to these zones.\n. Here is the app-related manifest part:\ninstance_groups:\n- name: app\n  instances: 2\n  azs: [z1, z2]\n  jobs:\n  - name: app\n    release: sinatra\n    properties:\n      port: 8002\n  vm_type: small\n  stemcell: warden\n  networks:\n  - name: main\n. Here is the app-related manifest part:\ninstance_groups:\n- name: app\n  instances: 2\n  azs: [z1, z2]\n  jobs:\n  - name: app\n    release: sinatra\n    properties:\n      port: 8002\n  vm_type: small\n  stemcell: warden\n  networks:\n  - name: main\n. @cppforlife yes\n. @cppforlife yes\n. @cppforlife Didn't know about this option, will try it soon. Thank you for the link!\n. ",
    "xacaxulu": "+1\n. ",
    "DanielJonesEB": "+1\nJust occurred on my MacBook, OSX 10.10.5\nruby 2.1.3p242 (2014-09-19 revision 47630) [x86_64-darwin14.0]\n```\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    bosh-aws-xen-hvm-ubuntu-trusty-go_agent\nVersion: 3063\nStarted deploy micro bosh\n  Started deploy micro bosh > Unpacking stemcell. Done (00:00:00)\n  Started deploy micro bosh > Uploading stemcell. Done (00:00:25)\n  Started deploy micro bosh > Creating VM from ami-e1577396 light. Done (00:00:34)\n  Started deploy micro bosh > Waiting for the agent. Done (00:02:34)\n  Started deploy micro bosh > Updating persistent disk\n  Started deploy micro bosh > Create disk. Done (00:00:07)\n  Started deploy micro bosh > Mount disk. Done (00:00:08)\n     Done deploy micro bosh > Updating persistent disk\n  Started deploy micro bosh > Stopping agent services. Done (00:00:02)\n  Started deploy micro bosh > Applying micro BOSH spec. Done (00:00:39)\n  Started deploy micro bosh > Starting agent services. Done (00:00:01)\n  Started deploy micro bosh > Waiting for the director. Done (00:00:08)log writing failed. can't be called from trap context\nDeployed deployment.yml' tohttps://52.19.87.7:25555', took 00:04:45 to complete\n/Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/ruby_compat.rb:25:in select': Bad file descriptor (Errno::EBADF)\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/ruby_compat.rb:25:inio_select'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:210:in process'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:inblock in loop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:in loop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:inloop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/bosh_cli_plugin_micro-1.3063.0/lib/bosh/deployer/remote_tunnel.rb:43:in `block in monitor_session'\n```\n. +1\nJust occurred on my MacBook, OSX 10.10.5\nruby 2.1.3p242 (2014-09-19 revision 47630) [x86_64-darwin14.0]\n```\nVerifying stemcell...\nFile exists and readable                                     OK\nVerifying tarball...\nRead tarball                                                 OK\nManifest exists                                              OK\nStemcell image file                                          OK\nStemcell properties                                          OK\nStemcell info\nName:    bosh-aws-xen-hvm-ubuntu-trusty-go_agent\nVersion: 3063\nStarted deploy micro bosh\n  Started deploy micro bosh > Unpacking stemcell. Done (00:00:00)\n  Started deploy micro bosh > Uploading stemcell. Done (00:00:25)\n  Started deploy micro bosh > Creating VM from ami-e1577396 light. Done (00:00:34)\n  Started deploy micro bosh > Waiting for the agent. Done (00:02:34)\n  Started deploy micro bosh > Updating persistent disk\n  Started deploy micro bosh > Create disk. Done (00:00:07)\n  Started deploy micro bosh > Mount disk. Done (00:00:08)\n     Done deploy micro bosh > Updating persistent disk\n  Started deploy micro bosh > Stopping agent services. Done (00:00:02)\n  Started deploy micro bosh > Applying micro BOSH spec. Done (00:00:39)\n  Started deploy micro bosh > Starting agent services. Done (00:00:01)\n  Started deploy micro bosh > Waiting for the director. Done (00:00:08)log writing failed. can't be called from trap context\nDeployed deployment.yml' tohttps://52.19.87.7:25555', took 00:04:45 to complete\n/Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/ruby_compat.rb:25:in select': Bad file descriptor (Errno::EBADF)\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/ruby_compat.rb:25:inio_select'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:210:in process'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:inblock in loop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:in loop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/net-ssh-2.10.0.beta1/lib/net/ssh/connection/session.rb:170:inloop'\n    from /Users/deejay/.gem/ruby/2.1.3/gems/bosh_cli_plugin_micro-1.3063.0/lib/bosh/deployer/remote_tunnel.rb:43:in block in monitor_session'\n```\n. In case people turn up here courtesy of a search engine: as of the time of writing and BOSH CLI versionversion 5.4.0-891ff634-2018-11-14T00:21:14Z, deletingblobs.yml` will result in an error, and emptying the file will also make BOSH believe you have no blobs.\nThe remedy is to\n remove all object_id fields in blobs.yml\n bosh upload-blobs\nThis is what I had to do when moving from one blobstore to another.. In case people turn up here courtesy of a search engine: as of the time of writing and BOSH CLI version version 5.4.0-891ff634-2018-11-14T00:21:14Z, deleting blobs.yml will result in an error, and emptying the file will also make BOSH believe you have no blobs.\nThe remedy is to\n remove all object_id fields in blobs.yml\n bosh upload-blobs\nThis is what I had to do when moving from one blobstore to another.. ",
    "bkcisco": "actually i did not setup openstack, there is no floating ip available, and 172.28.132.239 is one of ext-net.\ni tried some more scenario, and turned out, any job has static ip will not bring up after VM created.\nand log say nothing special. static ip in manifest should be a floating ip ?\n. actually i did not setup openstack, there is no floating ip available, and 172.28.132.239 is one of ext-net.\ni tried some more scenario, and turned out, any job has static ip will not bring up after VM created.\nand log say nothing special. static ip in manifest should be a floating ip ?\n. ",
    "mikedillion": "@cppforlife Is there any more doc I could add to this? Or any other due diligence for that matter.\n. @cppforlife Is there any more doc I could add to this? Or any other due diligence for that matter.\n. Fog will handle the nil nicely here.\n. ",
    "boebu": "We're using Piston 3.0x with ceph as the cinder backend\n. We're using Piston 3.0x with ceph as the cinder backend\n. sorry for the delay.\nConfirmed - disk mounting works now as expected with openstack & config drive.\n. @petarz is there a specific reason why only heartbeat events could be sent to graphite?\nhttps://github.com/cloudfoundry/bosh/pull/714/files#diff-4bd1307eb75d6edd592d6b45474b6103R140\n. @petarz is there a specific reason why only heartbeat events could be sent to graphite?\nhttps://github.com/cloudfoundry/bosh/pull/714/files#diff-4bd1307eb75d6edd592d6b45474b6103R140\n. ",
    "xian": "@dkoper, could you check if 216a1c32f9 works for you? We switched the env vars to HTTP_PROXY etc.\n. @dkoper, could you check if 216a1c32f9 works for you? We switched the env vars to HTTP_PROXY etc.\n. Gotcha, didn't realize the curl issue. Merging your original changes. Thanks @dkoper!\n. Gotcha, didn't realize the curl issue. Merging your original changes. Thanks @dkoper!\n. Thanks @drnic!\n. Thanks @drnic!\n. ",
    "petarz": "@cppforlife I am contributing this on behalf of my Org SAP, \nI made this publicly visible in my profile now (it wasn't before) \n. @cppforlife I am contributing this on behalf of my Org SAP, \nI made this publicly visible in my profile now (it wasn't before) \n. Thanks for merging this PR @zhang-hua & @mariash\n@boebu well Graphite is a tool build for graphing and trending around metrics and I do not see how alerts fit with it\n. Thanks for merging this PR @zhang-hua & @mariash\n@boebu well Graphite is a tool build for graphing and trending around metrics and I do not see how alerts fit with it\n. @cppforlife the change was tested with graphite on our side and worked as expected\n. @cppforlife the change was tested with graphite on our side and worked as expected\n. ",
    "owaism": "AWS has stopped support for MySql 5.5. \nI had to change the following code to get it working:\n<gems_location>/gems/ruby-1.9.3-p551/gems/bosh_cli_plugin_aws-1.2824.0/lib/bosh_cli_plugin_aws/rds.rb to point to 5.6.21. Also had to change code in the same file to point to 5.6 (previously 5.5) Database Option Set.\n@cppforlife what do you mean by bosh aws create is only meant for quick dev setup?  Followed the documentation provided here http://docs.cloudfoundry.org/deploying/ec2/ to deploy CloudFoundry to AWS. It does not tell that Bootstrap on AWS VPC is for dev setup only. \nCan you please elaborate? Also if this is for dev setup only, can you please point me to instructions which will help me deploy a production quality Cloud Foundry.\n. ",
    "asanramon": "Any resolution on this. I am getting this error now too. I just changed a couple of passwords on my deployment manifest. When I did bosh deploy, I get the errors above.\n. Hi drnic/cppforlife,\nFor some reason I am able to deploy bosh now from microbosh. I didn't change anything on my manifest. The only reason I can think of that cause the issue was that someone else was deploying something when I deploy bosh.\nOff topic, is it possible to enable resurrector on microbosh? All our deployments are deployed through microbosh, and we would like to automatically rebuild instances when it goes down.\nThanks for all the reply.\n. ",
    "bahatelos": "Hello,\nIndeed the problem was the bosh_cloud_cpi version, which was 0.7.1.\nThat is the default cpi for bosh 2788.\nI can confirm that Bosh 2811 comes with bosh_cloud_cpi version 0.7.2, which works as expected.\nThe issue can be closed/cancelled.\nThanks for your help!\n. ",
    "mfine30": "We're not actively planning to address this issue as it hasn't come up as a major priority. If it continues to be a problem and you'd like to continue the conversation feel free to reopen this issue.. Closing due to inactivity. If this is still a problem in more recent stemcells, please open an issue in https://github.com/cloudfoundry/bosh-linux-stemcell-builder.. @bandesz if this is an issue you are still having, could you please open a new issue describing it? \nClosing this issue for inactivity.. We have since introduced a --fix flag which should only be used in rare cases. Since bosh is now significantly different than when this issue was opened, closing for inactivity. If the issue persists, feel free to open a new issue with updated versions.. Closing for inactivity. If you'd like to continue the conversation, feel free to reopen this thread.. Thanks all for the feedback. I've created story: https://www.pivotaltracker.com/story/show/159605314 to look further into this. Not sure on timelines yet, but the large number of +1's is certainly encouraging.. Closing for inactivity. If this is still an issue on more recent versions, please feel free to continue the conversation.. This should be addressed in the new golang cli. Closing. If it comes up again, please open a new issue.. The bosh team has done significant work on the errands implementation since this issue was opened. I don't believe it should still be an issue. If it is, please reopen this github issue.. @voelzmo do you know if this is still an issue? Can we close this?. While this is a valid use case, it is outside of the scope of work that we are currently interested in addressing on the bosh team. To help gauge interest though, please feel free to comment and leave specific use cases.. While this is a valid use case, it is outside of the scope of work that we are currently interested in addressing on the bosh team. To help gauge interest though, please feel free to comment and leave specific use cases.. Closing due to inactivity.. Closing due to inactivity.. @voelzmo wondering the status on this issue. It looks like it hasn't gotten much attention in recent years. Do you have any updates or thoughts on next steps here?. @voelzmo wondering the status on this issue. It looks like it hasn't gotten much attention in recent years. Do you have any updates or thoughts on next steps here?. Closing in favor of #1163 since it has more context. Closing in favor of #1163 since it has more context. #131320561. #131320561. @freddesbiens @pivotal-jamil-shamy this seems to touch on some of the work you all have been doing. Perhaps you can provide an update here?. @freddesbiens @pivotal-jamil-shamy this seems to touch on some of the work you all have been doing. Perhaps you can provide an update here?. @voelzmo copying my question from #1128 over here:\n\nwondering the status on this issue. It looks like it hasn't gotten much attention in recent years. Do you have any updates or thoughts on next steps here?. @voelzmo copying my question from #1128 over here:\nwondering the status on this issue. It looks like it hasn't gotten much attention in recent years. Do you have any updates or thoughts on next steps here?. Closing as this isn't something that we are intending to address. We'd rely on clients doing a better job of managing the deployment lifecycle and potential error cases. . Closing as this isn't something that we are intending to address. We'd rely on clients doing a better job of managing the deployment lifecycle and potential error cases. . Seems like there haven't been any follow up questions on this for a while. Additionally, there is now http://bosh.io/docs/dns. If you still have questions, feel free to reopen or open a new issue.. Seems like there haven't been any follow up questions on this for a while. Additionally, there is now http://bosh.io/docs/dns. If you still have questions, feel free to reopen or open a new issue.. Closing this for inactivity. If this is still an issue, feel free to open a new issue with more current details.. Closing this for inactivity. If this is still an issue, feel free to open a new issue with more current details.. Fixed in new golang cli. Closing. Fixed in new golang cli. Closing. I believe this has been fixed in more recent versions. However, if this is still an issue, could you please open an issue in https://github.com/cloudfoundry/bosh-cli.. I believe this has been fixed in more recent versions. However, if this is still an issue, could you please open an issue in https://github.com/cloudfoundry/bosh-cli.. Given some roadmap items and the overall shift to moving towards compiled releases, this is not a big priority for us. I'm going to close it, but if you'd like to continue the conversation, feel free to reopen.. Given some roadmap items and the overall shift to moving towards compiled releases, this is not a big priority for us. I'm going to close it, but if you'd like to continue the conversation, feel free to reopen.. Given updates to the director and more modern directors, this is less likely to be an issue. I'm going to close this, but if it's still a problem, feel free to reopen.. Given updates to the director and more modern directors, this is less likely to be an issue. I'm going to close this, but if it's still a problem, feel free to reopen.. Added Pivotal Tracker #159805408 to track this. Added Pivotal Tracker #159805408 to track this. Closing for inactivity, and the overall movement towards bosh-dns.. Closing for inactivity, and the overall movement towards bosh-dns.. Closing due to inactivity. Closing due to inactivity. @voelzmo I believe this was before vars so the CLI can dynamically generate its own certs. I'm going to close this issue.. @voelzmo I believe this was before vars so the CLI can dynamically generate its own certs. I'm going to close this issue.. Looks like the PR for this issue was merged - I'm going to close the corresponding issue. Looks like the PR for this issue was merged - I'm going to close the corresponding issue. Closing in favor of #1946 . Closing in favor of #1946 . One potential workaround is to empty the config/blobs.yml and then bosh upload-blobs. We're also now using the golang cli rather than ruby which changes some symlinking behavior. If this is still a concern, could you open a new issue in github.com/cloudfoundry/bosh-cli. One potential workaround is to empty the config/blobs.yml and then bosh upload-blobs. We're also now using the golang cli rather than ruby which changes some symlinking behavior. If this is still a concern, could you open a new issue in github.com/cloudfoundry/bosh-cli. We've rewritten some of the code in this part of Director so this behavior may no longer be an issue. If it is, feel free to reopen. We've rewritten some of the code in this part of Director so this behavior may no longer be an issue. If it is, feel free to reopen. @voelzmo is this still a concern?. @voelzmo is this still a concern?. Hey @kitsirota it doesn't look like this issue has gotten much traction. This isn't a feature which is on our near/mid-term roadmap right now, so I'm going to go ahead and close this. If you or others have more thoughts on how this might work in the future, please let us know.. This isn't a feature which is on our near/mid-term roadmap right now, so I'm going to go ahead and close this.\n\nAt scale, our recommendation would be to avoid collocation where possible, and focus on service specific, redundant instance groups.. Hey @Kiemes @voelzmo is this still an issue you are seeing?. We're looking to remove the global package cache in the mid-future in favor of a push towards more compiled releases. As a result, I don't see us prioritizing this work, so I'm going to close this issue.. @lnguyen could you provide more context on your use case for having it on by default? Could you share which releases you're relying on for this functionality?. Hey @keymon is this still of interest to you? If so, are you interested in chatting about submitting a PR for it? . Hey @Lafunamor @MatthiasWinzeler is balancing across AZs in this case this still an issue for you? Is this something you'd be interested in chatting about submitting a PR?\n. Closing in favor of #1806 - looks like we've done some refactoring and that issue has an updated stacktrace.. This is along the lines of manifest strictness/validation (related note). This is kind of a more general case of https://github.com/cloudfoundry/bosh/issues/1686, but since this isn't on our short-term roadmap, I'm going to close this. Let's consolidate further discussion around validation in that other issue.. Quite a bit has changed since this was originally raised. I'm going to close for now, but let us know if this is still an ongoing issue that you would like more help with.. This error happens when the VM is under very high load. The recommended solution would be to scale to a larger VM if you're consistently seeing this issue. Unfortunately, there isn't much we can do besides cleaning up the error message which is pretty low on our priority list right now.. Hey @geofffranks, sounds like this issue has been resolved in more recent versions of the director, and there hasn't been much activity in the last year. If you're still concerned, feel free to re-open and let us know.. As Tom pointed out, this file contains information, much of which we need to keep up to date for authentication. I don't think this is something that we are planning to add to the cli. I'd suggest looking to modify how the version control interplays with the CLI.. @mariash is this still an issue? The error sounds similar to other errors we've seen for UTF-8 encoding issues, and I believe there was work done recently to better support that.. This isn't something that we are likely to add to BOSH. While tedious, links and a strong contract between various jobs/releases is still our recommended to ensure release authors are explicit in the behavior that they intend to support.. BATS now runs with the golang CLI which has better support for UAA auth through the BOSH_CLIENT and BOSH_CLIENT_SECRET environment variables. I'd recommend giving that a try and see if you are still having issues. Feel free to start a new issue if new errors are happening with it.. @kieron-pivotal I'm guessing that Alex opened this on behalf of ODB last year. Do you know if this is an issue that you all are interested in? . Great. I'm going to close this issue - thanks!. We recently updated the docs here: https://bosh.io/docs/director-users-uaa-perms/#full-read. Given the limited activity, I'm going to close this due to age.. The bosh team has done significant work on the errands implementation since this issue was opened. I don't believe it should still be an issue. If it is, please reopen this github issue.. Reading the issue, it looks like you found a way around. Due to the limited activity in the last year, I'm going to close this. If it is still an issue on more recent stemcell lines, please open a new updated issue.. I believe this has been fixed in more recent versions. This does seem like it would be valuable, but isn't on our immediate roadmap and doesn't seem to have gotten much traction in the last year or so. That being said,  we are looking to expose more health metrics from the director so could look to expose similar information when we get to that work in a few months.. Closing in favor of #1568 . Closing and re-opening so we have a tracker story.. The most recent versions of bosh have been patched to address this issue. For example, v266.10.0 should resolve it. If it continues to be an issue, please let us know.. Hey @Kiemes do you know what version you saw this on? Is it still a problem?. Hey @zhangtbj I agree this is an issue. While it is something we are interested in addressing, I don't think it is on our short term roadmap so I'm going to close this for now. If you'd like to continue the discussion, feel free to re-open it though.. Given the overall product shift towards bosh-dns, I'm going to close this issue. If you'd like to continue the discussion, feel free to re-open it. . Closing and re-opening to trigger a gitbot story. Hey @glyn, adding first class support to bosh run-errand <ERRAND> so that it immediately streams logs is not something that we are likely to get to in the short term. \nThat being said, there is an alternative flow you could try:\nIn Shell 1\n$ bosh run-errand <ERRAND>\nIn Shell 2\n$ bosh logs -f <ERRAND-JOB>/0. cc @freddesbiens @pivotal-jamil-shamy since this seems to fall well into the links realm, tagging you. . Hey @beyhan @voelzmo is this an issue you're still interested in discussing? . Closing and re-opening to trigger gitbot story. Seems like this was fixed in 266+ which was released several months ago. Closing. Seems related to: https://www.pivotaltracker.com/story/show/159469692, which the Toronto BOSH team is actively investigating. cc @freddesbiens @pivotal-jamil-shamy . @freddesbiens @pivotal-jamil-shamy this seems again very related to the work that you all are currently focused on around links and certs. Marking as triaged, but feel free to respond.. Closing and re-opening for gitbot story creation. @voelzmo @beyhan it seems like this bug is directly related because of the registry? Since we're working on removing it, do you think this bug will still exist? \nI'm going to close it, but if you believe it will, please re-open.. Marking as triaged since it looks like there's a story that @voelzmo made.. Hey @menicosia, we've recently done some work around how we handle the bootstrapping behavior for ignoring instances. I suspect that this will impact the way this bug appears if at all. If you come across it again, could you please open a new issue with updated logs?\nGoing to close this for now though since I think it should be improved by our recent changes.. This makes sense to me. Closing/re-opening to trigger a story. We ended up actually doing the work for this in a separate story. The solution we took is to effectively shut down workers in reverse-numerical order and consider worker 0 a prioirty worker. I gave it a go and it works as expected. \nOne interesting thing I did notice is that bosh logs is not a priority task but bosh ssh is. I'm hoping to figure out what's what either offline or in a follow up. @keithkroeger bbr has two options for how you execute a backup/restore - for a bootstrapped VM like director, or for VMs which are part of a BOSH deployment. It sounds like you may be backing up a boostrapped VM like director, and in this case you need to be sure to provision a sudo, passwordless user for backup purposes. The user_add job might be useful to you.. Closing due to inactivity. Closing due to inactivity. If you're still interested, could you please reopen with more information. Closing/re-opening to make a tracker story. @voelzmo is this something you all are interested in owning? . Closing due to inactivity. If this continues to be a problem, please re-open with updated logs.. This seems valuable, but in reality is something that we're not likely to get to in the near future. If you're interested in chatting more, please feel free to re-open.. This is definitely something we do want to get to, but the solution is unfortunately not trivial. I'll keep this issue open to track concerns and discussion, but still probably a way off before we can dedicate time to really digging into it.. Closing and re-opening to trigger a tracker story\n. Closing in favor of the linked bosh-note above. Closing and re-opening to make a tracker story. Seems related to: #2077 . @xoebus is this still a thing? . This might be an upcoming track of work in the next few months. Could you provide more information about use cases so we can help include those when making decisions? . This does seem like something that would be valuable. Unfortunately it also has quite a lot of nuances as I start to think about it more. As a result, we're unlikely to get to it in the near future so I'm going to close this issue. If you are interested in continuing the conversation, feel free to re-open.. Makes sense, seems pretty straightforward. Closing/re-opening to trigger a tracker story.. Hey @antonsoroko in regards to the wildcard solution, we haven't heard a ton of need for it more broadly. Before we make such a impactful feature, I'd want to hear more from folks about a need for it. As a result, I'm going to close this for now.. @gdenn thanks for the verbose problem statement. We're actively looking into ways of addressing this and improving it. Hopefully you'll start seeing some improvements in this area in future releases of the BOSH director. I'm going to close it for now since it's an active stream for the team, but feel free to re-open if you have additional thoughts.. Closing due to inactivity. If you want to continue the conversation, feel free to re-open. Looking at those thumbs up emoji's looks like Danny's answer was sufficient to solve your problem. I'm going to go ahead and close this issue, but feel free to re-open it if it's still a problem. This does seem like a nice one to add, but would be pretty low priority. I'll close/re-open the issue to trigger a story, but we're probably not going to get to it super soon. \nIf the community is interested in this functionality, someone could propose an interface/solution and potentially it in.. Hi again,\nLooks like this issue has been open for a little while now. I'm going to go ahead and close it. If it's still an issue, feel free to re-open.. As @ndhanushkodi said: \"THE BOSH IGNORE BUG FOR INSTANCE 0 HAS BEEN FIXED IN DIRECTOR VERSION 266.12, 267.07, and 268!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\"\nClosing. @voelzmo Danny and I chatted with the CAPI team. It sounds like they write their CEF formatted logs as a separate file in /var/vcap/sys/log/JOB/* and they haven't had any known complaints or issues. \nDo you think we can do the same and it would meet our product requirements?. @pivotal-jamil-shamy is there a story in the Toronto backlog tracking this? Is this something that would make more sense for SF or Toronto to address? cc @dpb587 @freddesbiens . Thanks @ashwin-venkatesh \n@jfmyers9 @dpb587 does this seem related to #1926?. @pivotal-jamil-shamy definitely a fan of trying to expose this to operators in some fashion. I'm not sure I agree with bosh env though and if there's a better spot.\nAdditionally, how does credhub play into this? And are we scoping this specific issue just to the certs that the BOSH \"system\" is using, rather than also including the various certs in the deployments?. This was implemented in August, per upstream comments that Danny provided. Closing. Closing per comment above. . Closing for inactivity. This behavior should be resolved in v268.0.1. I'm going to close this issue, but if it's still an issue with the newer version, please open a new/updated issue.. Thank you for opening the issue @amuessig. We are aware of this; it has to do with a hiccup in our CI as we were cutting it. If you use bosh v267.3.0 the issue should be resolved.. Should be fixed in v267.5.0. @vito are you seeing this consistently or just as a one off?. Hey @gberche-orange thanks for bringing this up. Definitely agree on the pain point here. There are a few potential ways we're considering addressing this and making it easier to clean up credentials after deleting a deployment. \nHowever, it's in reality more of a mid to long term roadmap item, since as you mentioned there is an alternative, albeit a bit of a tedious one.. Thanks! This should be updated in v268.1.0. This should be fixed in v267.7.0 of the Director. . @voelzmo Yes, it should be fixed in: https://github.com/cloudfoundry/bosh/commit/075222d25. I was planning to cut a new minor next week, but if you think this is more urgent we can release a patch more quickly. I don't know how similar this is, but I @freddesbiens and I have spent a bit of time talking about better monitoring for the director in general as something to focus on in the mid-term. It seems like this could fit well into that sort of effort?. @degaurab did you want to ping specific folks on the team to get this merged? It seems pretty internally focused and not really product surface area, so I trust that you can get the right people on board and work with them to prioritize this. . Hey @keymon I'm somewhat hesitant to add support for more custom scopes in the local bosh identity provider. Anything auth related increases surface area for security risks and the local users are pretty lightweight, to the point that I think it is obvious that they are not intended for serious use cases. \nI'm also trying to understand your rationale behind this request, along with the Prometheus PR you just closed. Not having admin (write) users for prometheus makes sense, but if you're in an environment where something like that is a concern, I would say using UAA over the changes you're suggesting makes more sense. . Hey @ishustava thanks for opening this! How frequently does this fail and how much of a problem is this causing for your CI? Trying to gauge the urgency we should address this based on how it is impacting your team. You should be able to change the number of instances via the instances key for the appropriate instance group. You can see an example manifest here: https://bosh.io/docs/deployment-basics/. Thanks for the context and conversations. Let's work and prioritize this in IPM today. @rosenhouse thanks for the poke. We IPM'd on it on Monday and I apparently forgot to prioritize it. It should be prioritized now though! . Thanks Aaron (and hey BK). I believe the BOSH Toronto team is actively looking into this or something very similar. I'll connect you all over email in case Fr\u00e9d\u00e9ric (Toronto PM) has any questions.\nping @freddesbiens @xtreme-andrew-su this seems very related to one of your active stories\nhttps://www.pivotaltracker.com/story/show/161762467. I believe that we have addressed this issue as it seems very similar to the work we did in the story: #163150850. We have since released a director with these fixes. If you are on a more recent version of director and continue to see this as an issue, please open a new issue and let us know!\nThanks!. @miguelverissimo @dpb587 my interpretation of reading this thread is there is some potential UX improvements we could make, but they may be challenging based on current architecture/code behavior. I'd be inclined to say we should merge this and we can improve the output if/when we get feedback that we should add more.. Just tested this out and it worked successfully using the latest of master of the AWS CPI release repo. We'll need to cut a new AWS CPI to make this more easily usable/available, but I'm going to close this and cut a new release shortly. ",
    "lnguyen": "so there is no way to preserve this if it's not in bin/?\n. so there is no way to preserve this if it's not in bin/?\n. Actually this is good for me to know. Thanks @karlkfi \n. Actually this is good for me to know. Thanks @karlkfi \n. you can ssh into jumpbox and return provision.sh\nmake provision should be able to run it again from terraform folder\n. you can ssh into jumpbox and return provision.sh\nmake provision should be able to run it again from terraform folder\n. Or maybe add bosh stemcell for hvm\n.   thanks Dmitriy!\n\nOn Feb 25, 2015, at 1:47 PM, Dmitriy Kalinin notifications@github.com wrote:\nI've prioritized a story this week to start doing so:\u00a0https://www.pivotaltracker.com/story/show/89044786\n\u2014\nReply to this email directly or\u00a0view it on GitHub\n.\n. Any status branch wip-cm-89044786-light-stemcells-for-everyone ? \n. Thank you!\n. It happen consistently, it's interesting that when I revert back to older version it just works and I know remote isn't corrupted because bosh.io\u00a0can even cut releases. Will try again today and see what happens.\u00a0\n\nOn January 6, 2016 at 7:49:27 PM, Dmitriy Kalinin (notifications@github.com) wrote:\nDoes this happen consistently? This error is shown when downloaded blob did not match expected sha1. That may happen when somehow either remote blob is corrupted or download procedure was somehow corrupted and transferred bits are not correct.\n\u2014\nReply to this email directly or view it on GitHub.\n. yeah it would be nice just to make sure what we're changing is what we want. \n. Also what is the criteria for a CVE being added to stemcell? (for my own education) \n. Ok thanks!\u00a0\nOn March 3, 2016 at 12:43:24 PM, Dmitriy Kalinin (notifications@github.com) wrote:\nWe follow Ubuntu's severity recommendations. If there is a USN with high severity or above we immediately issue a hotfix release, otherwsie bumps to OS packages happen in subsequent builds.\n\u2014\nReply to this email directly or view it on GitHub.\n. ahh it look like side effect of string-int conversions, is it possible to do this at a director level to convert to string before comparing? \n. Mind if we turn it on by default now?\nOn Fri, Oct 21, 2016 at 1:51 PM Dmitriy Kalinin\n<\nmailto:Dmitriy Kalinin notifications@github.com\n\nwrote:\n\na, pre, code, a:link, body { word-wrap: break-word !important; }\nhttps://github.com/lnguyen\nmost of the larger features are not enabled by default as they get rolled out. this fell into that category.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly,\nhttps://github.com/cloudfoundry/bosh/issues/1486#issuecomment-255433233\n, or\nhttps://github.com/notifications/unsubscribe-auth/AA4rDIrraWZ8paTnG9RDRTwNCwWpb4M5ks5q2PubgaJpZM4KcxNz\n.\n. ",
    "uzzz": "@mkocher do you mean AGPL license? If so then no, it's GPLv3, as well as 5.2.4 was.\n. @mkocher do you mean AGPL license? If so then no, it's GPLv3, as well as 5.2.4 was.\n. Looks like this is mistake in your deployment manifest. Take a look at your \"networks\" section, network with type \"manual\" (or without type \u2013 network without type is a manual by default) \u2013 there's a missing required \"range\" argument in one of subnets. See http://bosh.io/docs/networks.html#manual for details.\n. Looks like this is mistake in your deployment manifest. Take a look at your \"networks\" section, network with type \"manual\" (or without type \u2013 network without type is a manual by default) \u2013 there's a missing required \"range\" argument in one of subnets. See http://bosh.io/docs/networks.html#manual for details.\n. Looks like it already fixed\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh-director/spec/unit/api/instance_lookup_spec.rb#L58\n. Looks like it already fixed\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh-director/spec/unit/api/instance_lookup_spec.rb#L58\n. Looks like it won't work. InstanceManager class has template method instance_ips \u2013 https://github.com/cloudfoundry/bosh/blob/master/bosh-registry/lib/bosh/registry/instance_manager.rb#L45 which is defined only in subclasses.\n. Looks like it won't work. InstanceManager class has template method instance_ips \u2013 https://github.com/cloudfoundry/bosh/blob/master/bosh-registry/lib/bosh/registry/instance_manager.rb#L45 which is defined only in subclasses.\n. I see. So maybe it would be useful to add default method implementation to InstanceManager class, something like that:\nruby\ndef instance_ips(instance_id)\n  raise NotImplemenedError, \"Default implementation of InstanceManager does not support \" \\ \n    \"IPs retrieval. Create IaaS-specific subclass and override this method\"\nend\nIt would be nicer to get such explanation in case if someone will call read_settings with second argument specified instead of just NoMethodError.\n. I see. So maybe it would be useful to add default method implementation to InstanceManager class, something like that:\nruby\ndef instance_ips(instance_id)\n  raise NotImplemenedError, \"Default implementation of InstanceManager does not support \" \\ \n    \"IPs retrieval. Create IaaS-specific subclass and override this method\"\nend\nIt would be nicer to get such explanation in case if someone will call read_settings with second argument specified instead of just NoMethodError.\n. Stemcell for bosh-lite is bit outdated. Stemcells for other IaaSes contains monit 5.2.5.\n. Stemcell for bosh-lite is bit outdated. Stemcells for other IaaSes contains monit 5.2.5.\n. ",
    "fckbo": "Hi me again,\nI did change my manifest file to remove the warning about the \"template\" statement being deprecated when having several components deployed within the same vm by replacing with a \"templateS\" statement (in the job section of the manifest) thinking that it might fixed the problem but unfortunately it did not & I still have the same error as before when Binding configuration takes place:\nStarted preparing configuration > Binding configuration. Failed: Error filling in template director.yml.erb.erb' forbosh_api/0' (line 332: undefined method `each' for nil:NilClass) (00:00:00)\nError 100: Error filling in template director.yml.erb.erb' forbosh_api/0' (line 332: undefined method `each' for nil:NilClass)\nAny idea ?\nSEE BELOW THE NEW MANIFEST FILE I'M USING\nname: bosh2\nlogging:\n  -level: DEBUG\ndirector_uuid: 35b5ea35-213b-4494-8fb7-c1c191178824 \nrelease: {name: bosh, version: 139}\nnetworks:\n- name: default\n  type: manual\n  subnets:\n  - range: 10.105.144.224/28\n    gateway: 10.105.144.225\n    static:\n    - 10.105.144.237\n    - 10.105.144.238\n      reserved:\n      # .224 is special, not sure if I need to put it in the reserved range\n    - 10.105.144.235 - 10.105.144.236\n      # .239 is special, not sure if I need to put it in the reserved range\n      dns:\n      - 10.0.80.11\n      - 10.0.80.12\n        cloud_properties:\n        name: Private Network - vmnic0 vmnic2\nresource_pools:\n- name: default\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu-trusty-go_agent \n    version: 2830\n  network: default\n  size: 5\n  cloud_properties:\n    cpu: 2\n    ram: 512\n    disk: 2000\ncompilation:\n  reuse_compilation_vms: true\n  workers: 3\n  network: default\n  cloud_properties:\n    ram: 512\n    disk: 6000\n    cpu: 2\nupdate:\n  canaries: 1\n  canary_watch_time: 30000-90000\n  update_watch_time: 30000-90000\n  max_in_flight: 1\njobs:\n- name: bosh_data\n  templates:\n  - name: blobstore\n  - name: postgres\n  - name: redis\n    instances: 1\n    resource_pool: default\n    persistent_disk: 8_000\n    networks:\n  - name: default\n    static_ips: [10.105.144.238]\n- name: bosh_api\n  templates:\n  - name: nats\n  - name: director\n  - name: health_monitor\n  - name: powerdns\n    instances: 1\n    resource_pool: default\n    networks:\n  - name: default\n    static_ips: [10.105.144.237]\nproperties:\n  env:\nntp:\n    - servertime.service.softlayer.com\nnats:\n    user: nats\n    password: nats-password\n    address: 10.105.144.237\n    port: 4222\nblobstore:\n    address: 10.105.144.238\n    port: 25251\n    backend_port: 25552\n    agent:\n      user: agent\n      password: agent\n    director:\n      user: director\n      password: director\npostgres: &bosh_db\n    user: bosh\n    password: bosh\n    host: 10.105.144.238\n    port: 5432\n    database: bosh\nredis:\n    password: redis\n    address: 10.105.144.238\n    port: 25255\ndirector:\n    name: bosh2\n    address: 10.105.144.237\n    port: 25555\n    encryption: false\n    # Check if the CPI for your IaaS supports snapshots, otherwise disable it.\n    # As an example vCloud CPI 0.5.2 does not support snapshots\n    enable_snapshots: false\n    max_tasks: 100\n    db: *bosh_db\n    # If needed, limit the number of threads used to concurrently instantiate new vms (32 by default)\n    # max_threads: 1\nhm:\n    http:\n      port: 25923\n      user: admin\n      password: admin\n    director_account:\n      user: admin\n      password: admin\n    intervals:\n      poll_director: 60\n      poll_grace_period: 30\n      log_stats: 300\n      analyze_agents: 60\n      agent_timeout: 180\n      rogue_agent_alert: 180\n    loglevel: info\n    email_notifications: false\n    tsdb_enabled: false\n    cloud_watch_enabled: false\n    resurrector_enabled: true\ndns:\n    address: 10.105.144.237\n    recursor: 10.0.80.11\n    db: *bosh_db\nvcenter:\n    address: 10.105.220.39\n    user: Administrator\n    password: a5pattes\n    datacenters:\n      - name: bschmcSL1\n        vm_folder: mycfvm\n        template_folder: mycftemplate\n        disk_path: /cfdir\n        datastore_pattern: datastore1\n        persistent_datastore_pattern: datastore1\n        allow_mixed_datastores: true\n        clusters:\n          - cfcluster:\n. Hi me again,\nI did change my manifest file to remove the warning about the \"template\" statement being deprecated when having several components deployed within the same vm by replacing with a \"templateS\" statement (in the job section of the manifest) thinking that it might fixed the problem but unfortunately it did not & I still have the same error as before when Binding configuration takes place:\nStarted preparing configuration > Binding configuration. Failed: Error filling in template director.yml.erb.erb' forbosh_api/0' (line 332: undefined method `each' for nil:NilClass) (00:00:00)\nError 100: Error filling in template director.yml.erb.erb' forbosh_api/0' (line 332: undefined method `each' for nil:NilClass)\nAny idea ?\nSEE BELOW THE NEW MANIFEST FILE I'M USING\nname: bosh2\nlogging:\n  -level: DEBUG\ndirector_uuid: 35b5ea35-213b-4494-8fb7-c1c191178824 \nrelease: {name: bosh, version: 139}\nnetworks:\n- name: default\n  type: manual\n  subnets:\n  - range: 10.105.144.224/28\n    gateway: 10.105.144.225\n    static:\n    - 10.105.144.237\n    - 10.105.144.238\n      reserved:\n      # .224 is special, not sure if I need to put it in the reserved range\n    - 10.105.144.235 - 10.105.144.236\n      # .239 is special, not sure if I need to put it in the reserved range\n      dns:\n      - 10.0.80.11\n      - 10.0.80.12\n        cloud_properties:\n        name: Private Network - vmnic0 vmnic2\nresource_pools:\n- name: default\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu-trusty-go_agent \n    version: 2830\n  network: default\n  size: 5\n  cloud_properties:\n    cpu: 2\n    ram: 512\n    disk: 2000\ncompilation:\n  reuse_compilation_vms: true\n  workers: 3\n  network: default\n  cloud_properties:\n    ram: 512\n    disk: 6000\n    cpu: 2\nupdate:\n  canaries: 1\n  canary_watch_time: 30000-90000\n  update_watch_time: 30000-90000\n  max_in_flight: 1\njobs:\n- name: bosh_data\n  templates:\n  - name: blobstore\n  - name: postgres\n  - name: redis\n    instances: 1\n    resource_pool: default\n    persistent_disk: 8_000\n    networks:\n  - name: default\n    static_ips: [10.105.144.238]\n- name: bosh_api\n  templates:\n  - name: nats\n  - name: director\n  - name: health_monitor\n  - name: powerdns\n    instances: 1\n    resource_pool: default\n    networks:\n  - name: default\n    static_ips: [10.105.144.237]\nproperties:\n  env:\nntp:\n    - servertime.service.softlayer.com\nnats:\n    user: nats\n    password: nats-password\n    address: 10.105.144.237\n    port: 4222\nblobstore:\n    address: 10.105.144.238\n    port: 25251\n    backend_port: 25552\n    agent:\n      user: agent\n      password: agent\n    director:\n      user: director\n      password: director\npostgres: &bosh_db\n    user: bosh\n    password: bosh\n    host: 10.105.144.238\n    port: 5432\n    database: bosh\nredis:\n    password: redis\n    address: 10.105.144.238\n    port: 25255\ndirector:\n    name: bosh2\n    address: 10.105.144.237\n    port: 25555\n    encryption: false\n    # Check if the CPI for your IaaS supports snapshots, otherwise disable it.\n    # As an example vCloud CPI 0.5.2 does not support snapshots\n    enable_snapshots: false\n    max_tasks: 100\n    db: *bosh_db\n    # If needed, limit the number of threads used to concurrently instantiate new vms (32 by default)\n    # max_threads: 1\nhm:\n    http:\n      port: 25923\n      user: admin\n      password: admin\n    director_account:\n      user: admin\n      password: admin\n    intervals:\n      poll_director: 60\n      poll_grace_period: 30\n      log_stats: 300\n      analyze_agents: 60\n      agent_timeout: 180\n      rogue_agent_alert: 180\n    loglevel: info\n    email_notifications: false\n    tsdb_enabled: false\n    cloud_watch_enabled: false\n    resurrector_enabled: true\ndns:\n    address: 10.105.144.237\n    recursor: 10.0.80.11\n    db: *bosh_db\nvcenter:\n    address: 10.105.220.39\n    user: Administrator\n    password: a5pattes\n    datacenters:\n      - name: bschmcSL1\n        vm_folder: mycfvm\n        template_folder: mycftemplate\n        disk_path: /cfdir\n        datastore_pattern: datastore1\n        persistent_datastore_pattern: datastore1\n        allow_mixed_datastores: true\n        clusters:\n          - cfcluster:\n. Karl,\nthx a lot that was it!!  Apparently I need to specify a resource pool under the cluster name.\nWhat bugs me a bit is that:\n1) As I said this manifest is working properly with release 912 (but ok things change ...)\n2) but reading the documentation here http://docs.cloudfoundry.org/bosh/vsphere-cpi.html#resource-pools,  it says resource pool is optional: \n\" clusters [Array, required]: Array of clusters to use for VM placement.\n     [String, required]: Cluster name.\n        resource_pool [String, optional]: Specific vSphere resource pool to use within the cluster.\"\nI actually put the following\n        clusters:\n        - cfcluster:\n            resource_pool: mycf\nwithout creating the resource_pool \"mycf\" on  vcenter and it worked: Install succeeded, still need to check now that I can deploy new stuff (in fact a CF) on it.\nI've got the following questions:\n1) any insights on how bosh uses resource_pool parameter ?\n2) before posting this issue I had done a couple of tries to patch the bosh ruby code with additional logs/traces on the vm running the MicroBosh that is used to deploy my BOSH instance but it did take into account my changes. For example I made some changes in the following file\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-core-1.2830.0/lib/bosh/director/core/templates/source_erb.rb but none of my new traces were taken in account. I guess it's because some  compilation is taking place to generate the director on the microbosh server when its installation took place so is there anyway to re-generate on the microbosh server itself a module ?\nThank again for your help. \n. Karl,\nthx a lot that was it!!  Apparently I need to specify a resource pool under the cluster name.\nWhat bugs me a bit is that:\n1) As I said this manifest is working properly with release 912 (but ok things change ...)\n2) but reading the documentation here http://docs.cloudfoundry.org/bosh/vsphere-cpi.html#resource-pools,  it says resource pool is optional: \n\" clusters [Array, required]: Array of clusters to use for VM placement.\n     [String, required]: Cluster name.\n        resource_pool [String, optional]: Specific vSphere resource pool to use within the cluster.\"\nI actually put the following\n        clusters:\n        - cfcluster:\n            resource_pool: mycf\nwithout creating the resource_pool \"mycf\" on  vcenter and it worked: Install succeeded, still need to check now that I can deploy new stuff (in fact a CF) on it.\nI've got the following questions:\n1) any insights on how bosh uses resource_pool parameter ?\n2) before posting this issue I had done a couple of tries to patch the bosh ruby code with additional logs/traces on the vm running the MicroBosh that is used to deploy my BOSH instance but it did take into account my changes. For example I made some changes in the following file\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-core-1.2830.0/lib/bosh/director/core/templates/source_erb.rb but none of my new traces were taken in account. I guess it's because some  compilation is taking place to generate the director on the microbosh server when its installation took place so is there anyway to re-generate on the microbosh server itself a module ?\nThank again for your help. \n. karlfi, cppforlife,\nthx  a lot for your answers, removing the ':' after cluster make it work without resource pool.\nI didn't know anything about bosh few days ago and now go it up & running on two environments with 2 different versions so I kind of like this tool\nI'm now trying to make use of it to first deploy jenkins on one environment and then cf on another environment, let see how that goes...\nThx again.\n. karlfi, cppforlife,\nthx  a lot for your answers, removing the ':' after cluster make it work without resource pool.\nI didn't know anything about bosh few days ago and now go it up & running on two environments with 2 different versions so I kind of like this tool\nI'm now trying to make use of it to first deploy jenkins on one environment and then cf on another environment, let see how that goes...\nThx again.\n. ",
    "cwb124": "Geokala, when you got this error did you determine if the disk space issues was on the local box or the director?. +1. Here is what my resource_pools section looks like:\nresource_pools:\n- name: vms\n  network: default\n  env:\n    bosh:\n      password: password\n      mbus:\n        cert: ((mbus_bootstrap_ssl))\nI assume this sets the vcap password to 'password'?  I have done this but can't ssh vcap@director_ip using that password.  . So I made some changes to the formatting for all the certs and keys under the consul section of the deployment manifest.  I linted the yml and it's good.  I reran bosh deploy and still failed but it's slightly different now.  Instead of the consul agent failing and the metron agent being unknown, now the consul_agent is running and the metron_agent is unknown.  A slight improvement.\nThe exact error is:\nconsul_z1/0 (d47b3957-676c-47c2-b1bd-07bb4af9732b)' is not running after update. Review logs for failed jobs: metron_agent (00:10:58)\nError 400007: 'consul_z1/0 (d47b3957-676c-47c2-b1bd-07bb4af9732b)' is not running after update. Review logs for failed jobs: metron_agent\nThe metron.log shows the same error as I pasted above, but now it's apparently the only roadblock I have to a successful deployment:\n2017/08/02 21:04:51 Could not use GRPC creds for client: failed to load keypair: tls: failed to find any PEM data in certificate input\nShould I still tinker with the cert format?  Which specific cert/key is the issue here?  . We can close the issue.  I'm not sure what the fix was but I rebuilt the deployment manifest from the stub example on the Cloud Foundry docs and paid close attention to the certs and it worked just fine.  Apologies for not having a solution for the next person who finds this.. ",
    "geokala": "Hi cwb124,\nIt's been a while now so my memory may be faulty but I think it was the local machine (the one on which the bosh CLI was being run, not the director).. ",
    "dreammonger": "Commenting line 171 in vpc.rb solved the problem for me. Thanks a lot.\n. ",
    "cerdmann": "I needed to do this as well.\n. ",
    "gibbsoft": "I also hit this bug.  Thanks for the workaround.\n. ",
    "jakubincloud": "Thanks for reporting the issue. I hope workaround can be pushed to production\n. ",
    "dignajar": "I hit the bug too.. commented the line and is work now..\n. ",
    "eitansuez": "interesting. does this imply that the official cloudfoundry docs are stale?\nhttp://docs.cloudfoundry.org/deploying/aws/setup_aws.html. ",
    "biolounge": "Its seems like this change havent pushed to production yet.\nStill getting the error:\n/var/lib/gems/2.3.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:375:in return_or_raise': The dhcpOptions 'dopt-2f97754b' has dependencies and cannot be deleted. (AWS::EC2::Errors::DependencyViolation)\n    from /var/lib/gems/2.3.0/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:476:inclient_request'\n    from (eval):3:in `delete_dhcp_options'\n. ",
    "jsilberm": "Is there a workaround, please?\n. ",
    "rhuiser": "Dead end... deployment fails later on with MySQL error: \n``\n.rvm/gems/ruby-2.2.2/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:375:inreturn_or_raise': Cannot find version 5.5.40a for mysql (AWS::RDS::Errors::InvalidParameterCombination)\n    from /Users/robinh/.rvm/gems/ruby-2.2.2/gems/aws-sdk-v1-1.60.2/lib/aws/core/client.rb:476:in client_request'\n    from (eval):3:increate_db_instance'\n```. ",
    "djvdorp": "We're running into the exact same issue as described above by @arekkas .\n. Hi @Amit-PivotalLabs \nI must apologize that I missed your response to my Slack question, which I will put below for other people's reference which might be useful:\n```\namitgupta [6:09 PM] \n@djvdorp: the problem with those docs and tools is they can get out of date. We're working on some tooling that will be actively supported. For now you've got a few options\n\nStill try to use the \"bosh aws\" tooling. Last time we tried those instructions they worked, with some small road bumps though\nThose docs also outline a manual procedure you could try\nThe new tooling is still in experimental phase, but I can point you to it\n```\n\nI did not have the opportunity yet to discuss this with my colleagues who are working with me on deploying CloudFoundry on AWS, but today we have spent quite some time on getting option 1 to work. For tomorrow we were thinking about looking into option 2, but I would be very interested to learn more about option 3 since I highly prefer automation over manual work. \nCould you point us towards information regarding this option, and guide us in using this new tooling to try out? Thank you for your time and effort so far, it is highly appreciated!\n. ",
    "riccardomc": "The problem seems to be in bosh_aws_cpi aki_picker module. It expects to find a kernel image (AKI) with a name matching a certain regex.\nIn Frankfurt such an AKI is not available. It easy to verify it with:\naws ec2 --region=eu-central-1 describe-images --filters \\\n        Name=image-type,Values=kernel    \\\n        Name=owner-alias,Values=amazon \\\n        Name=architecture,Values=x86_64 | grep ImageLocation | grep pv-grub\nwhich returns:\n\"ImageLocation\": \"amzn-ami-eu-central-1/pv-grub-hd0_1.04-x86_64.gz.manifest.xml\", \n\"ImageLocation\": \"amzn-ami-eu-central-1/pv-grub2-hd0_2.02-1.b2-x86_64.xz.manifest.xml\",\nThis patch bypasses the problem, and a way to monkey patch the module can be found in this fork.\nHowever, this is far from being a solution. The same issue pops up again later on in the bosh prepare deployment step.\n. Thanks for the reply.\nYes, you are right, but the provision.sh script is uploaded to the jump/bastion instance. Is this what you mean?\n. Another update.\nExecuting again make provision fails while trying bosh -n deploy with:\nStarted preparing deployment > Binding stemcells. Failed: Stemcell `bosh-aws-xen-ubuntu-trusty-go_agent/2977' doesn't exist (00:00:00)\nSo I tried to upload the stemcell manually from the bastion instance:\nbosh upload stemcell ./workspace/deployments/microbosh/deployments/bosh-vpc-8c28eee5/bosh-stemcell-2977-aws-xen-ubuntu-trusty-go_agent.tgz\nWhich also fails with Uploading stemcellcreate stemcell failed: unable to find AKI. This means that aki_picker.rb on the BOSH instance needs to be patched as well. After applying the patch, make provision continues.\nSadly, now I am running into this:\nhttps://github.com/cloudfoundry/bosh-lite/issues/265\n. ",
    "ljfranklin": "@dpb587 possibly related to this change from March 2016, but honestly I have no idea. I definitely haven't seen this in the last 6 months. You're probably safe to close this out.. In the AWS CPI we have a bit of code that seems like it was intended to fix a similar issue:\nhttps://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/blob/77f493fd1723e0403e57e55f7e41132ad2a65d3d/src/bosh_aws_cpi/lib/cloud/aws/cloud.rb#L61-L63\n```\nAWS Ruby SDK is threadsafe but Ruby autoload isn't,\nso we need to trigger eager autoload while constructing CPI\nAWS.eager_autoload!\n```\nMight be worth a try to drop that line into the blobstore client and see if the issue persists.\n. @Niranjana-5588 the folks that manage the cf-release repo would be better equipped to answer manifest generation questions. You could also ask at cloudfoundry.slack.com in the #release-integration channel.\n. Apparently I can't close issues in this repo, pinging @tylerschultz or @cppforlife to close.\n. Hello @LinuxBozo and friends,\nDefinitely agree that the requirement to be within a full EC2 instance is a no-go in Concourse. We currently don't have a nice workaround, but sounded like @cppforlife would be open to supporting your use-case in the CPI.\nIf you are feeling adventurous, you could try to produce your own light stemcells. We created the light stemcell builder tool to produce the official light AWS stemcells. This runs within Concourse (see pipeline config) and creates a light stemcell with the following workflow:\n1. Convert heavy stemcell image to stream optimized VMDK (shortens upload time)\n2. Upload VMDK to S3 bucket\n3. Create Volume from file on S3\n4. Create Snapshot from Volume\n5. Create AMI from Snapshot\nOne problem, I don't think this tool currently provides a way to configure the AWS endpoint. You could try your luck at a PR to make the endpoint configurable, or talk @cppforlife into prioritizing that work. Let us know if you have other questions / thoughts.\n. Glad to hear it @dlapiduz! If you have the time, would you mind submitting a PR with your changes to the light stemcell project? It's always tough for us to add GovCloud changes on our own as we don't have access to that environment.\n. @cppforlife @tylerschultz I think this can be closed out\n. Closing this for now, we'll re-open once we finish https://www.pivotaltracker.com/story/show/121077527 and https://www.pivotaltracker.com/story/show/120780091 and bump to the latest version.\n. Once this is merged, we should publish a new CPI gem for the Ruby CPIs to consume.\n. Once this is merged, we need to publish a new version of the gem. Not sure what the process is for that.\nAlso if you close and reopen the PR, DreddBot will show that Forrest has signed the CLA.\n. Story is here: https://www.pivotaltracker.com/story/show/128111241\n. After this is merged, you can close out https://github.com/cloudfoundry/bosh/issues/1387\n. @youngm nice detective work! We've updated this story to reflect that finding. We're currently planning on adding the following property:\nyaml\nproperties:\n  vsphere:\n    default_disk_type: 'thin' # defaults to 'preallocated', only supported values are 'preallocated' and 'thin'\nThis would be used as the default for both ephemeral and persistent disks. However, the default persistent disk type could still be overridden by specifying disk_pools.cloud_properties.type.\nSound okay?\n. @youngm I think we should keep eagerlyScrub to false for now. Scrubbing doesn't apply to thin disks as they are not allocated up front, and preallocated disks are not zero-ed out in advance to make disk creation faster. Seems like the only advantage to eagerlyScrub is to enable vSphere fault tolerance, but if you're already using BOSH you can deploy multiple instances across different AZs (Datacenter or cluster) to get the same redundancy.\nWhat worries us is the long time to bring up a VM if eagerlyScrub is true. For example, let's assume a Concourse CI VM with a 100GB disk that's backed by magnetic media with a throughput of 50MB/s \u2014 it would take ~33 minutes to zero-out the disk before we could bring the VM.\nYou will still be able to specify the full range of disk types (e.g. eagerZeroedThick) on your persistent disk pools by setting disk_pools.cloud_properties.type.\n. @youngm we were a little messy when we squashed our commits. Ephemeral disk change is in the next commit: https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/commit/77f14a29c14ced0c68036d678e1bab255f24354e\n. @youngm :( that's a little embarrassing, sorry about that. Ping @cppforlife and @cunnie to prioritize the fix. \n. Turns out we do need to remove /etc/dhcp/dhclient-exit-hooks.d/set_hostname as this hook causes dhclient to override the hostname (which should be the agent ID) with the VM name.\n. Some food for thought going forward, Concourse recently switched to producing dynamically linked OSX binaries for fly. Apparently this fixes some DNS related issues and OSX Keychain integration. I also remember a blog post recommending to always statically compile Golang Linux binaries, but recommended dynamically linking OSX binaries for similar reason. Not sure if the BOSH team would want to do the same long-term.\n. cc @zaksoup, new CPI team anchor. @cppforlife @MatthiasWinzeler question about the context hash handling in the CPIs, e.g. cloud_properties['openstack'].merge!(context['openstack']). I like that the global CPI configuration is now dynamic, but we seem to have this weird mix of dynamic values vs static values in the cpi.json file.\nSince the expectation is for all cloud.properties.openstack values to be pulled at runtime from the context, I'd like to push back against the merge call and go back to overriding: cloud_properties['openstack'] = context['openstack']. Having to add properties to spec files and ERB templates seems like a waste when its only use is to provide default values. I think it would be easier as a developer to have default values in the code instead and make everything dynamic. I'd imagine that users don't look at the CPI spec file for defaults, but would use the bosh.io CPI docs instead.\nBig thanks for the PR btw \ud83d\udc4d \n. @cppforlife agreed that cloud.properties.{registry,agent,blobstore} should come from cpi.json for now. But I would like for cloud.properties.openstack to come exclusively from dynamic values by removing the merge call.\n. @voelzmo I agree that all defaults should ideally go in a single place. But a CPI can have default properties for vm_type.cloud_properties or disk_pool.cloud_properties that cannot live in the spec file. So you now have some defaults in your code (e.g. AWS disk_pools.cloud_properties.type) and some in the spec file (e.g. use_dhcp). So the \"all defaults in one place\" rule for CPIs has to be defaults defined in the code. Defaults in the code are also much easier to unit test as compared to ERB rendering.\n. @cppforlife @MatthiasWinzeler I realize this thought is coming a little late, but do we actually need a separate cpi-config file and command or could we just add those properties to the cloud-config?\nThe distinction is more or less an implementation detail as the cloud config is already full of properties that will be passed to the CPI. I'm worried users not familiar with the internals of BOSH won't understand why some IaaS properties need to go in cloud-config and others in cpi-config. I'd imagine this also simplifies the multi-cpi implementation and solves the issue mentioned by @tjvman where multi-cpi could break bosh recreate. Let me know if I'm missing something.\n. @knail1 I don't think the CPI ever tries to contact the IAM endpoint, only the EC2 and ELB endpoints. Does your director manifest have credentials_source: env_or_profile? If so, my guess is you are running bosh-init from a workstation instead of within an EC2 instance so the CPI is not able to contact the AWS metadata endpoint.\n. @knail1 are you still seeing this issue?\n. > this was always links behaviour for dynamic networks.\nMaybe I jumped the gun a bit on the title. More specifically, I have Concourse deployed to a single VM so a bunch of jobs are all co-located. I used a dynamic network to deploy this VM as I didn't require any fixed internal IPs. Prior to the latest BOSH director release (v260), the Concourse templates were able to resolve spec.address correctly, e.g. here. I'm not sure if spec.address was returning localhost or similar before, but the Concourse deployment was definitely working. Maybe this issue only affects single VM deployments that use links and dynamic networks?. @cppforlife this was for my personal Concourse deployment.. @cppforlife @tylerschultz don't merge this one, please close out in favor of https://github.com/cloudfoundry/bosh/pull/1533. Please publish a new version of the bosh_cpi gem after merging this. Yeah, symlink is not strictly necessary. I assumed they were just being defensive around people using absolute paths and expecting tcpdump to be at its canonical path.. @dpb587 solving the problem holistically sounds like it will require a lot of work and coordination. But maybe there's a smaller, easier fix related to the shared bosh-package releases, e.g. golang-release. Right now if release A and release B are co-located and both vendor the golang-1.10-linux package, they'll get an error if one is on a different patch version of golang correct? If all releases moved to the vendor package flow, I'd imagine this would be a super-common occurrence. To sidestep this issue, would it be better to have the vendored package names include the patch version, e.g. golang-1.10.0-linux? Then when one release bumps to a newer patch version they can still be co-located without waiting for the other release to also bump. Please let me know if I'm misunderstanding something about the vendor package implementation though.. @dpb587 a proxy server might work for consuming private releases internally, but we'd like a mechanism to share private releases with people outside our company. You could argue that sharing read-only S3 keys with a large number of people isn't really \"private\" anyway, but making the buckets public is meeting a bit of push back.. ",
    "jfuerth": "We've pushed this to develop: ee9361cd981f9ca6b422bd51e5ea6d420919c62f\n. Thanks! This is on develop now as 2440b7689abe48715572d3ea8a267d5edb877249\n-Nader and Jonathan\n. We've pushed this to develop: ffe72bb4a269629a45519da7e7b43fab6c71c326\n. We've pushed this to develop: ffe72bb4a269629a45519da7e7b43fab6c71c326\n. We've pushed this to develop: 1c0852b05f85d60f3902e5fab0cb84ea3c0ee67a\n. We've pushed this to develop: 1c0852b05f85d60f3902e5fab0cb84ea3c0ee67a\n. ",
    "chipchilders": "This contributor has filed an ICLA, so the pull request is OK to be merged. We will get the CLA whitelist updated for him soon.\n. This contributor has filed an ICLA, so the pull request is OK to be merged. We will get the CLA whitelist updated for him soon.\n. @yuanzhao if you work for Pivotal, can you please join and make public your membership in one of the following GitHub orgs? You should be covered by the Pivotal CLA.\npivotal\npivotal-cf\nPivotal-Field-Engineering\npivotal-golang\nOtherwise, please send to contributors at cloudfoundry dot org instead of as an image in the PR.\n. @yuanzhao if you work for Pivotal, can you please join and make public your membership in one of the following GitHub orgs? You should be covered by the Pivotal CLA.\npivotal\npivotal-cf\nPivotal-Field-Engineering\npivotal-golang\nOtherwise, please send to contributors at cloudfoundry dot org instead of as an image in the PR.\n. CLA received. This is cleared for merge if technically accepted. cc/ @cppforlife \n. CLA received. This is cleared for merge if technically accepted. cc/ @cppforlife \n. ",
    "cyrille-leclerc": "Indeed, I am using traveling bosh, it was installed this week so the version https://github.com/cloudfoundry-community/traveling-bosh/releases/tag/v1.2809.0\n. Thanks @drnic, I'll try but I am completely new to Ruby and the learning curve has been stiff fore te moment :-)\nEnhancing error message of the CLI may be an easier entry point for me.\nAnyway, I'll try, thanks!\n. @cppforlife this looks perfect.\n. ",
    "JamesClonk": "I'm having the same issue, but with different error codes. I'm not using traveling-bosh though, but have it installed as a ruby gem.\nIt seems bosh deploy always returns with exitcode 0 even if there were errors during the deployment.\nFor example:\n```\n$ bosh deploy\nGenerating deployment manifest\nProcessing deployment manifest\nGetting deployment properties from director...\nCompiling deployment manifest...\nDeploying\nDeployment name: 'cf.yml'\nDirector name: 'bosh'\nDirector task 11\n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done (00:00:00)\n  Started preparing deployment > Binding releases. Done (00:00:00)\n  Started preparing deployment > Binding existing deployment. Done (00:00:00)\n  Started preparing deployment > Binding resource pools. Done (00:00:00)\n  Started preparing deployment > Binding stemcells. Done (00:00:00)\n  Started preparing deployment > Binding templates. Done (00:00:00)\n  Started preparing deployment > Binding properties. Done (00:00:00)\n  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)\n  Started preparing deployment > Binding instance networks. Failed: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use (00:00:00)\n   Failed preparing deployment (00:00:00)\nError 130008: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use\nTask 11 error\nFor a more detailed error report, run: bosh task 11 --debug\n$ echo $?\n0\n```\n$ bosh version\nBOSH 1.2922.0\n. I'm having the same issue, but with different error codes. I'm not using traveling-bosh though, but have it installed as a ruby gem.\nIt seems bosh deploy always returns with exitcode 0 even if there were errors during the deployment.\nFor example:\n```\n$ bosh deploy\nGenerating deployment manifest\nProcessing deployment manifest\nGetting deployment properties from director...\nCompiling deployment manifest...\nDeploying\nDeployment name: 'cf.yml'\nDirector name: 'bosh'\nDirector task 11\n  Started preparing deployment\n  Started preparing deployment > Binding deployment. Done (00:00:00)\n  Started preparing deployment > Binding releases. Done (00:00:00)\n  Started preparing deployment > Binding existing deployment. Done (00:00:00)\n  Started preparing deployment > Binding resource pools. Done (00:00:00)\n  Started preparing deployment > Binding stemcells. Done (00:00:00)\n  Started preparing deployment > Binding templates. Done (00:00:00)\n  Started preparing deployment > Binding properties. Done (00:00:00)\n  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)\n  Started preparing deployment > Binding instance networks. Failed: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use (00:00:00)\n   Failed preparing deployment (00:00:00)\nError 130008: 'router_z1/0' asked for a static IP 192.168.1.80 but it's already reserved/in use\nTask 11 error\nFor a more detailed error report, run: bosh task 11 --debug\n$ echo $?\n0\n```\n$ bosh version\nBOSH 1.2922.0\n. I'm using rvm and bundler to handle my ruby env.\n$ which bosh\n~/.rvm/gems/ruby-2.2.1/bin/bosh\n. I'm using rvm and bundler to handle my ruby env.\n$ which bosh\n~/.rvm/gems/ruby-2.2.1/bin/bosh\n. @hochm Sorry, my bad. Didn't have the org-membership set public yet. Changed it now though.\n. @hochm Sorry, my bad. Didn't have the org-membership set public yet. Changed it now though.\n. @cppforlife If multi-VM bosh is not supposed to be used anymore, how can we have a bosh capable of managing deployments that target separate openstack tenants?\nIn our current setup bosh and CF are not within the same tenant. In the bosh manifest we configured the openstack credentials for the other tenant where all the deployments get put by bosh.\n\nedit: never mind, I just had a closer look at the bosh-init manifest. Silly me.\nTried with different tenants and it worked. :smiley: \nSo anyway, the aim is to not use \"old\" multi-VM bosh anymore, but switch over to a single director VM by bosh-init?\n. :+1: \n. :+1: \n. it worked in bosh 256.10 still, so this problem was introduced with bosh 257.\n. I can confirm this, we are seeing the same issue on bosh 258, also running latest stemcell 3263.8.\nI also couldnt find anything special in the logs. No idea whats causing this. \ud83d\ude26 \nWe noticed this because we kept getting timeouts on director API GET calls.\n. I can confirm this, we are seeing the same issue on bosh 258, also running latest stemcell 3263.8.\nI also couldnt find anything special in the logs. No idea whats causing this. \ud83d\ude26 \nWe noticed this because we kept getting timeouts on director API GET calls.\n. @paolostivanin Yes, we're also running on Openstack, using the Ubuntu stemcell.\n. @paolostivanin Yes, we're also running on Openstack, using the Ubuntu stemcell.\n. Maybe its not an Openstack problem, but of the bosh db?\nWe're not using postgres, but an external mariadb galera cluster to host the bosh db.\n. Maybe its not an Openstack problem, but of the bosh db?\nWe're not using postgres, but an external mariadb galera cluster to host the bosh db.\n. ``\nD, [2018-06-13T14:01:56.153806 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000085s) (conn: 47282660238500) BEGIN                                                                                                     [18/5050]\nD, [2018-06-13T14:01:56.154497 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000324s) (conn: 47282660238500) UPDATEtasksSETevent_output= CONCAT(event_output, '{\\\"time\\\":1528898516,\\\"stage\\\":\\\"Preparing deployment\\\"\n,\\\"tags\\\":[],\\\"total\\\":1,\\\"task\\\":\\\"Preparing deployment\\\",\\\"index\\\":1,\\\"state\\\":\\\"failed\\\",\\\"progress\\\":100,\\\"data\\\":{\\\"error\\\":\\\"765: unexpected token at \\'{\\\\\\\"mapped_properties\\\\\\\":{\\\\\\\"consul\\\\\\\":{\\\\\\\"agent_cert\\\\\\\":\\\\\\\"-----BEGIN CERTIFICATE-----\\\\\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\\\\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi\\'\\\"}}\\n') WHERE (id= 6993)\nD, [2018-06-13T14:01:56.155194 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000586s) (conn: 47282660238500) COMMIT\nD, [2018-06-13T14:01:56.156848 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000628s) (conn: 70139450127740) DELETE FROMlocksWHERE ((name= 'lock:deployment:cloudfoundry') AND (uid= '4094bccd-36b8-4cfb-a6cc-e197dd121f5e'))\nD, [2018-06-13T14:01:56.156933 #10236] [task:6993] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:cloudfoundry uid: 4094bccd-36b8-4cfb-a6cc-e197dd121f5e\nD, [2018-06-13T14:01:56.157754 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000067s) (conn: 70139450196440) BEGIN\nD, [2018-06-13T14:01:56.158311 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000167s) (conn: 70139450196440) INSERT INTOevents(parent_id,timestamp,user,action,object_type,object_name,error,task,d\neployment,instance,context_json) VALUES (NULL, '2018-06-13 14:01:56', 'scis-tgdbefa3', 'release', 'lock', 'lock:deployment:cloudfoundry', NULL, '6993', 'cloudfoundry', NULL, '{}')\nD, [2018-06-13T14:01:56.158695 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000177s) (conn: 70139450196440) SELECT * FROMeventsWHEREid= 30334019\nD, [2018-06-13T14:01:56.159330 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000440s) (conn: 70139450196440) COMMIT\nI, [2018-06-13T14:01:56.159665 #10236] [task:6993]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2018-06-13T14:01:56.159763 #10236] [task:6993] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"28ba7e70-2c63-483c-ab87-81284f7333af\",\"severity\":3,\"source\":\"director\",\"title\":\"director - error during update deployme\nnt\",\"summary\":\"Error during update deployment for 'cloudfoundry' against Director '50e519ae-15d7-412c-9b9f-eb9a54e2f5ef': #<JSON::ParserError: 765: unexpected token at '{\\\"mapped_properties\\\":{\\\"consul\\\":{\\\"agent_cert\\\":\\\"-----BE\nGIN CERTIFICATE-----\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi'>\",\"created_at\":1528898\n516,\"deployment\":\"cloudfoundry\"}\nD, [2018-06-13T14:01:56.160749 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000064s) (conn: 47282650113180) BEGIN\nD, [2018-06-13T14:01:56.161254 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000140s) (conn: 47282650113180) INSERT INTOevents(parent_id,timestamp,user,action,object_type,object_name,error,task,d\neployment,instance,context_json) VALUES (30333466, '2018-06-13 14:01:56', 'scis-tgdbefa3', 'update', 'deployment', 'cloudfoundry', '765: unexpected token at \\'{\\\"mapped_properties\\\":{\\\"consul\\\":{\\\"agent_cert\\\":\\\"-----BEGIN\n CERTIFICATE-----\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi\\'', '6993', 'cloudfoundry'\n, NULL, '{}')\nD, [2018-06-13T14:01:56.161567 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000145s) (conn: 47282650113180) SELECT * FROMeventsWHEREid= 30334020\nD, [2018-06-13T14:01:56.162174 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000418s) (conn: 47282650113180) COMMIT\nD, [2018-06-13T14:01:56.167568 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.004558s) (conn: 70139461256520) SELECT * FROMdeploymentsWHERE (name= 'cloudfoundry') LIMIT 1\nD, [2018-06-13T14:01:56.168961 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000086s) (conn: 70139460640120) SELECT * FROMvariable_setsWHERE (variable_sets.deployment_id= 13) ORDER BYcreated_atDESC LIMIT 1\nD, [2018-06-13T14:01:56.169535 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000066s) (conn: 70139450127740) BEGIN\nD, [2018-06-13T14:01:56.170045 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000189s) (conn: 70139450127740) UPDATEvariable_setsSETwritable= 0 WHERE (id= 147) LIMIT 1\nD, [2018-06-13T14:01:56.170699 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000537s) (conn: 70139450127740) COMMIT\nD, [2018-06-13T14:01:56.171282 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000061s) (conn: 70139450196440) BEGIN\nD, [2018-06-13T14:01:56.171777 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000199s) (conn: 70139450196440) UPDATEtasksSETevent_output= CONCAT(event_output, '{\\\"time\\\":1528898516,\\\"error\\\":{\\\"code\\\":100,\\\"message\\\n\":\\\"765: unexpected token at \\'{\\\\\\\"mapped_properties\\\\\\\":{\\\\\\\"consul\\\\\\\":{\\\\\\\"agent_cert\\\\\\\":\\\\\\\"-----BEGIN CERTIFICATE-----\\\\\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\\\\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2\nEwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi\\'\\\"}}\\n') WHERE (id= 6993)\nD, [2018-06-13T14:01:56.172327 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000459s) (conn: 70139450196440) COMMIT\nE, [2018-06-13T14:01:56.172459 #10236] [task:6993] ERROR -- DirectorJobRunner: 765: unexpected token at '{\"mapped_properties\":{\"consul\":{\"agent_cert\":\"-----BEGIN CERTIFICATE-----\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJK\noZIhvcNAQELBQAw\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/json-2.1.0/lib/json/common.rb:156:inparse'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/json-2.1.0/lib/json/common.rb:156:in parse'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/links/links_manager.rb:284:inblock (2 levels) in update_provider_intents_contents'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/links/links_manager.rb:281:in each'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/links/links_manager.rb:281:inblock in update_provider_intents_contents'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/links/links_manager.rb:279:in each'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/links/links_manager.rb:279:inupdate_provider_intents_contents'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/deployment_plan/assembler.rb:151:in bind_links'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/deployment_plan/assembler.rb:88:inbind_models'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:91:in block (2 levels) in perform'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/event_log.rb:105:inadvance_and_track'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:79:in block in perform'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/lock_helper.rb:13:inblock in with_deployment_lock'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/lock.rb:79:in lock'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/lock_helper.rb:13:inwith_deployment_lock'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:58:in perform'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:99:inperform_job'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in block in run'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:52:inwith_thread_name'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in run'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:inperform'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:in block in perform'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:83:inblock (3 levels) in run'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/eventmachine-1.2.5/lib/eventmachine.rb:1076:in block in spawn_threadpool'\n/var/vcap/data/packages/director/4d1cc6d9fe5c6bb2cdd1c5a6d225ecdf3a674690/gem_home/ruby/2.4.0/gems/logging-2.2.2/lib/logging/diagnostic_context.rb:474:inblock in create_with_logging_context'\nD, [2018-06-13T14:01:56.173022 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000168s) (conn: 47282650113180) SELECT * FROM tasks WHERE id = 6993\nD, [2018-06-13T14:01:56.173852 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000065s) (conn: 47282660238500) BEGIN\nD, [2018-06-13T14:01:56.174545 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000258s) (conn: 47282660238500) UPDATE tasks SET state = 'error', timestamp = '2018-06-13 14:01:56', description = 'create deployment', re\nsult = '765: unexpected token at \\'{\\\"mapped_properties\\\":{\\\"consul\\\":{\\\"agent_cert\\\":\\\"-----BEGIN...', output = '/var/vcap/store/director/tasks/6993', checkpoint_time = '2018-06-13 14:01:55', type = 'update_deployment', username = 'scis-tgdbefa3', deployment_name = 'cloudfoundry', started_at = '2018-06-13 13:59:25', event_output = '{\\\"time\\\":1528898365,\\\"stage\\\":\\\"Preparing deployment\\\",\\\"tags\\\":[],\\\"total\\\":1,\\\"task\\\":\\\"Preparing deploym\nent\\\",\\\"index\\\":1,\\\"state\\\":\\\"started\\\",\\\"progress\\\":0}\\n{\\\"time\\\":1528898516,\\\"stage\\\":\\\"Preparing deployment\\\",\\\"tags\\\":[],\\\"total\\\":1,\\\"task\\\":\\\"Preparing deployment\\\",\\\"index\\\":1,\\\"state\\\":\\\"failed\\\",\\\"progress\\\":100,\\\"data\\\"\n:{\\\"error\\\":\\\"765: unexpected token at \\'{\\\\\"mapped_properties\\\\\":{\\\\\"consul\\\\\":{\\\\\"agent_cert\\\\\":\\\\\"-----BEGIN CERTIFICATE-----\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\nFjEUMBIGA1UEAwwLYW1j\nX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb25zdWxfYWdlbnQwggEi\\'\\\"}}\\n{\\\"time\\\":1528898516,\\\"error\\\":{\\\"code\\\":100,\\\"message\\\":\\\"765: unexpected token at \\'{\\\\\"mapped_properties\\\\\":{\\\\\"consul\n\\\\\":{\\\\\"agent_cert\\\\\":\\\\\"-----BEGIN CERTIFICATE-----\\\\nMIIEKTCCAhGgAwIBAgIRANOnpq2Z73OldyoTTlykphkwDQYJKoZIhvcNAQELBQAw\\\\nFjEUMBIGA1UEAwwLYW1jX3Jvb3RfY2EwHhcNMTcxMjA1MTI0MzA4WhcNMzcxMjA1\\\\nMTI0MzI4WjAXMRUwEwYDVQQDDAxjb2\n5zdWxfYWdlbnQwggEi\\'\\\"}}\\n', result_output = '', context_id = '' WHERE (id = 6993) LIMIT 1\nD, [2018-06-13T14:01:56.175084 #10236] [task:6993] DEBUG -- DirectorJobRunner: (0.000420s) (conn: 47282660238500) COMMIT\n. It seems to have something to do with the changed links handling inside BOSH.. Just confirmed it through testing, it is because of the link_provider_intents.metadata column.\nThe current db schema migration only creates/sets it as a varchar(255), but it needs to be bigger.\nI manually solved the problem by running alter table link_provider_intents modify metadata longtext; on the BOSH database.\nShould probably be added to the db migrations like this:\nif [:mysql, :mysql2].include? adapter_scheme\n      set_column_type :link_provider_intents, :metadata, 'longtext'\nend\nPlease add to db schema migrations. \ud83d\udc4d . Thanks, works now! \ud83d\udc4d . Thanks, works now! \ud83d\udc4d . Hi @xtreme-andrew-su,\nNo, there are no changes in the deployment manifest between 265 and 266. In fact I can download an already deployed manifest (that was previously deployed with 265 or earlier) and try to redeploy it with 266 and it will fail with above mentioned error message.\nJust verified that once more with another deployment that contains an errand and hasn't been touched in a while:\n$ bosh -d docker-registry manifest > test.yml\n$ bosh -n -d docker-registry deploy test.yml --no-redact\n...\nTask 112701 | 17:33:58 | Preparing deployment: Preparing deployment (00:00:12)\n                      L Error: Can't generate network settings without an IP\nTask 112701 | 17:34:10 | Error: Can't generate network settings without an IP\n\ud83d\ude29 \nThere's not much to see in the manifest. There's simply an errand that consumes a BOSH-link from another instance-group in the same deployment. The deployment fails until I remove the errand itself or remove the consumes from it (but then the errand will fail because it needs to know about the other instance-group).\nNo, no changes done to cloud-config between 265 and 266, at least regarding networking. There might have been some small changes to some vm-types, but unrelated to and not affecting that particular deployments vm-types.\nAnyway, here's a distilled down example I was using for testing the problem:\n```\ninstance_groups:\n- name: server\n  instances: 3\n  networks:\n  - default:\n    - dns\n    - gateway\n    name: test\n  stemcell: default\n  vm_type: small\n  jobs:\n  - name: consul\n    release: consul\n    consumes:\n      consul:\n        deployment: consul-test\n        from: consul\n    provides:\n      consul:\n        as: consul\n    properties: { ... lots of stuff here ... }\n\nname: agent\n  instances: 1\n  lifecycle: errand\n  networks:\ndefault:\ndns\ngateway\nname: test\n  stemcell: default\n  vm_type: errand\n  jobs:\n\n\nname: smoke-tests\n    release: consul\n    consumes:\n      consul:\n        deployment: consul-test\n        from: consul\n    properties: {}\n```\n\n(note: that's our own internal custom Consul release, not the one used by Cloud Foundry. It has a job consul which provides and consumes itself to know about its peers, and a smoke-test errand that also wants to consume the consul job to find the server-nodes)\nAs soon as I remove the errand from the deployment manifest it will work on BOSH 266. Until then it fails. Works fine either way on BOSH 265 or earlier.. Hi @xtreme-andrew-su,\nNo, there are no changes in the deployment manifest between 265 and 266. In fact I can download an already deployed manifest (that was previously deployed with 265 or earlier) and try to redeploy it with 266 and it will fail with above mentioned error message.\nJust verified that once more with another deployment that contains an errand and hasn't been touched in a while:\n$ bosh -d docker-registry manifest > test.yml\n$ bosh -n -d docker-registry deploy test.yml --no-redact\n...\nTask 112701 | 17:33:58 | Preparing deployment: Preparing deployment (00:00:12)\n                      L Error: Can't generate network settings without an IP\nTask 112701 | 17:34:10 | Error: Can't generate network settings without an IP\n\ud83d\ude29 \nThere's not much to see in the manifest. There's simply an errand that consumes a BOSH-link from another instance-group in the same deployment. The deployment fails until I remove the errand itself or remove the consumes from it (but then the errand will fail because it needs to know about the other instance-group).\nNo, no changes done to cloud-config between 265 and 266, at least regarding networking. There might have been some small changes to some vm-types, but unrelated to and not affecting that particular deployments vm-types.\nAnyway, here's a distilled down example I was using for testing the problem:\n```\ninstance_groups:\n- name: server\n  instances: 3\n  networks:\n  - default:\n    - dns\n    - gateway\n    name: test\n  stemcell: default\n  vm_type: small\n  jobs:\n  - name: consul\n    release: consul\n    consumes:\n      consul:\n        deployment: consul-test\n        from: consul\n    provides:\n      consul:\n        as: consul\n    properties: { ... lots of stuff here ... }\n\nname: agent\n  instances: 1\n  lifecycle: errand\n  networks:\ndefault:\ndns\ngateway\nname: test\n  stemcell: default\n  vm_type: errand\n  jobs:\n\n\nname: smoke-tests\n    release: consul\n    consumes:\n      consul:\n        deployment: consul-test\n        from: consul\n    properties: {}\n```\n\n(note: that's our own internal custom Consul release, not the one used by Cloud Foundry. It has a job consul which provides and consumes itself to know about its peers, and a smoke-test errand that also wants to consume the consul job to find the server-nodes)\nAs soon as I remove the errand from the deployment manifest it will work on BOSH 266. Until then it fails. Works fine either way on BOSH 265 or earlier.. @xtreme-andrew-su \nTLDR: https://github.com/xtreme-andrew-su/test-release/pull/1\n\nI was looking at your test-release, and the first I thought about was adding an erb template to the consumer job that will actually use the link to get the providers IP address, thinking that that would cause it to misbehave:\ntemplates:\n  test.yml.erb: config/test.yml\n<%=\n  provider_link = link('provider')\n  provider_ips = provider_link.instances.map { |instance| instance.address }\n  config = { \n    IPs: provider_ips.map {|ip| \"http://#{ip}\" },\n  }\n  JSON.pretty_generate(config)\n%>\nBut to my surprise it still worked. The BOSH director 266.x.0 that I'm using that is giving me trouble with these other errands was able to deploy your test-release just fine.\nSo I've tried again with one of my own deployments, but this time I was thinking along the lines that maybe the reason for it working is that it was a completely new deployment, not updating an existing one.\nI downloaded an existing manifest, modified the deployment name, and tried deploying it:\n$ bosh -d docker-registry manifest > docker-2.yml\n$ vi docker-2.yml # modifying name to docker-2, nothing else..\n$ bosh -d docker-2 deploy docker-2.yml\n...\nTask 112790 | 08:24:14 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112790 | 08:24:25 | Error: Can't generate network settings without an IP\nSame error message.\nThis is weird. So I added the same consul job that's inside my docker-registry's errand to your test-release deployment, to consume the consul link. Then deployed it again:\n```\nUsing deployment 'test_dep'\nreleases:\n+ - name: consul\n+   version: 2.0.9\ninstance_groups:\n  - name: provider_ig\n    jobs:\n    - name: provider\n+     release: test\n  - name: consumer_ig\n    jobs:\n    - name: consumer\n+     release: test\n+   - consumes:\n+       consul:\n+         deployment: consul\n+         from: consul\n+     name: consul\n+     properties: {}\n+     release: consul\nTask 112793\nTask 112793 | 08:28:34 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112793 | 08:28:45 | Error: Can't generate network settings without an IP\n```\nNow I'm thinking that it might be that the main consul deployment providing the link is somehow broken. So I redeploy that one hoping it would fix the issue:\n$ bosh -d consul manifest > consul.yml\n$ bosh -d consul deploy consul.yml\n...\nTask 112794 done\nTrying again the test_dep:\n```\n$ bosh -n -d test_dep deploy manifests/manifest.yml \nUsing deployment 'test_dep'\nreleases:\n+ - name: consul\n+   version: 2.0.9\ninstance_groups:\n  - name: provider_ig\n    jobs:\n    - name: provider\n+     release: test\n  - name: consumer_ig\n    jobs:\n    - name: consumer\n+     release: test\n+   - consumes:\n+       consul:\n+         deployment: consul\n+         from: consul\n+     name: consul\n+     properties: {}\n+     release: consul\nTask 112796\nTask 112796 | 08:45:05 | Preparing deployment: Preparing deployment (00:00:12)\n                      L Error: Can't generate network settings without an IP\nTask 112796 | 08:45:17 | Error: Can't generate network settings without an IP\nTask 112796 Started  Sat Jun 16 08:45:05 UTC 2018\nTask 112796 Finished Sat Jun 16 08:45:17 UTC 2018\nTask 112796 Duration 00:00:12\nTask 112796 error\nUpdating deployment:\n  Expected task '112796' to succeed but state is 'error'\nExit code 1\n```\nStill no luck! :(\nTried some more things, like remote the lifecycle: errand from the consumer job, then the deployment will work.\nOr moving the consul job from the consumer errand to the provider job, then the deployment will also work.\nSo it boils down to the consul job from our consul release containing something that makes it fail when used in an errand, starting with BOSH directo 266.x.0 series (It still works completely fine on BOSH 265.x or earlier).\nAfter some more digging and comparing the differences between your consumer job and our consul job I finally found the cause.\nThe job has a consumes and provides of the same link at the same time, with the consumes being marked as optional.\nThis used to perfectly work on earlier BOSH director versions.\nI've made a PR for your test-release (https://github.com/xtreme-andrew-su/test-release/pull/1) where you can check it out. It contains the new job both and an example manifest manifest-both.yml.\nDeploying the manifest-both.yml works on BOSH 265 or earlier, tested it with this older BOSH here:\nName      test-bosh  \nUUID      f3abb886-deee-426f-956b-71431826d8ac  \nVersion   264.6.0 (00000000)  \nCPI       vsphere_cpi  \nFeatures  compiled_package_cache: enabled  \n          config_server: disabled  \n          dns: disabled  \n          snapshots: disabled\nWorked without a hitch.\n```\nTask 726 | 09:21:16 | Preparing deployment: Preparing deployment (00:00:02)\nTask 726 | 09:21:18 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 726 | 09:21:18 | Creating missing vms: provider_ig/cdf8cc86-7190-44f9-9f81-039640c274ae (0) (00:01:03)\nTask 726 | 09:22:22 | Updating instance provider_ig: provider_ig/cdf8cc86-7190-44f9-9f81-039640c274ae (0) (canary) (00:00:28)\nTask 726 Started  Sat Jun 16 09:21:16 UTC 2018\nTask 726 Finished Sat Jun 16 09:22:50 UTC 2018\nTask 726 Duration 00:01:34\nTask 726 done\nSucceeded\n```\nOn a 266.x BOSH it fails with Error: Can't generate network settings without an IP:\n```\nTask 112816 | 09:26:34 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112816 | 09:26:45 | Error: Can't generate network settings without an IP\nTask 112816 Started  Sat Jun 16 09:26:34 UTC 2018\nTask 112816 Finished Sat Jun 16 09:26:45 UTC 2018\nTask 112816 Duration 00:00:11\nTask 112816 error\nUpdating deployment:\n  Expected task '112816' to succeed but state is 'error'\nExit code 1\n```\nSame release, same manifest... @xtreme-andrew-su \nTLDR: https://github.com/xtreme-andrew-su/test-release/pull/1\n\nI was looking at your test-release, and the first I thought about was adding an erb template to the consumer job that will actually use the link to get the providers IP address, thinking that that would cause it to misbehave:\ntemplates:\n  test.yml.erb: config/test.yml\n<%=\n  provider_link = link('provider')\n  provider_ips = provider_link.instances.map { |instance| instance.address }\n  config = { \n    IPs: provider_ips.map {|ip| \"http://#{ip}\" },\n  }\n  JSON.pretty_generate(config)\n%>\nBut to my surprise it still worked. The BOSH director 266.x.0 that I'm using that is giving me trouble with these other errands was able to deploy your test-release just fine.\nSo I've tried again with one of my own deployments, but this time I was thinking along the lines that maybe the reason for it working is that it was a completely new deployment, not updating an existing one.\nI downloaded an existing manifest, modified the deployment name, and tried deploying it:\n$ bosh -d docker-registry manifest > docker-2.yml\n$ vi docker-2.yml # modifying name to docker-2, nothing else..\n$ bosh -d docker-2 deploy docker-2.yml\n...\nTask 112790 | 08:24:14 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112790 | 08:24:25 | Error: Can't generate network settings without an IP\nSame error message.\nThis is weird. So I added the same consul job that's inside my docker-registry's errand to your test-release deployment, to consume the consul link. Then deployed it again:\n```\nUsing deployment 'test_dep'\nreleases:\n+ - name: consul\n+   version: 2.0.9\ninstance_groups:\n  - name: provider_ig\n    jobs:\n    - name: provider\n+     release: test\n  - name: consumer_ig\n    jobs:\n    - name: consumer\n+     release: test\n+   - consumes:\n+       consul:\n+         deployment: consul\n+         from: consul\n+     name: consul\n+     properties: {}\n+     release: consul\nTask 112793\nTask 112793 | 08:28:34 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112793 | 08:28:45 | Error: Can't generate network settings without an IP\n```\nNow I'm thinking that it might be that the main consul deployment providing the link is somehow broken. So I redeploy that one hoping it would fix the issue:\n$ bosh -d consul manifest > consul.yml\n$ bosh -d consul deploy consul.yml\n...\nTask 112794 done\nTrying again the test_dep:\n```\n$ bosh -n -d test_dep deploy manifests/manifest.yml \nUsing deployment 'test_dep'\nreleases:\n+ - name: consul\n+   version: 2.0.9\ninstance_groups:\n  - name: provider_ig\n    jobs:\n    - name: provider\n+     release: test\n  - name: consumer_ig\n    jobs:\n    - name: consumer\n+     release: test\n+   - consumes:\n+       consul:\n+         deployment: consul\n+         from: consul\n+     name: consul\n+     properties: {}\n+     release: consul\nTask 112796\nTask 112796 | 08:45:05 | Preparing deployment: Preparing deployment (00:00:12)\n                      L Error: Can't generate network settings without an IP\nTask 112796 | 08:45:17 | Error: Can't generate network settings without an IP\nTask 112796 Started  Sat Jun 16 08:45:05 UTC 2018\nTask 112796 Finished Sat Jun 16 08:45:17 UTC 2018\nTask 112796 Duration 00:00:12\nTask 112796 error\nUpdating deployment:\n  Expected task '112796' to succeed but state is 'error'\nExit code 1\n```\nStill no luck! :(\nTried some more things, like remote the lifecycle: errand from the consumer job, then the deployment will work.\nOr moving the consul job from the consumer errand to the provider job, then the deployment will also work.\nSo it boils down to the consul job from our consul release containing something that makes it fail when used in an errand, starting with BOSH directo 266.x.0 series (It still works completely fine on BOSH 265.x or earlier).\nAfter some more digging and comparing the differences between your consumer job and our consul job I finally found the cause.\nThe job has a consumes and provides of the same link at the same time, with the consumes being marked as optional.\nThis used to perfectly work on earlier BOSH director versions.\nI've made a PR for your test-release (https://github.com/xtreme-andrew-su/test-release/pull/1) where you can check it out. It contains the new job both and an example manifest manifest-both.yml.\nDeploying the manifest-both.yml works on BOSH 265 or earlier, tested it with this older BOSH here:\nName      test-bosh  \nUUID      f3abb886-deee-426f-956b-71431826d8ac  \nVersion   264.6.0 (00000000)  \nCPI       vsphere_cpi  \nFeatures  compiled_package_cache: enabled  \n          config_server: disabled  \n          dns: disabled  \n          snapshots: disabled\nWorked without a hitch.\n```\nTask 726 | 09:21:16 | Preparing deployment: Preparing deployment (00:00:02)\nTask 726 | 09:21:18 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 726 | 09:21:18 | Creating missing vms: provider_ig/cdf8cc86-7190-44f9-9f81-039640c274ae (0) (00:01:03)\nTask 726 | 09:22:22 | Updating instance provider_ig: provider_ig/cdf8cc86-7190-44f9-9f81-039640c274ae (0) (canary) (00:00:28)\nTask 726 Started  Sat Jun 16 09:21:16 UTC 2018\nTask 726 Finished Sat Jun 16 09:22:50 UTC 2018\nTask 726 Duration 00:01:34\nTask 726 done\nSucceeded\n```\nOn a 266.x BOSH it fails with Error: Can't generate network settings without an IP:\n```\nTask 112816 | 09:26:34 | Preparing deployment: Preparing deployment (00:00:11)\n                      L Error: Can't generate network settings without an IP\nTask 112816 | 09:26:45 | Error: Can't generate network settings without an IP\nTask 112816 Started  Sat Jun 16 09:26:34 UTC 2018\nTask 112816 Finished Sat Jun 16 09:26:45 UTC 2018\nTask 112816 Duration 00:00:11\nTask 112816 error\nUpdating deployment:\n  Expected task '112816' to succeed but state is 'error'\nExit code 1\n```\nSame release, same manifest... @xtreme-andrew-su cool, thanks! \ud83d\ude03 . @xtreme-andrew-su cool, thanks! \ud83d\ude03 . ",
    "sigusro": "It didn't worked since it require vApps Optin enabled and a pool of IPs assgined to host. stemcell 2865+ seems to support now dynamic IPs but with this limitation. Meantime I've managed to reserve some fixed IPs and I'll play with them.\n. It didn't worked since it require vApps Optin enabled and a pool of IPs assgined to host. stemcell 2865+ seems to support now dynamic IPs but with this limitation. Meantime I've managed to reserve some fixed IPs and I'll play with them.\n. ",
    "bonzofenix": "@cppforlife I changed the commit. \nWhy you would not recommend using only the director class only?\nThanks \n. @cppforlife I changed the commit. \nWhy you would not recommend using only the director class only?\nThanks \n. We encounter the same thing. I think its because params['dns'] has not been initialized with an {}\n. We encounter the same thing. I think its because params['dns'] has not been initialized with an {}\n. @cppforlife here is the manifest that failed. It is currently working with the same template on version 217.\nhttps://gist.github.com/bonzofenix/9d0ad5273c1112e98a28\n. @cppforlife here is the manifest that failed. It is currently working with the same template on version 217.\nhttps://gist.github.com/bonzofenix/9d0ad5273c1112e98a28\n. Why not just do \n```\nparams['dns'] ||= {}\nparams['dns']['domain_name'] = domain_name\n```\n. ",
    "julweber": "Thanks for the reply.\nWe will prepare and create a pull request concerning this topic.\nRegards,\nJulian Weber\n\nanynines.com\n. The PR can be found here https://github.com/cloudfoundry/bosh/pull/785.\nCheers,\nJulian\n. The needed CLA is already done for anynines.com .\n. The needed CLA is already done for anynines.com .\n. Thanks for merging. \nCheers,\nJulian\n. ",
    "mrwangxc": "Sure. This option supports a list of ranges, thus gives us full control of multiple releases' network deployment.\n. ",
    "blade2005": "In addition to this building cf-release fails (see https://github.com/cloudfoundry/cf-release/issues/1061 ) I had to uninstall bosh from 1.9.3 and install it on ruby 2.x and the problem went away. I would say that due to this issue bosh cli isn't fully working on 1.9.x\nEdit:\nIn the notes from https://bosh.io/docs/bosh-cli.html it's mentioned that BOSH CLI requires Ruby 2+ however the Gemfile doesn't reflect that and thus can still be installed on 1.9.x\n. In addition to this building cf-release fails (see https://github.com/cloudfoundry/cf-release/issues/1061 ) I had to uninstall bosh from 1.9.3 and install it on ruby 2.x and the problem went away. I would say that due to this issue bosh cli isn't fully working on 1.9.x\nEdit:\nIn the notes from https://bosh.io/docs/bosh-cli.html it's mentioned that BOSH CLI requires Ruby 2+ however the Gemfile doesn't reflect that and thus can still be installed on 1.9.x\n. Hrm it should all match up.\nI redeployed with the passwords all being the same and it still failed with 401.\n```\nname: bosh\nreleases:\n- name: bosh\n  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=257.3\n  sha1: e4442afcc64123e11f2b33cc2be799a0b59207d0\n- name: bosh-aws-cpi\n  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-aws-cpi-release?v=57\n  sha1: cbc7ed758f4a41063e9aee881bfc164292664b84\nresource_pools:\n- name: vms\n  network: private\n  stemcell:\n    url: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3262.8\n    sha1: 48ca720f4c458cfbdbf83732d9349c9e8beee6b8\n  cloud_properties:\n    instance_type: m3.xlarge\n    ephemeral_disk: {size: 25_000, type: gp2}\n    availability_zone: us-west-2a\ndisk_pools:\n- name: disks\n  disk_size: 20_000\n  cloud_properties: {type: gp2}\nnetworks:\n- name: private\n  type: manual\n  subnets:\n  - range: 10.0.0.0/24\n    gateway: 10.0.0.1\n    dns: [10.0.0.2]\n    cloud_properties: {subnet: subnet-bfc8bac9}\n- name: public\n  type: vip\njobs:\n- name: bosh\n  instances: 1\ntemplates:\n  - {name: nats, release: bosh}\n  - {name: postgres, release: bosh}\n  - {name: blobstore, release: bosh}\n  - {name: director, release: bosh}\n  - {name: health_monitor, release: bosh}\n  - {name: registry, release: bosh}\n  - {name: aws_cpi, release: bosh-aws-cpi}\nresource_pool: vms\n  persistent_disk_pool: disks\nnetworks:\n  - name: private\n    static_ips: [10.0.0.6]\n    default: [dns, gateway]\n  - name: public\n    static_ips: [PUBLICIP]\nproperties:\n    nats:\n      address: 127.0.0.1\n      user: nats\n      password: PASSWORD__1\npostgres: &db\n  listen_address: 127.0.0.1\n  host: 127.0.0.1\n  user: postgres\n  password: PASSWORD__1\n  database: bosh\n  adapter: postgres\n\nregistry:\n  address: 10.0.0.6\n  host: 10.0.0.6\n  db: *db\n  http: {user: admin, password: PASSWORD__1, port: 25777}\n  username: admin\n  password: admin\n  port: 25777\n\nblobstore:\n  address: 10.0.0.6\n  port: 25250\n  provider: dav\n  director: {user: director, password: PASSWORD__1}\n  agent: {user: agent, password: PASSWORD__1}\n\ndirector:\n  address: 127.0.0.1\n  name: my-bosh\n  db: *db\n  cpi_job: aws_cpi\n  max_threads: 10\n  user_management:\n    provider: local\n    local:\n      users:\n      - {name: admin, password: PASSWORD__1}\n      - {name: hm, password: PASSWORD__1}\n\nhm:\n  director_account: {user: hm, password: PASSWORD__1}\n  resurrector_enabled: true\n\naws: &aws\n  access_key_id: XXXXXXXXXXXXXXXXXXXX\n  secret_access_key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n  default_key_name: bosh\n  default_security_groups: [bosh]\n  region: us-west-2\n\nagent: {mbus: \"nats://nats:nats-password@10.0.0.6:4222\"}\n\nntp: &ntp [0.pool.ntp.org, 1.pool.ntp.org]\n\ncloud_provider:\n  template: {name: aws_cpi, release: bosh-aws-cpi}\nssh_tunnel:\n    host: PUBLICIP\n    port: 22\n    user: vcap\n    private_key: ./bosh.pem\nmbus: \"https://mbus:mbus-password@PUBLICIP:6868\"\nproperties:\n    aws: aws\n    agent: {mbus: \"https://mbus:mbus-password@0.0.0.0:6868\"}\n    blobstore: {provider: local, path: /var/vcap/micro_bosh/data/cache}\n    ntp: ntp\n```\nAt one point I dumped the config from the bosh-registry daemon and tried to find registry passwords in the bosh-director config that would tell it how to connect to bosh-registry.\nI followed instructions from here for the deploy of the bosh/0 https://bosh.io/docs/init-aws.html\nShould my individual deploy of services include the registry password or not?\nThis deploy is taking place in AWS. I looked at the auth for registry daemon and the AWS secret id/key match. \nHow can I check what information the bosh-director is using to authenticate to the bosh-registry?\n. To make sure I'm clear on this. So under the aws section\nproperties:\n  aws:\n    registry\n      password: PASSWORD__1\nor is the correct update this\nproperties:\n  registry:\n    password: PASSWORD__1\n    http:\n      password: PASSWORD__1\nOr am I not quite understanding where I need to put registry.password?\n. Alright I see where I was missing it\njobs:\n- name: bosh\n  properties:\n    registry:\n      address: 10.0.0.6\n      host: 10.0.0.6\n      db: *db\n      http: {user: admin, password: PASSWORD__1, port: 25777}\n      username: admin\n      password: PASSWORD__1\n      port: 25777\nI had\njobs:\n- name: bosh\n  properties:\n    registry:\n      address: 10.0.0.6\n      host: 10.0.0.6\n      db: *db\n      http: {user: admin, password: PASSWORD__1, port: 25777}\n      username: admin\n      password: admin\n      port: 25777\nWhy is it that bosh requires me to set the same password twice for the same block of properties?\n. Well that story makes me fairly happy. Let's go ahead and close this as the issue has been discovered and it's in progress for a fix.\n. Bucket is set to region for oregon. I updated my final.yml file to reflect that\n```\nfinal_name: wordpress\nmin_cli_version: 1.0.3\nblobstore: \n  provider: s3\n  options:\n    region: us-west-2\n    endpoint: https://s3-us-west-2.amazonaws.com\n    bucket_name: pst-cf-wordpress\n```\nStill got the same error.\nEdited for spelling\n. Bucket is set to region for oregon. I updated my final.yml file to reflect that\n```\nfinal_name: wordpress\nmin_cli_version: 1.0.3\nblobstore: \n  provider: s3\n  options:\n    region: us-west-2\n    endpoint: https://s3-us-west-2.amazonaws.com\n    bucket_name: pst-cf-wordpress\n```\nStill got the same error.\nEdited for spelling\n. Thanks. I see what I did wrong.\n. Thanks. I see what I did wrong.\n. Alright for the time being I changed the compilation network to cf_public\ncompilation:\n  workers: 6\n  network: cf_public\nThis has corrected the issue.\nNow I also switched the cf_private subnet setting to autoassing public dns which should force in aws public routing. I will rerun my deploy with cf_private as the subnet once this one completes.\n. Alright for the time being I changed the compilation network to cf_public\ncompilation:\n  workers: 6\n  network: cf_public\nThis has corrected the issue.\nNow I also switched the cf_private subnet setting to autoassing public dns which should force in aws public routing. I will rerun my deploy with cf_private as the subnet once this one completes.\n. ",
    "guanglinlv": "hi @cppforlife ,\ni have checked my microbosh.yml,there is a resources.persistent_disk configed,but i cant find out persistent in micrbosh VM /var/vcap/bosh/settings.json.\nmicrobosh.yml:\n``` yml\nname: microbosh\nlogging:\n  # If needed increase the default logging level to trace REST traffic with IaaS providers. Default is info\n  level: debug\n  # Default location is /bosh_micro_deploy.log\n  # file :\nnetwork:\n  type: manual \n  vip: 10.63.50.82  #float IP you apply for micro bosh\n  ip: 192.168.2.3\n  cloud_properties:\n    net_id: bd60312e-246b-40eb-b3b9-b91d27a921de  #net_id of your VM\nresources:\n  persistent_disk: 20480 #it is very important,the volume size  connect to VM \n  cloud_properties:\n    instance_type: m1.medium\n    availability_zone: szxts01-az\ncloud:\n  plugin: openstack\n  properties:\n    openstack:\n      auth_url: http://10.63.49.40:5000/v2.0   #change it with your openstack ip\n      username: lv #openstack username\n      api_key: lv #openstack password\n      tenant: CloudFoundry01     #the project of openstack\nregion: RegionOne\n  default_security_groups: [\"default\", \"default\"]\n  default_key_name: lv_microbosh\n  private_key: /root/.ssh/lv_microbosh.pem   #change it with your keypair route.\n  ignore_server_availability_zone: true\n\napply_spec:\n  properties:\n    director:\n      max_threads: 3\n    hm:\n      resurrector_enabled: true\n    ntp:\n      - 0.north-america.pool.ntp.org\n      - 1.north-america.pool.ntp.org\n```\nmicbosh VM /var/vcap/bosh/agent.json:\n``` json\n{\n  \"Platform\": {\n    \"Linux\": {\n      \"DevicePathResolutionType\": \"virtio\"\n    }\n  },\n  \"Infrastructure\": {\n    \"NetworkingType\": \"dhcp\",\n    \"Settings\": {\n      \"Sources\": [\n        {\n          \"Type\": \"ConfigDrive\",\n          \"DiskPaths\": [\n            \"/dev/disk/by-label/CONFIG-2\",\n            \"/dev/disk/by-label/config-2\"\n          ],\n          \"MetaDataPath\": \"ec2/latest/meta-data.json\",\n          \"UserDataPath\": \"ec2/latest/user-data\"\n        },\n        {\n          \"Type\": \"HTTP\",\n          \"URI\": \"http://169.254.169.254\"\n        }\n      ],\n  \"UseServerName\": true,\n  \"UseRegistry\": true\n}\n\n}\n}\n```\nmicrobosh VM /var/vcap/bosh/settings.json:\njson\n{\"agent_id\":\"bm-35e0be3a-bbe1-4848-9046-c7885b600e4f\",\"blobstore\":{\"provider\":\"local\",\"options\":{\"blobstore_path\":\"/var/vcap/micro_bosh/data/cache\"}},\"disks\":{\"system\":\"/dev/sda\",\"ephemeral\":\"\",\"persistent\":{}},\"env\":{\"bosh\":{\"password\":\"\"}},\"networks\":{\"bosh\":{\"type\":\"manual\",\"ip\":\"192.168.2.3\",\"netmask\":\"\",\"gateway\":\"\",\"default\":[\"dns\",\"gateway\"],\"dns\":null,\"mac\":\"\"},\"vip\":{\"type\":\"vip\",\"ip\":\"10.63.50.82\",\"netmask\":\"\",\"gateway\":\"\",\"default\":null,\"dns\":null,\"mac\":\"\"}},\"ntp\":[],\"mbus\":\"https://vcap:b00tstrap@0.0.0.0:6868\",\"vm\":{\"name\":\"vm-c02d1580-eb67-4697-b083-40f0a2f3ac66\"}}\n. hi @cppforlife ,\ni have checked my microbosh.yml,there is a resources.persistent_disk configed,but i cant find out persistent in micrbosh VM /var/vcap/bosh/settings.json.\nmicrobosh.yml:\n``` yml\nname: microbosh\nlogging:\n  # If needed increase the default logging level to trace REST traffic with IaaS providers. Default is info\n  level: debug\n  # Default location is /bosh_micro_deploy.log\n  # file :\nnetwork:\n  type: manual \n  vip: 10.63.50.82  #float IP you apply for micro bosh\n  ip: 192.168.2.3\n  cloud_properties:\n    net_id: bd60312e-246b-40eb-b3b9-b91d27a921de  #net_id of your VM\nresources:\n  persistent_disk: 20480 #it is very important,the volume size  connect to VM \n  cloud_properties:\n    instance_type: m1.medium\n    availability_zone: szxts01-az\ncloud:\n  plugin: openstack\n  properties:\n    openstack:\n      auth_url: http://10.63.49.40:5000/v2.0   #change it with your openstack ip\n      username: lv #openstack username\n      api_key: lv #openstack password\n      tenant: CloudFoundry01     #the project of openstack\nregion: RegionOne\n  default_security_groups: [\"default\", \"default\"]\n  default_key_name: lv_microbosh\n  private_key: /root/.ssh/lv_microbosh.pem   #change it with your keypair route.\n  ignore_server_availability_zone: true\n\napply_spec:\n  properties:\n    director:\n      max_threads: 3\n    hm:\n      resurrector_enabled: true\n    ntp:\n      - 0.north-america.pool.ntp.org\n      - 1.north-america.pool.ntp.org\n```\nmicbosh VM /var/vcap/bosh/agent.json:\n``` json\n{\n  \"Platform\": {\n    \"Linux\": {\n      \"DevicePathResolutionType\": \"virtio\"\n    }\n  },\n  \"Infrastructure\": {\n    \"NetworkingType\": \"dhcp\",\n    \"Settings\": {\n      \"Sources\": [\n        {\n          \"Type\": \"ConfigDrive\",\n          \"DiskPaths\": [\n            \"/dev/disk/by-label/CONFIG-2\",\n            \"/dev/disk/by-label/config-2\"\n          ],\n          \"MetaDataPath\": \"ec2/latest/meta-data.json\",\n          \"UserDataPath\": \"ec2/latest/user-data\"\n        },\n        {\n          \"Type\": \"HTTP\",\n          \"URI\": \"http://169.254.169.254\"\n        }\n      ],\n  \"UseServerName\": true,\n  \"UseRegistry\": true\n}\n\n}\n}\n```\nmicrobosh VM /var/vcap/bosh/settings.json:\njson\n{\"agent_id\":\"bm-35e0be3a-bbe1-4848-9046-c7885b600e4f\",\"blobstore\":{\"provider\":\"local\",\"options\":{\"blobstore_path\":\"/var/vcap/micro_bosh/data/cache\"}},\"disks\":{\"system\":\"/dev/sda\",\"ephemeral\":\"\",\"persistent\":{}},\"env\":{\"bosh\":{\"password\":\"\"}},\"networks\":{\"bosh\":{\"type\":\"manual\",\"ip\":\"192.168.2.3\",\"netmask\":\"\",\"gateway\":\"\",\"default\":[\"dns\",\"gateway\"],\"dns\":null,\"mac\":\"\"},\"vip\":{\"type\":\"vip\",\"ip\":\"10.63.50.82\",\"netmask\":\"\",\"gateway\":\"\",\"default\":null,\"dns\":null,\"mac\":\"\"}},\"ntp\":[],\"mbus\":\"https://vcap:b00tstrap@0.0.0.0:6868\",\"vm\":{\"name\":\"vm-c02d1580-eb67-4697-b083-40f0a2f3ac66\"}}\n. hi @cppforlife \nthanks for your answer,i have config openstack falvor ephemeral disk size,and deploy microbosh successfully.\nthanks.\n. hi @cppforlife ,\nI had avoid the problem by modifying stemcell, add /usr/local/lib into /etc/ld.so.conf.\nexcept for this problem, i also got some other likely problems.\nby the way,where can i get the centos 7 stemcell for openstack?\n. hi @cppforlife i have try the official centos 7 stemcell,the CF compiling problem is fixed.but also get some other problems;\nQ1, the consul_agent execute failed because of /etc/resolvconf/resolv.conf.d/head isnot exist in centos.\nQ2, the /etc/resolv.conf is wrong int the VM instance of CF, it only include nameserver <bosh-powerdns-ip>, i also need search openstacklocal and openstack-sub-net-dhcp-ip\nQ3, the CF deploying with centos 7 stemcell cant work correctly,i can not start any application at that.\n. hi @cppforlife i have try the official centos 7 stemcell,the CF compiling problem is fixed.but also get some other problems;\nQ1, the consul_agent execute failed because of /etc/resolvconf/resolv.conf.d/head isnot exist in centos.\nQ2, the /etc/resolv.conf is wrong int the VM instance of CF, it only include nameserver <bosh-powerdns-ip>, i also need search openstacklocal and openstack-sub-net-dhcp-ip\nQ3, the CF deploying with centos 7 stemcell cant work correctly,i can not start any application at that.\n. @cppforlife,\n\n\nQ3, the CF deploying with centos 7 stemcell cant work correctly,i can not start any application at that.\n\ncf-release does not fully support centos 7. it only works on ubuntu trusty stemcells. in future cf may work on centos but the timeline is not determined.\n\nactually, i got a crash problem on wshd when it try to create a container with the official centos 7 stemcell from bosh.io. wshd crash with centos-7 issue\n. @cppforlife,\n\n\nQ3, the CF deploying with centos 7 stemcell cant work correctly,i can not start any application at that.\n\ncf-release does not fully support centos 7. it only works on ubuntu trusty stemcells. in future cf may work on centos but the timeline is not determined.\n\nactually, i got a crash problem on wshd when it try to create a container with the official centos 7 stemcell from bosh.io. wshd crash with centos-7 issue\n. I update the bosh to latest and try to build stemcell again, i got some new errors :\n``` sh\nFailures:\n1) CentOS OS image behaves like an OS image installed by rsyslog File \"/etc/init/rsyslog.conf\" should contain \"/usr/local/sbin/rsyslogd\"\n     Failure/Error: it { should contain('/usr/local/sbin/rsyslogd') }\n       grep -q -- /usr/local/sbin/rsyslogd /etc/init/rsyslog.conf || grep -qF -- /usr/local/sbin/rsyslogd /etc/init/rsyslog.conf\n       grep: /etc/init/rsyslog.conf: No such file or directory\n   expected File \"/etc/init/rsyslog.conf\" to contain \"/usr/local/sbin/rsyslogd\"\n Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n # ./spec/support/os_image_shared_examples.rb:31:in `block (4 levels) in <top (required)>'\n\n2) CentOS OS image behaves like an OS image installed by rsyslog Command \"rsyslogd -v\" should return stdout /7.4.6/\n     Failure/Error: it { should return_stdout /7.4.6/ }\n       rsyslogd -v\n       rsyslogd 7.4.7, compiled with:\n    FEATURE_REGEXP:             Yes\n    FEATURE_LARGEFILE:          No\n    GSSAPI Kerberos 5 support:      Yes\n    FEATURE_DEBUG (debug build, slow code): No\n    32bit Atomic operations supported:  Yes\n    64bit Atomic operations supported:  Yes\n    Runtime Instrumentation (slow code):    No\n    uuid support:               Yes\nSee http://www.rsyslog.com for more information.\n   expected Command \"rsyslogd -v\" to return stdout /7\\.4\\.6/\n Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n # ./spec/support/os_image_shared_examples.rb:53:in `block (4 levels) in <top (required)>'\n\n3) CentOS OS image behaves like an OS image installed by rsyslog Service \"rsyslog\" should be enabled\n     Failure/Error: it { should be_enabled.with_level(2) }\n       systemctl --plain list-dependencies runlevel2.target | grep '^rsyslog.service$'\n       expected Service \"rsyslog\" to be enabled\n     Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n     # ./spec/support/os_image_shared_examples.rb:63:in `block (4 levels) in '\n4) CentOS OS image behaves like an OS image installed by rsyslog Service \"rsyslog\" should be enabled\n     Failure/Error: it { should be_enabled.with_level(3) }\n       systemctl --plain list-dependencies runlevel3.target | grep '^rsyslog.service$'\n       expected Service \"rsyslog\" to be enabled\n     Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n     # ./spec/support/os_image_shared_examples.rb:64:in `block (4 levels) in '\n5) CentOS OS image behaves like an OS image installed by rsyslog Service \"rsyslog\" should be enabled\n     Failure/Error: it { should be_enabled.with_level(4) }\n       systemctl --plain list-dependencies runlevel4.target | grep '^rsyslog.service$'\n       expected Service \"rsyslog\" to be enabled\n     Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n     # ./spec/support/os_image_shared_examples.rb:65:in `block (4 levels) in '\n6) CentOS OS image behaves like an OS image installed by rsyslog Service \"rsyslog\" should be enabled\n     Failure/Error: it { should be_enabled.with_level(5) }\n       systemctl --plain list-dependencies runlevel5.target | grep '^rsyslog.service$'\n       expected Service \"rsyslog\" to be enabled\n     Shared Example Group: \"an OS image\" called from ./spec/os_image/centos_spec.rb:4\n     # ./spec/support/os_image_shared_examples.rb:66:in `block (4 levels) in '\n7) CentOS OS image installed by base_centos File \"/etc/sysconfig/i18n\" should be file\n     Failure/Error: it { should be_file }\n       test -f /etc/sysconfig/i18n\n       expected file? to return true, got false\n     # ./spec/os_image/centos_spec.rb:34:in `block (4 levels) in '\n8) CentOS OS image installed by base_centos File \"/etc/sysconfig/i18n\" should contain \"en_US.UTF-8\"\n     Failure/Error: it { should contain 'en_US.UTF-8' }\n       grep -q -- en_US.UTF-8 /etc/sysconfig/i18n || grep -qF -- en_US.UTF-8 /etc/sysconfig/i18n\n       grep: /etc/sysconfig/i18n: No such file or directory\n   expected File \"/etc/sysconfig/i18n\" to contain \"en_US.UTF-8\"\n # ./spec/os_image/centos_spec.rb:35:in `block (4 levels) in <top (required)>'\n\n9) CentOS OS image installed by base_yum Package \"upstart\" should be installed\n     Failure/Error: it { should be_installed }\n       rpm -q upstart\n       warning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\npackage upstart is not installed\n   expected Package \"upstart\" to be installed\n # ./spec/os_image/centos_spec.rb:78:in `block (5 levels) in <top (required)>'\n\n10) CentOS OS image installed by base_yum Package \"nc\" should be installed\n     Failure/Error: it { should be_installed }\n       rpm -q nc\n       warning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\npackage nc is not installed\n   expected Package \"nc\" to be installed\n # ./spec/os_image/centos_spec.rb:78:in `block (5 levels) in <top (required)>'\n\n11) CentOS OS image installed by system_grub Package \"grub\" should be installed\n     Failure/Error: it { should be_installed }\n       rpm -q grub\n       warning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\nwarning: Failed to read auxiliary vector, /proc not mounted?\npackage grub is not installed\n   expected Package \"grub\" to be installed\n # ./spec/os_image/centos_spec.rb:85:in `block (4 levels) in <top (required)>'\n\n12) CentOS OS image File \"/boot/grub/e2fs_stage1_5\" should be file\n     Failure/Error: it { should be_file }\n       test -f /boot/grub/e2fs_stage1_5\n       expected file? to return true, got false\n     # ./spec/os_image/centos_spec.rb:91:in `block (4 levels) in '\n13) CentOS OS image File \"/boot/grub/stage1\" should be file\n     Failure/Error: it { should be_file }\n       test -f /boot/grub/stage1\n       expected file? to return true, got false\n     # ./spec/os_image/centos_spec.rb:91:in `block (4 levels) in '\n14) CentOS OS image File \"/boot/grub/stage2\" should be file\n     Failure/Error: it { should be_file }\n       test -f /boot/grub/stage2\n       expected file? to return true, got false\n     # ./spec/os_image/centos_spec.rb:91:in `block (4 levels) in '\nFinished in 29.81 seconds\n71 examples, 14 failures\n```\n. @cppforlife \nit's too large, the part of debug log is given here:\nbosh-155 is successful with the same cf(release and deployment file) and bosh(deployment file).\nthis failure is happened once each time when update jobs for each VM instance of CF,bosh deploy again after once failure will be successful.\n`` log\n{\"method\":\"get_task\",\"arguments\":[\"8f11a8df-c6fd-4e58-4afa-240954c82352\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4 {\"value\":\"prepared\"}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"drain\",\"arguments\":[\"update\",{\"deployment\":\"cf-warden\",\"job\":{\"name\":\"ha_proxy_z1\",\"templates\":[{\"name\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},{\"name\":\"metron_agent\",\"version\":\"8ad734ba7952b6184ac127a45608e64ebb1c4611\",\"sha1\":\"6f229f6f524dafc9ee4d46188c8cd7af1bcb8098\",\"blobstore_id\":\"3ccf6ca4-ff9d-4b4a-8b91-0d30f0dd11c8\"}],\"template\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},\"index\":0,\"networks\":{\"floating\":{\"type\":\"vip\",\"ip\":\"9.91.39.29\",\"cloud_properties\":{},\"dns_record_name\":\"0.ha-proxy-z1.floating.cf-warden.bosh\"},\"cf1\":{\"ip\":\"10.10.10.140\",\"netmask\":\"255.255.255.0\",\"cloud_properties\":{\"net_id\":\"67a8ddc7-4d5f-432f-a154-660df2e8e69c\",\"security_groups\":[\"default\"]},\"default\":[\"dns\",\"gateway\"],\"dns\":[\"10.10.10.3\",\"10.10.10.64\"],\"gateway\":\"10.10.10.1\",\"dns_record_name\":\"0.ha-proxy-z1.cf1.cf-warden.bosh\"}},\"resource_pool\":{\"name\":\"router_z1\",\"cloud_properties\":{\"instance_type\":\"m1.medium\"},\"stemcell\":{\"name\":\"bosh-openstack-kvm-centos-7-go_agent-raw-m\",\"version\":\"2962\"}},\"packages\":{\"haproxy\":{\"name\":\"haproxy\",\"version\":\"630ad6d6e1d3cab4547ce104f3019b483f354613.1\",\"sha1\":\"33522a7aef69d54845e4dbb2e3706a6e19f1b87d\",\"blobstore_id\":\"148473d8-c738-4dce-5dc2-1e064a502354\"},\"common\":{\"name\":\"common\",\"version\":\"99c756b71550530632e393f5189220f170a69647.1\",\"sha1\":\"f4e3d8668a5cd7bbce48a5a600936e8fd1813e2b\",\"blobstore_id\":\"3f8c19b4-aaaf-4ef0-5228-688538998b59\"},\"metron_agent\":{\"name\":\"metron_agent\",\"version\":\"27c6122c3dcabaeac758bae7cb258910a5ac1e42.1\",\"sha1\":\"d50ec240046610a5a6e3493258cc269778baa82d\",\"blobstore_id\":\"1e91a9bd-8bb7-4f08-68ad-186fb62afef1\"}},\"configuration_hash\":\"5ddbb2da0fc22bb3790622806d0b3ebce5e44cfc\",\"properties\":{\"ha_proxy\":{\"ssl_pem\":\"-----BEGIN CERTIFICATE-----\\nMIIDETCCAfmgAwIBAgIJANZuykf1uh3LMA0GCSqGSIb3DQEBBQUAMB8xHTAbBgNV\\nBAMMFCouMTAuMjQ0LjAuMzQueGlwLmlvMB4XDTE0MTIyNDIzMTkxM1oXDTI0MTIy\\nMTIzMTkxM1owHzEdMBsGA1UEAwwUKi4xMC4yNDQuMC4zNC54aXAuaW8wggEiMA0G\\nCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCjq73Fgwfj2UT0/+wR9kVVsGAguMj\\npoA0opLCgE0yHStAhSvqq7YpO39dH3vBMWXyr2xIfDyaeZyhV86jWu/ZKswGjNGI\\nZKv/yUINe1bqukOBqd+SHVvkVhxSLJuD1MR83JQMONRjOPJp661/ABpVhnrNfBiA\\nAA6aaFv4/KbyGY/E1FHoUXqEdh4WxaJdfX6SbgG05ArWxhSD7PNj4CYvJWGCdvqP\\nKBsvWFDrkxBHn5h1JIDfZJB8FKP6vaHBr7MU4pIHM+qaZ1Y+8ja0wcgkHn4YHcp6\\nIOhqpck7LaH5Qq2ydYFNTcG4fTbG0jXqcit2WSUxRkXzWnrgo2E0SiHBAgMBAAGj\\nUDBOMB0GA1UdDgQWBBSpCtEDtEvMwaZzXN6Lvk5U7Eyn2zAfBgNVHSMEGDAWgBSp\\nCtEDtEvMwaZzXN6Lvk5U7Eyn2zAMBgNVHRMEBTADAQH/MA0GCSqGSIb3DQEBBQUA\\nA4IBAQB1YIHmw3gPiMn8WDR4yVxDvSVgFHY6ZE1iZb17vVs4N2/mhQZXWJ2nZV02\\ngoAivtgxHOj39sK5OBWsGvrQo5H8dt1t4XmbwB1C6xRerGc25dhDRq42RqhCN0RJ\\nzzjd9b8YSiwtAaZlW36l2jVDLfRapb00tWToF9qYrDrmKy2sekS7g2hbiRStcue/\\nbpT4X/CHxb/lUbpL4m8BpDbkGiOJgl+SEHRx5tZ0Kob/RDQRCcN3p+71FRbDIEBj\\n+8sJl/yUUUPwQ6PNYx6cjtlICWJ1G0l0hRa141VXPqSNCmxYS4dp/8ifPCSoLc+k\\n9TXFkuGl+86CPTyyMJxyMhEcAGZT\\n-----END CERTIFICATE-----\\n-----BEGIN RSA PRIVATE KEY-----\\nMIIEpAIBAAKCAQEAwo6u9xYMH49lE9P/sEfZFVbBgILjI6aANKKSwoBNMh0rQIUr\\n6qu2KTt/XR97wTFl8q9sSHw8mnmcoVfOo1rv2SrMBozRiGSr/8lCDXtW6rpDganf\\nkh1b5FYcUiybg9TEfNyUDDjUYzjyaeutfwAaVYZ6zXwYgAAOmmhb+Pym8hmPxNRR\\n6FF6hHYeFsWiXX1+km4BtOQK1sYUg+zzY+AmLyVhgnb6jygbL1hQ65MQR5+YdSSA\\n32SQfBSj+r2hwa+zFOKSBzPqmmdWPvI2tMHIJB5+GB3KeiDoaqXJOy2h+UKtsnWB\\nTU3BuH02xtI16nIrdlklMUZF81p64KNhNEohwQIDAQABAoIBAC1U3YOIyY5Y9O4n\\nyT2jn/sO2cs9s/rMgrbA4n0bM+FnVnqUDOWC2NDGoihqe4VKIzzmjs5c1CoSB+K3\\n+NerCpOJGzyzdubWvhS9KfzGLjxG5g/CKut6l7yeK78h0aJn4thM9NncK/BqhmET\\nnrsmpPwkd1yFe5fna3+irTtYcvWZgxp8DK4JxIRB0QpJvEwbs9fUFE1E0DVw/uBV\\nCytTIrik2O+n1m8S5xzsFXHXDjXT/TVNi3jtN12Oaj1avYZRP2q45RmcODhtHQIy\\n4o0vqjyAmZb3jOcYysyXVwfyNreIH3Qn0/waflPkyaljMpa1OHVOCJFjh2Xl+aIc\\ndQsME3kCgYEA4aXU2xb9SHew84/3ow6ZmBeYLm+7B8GUjDTe7HqLldtTUhlrJSdB\\nSHZHZ/BXEQ126FnbfZ0IISkBjqVCQBw8MgjZAlaEIInDA1DRTLuOfrcyUD6PeSLi\\ngVZHNYbR9MDnmJ3/So5HXiRy2rBWtLKwPlciqbwY+l3xYr2kP/QdCisCgYEA3LpB\\nTBDTqp9j8QVvB/YjCakS+z+kgtO5HyMZO/uh6o9PRHrFQHwZA5y/MRrx9lxJ9zOd\\nfqysKfA7fXs4VzNnRSnOfBmuDHF0HhlIgAShX/RnB94p9xnsQaMae/o2nXcknQng\\n3qogvHWTo16GCJRE+YTmhA0QtvSlScJlTzHfqcMCgYAPRt/rWVoajufvBX85jeJ+\\nNpK6Chx6gPOirm2tSvqqUagJdekYIdk8o61f7xil8ehsALFohronLJSLaMrcdkzp\\nAkpW6y6U2V7XmaAh9szF7Xc9kY67H85//SxjBlauoGTNo1zGWm2ghQ01mxyzrSlb\\nfyC8pxx1zuhpy/cT0V4p8wKBgQDb+EJaq+pFf9L5v5CHPqRsXDKucR5hwt4aScA8\\nJumV+HvmovMw8Ht9PhjLty6rdg3AbY/nTe3FXcPrqYDcZj3kj2VYB7+MZwRxeoDm\\nE7c/CTIkhSMNPqhUQVeDdjg3dSTn25BeVu2I4yPfC7RHmHukru2La/ncWrLebvzH\\nj8x2QQKBgQDevjRDDTWbBkg8HdCRxxCvhfaHBntoSJdTHlr14Gce48NXGaXRJPQT\\n9dMsPsSHkFxEra7G2clGnhpe+pK9V+WTrD9Qnoc+tK808hX1YQ6mBlnR6w99jlaR\\nHVTi2pRhEhbWUkBv2kooXvD6ANb15PbPSF1FK7YyW1KHqcbm+lF22g==\\n-----END RSA PRIVATE KEY-----\\n\",\"disable_http\":false,\"ssl_ciphers\":\"ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-RC4-SHA:AES128:AES256:RC4-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK\"},\"request_timeout_in_seconds\":900,\"router\":{\"servers\":{\"z1\":[\"10.10.10.141\"],\"z2\":[]},\"port\":80},\"networks\":{\"apps\":\"cf1\"},\"syslog_daemon_config\":{\"address\":null,\"port\":null,\"transport\":\"tcp\"},\"metron_agent\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457,\"statsd_incoming_port\":8125,\"debug\":false,\"status\":{\"user\":\"\",\"password\":\"\",\"port\":0},\"zone\":\"z1\",\"etcd_query_interval_milliseconds\":5000,\"collector_registrar_interval_milliseconds\":60000},\"loggregator\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457},\"loggregator_endpoint\":{\"shared_secret\":\"loggregator-secret\"},\"nats\":{\"user\":\"nats\",\"password\":\"nats\",\"machines\":[\"10.10.10.142\"],\"port\":4222},\"etcd\":{\"machines\":[\"10.10.10.143\"],\"maxconcurrentrequests\":10}},\"dns_domain_name\":\"bosh\",\"persistent_disk\":0,\"template_hashes\":{\"haproxy\":\"66baf4bb051487714f5b6886a8fa27b645f783f2\",\"metron_agent\":\"aa6290cab96fd7e456be9c34299a42c1aa90a104\"},\"rendered_templates_archive\":{\"blobstore_id\":\"d7018513-3179-436d-85a3-b12f719f3528\",\"sha1\":\"0f48d34d5a0976dd906915472bbd8ffa3c9bb716\"}}],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258 {\"value\":0}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000893s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000891s) SELECT * FROM \"tasks\" WHERE \"id\" = 43\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"stop\",\"arguments\":[],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af {\"value\":{\"agent_task_id\":\"3d66d0dd-07dc-455b-64e5-29f8f7249525\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"3d66d0dd-07dc-455b-64e5-29f8f7249525\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e {\"value\":\"stopped\"}\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Snapshots are disabled; skipping\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping VM update\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping network re-configuration\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000803s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000555s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000515s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000464s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000735s) SELECT * FROM \"records\" WHERE ((\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'A') AND (\"content\" = '9.91.39.29')) LIMIT 1\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.floating.cf-warden.bosh to 9.91.39.29\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000345s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000472s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000347s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000942s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.floating.cf-warden.bosh', 'A', 300, '9.91.39.29', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001686s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000352s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000369s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000311s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000448s) SELECT * FROM \"domains\" WHERE ((\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000285s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000247s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000429s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('39.91.9.in-addr.arpa', 'NATIVE') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001353s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000301s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000409s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000339s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000490s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000416s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000266s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000497s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (6, '39.91.9.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001004s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000492s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000363s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000323s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000817s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (6, '39.91.9.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001443s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000370s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000291s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000262s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000408s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000441s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000789s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (6, '29.39.91.9.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.floating.cf-warden.bosh', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001202s) COMMIT\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.cf1.cf-warden.bosh to 10.10.10.140\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000457s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000428s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000680s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.cf1.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000586s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.cf1.cf-warden.bosh', 'A', 300, '10.10.10.140', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000957s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000244s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000297s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT * FROM \"domains\" WHERE ((\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000223s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000430s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('10.10.10.in-addr.arpa', 'NATIVE') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001081s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000357s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000336s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000324s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000488s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000361s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000509s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (7, '10.10.10.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001014s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000331s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000463s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000473s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000892s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (7, '10.10.10.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001340s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000479s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000447s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000382s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000606s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.cf1.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000801s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (7, '140.10.10.10.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.cf1.cf-warden.bosh', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001254s) COMMIT\nE, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] ERROR -- DirectorJobRunner: Error updating canary instance: #<Errno::ENOENT: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl>\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl - /var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Shutting down pool\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.005891s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.001478s) SELECT \"stemcells\".* FROM \"stemcells\" INNER JOIN \"deployments_stemcells\" ON ((\"deployments_stemcells\".\"stemcell_id\" = \"stemcells\".\"id\") AND (\"deployments_stemcells\".\"deployment_id\" = 3))\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:cf-warden\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:cf-warden\nI, [2015-05-05 11:48:23 #20891] [task:43]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"2f9236d9-aa9a-464f-be68-2173793a47d4\",\"severity\":3,\"title\":\"director - error during update deployment\",\"summary\":\"Error during update deployment for 'cf-warden' against Director '609afeed-cd42-48dd-a3a6-08e5bf044992': #<Errno::ENOENT: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl>\",\"created_at\":1430826503}\nE, [2015-05-05 11:48:23 #20891] [task:43] ERROR -- DirectorJobRunner: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000293s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000943s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2015-05-05 11:48:23.258668+0000', \"description\" = 'create deployment', \"result\" = 'No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl', \"output\" = '/var/vcap/store/director/tasks/43', \"checkpoint_time\" = '2015-05-05 11:48:20.863669+0000', \"type\" = 'update_deployment', \"username\" = 'admin' WHERE (\"id\" = 43)\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.002082s) COMMIT\nI, [2015-05-05 11:48:23 #20891] []  INFO -- DirectorJobRunner: Task took 1 minute 32.420035999999996 seconds to process.\nTask 43 error\n```\n. @cppforlife \nit's too large, the part of debug log is given here:\nbosh-155 is successful with the same cf(release and deployment file) and bosh(deployment file).\nthis failure is happened once each time when update jobs for each VM instance of CF,bosh deploy again after once failure will be successful.\n`` log\n{\"method\":\"get_task\",\"arguments\":[\"8f11a8df-c6fd-4e58-4afa-240954c82352\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.236dd6cc-65ba-42da-afd9-f10e704fe6e4 {\"value\":\"prepared\"}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"drain\",\"arguments\":[\"update\",{\"deployment\":\"cf-warden\",\"job\":{\"name\":\"ha_proxy_z1\",\"templates\":[{\"name\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},{\"name\":\"metron_agent\",\"version\":\"8ad734ba7952b6184ac127a45608e64ebb1c4611\",\"sha1\":\"6f229f6f524dafc9ee4d46188c8cd7af1bcb8098\",\"blobstore_id\":\"3ccf6ca4-ff9d-4b4a-8b91-0d30f0dd11c8\"}],\"template\":\"haproxy\",\"version\":\"7bd402abf9d8fe86c1adf649c791fed92a03653c\",\"sha1\":\"88a89bd6cd01c9b1390a2f0d27443c83b3a697a9\",\"blobstore_id\":\"2dc34358-7cbe-4edd-84e6-aafee04dcd66\"},\"index\":0,\"networks\":{\"floating\":{\"type\":\"vip\",\"ip\":\"9.91.39.29\",\"cloud_properties\":{},\"dns_record_name\":\"0.ha-proxy-z1.floating.cf-warden.bosh\"},\"cf1\":{\"ip\":\"10.10.10.140\",\"netmask\":\"255.255.255.0\",\"cloud_properties\":{\"net_id\":\"67a8ddc7-4d5f-432f-a154-660df2e8e69c\",\"security_groups\":[\"default\"]},\"default\":[\"dns\",\"gateway\"],\"dns\":[\"10.10.10.3\",\"10.10.10.64\"],\"gateway\":\"10.10.10.1\",\"dns_record_name\":\"0.ha-proxy-z1.cf1.cf-warden.bosh\"}},\"resource_pool\":{\"name\":\"router_z1\",\"cloud_properties\":{\"instance_type\":\"m1.medium\"},\"stemcell\":{\"name\":\"bosh-openstack-kvm-centos-7-go_agent-raw-m\",\"version\":\"2962\"}},\"packages\":{\"haproxy\":{\"name\":\"haproxy\",\"version\":\"630ad6d6e1d3cab4547ce104f3019b483f354613.1\",\"sha1\":\"33522a7aef69d54845e4dbb2e3706a6e19f1b87d\",\"blobstore_id\":\"148473d8-c738-4dce-5dc2-1e064a502354\"},\"common\":{\"name\":\"common\",\"version\":\"99c756b71550530632e393f5189220f170a69647.1\",\"sha1\":\"f4e3d8668a5cd7bbce48a5a600936e8fd1813e2b\",\"blobstore_id\":\"3f8c19b4-aaaf-4ef0-5228-688538998b59\"},\"metron_agent\":{\"name\":\"metron_agent\",\"version\":\"27c6122c3dcabaeac758bae7cb258910a5ac1e42.1\",\"sha1\":\"d50ec240046610a5a6e3493258cc269778baa82d\",\"blobstore_id\":\"1e91a9bd-8bb7-4f08-68ad-186fb62afef1\"}},\"configuration_hash\":\"5ddbb2da0fc22bb3790622806d0b3ebce5e44cfc\",\"properties\":{\"ha_proxy\":{\"ssl_pem\":\"-----BEGIN CERTIFICATE-----\\nMIIDETCCAfmgAwIBAgIJANZuykf1uh3LMA0GCSqGSIb3DQEBBQUAMB8xHTAbBgNV\\nBAMMFCouMTAuMjQ0LjAuMzQueGlwLmlvMB4XDTE0MTIyNDIzMTkxM1oXDTI0MTIy\\nMTIzMTkxM1owHzEdMBsGA1UEAwwUKi4xMC4yNDQuMC4zNC54aXAuaW8wggEiMA0G\\nCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCjq73Fgwfj2UT0/+wR9kVVsGAguMj\\npoA0opLCgE0yHStAhSvqq7YpO39dH3vBMWXyr2xIfDyaeZyhV86jWu/ZKswGjNGI\\nZKv/yUINe1bqukOBqd+SHVvkVhxSLJuD1MR83JQMONRjOPJp661/ABpVhnrNfBiA\\nAA6aaFv4/KbyGY/E1FHoUXqEdh4WxaJdfX6SbgG05ArWxhSD7PNj4CYvJWGCdvqP\\nKBsvWFDrkxBHn5h1JIDfZJB8FKP6vaHBr7MU4pIHM+qaZ1Y+8ja0wcgkHn4YHcp6\\nIOhqpck7LaH5Qq2ydYFNTcG4fTbG0jXqcit2WSUxRkXzWnrgo2E0SiHBAgMBAAGj\\nUDBOMB0GA1UdDgQWBBSpCtEDtEvMwaZzXN6Lvk5U7Eyn2zAfBgNVHSMEGDAWgBSp\\nCtEDtEvMwaZzXN6Lvk5U7Eyn2zAMBgNVHRMEBTADAQH/MA0GCSqGSIb3DQEBBQUA\\nA4IBAQB1YIHmw3gPiMn8WDR4yVxDvSVgFHY6ZE1iZb17vVs4N2/mhQZXWJ2nZV02\\ngoAivtgxHOj39sK5OBWsGvrQo5H8dt1t4XmbwB1C6xRerGc25dhDRq42RqhCN0RJ\\nzzjd9b8YSiwtAaZlW36l2jVDLfRapb00tWToF9qYrDrmKy2sekS7g2hbiRStcue/\\nbpT4X/CHxb/lUbpL4m8BpDbkGiOJgl+SEHRx5tZ0Kob/RDQRCcN3p+71FRbDIEBj\\n+8sJl/yUUUPwQ6PNYx6cjtlICWJ1G0l0hRa141VXPqSNCmxYS4dp/8ifPCSoLc+k\\n9TXFkuGl+86CPTyyMJxyMhEcAGZT\\n-----END CERTIFICATE-----\\n-----BEGIN RSA PRIVATE KEY-----\\nMIIEpAIBAAKCAQEAwo6u9xYMH49lE9P/sEfZFVbBgILjI6aANKKSwoBNMh0rQIUr\\n6qu2KTt/XR97wTFl8q9sSHw8mnmcoVfOo1rv2SrMBozRiGSr/8lCDXtW6rpDganf\\nkh1b5FYcUiybg9TEfNyUDDjUYzjyaeutfwAaVYZ6zXwYgAAOmmhb+Pym8hmPxNRR\\n6FF6hHYeFsWiXX1+km4BtOQK1sYUg+zzY+AmLyVhgnb6jygbL1hQ65MQR5+YdSSA\\n32SQfBSj+r2hwa+zFOKSBzPqmmdWPvI2tMHIJB5+GB3KeiDoaqXJOy2h+UKtsnWB\\nTU3BuH02xtI16nIrdlklMUZF81p64KNhNEohwQIDAQABAoIBAC1U3YOIyY5Y9O4n\\nyT2jn/sO2cs9s/rMgrbA4n0bM+FnVnqUDOWC2NDGoihqe4VKIzzmjs5c1CoSB+K3\\n+NerCpOJGzyzdubWvhS9KfzGLjxG5g/CKut6l7yeK78h0aJn4thM9NncK/BqhmET\\nnrsmpPwkd1yFe5fna3+irTtYcvWZgxp8DK4JxIRB0QpJvEwbs9fUFE1E0DVw/uBV\\nCytTIrik2O+n1m8S5xzsFXHXDjXT/TVNi3jtN12Oaj1avYZRP2q45RmcODhtHQIy\\n4o0vqjyAmZb3jOcYysyXVwfyNreIH3Qn0/waflPkyaljMpa1OHVOCJFjh2Xl+aIc\\ndQsME3kCgYEA4aXU2xb9SHew84/3ow6ZmBeYLm+7B8GUjDTe7HqLldtTUhlrJSdB\\nSHZHZ/BXEQ126FnbfZ0IISkBjqVCQBw8MgjZAlaEIInDA1DRTLuOfrcyUD6PeSLi\\ngVZHNYbR9MDnmJ3/So5HXiRy2rBWtLKwPlciqbwY+l3xYr2kP/QdCisCgYEA3LpB\\nTBDTqp9j8QVvB/YjCakS+z+kgtO5HyMZO/uh6o9PRHrFQHwZA5y/MRrx9lxJ9zOd\\nfqysKfA7fXs4VzNnRSnOfBmuDHF0HhlIgAShX/RnB94p9xnsQaMae/o2nXcknQng\\n3qogvHWTo16GCJRE+YTmhA0QtvSlScJlTzHfqcMCgYAPRt/rWVoajufvBX85jeJ+\\nNpK6Chx6gPOirm2tSvqqUagJdekYIdk8o61f7xil8ehsALFohronLJSLaMrcdkzp\\nAkpW6y6U2V7XmaAh9szF7Xc9kY67H85//SxjBlauoGTNo1zGWm2ghQ01mxyzrSlb\\nfyC8pxx1zuhpy/cT0V4p8wKBgQDb+EJaq+pFf9L5v5CHPqRsXDKucR5hwt4aScA8\\nJumV+HvmovMw8Ht9PhjLty6rdg3AbY/nTe3FXcPrqYDcZj3kj2VYB7+MZwRxeoDm\\nE7c/CTIkhSMNPqhUQVeDdjg3dSTn25BeVu2I4yPfC7RHmHukru2La/ncWrLebvzH\\nj8x2QQKBgQDevjRDDTWbBkg8HdCRxxCvhfaHBntoSJdTHlr14Gce48NXGaXRJPQT\\n9dMsPsSHkFxEra7G2clGnhpe+pK9V+WTrD9Qnoc+tK808hX1YQ6mBlnR6w99jlaR\\nHVTi2pRhEhbWUkBv2kooXvD6ANb15PbPSF1FK7YyW1KHqcbm+lF22g==\\n-----END RSA PRIVATE KEY-----\\n\",\"disable_http\":false,\"ssl_ciphers\":\"ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:ECDHE-RSA-RC4-SHA:ECDHE-ECDSA-RC4-SHA:AES128:AES256:RC4-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!3DES:!MD5:!PSK\"},\"request_timeout_in_seconds\":900,\"router\":{\"servers\":{\"z1\":[\"10.10.10.141\"],\"z2\":[]},\"port\":80},\"networks\":{\"apps\":\"cf1\"},\"syslog_daemon_config\":{\"address\":null,\"port\":null,\"transport\":\"tcp\"},\"metron_agent\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457,\"statsd_incoming_port\":8125,\"debug\":false,\"status\":{\"user\":\"\",\"password\":\"\",\"port\":0},\"zone\":\"z1\",\"etcd_query_interval_milliseconds\":5000,\"collector_registrar_interval_milliseconds\":60000},\"loggregator\":{\"incoming_port\":3456,\"dropsonde_incoming_port\":3457},\"loggregator_endpoint\":{\"shared_secret\":\"loggregator-secret\"},\"nats\":{\"user\":\"nats\",\"password\":\"nats\",\"machines\":[\"10.10.10.142\"],\"port\":4222},\"etcd\":{\"machines\":[\"10.10.10.143\"],\"maxconcurrentrequests\":10}},\"dns_domain_name\":\"bosh\",\"persistent_disk\":0,\"template_hashes\":{\"haproxy\":\"66baf4bb051487714f5b6886a8fa27b645f783f2\",\"metron_agent\":\"aa6290cab96fd7e456be9c34299a42c1aa90a104\"},\"rendered_templates_archive\":{\"blobstore_id\":\"d7018513-3179-436d-85a3-b12f719f3528\",\"sha1\":\"0f48d34d5a0976dd906915472bbd8ffa3c9bb716\"}}],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.461607b8-e838-4540-a337-362ad963492a {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:22 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe\"}\nD, [2015-05-05 11:48:22 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.9474347c-5bd4-4c6f-9049-7436b19c25fe {\"value\":{\"agent_task_id\":\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"366d36d2-3f2f-4f51-4e51-e16f4a5e47f4\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.456a41fb-0e12-46f6-ad9c-41bd526ca258 {\"value\":0}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000893s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000891s) SELECT * FROM \"tasks\" WHERE \"id\" = 43\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"stop\",\"arguments\":[],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.31622716-e832-4e67-9dbb-ebb27830a8af {\"value\":{\"agent_task_id\":\"3d66d0dd-07dc-455b-64e5-29f8f7249525\",\"state\":\"running\"}}\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: SENT: agent.9fa93e6b-52cb-445d-8c88-fdb11079d82f {\"method\":\"get_task\",\"arguments\":[\"3d66d0dd-07dc-455b-64e5-29f8f7249525\"],\"reply_to\":\"director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e\"}\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: RECEIVED: director.e08d0256-4216-4584-bc68-e44f890dde40.ed386159-0e39-4616-92a7-c0feb863f38e {\"value\":\"stopped\"}\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Snapshots are disabled; skipping\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping VM update\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Skipping network re-configuration\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000803s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000555s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000515s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000464s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000735s) SELECT * FROM \"records\" WHERE ((\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'A') AND (\"content\" = '9.91.39.29')) LIMIT 1\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.floating.cf-warden.bosh to 9.91.39.29\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000345s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000472s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.floating.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000347s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000942s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.floating.cf-warden.bosh', 'A', 300, '9.91.39.29', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001686s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000352s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000369s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000311s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000448s) SELECT * FROM \"domains\" WHERE ((\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000285s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000247s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000429s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('39.91.9.in-addr.arpa', 'NATIVE') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001353s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000301s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000409s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000339s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000490s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000416s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000266s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000497s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (6, '39.91.9.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001004s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000329s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000492s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 6) AND (\"name\" = '39.91.9.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000363s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000323s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000817s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (6, '39.91.9.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001443s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000370s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000291s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000262s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.floating.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000408s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000441s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000789s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (6, '29.39.91.9.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.floating.cf-warden.bosh', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001202s) COMMIT\nI, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)]  INFO -- DirectorJobRunner: Updating DNS for: 0.ha-proxy-z1.cf1.cf-warden.bosh to 10.10.10.140\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000457s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000428s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000680s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 1) AND (\"name\" = '0.ha-proxy-z1.cf1.cf-warden.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000375s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000586s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (1, '0.ha-proxy-z1.cf1.cf-warden.bosh', 'A', 300, '10.10.10.140', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000957s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000244s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000297s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000391s) SELECT * FROM \"domains\" WHERE ((\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NATIVE')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000256s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000223s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000430s) INSERT INTO \"domains\" (\"name\", \"type\") VALUES ('10.10.10.in-addr.arpa', 'NATIVE') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001081s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000357s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000336s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000324s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000488s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'SOA') AND (\"content\" = 'localhost hostmaster@localhost 0 10800 604800 30') AND (\"ttl\" = 14400)) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000361s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000509s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"content\", \"ttl\") VALUES (7, '10.10.10.in-addr.arpa', 'SOA', 'localhost hostmaster@localhost 0 10800 604800 30', 14400) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001014s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000338s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000287s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000331s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000289s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000463s) SELECT * FROM \"records\" WHERE ((\"domain_id\" = 7) AND (\"name\" = '10.10.10.in-addr.arpa') AND (\"type\" = 'NS') AND (\"ttl\" = 14400) AND (\"content\" = 'ns.bosh')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000330s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000473s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000892s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\") VALUES (7, '10.10.10.in-addr.arpa', 'NS', 14400, 'ns.bosh') RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001340s) COMMIT\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000479s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000447s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000382s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000606s) SELECT * FROM \"records\" WHERE ((\"content\" = '0.ha-proxy-z1.cf1.cf-warden.bosh') AND (\"type\" = 'PTR')) LIMIT 1\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000499s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000337s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.000801s) INSERT INTO \"records\" (\"domain_id\", \"name\", \"type\", \"ttl\", \"content\", \"change_date\") VALUES (7, '140.10.10.10.in-addr.arpa', 'PTR', 300, '0.ha-proxy-z1.cf1.cf-warden.bosh', 1430826503) RETURNING *\nD, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] DEBUG -- DirectorJobRunner: (0.001254s) COMMIT\nE, [2015-05-05 11:48:23 #20891] [canary_update(ha_proxy_z1/0)] ERROR -- DirectorJobRunner: Error updating canary instance: #<Errno::ENOENT: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl>\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl - /var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Shutting down pool\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.005891s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.001478s) SELECT \"stemcells\".* FROM \"stemcells\" INNER JOIN \"deployments_stemcells\" ON ((\"deployments_stemcells\".\"stemcell_id\" = \"stemcells\".\"id\") AND (\"deployments_stemcells\".\"deployment_id\" = 3))\nD, [2015-05-05 11:48:23 #20891] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:cf-warden\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:cf-warden\nI, [2015-05-05 11:48:23 #20891] [task:43]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"2f9236d9-aa9a-464f-be68-2173793a47d4\",\"severity\":3,\"title\":\"director - error during update deployment\",\"summary\":\"Error during update deployment for 'cf-warden' against Director '609afeed-cd42-48dd-a3a6-08e5bf044992': #<Errno::ENOENT: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl>\",\"created_at\":1430826503}\nE, [2015-05-05 11:48:23 #20891] [task:43] ERROR -- DirectorJobRunner: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:inspawn'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:193:in popen_run'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:93:inpopen3'\n/var/vcap/packages/ruby/lib/ruby/2.1.0/open3.rb:252:in capture3'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/dns_helper.rb:207:inflush_dns_cache'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:215:in update_dns'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inblock in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:37:in step'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/instance_updater.rb:71:inupdate'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:74:in block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:72:in block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/event_log.rb:97:in advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:71:inupdate_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.2962.0/lib/bosh/director/job_updater.rb:65:in block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.2962.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:incall'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000320s) SELECT NULL\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000293s) BEGIN\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.000943s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2015-05-05 11:48:23.258668+0000', \"description\" = 'create deployment', \"result\" = 'No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl', \"output\" = '/var/vcap/store/director/tasks/43', \"checkpoint_time\" = '2015-05-05 11:48:20.863669+0000', \"type\" = 'update_deployment', \"username\" = 'admin' WHERE (\"id\" = 43)\nD, [2015-05-05 11:48:23 #20891] [task:43] DEBUG -- DirectorJobRunner: (0.002082s) COMMIT\nI, [2015-05-05 11:48:23 #20891] []  INFO -- DirectorJobRunner: Task took 1 minute 32.420035999999996 seconds to process.\nTask 43 error\n```\n. @bosh-ci @cppforlife ,\nI use microbosh to deploy bosh. then, i use bosh to deploy CF.\nso, the bosh  director is collocated with powerDNS, is there some difference between bosh-155 and later version on the deployment file?\nhere is my deployment file:\n``` yml\nname: bosh\ndirector_uuid: 11a6c6ab-8419-4db9-9730-7e15a9f42ed0 # CHANGE\nrelease:\n  name: bosh\n  version: latest\ncompilation:\n  workers: 3\n  network: default\n  reuse_compilation_vms: true \n  cloud_properties:\n    instance_type: m1.medium # CHANGE\navailability_zone: szxts01-az\nupdate:\n  canaries: 1\n  canary_watch_time: 3000-120000\n  update_watch_time: 3000-120000\n  max_in_flight: 50\n  max_errors: 1\nserial: true\nnetworks:\n  - name: floating\n    type: vip\n    cloud_properties: {}\n  - name: default\n    type: manual\n    subnets:\n      - name: private\n        range: 10.10.10.0/24 # CHANGE\n        gateway: 10.10.10.1 # CHANGE\n        dns: [10.10.10.3] #CHANGE\n        reserved:\n          - 10.10.10.2 - 10.10.10.60 # CHANGE,100-110 larger than resource_pools[\"size\"]\n        static:\n          - 10.10.10.61 - 10.10.10.70 # CHANGE\n        cloud_properties:\n          net_id: 67a8ddc7-4d5f-432f-a154-660df2e8e69c # CHANGE\nresource_pools:\n  - name: common\n    network: default\n    size: 8\n    stemcell:\n      name: bosh-openstack-kvm-centos-7-go_agent-raw-m\n      version: latest\n    cloud_properties:\n      instance_type: m1.medium # CHANGE\navailability_zone: szxts01-az\njobs:\n  - name: nats\n    template: nats\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\n        static_ips:\n          - 10.10.10.61 # CHANGE\n\n\nname: redis\n    template: redis\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.62 # CHANGE\n\n\n\n\n\nname: postgres\n    template: postgres\n    instances: 1\n    resource_pool: common\n    persistent_disk: 16384\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.63 # CHANGE\n\n\n\n\n\nname: powerdns\n    template: powerdns\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.64 # CHANGE\n\n\nname: floating\n    static_ips:\n9.91.39.27 # CHANGE\n\n\n\n\n\nname: blobstore\n    template: blobstore\n    instances: 1\n    resource_pool: common\n    persistent_disk: 51200\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.65 # CHANGE\n\n\n\n\n\nname: director\n    template: director\n    instances: 1\n    resource_pool: common\n    persistent_disk: 16384\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.66 # CHANGE\n\n\nname: floating\n    static_ips:\n9.91.39.28 # CHANGE\n\n\n\n\n\nname: registry\n    template: registry\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.67 # CHANGE\n\n\n\n\n\nname: health_monitor\n    template: health_monitor\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.68 # CHANGE\n\n\n\n\n\nproperties:\n  nats:\n    address: 10.10.10.61 # CHANGE\n    user: nats\n    password: nats\nredis:\n    address: 10.10.10.62 # CHANGE\n    password: redis\npostgres: &bosh_db\n    host: 10.10.10.63 # CHANGE\n    user: postgres\n    password: postgres\n    database: bosh\ndns:\n    address: 10.10.10.64 # CHANGE\n    db: *bosh_db\n    recursor: 9.91.39.27 # CHANGE\nblobstore:\n    address: 10.10.10.65 # CHANGE\n    agent:\n      user: agent\n      password: agent\n    director:\n      user: director\n      password: director\ndirector:\n    name: bosh\n    address: 10.10.10.66 # CHANGE\n    db: *bosh_db\nregistry:\n    address: 10.10.10.67 # CHANGE\n    db: *bosh_db\n    http:\n      user: registry\n      password: registry\nhm:\n    http:\n      user: hm\n      password: hm\n    director_account:\n      user: admin\n      password: admin\n    resurrector_enabled: true\nntp:\n    - 0.north-america.pool.ntp.org\n    - 1.north-america.pool.ntp.org\nopenstack:\n    auth_url: http://9.91.17.18:5000/v2.0 # CHANGE\n    username: admin # CHANGE\n    api_key: Huawei # CHANGE\n    tenant: admin # CHANGE\nregion: RegionOne # CHANGE\ndefault_security_groups: [\"default\", \"default\"] # CHANGE\ndefault_key_name: lv_microbosh # CHANGE\n\nignore_server_availability_zone: true\n```\n. @bosh-ci @cppforlife ,\nI use microbosh to deploy bosh. then, i use bosh to deploy CF.\nso, the bosh  director is collocated with powerDNS, is there some difference between bosh-155 and later version on the deployment file?\nhere is my deployment file:\n``` yml\nname: bosh\ndirector_uuid: 11a6c6ab-8419-4db9-9730-7e15a9f42ed0 # CHANGE\nrelease:\n  name: bosh\n  version: latest\ncompilation:\n  workers: 3\n  network: default\n  reuse_compilation_vms: true \n  cloud_properties:\n    instance_type: m1.medium # CHANGE\navailability_zone: szxts01-az\nupdate:\n  canaries: 1\n  canary_watch_time: 3000-120000\n  update_watch_time: 3000-120000\n  max_in_flight: 50\n  max_errors: 1\nserial: true\nnetworks:\n  - name: floating\n    type: vip\n    cloud_properties: {}\n  - name: default\n    type: manual\n    subnets:\n      - name: private\n        range: 10.10.10.0/24 # CHANGE\n        gateway: 10.10.10.1 # CHANGE\n        dns: [10.10.10.3] #CHANGE\n        reserved:\n          - 10.10.10.2 - 10.10.10.60 # CHANGE,100-110 larger than resource_pools[\"size\"]\n        static:\n          - 10.10.10.61 - 10.10.10.70 # CHANGE\n        cloud_properties:\n          net_id: 67a8ddc7-4d5f-432f-a154-660df2e8e69c # CHANGE\nresource_pools:\n  - name: common\n    network: default\n    size: 8\n    stemcell:\n      name: bosh-openstack-kvm-centos-7-go_agent-raw-m\n      version: latest\n    cloud_properties:\n      instance_type: m1.medium # CHANGE\navailability_zone: szxts01-az\njobs:\n  - name: nats\n    template: nats\n    instances: 1\n    resource_pool: common\n    networks:\n      - name: default\n        default: [dns, gateway]\n        static_ips:\n          - 10.10.10.61 # CHANGE\n\n\nname: redis\n    template: redis\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.62 # CHANGE\n\n\n\n\n\nname: postgres\n    template: postgres\n    instances: 1\n    resource_pool: common\n    persistent_disk: 16384\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.63 # CHANGE\n\n\n\n\n\nname: powerdns\n    template: powerdns\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.64 # CHANGE\n\n\nname: floating\n    static_ips:\n9.91.39.27 # CHANGE\n\n\n\n\n\nname: blobstore\n    template: blobstore\n    instances: 1\n    resource_pool: common\n    persistent_disk: 51200\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.65 # CHANGE\n\n\n\n\n\nname: director\n    template: director\n    instances: 1\n    resource_pool: common\n    persistent_disk: 16384\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.66 # CHANGE\n\n\nname: floating\n    static_ips:\n9.91.39.28 # CHANGE\n\n\n\n\n\nname: registry\n    template: registry\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.67 # CHANGE\n\n\n\n\n\nname: health_monitor\n    template: health_monitor\n    instances: 1\n    resource_pool: common\n    networks:\n\nname: default\n    default: [dns, gateway]\n    static_ips:\n10.10.10.68 # CHANGE\n\n\n\n\n\nproperties:\n  nats:\n    address: 10.10.10.61 # CHANGE\n    user: nats\n    password: nats\nredis:\n    address: 10.10.10.62 # CHANGE\n    password: redis\npostgres: &bosh_db\n    host: 10.10.10.63 # CHANGE\n    user: postgres\n    password: postgres\n    database: bosh\ndns:\n    address: 10.10.10.64 # CHANGE\n    db: *bosh_db\n    recursor: 9.91.39.27 # CHANGE\nblobstore:\n    address: 10.10.10.65 # CHANGE\n    agent:\n      user: agent\n      password: agent\n    director:\n      user: director\n      password: director\ndirector:\n    name: bosh\n    address: 10.10.10.66 # CHANGE\n    db: *bosh_db\nregistry:\n    address: 10.10.10.67 # CHANGE\n    db: *bosh_db\n    http:\n      user: registry\n      password: registry\nhm:\n    http:\n      user: hm\n      password: hm\n    director_account:\n      user: admin\n      password: admin\n    resurrector_enabled: true\nntp:\n    - 0.north-america.pool.ntp.org\n    - 1.north-america.pool.ntp.org\nopenstack:\n    auth_url: http://9.91.17.18:5000/v2.0 # CHANGE\n    username: admin # CHANGE\n    api_key: Huawei # CHANGE\n    tenant: admin # CHANGE\nregion: RegionOne # CHANGE\ndefault_security_groups: [\"default\", \"default\"] # CHANGE\ndefault_key_name: lv_microbosh # CHANGE\n\nignore_server_availability_zone: true\n```\n. @bosh-ci ,thanks your help,i will try to deploy bosh director and powerDNS on the same VM.\nalso @mariash , can you help me to find out the scenario that bosh director is on\na separate VM from the PowerDNS? thanks.\n. @bosh-ci ,thanks your help,i will try to deploy bosh director and powerDNS on the same VM.\nalso @mariash , can you help me to find out the scenario that bosh director is on\na separate VM from the PowerDNS? thanks.\n. hi @cppforlife ,\nif i deploy bosh director and powerDNS to a separate VM,just like my deployment said, i will get a error when i deploy CF with bosh.\n\nHi ,\nIt will be failed if i use bosh >155 to deploy CF,bosh-155 is successful.\nStarted preparing configuration > Binding configuration. Done (00:00:04)\nStarted updating job ha_proxy_z1 > ha_proxy_z1/0 (canary). Failed: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl (00:00:03)\nError 100: No such file or directory - /var/vcap/jobs/powerdns/bin/powerdns_ctl\nTask 3 error\npowerdns is a component of bosh, right? why is it reported here?\n\nhowever, if i let bosh director and powerDNS reside in the same VM, there is no this error.\n. @cppforlife \noh,it means that i cant deploy bosh director and powerDNS to different VM,right? it looks like a new limitation.if it is yes, i think the example of deployment would better update.\nthanks.\n. @cppforlife \noh,it means that i cant deploy bosh director and powerDNS to different VM,right? it looks like a new limitation.if it is yes, i think the example of deployment would better update.\nthanks.\n. hi @cppforlife ,\nyes,i am using openstack. that's because i got a pivot_root failure problem when diego create and start a container,i guess that the ephemeral disk maybe doesnt support pivot_root,like a  ramdisk?\nthe pivot_root issue\nAnd, if i install parted on centos, is it can work normally?\nhere is the partition info of my VM with centos\nshell\nFilesystem     Type      Size  Used Avail Use% Mounted on\ndevtmpfs       devtmpfs  2.0G     0  2.0G   0% /dev\ntmpfs          tmpfs     2.0G     0  2.0G   0% /dev/shm\ntmpfs          tmpfs     2.0G   41M  1.9G   3% /run\ntmpfs          tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/vda1      ext4       38G   12G   25G  32% /\n/dev/vdb2      ext4       36G  1.6G   33G   5% /var/vcap/data\ntmpfs          tmpfs     1.0M   28K  996K   3% /var/vcap/data/sys/run\n/dev/loop0     ext4      120M  1.6M  115M   2% /tmp\ncgroup         tmpfs     2.0G   48K  2.0G   1% /tmp/garden-/cgroup\nthanks\n. hi\nparted is already there in centos 7.1,i have try it again without ephemeral disk. it looks like correctly,but pivot_root also complains Invalid argument.\nshell\nFilesystem     Type      Size  Used Avail Use% Mounted on\ndevtmpfs       devtmpfs  2.0G     0  2.0G   0% /dev\ntmpfs          tmpfs     2.0G     0  2.0G   0% /dev/shm\ntmpfs          tmpfs     2.0G  8.3M  2.0G   1% /run\ntmpfs          tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup\n/dev/vda1      ext4       38G   11G   25G  30% /\n/dev/vda3      ext4       38G  1.5G   34G   5% /var/vcap/data\ntmpfs          tmpfs     1.0M   28K  996K   3% /var/vcap/data/sys/run\n/dev/loop0     ext4      120M  1.6M  115M   2% /tmp\ncgroup         tmpfs     2.0G  8.0K  2.0G   1% /tmp/garden-/cgroup\nnone           aufs       38G  1.5G   34G   5% /var/vcap/data/garden-linux/overlays/ngsn71i2p5t/rootfs\nnone           aufs       38G  1.5G   34G   5% /var/vcap/data/garden-linux/overlays/ngsn71i2p5t/rootfs\n. hi @cppforlife ,\nyes,the pivot_root error is not relate to the disk partitioning at all. I have verify that.\nand bosh support re-partitioning of root disk on centos 7 stemcell.the limitation can be removed. \nthanks.\n. sorry for it,\nthat because i have did some kernel changes in stemcell. I had recompile the kernel with linux-4.0.2 source code and install headers.it cause scsi/scsi.h missed.\n. sorry for it,\nthat because i have did some kernel changes in stemcell. I had recompile the kernel with linux-4.0.2 source code and install headers.it cause scsi/scsi.h missed.\n. ",
    "wayneeseguin": "What a brilliant suggestion! I second this motion!\nWe have had great success with using Consul DNS for our service, this would address HA concerns quite nicely.\n. FYI I just issued Pull Request 849 which addresses this issue. \nAlso, this was likely an issue on not just CentOS stemcells because of the behavior however I did not verify other stemcell flavors.\n. ",
    "jrbudnack": "bump @cppforlife ^^\n. bump @cppforlife ^^\n. Here is an example of what we see in the logs (shortened and annotated, with sensitive data redacted):\nCREATE THE FIRST VM:\nI, [2016-01-19T18:03:42.870144 #26066]  INFO -- : Creating vm: vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a on <[Vim.ClusterComputeResource] domain-c8> stored in <[Vim.Datastore] datastore-97>\nI, [2016-01-19T18:03:42.881412 #26066]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9.\nI, [2016-01-19T18:03:42.887621 #26066]  INFO -- : Found local stemcell replica: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:03:42.887684 #26066]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:03:42.944202 #26066]  INFO -- : Cloning vm: (VSphereCloud::Resources::VM (cid=\"sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9\")) to vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\nD, [2016-01-19T18:03:42.971320 #26066] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-533>\nI, [2016-01-19T18:03:50.566056 #26066]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\", \"id\"=>\"vm-11504\"},\n \"agent_id\"=>\"6f99c79b-cf47-457f-9005-24ccae5f471b\",\n \"networks\"=>\n  {\"default\"=>\n    {\"ip\"=>\"10.19.48.248\",\n     \"netmask\"=>\"255.255.240.0\",\n     \"cloud_properties\"=>{\"name\"=>\"SB_PCF\"},\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"dns\"=>[\"10.18.99.10\"],\n     \"gateway\"=>\"10.19.48.1\",\n     \"mac\"=>\"00:50:56:ae:57:83\"}},\n \"disks\"=>{\"system\"=>\"0\", \"ephemeral\"=>\"1\", \"persistent\"=>{}},\n \"ntp\"=>[\"10.18.99.10\", \"10.18.99.5\"],\n \"blobstore\"=>\n  {\"provider\"=>\"dav\",\n   \"options\"=>\n    {\"endpoint\"=>\"http://10.19.64.4:25250\",\n     \"user\"=>\"agent\",\n     \"password\"=>\"REDACTED\"}},\n \"mbus\"=>\"nats://nats:nats@10.19.64.4:4222\",\n \"env\"=>\n  {\"bosh\"=>\n    {\"password\"=>\n      \"REDACTED\"}}}\n...\nPASSWORD CHANGE:\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: env_changed? changed FROM: {} TO: {\"bosh\"=>{\"password\"=>\"REDACTED\"}} on instance push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625)\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: VM needs to be shutdown before it can be updated.\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: Failed to update in place. Recreating VM\nI, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))]  INFO -- DirectorJobRunner: Deleting VM\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: External CPI sending request: {\"method\":\"delete_vm\",\"arguments\":[\"vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\"],\"context\":{\"director_uuid\":\"095aa975-c092-40c4-aa2a-2a5337a30353\"}} with command: /var/vcap/jobs/vsphere_cpi/bin/cpi\n...\nCREATE SECOND VM:\n[2016-01-19T18:05:03.638964 #26391] DEBUG -- : VM creator initialized with memory: 4096, disk: , cpu: 2, placer: (VSphereCloud::FixedClusterPlacer (cluster=\"(VSphereCloud::Resources::Cluster (name=\"FD1\"))\"))\nD, [2016-01-19T18:05:03.745740 #26391] DEBUG -- : Looking for a ephemeral datastore in FD1 with 17408MB free space.\nD, [2016-01-19T18:05:03.745989 #26391] DEBUG -- : All datastores within cluster FD1: [\"STORAGE3-BLOCK (6689244MB free of 31457024MB capacity)\", \"STORAGE2-BLOCK (9354899MB free of 36699904MB capacity)\", \"STORAGE1-BLOCK (8572882MB free of 36699904MB capacity)\"]\nD, [2016-01-19T18:05:03.746187 #26391] DEBUG -- : Datastores with enough space: [\"STORAGE3-BLOCK (6689244MB free of 31457024MB capacity)\", \"STORAGE2-BLOCK (9354899MB free of 36699904MB capacity)\", \"STORAGE1-BLOCK (8572882MB free of 36699904MB capacity)\"]\nI, [2016-01-19T18:05:03.746565 #26391]  INFO -- : Creating vm: vm-fa204871-c84c-4cd8-9b3e-be1832fb5745 on <[Vim.ClusterComputeResource] domain-c8> stored in <[Vim.Datastore] datastore-97>\nI, [2016-01-19T18:05:03.773032 #26391]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9.\nI, [2016-01-19T18:05:03.787062 #26391]  INFO -- : Found local stemcell replica: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:05:03.787209 #26391]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:05:03.903580 #26391]  INFO -- : Cloning vm: (VSphereCloud::Resources::VM (cid=\"sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9\")) to vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\nD, [2016-01-19T18:05:03.947784 #26391] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-533>\nI, [2016-01-19T18:05:11.549443 #26391]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\", \"id\"=>\"vm-11505\"},\n \"agent_id\"=>\"85e712f6-95e9-429b-b812-1795bfb3024d\",\n \"networks\"=>\n  {\"default\"=>\n    {\"ip\"=>\"10.19.48.248\",\n     \"netmask\"=>\"255.255.240.0\",\n     \"cloud_properties\"=>{\"name\"=>\"SB_PCF\"},\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"dns\"=>[\"10.18.99.10\"],\n     \"gateway\"=>\"10.19.48.1\",\n     \"mac\"=>\"00:50:56:ae:2c:f5\"}},\n \"disks\"=>{\"system\"=>\"0\", \"ephemeral\"=>\"1\", \"persistent\"=>{}},\n \"ntp\"=>[\"10.18.99.10\", \"10.18.99.5\"],\n \"blobstore\"=>\n  {\"provider\"=>\"dav\",\n   \"options\"=>\n    {\"endpoint\"=>\"http://10.19.64.4:25250\",\n     \"user\"=>\"agent\",\n     \"password\"=>\"REDACTED\"}},\n \"mbus\"=>\"nats://nats:nats@10.19.64.4:4222\",\n \"env\"=>\n  {\"bosh\"=>\n    {\"password\"=>\n      \"REDACTED\"}}}\nat depth 0 - 20: unable to get local issuer certificate\nat depth 0 - 20: unable to get local issuer certificate\nI, [2016-01-19T18:05:18.108711 #26391]  INFO -- : Powering on VM: (VSphereCloud::Resources::VM (cid=\"vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\"))\n...\nERROR:\nError 450002: Timed out sending `update_settings' to 6f99c79b-cf47-457f-9005-24ccae5f471b after 45 seconds\n. Here is an example of what we see in the logs (shortened and annotated, with sensitive data redacted):\nCREATE THE FIRST VM:\nI, [2016-01-19T18:03:42.870144 #26066]  INFO -- : Creating vm: vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a on <[Vim.ClusterComputeResource] domain-c8> stored in <[Vim.Datastore] datastore-97>\nI, [2016-01-19T18:03:42.881412 #26066]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9.\nI, [2016-01-19T18:03:42.887621 #26066]  INFO -- : Found local stemcell replica: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:03:42.887684 #26066]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:03:42.944202 #26066]  INFO -- : Cloning vm: (VSphereCloud::Resources::VM (cid=\"sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9\")) to vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\nD, [2016-01-19T18:03:42.971320 #26066] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-533>\nI, [2016-01-19T18:03:50.566056 #26066]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\", \"id\"=>\"vm-11504\"},\n \"agent_id\"=>\"6f99c79b-cf47-457f-9005-24ccae5f471b\",\n \"networks\"=>\n  {\"default\"=>\n    {\"ip\"=>\"10.19.48.248\",\n     \"netmask\"=>\"255.255.240.0\",\n     \"cloud_properties\"=>{\"name\"=>\"SB_PCF\"},\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"dns\"=>[\"10.18.99.10\"],\n     \"gateway\"=>\"10.19.48.1\",\n     \"mac\"=>\"00:50:56:ae:57:83\"}},\n \"disks\"=>{\"system\"=>\"0\", \"ephemeral\"=>\"1\", \"persistent\"=>{}},\n \"ntp\"=>[\"10.18.99.10\", \"10.18.99.5\"],\n \"blobstore\"=>\n  {\"provider\"=>\"dav\",\n   \"options\"=>\n    {\"endpoint\"=>\"http://10.19.64.4:25250\",\n     \"user\"=>\"agent\",\n     \"password\"=>\"REDACTED\"}},\n \"mbus\"=>\"nats://nats:nats@10.19.64.4:4222\",\n \"env\"=>\n  {\"bosh\"=>\n    {\"password\"=>\n      \"REDACTED\"}}}\n...\nPASSWORD CHANGE:\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: env_changed? changed FROM: {} TO: {\"bosh\"=>{\"password\"=>\"REDACTED\"}} on instance push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625)\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: VM needs to be shutdown before it can be updated.\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: Failed to update in place. Recreating VM\nI, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))]  INFO -- DirectorJobRunner: Deleting VM\nD, [2016-01-19 18:04:48 #26039] [canary_update(push-apps-manager/0 (99d94835-0e6a-4fc9-aac2-8ece3ce81625))] DEBUG -- DirectorJobRunner: External CPI sending request: {\"method\":\"delete_vm\",\"arguments\":[\"vm-fbd9071f-8257-4b2d-b3bc-58f2ab2d188a\"],\"context\":{\"director_uuid\":\"095aa975-c092-40c4-aa2a-2a5337a30353\"}} with command: /var/vcap/jobs/vsphere_cpi/bin/cpi\n...\nCREATE SECOND VM:\n[2016-01-19T18:05:03.638964 #26391] DEBUG -- : VM creator initialized with memory: 4096, disk: , cpu: 2, placer: (VSphereCloud::FixedClusterPlacer (cluster=\"(VSphereCloud::Resources::Cluster (name=\"FD1\"))\"))\nD, [2016-01-19T18:05:03.745740 #26391] DEBUG -- : Looking for a ephemeral datastore in FD1 with 17408MB free space.\nD, [2016-01-19T18:05:03.745989 #26391] DEBUG -- : All datastores within cluster FD1: [\"STORAGE3-BLOCK (6689244MB free of 31457024MB capacity)\", \"STORAGE2-BLOCK (9354899MB free of 36699904MB capacity)\", \"STORAGE1-BLOCK (8572882MB free of 36699904MB capacity)\"]\nD, [2016-01-19T18:05:03.746187 #26391] DEBUG -- : Datastores with enough space: [\"STORAGE3-BLOCK (6689244MB free of 31457024MB capacity)\", \"STORAGE2-BLOCK (9354899MB free of 36699904MB capacity)\", \"STORAGE1-BLOCK (8572882MB free of 36699904MB capacity)\"]\nI, [2016-01-19T18:05:03.746565 #26391]  INFO -- : Creating vm: vm-fa204871-c84c-4cd8-9b3e-be1832fb5745 on <[Vim.ClusterComputeResource] domain-c8> stored in <[Vim.Datastore] datastore-97>\nI, [2016-01-19T18:05:03.773032 #26391]  INFO -- : Stemcell lives on a different datastore, looking for a local copy of: sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9.\nI, [2016-01-19T18:05:03.787062 #26391]  INFO -- : Found local stemcell replica: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:05:03.787209 #26391]  INFO -- : Using stemcell VM: <[Vim.VirtualMachine] vm-10871>\nI, [2016-01-19T18:05:03.903580 #26391]  INFO -- : Cloning vm: (VSphereCloud::Resources::VM (cid=\"sc-1a8926ab-d1f7-4649-b2e6-743ef7a4d0a9\")) to vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\nD, [2016-01-19T18:05:03.947784 #26391] DEBUG -- : Found requested resource pool: <[Vim.ResourcePool] resgroup-533>\nI, [2016-01-19T18:05:11.549443 #26391]  INFO -- : Setting VM env: {\"vm\"=>{\"name\"=>\"vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\", \"id\"=>\"vm-11505\"},\n \"agent_id\"=>\"85e712f6-95e9-429b-b812-1795bfb3024d\",\n \"networks\"=>\n  {\"default\"=>\n    {\"ip\"=>\"10.19.48.248\",\n     \"netmask\"=>\"255.255.240.0\",\n     \"cloud_properties\"=>{\"name\"=>\"SB_PCF\"},\n     \"default\"=>[\"dns\", \"gateway\"],\n     \"dns\"=>[\"10.18.99.10\"],\n     \"gateway\"=>\"10.19.48.1\",\n     \"mac\"=>\"00:50:56:ae:2c:f5\"}},\n \"disks\"=>{\"system\"=>\"0\", \"ephemeral\"=>\"1\", \"persistent\"=>{}},\n \"ntp\"=>[\"10.18.99.10\", \"10.18.99.5\"],\n \"blobstore\"=>\n  {\"provider\"=>\"dav\",\n   \"options\"=>\n    {\"endpoint\"=>\"http://10.19.64.4:25250\",\n     \"user\"=>\"agent\",\n     \"password\"=>\"REDACTED\"}},\n \"mbus\"=>\"nats://nats:nats@10.19.64.4:4222\",\n \"env\"=>\n  {\"bosh\"=>\n    {\"password\"=>\n      \"REDACTED\"}}}\nat depth 0 - 20: unable to get local issuer certificate\nat depth 0 - 20: unable to get local issuer certificate\nI, [2016-01-19T18:05:18.108711 #26391]  INFO -- : Powering on VM: (VSphereCloud::Resources::VM (cid=\"vm-fa204871-c84c-4cd8-9b3e-be1832fb5745\"))\n...\nERROR:\nError 450002: Timed out sending `update_settings' to 6f99c79b-cf47-457f-9005-24ccae5f471b after 45 seconds\n. ",
    "ipolyzos": "sorry about this, i have finxes the formating now. Yes indeed this is the case, blobstore and compiled cache https settings are false but agent https is true.  Thank you for the advice and help.\n. It seems that microbosh configuration is not used. Tried passing the following configuration but the director.yml deployedin director has the ssl use and verification true.\ndirector:\n  properties:\n    blobstore:\n       use_ssl: false\n       ssl_verify_peer: false\n. Going through the floowing configurations seems not to apply changes in director.yml deployed in bosh director:\n1. \ncloud:\n  ...\n  properties:\n    agent:\n      blobstore:\n        use_ssl: false\n        ssl_verify_peer: false\n2.\napply_spec:\n  properties:\n    director:\n      max_upload_size: \"10000m\"\n      max_threads: 3\n      properties:\n      agent:\n          blobstore:\n            use_ssl: false\n            ssl_verify_peer: false\n. hey guys, has anyone looked at it? is there any updates?\n. ",
    "Nopik": "Nice found, I've just got bitten by this bug.\n. Nice found, I've just got bitten by this bug.\n. Thanks!\n. Thanks!\n. FYI, I didn't managed to get that vm in order. I did bosh delete deployment concourse and then  bosh deploy again and it worked.\n. FYI, I didn't managed to get that vm in order. I did bosh delete deployment concourse and then  bosh deploy again and it worked.\n. ",
    "tushar-dadlani": "Nice find. We ran into the same issue and found this helpful. \n+1 \n. Nice find. We ran into the same issue and found this helpful. \n+1 \n. Yes. Sorry for the confusion.\nOn Thu, Oct 13, 2016 at 11:26 AM Dmitriy Kalinin notifications@github.com\nwrote:\n\nTo confirm you are saying it should be \"Started updating instance group\nweb\"?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1475#issuecomment-253596982,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJyOOLmSJWlUIYReSAM5GUdxuDWPgBWks5qzne4gaJpZM4KWL24\n.\n. Yes. Sorry for the confusion.\nOn Thu, Oct 13, 2016 at 11:26 AM Dmitriy Kalinin notifications@github.com\nwrote:\nTo confirm you are saying it should be \"Started updating instance group\nweb\"?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1475#issuecomment-253596982,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJyOOLmSJWlUIYReSAM5GUdxuDWPgBWks5qzne4gaJpZM4KWL24\n.\n. We managed to recover from this by commenting the errand job, running bosh deploy and then uncommenting the errand job and then bosh deploy\n. We managed to recover from this by commenting the errand job, running bosh deploy and then uncommenting the errand job and then bosh deploy\n. @paolostivanin We are running v258 with the following stemcell on an AWS env with the following stemcell : bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3263.7. We do not see this problem.\n\n@JamesClonk is probably running on Openstack.\n. @paolostivanin We are running v258 with the following stemcell on an AWS env with the following stemcell : bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3263.7. We do not see this problem.\n@JamesClonk is probably running on Openstack.\n. Unfortunately , we don't have any OpenStack environments.\nOn Wed, Oct 26, 2016 at 2:27 AM Paolo Stivanin notifications@github.com\nwrote:\n\nI have just tested bosh 258 on our AWS deployment and it is working super\nfine. No problem at all on aws.\nSo this narrow down the issue on Openstack.\n@tushar-dadlani https://github.com/tushar-dadlani do you have an OS env\nwith which you could try bosh 258?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1491#issuecomment-256295287,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJyOGLBpr1QUawv-8DjoG0XvXo8XQYKks5q3xzugaJpZM4Kf4uN\n.\n. Unfortunately , we don't have any OpenStack environments.\nOn Wed, Oct 26, 2016 at 2:27 AM Paolo Stivanin notifications@github.com\nwrote:\nI have just tested bosh 258 on our AWS deployment and it is working super\nfine. No problem at all on aws.\nSo this narrow down the issue on Openstack.\n@tushar-dadlani https://github.com/tushar-dadlani do you have an OS env\nwith which you could try bosh 258?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1491#issuecomment-256295287,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAJyOGLBpr1QUawv-8DjoG0XvXo8XQYKks5q3xzugaJpZM4Kf4uN\n.\n. @paolostivanin @beyhan @JamesClonk We have seen this issue on AWS as well. It depends on how many deployments you are running. The previous update was based on an environment with 2 deployments. We have seen this issue when the number of deployments increases.\n. @paolostivanin @beyhan @JamesClonk We have seen this issue on AWS as well. It depends on how many deployments you are running. The previous update was based on an environment with 2 deployments. We have seen this issue when the number of deployments increases.\n. \n",
    "poblin-orange": "+1. no idea yet why ..\n. Met this issue again today using bosh create release.\nShould the fix be applied also on bosh-cli (using BOSH 1.3068.0)\n. @voelzmo done, thks\n. Yes the aim is to target local storage type for persistent disk. My understanding is the cpi api does not manage ephemeral disk, only persistent disk.\nThe idea is to target the persistent disk when recreating vm (typically after a bosh release update)\n. Yes, we added a cloud_properties ephemeral_disk_offering, so our cloudstack CPI can use a dedicated iaas disk offering, generally local, for ephemeral disk. This cause no issue, as the ephemeral disk and vm lifecycle are the same.\nFor the persistent disk, a cloud_properties disk_offering let the CPI place the persistent disk with a distinct offering (local / or shared - nas, its up to the iaas admin).\nAs our external CPI has intimate link with the Iaas, it will have special support if the persistent disk is local, ie create the vm on the same host. We are also considering moving the disk, with a snapshot / recreate api call sequence for example.\nHooking this mecanism in create_vm is more \"ops intuitive\" , and the disk_ids hint from Bosh Director would be usefull.\nIf not, this mechanism could be plugged @attach_disk time\n. @cppforlife yes disk_cids would be enough, if bosh director feeds it, which it doesnt (it was my initial point. Is there a configuration to bosh director to enable disk_id hint ? )\n. Nice feature  !!\nWell, hope we will have http proxy support for poor corporate bosh ops ;-)\n. I did previously open an issue on this point (maybe in a wrong location)\nhttps://github.com/cloudfoundry-attic/bosh_vcloud_cpi/issues/16\nHad no time to reproduce the issue, but its looking very similar (vcloud cpi, multi homed nics)\n. Just wondering : i see you added release/micro/softlayer.yml. Wiltou include the softlayer cpi in micro-bosh, or only available with bosh-init + softlayer external-cpi ?\n. Hello,\nyou probably should give more context to your issue (which iaas, version of cloudfoundry, do you generate your manifest with the generate_manifest spiff script ?). Your question seem more related to cloudfoundry than bosh. Maybe u could use gist to show your manifest and ask help  on the mailing list ?\nBest regards\n. Just met this with bosh-cli:  BOSH 1.3167.0\nbosh upload release ./releases/networking/networking-5.yml\nActing as user 'admin' on 'micro-bosh'\nDownloading from blobstore (id=772fcc8a-c4c7-4119-9722-c51a9140c47a)...\nBlobstore error: sha1 mismatch expected=28a1d88aba72724f0a4b0173a814c8c3bd36448c actual=f97f1ebad98e3d29ac2aed31c9df8381f25bea48\n. @voelzmo \nIm using multiple static NIC with the following combo:\nstemcell-version: 3363.14\nbosh-version: 260.4\nbosh-openstack-cpi-release-version: 31\nIm experiencing the same type of issue mentioned here. Despite setting the default gateway on private network, seems bosh ssh randomly chooses the secondary NIC ip and fails.\n```\n+-------------------------------------------------------+---------+----+---------+----------------+\n| Instance                                              | State   | AZ | VM Type | IPs            |\n+-------------------------------------------------------+---------+----+---------+----------------+\n| to-intranet/0 (335de3c9-9c77-427b-bf39-c7ffaaa7273a)* | running | z1 | small   | 192.168.99.34  |\n|                                                       |         |    |         | 10.228.175.126 |\n+-------------------------------------------------------+---------+----+---------+----------------+\n```\nI see this issue is still opened. Is there a stemcell correction or workaround we could apply ?\nthks\n. agreed didnt find either.\nPrepackaging is quite convenient, we use it for java/maven packaging to do the compilation @ bosh release creation time (bosh compile time is too late for maven resolution, and is usually not connected to internet). I think however it is deprecated / not recommanded ?\nHere the example i used for our own bosh releases (just a script named pre_packaging)\nhttps://github.com/cloudfoundry-community/norouter-release/blob/master/packages/norouter/pre_packaging\n. Hello,\nalso using vcloud with bosh (oldest version though)\nDid u try specifying the exact vapp_storage_profile, as found in vcloud director portal ?\nHere is our config :\nentities:\n      organization: Cube4PaasProd\n      virtual_datacenter: Apps-Dev\n      vapp_catalog: bosh\n      media_catalog: bosh\n      media_storage_profile: \"Standard Performances\"\n      vapp_storage_profile: \"Standard Performances\"\n      vm_metadata_key: cf-agent-env\n      description: vcd-cf\n. Hello,\nnothing obvious in your logs. I would guess theres is something wrong with the vcloud / vsphere configuration for independant disk. The bosh persistent disk is created with a vcloud independent disk, which you cant create from the vcloud portal to troubleshoot.\n. Unfortunatly, no easy way.\nThe vcloud API is restfull, but very complex to use (SOAP payload).\nI guess you cloud try creating a simple ruby / fog script (ie: create independent disk, attach to an exisiting vm)...\n. @jleavers thks, didnt know about these tools .\n. Hello,\ni guess these pres will help you grasp the value https://www.infoq.com/presentations/bosh-cloud-foundry\nThe aspects i like most :\n1 declarative complex architecture description (analog to puppet, plus inter-service links with bosh 2)\n2 end to end support, built in iaas vm provisionning\n3 health management / auto repair\n4 strong end to end versionning (bosh releases are self sufficient, iaas and os agnostic)\nclean externalized configuration\n5 best practices built-in. (for example: distinct disk for data and root disk, enabling smooth vm recreation)\nAgreed that the declarative approach is not adapted for strong imperative orchestration. Id say its usually an  error to put too much logic in deployment. Microservices architectures tend to have robust small components, able to deal with transient communication failure. Ie, no point in starting the db before the front end, provided the front is able to reconnect on the fly to the db.\n. @Freyert  Sure, Bosh creates and manages all our vms (few hundreds vms)\nYou have the concept of job/instance groups, and for one-off action, the errands. Bosh ssh is quite a nice feature (sets the private key on the fly on the target vms, let ssh access, then remove when logging off).\nAgreed the bootstrap sequence is bit complex, and logs could be improved.\nDont hesitate to ask for help on slack to bootstrap your deployment\nhttps://cloudfoundry.slack.com/messages/bosh/\n. @voelzmo thank you !\n. Just tested with our scality s3 provider\n```\nStarted creating new packages\n  Started creating new packages > openjdk/93c072891062f43e6a7fad1f3232064da90429af. Failed: Failed to download S3 object, code 1, output: '', error: '2016/10/06 20:55:57 performing operation get: NoSuchKey: The specified key does not exist.\n        status code: 404, request id:\n' (00:00:11)\nError 100: Failed to download S3 object, code 1, output: '', error: '2016/10/06 20:55:57 performing operation get: NoSuchKey: The specified key does not exist.\n        status code: 404, request id:\n```\nIm afraid it requires path_style bucket, which i can't set to fog through bosh spec properties ?\n. Hitting the same issue no one of our deployment (logsearch).\nbosh 261.4 stemcell 3421.4\nThis blocks bosh deploy.\n```\nDeploying\n\nDirector task 609483\nDeprecation: Ignoring cloud config. Manifest contains 'networks' section.\nStarted preparing deployment > Preparing deployment. Failed: Action Failed get_state: Getting processes status: Getting service status: Unmarshalling Monit status: read tcp 127.0.0.1:44350->127.0.0.1:2822: read: connection reset by peer (00:00:03)\nError 450001: Action Failed get_state: Getting processes status: Getting service status: Unmarshalling Monit status: read tcp 127.0.0.1:44350->127.0.0.1:2822: read: connection reset by peer\nTask 609483 error\nFor a more detailed error report, run: bosh task 609483 --debug\ncommand 'bosh -n --color -t https://192.168.116.158:25555 -d /tmp/bosh_manifest20170714-7-1goekym deploy' failed!\n```\nThis might be related to highload on target deployments vms. Is there a configurable setting (timeout) or workaround to force a correct deploy ?\n. had this issue several times. Uncomplete deployments seem to cause this.\n. @dpb587 @pivotal-jamil-shamy \nHello, i just tested with bosh 268.6, still having issues with cloud-config properties with credhub reference. Should i open another issue ?. ",
    "robbig2871": "for me it was: key  : , should be just key:\ni found it parsing spec file. \nirb\nrequire 'yaml'\nthing = YAML.load_file('jobs/vault/spec')\n. ",
    "mtekel": "+1, I had a missing :\nIt should really say syntax error, ideally point to the issue....\n. +1. Same problem with cf210 release. Luckily it goes away after a while...\n. Seems like I had resurrector running in the background...\n. Hi,\nhaving terse output on stemcells is useful whenever you want to automate some process around stemcell management. E.g. imagine you want to sync what stemcells you have in multiple bosh instances. And sync that against some remote stemcell repository (e.g. I add new stemcell to that repo, I want to have it added to all my boshes, I remove old stemcell from repo, I want it removed from all my bosh instances). \nIs bosh-cli planning to implement that kind of functionality? Or instead, is the intention of bosh-cli to never provide any machine parsable output, so that you can explore every single possible use-case? Or is the intention here to force people using ruby instead? In that case, you should remove --terse output from other CLI commands that currently have it...\n. Hi,\nthanks for your perspective. This terse switch is specifically useful to enable lots of these different, too uncommon to be implemented in client, scenarios. For example, in my case I would also check if the stemcells are in sync between environments (e.g. I'd implement a nagios check to do this - as stemcell uploads can fail, cloud providers can delete stemcells, bosh DBs can get corrupt/deleted by accident on different environments etc.; default --skip-if-exists won't help me here). \nWhen are you going to implement nagios support in the CLI? Do we need to get enough people to vote for it? Would you ever consider implementing a feature, that's not a final end use case/scenario, but enables lots of people implement their own specific, but very different niche use cases?\n. Where are those release notes? I can only see v206/3072 notes, but for 3074 there's nothing. Curiously, CF v218 release notes dissappeared recently...\n. thanks\n. 3202 release notes missing again. Is it that hard? Just make it mandatory for your release process. Changelog.MD has last update 2 months ago...\n. I have decided to not use that release, since no notes means there could be a breaking change or anything. It looks like the notes were forgotten. I simply wouldn't assume \"no notes\" = safe. Also, unannounced or disabled features = no reason to upgrade. Why make such releases?\n. Hi,\nfrom my (operator's) point of view, I am interested if that release contains something significant - e.g. a security fix, a new feature that I can use, a breaking change etc. When I see a \"final\" stable release, I expect it is that kind of release. You could perhaps use dev releases on git, or not tag something \"stable\" until you're ready to make it a proper, significant release. That way I can know that this is dev only and I'll simply ignore it. I definitely don't want to interfere with your development process, but usually it's good practice to separate \"production\" (final, stable etc.) from dev - even for the consumers you are testing with.\nThis time, I have spent time again wondering what that release was, trying to find release notes anywhere, comparing changes in git (there were actually no changes from 3200 and even from master, making me wonder even more what's different in that release), then downloading release, verifying that this is really the release it should be, computing sha1 hashes (see https://github.com/cloudfoundry/bosh/issues/1157).... just to realize that this is only dev after your responses.\nI see you have pulled that release from github, but on bosh.io, the \"latest official stemcell\" is still 3202...\nIMO bosh.io site should also contain release notes/changelog with each release, so that I don't have to come here every time...\n. Thanks, that --fix option would solve/help my use case, as I wouldn't have to delete the stemcell then...\n. When is that expected to happen?\n. Looks like this is implemented now. Whenever I get AWS rate exceeded errors, on the 2nd run BOSH only re-creates VMs it failed previously, instead of deleting everything and starting over...\n. Looks like this was released...\n. Thanks for update. Do you have any further information when that will become a part of standard releases? Based on what you described it should be just a small change that's very easy to implement...\n. Our microbosh is T2.medium (2 cores, 2.5GHz) with full CPU credits. CPU load on the VM is quite low. Only components that use CPU (from time to time) are AWS CPI (limited to a single core) and registry (again limited to single core) when it is broken. We have not seen anything similar prior to this version.\nNevertheless, I think even if the VM was a bit slow, things should only get slower and not fail (perhaps only on timeouts if it's really too slow).\n. We run 255.8 previously. But this also happens on clean deploy (from scratch - all new, including BOSH). We are currently deploying 32 VMs in 1 deployment.\n. This is the sanitized manifest we use for tests (it's the same as real except missing credentials and real IPs).\n. we're using AWS RDS. db.T2.medium instance which allows up to 136 connections. We're seeing around 17 max. At the time of the issue, there were 8 connections.\n. One correction: CPI version is 44. This removes CPI as potential cause of the issues as we have been running till now with that version...\n. We are using RDS and S3 as blobstore. UAA is not running on this VM or at all. The microbosh is deployed from the manifest I shared above. The issue happens specifically during creation or deletion of VMs. During this time, AWS CPI consumes 100% performance of a single core, the second core is at around 20% cpu load. The issue happens exactly at the time when the \"NATS error\" is logged in the worker error log.\nWe use 32 threads and when we run deploy/delete it starts creating/deleting 32 VMs in parallel. But this was fine with 255.8 BOSH and the only issues we had with that previously was with hitting AWS API throttling, which is now solved.\n. Maybe you can try severing the worker connection to NATS or restarting the NATS server during VM creation/deletion...\n. Ok, so restarting NATS doesn't cause it....\n. We deployed new 3262.14 (or v257.14) on the same VM (t2.medium - 2 cpus, 4G ram) and it works fine. The cpu load is still high (mainly from aws-cpi - which creates 90% of all CPU load during normal deployment), but deployment doesn't break. Before it was very consistent and would break almost every time (say 90% failure rate), now it seems to just work...\nI will close this issue and re-open if we have any new problems.\n. Hi,\nI don't have the logs from the time of the initial description. I will check again if I can reproduce it and provide full logs then. This all was/will be on BOSH 255.8. Maybe you have changed something how this command works/behaves since then, the logs might be a bit less useful then.\n. Hi,\nI have checked again. Find the logs attached - they also contain CLI (1.3202.0) and BOSH (1.3215.3.0) version details. Initially I have seen some issues with PSQL (there's connection issue in debug log). Then PSQL complains some items can't be removed from DB as they have dependencies. But even in the end, when the errors go away, it takes multiple successful bosh cleanup --all runs to really remove all the things. Altogether, it took me 4 tasks to clean up everything...\n. Some issues attaching zip, so plaintext files instead:\n363.debug.txt\n363.txt\n364.debug.txt\n364.txt\n365.debug.txt\n365.txt\n366.debug.txt\n366.txt\n367.debug.txt\n367.txt\nbosh.txt\n. With the latest bosh (v257.14, also known (and reported by BOSH) as 3262.14 - you should only have one version number scheme) this seems to be fixed. It cleans up everything in one pass. Will reopen if I see the issue again.\n. Looks like 5.2.5 was released on 29/03/2011... making it 5 years and 3 months old...\n. That comment about the future was made 2 years ago. The future is now!\nIt would be good if we could at least do some compatible update. I am not sure, but perhaps for some OS version, fixes of monit might have been backported to previous monit versions? I myself have experienced the 90s/90x restarts issue, which was not nice.\nRegarding healthchecks - you now also use consul. And for consul you define healthchecks (if you want it to dynamically update service availability in its store/DNS). It would be great if monit and consul could use the same healthchecks.\nFrom other comments it seems like for some reason monit has been 'frozen' and there's no way/process/CI for you to upgrade it easily. I wonder what's behind this decision, as this seems to be quite important component wrt what BOSH does...\n. Being able to specify URL + SHA would be great. This would enable, for example, specifying custom built stemcells which you can host e.g. internally.\n. Indeed this is now present in the recent stemcells.\n. ",
    "dthaluru": "I am also having similar issue. +1\n. I am also having similar issue. +1\n. Yes I was not able to build stem cell on my Mac but AWS works fine.\n. Yes I was not able to build stem cell on my Mac but AWS works fine.\n. I was able to fix this issue. When ever I am trying from VPN network I have above issue other wise it works. I don't know the exact root cause, but it looks this way.   \n. I was able to fix this issue. When ever I am trying from VPN network I have above issue other wise it works. I don't know the exact root cause, but it looks this way.   \n. Sure\n. Sure\n. Updated document how to build Photon stemcell.\n. I will send PR To dev . Closing this PR.\n. Signed-off-by: dthaluru@vmware.com\n. @mariash  I have revereted the changes you suggested me. \n. Okay I can revert this change. Previous version of Photon used to come with grub and grub2 packages. To avoid installing grub package I have changed the order. As latest version TP2 of Photon only comes with grub2, so reverting change should not affect building Photon stemcell.\n. With escape character it is not able to change password in Photon. I verified in Fedora and Ubuntu with out escape character and it works. Let me know if I have to revert this change.\n. I tried changing password manually with escape character on Ubuntu trusty and it did not work.  Looks like this code is never seemed to be working correctly. \n. I am wrong. I have tested again with escape character, it seems to be working. I am going to revert this change.\n. ",
    "dwu-pivotal": "Yes I was on my local ubuntu. In AWS, everything works fine. \n. yes, that's true... \n. ",
    "schwarzmx": "Sure, I haven't touched the bosh code before; what's the fastest way to get up to speed to fix this? \n. ",
    "hochm": "JamesClonk is member of the Swisscom Team / see signed ccla; \n. @drnic I thought that they should just add the ccla-company-org group ? and all members are directly in the membership-list\n. ",
    "Jam-Lin": "yes, i found that the manifest can config the key_name. thank you all the same.\n. yes, i found that the manifest can config the key_name. thank you all the same.\n. ",
    "bobhlo": "Not sure what caused this, but we found the lock in redis and deleted it:\n```\n/var/vcap/packages/redis/bin/redis-cli -p 25255 -a PASSWORD\nredis 127.0.0.1:25255> keys \"lock:*\"\n1) \"lock:deployment:cf-r2-logsearch\"\nredis 127.0.0.1:25255> del lock:deployment:cf-r2-logsearch\n(integer) 1\n```\n. Work is done on director, BTW.\n. ",
    "AbelHu": "Sure. I will provide more details next week after I come back to work.\nSent by Android Outlook\nOn Wed, May 20, 2015 at 10:27 AM -0700, \"Dmitriy Kalinin\" notifications@github.com<mailto:notifications@github.com> wrote:\nCould you provide a bit more info why do we need to make these changes?\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/828#issuecomment-103968103.\n. Issue 1:\nNeed to add below line in Line 97 in bosh-stemcell/Vagrantfile\n( apt-get install -y cmake pkg-config )\n1) Below error is thrown without \"cmake\"\n==> remote: Installing rubocop 0.31.0\n==> remote: Gem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension.\n==> remote:\n==> remote:         /usr/local/rbenv/versions/1.9.3-p448/bin/ruby extconf.rb\n==> remote: checking for gmake... no\n==> remote: checking for make... yes\n==> remote: checking for cmake... no\n==> remote: ERROR: CMake is required to build Rugged.\n==> remote: * extconf.rb failed *\n==> remote: Could not create Makefile due to some reason, probably lack of\n==> remote: necessary libraries and/or headers.  Check the mkmf.log file for more\n==> remote: details.  You may need configuration options.\n==> remote:\n==> remote: Provided configuration options:\n==> remote:     --with-opt-dir\n==> remote:     --without-opt-dir\n==> remote:     --with-opt-include\n==> remote:     --without-opt-include=${opt-dir}/include\n==> remote:     --with-opt-lib\n==> remote:     --without-opt-lib=${opt-dir}/lib\n==> remote:     --with-make-prog\n==> remote:     --without-make-prog\n==> remote:     --srcdir=.\n==> remote:     --curdir\n==> remote:     --ruby=/usr/local/rbenv/versions/1.9.3-p448/bin/ruby\n==> remote:     --use-system-libraries\n==> remote:\n==> remote:\n==> remote: Gem files will remain installed in /usr/local/rbenv/versions/1.9.3-p                 448/lib/ruby/gems/1.9.1/gems/rugged-0.22.2 for inspection.\n==> remote: Results logged to /usr/local/rbenv/versions/1.9.3-p448/lib/ruby/gems                 /1.9.1/gems/rugged-0.22.2/ext/rugged/gem_make.out\n==> remote: An error occurred while installing rugged (0.22.2), and Bundler cannot continue.\n==> remote: Make sure that gem install rugged -v '0.22.2' succeeds before bundling.\nThe SSH command responded with a non-zero exit status. Vagrant\nassumes that this means the command failed. The output for this command\nshould be in the log above. Please read the output to determine what\nwent wrong.\n2) Below error is thrown without pkg-config\nGem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension.\n/usr/local/rbenv/versions/1.9.3-p448/bin/ruby extconf.rb\nchecking for gmake... no\nchecking for make... yes\nchecking for cmake... yes\nchecking for pkg-config... no\nERROR: pkg-config is required to build Rugged.\n* extconf.rb failed *\nIssue 2:\nvagransh ssh remote and execute command \"bundle exec rake stemcell:build_os_image[ubuntu,trusty,/tmp/ubuntu_base_image.tgz]\". Below error is thrown.\n/bosh$ bundle exec rake stemcell:build_os_image[ubuntu,trusty,/tmp/ubuntu_base_image.tgz]\nrake aborted!\nNameError: uninitialized constant Fog::AWS::CredentialFetcher\n/bosh/bosh-dev/lib/bosh/dev/upload_adapter.rb:1:in require'\n/bosh/bosh-dev/lib/bosh/dev/upload_adapter.rb:1:in'\n/bosh/bosh-dev/lib/bosh/dev/build.rb:5:in require'\n/bosh/bosh-dev/lib/bosh/dev/build.rb:5:in'\n/bosh/bosh-dev/lib/bosh/dev/bat_helper.rb:1:in require'\n/bosh/bosh-dev/lib/bosh/dev/bat_helper.rb:1:in'\n/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:4:in require'\n/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:4:in'\n(See full trace by running task with --trace)\n. Revert Gemfile.lock and it works.\n. Please reference https://github.com/cloudfoundry/bosh/blob/master/bosh-registry/lib/bosh/registry/api_controller.rb#L21-L22\n. From my understanding, only when the registry client connects to the server without username/password, the registry server will check the client's IP address.\n. I think your suggestion is good. I will add the default instance_ips in next submit.\n. @cppforlife Thanks for the reminder. We will do it.\n. If no empty line, the last line in the final /etc/sudoers will be \"sudoers ALL=(ALL) NOPASSWD: ALL#includedir /etc/sudoers.d\"\nIt will prompts you need to input the password for the user vcap when using 'sudo su'.\nBest Regards,\nAbel HU\n? 2015?10?23??00:21?Nader Ziada notifications@github.com<mailto:notifications@github.com> ???\nWhy is the empty line needed in the sudoers file? is it causing a problem?\n\nReply to this email directly or view it on GitHubhttps://na01.safelinks.protection.outlook.com/?url=https%3a%2f%2fgithub.com%2fcloudfoundry%2fbosh%2fpull%2f992%23issuecomment-150278677&data=01%7c01%7cabelch%40064d.mgd.microsoft.com%7cc3c4a096a26747ef85f908d2dafcd22d%7c72f988bf86f141af91ab2d7cd011db47%7c1&sdata=mO6qLdfCiGeAetZE%2ff3Ew3o7JHarF6bp9wKwjfo4LUs%3d.\n. @cppforlife New WALA supports new protocols on Azure. It also supports AzureStack.\n. The old stemcell still works now but in future it can not work after the protocols are switched.\nBest Regards,\nAbel HU\n? 2016?3?3??08:59?Dmitriy Kalinin notifications@github.com<mailto:notifications@github.com> ???\nDoes that mean older stemcells will stop working because they dont support newer protocol?\n\nReply to this email directly or view it on GitHubhttps://github.com/cloudfoundry/bosh/pull/1153#issuecomment-191520095.\n. @cppforlife Azure still supports the old protocols in future. But AzureStack only supports new protocols. In order to use the same stemcell both on Azure and AzureStack, we need to update WALA to 2.1.3.\n. @zaksoup I have changed my profile to set my email address to public. I will add the verification for 'DevicePathResolutionType' and 'CreatePartitionIfNoEphemeralDisk' in the case 'installed by bosh_azure_agent_settings' in bosh-stemcell/spec/stemcells/ubuntu_trusty_spec.rb.\n. @monkeyherder I have made my membership in Microsoft Azure public. I also updated the code as you said. Move the test case to bosh-stemcell/spec/stemcells/azure_spec.rb.\n. Please do not merge this PR. It needs to be updated. Thanks.\n. @gossion Could you update your PR for the conflicts?. @ljfranklin This will cause the VM cannot be provisioned successfully when multiple NICs are enabled on Azure. It will cause the BATs failure. Please see below logs. It is a block issue when we are enabling BATs against centos stemcell in Azure CPI pipeline.\n```\nFailures:\n1) network configuration when using manual networking deploys multiple manual networks\n     Failure/Error: expect(bosh('deploy')).to succeed\n     Bosh::Exec::Error:\n       command 'bundle exec bosh --non-interactive -P 1 --config /tmp/bosh_config20170221-362-1h9q7to --user azureuser --password Binxia5537 deploy --no-redact 2>&1' failed with exit code 1\n     # ./lib/bat/bosh_runner.rb:19:in bosh'\n     # ./lib/bat/bosh_helper.rb:14:inbosh'\n     # ./spec/system/network_configuration_spec.rb:79:in `block (3 levels) in '\nFinished in 247 minutes 56 seconds (files took 0.21033 seconds to load)\n82 examples, 1 failure, 1 pending\n``. bosh-init v0.0.101 works with the stemcell v3363.1. Thanks, @cppforlife . Will have a try to addstrace. @dpb587-pivotal Now @bingosummer is tracking this issue. He may be able to answer your question.. Execute the commandbin/bosh-registry -c reg.cfg`\nreg.cfg\n```\n---\nloglevel: debug\n\nhttp:\n  port: 25695\n  user: admin\n  password: admin\n\ndb:\n  database: \"sqlite:///:memory:\"\n  max_connections: 32\n  pool_timeout: 10\n  adapter: sqlite\n\n``. Azure CPI has naming rules for disk names. Please see the rules [here](https://github.com/cloudfoundry-incubator/bosh-azure-cpi-release/blob/master/src/bosh_azure_cpi/lib/cloud/azure/disk_id.rb). . @sauravmndl It seems like that you used an invalid cachingxxxxxxxxxx. Please see the code [here](https://github.com/cloudfoundry-incubator/bosh-azure-cpi-release/blob/1257c994ec21ae427849e04899fcdee89c664ba8/src/bosh_azure_cpi/lib/cloud/azure/helpers.rb#L177-L181).\nvalid_caching = ['None', 'ReadOnly', 'ReadWrite']. @sauravmndl From the disk namedisk_name:bosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71ae;caching:None;resource_group_name:rg-sample-azure-dev1in your latest logs, it shows that you are using **managed disks** in V2 format. If so, you should be able to find the managed diskbosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71aeunder the resource grouprg-sample-azure-dev1in [Azure portal](https://ms.portal.azure.com/). Please confirm it at first.. @sauravmndl It seems like that Azure CPI has attached this disk to the VM (I think you can double confirm this in Azure portal) and the error was thrown by bosh-agent. I suggest you to login to the VM, check attached disks bylsblkand check the disks' information in/var/vcap/bosh/settings.json.. @sauravmndl From/var/vcap/bosh/setings.json, it seems like that Azure CPI did have attached the data disk successfully.\n1. Did you check whether the disk is attached to the VM in Azure portal?\n2. Could you see sdd bysudo fdisk -l` inside the VM?\nIf both are yes, maybe bosh-agent failed to format the new disk. You should be able to find some errors in bosh-agent's logs /var/vcap/bosh/log/* inside the VM.\n. It is because that Windows Azure Linux agent needs to access network before generating CustomData. bosh-agent will read CustomData before configuring network. So we have to set this.\n. We have set use_dhcp in Azure CPI https://github.com/Azure/bosh-azure-cpi-release/blob/master/src/bosh_azure_cpi/lib/cloud/azure/cloud.rb#L416. I will remove this from agent.json as AWS did https://github.com/cloudfoundry/bosh/commit/2f95fd89b8bbb1edad8e9e52d6f80d5996acd629.\n. Done\n. ",
    "vlerenc": "Please see proposal https://github.com/cloudfoundry/bosh/pull/837.\n. Hello @cppforlife,\nI believe you could come up with some automatism to handle multiple network adapters, but this will probably not cover the case with dynamic/elastic IPs of which only the client, but not MicroBosh knows of. For the client it is important that the response IP matches the DNS lookup request IP (via UDP).\nTherefore and because it is a less complex and more general solution (in line with PowerDNS), I would rather stick to the PowerDNS settings and just expose them.\nCheers, Vedran\n. Hi @cppforlife,\n@voelzmo helped me understand your proposal and sure, you are the expert. Such a solution would definitely also solve the issue.\nOn the other hand, personally I would rather tend to stick to a direct mapping. Less headache later, if assumptions break down.\nSo what do you propose? Is this PR OK (pro: most direct way/mapping and least complex) or should we prepare a more elaborated proposal where we extract the IP automatically based on the given named network (pro: no explicit IP, but network name only)?\nBest regards, Vedran\n. Ah, sounds good (recreate only focusing on the VM) and thank you for the hint with the log! I missed that somehow.\n. ",
    "xingzhou": "updated the PR based on @cppforlife 's comments\n. @mariash and @cppforlife, have resubmitted the PR, for functional test, we found the s3_spec is out of date, we made some additional changes to this spec to make the test cases pass, FYI.\n. To @mariash , there is an rubocop violation:\n\nit seems related to the fix of story [103642514], are you going to fix it?\n. Thanks @mariash , we have rebased and resubmitted the PR\n. @wcamarao & @cunnie\nLong time no see:), this is Tom from IBM, thanks for your review:)\nfor the options comment, you will help to modify the code and merge the PR finally, am I understanding correctly? hehe, thanks for your time:)\n. yeap, a bug, fixing it now\n. @mariash, at present, we can not prevent user from editing the startup menu. In this situation, we generated a random password which we do not even know what the password exactly is, under this situation, the start up menu will be protected.\n. @mariash, \n(1). grub-md5-crypt is there when grub V1 is installed. in CentOS 7 system, we only install grub V2, which will use another method(edit the 00_header config file), so in CentOS 7, grub-md5-cyprt is not there.\n(2). I don't catch you on the comment, would you please make some detail description? thanks\n(3). for system installed grub V2, the grub config file is /boot/grub2/grub.cfg not /boot/grub/grub.conf, so in this case, we don't chmod/chown grub.conf \n. @dk, the blobstore client like dav-blobstore-client will not throw NotFound exception, instead, it will generally throw out a BlobstoreError exception and saying that \"Could not delete object\", from this generic error, we can not tell if this error is because of object not found or object deletion errors\n. @dk, please see my comments above,thx\n. @dk, yes, but one thing is we need to change all kinds of blobstore clients(dav, local, s3) to raise NotFound error instead of BlobstoreError, this may introduce more changes, if you think is ok, we will change the Blobstore client then\n. sure, will resubmit this then\n. @mariash, changes noticed\n. we will fix this and submit a PR later\n. @mariash, changes noticed\n. @cppforlife, according to the changes made by Maria, this method will not be used anymore, has already deleted in the code\n. @mariash, based on our understanding, if we provide --fix option together with --name & --version, we will ignore the check of whether the release is already uploaded, so we don't check this in the test\n. no specific reasons, just want to confirm to use certain version of ntp, do you think we need to change it?\n. ",
    "kennetham": "@uzzz Right, I fixed that. It is working now, but more errors popping up.\n. @zaksoup @jpalermo would you mind directing me to the right team or if you could help me resolve this issue.\nI would like to highlight to the team,\nhttp://stackoverflow.com/questions/13980440/deploying-cloud-foundry-with-bosh-what-does-a-bosh-delete-deployment-clean-u\nIt seems like I am facing this issue right now. When I do a bosh delete deployment <deployment_name> it deletes the VMs but my compilation history still exists. Apparently it continues the compilation from where it last left off rather than a new compilation and new VMs. The same issue as were highlighted in StackOverflow, I am receiving the same error, the same procedure of running bosh deploy a couple of times after it failed, as well as deleting my deployments and rerunning it again.\nThe current deployment using OpenStack CPI.\nIf this was supposed to be highlighted back in 2013, but it wasn't, here it is. Please raise a fix for this.\n. @jpalermo I did a bosh cloudcheck and bosh cck which I believe is the same. I did not have any bad state that showed up though. I see. I have been getting countless of errors with the limited resources I supposed.\n. @cppforlife : Hope you can assist me here with these errors. I am not sure if this should be common or uncommon. I have been running bosh deploy for almost 20 over times just to deploy and yet unsuccessful. It could be due to the limitation of resources we are testing with.\nFirst, I tried to bosh deploy and the log as shown below.\n``` log\n[root@vmstack-01 cf-release]# bosh deploy\nProcessing deployment manifest\nGetting deployment properties from director...\nCompiling deployment manifest...\nPlease review all changes carefully\nDeploying\nDeployment name: cf-deployment.yml'\nDirector name:microbosh-centos'\nAre you sure you want to deploy? (type 'yes' to continue): yes\nDirector task 80\n  Started unknown\n  Started unknown > Binding deployment. Done (00:00:00)\nStarted preparing deployment\n  Started preparing deployment > Binding releases. Done (00:00:00)\n  Started preparing deployment > Binding existing deployment. Done (00:00:00)\n  Started preparing deployment > Binding resource pools. Done (00:00:00)\n  Started preparing deployment > Binding stemcells. Done (00:00:00)\n  Started preparing deployment > Binding templates. Done (00:00:00)\n  Started preparing deployment > Binding properties. Done (00:00:00)\n  Started preparing deployment > Binding unallocated VMs. Done (00:00:00)\n  Started preparing deployment > Binding instance networks. Done (00:00:00)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:01)\nStarted preparing dns > Binding DNS. Done (00:00:00)\nStarted creating bound missing vms\n  Started creating bound missing vms > small_z1/0\n  Started creating bound missing vms > small_z1/1\n  Started creating bound missing vms > medium_z1/0\n     Done creating bound missing vms > small_z1/0 (00:02:17)\n  Started creating bound missing vms > medium_z1/1. Failed: No route to host - connect(2) for 10.x.x.x:8774 (Errno::EHOSTUNREACH) (00:00:00)\n     Done creating bound missing vms > small_z1/1 (00:05:07)\n     Done creating bound missing vms > medium_z1/0 (00:05:35)\nError 100: No route to host - connect(2) for 10.x.x.x:8774 (Errno::EHOSTUNREACH)\nTask 80 error\nFor a more detailed error report, run: bosh task 80 --debug\n```\nThen, here's the details of the VMs\n`` log\n[root@vmstack-01 cf-release]# bosh vms --details\nDeploymentcf-01'\nDirector task 81\nTask 81 done\n+-----------------+---------+---------------+----------------+--------------------------------------+--------------------------------------+--------------+\n| Job/index       | State   | Resource Pool | IPs            | CID                                  | Agent ID                             | Resurrection |\n+-----------------+---------+---------------+----------------+--------------------------------------+--------------------------------------+--------------+\n| unknown/unknown | running | small_z1      | 192.168.x.x | adfcf70f-b7bf-4fc4-97cb-36cef3851260 | 2f4ed267-51d2-47dc-b6b9-44ff0b4a9799 | active       |\n+-----------------+---------+---------------+----------------+--------------------------------------+--------------------------------------+--------------+\nVMs total: 1\n[root@vmstack-01 cf-release]# bosh ssh\nProcessing deployment manifest\n\nconsul_z1/0\nha_proxy_z1/0\nnats_z1/0\netcd_z1/0\nstats_z1/0\nnfs_z1/0\npostgres_z1/0\nuaa_z1/0\nlogin_z1/0\napi_z1/0\nclock_global/0\napi_worker_z1/0\nhm9000_z1/0\nrunner_z1/0\nloggregator_z1/0\nloggregator_trafficcontroller_z1/0\nrouter_z1/0\nChoose an instance:\n```\n\nHere is my bosh cloudcheck or bosh cck result. Everything is showing OK.\n``` log\n[root@vmstack-01 cf-release]# bosh cloudcheck\nPerforming cloud check...\nProcessing deployment manifest\nDirector task 84\n  Started scanning 6 vms\n  Started scanning 6 vms > Checking VM states. Done (00:00:00)\n  Started scanning 6 vms > 6 OK, 0 unresponsive, 0 missing, 0 unbound, 0 out of sync. Done (00:00:00)\n     Done scanning 6 vms (00:00:00)\nStarted scanning 0 persistent disks\n  Started scanning 0 persistent disks > Looking for inactive disks. Done (00:00:00)\n  Started scanning 0 persistent disks > 0 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 0 persistent disks (00:00:00)\nTask 84 done\nStarted         2015-06-17 23:58:32 UTC\nFinished        2015-06-17 23:58:32 UTC\nDuration        00:00:00\nScan is complete, checking if any problems found...\nNo problems found\n```\nThis was what I got back. However I am pretty sure the compilation was completed not the deployment, so it puzzles me.\n. @cppforlife This is going to be very long. For security reasons, some of the details have been masked.\nhttps://gist.github.com/kennetham/5ccac117f2c1d8fb0da5\n. @cppforlife I tried that too. Unfortunately no as well. Am I supposed to explicitly configure a floating IP or so?\n. Can I see your manifest.yml? I have recently been deploying to Openstack as well, had a few similar instances where such errors pop up, let me see if I can help you out.\n. @springzcl can I confirm that the yml file that you sent me above are properly indented on your side, the yml have very strict settings to that.\nYour yml file looks correct. The error looks like a SSH issue.\nFirst, delete your SSH .pem keys on your computer. Remove any fingerprints in .ssh/known_hosts etc.\nSecond, delete the SSH Keypair on Openstack. Then, generate a new keypair with a different name.\nThird, reconfigure your yml file with your new keypair.\nNow try to deploy microbosh again. After the VM has been created, login to the VM instance,\n1. sudo su\n2. tail -f /var/log/auth.log\n3. From your host machine, ssh -i <ssh_key.pem> user@ip see if you can login.\nMonitor the status on your instance if it is returning you Permission denied to SSH. If it is so, then you keypair has an error.\nTo be safe, delete the bosh-deployments.yml in your host machine.\n. @springzcl Can I understand, did you follow through these steps? http://bosh.io/docs/deploy-microbosh-to-openstack.html and you got these errors?\nI assume your Openstack setup is without error and you have floating IPs made available. If that is so, could you remove the VM instance of your microbosh, remove the bosh-deployments.yml file, and try deploying again.\nI had similar errors as you did but with the agent. I can't explain the error as well because it's really odd where my SSH keys were rejected multiple times. I had to ensure that I create a name that was unique to that instance and not repeated, and I had to also remove all the unwanted files especially bosh-deployments.yml. Then it worked for me.\nAre you able to access your MicroBosh instance -> tail -f /var/log/auth.log and try to SSH from your host machine, see if it's an SSH issue. If you get a permission denied, then yes, your SSH key somehow was a mismatch and therefore unable to login.\n. @springzcl Hmm, this sounds like Postgre can't be connected. Could you get the log and show me from /var/vcap/bosh/log/current?\n. @springzcl did you add security groups? It seemed like it is not reaching a destination\n. Solved. No idea why, but the SSH Keypair didn't match. Regenerated a new key, fixed the issue.\n. Solved. No idea why, but the SSH Keypair didn't match. Regenerated a new key, fixed the issue.\n. ",
    "jpalermo": "The BOSH team is the right team to answer this. But here are some things which might help.\nI don't think that running bosh delete deployment is the correct response to that error. Instead, try running bosh cloudcheck and clean up any instances in a bad state, then try running bosh deploy again.\nDeleting the deployment won't destroy the release and any packages that have been compiled for it. If you really want to start everything over, you would need to delete the release and upload it again, but I'm not sure how that would help you.\n. I'm not familiar with what happens in OpenStack when you don't have enough resources, but it wouldn't surprise me if Failed: Timed out sendingget_state'` were caused by that.\n. Updated. Tests should pass again. Fixed a bug where run_nested_command would not exit if the nested command failed.\n. @mavenraven It looks like they have been merged as part of #969. They were rebased so the PR doesn't have the merge in it.\nI think the head of the code pull is here 6d2fb2bba0d6bb022ce6207cfc211a14e4e2456f\n. ",
    "springzcl": "thanks\n. @kennetham \n\nname: microbosh-openstack\nlogging:\n  level: DEBUG\nnetwork:\n  type: dynamic\n  vip: 10.2.19.132\n  cloud_properties:\n    net_id: 378d1fb5-c269-4836-a4b8-73fac84ec3f8\nresources:\n  persistent_disk: 16384\n  cloud_properties:\n    instance_type: v1.small\ncloud:\n  plugin: openstack\n  properties:\n    openstack:\n      auth_url: http://10.2.19.170:5000/v2.0\n      username: demo\n      api_key: DEMO_PASS\n      tenant: demo\n      region: regionOne\n      default_security_groups: [\"default\"]\n      default_key_name: microbosh\n      private_key: ~/microbosh.pem\napply_spec:\n  properties:\n    director:\n      max_threads: 3\n    hm:\n      resurrector_enabled: true\n    ntp:\n      - 0.north-america.pool.ntp.org\n      - 1.north-america.pool.ntp.org\n. @kennetham Here is the yml file. And now, if i change the stemcell to a old one, the error can pass.\nBut it fails the last step: waiting for the director\n. root@ubuntu:~/bosh-workspace/deployments/microbosh-openstack# cat bosh_micro_deploy.log \nI, [2015-06-29T12:08:47.803815 #19859] [0x85d324]  INFO -- : Loading existing deployment data from: /root/bosh-workspace/deployments/bosh-deployments.yml\nI, [2015-06-29T12:11:04.643297 #19859] [0x85d324]  INFO -- : bosh-registry is ready on port 25889\nI, [2015-06-29T12:11:44.687828 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:11:58.415972 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:12:00.583472 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:12:00.929749 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:12:01.243579 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:12:01.533574 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:13:21.904925 #19859] [detach_disk(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Detaching volume 6d8e5301-fb2a-4e78-a437-1dcc1af61049' from9bf93ff0-d6ca-4c8f-8025-3f55a1286c38'...\nI, [2015-06-29T12:13:27.752236 #19859] [detach_disk(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Volume 6d8e5301-fb2a-4e78-a437-1dcc1af61049' is now available, took 5.32873181s\nI, [2015-06-29T12:13:27.753372 #19859] [detach_disk(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Updating settings for server9bf93ff0-d6ca-4c8f-8025-3f55a1286c38'...\nI, [2015-06-29T12:13:28.291195 #19859] [delete_vm(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38)]  INFO -- : Deleting server 9bf93ff0-d6ca-4c8f-8025-3f55a1286c38'...\nI, [2015-06-29T12:13:34.045009 #19859] [delete_vm(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38)]  INFO -- : Server9bf93ff0-d6ca-4c8f-8025-3f55a1286c38' is now terminated, deleted, took 5.37126418s\nI, [2015-06-29T12:13:34.046443 #19859] [delete_vm(9bf93ff0-d6ca-4c8f-8025-3f55a1286c38)]  INFO -- : Deleting settings for server 9bf93ff0-d6ca-4c8f-8025-3f55a1286c38'...\nI, [2015-06-29T12:13:34.202659 #19859] [delete_stemcell(1560de40-7c13-4c34-b21d-557f168628ba)]  INFO -- : Deleting stemcell1560de40-7c13-4c34-b21d-557f168628ba'...\nI, [2015-06-29T12:13:34.813631 #19859] [delete_stemcell(1560de40-7c13-4c34-b21d-557f168628ba)]  INFO -- : Stemcell 1560de40-7c13-4c34-b21d-557f168628ba' is now deleted\nI, [2015-06-29T12:14:45.584226 #19859] [0x85d324]  INFO -- : Loading yaml from /tmp/d20150629-19859-11xkjzy/sc-20150629-19859-1kr80yk/stemcell.MF\nI, [2015-06-29T12:14:45.600262 #19859] [create_stemcell(/tmp/d20150629-19859-11xkjzy/sc-20150629-19859-1kr80yk/image...)]  INFO -- : Creating new image...\nI, [2015-06-29T12:14:45.603046 #19859] [create_stemcell(/tmp/d20150629-19859-11xkjzy/sc-20150629-19859-1kr80yk/image...)]  INFO -- : Extracting stemcell file to/tmp/d20150629-19859-11xkjzy/d20150629-19859-1rcs0su'...\nI, [2015-06-29T12:18:11.027601 #19859] [create_stemcell(/tmp/d20150629-19859-11xkjzy/sc-20150629-19859-1kr80yk/image...)]  INFO -- : Creating new image 53088579-b63d-4d96-8963-555bb0fdbd34'...\nI, [2015-06-29T12:18:11.165086 #19859] [create_stemcell(/tmp/d20150629-19859-11xkjzy/sc-20150629-19859-1kr80yk/image...)]  INFO -- : Image53088579-b63d-4d96-8963-555bb0fdbd34' is now active, took 0.13515493s\nI, [2015-06-29T12:18:14.408127 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Creating new server...\nI, [2015-06-29T12:18:15.431976 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Creating new server a0287fe5-9d0c-4616-a00c-3b64c731d864'...\nI, [2015-06-29T12:18:36.843165 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Servera0287fe5-9d0c-4616-a00c-3b64c731d864' is now active, took 21.40704397s\nI, [2015-06-29T12:18:36.844003 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Configuring network for server a0287fe5-9d0c-4616-a00c-3b64c731d864'...\nI, [2015-06-29T12:18:36.978389 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Associating servera0287fe5-9d0c-4616-a00c-3b64c731d864' with floating IP 10.2.19.165'\nI, [2015-06-29T12:18:37.797491 #19859] [create_vm(bm-fc30944f-1055-471b-a66d-0acd98163c4d, ...)]  INFO -- : Updating settings for servera0287fe5-9d0c-4616-a00c-3b64c731d864'...\nI, [2015-06-29T12:18:38.952982 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:18:39.267523 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:39.144373 #19859] [0x85d324]  INFO -- : Starting SSH session for port forwarding to vcap@10.2.19.165...\nI, [2015-06-29T12:20:42.070707 #19859] [0x85d324]  INFO -- : SSH forwarding for port 25889 started: OK\nI, [2015-06-29T12:20:42.075572 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:42.395648 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:43.441629 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:43.729717 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:44.769480 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:45.133358 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:46.177549 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:46.466722 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:47.496549 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:47.808121 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:48.837347 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:49.176308 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:50.205512 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:50.498562 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:51.529266 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:51.826303 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:52.858764 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:53.340803 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:54.369061 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:54.643698 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:55.674385 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:55.964428 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:57.006160 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:57.307372 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:58.379722 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:58.656504 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:20:59.681283 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:20:59.976550 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:01.005538 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:01.320300 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:02.394650 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:02.879901 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:03.907573 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:04.216280 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:05.246472 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:05.533443 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:06.557378 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:06.859644 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:07.897663 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:08.348179 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:09.392258 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:09.672121 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:10.716240 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:11.018569 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:12.047734 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:12.368354 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:13.394440 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:13.683284 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:14.710086 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:15.009664 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:16.039920 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:16.332598 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:17.365122 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:17.668758 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:18.701694 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:19.008889 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:20.041727 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:20.359838 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:21.387106 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:22.092831 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:39.667401 #19859] [attach_disk(a0287fe5-9d0c-4616-a00c-3b64c731d864, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Attaching volume 6d8e5301-fb2a-4e78-a437-1dcc1af61049' to servera0287fe5-9d0c-4616-a00c-3b64c731d864'...\nI, [2015-06-29T12:21:39.901075 #19859] [attach_disk(a0287fe5-9d0c-4616-a00c-3b64c731d864, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Attaching volume 6d8e5301-fb2a-4e78-a437-1dcc1af61049' to servera0287fe5-9d0c-4616-a00c-3b64c731d864', device name is /dev/sdb'\nI, [2015-06-29T12:21:45.648005 #19859] [attach_disk(a0287fe5-9d0c-4616-a00c-3b64c731d864, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Volume6d8e5301-fb2a-4e78-a437-1dcc1af61049' is now in-use, took 5.40211718s\nI, [2015-06-29T12:21:45.649098 #19859] [attach_disk(a0287fe5-9d0c-4616-a00c-3b64c731d864, 6d8e5301-fb2a-4e78-a437-1dcc1af61049)]  INFO -- : Updating settings for server `a0287fe5-9d0c-4616-a00c-3b64c731d864'...\nI, [2015-06-29T12:21:45.895817 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:46.198854 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:55.854718 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:56.222495 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:57.931952 #19859] [0x85d324]  INFO -- : discovering agent services ip\nI, [2015-06-29T12:21:58.227218 #19859] [0x85d324]  INFO -- : discovered bosh ip=192.168.1.91\nI, [2015-06-29T12:21:58.228809 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:58.501307 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:21:58.864368 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:21:59.212625 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:27:48.828200 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:27:49.189364 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\nI, [2015-06-29T12:27:50.381252 #19859] [0x85d324]  INFO -- : discovering client services ip\nI, [2015-06-29T12:27:50.681340 #19859] [0x85d324]  INFO -- : discovered bosh ip=10.2.19.165\n. @kennetham   i  tried it again.  Now the problem is that the deployment failed when it was waiting for the director. \nThe bosh_micro_deploy.log is that:\nD, [2015-06-29T15:11:28.931029 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:31.667626 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:33.712058 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:35.707232 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:37.371614 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:38.976151 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:40.534079 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:42.247917 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\nD, [2015-06-29T15:11:43.755347 #1691] [0xc91314] DEBUG -- : Waiting for director to be ready: #\n. Waiting for director to be ready:  Bosh::Deployer::DirectorGatewayError: Nginx has started but the application it is proxying to has not started yet.\n. ",
    "benmoss": "Looks like Travis was failing already, and the tests that are red don't seem to have anything to do with the changes here.\n. Duplicate of https://github.com/cloudfoundry/bosh-init/issues/41\n. Yep, we just ran into the same issue and we realized we didn't have BOSH_CLIENT / BOSH_CLIENT_SECRET in our environment for the director we were targeting.. ",
    "valeriap": "@maximilien, I've updated the issue and added the missing information. \nThanks.\n. @Amit-PivotalLabs \n+1 to the requirement in a different scenario\nConsider a database server shared among different client apps (like the postgres in cf bosh-lite that is shared among cc and uaa). Each client app would use its own database, user, and password. This information is currently duplicated in the server and in the clients.\nI would like to optionally consume it in the database and let each client app to provide it. \nIn this scenario the number of providers in undefined so I cannot specify a consume in the database server spec for each of them unless hardcoding the number of possible clients. It would be nice to have a way to consume the same link \"from\" more providers.\nI prefer this approach rather having the db providing this info for security. In this way only the specific app would have access to its data.. ",
    "evanfarrar": "I can't really reason about this failing build. Monit retry timeout test?\n. I can't really reason about this failing build. Monit retry timeout test?\n. ",
    "dpb587": "Okay, so it's safe to just commit the tgz into the assets directory then? It was only ~100 KB.\nIn theory the SR-IOV would be supported on CentOS images, as far as AWS is concerned. But it wasn't my intention to make it a global AWS setting since I'm only able to test and verify the Ubuntu images, and since I only added the newer ixgbevf onto Ubuntu . Is there a way to make it only an Ubuntu flag until somebody familiar with CentOS is able to look into adding/testing the support?\nIn regards to CLA, @mrdavidlaing commented; should be on file somewhere.\nIn regards to Travis saying test failures, I didn't realize there were more tests than already ran during build steps. I'll take a closer look at what it's doing and see if there's things I can fix.\n@mrdavidlaing, the deployments I'm testing this on don't really have enough detailed monitors for network performance to be able to add a fancy comparison here. And we don't really have a test focused on this area. So for now, I'd just trust results that other companies publish which are dedicated to comparing AWS SR-IOV vs not.\n. I rebased to the latest develop and fixed the failing test from my change. Let me know if there's anything else I can do to assist in merging.\n. I think this can be closed now? https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/commit/09ae08d3cf0622ad0a0fb61da6a1f026ddb9d24f\n. I appreciate the more detailed release notes of v255.4 - thanks!\n. Ah, that's a great alternative. Thanks!\n. Oops, I forgot to double check that. I'll verify that tonight.\nOn Monday, February 1, 2016, Maria Shaldibina notifications@github.com\nwrote:\n\nHey Danny, this is an old style diff. Are you using old cli? The new style\ndoes not differentiate version types.\nMaria\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1120#issuecomment-178020716.\n\n\nDanny Berger\nhttp://dpb587.me\n. Upgraded bosh_cli to 3184.1 and this is no longer happening (although more weirdness around null values). It'd be nice if cli would warn me if it's too far out of date with the director it's talking to.\n. The above happened with an older CLI which I didn't realize. I updated, but now I get new, odd behavior. It shows me the following on every deploy even though the manifest doesn't change. \nNow with a :sparkles:screenshot...\n\n```\n$ cat bosh.yml\nproperties:\n    consul:\n        agent_config:\n            advertise_addr: null\n            bootstrap_expect: null\n$ bosh download manifest\nproperties:\n  consul:\n    agent_config:\n      advertise_addr: \n      bootstrap_expect: \n``\n. Seems fixed as of ~3215.3\n. Personally, I agree as well - was surprised it was happening on the director and always redacting. For at least some manual deployments it would be nice if I could verify the diff - especially when interpolation/variables are involved in the manifest and I want to make sure they are being included as intended.\n. +1 for the--redact-propertiesapproach\n. Per offline discussion, the current behavior is expected and desired -cloud_properties` are not merged recursively. I'll reconsider how I configure VM extensions or try to come up with something more tangible to discuss.\n. On a multi-worker bosh the second deployment task will likely fail with \"unable to acquire deployment lock.\" It does seem like a valid, broken scenario on busy BOSHes though.\nStill seems like there might be timing issues if the /deployment endpoint starts the lock, but the worker may not pick up the task right away (might be busy working other concurrent deployments). And who would be responsible for the lock in the meantime - the controller responding to the task shouldn't keep it and the workers might not be ready for it yet.\nAnother thing to note is that the diff call is completely independent from the command to deploy a manifest. Another more involved, possibly more correct option might be for the director diff call to respond back with a hash of the current manifest which then gets forwarded to the actual deploy task. If the deployment has changed by the time it's ready to deploy, it errors.\nJust some thoughts. Not sure what @cppforlife thinks.\n. I also ran into this when redeploying a manifest on director v256.2 which previously worked on director v250. Fixed it by adding the unlisted property back to the release job's spec and uploading a new release.\n. To fix this, is there a particular method for hm figuring out if the instance is \"resurrectable\"? Would the fix be dependent on the vms/instances endpoint switch mentioned earlier this week?\n. @cppforlife, this story was about this issue as well, but sounds like it can be closed as fixed elsewhere?\n. Personally, I wish this was still public. As an open source repo, I think it's important to be able to refer people to what the builds are and what tests should look like. Particularly when contributors are interested in running tests locally and looking for a baseline. I also tend to trust repos more if they're able to show me their \"continuous\"ness, even if it may be a bit superficial.\nIf security is a concern for that, we should have a separate, super-restricted deployment for stemcells in a dedicated account.. Could we add support in the YAML/CLI to explicitly state a release URL reference as compiled?\nFrom CLI, it'd require a new version which knows to inspect-release to check if the compiled version already appears. I suppose a smarter director endpoint could be added? I could imagine a potential bug/unfortunate behavior if a delete of the compiled release partially fails, leaving around a subset of packages, but the fix would be to remove the option, or retry deleting the release - either way, the user should have seen an earlier failure.\nbosh upload-release \\\n  --name=openvpn \\\n  --version=3.2.1 \\\n  --compiled=ubuntu-trust/3421.9 \\\n  https://...\nThe releases section could look similarly. The field would have to be updated whenever it's recompile's which is a slight shame, but that's the way it works for version already, so not a new argument.\nPerhaps compiled-for / exported as alternatives.. I've run into this, too. Wasn't sure the correct workaround and ended up patching director code to ignore the specific address where it would have raised. Was able to continue the deploy since it deleted the existing VM first. I don't recommend patching director code though :). I've run into this, too. Wasn't sure the correct workaround and ended up patching director code to ignore the specific address where it would have raised. Was able to continue the deploy since it deleted the existing VM first. I don't recommend patching director code though :). A couple years ago I couldn't find a good solution for this and ended up writing an external, scheduled task to regularly cleanup with the needed retention periods. I agree it would be nice if something around snapshot cleanup was built-in.. A couple years ago I couldn't find a good solution for this and ended up writing an external, scheduled task to regularly cleanup with the needed retention periods. I agree it would be nice if something around snapshot cleanup was built-in.. I've wondered if this functionality should be removed. Personally I avoid it because it introduces a third place where settings are coming from (in addition to manifest and spec defaults). I prefer the spec because it is more consolidated and static, so it can be rendered in docs (as opposed to having to parse or grok ERB).. I've wondered if this functionality should be removed. Personally I avoid it because it introduces a third place where settings are coming from (in addition to manifest and spec defaults). I prefer the spec because it is more consolidated and static, so it can be rendered in docs (as opposed to having to parse or grok ERB).. @cppforlife interesting. i tried, but failed. notice anything?. @cppforlife interesting. i tried, but failed. notice anything?. Switched to ubuntu:xenial and removed ppa:brightbox/ruby-ng and tested a couple things with it and it all seems to work well.\n```\nruby --version\nruby 2.3.1p112 (2016-04-26) [x86_64-linux-gnu]\n``. Both make sense to me, similar to how instance name and UUID are available for different use cases.. I changed this because the [images.create](https://github.com/aws/aws-sdk-ruby/blob/v1.60.2/lib/aws/ec2/image_collection.rb#L193) function wants to convert{'sdb':'ephemeral0'}Hash values, but we're giving it the fullBlockDeviceMappingtype.\n. I was getting the following error when I first tried usingmerge` on the same line. Probably some ruby n00b mistake, so I switched to this but I imagine there might be a more correct solution.\nNoMethodError:\n   undefined method `merge' for [{:device_name=>\"/dev/xvda\", :ebs=>{:snapshot_id=>\"id\"}}]:Array\n. The Ruby API for requesting spot requests seemed to only accept it in this format. It's also the closer-to-AWS-API-specs format, so it seemed more correct to change it than try hacky workarounds.\n. Also... I'm not sure how to properly mock this change for testing (aka, help?).\n. Fixed - thanks for the help.\n. Minor feedback/FYI... since this property is defaulted to a value in the job spec file, we don't need to duplicate the default value of 3 here (no second param needed) \u2013 helps avoid duplicate defaults which could get out of sync and cause confusion over time.\n. ",
    "camelpunch": "For the absolute noob, what do I do to get around this? Can I get the stemcell uploaded without using bosh upload stemcell?\n. Solved this for bosh-init by using the latest AWS CPI release, which includes the fix. Change bosh.yml lines 9 and 10 to use v=28 instead of v=27. Change sha1 to c7ce03393ebedd87a860dc609758ddb9654360fa.\n. We have already done this as a workaround. How would we specify AZs that an instance group should be balanced across using the single subnet syntax?\n. We have already done this as a workaround. How would we specify AZs that an instance group should be balanced across using the single subnet syntax?\n. Something bad happened. I'll start another PR.. Something bad happened. I'll start another PR.. Thanks @dpb587-pivotal !. Thanks @dpb587-pivotal !. ",
    "maryoush": "I spent some time figuring it out how to make it ... -  some nice description would help\n. ",
    "sslavic": "+1\nWas getting \"Please enter valid source and destination paths\" while actual issue was that the job index part was missing.\n. ",
    "geofffranks": "The docs for this really need fixing. Having only 1 api_worker_z1 node, I just tried to bosh scp, getting rather unhelpful output from error messages. \n``\n$ bosh scp --upload api_worker_z1 ~/.s3cfg /tmp\nActing as user 'concourse' on deployment 'cloudfoundry' on sandbox'\nExecuting file operations on job api_worker_z1\nTarget deployment isasv-sb-cloudfoundry'\nSetting up ssh artifacts\nDirector task 118619\nError 100: undefined method `each' for nil:NilClass\nTask 118619 error\nFailed to set up SSH: see task 118619 log for details\n```\nThe help message for scp was also not very useful at pointing me in the right direction. Looking up the docs change in #973 was the only way I could figure out the correct incantation to use.  Would be really nice to get this resolved.\n. @voelzmo @cppforlife What needs to happen to get this in a merge-able state?\n. +1\nWhat would it take to have bosh know which jobs were affected (via package updates, or job templates changing), and only restart the corresponding monit processes?\n. :+1: \n. Would is be possible to make the redacting behavior configurable via .bosh_config as well?\n\nOn Mar 3, 2016, at 12:44 PM, Dmitriy Kalinin notifications@github.com wrote:\nSounds like it's worth introducing an option to show redacted values e.g. bosh deploy --show-redacted or something like that.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/cloudfoundry/bosh/issues/1158#issuecomment-191881053.\n. Apparently this is a duplicate of #1159 \n. That doesn't allow users to know the vcap/root passwords after the fact, correct? That's the only downside I would see with that. Though, I only really ever need the vcap user/pass so I can get into a bosh director itself. I don't think I've ever needed to ssh into a VM without bosh being up, but perhaps someone might have a DR plan that required it for changing something quickly while the bosh director is offline.\n\nFor my immediate needs, the generate_vm_passwords setting should be fine, but this might be useful eventually? Or maybe Cloud Config makes that easier, since we can leverage spruce/spiff to templatize each resource pool and ensure they get the password overrides.\n\nOn Jun 20, 2016, at 12:22 PM, Dmitriy Kalinin notifications@github.com wrote:\nWill this do? https://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh&version=256.10#p=director.generate_vm_passwords https://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh&version=256.10#p=director.generate_vm_passwords\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/cloudfoundry/bosh/issues/1308#issuecomment-227192960, or mute the thread https://github.com/notifications/unsubscribe/AG2SUrN0rX7T_sJKZKxfSnsi9yZB3q9Bks5qNr5ZgaJpZM4I511L.\n. Works for me\nOn Jun 20, 2016, at 3:35 PM, Dmitriy Kalinin notifications@github.com wrote:\nI think if such configuration is necessary user_add addon https://bosh.io/docs/addons-common.html#misc-users can be used to add additional users.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/cloudfoundry/bosh/issues/1308#issuecomment-227245723, or mute the thread https://github.com/notifications/unsubscribe/AG2SUu-_clVlabh843dWfSK9OKpQIqM3ks5qNut4gaJpZM4I511L.\n. @dpb587 or @cppforlife is the pivotal tracker integration for this broken? Shouldn't I see a link to a ticket for this PR?\n. Thanks!\n\nSome background for testing - BOSH was behaving with the proxy settings for most everything (talking to the CPI for instance). However when doing bosh upload release or bosh upload stemcell using remote URLs, the content download was not using the proxy (though the upload to the blobstore after content was downloaded is using the proxy).\n. +1 ran into this today, it's still a problem.. Tried using the delete-vm command from the bosh2 cli, and that did not fail, but it also did not help my errand to be able to run. Also tried start and unignore with no luck.. @ronakbanka I believe you just need to specify migrated_from on the job. Keep the name the same, and set the migrated_from.az to the AZ you want the VM to be placed in when bosh recreates it.. Can this also be done for runtime-config? (Don't need to see that you're adding releases/jobs to all my VMs if it's not actually being included in this deployment). \"got a deployment lock\" meant \"got an error indicating the deployment was locked\". The locking task was the resurrector trying to bring back the machine that I ran stop --hard against. It did resurrect, but bosh still thought the job was stopped and couldn't do anything, until I manually updated the database to fix the detached state.. +1 on this. I'd like to have some functionality to provide an arbitrary list of links onto haproxy for various tcp backends (to migrate https://github.com/cloudfoundry-incubator/haproxy-boshrelease/blob/master/jobs/haproxy/spec#L152-L167) into links and out of manifests.. I'm also seeing this behavior due to the following other messages:\nhealth_monitor.log-I, [2017-04-27T16:11:58.438951 #52726]  INFO : [ALERT] Alert @ 2017-04-27 16:11:58 UTC, severity 1: process is not running\nhealth_monitor.log-I, [2017-04-27T16:11:58.439098 #52726]  INFO : (Event logger) notifying director about event: Alert @ 2017-04-27 16:11:58 UTC, severity 1: process is not running\nhealth_monitor.log:W, [2017-04-27T16:11:58.439553 #52726]  WARN : (Resurrector) notifying director to recreate unresponsive VM: warden-prometheus prometheus/05d41d5e-0afc-4894-8e0f-45b4c8260d0f\n(it appears a process is repeatedly crashing on that VM due to lack of disk space, but monit keeps restarting it.. yet this is trying to trigger a VM recreation?)\nI, [2017-04-27T16:06:20.072576 #8017]  INFO : [ALERT] Alert @ 2017-04-27 16:06:20 UTC, severity 4: Connection closed by 101.200.122.213 [preauth]\nI, [2017-04-27T16:06:20.072745 #8017]  INFO : (Event logger) notifying director about event: Alert @ 2017-04-27 16:06:20 UTC, severity 4: Connection closed by 101.200.122.213 [preauth]\nW, [2017-04-27T16:06:20.073258 #8017]  WARN : (Resurrector) notifying director to recreate unresponsive VM: eastus-jumpbox jumpbox/5b46a0c7-1724-438e-a560-eb4a94044672\n(an artifact of having a public IP). In my case it was my_link.p(\"property-that-links-might-provide\", p(\"property-from-spec-to-default-to\"), to be able to work around when some things providing a link  provide a property, and some don't.\n\nOn May 4, 2017, at 12:22 PM, Danny Berger notifications@github.com wrote:\nI've wondered if this functionality should be removed. Personally I avoid it because it introduces a third place where settings are coming from (in addition to manifest and spec defaults). I prefer the spec because it is more consolidated and static, so it can be rendered in docs (as opposed to having to parse or grok ERB).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/cloudfoundry/bosh/issues/1673#issuecomment-299236436, or mute the thread https://github.com/notifications/unsubscribe-auth/AG2SUhMVqWmFAZ1rqgF62MW_iFHHMigXks5r2fsrgaJpZM4NQtfO.\n\n\n. Ah. Didn\u2019t know about the possiblility of extra login prompts. Thanks!. That\u2019s what we ended up doing as a stop gap. No worries.\nSent from my iPhone\n\nOn Jun 13, 2018, at 6:14 PM, Dmitriy Kalinin notifications@github.com wrote:\n@geofffranks im not too hot about adding such feature as it would mean we would have to keep sensitive info in state file. you can already do something like this yourself but using bosh interpolate to generate manifest, saving it somewhere safe (dependent on your environment), and then using it with bosh create-env.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "linsun": "I struggled for 10+ mins to figure out the 'bosh scp' syntax and failed! Found this git issue, which helped me.  Totally agreed, it is too confusing to use 'bosh scp' based on current doc.\n. ",
    "wendorf": "No, curl or wget are fine. Closing.\n. It would also be great if, once this is fixed, the bosh-cli also populates osName when fetching deployment stemcells.. ",
    "mstrzele": "@amulyas Can you write here the answer from @drnic? I'm really curious if BOSH is able to tag the resources.\n. @amulyas Can you write here the answer from @drnic? I'm really curious if BOSH is able to tag the resources.\n. Still the same with 3094.\n```\n$ bosh ssh foo/0\n[WARNING] Loading the cli took 15.4 seconds, consider cleaning your gem environment\nActing as user 'admin' on deployment 'foo' on 'bar'\nEnter password (use it to sudo on remote host): **                                                                                                                                                                                                                                                                     \nTarget deployment is `foo'\nSetting up ssh artifacts\nDirector task 25\nTask 25 done\nStarting interactive shell on job foo/0\nNo ECDSA host key is known for 1.2.3.4 and you have requested strict checking.\nHost key verification failed.\nCleaning up ssh artifacts\nDirector task 26\nTask 26 queued\n$ bosh --version\n[WARNING] Loading the cli took 12.7 seconds, consider cleaning your gem environment\nBOSH 1.3094.0\n```\n. Yeah, I did it.\nI'm using Ruby 1.9.3 because it's default for Ubuntu 14.04 LTS. According to documentation, bosh_cli should have support for this version.\n. Yeah, I did it.\nI'm using Ruby 1.9.3 because it's default for Ubuntu 14.04 LTS. According to documentation, bosh_cli should have support for this version.\n. There's similar problem with fog-google gem:\nERROR:  Error installing bosh_cli:\n    fog-google requires Ruby version >= 2.0.\n. There's similar problem with fog-google gem:\nERROR:  Error installing bosh_cli:\n    fog-google requires Ruby version >= 2.0.\n. The prerequisites sections also should be updated.\nOn Ubuntu Trusty, the ruby2.0 and ruby2.0-dev packages need to be installed and changed to the default version by creating links in /usr/bin (there's a bug in Ubuntu for this).\nOn RHEL/CentOS there's no possibility to install Ruby 2.0 using Yum.\n. The prerequisites sections also should be updated.\nOn Ubuntu Trusty, the ruby2.0 and ruby2.0-dev packages need to be installed and changed to the default version by creating links in /usr/bin (there's a bug in Ubuntu for this).\nOn RHEL/CentOS there's no possibility to install Ruby 2.0 using Yum.\n. I still think, that changing Gemspec to include so called pessimistic version constraint is better approach. This is the one of common practices. \n. I still think, that changing Gemspec to include so called pessimistic version constraint is better approach. This is the one of common practices. \n. What about fog-google gem?\nERROR:  Error installing bosh_cli:\n    fog-google requires Ruby version >= 2.0.\nProbably other gems without pessimistic version constraint can cause this issue in the future.\n. ",
    "ssurenr": "+1\n. +1. I am facing this in AWS.\n. I guess this needs a fix in Bosh side as well\n. ",
    "liuxiaoxi2237": "thanks.\n. Thanks a lot for your help.\nFor contrib service, seems it require bosh 1.2778. But my bosh version is 1.3.\nDo you try  deploying the contrib service with other bosh version, and  does it works fine?( Since I do not want to build another bosh environment.)\nthanks,\nDavid\n. ",
    "bortoelnino": "@liuxiaoxi2237 - how did you fix this ? I'm seeing it too (on vSphere).\n. @liuxiaoxi2237 - how did you fix this ? I'm seeing it too (on vSphere).\n. @dpb587-pivotal Thanks! that worked (specifically setting  skip_cert_verify: true) \n. @dpb587-pivotal Thanks! that worked (specifically setting  skip_cert_verify: true) \n. ",
    "jvshahid": "Great, thanks!\n. ",
    "msschwartz": "The error was happening because the VM was renamed. boot_from_volume value is irrelevant. \n. ",
    "zhang-hua": "@cppforlife , resubmitted this PR as you suggested. Thanks!\n. @cppforlife , okay! will update it.\n. @cppforlife , okay! will update it.\n. @cppforlife , ok, misunderstand it just for --ps. will update it.\n. @cppforlife , good point, will fix it.\n. @cppforlife , will fix it.\n. @cppforlife , the line restrict 127.0.0.1 is used to allow unrestricted access from the localhost so that bosh-agent can query localhost by using ntpq to get time and offset.\n. ",
    "CAFxX": "Any chance oh having this pulled in?\n. @tedsuo understood, but then I have a few followup questions: what will the \"better cross platform interface\" be and when it is slated to be implemented? Also, assuming we emulate the behavior we need, what are the odds that our solution will keep working once this new interface comes around?\n. Sorry I missed some of @voelzmo questions in #1128, so let me answer here. Just to give some context, our need is based on the pre-existing naming conventions in effect on our internal IaaSes. These naming conventions are outside our control and we should follow them to be \"good citizens\" not just of the IaaS we're on, but of the whole ecosystem of inventory/monitoring/security tools we have internally.\n\nWhy would you want the IP in the VM name? Don't basically all tools show you name and IP side-by-side?\n\nThe naming convention is roughly as follows:\n<project-id>-<dash-separated-IP>-<type>-<vm-id>\nwhere <project-id> is the internal ID of project we are assigned, <dash-separated-IP> is something like 192-168-100-1, <type> is a string (can be considered a constant) and <vm-id> is basically free-form for the project to choose (in our case we would like to have \"instance group-index-guid\"\n\nWhat does role in your above example mean? Are you referring to a jobname, such as nats or runner, or is it something different?\n\nwe used to call \"role\" what is now known as \"instance group\"\n\nWould your naming scheme be the same across all deployments deployed with the same BOSH Director? In your example you include cf- as a prefix, so I assume this would be different for each deployment?\n\nIdeally it would need to change based on the deployment. (i.e. different deployments on the same director may have different prefixes)\n\nBOSH uses different names/identifiers for the VMs, so if you execute bosh vms or bosh instances in the commandline, you would see an entirely different list than you would see in your IaaS management UI. Is this an issue for your operators? How do they find the VM in the IaaS given an identifier from the BOSH commandline?\n\nThat is the reason why we would like to have \"instance group-index-guid\" in the <vm-id> part. IIRC we can also query using the VM metadata stored on vSphere/Openstack.. The travis CI seems broken as already noted in https://github.com/cloudfoundry/bosh/pull/1251#issuecomment-216736513\n. @cppforlife flakiness? meaning, failing makes?\n. > And just as an FYI, most teams that I've spoken with who have created their own releases include steps to either create an SSH user, or change the VCAP password so that they can SSH natively.\nYup that's we have also done.. so maybe something like this?\n\nagent: stop all jobs on the VM\nagent: (if shrinking) unmount and shrink the FS\nagent/CPI: shutdown VM\nCPI: resize volume\nCPI: start VM\nagent: (if extending) extend the FS\nagent: start all jobs on the VM\n\nit may be less than ideal (requires a VM restart in all cases), but it should still solve the major pain points listed in my first message. Btw, related to @cppforlife comment about vSphere (https://github.com/cloudfoundry/bosh/issues/1569#issuecomment-277114859) it seems that the 2TB limitation is gone starting from ESXi 6.5. @voelzmo about the 6 hours rate limit on EBS, my understanding is that it is per-volume. Coupled with the fact you are still going to have to support a fallback for things like shrinking, or for the IaaSes that don't support volume extension, it seems to me it would be still valuable to have support for extending EBS volumes (unless you want to resize the same disks multiple times per day - in which case #2021 may come in handy anyway).. > We hope that this future feature will provide the ability to have a more complex deploy process.\nIs there a proposal I can comment on, to make sure this doesn't get lost? @jfmyers9 . ",
    "sykesm": "@tedsuo, @cppforlife: Our environment is hitting a number of bugs related to monit that appear to have been fixed in more recent levels. We have no interest in using new features but we do need to stop working around the bugs.\nRight now, monit is at the core of what bosh does, so, without a proposal or replacement in hand, why should this pull request get stalled?\nI'd also like to point out that PR #743 has been open for more than seven months without integration. That's quite troubling given we're talking about such a simple change.\n. @cppforlife: I'm following up on this side to get the precise problems.\nAs to effort and complexity, I saw the story was a 1 point so I made an assumption.\n. @cppforlife: The problems we hit are either the \"check process\" bug that was fixed in 5.2.5 or the bug fixed in 5.5 that was described as:\n\n\nIn the case that the process start/restart execution failed, monit kept \"Execution failed\" flag even if the process was recovered later (for example it was starting slowly or manually recovered).\n\n\nIn either case, an update is needed to resolve the issue. Ideally we'd see something at 5.5 or better.\n@CAFxX: Apologies for hijacking your PR.\n. The change isn't in included with the tag used for v257.3 but it is present in the v257.x branch.\nBased on your comments, it sounds like dynamic networks will now require bosh DNS or the bosh management of /etc/hosts (when it's ready). If that's the case, it would be good to update your documentation on bosh.io.\nThanks.\n. I'm having the same issue with bosh and dynamic networks.  In my case, I'm using dynamic networks but bosh knows the IP addresses associated with the jobs and I do not have BOSH DNS enabled.  In a situation like this, I don't understand why links would generate DNS names instead of using the IP. Without BOSH DNS, there's little hope of resolving the addresses out of band - even if it is possible.\nTo me, this is behavior that should be configurable at the director level. There should be a small bit of configuration that says to prefer IP addresses (when known) to DNS names - even when using a dynamic network.. I'll also add that the user experience here is pretty awful. In the case of concourse, my deployment was successful but ATC never started because the link to the postgres pointed to a black hole.\n/cc @vito . ",
    "mattcui": "@CAFxX Did you verify if all jobs work well (like CC job) after upgrading to monit 5.14 or later version? Thanks.\n. @cppforlife We ran \"apt-get -y install --install-recommends linux-generic-ltd-vivid\" to upgrade to the latest level of 3.19.0.x for Ubuntu 14.04. Actually, I have more concerns about \"apt-get upgrade\", do you think if we could run this command to upgrade library packages in the stemcell? Thanks.\n. @cppforlife Currently we used a pretty old code base to build stemcell (we will move to use community version soon). We upgraded kernel level, but we are not very sure if we could upgrade packages with \"apt-get upgrade\". So from your comments, community stemcell always install the latest level for packages, if so, I think we could upgrade packages also, right? Thanks.\n. @cppforlife @maximilien This requirement is from Bluemix, there are some Bosh releases/jobs we created for post-install during deployment, like creating additional accounts, applying security patches and etc. We need to have some jobs running before some other jobs during deployment, but currently, all Bosh jobs are running in parallel. We get confirmed that the latest version of monit (5.15) supports to define the dependencies between jobs, but the feature is not supported by the monit in the current Bosh release since it's a very old version (5.2.4 in 2011), I am going to submit a PR to upgrade monit first, and then we need to investigate how to define the dependencies between jobs running in a VM. Please share with us if you or anyone else have such experiences on such requirement. Thanks.\n. @wayneszalinsky Did you follow SERVICE DEPENDENCIES in monit document to have a try?\n. After updating monit to 5.15 in the stemcell, all ruby jobs failed to start, all ruby processes were shown as zombie processes from the output of \"ps -ef | grep ruby\"\n. Sorry, this part of configuration is added by our code.\n. It's openstack.\n. Our customers need to install some security/audit softwares, they are pretty big....\n. @cppforlife Seems test cases operating on the same database are polluted by each other, after adding my db migration and unit test, the unit test for \"AttachDisk\" always failed, you can see the error in the travis log. I remember it failed even I removed my test case (still retain migration db script). Do you have any idea on how to fix that? Thanks.\n@maximilien\n. @jhunt  Thank you, so just need to use sha1sum command to generate it.\n. @liuweichu ruby 1.9.3 is very old version, there are many security issues. For BOSH, that ruby is totally no use after BOSH compilation. I believe your component shouldn't rely on that ruby, you need to install and maintain your own ruby (like CF release) if you need to use one. Thanks.\n. @swetharepakula how is going on with this issue? Have you already found a solution or workaround? Thanks.\n.  @swetharepakula So do you know how many processes will trigger this problem? Is it just relative to the amount of monitor processes or there might have something to do with process startup time? Thanks.\n/cc @maximilien @jianqiu\n. @dpb587-pivotal Yes, we checked agent log, we saw the agent got a message from director to start jobs, but we didn't see the agent sent back a response message (may be because jobs are not started within a given time). We always saw \"45s timeout\" exception in bosh debug log, do you know what kind of problems would trigger this \"45s timeout\" exception? Thanks.\n. @cppforlife Possible to upgrade to 9.5.x? The security issue is fixed in 9.5.2 according to vulnerability reports. Thanks. \n. @cppforlife We tried some versions of recent stemcell from 3262.4 to 3262.8, the kenerl is always 3.19.0-66\n```\nuname -a\nLinux ad1ff727-d5cd-49f8-8e48-37a02cfe544f 3.19.0-66-generic #74~14.04.1-Ubuntu SMP Tue Jul 19 19:56:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\nI also checked the stemcell_dpkg_l.txt of the latest aws stemcell, there listed 3.19.0-66 also\nWhy isn't the kernel upgraded to 4.4? Thanks.\n. @cppforlife only this version was upgraded to 4.4? All subsequent versions were rolled back to 3.19? Thanks.\n. @domdom82 I checked, only 3262.5 has 4.4 kernel\n```\nuname -a\nLinux test-kernel-version-3262-5.softlayer.com 4.4.0-31-generic #50~14.04.1-Ubuntu SMP Wed Jul 13 01:07:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n. Possible to give a due date? Our security team asked me for it. Thanks.\n. @rfreddi We are waiting BOSH community for publishing a new base image which includes the latest fixes of bosh agent, once it's available, we will build a new version of stemcell for your test again. Thanks.\n. @rfreddi We are waiting BOSH community for publishing a new base image which includes the latest fixes of bosh agent, once it's available, we will build a new version of stemcell for your test again. Thanks.\n. I checked the director, there is enough space. It's a strange problem, I just had a similar tryout in another environment, but didn't see such compilation errors, so I am closing this issue. Sorry for taking  your time.\n. I checked the director, there is enough space. It's a strange problem, I just had a similar tryout in another environment, but didn't see such compilation errors, so I am closing this issue. Sorry for taking  your time.\n. @cppforlife  Thank you and sorry for taking your time. Closing this issue now.\n. @cppforlife Since we canceled the job update when there completed the first 2 jobs (ipsec-orchestrator and nfs_WAL_server) update, the log only showed the change for these 2 jobs:\nroot@boshcli:# grep -rn \"configuration_changed? changed FROM\" 696240.log\n274064:D, [2017-01-18 07:51:50 #11475] [task:696240] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM: f7aa6157f948753176adbed0c46c07e8acf32d2\n9 TO: e7575273d16eebbce1560678b3e8a4b4429ebb51 on instance ipsec-orchestrator/0 (cd706b03-d153-456d-bccb-6b4f56e0871e)\n274373:D, [2017-01-18 07:52:49 #11475] [task:696240] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM: 06414956874c4885791149931acd5dda05c3186\nd TO: 1412a5944ee968ff4dbd7a9f1234d328a0282c2c on instance nfs_WAL_server/0 (9d4c037f-0730-487d-a6aa-689e0c2c1710)\nAs I showed in the log in the description, besides the change in configuration, there are also some changes in dns. But please note that, there only tried to update jobs (but not VM recreation) when we did the 1st deployment after director migration.. @cppforlife Since we canceled the job update when there completed the first 2 jobs (ipsec-orchestrator and nfs_WAL_server) update, the log only showed the change for these 2 jobs:\nroot@boshcli:# grep -rn \"configuration_changed? changed FROM\" 696240.log\n274064:D, [2017-01-18 07:51:50 #11475] [task:696240] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM: f7aa6157f948753176adbed0c46c07e8acf32d2\n9 TO: e7575273d16eebbce1560678b3e8a4b4429ebb51 on instance ipsec-orchestrator/0 (cd706b03-d153-456d-bccb-6b4f56e0871e)\n274373:D, [2017-01-18 07:52:49 #11475] [task:696240] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM: 06414956874c4885791149931acd5dda05c3186\nd TO: 1412a5944ee968ff4dbd7a9f1234d328a0282c2c on instance nfs_WAL_server/0 (9d4c037f-0730-487d-a6aa-689e0c2c1710)\nAs I showed in the log in the description, besides the change in configuration, there are also some changes in dns. But please note that, there only tried to update jobs (but not VM recreation) when we did the 1st deployment after director migration.. The default permission of \"/var/log\" is set to 775 at line 40, but it's changed to 770 at line 907-910 in  commit 257a51094b68933f3e26a65e561c6c49cdb9b6a2. Need to understand why it's changed to 770.\n/cc @aalbanes @maximilien @cppforlife . The default permission of \"/var/log\" is set to 775 at line 40, but it's changed to 770 at line 907-910 in  commit 257a51094b68933f3e26a65e561c6c49cdb9b6a2. Need to understand why it's changed to 770.\n/cc @aalbanes @maximilien @cppforlife . @aalbanes yes, I think there needs to copy all content in /var/log to /var/vcap/data/root_log before bind mount.. @aalbanes yes, I think there needs to copy all content in /var/log to /var/vcap/data/root_log before bind mount.. @maximilien @jianqiu could you please help discuss with @cppforlife or other guys there when you get a chance. Thanks.. @maximilien @jianqiu could you please help discuss with @cppforlife or other guys there when you get a chance. Thanks.. @maximilien Please help when you get time. Thank you!. @jfmyers9 we met a critical problem that persistent disks were formatted when upgrading to new Xenial stemcell, you mentioned there needs migration if it's msdos, what do you mean by \"migration\", we need to change disk size in yml to force it migrated? Thanks.. @jfmyers9 we met a critical problem that persistent disks were formatted when upgrading to new Xenial stemcell, you mentioned there needs migration if it's msdos, what do you mean by \"migration\", we need to change disk size in yml to force it migrated? Thanks.. @cppforlife The task actually ran well, so no error in task debug log. I think it's a performance problem, if it's not easy to solve on server side, possible to add retry logic in bosh-cli code? Thanks.\nhttps://github.com/cloudfoundry/bosh-cli/blob/master/director/client_request.go#L236. @cppforlife The task actually ran well, so no error in task debug log. I think it's a performance problem, if it's not easy to solve on server side, possible to add retry logic in bosh-cli code? Thanks.\nhttps://github.com/cloudfoundry/bosh-cli/blob/master/director/client_request.go#L236. @cppforlife We added 502/500 error check in bosh-utils at https://github.com/cloudfoundry/bosh-utils/blob/master/httpclient/retry_clients.go#L47 :\nif err != nil || ((resp.Request.Method == \"GET\" || resp.Request.Method == \"HEAD\") && (resp.StatusCode == http.StatusGatewayTimeout || resp.StatusCode == http.StatusServiceUnavailable || resp.StatusCode == http.StatusBadGateway || resp.StatusCode == http.StatusInternalServerError)) {\nThe test looked good, do you think if we could submit a PR to bosh-utils for the change? Thanks.\n/cc @maximilien . @cppforlife We added 502/500 error check in bosh-utils at https://github.com/cloudfoundry/bosh-utils/blob/master/httpclient/retry_clients.go#L47 :\nif err != nil || ((resp.Request.Method == \"GET\" || resp.Request.Method == \"HEAD\") && (resp.StatusCode == http.StatusGatewayTimeout || resp.StatusCode == http.StatusServiceUnavailable || resp.StatusCode == http.StatusBadGateway || resp.StatusCode == http.StatusInternalServerError)) {\nThe test looked good, do you think if we could submit a PR to bosh-utils for the change? Thanks.\n/cc @maximilien . We did see some issues when using community edition (v3.0.1) of bosh-cli, I will have some initial investigations, then I may raise the issue to the community if needed.. We did see some issues when using community edition (v3.0.1) of bosh-cli, I will have some initial investigations, then I may raise the issue to the community if needed.. The problem I mentioned in above comment should be a known issue, we are going to increase postgres.max_connections to 200 in our environments. I will submit a PR to bosh-utils as I mentioned in https://github.com/cloudfoundry/bosh/issues/1926#issuecomment-381950666\n/cc @cppforlife @maximilien . The problem I mentioned in above comment should be a known issue, we are going to increase postgres.max_connections to 200 in our environments. I will submit a PR to bosh-utils as I mentioned in https://github.com/cloudfoundry/bosh/issues/1926#issuecomment-381950666\n/cc @cppforlife @maximilien . @jfmyers9 We tried your approach, seems it didn't do anything after run --fix command, there is no change, we expect softlayer-legacy can be set there as the CPI and only one record for the stemcell.\nName                                     Version     OS             CPI               CID\nbosh-bluemix-xen-ubuntu-trusty-go_agent  3468.17.2*  ubuntu-trusty  -                 1840873. @jfmyers9 We tried your approach, seems it didn't do anything after run --fix command, there is no change, we expect softlayer-legacy can be set there as the CPI and only one record for the stemcell.\nName                                     Version     OS             CPI               CID\nbosh-bluemix-xen-ubuntu-trusty-go_agent  3468.17.2*  ubuntu-trusty  -                 1840873. Thanks, closing this issue. It's due to the db migration change in https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/db/migrations/director/20171201153629_remove_unused_template_columns.rb. It's due to the db migration change in https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/db/migrations/director/20171201153629_remove_unused_template_columns.rb. @jfmyers9 I think it should grep changed, right? Below are msgs for changed:\nD, [2018-08-13T03:41:42.348254 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy_with_properties\", \"templates\"=>[{\"name\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\"}], \"template\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\"} TO: {\"name\"=>\"dummy_with_properties\", \"templates\"=>[{\"name\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\", \"logs\"=>[]}], \"template\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\", \"logs\"=>[]} on instance dummy_with_properties/993428d3-85ee-4170-bcb8-092aecf4bd46 (0)\nD, [2018-08-13T03:41:42.352907 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy_with_package\", \"templates\"=>[{\"name\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\"}], \"template\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\"} TO: {\"name\"=>\"dummy_with_package\", \"templates\"=>[{\"name\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\", \"logs\"=>[]}], \"template\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\", \"logs\"=>[]} on instance dummy_with_package/40bc03b8-32bd-44ff-9a6e-4ea7e4e71c6d (0)\nD, [2018-08-13T03:41:42.356680 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy\", \"templates\"=>[{\"name\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\"}], \"template\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\"} TO: {\"name\"=>\"dummy\", \"templates\"=>[{\"name\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\", \"logs\"=>[]}], \"template\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\", \"logs\"=>[]} on instance dummy/c7b8d0e6-bcef-4fd4-b607-9c0e634608ec (0)\nD, [2018-08-13T03:41:42.415398 #18003] [] DEBUG -- DirectorJobRunner: packages_changed? changed FROM:  TO: {} on instance dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1)\nD, [2018-08-13T03:41:42.416390 #18003] [] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM:  TO: b5bfc5773d6e563f242666a92ed638dcfe41f3dd on instance dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1)\nD, [2018-08-13T03:41:42.440139 #18003] [] DEBUG -- DirectorJobRunner: dns_changed? The requested dns record with name '1.dummy.default.gubin-slcpi-dummy.microbosh' and ip '10.112.116.16' was not found in the db.\nD, [2018-08-13T03:42:32.190772 #18003] [instance_update(dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1))] DEBUG -- DirectorJobRunner: dns_changed? The requested dns record with name '1.dummy.default.gubin-slcpi-dummy.microbosh' and ip '10.112.116.16' was not found in the db.. @jfmyers9 I think it should grep changed, right? Below are msgs for changed:\nD, [2018-08-13T03:41:42.348254 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy_with_properties\", \"templates\"=>[{\"name\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\"}], \"template\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\"} TO: {\"name\"=>\"dummy_with_properties\", \"templates\"=>[{\"name\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\", \"logs\"=>[]}], \"template\"=>\"dummy_with_properties\", \"version\"=>\"4130de1cabfe9c169f0209c195ce7533fb5e5e7f\", \"sha1\"=>\"93a18da183df6aea459f7a648445d8f20152bfbc\", \"blobstore_id\"=>\"201f610b-6acc-4df8-917e-986cbdf42e01\", \"logs\"=>[]} on instance dummy_with_properties/993428d3-85ee-4170-bcb8-092aecf4bd46 (0)\nD, [2018-08-13T03:41:42.352907 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy_with_package\", \"templates\"=>[{\"name\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\"}], \"template\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\"} TO: {\"name\"=>\"dummy_with_package\", \"templates\"=>[{\"name\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\", \"logs\"=>[]}], \"template\"=>\"dummy_with_package\", \"version\"=>\"b819fee41c8e2c461e26f338bc0a510841d65c48\", \"sha1\"=>\"273f4a84da8ee73362d1c9cc2a25aec212eeae44\", \"blobstore_id\"=>\"edf86999-e32d-4d37-b547-3f7c350e741b\", \"logs\"=>[]} on instance dummy_with_package/40bc03b8-32bd-44ff-9a6e-4ea7e4e71c6d (0)\nD, [2018-08-13T03:41:42.356680 #18003] [] DEBUG -- DirectorJobRunner: job_changed? changed FROM: {\"name\"=>\"dummy\", \"templates\"=>[{\"name\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\"}], \"template\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\"} TO: {\"name\"=>\"dummy\", \"templates\"=>[{\"name\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\", \"logs\"=>[]}], \"template\"=>\"dummy\", \"version\"=>\"289ecbf3fa7359e84d84e5d7c5edd22689ad81d4\", \"sha1\"=>\"4f8454e2984dd439c890d8d398facd48f0617141\", \"blobstore_id\"=>\"5423c0b6-3d50-4bfe-9880-2351c38d6ff1\", \"logs\"=>[]} on instance dummy/c7b8d0e6-bcef-4fd4-b607-9c0e634608ec (0)\nD, [2018-08-13T03:41:42.415398 #18003] [] DEBUG -- DirectorJobRunner: packages_changed? changed FROM:  TO: {} on instance dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1)\nD, [2018-08-13T03:41:42.416390 #18003] [] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM:  TO: b5bfc5773d6e563f242666a92ed638dcfe41f3dd on instance dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1)\nD, [2018-08-13T03:41:42.440139 #18003] [] DEBUG -- DirectorJobRunner: dns_changed? The requested dns record with name '1.dummy.default.gubin-slcpi-dummy.microbosh' and ip '10.112.116.16' was not found in the db.\nD, [2018-08-13T03:42:32.190772 #18003] [instance_update(dummy/c8c28a8b-6f30-4ac1-9b5a-4986e0298e40 (1))] DEBUG -- DirectorJobRunner: dns_changed? The requested dns record with name '1.dummy.default.gubin-slcpi-dummy.microbosh' and ip '10.112.116.16' was not found in the db.. @jfmyers9 Thanks for your responses, but sorry that I didn't understand your solution to solve my problem with migrated_from in cpi-config, what kind of value should I set there? This is the first time to use multiple CPI, we didn't use the feature before. Below is our cpi-config:\ncpis:\n- name: softlayer_legacy\n  type: softlayer_legacy\n- name: softlayer\n  type: softlayer\nThanks.. @jfmyers9 Thanks for your responses, but sorry that I didn't understand your solution to solve my problem with migrated_from in cpi-config, what kind of value should I set there? This is the first time to use multiple CPI, we didn't use the feature before. Below is our cpi-config:\ncpis:\n- name: softlayer_legacy\n  type: softlayer_legacy\n- name: softlayer\n  type: softlayer\nThanks.. @jfmyers9 I know what you mean from your another reply (https://github.com/cloudfoundry/bosh/issues/2026#issuecomment-413343388), I will verify if it works. Thank you!. @jfmyers9 I know what you mean from your another reply (https://github.com/cloudfoundry/bosh/issues/2026#issuecomment-413343388), I will verify if it works. Thank you!. Hi Danny @dpb587-pivotal , it's a blocking issue for us to do deployment, could you please help have a quick check? Thank you very much!. Hi Danny @dpb587-pivotal , it's a blocking issue for us to do deployment, could you please help have a quick check? Thank you very much!. Hi Max @maximilien, are you aware if there is trick to disable it? Thanks.. Hi Max @maximilien, are you aware if there is trick to disable it? Thanks.. 2 more findings:\n\n\nThe cpi property kept empty for all persistent disks which were created before deploying multiple CPIs.\n\n\nThe older version of BOSH (like 263.3) doesn't care much about cpi property for persistent disks, there always use the new CPI to detach/attach disks during stemcell update. But the latest version of BOSH (like 268.3.0) cares about it, it used legacy CPI to operate disks without softlayer cpi property.. 2 more findings:\n\n\nThe cpi property kept empty for all persistent disks which were created before deploying multiple CPIs.\n\n\nThe older version of BOSH (like 263.3) doesn't care much about cpi property for persistent disks, there always use the new CPI to detach/attach disks during stemcell update. But the latest version of BOSH (like 268.3.0) cares about it, it used legacy CPI to operate disks without softlayer cpi property.. \n\n",
    "fabianschwarzfritz": "It seems to like there are no plans to update monit to a more recent version in the near future. The version that's currently deployed is quite outdated.\nIn the current version I'm missing a feature provided in monit version 5.9.0 (https://mmonit.com/monit/changes/ --> 5.9.0). It is the check program feature that would enable us to do a more sophisticated check via bash scripts if our service is up and running.\nWhat's the status of this ticket? Will there be any update of monit to a never version in the near future?\n. Hi cf folks,\nI cannot see the codeclimate result, how can I see what the issue is?\nCheers,\nFabian\n. Hi @cppforlife ,\nthanks for the response! I think the error message is all right :smile: . \nI'll close this pull request then.\nCheers,\nFabian\n. Hey @cppforlife thanks for the second notification! I had a longer holiday and only came back a few days ago. I am looking forward to work on that next weekend.\n. @cppforlife had some time today and added a unit test, that covers the scenario. I am a ruby beginner.... what do you think?\n. ",
    "rradecki": "What is the current status of this problem? I am also having mentioned issue and fog upgrade does not help :)\n. What is the current status of this problem? I am also having mentioned issue and fog upgrade does not help :)\n. +1\n. +1\n. ",
    "jhiemer": "Hi @voelzmo,\nfrom the tickets I can't see any timeline for V3 support. Do you have any information here? :-)\n. Hi @cppforlife,\nI have just deployed my Director with my credentials and it worked. Is there any other option to set the credentials in the environment properties?\n. My fault forgot to login with the new user. \n. @lordcf show the deployment manifest of your bosh director.. ",
    "geemus": "Thanks!\nOn Sun, Nov 1, 2015 at 11:24 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nKeystone v3 support is available:\nhttp://bosh.io/docs/openstack-keystonev3.html\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/939#issuecomment-152941400.\n. Thanks!\n\nOn Sun, Nov 1, 2015 at 11:24 PM, Dmitriy Kalinin notifications@github.com\nwrote:\n\nKeystone v3 support is available:\nhttp://bosh.io/docs/openstack-keystonev3.html\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/939#issuecomment-152941400.\n. \n",
    "Jonty": "The corporate CLA has now been correctly filed. Can this pull request be considered now?\n. @cppforlife We are automating the removal of old stemcells from storage. Currently parsing bosh stemcells involves some horrendous grep/awk/sed and we'd like to make this more robust.\n. @bosh-ci-push-pull No, that does not solve our specific use case.\nIf this pull request was merged I was planning to add machine-parsable output to many of the bosh commands as we have other instances where we need to process output from bosh commands in automated environments.\n. @simonjohansson The machine-parsable output is generally used by people who are comfortable in bash, but not in ruby, hence this change.\nGiven --terse already exists for one of the other commands, is there a particular reason why replicating this functionality for other commands is not wanted?\n. @cppforlife As I have said this commit was the first in a series as I intended to add --terse as an option for all bosh commands - we parse the output of many of them, not just this one.\nIf you wish to talk about extending the cleanup command I'm happy to do so in a new issue, but continuing the discussion here derails from getting this change merged.\nAre the bosh maintainers actively against adding machine-readable output?\n@simonjohansson Exactly the reason I added this. While I would like to see as much functionality as possible handled by the bosh CLI there are always going to be awful one-off tasks that can be made easier by providing easy to parse output.\n. @cppforlife As I have said this commit was the first in a series as I intended to add --terse as an option for all bosh commands - we parse the output of many of them, not just this one.\nIf you wish to talk about extending the cleanup command I'm happy to do so in a new issue, but continuing the discussion here derails from getting this change merged.\nAre the bosh maintainers actively against adding machine-readable output?\n@simonjohansson Exactly the reason I added this. While I would like to see as much functionality as possible handled by the bosh CLI there are always going to be awful one-off tasks that can be made easier by providing easy to parse output.\n. It's been a year since this PR was opened, and almost as long since it was marked as ready-to-pull-in. Is it likely to be merged, or should I close it?\n. It's been a year since this PR was opened, and almost as long since it was marked as ready-to-pull-in. Is it likely to be merged, or should I close it?\n. The corporate CLA has now been correctly filed. Can this pull request be considered now?\n. The corporate CLA has now been correctly filed. Can this pull request be considered now?\n. ",
    "keymon": "I agree that a cleanup with additional options would help here. In this case a good option to the clean up is specify how many releases to keep (~~in this case --all is misleading, it keeps 2 releases~~). We could add --keep NUM to keep latest NUM releases.\nBut on the other hand, automating bosh with shell scripts a quite useful thing, for quick prototyping, etc. We all know the benefits of a quick shell script :). The current output of bosh commands makes it really difficult to handle in scripting. \nOther example of difficult to parse output is bosh releases output, and upload a release to finally find out that has been already uploaded is really slow.\nThe cloudfoundry client has a 'curl' option, which is extremely useful for debugging and also scripting.\nI would say that +1 for a --terse option to ease the management.\nIf not, for the time being, we can always use:\n``` ruby\n!/usr/bin/env ruby\nrequire 'cli'\nbosh_cli = Bosh::Cli::Command::Base.new.director\nbosh_cli.list_stemcells.each { |s| puts \"#{s['name']}/#{s['version']}\" } ;\n```\n. +1 @mtekel \nTo support the argumentation of parseable output: I think that BOSH cli, as a command line tool where its users are unix/linux operators, it should enable and empower them by subscribing the Unix Command Line philosophy.\nI am not saying that we don't implement full out of the box use cases, but parseable output is desired and not so difficult to implement.\n. Personally I do not understand why it fails.  The module is defined in aws-sdk-resources:\n$ GEM_HOME=/var/vcap/packages/director/gem_home/ruby/2.1.0/ /var/vcap/packages/ruby/bin/ruby -raws-sdk-resources -e 'puts Aws::S3::Errors.class'\nModule\n. Actually, if you are using environment variables to pass the credentials (#941), or IAM profiles (#1402) the file content has to be empty:\n```\nblobstore:\n  s3:\n    access_key_id: ''\n    secret_access_key: ''\n```\nWhich is a little bit silly. It should simply not require it in that case.\n. Maybe it is related to #1149?\n. ```\n # bosh status\nConfig\n             /root/.bosh_config\nDirector\n  Name       my-bosh\n  URL        https://bosh.mydomain.com:25555\n  Version    1.3215.3.0 (00000000)\n  User       admin\n  UUID       9e89d8e8-2516-48c1-8c6d-677249af9141\n  CPI        aws_cpi\n  dns        disabled\n  compiled_package_cache enabled (provider: s3)\n  snapshots  disabled\nDeployment\n  Manifest   /tmp/build/04dcec6b/cf-manifest-with-uuid.yml\n```\n. Tbh I do not work with this at the moment. My ex coworkers do /cc @dcarley @saliceti. Yes, it shall be interesting. I IIRC we ended workingaround this by using other tags like the deployment name and write some more complex monitors queries. \nIn order to do a PR, I will have a look to it and come back to you. . Yes it is true that you can implement this with UAA. But let me disagree here:\nThis change allows you to restrict the scopes for local users, with a minimal fingerprint. The code change is quite small, and any local user is a bosh.admin which means all the permissions anyway. \nInstead, having to install a complete UAA, with all the scaffolding and leverage it does actually increase the surface area: new service, new port, new complex auth flow. Also makes it more heavy to run in small and test environments. \nThis PR does not add too much logic. It simply allows parametrise a previously hardcoded value (bosh.admin) using that value as default.. Same problem for us, after a upgrade of cf-deployment v4.5.0 to v6.0.0.\nWe are running bosh 268.2.0, as per https://github.com/alphagov/paas-bootstrap/blob/49d7fa06459c49ea62c42edd06644e97a4d765df/manifests/bosh-manifest/bosh-manifest.yml\n. ",
    "1stvamp": "\nthat's somewhat purposeful to prevent people reinventing the wheel; or inventing it and not contributing it to bosh CLI itself.\n\nMaking something difficult for users of a project is not the way to get them to contribute back.\n. ",
    "guoger": "@mariash Sure, we'll look into that!\n. @cppforlife , just to comment on your rejection in the tracker, package.sha1 should not be affected since we are replacing corrupted blobs with correct ones, which should already match the sha1 in database, since they are identical packages.\nThanks!\n/Jay\n. @cppforlife That's true, unlike simple tar, gzip (-z) indeed produces different bits out of same content. Didn't know that and thanks for pointing it out.\n. Hi there, I'm not very familiar with BOSH micro plug-in but you probably could use bosh-init instead. http://bosh.io/docs/using-bosh-init.html That's how we do it in our OpenStack envrionment.\nCheers,\n/Jay\n. @voelzmo We believe this is related to this story https://www.pivotaltracker.com/story/show/103869294 and we were indeed only focusing on broken packages but not jobs. Probably we should hear back from @cppforlife on this.\n. ",
    "mavenraven": "Hey @cppforlife @Amit-PivotalLabs, is there any update on this functionality being merged? Thanks!\n. Got it, thanks @jpalermo!\n. Yeah, I'm using 1.3160.0.\n. FWIW, this is the dockerfile of the image that I'm running inside: https://github.com/mavenraven/cloudfoundry-installer/blob/master/Dockerfile\n. Thanks @voelzmo, I got it working. One thing I noticed is that even correcting the urls and names to match, it wasn't working when I had the version set to latest. It seems like bosh should do better validation of name/version, like you said. It also would be nice if bosh.io listed sha1s as well as md5s to make it harder to use the wrong type of stemcell.\n. \n. ",
    "bingosummer": "Thanks @cppforlife for your quick response and kindly help.\n. @dpb587-pivotal \nAfter partitioning the kernel needs to re-read the partition table, and then it will recreate the symlinks in /dev for each partition.  So there is a race here if bosh-agent moves too quickly. We are testing to sleep several seconds after partitioning. It seems working in 3363.9. But still need a verification to make sure it works every time.\nIf that's the root cause, the fix may be to add a small loop after partitioning to test and wait for the block device (i.e. /dev/sdc1) to exist, and then proceed with formatting. Do you think it makes sense?. We hit the error right after the director is deployed. For example, when we try to upload a stemcell (task 1), we got the error. From the director VM, we can get the director error log:\n```\ncat /var/vcap/sys/log/director/error.log\n2018/06/17 14:24:44 [error] 6#0: *175 upstream prematurely closed connection while reading response header from upstream, client: , server: , request: \"GET /tasks/1/output?type=event HTTP/1.1\", upstream: \"http://127.0.0.1:25556/tasks/1/output?type=event\", host: \":25555\"\n. Although I don't know why we get the error, but the fix for bosh-utils seems working for me.. @dpb587-pivotal Just checking what the next step is? Will BOSH director pass an encoded data?\n. I hit a similar issue in `attach_disk`. After encoding `;` to `%3B`, the command `attach-disk` works.\nazureuser@binxi091701:~$ bosh -e azure -d cf attach-disk database/6672de96-12bf-4d84-af87-ebb88ddbe26c caching:None%3Bdisk_name:bosh-disk-data-8115562f-fc8f-4cb0-8578-c4e75cee7bba%3Bresource_group_name:binxi091701-vm-size\nUsing environment '10.0.0.4' as client 'admin'\nUsing deployment 'cf'\nTask 353. Done\nSucceeded\n```. I also hit a similar issue. But my case is linux stemcell (3GB). BTW, bosh.io doesn't have heavy stemcell for windows. Is it a new feature?. Thanks @dpb587-pivotal for response.\nFirst of all, here are the scenario of creating missing VMs.\ncf-deployment.yml:\n...\n- name: consul\n  azs:\n  - z1\n  - z2\n  - z3\n  instances: 3\n  persistent_disk_type: 5GB\n  vm_resources:\n    cpu: 1\n    ram: 1024\n    ephemeral_disk_size: 204800\n...\n- name: log-api\n  azs:\n  - z1\n  - z2\n  instances: 2\n  vm_resources:\n    cpu: 2\n    ram: 2048\n    ephemeral_disk_size: 204800\n...\nI'm implementing Azure CPI's calculate_vm_cloud_properties to return a list of instance types which meet vm_resources.\nFor consul:\nI, [2018-09-17T02:12:20.490048 #64396] [task:60]  INFO -- DirectorJobRunner: CPI  calculated vm cloud properties '{\"instance_types\"=>[\"Standard_B1s\", \"Standard_A1\", \"Standard_A1_v2\", \"Standard_B1ms\", \"Standard_F1s\", \"Standard_F1\", \"Standard_DS1_v2\", \"Standard_D1_v2\", \"Standard_D1\", \"Standard_A2\", \"Standard_A2_v2\", \"Standard_F2s_v2\", \"Standard_B2s\", \"Standard_F2s\", \"Standard_F2\", \"Standard_DS2_v2\", \"Standard_D2_v2\", \"Standard_D2\", \"Standard_D2s_v3\", \"Standard_D2_v3\", \"Standard_B2ms\", \"Standard_DS11_v2\", \"Standard_D11_v2\", \"Standard_D11\", \"Standard_A5\", \"Standard_E2s_v3\", \"Standard_E2_v3\", \"Standard_A2m_v2\", \"Standard_GS1\", \"Standard_G1\", \"Standard_A3\", \"Standard_A4_v2\", \"Standard_F4s_v2\", \"Standard_F4s\", \"Standard_F4\", \"Standard_DS3_v2\", \"Standard_D3_v2\", \"Standard_D3\", \"Standard_D4s_v3\", \"Standard_D4_v3\", \"Standard_B4ms\", \"Standard_DS12_v2\", \"Standard_D12_v2\", \"Standard_D12\", \"Standard_A6\", \"Standard_E4s_v3\", \"Standard_E4_v3\", \"Standard_A4m_v2\", \"Standard_GS2\", \"Standard_G2\", \"Standard_A4\", \"Standard_A8_v2\", \"Standard_F8s_v2\", \"Standard_F8s\", \"Standard_F8\", \"Standard_DS4_v2\", \"Standard_D4_v2\", \"Standard_D4\", \"Standard_D8s_v3\", \"Standard_D8_v3\", \"Standard_B8ms\", \"Standard_DS13_v2\", \"Standard_D13_v2\", \"Standard_D13\", \"Standard_A7\", \"Standard_A8\", \"Standard_A10\", \"Standard_H8\", \"Standard_E8s_v3\", \"Standard_E8_v3\", \"Standard_A8m_v2\", \"Standard_GS3\", \"Standard_G3\", \"Standard_H8m\", \"Standard_F16s_v2\", \"Standard_F16s\", \"Standard_F16\", \"Standard_DS5_v2\", \"Standard_D5_v2\", \"Standard_D16s_v3\", \"Standard_D16_v3\", \"Standard_DS14_v2\", \"Standard_D14_v2\", \"Standard_D14\", \"Standard_A9\", \"Standard_A11\", \"Standard_H16\", \"Standard_H16r\", \"Standard_E16s_v3\", \"Standard_E16_v3\", \"Standard_GS4\", \"Standard_G4\", \"Standard_H16m\", \"Standard_H16mr\", \"Standard_DS15_v2\", \"Standard_D15_v2\", \"Standard_F32s_v2\", \"Standard_D32s_v3\", \"Standard_D32_v3\", \"Standard_E32s_v3\", \"Standard_E32_v3\", \"Standard_GS5\", \"Standard_G5\", \"Standard_F64s_v2\", \"Standard_D64s_v3\", \"Standard_D64_v3\", \"Standard_E64is_v3\", \"Standard_E64i_v3\", \"Standard_F72s_v2\"], \"ephemeral_disk\"=>{\"size\"=>204800}}' for vm requirements '{\"cpu\"=>1, \"ram\"=>1024, \"ephemeral_disk_size\"=>204800}'\nFor log-api:\nI, [2018-09-17T02:12:26.479645 #64396] [task:60]  INFO -- DirectorJobRunner: CPI  calculated vm cloud properties '{\"instance_types\"=>[\"Standard_A2\", \"Standard_A2_v2\", \"Standard_F2s_v2\", \"Standard_B2s\", \"Standard_F2s\", \"Standard_F2\", \"Standard_DS2_v2\", \"Standard_D2_v2\", \"Standard_D2\", \"Standard_D2s_v3\", \"Standard_D2_v3\", \"Standard_B2ms\", \"Standard_DS11_v2\", \"Standard_D11_v2\", \"Standard_D11\", \"Standard_A5\", \"Standard_E2s_v3\", \"Standard_E2_v3\", \"Standard_A2m_v2\", \"Standard_GS1\", \"Standard_G1\", \"Standard_A3\", \"Standard_A4_v2\", \"Standard_F4s_v2\", \"Standard_F4s\", \"Standard_F4\", \"Standard_DS3_v2\", \"Standard_D3_v2\", \"Standard_D3\", \"Standard_D4s_v3\", \"Standard_D4_v3\", \"Standard_B4ms\", \"Standard_DS12_v2\", \"Standard_D12_v2\", \"Standard_D12\", \"Standard_A6\", \"Standard_E4s_v3\", \"Standard_E4_v3\", \"Standard_A4m_v2\", \"Standard_GS2\", \"Standard_G2\", \"Standard_A4\", \"Standard_A8_v2\", \"Standard_F8s_v2\", \"Standard_F8s\", \"Standard_F8\", \"Standard_DS4_v2\", \"Standard_D4_v2\", \"Standard_D4\", \"Standard_D8s_v3\", \"Standard_D8_v3\", \"Standard_B8ms\", \"Standard_DS13_v2\", \"Standard_D13_v2\", \"Standard_D13\", \"Standard_A7\", \"Standard_A8\", \"Standard_A10\", \"Standard_H8\", \"Standard_E8s_v3\", \"Standard_E8_v3\", \"Standard_A8m_v2\", \"Standard_GS3\", \"Standard_G3\", \"Standard_H8m\", \"Standard_F16s_v2\", \"Standard_F16s\", \"Standard_F16\", \"Standard_DS5_v2\", \"Standard_D5_v2\", \"Standard_D16s_v3\", \"Standard_D16_v3\", \"Standard_DS14_v2\", \"Standard_D14_v2\", \"Standard_D14\", \"Standard_A9\", \"Standard_A11\", \"Standard_H16\", \"Standard_H16r\", \"Standard_E16s_v3\", \"Standard_E16_v3\", \"Standard_GS4\", \"Standard_G4\", \"Standard_H16m\", \"Standard_H16mr\", \"Standard_DS15_v2\", \"Standard_D15_v2\", \"Standard_F32s_v2\", \"Standard_D32s_v3\", \"Standard_D32_v3\", \"Standard_E32s_v3\", \"Standard_E32_v3\", \"Standard_GS5\", \"Standard_G5\", \"Standard_F64s_v2\", \"Standard_D64s_v3\", \"Standard_D64_v3\", \"Standard_E64is_v3\", \"Standard_E64i_v3\", \"Standard_F72s_v2\"], \"ephemeral_disk\"=>{\"size\"=>204800}}' for vm requirements '{\"cpu\"=>2, \"ram\"=>2048, \"ephemeral_disk_size\"=>204800}'\nBut when creating consul VM, the instance types are wrong.\nI, [2018-09-17T02:13:17.958712 #66216 #47013220724560] INFO -- [req_id cpi-167322]: create_vm(6d5a123f-23e9-4b9f-a438-70a5a733f19c, bosh-stemcell-7b0558d1-7b95-49d5-b615-9a9c71deb05a, {\"instance_types\"=>[\"Standard_A2\", \"Standard_A2_v2\", \"Standard_F2s_v2\", \"Standard_B2s\", \"Standard_F2s\", \"Standard_F2\", \"Standard_DS2_v2\", \"Standard_D2_v2\", \"Standard_D2\", \"Standard_D2s_v3\", \"Standard_D2_v3\", \"Standard_B2ms\", \"Standard_DS11_v2\", \"Standard_D11_v2\", \"Standard_D11\", \"Standard_A5\", \"Standard_E2s_v3\", \"Standard_E2_v3\", \"Standard_A2m_v2\", \"Standard_GS1\", \"Standard_G1\", \"Standard_A3\", \"Standard_A4_v2\", \"Standard_F4s_v2\", \"Standard_F4s\", \"Standard_F4\", \"Standard_DS3_v2\", \"Standard_D3_v2\", \"Standard_D3\", \"Standard_D4s_v3\", \"Standard_D4_v3\", \"Standard_B4ms\", \"Standard_DS12_v2\", \"Standard_D12_v2\", \"Standard_D12\", \"Standard_A6\", \"Standard_E4s_v3\", \"Standard_E4_v3\", \"Standard_A4m_v2\", \"Standard_GS2\", \"Standard_G2\", \"Standard_A4\", \"Standard_A8_v2\", \"Standard_F8s_v2\", \"Standard_F8s\", \"Standard_F8\", \"Standard_DS4_v2\", \"Standard_D4_v2\", \"Standard_D4\", \"Standard_D8s_v3\", \"Standard_D8_v3\", \"Standard_B8ms\", \"Standard_DS13_v2\", \"Standard_D13_v2\", \"Standard_D13\", \"Standard_A7\", \"Standard_A8\", \"Standard_A10\", \"Standard_H8\", \"Standard_E8s_v3\", \"Standard_E8_v3\", \"Standard_A8m_v2\", \"Standard_GS3\", \"Standard_G3\", \"Standard_H8m\", \"Standard_F16s_v2\", \"Standard_F16s\", \"Standard_F16\", \"Standard_DS5_v2\", \"Standard_D5_v2\", \"Standard_D16s_v3\", \"Standard_D16_v3\", \"Standard_DS14_v2\", \"Standard_D14_v2\", \"Standard_D14\", \"Standard_A9\", \"Standard_A11\", \"Standard_H16\", \"Standard_H16r\", \"Standard_E16s_v3\", \"Standard_E16_v3\", \"Standard_GS4\", \"Standard_G4\", \"Standard_H16m\", \"Standard_H16mr\", \"Standard_DS15_v2\", \"Standard_D15_v2\", \"Standard_F32s_v2\", \"Standard_D32s_v3\", \"Standard_D32_v3\", \"Standard_E32s_v3\", \"Standard_E32_v3\", \"Standard_GS5\", \"Standard_G5\", \"Standard_F64s_v2\", \"Standard_D64s_v3\", \"Standard_D64_v3\", \"Standard_E64is_v3\", \"Standard_E64i_v3\", \"Standard_F72s_v2\"], \"ephemeral_disk\"=>{\"size\"=>204800}}, {\"default\"=>{\"type\"=>\"manual\", \"ip\"=>\"10.0.16.4\", \"netmask\"=>\"255.255.240.0\", \"cloud_properties\"=>{\"security_group\"=>\"nsg-cf\", \"subnet_name\"=>\"CloudFoundry\", \"virtual_network_name\"=>\"boshvnet-crp\"}, \"default\"=>[\"dns\", \"gateway\"], \"dns\"=>[\"168.63.129.16\", \"8.8.8.8\"], \"gateway\"=>\"10.0.16.1\"}}, [], ...)\nWhen creating other VMs which I don't specify vm_resources, the cloud_properties also include instance_types which are calculated by CPI.\nI, [2018-09-17T02:13:18.558548 #66234 #47242947882840] INFO -- [req_id cpi-144818]: create_vm(f1fa4cd1-2139-4424-b6f0-d45c8778d955, bosh-stemcell-7b0558d1-7b95-49d5-b615-9a9c71deb05a, {\"instance_types\"=>[\"Standard_B1s\", \"Standard_A1\", \"Standard_A1_v2\", \"Standard_B1ms\", \"Standard_F1s\", \"Standard_F1\", \"Standard_DS1_v2\", \"Standard_D1_v2\", \"Standard_D1\", \"Standard_A2\", \"Standard_A2_v2\", \"Standard_F2s_v2\", \"Standard_B2s\", \"Standard_F2s\", \"Standard_F2\", \"Standard_DS2_v2\", \"Standard_D2_v2\", \"Standard_D2\", \"Standard_D2s_v3\", \"Standard_D2_v3\", \"Standard_B2ms\", \"Standard_DS11_v2\", \"Standard_D11_v2\", \"Standard_D11\", \"Standard_A5\", \"Standard_E2s_v3\", \"Standard_E2_v3\", \"Standard_A2m_v2\", \"Standard_GS1\", \"Standard_G1\", \"Standard_A3\", \"Standard_A4_v2\", \"Standard_F4s_v2\", \"Standard_F4s\", \"Standard_F4\", \"Standard_DS3_v2\", \"Standard_D3_v2\", \"Standard_D3\", \"Standard_D4s_v3\", \"Standard_D4_v3\", \"Standard_B4ms\", \"Standard_DS12_v2\", \"Standard_D12_v2\", \"Standard_D12\", \"Standard_A6\", \"Standard_E4s_v3\", \"Standard_E4_v3\", \"Standard_A4m_v2\", \"Standard_GS2\", \"Standard_G2\", \"Standard_A4\", \"Standard_A8_v2\", \"Standard_F8s_v2\", \"Standard_F8s\", \"Standard_F8\", \"Standard_DS4_v2\", \"Standard_D4_v2\", \"Standard_D4\", \"Standard_D8s_v3\", \"Standard_D8_v3\", \"Standard_B8ms\", \"Standard_DS13_v2\", \"Standard_D13_v2\", \"Standard_D13\", \"Standard_A7\", \"Standard_A8\", \"Standard_A10\", \"Standard_H8\", \"Standard_E8s_v3\", \"Standard_E8_v3\", \"Standard_A8m_v2\", \"Standard_GS3\", \"Standard_G3\", \"Standard_H8m\", \"Standard_F16s_v2\", \"Standard_F16s\", \"Standard_F16\", \"Standard_DS5_v2\", \"Standard_D5_v2\", \"Standard_D16s_v3\", \"Standard_D16_v3\", \"Standard_DS14_v2\", \"Standard_D14_v2\", \"Standard_D14\", \"Standard_A9\", \"Standard_A11\", \"Standard_H16\", \"Standard_H16r\", \"Standard_E16s_v3\", \"Standard_E16_v3\", \"Standard_GS4\", \"Standard_G4\", \"Standard_H16m\", \"Standard_H16mr\", \"Standard_DS15_v2\", \"Standard_D15_v2\", \"Standard_F32s_v2\", \"Standard_D32s_v3\", \"Standard_D32_v3\", \"Standard_E32s_v3\", \"Standard_E32_v3\", \"Standard_GS5\", \"Standard_G5\", \"Standard_F64s_v2\", \"Standard_D64s_v3\", \"Standard_D64_v3\", \"Standard_E64is_v3\", \"Standard_E64i_v3\", \"Standard_F72s_v2\"], \"ephemeral_disk\"=>{\"size\"=>10240}, \"instance_type\"=>\"Standard_D1_v2\"}, {\"default\"=>{\"type\"=>\"manual\", \"ip\"=>\"10.0.16.14\", \"netmask\"=>\"255.255.240.0\", \"cloud_properties\"=>{\"security_group\"=>\"nsg-cf\", \"subnet_name\"=>\"CloudFoundry\", \"virtual_network_name\"=>\"boshvnet-crp\"}, \"default\"=>[\"dns\", \"gateway\"], \"dns\"=>[\"168.63.129.16\", \"8.8.8.8\"], \"gateway\"=>\"10.0.16.1\"}}, [], ...)\nI'm guessing that Director would merge the calculated cloud properties (vm_resources) with the cloud properties from vm_type. There may be something wrong when merging. . @dpb587-pivotal Could you please have a look at the logs in my previous comment?. I also used bosh-lite/warden-cpi to reproduce the issue. @dpb587-pivotal @mfine30 @jfmyers9 @voelzmo \nBOSH version: 268.0.1\ncf-deployment: v4.5.0\nbosh-cli: 5.2.2\nwarden-cpi: a dev version based on v41\nSteps:\n1. I changed warden-cpi's CalculateVMCloudProperties from \nfunc (a CalculateVMCloudPropertiesMethod) CalculateVMCloudProperties(res apiv1.VMResources) (apiv1.VMCloudProps, error) {\n        return apiv1.NewVMCloudPropsFromMap(map[string]interface{}{}), nil\n}\nto\n```\nfunc (a CalculateVMCloudPropertiesMethod) CalculateVMCloudProperties(res apiv1.VMResources) (apiv1.VMCloudProps, error) {\nreturn apiv1.NewVMCloudPropsFromMap(map[string]interface{}{\"cpu\":res.CPU}), nil\n}\n```\n\nso that, we can see the difference in the bosh task debug log.\n\nAdd the vm_resources config in cf-deployment.yml.\n```\n...\nname: consul\n  azs:\nz1\nz2\nz3\n  instances: 3\n  persistent_disk_type: 5GB\n  vm_resources:\n    cpu: 1\n    ram: 1024\n    ephemeral_disk_size: 204800\n...\nname: log-api\n  azs:\nz1\n\nz2\n  instances: 2\n  vm_resources:\n    cpu: 2\n    ram: 2048\n    ephemeral_disk_size: 204800\n...\n```\n\n\nDeploy CF, and get the task debug log.\n\nFrom the log, the calculate_vm_cloud_properties are called correctly.\n...\nI, [2018-09-26T07:25:08.985306 #15654] [task:56]  INFO -- DirectorJobRunner: CPI  calculated vm cloud properties '{\"cpu\"=>1}' for vm requirements '{\"cpu\"=>1, \"ram\"=>2048, \"ephemeral_disk_size\"=>204800}'\n...\nI, [2018-09-26T07:25:09.265812 #15654] [task:56]  INFO -- DirectorJobRunner: CPI  calculated vm cloud properties '{\"cpu\"=>2}' for vm requirements '{\"cpu\"=>2, \"ram\"=>4096, \"ephemeral_disk_size\"=>204800}'\n...\nExpected: Only when creating consul and log-api VMs, the cloud properties contain the key cpu.\nActual: When creating any VM, the cloud properties contain the key cpu.\nD, [2018-09-26T07:26:35.923493 #15654] [create_missing_vm(adapter/e7023855-d848-43fb-b5c9-a1897734f534 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-734823] request: {\"method\":\"create_vm\",\"arguments\":[\"6b7acb68-d1de-499c-9354-49d0a9d92fca\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.130\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-adapter\",\"groups\":[\"bosh-lite\",\"cf\",\"adapter\",\"bosh-lite-cf\",\"cf-adapter\",\"bosh-lite-cf-adapter\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-734823\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:37.136866 #15654] [create_missing_vm(diego-api/13390873-e101-45cf-a157-16d07bf7542f (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-993437] request: {\"method\":\"create_vm\",\"arguments\":[\"b81c8de1-1fb3-4f6a-9d29-c4a46919ecda\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.132\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-diego-api\",\"groups\":[\"bosh-lite\",\"cf\",\"diego-api\",\"bosh-lite-cf\",\"cf-diego-api\",\"bosh-lite-cf-diego-api\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-993437\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:37.292209 #15654] [create_missing_vm(scheduler/30786f10-f586-48bf-ac5e-8285d19f03f1 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-213829] request: {\"method\":\"create_vm\",\"arguments\":[\"46c2c29a-a9a8-4a09-8a41-ccdbb7e71135\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.138\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-scheduler\",\"groups\":[\"bosh-lite\",\"cf\",\"scheduler\",\"bosh-lite-cf\",\"cf-scheduler\",\"bosh-lite-cf-scheduler\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-213829\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:37.326640 #15654] [create_missing_vm(uaa/470f91b5-f7e1-4c87-8027-8176cf7d1ba4 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-704022] request: {\"method\":\"create_vm\",\"arguments\":[\"dd916919-ee22-4dae-b247-fe6de755fc78\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.133\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-uaa\",\"groups\":[\"bosh-lite\",\"cf\",\"uaa\",\"bosh-lite-cf\",\"cf-uaa\",\"bosh-lite-cf-uaa\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-704022\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:38.372855 #15654] [create_missing_vm(credhub/aef0eeea-537d-4538-902b-aa162a5d922a (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-383859] request: {\"method\":\"create_vm\",\"arguments\":[\"9f308d5d-668b-4e29-b5f7-b0081997977c\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.142\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-credhub\",\"groups\":[\"bosh-lite\",\"cf\",\"credhub\",\"bosh-lite-cf\",\"cf-credhub\",\"bosh-lite-cf-credhub\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-383859\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:39.073113 #15654] [create_missing_vm(api/b35d8b46-1541-454f-b53a-02e1338d4249 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-694953] request: {\"method\":\"create_vm\",\"arguments\":[\"8d64169b-0ad0-4f41-bba3-cf2dbec3daba\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.135\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-api\",\"groups\":[\"bosh-lite\",\"cf\",\"api\",\"bosh-lite-cf\",\"cf-api\",\"bosh-lite-cf-api\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-694953\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:39.101675 #15654] [create_missing_vm(singleton-blobstore/48d7c3dc-80f7-457a-a170-a14892efb798 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-546939] request: {\"method\":\"create_vm\",\"arguments\":[\"c16b0919-9c4a-4fd3-8b79-651303dbdb9e\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.134\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-singleton-blobstore\",\"groups\":[\"bosh-lite\",\"cf\",\"singleton-blobstore\",\"bosh-lite-cf\",\"cf-singleton-blobstore\",\"bosh-lite-cf-singleton-blobstore\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-546939\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:39.184400 #15654] [create_missing_vm(router/096a6981-53d9-414a-abdd-bcd716f72403 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-197781] request: {\"method\":\"create_vm\",\"arguments\":[\"620bed02-5b0d-4967-81b8-7785d93bc3c8\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\",\"ports\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.34\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-router\",\"groups\":[\"bosh-lite\",\"cf\",\"router\",\"bosh-lite-cf\",\"cf-router\",\"bosh-lite-cf-router\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-197781\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:39.253727 #15654] [create_missing_vm(doppler/1b857023-f364-4c60-938c-069f11ae06e1 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-588983] request: {\"method\":\"create_vm\",\"arguments\":[\"21813d36-57dc-4a27-a1da-2afbf942a690\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.139\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-doppler\",\"groups\":[\"bosh-lite\",\"cf\",\"doppler\",\"bosh-lite-cf\",\"cf-doppler\",\"bosh-lite-cf-doppler\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-588983\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:39.296590 #15654] [create_missing_vm(log-api/3229785f-d461-4cd9-8d2f-0eba1438a2ae (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-784870] request: {\"method\":\"create_vm\",\"arguments\":[\"15a3cd8f-1484-466a-873a-dbf5648430fe\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.141\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-log-api\",\"groups\":[\"bosh-lite\",\"cf\",\"log-api\",\"bosh-lite-cf\",\"cf-log-api\",\"bosh-lite-cf-log-api\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-784870\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:40.156242 #15654] [create_missing_vm(tcp-router/e170fb40-8e68-40d6-a781-60457132d098 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-448312] request: {\"method\":\"create_vm\",\"arguments\":[\"ff75cffb-dd5f-4a6d-ae16-606d4b14e0b6\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\",\"ports\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.137\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-tcp-router\",\"groups\":[\"bosh-lite\",\"cf\",\"tcp-router\",\"bosh-lite-cf\",\"cf-tcp-router\",\"bosh-lite-cf-tcp-router\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-448312\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:40.308519 #15654] [create_missing_vm(diego-cell/adfd2ae9-223b-491a-b3d3-015dd5b71724 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-105291] request: {\"method\":\"create_vm\",\"arguments\":[\"b090868f-a881-4208-800c-1d63e2b0ac4a\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.140\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-diego-cell\",\"groups\":[\"bosh-lite\",\"cf\",\"diego-cell\",\"bosh-lite-cf\",\"cf-diego-cell\",\"bosh-lite-cf-diego-cell\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-105291\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:41.375601 #15654] [create_missing_vm(database/be94e19b-371e-4925-9210-e8a8a7d5a110 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-499417] request: {\"method\":\"create_vm\",\"arguments\":[\"d2c840c6-80ca-408f-9989-531dba2d53fc\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.131\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-database\",\"groups\":[\"bosh-lite\",\"cf\",\"database\",\"bosh-lite-cf\",\"cf-database\",\"bosh-lite-cf-database\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-499417\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:41.915057 #15654] [create_missing_vm(nats/c13e93eb-05d6-4a30-a815-070414fdb4f7 (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-291368] request: {\"method\":\"create_vm\",\"arguments\":[\"72ac6295-d134-4cf3-960e-16735cebdd35\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.129\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-nats\",\"groups\":[\"bosh-lite\",\"cf\",\"nats\",\"bosh-lite-cf\",\"cf-nats\",\"bosh-lite-cf-nats\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-291368\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:42.596169 #15654] [create_missing_vm(cc-worker/af553f4c-34aa-4944-a9ed-a7cfa484b01d (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-273216] request: {\"method\":\"create_vm\",\"arguments\":[\"d08f770e-5936-4a6d-8b04-ec8c66fffe0d\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.136\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-cc-worker\",\"groups\":[\"bosh-lite\",\"cf\",\"cc-worker\",\"bosh-lite-cf\",\"cf-cc-worker\",\"bosh-lite-cf-cc-worker\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-273216\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi\nD, [2018-09-26T07:26:44.072620 #15654] [create_missing_vm(consul/d36dd5d5-fb6b-4d85-8262-e7a84e12268d (0)/16)] DEBUG -- DirectorJobRunner: [external-cpi] [cpi-751706] request: {\"method\":\"create_vm\",\"arguments\":[\"4854e2bd-50c5-4ed2-a4c9-7b2d0a59336e\",\"fe81037c-315b-4e6e-5489-ddbdf503777c\",{\"cpu\":\"<redacted>\"},{\"default\":{\"type\":\"manual\",\"ip\":\"10.244.0.128\",\"netmask\":\"255.255.240.0\",\"cloud_properties\":{\"name\":\"<redacted>\"},\"default\":[\"dns\",\"gateway\"],\"gateway\":\"10.244.0.1\"}},[],{\"bosh\":{\"blobstores\":\"<redacted>\",\"mbus\":\"<redacted>\",\"password\":\"<redacted>\",\"group\":\"bosh-lite-cf-consul\",\"groups\":[\"bosh-lite\",\"cf\",\"consul\",\"bosh-lite-cf\",\"cf-consul\",\"bosh-lite-cf-consul\"]}}],\"context\":{\"director_uuid\":\"027d3f5c-bbaa-44b8-b516-186096d2fc07\",\"request_id\":\"cpi-751706\"},\"api_version\":1} with command: /var/vcap/jobs/warden_cpi/bin/cpi. @jaresty For bosh-lite, I just use the cloud config from cf-deployment repo.\nhttps://github.com/cloudfoundry/cf-deployment/blob/master/iaas-support/bosh-lite/cloud-config.yml. Thanks @s4heid. I test the fix and the feature vm_resources works as expected now.. \n",
    "berniedurfee-ge": "@drnic Is it even worth it? Isn't bosh-init going to ultimately replace 'bosh bootstrap'?\n. @drnic Is it even worth it? Isn't bosh-init going to ultimately replace 'bosh bootstrap'?\n. ",
    "goupeng212": "I verified. The workaround you mentioned works well!\nMany thanks!\n. I verified. The workaround you mentioned works well!\nMany thanks!\n. I don't think so.\nFrom the log shown, the job stopped succesfullly will have one entry which seems call the control file correctly. But the failed to stop jobs don't have such log entry. \nJOB_NAME stop: /var/vcap/jobs/unbound/bin/unbound_ctl\nAnd we found such problem on many envs ( we are using the bosh to deploy the same jobs on many different envs). The jobs failed to stop happened randomly, although the different env are using the same deployment codes.  For example , the job failed to stop are job 1,2,3 on env1. But the failed on env2 are job 4,5,6\n. Director version:  Version    1.3153.0 (00000000)\nStemcell version : 3169.1. Director version:  Version    1.3153.0 (00000000)\nStemcell version : 3169.1. ",
    "pmilewsk": "+1\n. +1\n. ",
    "wkielas": "+1\n. ",
    "jhunt": "@dpb587-pivotal - can we get this PR merged then?\n. +1. CLA: should be signed as a member of Stark & Wayne (also, I think I signed a personal one myself)\nUpdates: Looks like the affected vCenter has every possible integration (including some 3rd-party SAN solution) enabled.  Per VMware, those plugins and integrations are essentially microservices that vCenter combines in the API that BOSH queries, so any one of these (including those not developed inside of VMWare proper) could be returning output with mixed encodings, leading to this bug.  Unfortunately, this has made it difficult to reproduce in environments outside of the original failing one, due to licensing and hardware constraints.\nI will keep looking for a way to reproduce this.\nPossibly related: http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1791\n. Until cloudfoundry/bosh-init#41 gets taken care of, I've been using a variation on this:\ncurl -L https://bosh.io/d/github.com/cloudfoundry/bosh?v=255.3 | sha1sum\nand then either post-processing the output of sha1sum with awk { print $1 } for use in automated pipelines, or manually updating the manifest with a copy-paste.\nIt's workable, but definitely not ideal, because it necessitates downloading the stemcell / release at least twice. :disappointed: \n. Hah, yeah that's wrong.  Tests sure would be helpful here :grin:\n. I'm not 100% sure, but with the changes in place, things like bosh help ssh seem to work...\nIs there a way I can test that?\n. Yes.  bosh ssh jobname/0, bosh ssh jobname/0 ls /tmp and bosh ssh jobname 0 all work\n. Also, various invocations of bosh scp also still work.\n. Currently, bosh scp allows the following calling conventions:\nbosh scp box_z1/3 /a /b --upload\nbosh scp box_z1 3 /a /b --upload     # space-separated job and index number\nChanging the call signature of scp(*args) to either scp(job, index, source, destination) or scp(job_and_index, source, destination) would seem to force one and drop support for the other.\nI suppose we could do something like this:\ndef scp(job, index, source, destination)\n  if destination.nil? && job.match('/')\n    destination = source\n    source = index\n    job, index = job.split('/', 2)\n  end\n  # ...\nend\nI find that a bit more awkward than the current *args approach, but if that is preferred to (ab)using the usage string, I don't mind.  I really just want the help screen to reflect accurate usage of bosh scp, since I was also initially confused about how to invoke it.\n. I'm not actually even sure that the above code would work for the /-separated job/index form, since that's only three arguments and the function requires four.  And making the destination flag optional leads to bad documentation (since it's not really optional)\n. ",
    "jadekler": "+1\n. +1\n. We're seeing this problem, too\n. We're seeing this problem, too\n. +1, also having this problem. Think it may be related to an old openssl version\n. @dpb587-pivotal Thanks for the reply. We did end up with something like that (diff instead of cmp), but agreed that a warning makes sense.\n. @dpb587-pivotal Thanks for the reply. We did end up with something like that (diff instead of cmp), but agreed that a warning makes sense.\n. Thanks @cppforlife \n. ",
    "avade": "This is something I would be very interested in for the On Demand Broker. \nUsers want to specify a deployment prefix, so we would like to rename all existing instances to have this prefix.. @cppforlife has this been scheduled? Also causing our team an issue.\n. @cppforlife thoughts on this issue? Seems like it would be easier to know upfront if a deployment is going to fail due to an existing operation rather than having to poll another endpoint.\n. Do you mean the errand is exiting with the wrong exit code?. Ahh sorry fixed it :D . ",
    "kmacoskey": "+1\nThis was proposed a long time ago. Why is this still not a feature?. +1 yay! This issue was opened in 2015!. ",
    "doty-pivotal": "+1. ",
    "divyabhargov": "+1. ",
    "jriguera": "+1. ",
    "vlad-stoian": "+1. +1. Hi,\nI've hit the exact same problem while working on RabbitMQ related links which have the same structure as @hoegaarden has explained above. My intuition was that I would get the default values in the hash, but they are missing, which means I have to use something similar to a hash.fetch('key', default_value), which is not ideal, since one job has to know somehow the default value specified in the other job's spec.\nI'm interested in the questions above as well.. Hi,\nI've hit the exact same problem while working on RabbitMQ related links which have the same structure as @hoegaarden has explained above. My intuition was that I would get the default values in the hash, but they are missing, which means I have to use something similar to a hash.fetch('key', default_value), which is not ideal, since one job has to know somehow the default value specified in the other job's spec.\nI'm interested in the questions above as well.. ",
    "ramonskie": "+1. any update on this?\nbecause those deprecation warnings are really annoying..\n. solved this issue already?\nbecause we face the same issue\n. in the meantime i solved it with https://github.com/ramonskie/fakepackage-boshrelease\n. EDIT:\nabove won't work because of hard coded library path in backup.erb & restore.erb\nso will you accept a PR?\nso that bosh will also accept the use of pg 9.6 in backup/restore scripts?. @cppforlife i'm interested in ssh integration with ldap. ",
    "chrisahl": "+1. ",
    "pivotal-jbarrett": "+1. ",
    "kitsirota": "+1. +1 Looks like https://www.pivotaltracker.com/n/projects/956238/stories/159605314 hasnt been updated since Sept 27th.  \nWill this feature ever make it on the roadmap?. +1  Im running into this very issue right now.  When migrating to S3, all deployments and releases have to be deleted before bosh will actually upload the releases to the new blobstore.  This is a no go for any existing deployments for us.\nI can think of a number of ways a blobstore can get into a bad state.  The most obvious is if the S3 bucket is accidentally deleted.\n. +1 We ran into this very recently also.  \nIt would be great if we could also store director compiled assets on S3 to speed up bosh director updates and deployments.\n. This was particularly painful when we were transitioning all of our directors from a local blobstore to an S3-backed one.  There is no clear migration path for blobstores, we ran into a few cases where a director had been migrated but the deployments still had out of date bosh agent settings which pointed to the director for the blobstore.  Thinking back on this, listing the blobstore provider on the director wouldnt actually address the issue of outdated bosh agent configs on cells.  . +1 I just ran into this with 260.6. ",
    "pivotal-jamil-shamy": "@mfine30 \nPlease note that when using non absolute credhub variable names (i.e. they do not start with a /), the director will append the director name and Deployment Name before it fetches/generates these variables from credhub.\nNeed to take that into consideration when this feature is worked on.. @mogul in the next release of BOSH (soon), we're implementing mutual TLS for NATS communications. This will eliminate the need for passwords when connecting to the NATS server (for new stemcells only).\nNote: to be backwards compatible, we still support password authentication for old stemcells.\nWith mutual TLS, we'll have the ability to rotate certificate with potentially zero downtime (update: I believe for now a VM recreate will neccessary for the certificates rotation).. @wickyhasan to give some context:\n\nHistorically, NATS communication only used username/password for authentication and authorization. Changing NATS username/password or rotating them was not possible without hitting the unresponsive VMs situation.\nStarting with BOSH version 264 and linux stemcell version 3468 we added the ability for mutual TLS communication for the BOSH NATS. When communication is done through mTLS, the NATS username/password are not used for any authentication or authorization. But we still kept the ability for NATS clients to authenticate over username/password for backword compatibility reasons (For example an older stemcell).\nIf the operator knows that all the relevant components (BOSH Director and stemcells) support mTLS NATS communication, we added a configuration to the NATS server job in the BOSH release to only allow mTLS connections and drop username/password authentication.  As stated above this is the flag nats.allow_legacy_agents. This flag, when set to false, will guarantee that the communication is over mTLS.\nAt some point we will remove that flag and only allow mTLS NATS communication. But due to the older stemcell support, this did not happen yet.\n\nQuestion: What is the reason you are trying to change the NATS password ? Which version of the director and stemcells you are using ? . @antonsoroko could this be achieved through BOSH links ? . @loewenstein currently we don't support variables interpolation in the Cloud Config. There should be a plan to support it eventually.\nAlso, are you using credhub or config server in your bosh-deployment ? or are you just using bosh-cli for variable interpolation ?  . @loewenstein @beyhan true, error message could be better. In the meanwhile you can use the new cli to interpolate the cloud-config before uploading it. Example:\nbosh interpolate /path/to/cloud-config.yml -v dns_nameservers=\"8.8.8.8\" >> interpolated-cloud-config.yml\nThis command will interpolate the cloud-config, then you can separately upload interpolated-cloud-config.yml\nCheck bosh interpolate -h for more options.\n. @cppforlife preferably each consumer would get a separate \"client\" with dynamic authorities; great for auditing, tracing, and revocation of a link when needed.. thanks @peterellisjones for submitting the pull request !\nWe've just recently started working on enable this behavoir. \nRegarding the questions (in order):\n1- We usually render these kinds of certificates through an ERB template, rather than dumping them into files in config.rb.\n2- We're still thinking if we need integration tests for the TLS option or have a separate test suite for it.\n3- Answer in first point\nWe'll keep you posted as we go through the changes, we may end up merging the pull request and doing some modifications.\nthanks again for the pull request .  @peterellisjones , btw have you tried connecting to a postgres instance over TLS? In our development we needed to bump the sequel gem version to be able to connect.. @voelzmo interesting idea\nOne comment though: Maybe it can be renamed to be filter by instance type rather than VM type, so that it will not be mistaken for VM types defined in the cloud config(s), wdyt ? . @EdwardStudy there is a plan to remove the registry all together from BOSH architecture. We're going to start that work soon, so I would not worry too much about the registry SPOF for now.. Did this deployment ever succeed ? When asking for the manifest and it's returns nothing is a symptom for that\n\nFrom: William Martin notifications@github.com\nSent: Thursday, December 14, 2017 6:52:40 AM\nTo: cloudfoundry/bosh\nCc: Subscribed\nSubject: Re: [cloudfoundry/bosh] no implicit conversion of nil into String when Director has no manifest for deployment (#1854)\nI am also seeing this on:\nUsing environment '192.168.50.6' as client 'admin'\nName      Bosh Lite Director\nUUID      b6b85589-82b8-476d-9341-b51062feb96d\nVersion   264.1.0 (00000000)\nCPI       warden_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\nSucceeded\nbosh task --debug output:\nUsing environment '192.168.50.6' as client 'admin'\nTask 120\nI, [2017-12-14T11:51:23.360134 #2910] [0x2b1ab95d2f4c]  INFO -- TaskHelper: Director Version: 264.1.0\nI, [2017-12-14T11:51:23.360178 #2910] [0x2b1ab95d2f4c]  INFO -- TaskHelper: Enqueuing task: 120\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Looking for task with task id 120\nD, [2017-12-14T11:51:23 #650271] [] DEBUG -- DirectorJobRunner: (0.000337s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Found task #120, :state=>\"processing\", :timestamp=>2017-12-14 11:51:23 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/120\", :checkpoint_time=>2017-12-14 11:51:23 UTC, :type=>\"update_deployment\", :username=>\"admin\", :deployment_name=>\"cf\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Running from worker 'worker_3' on director/3cde8d74-66c1-4fdc-5850-0a770078f2e8 (127.0.0.1)\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Starting task: 120\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Creating job\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000271s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000269s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Performing task: #120, :state=>\"processing\", :timestamp=>2017-12-14 11:51:23 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/120\", :checkpoint_time=>2017-12-14 11:51:23 UTC, :type=>\"update_deployment\", :username=>\"admin\", :deployment_name=>\"cf\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000205s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000371s) UPDATE \"tasks\" SET \"state\" = 'processing', \"timestamp\" = '2017-12-14 11:51:23.648408+0000', \"description\" = 'create deployment', \"result\" = NULL, \"output\" = '/var/vcap/store/director/tasks/120', \"checkpoint_time\" = '2017-12-14 11:51:23.648534+0000', \"type\" = 'update_deployment', \"username\" = 'admin', \"deployment_name\" = 'cf', \"started_at\" = '2017-12-14 11:51:23.648481+0000', \"event_output\" = '', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000730s) COMMIT\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Reading deployment manifest\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000676s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000346s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000489s) INSERT INTO \"events\" (\"parent_id\", \"timestamp\", \"user\", \"action\", \"object_type\", \"object_name\", \"error\", \"task\", \"deployment\", \"instance\", \"context_json\") VALUES (NULL, '2017-12-14 11:51:23.655477+0000', 'admin', 'update', 'deployment', NULL, 'no implicit conversion of nil into String', '120', NULL, NULL, '{}') RETURNING *\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000713s) COMMIT\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000312s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000357s) UPDATE \"tasks\" SET \"event_output\" = (\"event_output\" || '{\"time\":1513252283,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n') WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000593s) COMMIT\nE, [2017-12-14T11:51:23 #650271] [task:120] ERROR -- DirectorJobRunner: no implicit conversion of nil into String\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:377:in parse'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:377:inparse_stream'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:325:in parse'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:252:inload'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:28:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:99:inperform_job'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in block in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:in block in perform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:83:inblock (3 levels) in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/eventmachine-1.2.5/lib/eventmachine.rb:1076:in block in spawn_threadpool'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/logging-2.2.2/lib/logging/diagnostic_context.rb:474:inblock in create_with_logging_context'\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000182s) SELECT * FROM \"tasks\" WHERE (\"id\" = 120) LIMIT 1\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000202s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000348s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2017-12-14 11:51:23.662943+0000', \"description\" = 'create deployment', \"result\" = 'no implicit conversion of nil into String', \"output\" = '/var/vcap/store/director/tasks/120', \"checkpoint_time\" = '2017-12-14 11:51:23.648534+0000', \"type\" = 'update_deployment', \"username\" = 'admin', \"deployment_name\" = 'cf', \"started_at\" = '2017-12-14 11:51:23.648481+0000', \"event_output\" = '{\"time\":1513252283,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.001053s) COMMIT\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Task took 0.019790606 seconds to process.\nWhen I do bosh manifest, the task reports it succeeded but I don't get the manifest back.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1854#issuecomment-351689994, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AESOLNkTljEZlH2_9U06hKgjAprnR5aaks5tAQwIgaJpZM4Q1j0W.\n. This should be fixed in 262.5 https://github.com/cloudfoundry/bosh/releases/tag/v262.5.0\n[edited: fixed link to release tag]\n\nFrom: Peter G\u00f6tz notifications@github.com\nSent: Thursday, December 14, 2017 5:18:08 AM\nTo: cloudfoundry/bosh\nCc: Subscribed\nSubject: [cloudfoundry/bosh] Preparing deployment incorrectly fails due to check with wrong job spec (#1859)\nWe recently deployed cloudfoundry/cf-deployment@3f17344https://github.com/cloudfoundry/cf-deployment/commit/3f173445ea5044ff3e06d6927bce2080fe164208 which downgrades the loggregator-release from 101.3 to 99 and got the following error message from BOSH:\n...\nRelease 'loggregator/99' already exists.\nRelease 'nats/22' already exists.\nRelease 'php-buildpack/4.3.44' already exists.\nRelease 'python-buildpack/1.6.3' already exists.\nRelease 'routing/0.168.0' already exists.\nRelease 'nodejs-buildpack/1.6.12' already exists.\nRelease 'ruby-buildpack/1.7.7' already exists.\nRelease 'staticfile-buildpack/1.4.20' already exists.\nRelease 'uaa/53' already exists.\nRelease 'bits-service/1.4.0-dev.32' already exists.\nRelease 'statsd-injector/1.0.30' already exists.\nreleases:\n  - name: loggregator\n-   sha1: d9ee90b2ba713f12f2a9a831840a62212ee774fb\n+   sha1: 2080e1e0594591dafa716c69f207eb29929bce3d\n-   url: https://bosh.io/d/github.com/cloudfoundry/loggregator-release?v=101.3\n+   url: https://bosh.io/d/github.com/cloudfoundry/loggregator-release?v=99\n-   version: '101.3'\n+   version: '99'\nTask 5900\nTask 5900 | 22:51:46 | Preparing deployment: Preparing deployment (00:00:01)\n                     L Error: Job 'metron_agent' in instance group 'consul' specifies link 'doppler', but the release job does not consume it.\nTask 5900 | 22:51:47 | Error: Job 'metron_agent' in instance group 'consul' specifies link 'doppler', but the release job does not consume it.\nTask 5900 Started  Fri Dec  8 22:51:46 UTC 2017\nTask 5900 Finished Fri Dec  8 22:51:47 UTC 2017\nTask 5900 Duration 00:00:01\nTask 5900 error\nCould not deploy: Updating deployment: Expected task '5900' to succeed but state is 'error'\nRunning the following, fixed it for us:\nbosh -e sl delete-release loggregator/101.3\nHowever, that shouldn't be necessary, because the check that is happening while Preparing deployment should not base that check on the latest available loggregator-release, but on the one specified in the deployment manifest.\nMore info:\nThe downgrade from 101.3 to 99 removes the following lines from the metron_agent job spec:\nconsumes:\n- name: doppler\n  type: doppler\n  optional: true\nBOSH director version: 262.3\nBOSH CPI: bosh-softlayer-cpi, version 23\n/cc @suhlighttps://github.com/suhlig @smoser-ibmhttps://github.com/smoser-ibm @idev4uhttps://github.com/idev4u\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1859, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AESOLMcWHAVsYLG54Mm49cJkIJXvxgtvks5tAPXggaJpZM4RB1T4.\n. There is a track of work currently in progress that is somehow related but not quite the same. We would be interested in any feedback.\nhttps://github.com/cloudfoundry/bosh-notes/blob/master/proposals/update-strategy-create.md\ncc @GarfieldIsAPhilosopher @monkeyherder\n\nFrom: Jason Keene notifications@github.com\nSent: Saturday, December 16, 2017 4:16:36 AM\nTo: cloudfoundry/bosh\nCc: Subscribed\nSubject: [cloudfoundry/bosh] Over-provisioning during deployment (#1860)\nWe were recently discussing migrating user workloads during an upgrade deployment of CFCRhttps://github.com/cloudfoundry-incubator/kubo-release. If a user is close to being fully utilized (for instance having enough pods to fill 4.5 out of 5 worker nodes), even if max_in_flight is set to 1, there is still potential downtime with a single instance being out of service.\nThe way k8s handles thishttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment is they have maxSurge and maxUnavailable settings. maxUnavailable sets a floor to prevent available capacity from going too low. For our situation this could be set to 0 to prevent any loss of capacity. maxSurge allows for over-provisioning. New instances are brought up to start handling workloads. Once these instances are up old instances can then be brought down.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHubhttps://github.com/cloudfoundry/bosh/issues/1860, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AESOLLF_HE7i5dg97J4eN_4lNBa48FKgks5tA4pzgaJpZM4RER5E.\n. @apsraps when doing bosh stop <specific instance> on a specific instance, bosh will stop the jobs processes on that VM; it will not stop the actual VM. When you do bosh stop --hard <instance>, bosh will destroy the actual VM, but will keep persistent disks if one is already attached to the VM.\nFrom bosh perspective (at least for now), there isn't a way to just stop the VM.\nWhat you are seeing when you are manually stopping the VMs from the AWS, is the bosh resurrector (it is a bosh health monitor plugin) will detect that one of the VMs are gone, and it will create a new VM to replace it.\n\nWhat is a clean way to stop the CF EC2 instances, so we dont incur cost when these instances are not in use.\n\nOne way you can do that is to bosh stop -- hard ... the instances, this will destroy the VMs, but will keep persistent disks for the VMs around, then when you want to bring the VMs back, you can bosh start the VMs, and this will created new VMs and attaches the already existing persistent disks to the corresponding VMs.\nI am assuming this is some king of development environment you have here ? \n. > Is it important to retain the deterministic update order where instances in the same AZ as the bootstrap instance get updated before other instances, even when the bootstrap instance itself is ignored?\nI think it should keep the order of update.\nReference Tracker Story. Pivotal tracker story for investigation. \n@Amit-PivotalLabs can you keep the environment around (if possible) for us to access it for investigation when we get to the story ? . Pivotal Tracker Story to Investigate. @mikomraz have you only seen the issue on ubuntu-xenial or did it happen also on ubuntu-trusty stemcells ? . @mikomraz any new update on this issue ? . @bandesz thanks for reporting this issue. Looks like the manual links introduced under name=log-cache-nozzle/consumes/reverse_log_proxy and name=log-cache-cf-auth-proxy/consumes?/cloud_controller? are also registering themselves as providers that cause this issue.\nWe will investigate more and submit a fix (if needed). Tracker Story for the issue.. @bandesz The issue should is fixed in 266.6.0 release.\nLet us know if that works for you.. @aegershman would this issue be solved by using more filters when you're retrieving events ? For example filtering by --task, --object-type, --object-name, and --action, this should narrow down the count of events you care about significantly.\nWhat's your main objective for getting more than 200 events at once ? . Hey @cunnie , thanks for submitting the issue.\nBOSH will not interpolate the variables before displaying the diff (for security reasons); I am a bit surprised that it is showing the diff.\nQuick question to understand the flow: did you supply run.pivotal.io variable through the CLI -v option before, and now you changed it to be supplied by credhub through BOSH ? . @cunnie any update on this issue ? . @mfine30 no story for it. I'll add one for Toronto to review and merge.\nStory for reference: https://www.pivotaltracker.com/story/show/159364702. @ninja-at-work there is currently no concept in BOSH where we mark a deployment state for deletion.\nFor the situation you posted, I see the benefit of telling BOSH to stop the resurrection of VMs in a deployment upon request. There is currently no way to tell BOSH to stop resurrection for a particular deployment through the CLI; there is a way to stop resurrection for all deployments globally, but that would be an overkill/not recommended for a similar situation as the one you posted.\nThere is a plan to add a resurrection configuration for BOSH that will contain granular controls over resurrection (I think the work on that is in progress). You can check this note containing more details about this new feature https://github.com/cloudfoundry/bosh-notes/blob/master/proposals/resurrection-config.md\n. @mfine30 \n\nI'm not sure I agree with bosh env though and if there's a better spot.\n\nI can see it as a dedicated BOSH CLI command (see below)\n\nAdditionally, how does credhub play into this? And are we scoping this specific issue just to the certs that the BOSH \"system\" is using, rather than also including the various certs in the deployments?\n\nNow that you have mentioned that, what about a BOSH CLI command (maybe bosh check-certificates or along its meaning) that does the following:\n\nbosh check-certificates --system will display the validity of certs used by the BOSH \"system\" components\nbosh check-certificates --deployment=<deployment-name> will display all the certs used a certain deployment and their expiry date (including certs consumed from other deployments using cross-deployment links). We already save all the variables versions IDs that are used by a specific deployment at any point in time (although we do not save the type of variables if it is a cert or not, so there maybe some work there).\n\nIn addition to a BOSH CLI feature that will throw a warning on each command if the director cert is will expire in x number of weeks.\nwdyt ?. @techie20122018 I believe @dpb587-pivotal was referring to changing the DNS entries for the bosh create-env manifest. You can use this ops file suggested above to accomplish that.. thanks @poblin-orange for reporting this.\nI was able to reproduce the issue. Looks like there is a problem with how cloud-config variables are being versioned in the director. . @s4heid thanks for reporting the issue. This recent commit should potentially fix this bug . @voelzmo \n\nWe're at a scale where NATS seems to become a bottleneck with the current configuration.\n\nI am curious about what is causing this bottleneck. \nIs there a tracker story or a Github issue that have more details ? . @degaurab I do not have any logs handy right now, but I can share them with you if you want. \nThough the issue is 100% reproducible; you should be able to easily reproduce it and check any specific logs in your environment. Probably more practical this way.. ",
    "frankgh": "+1. ",
    "tyacovone": "+1. ",
    "gaos1": "I will submit a pull request for this. \n. will wait for official support for china region. \n. ",
    "barthy1": "Fixed\n. @dpb587 thanks! The code is updated.\n. It's to be able to support default_update_config \nhttps://github.com/cloudfoundry/bosh/blob/7d727d83ce66b4b6c4902f9fbfdac1eb7ab35f15/bosh-director/lib/bosh/director/deployment_plan/update_config.rb#L52-L61\n. Hi @zaksoup  and @monkeyherder!\nThank you for your time and efforts. I think you are right in the comments before regarding confusion with returned types, I refactored the code to make it clearer, especially in case of max_in_flight and canaries initial values and its calculation. \nMethod parse_numerical_arguments was actually private, but I've added some unit testing for it. \n. @cppforlife Sorry, my bad. Fixed it.\n. @dpb587-pivotal Will update the code asap\n. This PR will be moved to separate repo: https://github.com/cloudfoundry/bosh-linux-stemcell-builder. @beyhan thank you for the info. I found your code very useful. I will update my PR according to it. @dpb587-pivotal Thank you for your opinion. I think your ideas are very reasonable. I've updated the code.. @antonsoroko, what feature do you mean? deployment filter is available for exclude section in addons and deployment level addons can contain exclude section. This PR works together with https://github.com/cloudfoundry/bosh-cli/pull/217. The code was updated. Please, review it, when you have some time. works with https://github.com/cloudfoundry/bosh-agent/pull/131. @cppforlife found use case to recreate the error - there should not be any \"healthy\" instances in the deployment and it prevented resurrector to send request to scan_and_fix. I added commit to fix it. @dpb587-pivotal thank you for catching that. @dpb587-pivotal thank you for your comments. Could you please review the pr again?. @jraqula thank you for your ideas, I think they are very reasonable. I moved the logi\u0441 into InstanceLookup, and now, I believe, code looks clearer. https://github.com/cloudfoundry/bosh/pull/1809#issuecomment-337423063 was taken into account. Fix is done, integration test is added.. Any comments are highly appreciated. I reviewed several approaches for rate limits and found that the implemented one has fewer interactions with db and fewer db record updates, but may have less flexibility in terms of potential extensions.. @mikexuu Code updates are done. Could you please have a look one more time? . The PR requires also https://github.com/cloudfoundry/bosh-cli/pull/386. It was mentioned in the description of task: \"bosh stop --hard is useful for releasing all of the compute resources without losing any persistent data\". Idle vm keeps resources to use.\nAdditionally, the task was \"bosh stop --hard job1 should no longer show that vm in bosh vms\"  - this idle vm is showed in bosh vms.\n. @dpb587-pivotal, thanks for the idea. Let's use \"Last Activity\"\n. @dpb587-pivotal: yes, this behaviour is expected. In case of instance recreation, current vm should be deleted (10, 11) and new vm should be created (12, 13). vm creation/deletion events are ending actions (https://github.com/cloudfoundry/bosh-notes/blob/master/events.md), so each event has starting and ending records. It's not important for \"deploy --recreate\" case, so we didn't add these lines to the table check.\n. Both cli's use *  job in case of entire deployment, just like * index_or_id. Let me move the check of '*' job from InstanceManager to DeploymentsController to make it clearer.\n. @cppforlife it works as is. Do I need to update the code to support event ids as integer? . it's part of installed by image_install_grub context and excluded for ppc64le. Sorry, I will remove it from this PR, it is part of another possible PR related to CI. Answering to your question, for ppc64le it should be\ndeb http://ports.ubuntu.com/ubuntu-ports/ trusty main restricted\ndeb-src http://ports.ubuntu.com/ubuntu-ports/ trusty main restricted\n...\ndeb http://ports.ubuntu.com/ubuntu-ports/ trusty-security universe\ndeb-src http://ports.ubuntu.com/ubuntu-ports/ trusty-security universe \nand have repos already in base ppc64le/ubuntu:14.04 image. I had a deeper look at it, and also don't understand the reason, why it works. So I've updated the code to feel more comfortable. by default with old versions of bosh_cpi, cpis return another error - InvalidCall, converted to UnknownError with varied text messages. Is it particular enough to check?. yep, just waiting for https://github.com/cloudfoundry/bosh-davcli/pull/6, to do all updates in one commit. It seems that this is not necessary. We had NameError: uninitialized constant Bosh::Director::Core::Templates::SecureRandom error on the machine without this require. Just checked on another machine and everything worked well. So, problem with local config.. @tylerschultz Mysql is not allowed to set default value for text type https://dev.mysql.com/doc/refman/5.7/en/blob.html. So I updated the test name and added extra checks, you mentioned. If you have an example, how it's possible to set default value for text, could you please share it with me?. I just didn't want to do extra checks (+ cpi call) in case if it is not necessary and stemcell is already available. But I see your point and will update the code. I updated it, because there was \"connected\" error in unit test. But now I see that I had to fix unit test instead. Thank you for the good catch. @zankich approach with before_create in model does not work for migration, when there are already some records. Anyway I've added it for new records.. @DennisDenuto Thank you for the review. Could you please provide any sample of the sequel concat for model objects? So far I found that Sequel.x works only for dataset actions. And in case if it is the dataset action, to update the model object I need to use task.refresh, so no reduction of interactions with db.. I will extend the unit tests. @dpb587-pivotal, thank you for you comment. Actually because of it I found some non-standard behaviour in case of cloud-check related vm recreation. In that case there is no release information so far. Investigating, how I could fix it.... @dpb587-pivotal the main idea, as far as I remember, that in case of vm delete, there are 2 exceptions - VMNotFound and InstanceNotFound, and both of them shouldn't be shown in logs as task failure. However today I haven't been able to recreate this misleading behaviour, so let me update the pr, removing this logic. @dpb587-pivotal at this moment you cannot specify job name as something like '1.1.1.1', because of error Invalid DNS canonical name '...', must begin with a letter. At least that's what I had. But you are correct, it's better to cover this possibility for the future possible updates.. I believe each method should check the input it used to avoid problems with nil array. has_applicable_team? method uses @applicable_teams array, so I think it's better to have the check. I think it's for the specified team. ",
    "Gary-Xie": "The another field named trusted_certs has the same issue \n. Does anyone know this story? Any question please ask me? I just want to find the reason.\n. ",
    "pivotal-nader-ziada": "Why is the empty line needed in the sudoers file? is it causing a problem?\n. Hello \nWe can't seem to recreate the issue, tried to create and upload a release on both mac and linux without issues. Can you maybe give us more details about your environment and maybe a debug log?\nThanks \n. can you run: bosh task {the_failed_task_number} --debug\n. is it possible to share the release tarball you were trying to upload? and are you running on mac?\n. We still can't seem to recreate the problem. Did you create the tarball using create release? We have seen cases where this error happens when the tarball is created manually.  Can you send the listing for tar -tvf against the tarball you are using? \n. ",
    "eLobeto": "I see the same issue and running debug shows me the following:\nD, [2015-12-03 19:08:36 #28459] [] DEBUG -- DirectorJobRunner: Worker thread raised exception: undefined method `cid' for nil:NilClass - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3126.0/lib/bosh/director/instance_deleter.rb:31:in `delete_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3126.0/lib/bosh/director/instance_deleter.rb:21:in `block (3 levels) in delete_instances'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3126.0/lib/common/thread_pool.rb:77:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3126.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3126.0/lib/common/thread_pool.rb:63:in `loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3126.0/lib/common/thread_pool.rb:63:in `block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\nD, [2015-12-03 19:08:36 #28459] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nNot sure if this is the same problem.\n. ",
    "hashmap": "@allomov yes, I was able to deploy bosh using bosh-init with ubuntu 14.04 default ruby (1.9.3)\n. @voelzmo do you deploy using bosh-init? I wonder why I  don't have this issue with this code on dev build.\n. Re-sending this pr against develop\n. @cppforlife my idea is to have this check in agent_client.drain only, why to keep this one? there is a spec for this change (stopper_spec.rb, the last one on this page). not sure it's related to agent version, it will be cancelled in bosh anyway and gets an error \"cancel is not supported for this action\" from older agents.\n. @cppforlife thanks for raising it, found i forgot to include a change in 'instance_deleter' to my commit.\n.  Tried, Bosh::Stemcell::Arch.ppc64le? + require on top doesn't look better. Or does it?\n. You mean we don't need to patch packages like ruby and yaml, sounds good. I thought it may be required for smth else.\n. Perhaps replace with the ternary operator?\n. ",
    "bryanrossUK": "vRealize provides very similar higher level abstraction as vCloud Director.  My understand however is that vCloud Director is now targeted at Service Providers and vRealize at Enterprises.  Confusingly enough, the vCloud Suite includes vSphere and vRealize!\nSome of the benefits of VRA is that it provides a general IT service catalog, more fine grained access control to allow proper multi-tenancy, abstraction of VM configuration, request approvals, and possibly most importantly, integration into vRealize Orchestrator.  vRO is a general purpose workflow tool for integrating a variety of systems into a companies provision cycle, such as CMDB, DNS, Monitoring, Alerting, Inventory, Software Compliance, etc.  Adding support for vRA would allow large enterprises to offer Cloud Foundry as a deployable service to internal product development teams; not to mention potentially using Bosh for the deployment of other tools.\n. ",
    "Penguin2600": "+1 for this.. +1 for this.. ",
    "jcarrothers-sap": "Yes, my mistake.  I tried to prune irrelevant information and failed to ensure the yml syntax was still valid.\nAs an additional piece of information, I've ensured I'm using the latest BOSH release (217) and the latest vcloud_cpi release (18).  I'm using the bosh-vcloud-esxi-ubuntu-trusty-go_agent stemcell release 3115 for the multihomed VMs deployed by BOSH.\n. Confirmed fixed with bosh-vcloud-esxi-ubuntu-trusty-go_agent v3137\n. ",
    "simonleung8": "@cppforlife I am using 3215 and run into the exact problem. I am using the default BSD tar that shipped with mac.\n. ",
    "coreyti": "Adjusted to be something like this: https://github.com/cloudfoundry/bosh/commit/6b9ecf19d3b4dd0d54380fe5eda4f4e8c2312b07\n. Adjusted to be something like this: https://github.com/cloudfoundry/bosh/commit/6b9ecf19d3b4dd0d54380fe5eda4f4e8c2312b07\n. Note that specs are currently failing at this point, due (in part) to the fact that the specs are passing a timestamp, instead of the Alert object that is now expected.\nSorry. We need to fix that before merging.. Note that specs are currently failing at this point, due (in part) to the fact that the specs are passing a timestamp, instead of the Alert object that is now expected.\nSorry. We need to fix that before merging.. This PR is superseded by https://github.com/cloudfoundry/bosh/pull/1680. This PR is superseded by https://github.com/cloudfoundry/bosh/pull/1680. Thanks @voelzmo !. Thanks @voelzmo !. @cppforlife @dpb587 @tylerschultz, updated this PR to restore the existing system.healthy definition, and simply add the new system.health.#{state} metrics.. @cppforlife @dpb587 @tylerschultz, updated this PR to restore the existing system.healthy definition, and simply add the new system.health.#{state} metrics.. fix or remove this comment?\n. perhaps we can use get_os_type instead of duplicating logic\n. seems like this is doing the exact same thing as https://github.com/cloudfoundry/bosh/blob/master/stemcell_builder/stages/prepare_raw_image_stemcell/apply.sh aside from name; perhaps existing raw type could be used?\n. should remove this section too\n. don't forget to swap these\n. perhaps check mode to ensure secured/not writeable by others\n. placeholder?\n. Removed this module.\n. We discovered that GCE requires that the uploaded machine image tarball contain a disk.raw file: https://cloud.google.com/compute/docs/tutorials/building-images#packaging.\n. Added check for ownership and 644 mode\n. - [x] .\n. - [x] .\n. - [x] .\n. - [x] .\n. ",
    "yyl8815": "HI @cppforlife \nThank you for sharing this. Appreciated.\nWhat I am trying but not sure how to do is setting up a correct routing. \nFor example, I have only one IP address in 10.x.x.x which can access public network and a whole set of 192.168.1.x in a private vSwitch. I want to set most of CloudFoundry machines in 192.168.1.x. And the bridge( I don't know if it is uaa or haproxy) can have both 10.x.x.x and 192.x.x.x. \nIn this way, user can access the cloudfoundry via 10.x.x.x.xip.io and other VMs can also access public network through the bridge VM(such as download buildpack).\n. For the created VMs, yes, i can.\nHowever, for the failing VMs decribed above, I cannot since it never successfully boot up :)\nThanks\n. Hi @cppforlife,\nActually all of the VMs locats at the same host.\nAfter I kept retrying \"bosh -n deploy\", finally all VMs got installed. However, it took me more than one day to create those VMs.\nI am using ThinkServer RD650 and ESXi 6.0.0.\nThe stemcell I am using is trusty, 3098.\n. Today, I reboot the stucking VMs via the vsphere client console and each VM can boot up successfully.\nHowever, it is never next to acceptable that a manual reboot of each VM is a must for deploying cloud foundry. \nCan anyone help on this?\n. Hi @cppforlife \nYou are correct. It's strange because I have set the dns in the network configuration. The domain should be recognized.\nWhatever, after I replace the domain with IP, it worked.\nThank you. @cppforlife \n. nfs_first is attached to host of \"first\" while nfs_second to host \"second\" and nfs_third for host \"third\"\n. I then set all the the _z2 jobs to 0 instance.\nHowever, I still get two VMs placed in cf_z2 cluster\n. Hi experts,\nCould anyone kindly help me ?\nThank you in advance\n. Hi @poblin-orange\nI am using deploying cf-223 to vcenter. And it's true that I use the generate_manifest script to generate the manifest. \nActually I have added link to my manifest in my first comment \n- cloud foundry manifest: https://gist.github.com/yyl8815/169db3d06daca748773e\n- bosh manifest: https://gist.github.com/yyl8815/bd3f3833d4b69b0238b4\nIt's more likely to be a bosh issue rather than a cf issue.\n. Hi @cppforlife \nI am aware of the doc you provided. Thank you all the same.\nThe timeout is due the the mis-configuration of the IP address of the mis-placed VMs which I mentioned above in the third item which I also copied here:\n- Some VMs are with unexpected IP of 10.62.48.X. I don't know why this happen.\nWhat I want to understand and need favor from you experts are:\n\nWhy the VMs are not placed as I configured in the manifest.\nWhy some VMs are configured with unexpected and undefined IP addresses.\n\nThank you\n. OK.\nI will have a try. \nI saw below part before. Is this a must?\nproperties:\n  vsphere:\n    host: 172.16.68.3\n    user: root\n    password: vmware\n    datacenters:\n    - name: BOSH_DC\n      vm_folder: prod-vms\n      template_folder: prod-templates\n      disk_path: prod-disks\n      datastore_pattern: '\\Aprod-ds\\z'\n      persistent_datastore_pattern: '\\Aprod-ds\\z'\n      clusters:\n      - BOSH_CL: {resource_pool: BOSH_RP}\n. Sure. I know you are referring to the resource_pools section.\nI am asking that if the vsphere section in the global properties section necessary. In the original manifest generated by spiff, there is no such section and I manually added that later. I wonder if I can remove the vsphere section in the global properites section\n. OK\nThank you very much, @cppforlife \n.  HI @cppforlife ,\nI got below error, any thoughts?\n``\n   Failed creating bound missing vms > large_z2/0: Unknown CPI error 'Unknown' w                                                    ith message 'undefined methodkeys' for \"cf_z2\":String' (00:00:27)\nError 100: Unknown CPI error 'Unknown' with message 'undefined method `keys' for                                                     \"cf_z1\":String'\n```\nMy manifest:\n- cloud_properties:\n    cpu: 1\n    disk: 4096\n    ram: 1024\n    datacenters:\n    - {name: cf, clusters: [cf_z2]}\n  env:\n    bosh:\n      password: $6$4gDD3aV0rdqlrKC$2axHCxGKIObs6tAmMTqYCspcdvQXh3JJcvWOY2WGb4SrdXtnCyNaWlrf3WEqvYR2MYizEGp3kMmbpwBC6jsHt0\n  name: small_z2\n  network: cf2\n  stemcell:\n    name: bosh-vsphere-esxi-ubuntu-trusty-go_agent\n    version: latest\n. This is due to mis-configuration\n. ",
    "patrice4github": "Hi,\nI have the exact same issue, with the latest nokogiri-1.6.6.4 while trying:\ngem install rails\nint t(void) { xmlParseDoc(); return 0; }\n              ~~~~~~~~~~~ ^\n/Users/patricegagnon/.rvm/gems/ruby-2.2.2/gems/nokogiri-1.6.6.4/ports/x86_64-apple-darwin13.4.0/libxml2/2.9.2/include/libxml2/libxml/parser.h:841:11: note: 'xmlParseDoc' declared here\nXMLPUBFUN xmlDocPtr XMLCALL\n          ^\n1 error generated.\nchecked program was:\n/* begin */\n 1: #include \"ruby.h\"\n 2: \n 3: #include <libxml/parser.h>\n 4: \n 5: /*top*/\nPerhaps using RVM changes something?\nSorry I could not figure out how to print the code correctly.\n. ",
    "dprotaso": "@jasonkeene I've seen this issue if you have xz package installed from brew\n. ",
    "jasonkeene": "I'm closing this issue. The new bosh cli is implemented in Go with all of its deps vendored and can be compiled as a static binary.. I'm closing this issue. The new bosh cli is implemented in Go with all of its deps vendored and can be compiled as a static binary.. I ran into this issue as well.\nI was making a change to the Loggregator acceptance tests to use links vs properties. Loggregator is deployed as it's own deployment and the acceptance tests errand is a separate deployment that consumes links for the locations of the endpoints for Loggregator (metron agent, rlp, and trafficcontroller). The metron agents deployed with Loggregator only listen on localhost so we can't talk over the network to them. To get around this we colocate a metron agent on the acceptance tests errand's instance group. It would be nice to allow the errand to consume links from jobs in its own instance group for ports, certs, etc.. I ran into this issue as well.\nI was making a change to the Loggregator acceptance tests to use links vs properties. Loggregator is deployed as it's own deployment and the acceptance tests errand is a separate deployment that consumes links for the locations of the endpoints for Loggregator (metron agent, rlp, and trafficcontroller). The metron agents deployed with Loggregator only listen on localhost so we can't talk over the network to them. To get around this we colocate a metron agent on the acceptance tests errand's instance group. It would be nice to allow the errand to consume links from jobs in its own instance group for ports, certs, etc.. The create strategy would work for us if the old instances get deleted after the deploy is finished. That said, I imagine creating and deleting all these VMs (even in bulk) will take considerable time over simply shipping new bits to existing VMs and rolling them.. The create strategy would work for us if the old instances get deleted after the deploy is finished. That said, I imagine creating and deleting all these VMs (even in bulk) will take considerable time over simply shipping new bits to existing VMs and rolling them.. ",
    "leizhu": "@cppforlife yes, I tried 'host' option, but it's not sufficient for s3 blob read_only mode..\n. @cppforlife I just tried the 'endpoint', yes, it's sufficient for readonly mode.\nSo that we could not need to add this PR, just use below config for both read_only and none_read_only mode:\n```\nfinal_name: cf-mysql\nblobstore:\n  provider: s3\n  options:\n    bucket_name: mysql-service-blobs\n    host: s3.cn-north-1.amazonaws.com.cn\n    endpoint: https://s3.cn-north-1.amazonaws.com.cn\n```\n. We're using a new branch, so close this PR\n. ",
    "chaitu554": "Actually I have installed Ruby 1.8.7 version and tried to install bosh_cli\nit is showing error as it require 1.9.3 version,if I ask for update it is\nnot updating.\nOn 16 Nov 2015 6:44 pm, \"Alexander Lomov\" notifications@github.com wrote:\n\nIt does seem to be a strange error. I haven't heard about such error\nbefore. At the moment I run ruby 2.1.5 on my jumpbox with latest bosh_cli:\n$ ruby -v\nruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]\n$ gem list bosh_cli\n* LOCAL GEMS *\nbosh_cli (1.3130.0, 1.3126.0, 1.3100.0)\nCould you tell more about the error? How do you install bosh cli? What is\nthe error message? Is there any stack trace?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1024#issuecomment-157024514.\n. I need clear steps to install bosh_cli . Will you help me through it? As\nI'm unable to install ruby 1.9.3 version.\n\nOn Mon, Nov 16, 2015 at 8:43 PM, samyuktha lyagala \nsamyuktha.lyagala@gmail.com wrote:\n\nActually I have installed Ruby 1.8.7 version and tried to install bosh_cli\nit is showing error as it require 1.9.3 version,if I ask for update it is\nnot updating.\nOn 16 Nov 2015 6:44 pm, \"Alexander Lomov\" notifications@github.com\nwrote:\n\nIt does seem to be a strange error. I haven't heard about such error\nbefore. At the moment I run ruby 2.1.5 on my jumpbox with latest bosh_cli:\n$ ruby -v\nruby 2.1.5p273 (2014-11-13 revision 48405) [x86_64-linux]\n$ gem list bosh_cli\n* LOCAL GEMS *\nbosh_cli (1.3130.0, 1.3126.0, 1.3100.0)\nCould you tell more about the error? How do you install bosh cli? What is\nthe error message? Is there any stack trace?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1024#issuecomment-157024514\n.\n. I need clear steps to install ruby (1.9.3 ) version and bosh_cli will you\nhelp through it?\n. thank you so much i will work out with this and let u know...\n\n\nOn Tue, Nov 17, 2015 at 12:22 PM, Alexander Lomov notifications@github.com\nwrote:\n\nSorry for late response. As far as I understand you need following things:\n- install Ruby with the version higher than 1.9.3\n- install BOSH CLI gem\nThe first task is easy to do with RVM https://rvm.io/ (ruby version\nmanagement tool). I believe you'll find useful this docs\nhttps://rvm.io/rvm/install. You haven't mentioned a system on your\nhost, still RVM is possible to use with Windows\nhttp://blog.developwithpassion.com/2012/03/30/installing-rvm-with-cygwin-on-windows/.\nDespite of this fact my advice is to use Linux.\nAfter you install RVM you'll need to run following commands:\nrvm install 2.1.5                       #  choose the version you need here\nrvm use 2.1.5@bosh --create             #   create and use a separate ruby environment for your work\ngem install bosh_cli --no-ri --no-rdoc  #   install BOSH CLI\nWish you luck with this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1024#issuecomment-157288843.\n. we are facing problem in deploying micro-bosh as we have followed this link\nbelow\n\nhttps://bosh.io/docs/deploy-microbosh-to-openstack.html\nkindly help me through it.\nOn Tue, Nov 17, 2015 at 2:45 PM, Zhang Hua notifications@github.com wrote:\n\nYou can try to install RVM at first, then use RVM to install ruby 1.9.3\nfor you easily.\nRemember to create a specific gem set  before installing bosh_cli which\nmay make it faster to load all the gems.\n-Edward\n----- \u539f\u59cb\u90ae\u4ef6 -----\u53d1\u4ef6\u4eba\uff1a chaitu554 notifications@github.com\u6536\u4ef6\u4eba\uff1a\ncloudfoundry/bosh bosh@noreply.github.com\u6284\u9001\uff1a\u4e3b\u9898\uff1a Re: [bosh] problem in\ninstalling bosh cli (#1024)\u65e5\u671f\uff1a 2015\u5e7411\u670817\u65e5 (\u5468\u4e8c) \u4e0b\u53482:31 I need clear steps\nto install ruby (1.9.3 ) version and bosh_cli will youhelp through it?\n\u2014Reply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1024#issuecomment-157315759.\n. \n",
    "ChandraNarayanasamy": "We are not using any plug-ins. just did an update and executed command bosh. \n[root@hello commands]# bosh\nFailed to load plugin /usr/local/share/gems/gems/bosh-workspace-0.9.4/lib/bosh/cli/commands/deployment_patch.rb: cannot load such file -- bosh/workspace\n. ",
    "jrsaravanan": "We tried bundle install from cf-boshworkspace repo\nFailed to load plugin /usr/local/share/gems/gems/bosh_cli_plugin_micro-1.3130.0/lib/bosh/cli/commands/micro.rb: cannot load such file -- bosh/deployer\n. We tried bundle install from cf-boshworkspace repo\nFailed to load plugin /usr/local/share/gems/gems/bosh_cli_plugin_micro-1.3130.0/lib/bosh/cli/commands/micro.rb: cannot load such file -- bosh/deployer\n. removed all the bosh components and all bosh dependencies  and installed BOSH 1.3143.0.\nIt is working fine now .\nThanks for your support !!\n. removed all the bosh components and all bosh dependencies  and installed BOSH 1.3143.0.\nIt is working fine now .\nThanks for your support !!\n. ",
    "smokingfly": "debug.txt\nAttached debug log.\n. Any opinions?\n. @cppforlife I can successfully ping NAT machine from microbosh instance. Is there anything else you can suggest that I should be looking into?\n. Oh, I have killed my instances. Let me redo the setup and share the logs here.\n. I tried setting this up and on following http://docs.cloudfoundry.org/deploying/ec2/configure_aws_cf.html, I ran into this:    Failed compiling packages > rootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea: execution expired (00:02:33)\n   Failed compiling packages > buildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3: execution expired (00:02:34)\n   Failed compiling packages > buildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3: execution expired (00:02:35)\n   Failed compiling packages > buildpack_binary/e0c8736b073d83c2459519851b5736c288311d92: execution expired (00:02:35)\n   Failed compiling packages > haproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171: execution expired (00:02:35)\n. I am not able to reopen, can you open the issue again? I am ready to submit new logs.\n. Please ignore the previous post about failing with execution expired, I made progress and am now stuck at the same point as originally mentioned in this ticket above. \nAttached is the manifest file cloudfoundry.yml, debug log of \"bosh deploy\" command and agent log from /var/vcap/bosh/log/current. I confirm that I am able to ping NAT instance from the bosh director as well as from my local PC. \nSecurity Group on both - bosh director and NAT, are set to open to ALL TRAFFIC and for \"Anywhere\".\nHere is the error message again:\n  Started compiling packages\n  Started compiling packages > rootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea\n  Started compiling packages > haproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171\n  Started compiling packages > buildpack_binary/e0c8736b073d83c2459519851b5736c288311d92\n  Started compiling packages > buildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3\n  Started compiling packages > buildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b\n  Started compiling packages > buildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3\n   Failed compiling packages > buildpack_staticfile/701bbed8b0f9bb56c78c10a0774108352527c1b3: Timed out pinging to 907c1235-e411-45b6-8da6-73f95f295c48 after 600 seconds (00:11:09)\n   Failed compiling packages > buildpack_python/b046c6a241c5dafa0e4aa0eb91aa4f3125ad5ea3: Timed out pinging to 3b815203-41b6-417a-9f1f-547cd93c718e after 600 seconds (00:11:09)\n   Failed compiling packages > buildpack_php/ef6e1efd7df3d83de9e0e4595cc8f54988b13f5b: Timed out pinging to 33a5bf4a-35b3-4d71-8915-136a29556adf after 600 seconds (00:11:09)\n   Failed compiling packages > buildpack_binary/e0c8736b073d83c2459519851b5736c288311d92: Timed out pinging to 7bbf7e3f-d29c-4a81-bedd-14ed9a0c186c after 600 seconds (00:11:09)\n   Failed compiling packages > rootfs_cflinuxfs2/d6f45b186015539373154aa0601ed228424a9bea: Timed out pinging to a2172311-4594-4069-a9fe-e86b197619e0 after 600 seconds (00:11:09)\n   Failed compiling packages > haproxy/f5d89b125a66892628a8cd61d23be7f9b0d31171: Timed out pinging to 49a5b905-341c-49f0-b27c-82c9fe3bf7ad after 600 seconds (00:11:09)\nError 450002: Timed out pinging to 907c1235-e411-45b6-8da6-73f95f295c48 after 600 seconds\nTask 7 error\ncloudfoundry.txt\ncurrent.txt\ndebug7.txt\n. The agents do not have any public IP assigned to them, could that be part of the problem? See the attached AWS screenshot.\n\n. I have been following http://docs.cloudfoundry.org/deploying/ec2/configure_aws_cf.html\nIs that not the right documentation?\n. I just tried reducing worker instances to 1 and that too failed with same errors, so it seems more likely to be a misconfiguration somewhere.\n. I did multiple \"bosh deploy\" and got rid of the errors and have now run into another one:\nStarted preparing configuration > Binding configuration. Failed: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]') (00:00:00)\nError 100: Error filling in template haproxy.conf.erb' forha_proxy_z1/0' (line 35: Can't find property `[\"ha_proxy.ssl_pem\"]')\nAny idea?\n. I built the cf manifest as instructed at http://docs.cloudfoundry.org/deploying/ec2/configure_aws_cf.html\nThe configuration is pretty much the same as I pasted above, only difference is bosh director UUID.\n. @Amit-PivotalLabs \nruby -ryaml -e\"p YAML.load_file('/home/cfuser/cf-deployment/cloudfoundry.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']['ssl_pem'].chomp\"\ngives this:\n-e:1:in <main>': undefined method[]' for nil:NilClass (NoMethodError)\nSo I looked into the configuration again and found the property ssl_pem had a  blank space in front of it, removed the blank space and the error is now gone.\nThanks for the debug command @Amit-PivotalLabs \n. I request to keep this ticket open for a couple of more days. I am very close to finish the deployment.\n. Yeah, its the first line that was failing:\nruby -ryaml -e\"p YAML.load_file('/home/cfuser/cf-deployment/cloudfoundry.yml')['jobs'].find { |j| j['name'] == 'ha_proxy_z1' }['properties']['ha_proxy']['ssl_pem']\"\n@Amit-PivotalLabs @cppforlife \nSo I made progress from this point and again ran into \" already associated with associate-id \" when I run 'bosh deploy'. I looked at http://bosh.io/docs/aws-cpi.html#errors as suggested by Dmitriy earlier, I found the error and it asked me to delete the VM associated with the Elastic IP. I found out that the Elastic IP configured for ha_proxy_z1 was associated with NATS vm that I created following http://docs.cloudfoundry.org/deploying/ec2/configure_aws_cf.html. I killed the NAT VM and 'bosh deploy' successfully completed. So my question:\n1. Since I see a \"nats_z1/0\" on 'bosh vms', do I need to manually start and keep the NAT VM running as mentioned in http://docs.cloudfoundry.org/deploying/ec2/configure_aws_cf.html ?\n2. If 1 is yes then do I need a Elastic IP to associate with the NAT VM or shall I run NAT VM without elastic IP? \n3. If 2 is yes then where do I configure NAT VM to be used by CF deployment?\n. And after \"bosh deploy\" is successful, I see the following VMs:\n+------------------------------------+---------+---------------+---------------+\n| Job/index                          | State   | Resource Pool | IPs           |\n+------------------------------------+---------+---------------+---------------+\n| api_z1/0                           | running | small_z1      | 10.0.16.4     |\n| doppler_z1/0                       | running | small_z1      | 10.0.16.6     |\n| etcd_z1/0                          | running | small_z1      | 10.0.16.104   |\n| ha_proxy_z1/0                      | running | small_z1      | 10.0.0.11     |\n|                                    |         |               | 52.70.161.122 |\n| hm9000_z1/0                        | running | small_z1      | 10.0.16.5     |\n| loggregator_trafficcontroller_z1/0 | running | small_z1      | 10.0.16.7     |\n| nats_z1/0                          | running | small_z1      | 10.0.16.103   |\n| nfs_z1/0                           | running | small_z1      | 10.0.16.105   |\n| postgres_z1/0                      | running | small_z1      | 10.0.16.101   |\n| router_z1/0                        | running | small_z1      | 10.0.16.102   |\n| runner_z1/0                        | running | small_z1      | 10.0.16.9     |\n| uaa_z1/0                           | running | small_z1      | 10.0.16.8     |\n+------------------------------------+---------+---------------+---------------+\nwhen I try to access one of the URL e.g. http://api.cloudrahul-cfapp.com, I get \"503 Service Unavailable\". A \"bosh show cf attributes\" results in the following:\nbosh-cloudfoundry-0.7.6/lib/bosh/cloudfoundry/release_version_cpi.rb:23:in `initialize': CPI aws_cpi not available for version 149 (RuntimeError)\nAll the cf commands are failing with similar error:\n$ cf target http://api.cloudrahul-cfapp.com\nSetting target to http://api.cloudrahul-cfapp.com... FAILED\nCFoundry::BadResponse: 503: 503 Service Unavailable\nNo server is available to handle this request.\n\nSomething is certainly missing in my config, could someone please help? I confirm that all the VMs listed above are up and running, including the NAT VM.\n. Thanks @Amit-PivotalLabs \nWill be looking forward to hear from your team.\n. ",
    "jianqiu": "@cppforlife  This is the PR for softlayer stemcell building.\n/cc @maximilien \n. @cppforlife  This is the PR for softlayer stemcell building.\n/cc @maximilien \n. @poblin-orange  release/micro/softlayer.yml is used to compile and install director package to the stemcell image in the process of building stemcell, and this process does not need CPI support, the behavior is the same with building stemcell for aws, vsphere, openstack, etc. Since there is no legacy softalyer cpi inlined in bosh project,  we only support bosh-init +softlayer external-cpi (https://github.com/maximilien/bosh-softlayer-cpi) @maximilien \n. @poblin-orange  release/micro/softlayer.yml is used to compile and install director package to the stemcell image in the process of building stemcell, and this process does not need CPI support, the behavior is the same with building stemcell for aws, vsphere, openstack, etc. Since there is no legacy softalyer cpi inlined in bosh project,  we only support bosh-init +softlayer external-cpi (https://github.com/maximilien/bosh-softlayer-cpi) @maximilien \n. @cppforlife\nCould you please review this PR which including a new feature flag to enable virtual deletion when doing instance upgrade? Thanks!\n. @cppforlife\nCould you please review this PR which including a new feature flag to enable virtual deletion when doing instance upgrade? Thanks!\n. @allomov \nI quite agree with you, the os_reload or virtual deletion is specific for SoftLayer, however, CPI could not make a distinction between delete_vm of delete deployment and instance upgrade if only CID was passed to CPI. Do you have some good ideas to implement it in CPI side? Thanks!\n. @allomov \nI quite agree with you, the os_reload or virtual deletion is specific for SoftLayer, however, CPI could not make a distinction between delete_vm of delete deployment and instance upgrade if only CID was passed to CPI. Do you have some good ideas to implement it in CPI side? Thanks!\n. @allomov \n It should be a big change to bosh-director to involve the reason of vm removal, but it seems that could be a general solution to this kind of requirements. Our solution might be a temporary solution that does not affect the normal function of other CPIs.\n@cppforlife \nDo you think it is possible to make bosh-director provide the context of VM removal? Thanks!\n. @allomov \n It should be a big change to bosh-director to involve the reason of vm removal, but it seems that could be a general solution to this kind of requirements. Our solution might be a temporary solution that does not affect the normal function of other CPIs.\n@cppforlife \nDo you think it is possible to make bosh-director provide the context of VM removal? Thanks!\n. @cppforlife\nWe ever tested the PR in our pipeline, and passed the bats. So, could you please look at it? Thanks!\n. @cppforlife\nWe ever tested the PR in our pipeline, and passed the bats. So, could you please look at it? Thanks!\n. @mattcui, fyi\n. @mattcui, fyi\n. @cppforlife @mattcui @maxim\nThe codes were polluted by the codes from master branch, so closing it and file another PR\n. @cppforlife @mattcui @maxim\nThe codes were polluted by the codes from master branch, so closing it and file another PR\n. @cppforlife @mattcui @maxim\nA new PR to trace this enhancement.\n. @cppforlife @mattcui @maxim\nA new PR to trace this enhancement.\n. @cppforlife @mattcui @maximilien\nA new PR to trace this enhancement.\n. @cppforlife @mattcui @maximilien\nA new PR to trace this enhancement.\n. @cppforlife @maximilien\nCould you please look into this PR? Thanks!\n. @cppforlife @maximilien\nCould you please look into this PR? Thanks!\n. @cppforlife @mattcui @medvedzver  @maximilien\nIt is pretty hard to squash all commits into single commit, so just closing it and submitted a new clean PR, sorry for the inconvenience caused for you.\nhttps://github.com/cloudfoundry/bosh/pull/1251\n. @swetharepakula \nWhat is the stemcell version you encountered the problem? Thanks!\n. @swetharepakula \nWhat is the stemcell version you encountered the problem? Thanks!\n. @cppforlife @maximiliem @mattcui\nHi Dmitriy, \nWhen I submitted the PR, the travis job failed due to bundler version, could you please help me look into it? Thanks!\nBundler could not find compatible versions for gem \"bundler\":\n  In Gemfile:\n    bundler (~> 1.11.0)\nCurrent Bundler version:\n    bundler (1.12.1)\nThis Gemfile requires a different version of Bundler.\nPerhaps you need to update Bundler by running gem install bundler?\nCould not find gem 'bundler (~> 1.11.0)' in any of the sources\n. @cppforlife @maximiliem @mattcui\nHi Dimitriy, could you please help me to look at the travis ci failure? Thanks!\n. @MichaelTrestman, @cppforlife, @maximiliem, @mattcui\nHi, Michael, thanks for your suggestion. After modified the codes, I still got the following error, in fact, I am not sure whether my changes impacted these cases. could you please help me look into the travis failure? Thanks for your kind help!\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:10 # add_teams migrates deployment teams over to Teams table\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:23 # add_teams preserves the teams associated with a deployment using a many-to-many table\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:37 # add_teams should check that deployments_teams has unique deployment_id and team_id pairs\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:47 # add_teams removes the teams column from deployment\n/home/travis/build/cloudfoundry/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:100:in unit_exec'\n/home/travis/build/cloudfoundry/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:138:inblock (6 levels) in '\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:inloop'\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:in block in create_thread'\n/home/travis/.rvm/gems/ruby-2.3.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nTasks: TOP => spec:unit:ruby\n. @MichaelTrestman, @cppforlife, @maximiliem, @mattcui\nHi, Michael, thanks for your suggestion. After modified the codes, I still got the following error, in fact, I am not sure whether my changes impacted these cases. could you please help me look into the travis failure? Thanks for your kind help!\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:10 # add_teams migrates deployment teams over to Teams table\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:23 # add_teams preserves the teams associated with a deployment using a many-to-many table\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:37 # add_teams should check that deployments_teams has unique deployment_id and team_id pairs\nrspec ./spec/unit/db/migrations/director/20160427164345_add_teams_spec.rb:47 # add_teams removes the teams column from deployment\n/home/travis/build/cloudfoundry/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:100:in unit_exec'\n/home/travis/build/cloudfoundry/bosh/bosh-dev/lib/bosh/dev/tasks/spec.rake:138:inblock (6 levels) in '\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:inloop'\n/home/travis/build/cloudfoundry/bosh/bosh_common/lib/common/thread_pool.rb:63:in block in create_thread'\n/home/travis/.rvm/gems/ruby-2.3.1/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nTasks: TOP => spec:unit:ruby\n. @cppforlife @MichaelTrestman @maximiliem @mattcui\nIs there any update to this PR? Thanks!\n. @cppforlife @MichaelTrestman @maximiliem @mattcui\nIs there any update to this PR? Thanks!\n. @cppforlife @maximilien @mattcui\nCould you please look into it? Thanks!\n. @cppforlife @maximilien @mattcui\nCould you please look into it? Thanks!\n. @cppforlife  Thanks for your kind help!\n. @cppforlife  Thanks for your kind help!\n. @cppforlife @mattcui @maximilien\nAny update?  Thanks!\n. @cppforlife @mattcui @maximilien\nAny update?  Thanks!\n. I saw the contents of this PR had been merged into develop branch, your help is greatly appreciated, and thanks for helping me make some corrections to my codes. So I am closing this PR.\n. I saw the contents of this PR had been merged into develop branch, your help is greatly appreciated, and thanks for helping me make some corrections to my codes. So I am closing this PR.\n. @cppforlife , @maximilien, @mattcui \nHi Dmitriy,\nWe found most of the time (45s) was consumed in \"monit reload\", and did not post \"start\" action to monit in 45s. But\uff0cwe could not determine why \"monit reload\" was blocked,  one possibility is that there are too many monit process (https://github.com/cloudfoundry/bosh/issues/1245), the other is that lots of security jobs failure in the process of starting. \nfunc (m monitJobSupervisor) Reload() error {\n    var currentIncarnation int\n```\noldIncarnation, err := m.getIncarnation()\nif err != nil {\n    return bosherr.WrapError(err, \"Getting monit incarnation\")\n}\n// Monit process could be started in the same second as monit reload runs\n// so it's ideal for MaxCheckTries * DelayBetweenCheckTries to be greater than 1 sec\n// because monit incarnation id is just a timestamp with 1 sec resolution.\nfor reloadI := 0; reloadI < m.reloadOptions.MaxTries; reloadI++ {\n    // Exit code or output cannot be trusted\n    , , _, err := m.runner.RunCommand(\"monit\", \"reload\")\n    if err != nil {\n        m.logger.Error(monitJobSupervisorLogTag, \"Failed to reload monit %s\", err.Error())\n    }\nfor checkI := 0; checkI < m.reloadOptions.MaxCheckTries; checkI++ {\n    currentIncarnation, err = m.getIncarnation()\n    if err != nil {\n        return bosherr.WrapError(err, \"Getting monit incarnation\")\n    }\n\n    // Incarnation id can decrease or increase because\n    // monit uses time(...) and system time can be changed\n    if oldIncarnation != currentIncarnation {\n        return nil\n    }\n\n    m.logger.Debug(\n        monitJobSupervisorLogTag,\n        \"Waiting for monit to reload: before=%d after=%d\",\n        oldIncarnation, currentIncarnation,\n    )\n\n    time.Sleep(m.reloadOptions.DelayBetweenCheckTries)\n}\n\n}\nreturn bosherr.Errorf(\n    \"Failed to reload monit: before=%d after=%d\",\n    oldIncarnation, currentIncarnation,\n)\n```\n}\n. @cppforlife , @maximilien, @mattcui \nHi Dmitriy,\nWe found most of the time (45s) was consumed in \"monit reload\", and did not post \"start\" action to monit in 45s. But\uff0cwe could not determine why \"monit reload\" was blocked,  one possibility is that there are too many monit process (https://github.com/cloudfoundry/bosh/issues/1245), the other is that lots of security jobs failure in the process of starting. \nfunc (m monitJobSupervisor) Reload() error {\n    var currentIncarnation int\n```\noldIncarnation, err := m.getIncarnation()\nif err != nil {\n    return bosherr.WrapError(err, \"Getting monit incarnation\")\n}\n// Monit process could be started in the same second as monit reload runs\n// so it's ideal for MaxCheckTries * DelayBetweenCheckTries to be greater than 1 sec\n// because monit incarnation id is just a timestamp with 1 sec resolution.\nfor reloadI := 0; reloadI < m.reloadOptions.MaxTries; reloadI++ {\n    // Exit code or output cannot be trusted\n    , , _, err := m.runner.RunCommand(\"monit\", \"reload\")\n    if err != nil {\n        m.logger.Error(monitJobSupervisorLogTag, \"Failed to reload monit %s\", err.Error())\n    }\nfor checkI := 0; checkI < m.reloadOptions.MaxCheckTries; checkI++ {\n    currentIncarnation, err = m.getIncarnation()\n    if err != nil {\n        return bosherr.WrapError(err, \"Getting monit incarnation\")\n    }\n\n    // Incarnation id can decrease or increase because\n    // monit uses time(...) and system time can be changed\n    if oldIncarnation != currentIncarnation {\n        return nil\n    }\n\n    m.logger.Debug(\n        monitJobSupervisorLogTag,\n        \"Waiting for monit to reload: before=%d after=%d\",\n        oldIncarnation, currentIncarnation,\n    )\n\n    time.Sleep(m.reloadOptions.DelayBetweenCheckTries)\n}\n\n}\nreturn bosherr.Errorf(\n    \"Failed to reload monit: before=%d after=%d\",\n    oldIncarnation, currentIncarnation,\n)\n```\n}\n. @cppforlife \nit works, thanks!\n. @cppforlife \nHi Dmitriy, \nCould you please help us look at the PR? We are looking forward for your comments, thanks!\ncc: @maximilien @mattcui\n. @mingxiao @cppforlife @maximilien \nI saw you merged this PR and then reverted it, and also changed my testcase? How could I handle this PR? Thanks! \n. @mingxiao, \nhttps://github.com/jianqiu/bosh/blob/533d03117c1a48c1bdad56bd197b0c7bde1fb15b/bosh-director/lib/bosh/director/deployment_plan/instance_plan.rb#L105 \nThe above line is not included in my PR. \nCould you please take a look at the issue 1360 which is the context of this PR.\n- The test on debug output is a false positive : what it the meaning? \n- Please make an assertion on the result of networks_changed.\n  I just took the following test case as example:\n```\n        it 'should log the changes' do\n          new_network_settings = {\n            'existing-network' =>{\n              'type' => 'dynamic',\n              'cloud_properties' =>{},\n              'dns' => '10.0.0.1',\n            },\n            'a' =>{\n              'ip' => '192.168.1.3',\n              'netmask' => '255.255.255.0',\n              'cloud_properties' =>{},\n              'default' => ['dns', 'gateway'],\n              'dns' =>['192.168.1.1', '192.168.1.2'],\n              'gateway' => '192.168.1.1',\n            }\n          }\n      allow(logger).to receive(:debug)\n      expect(logger).to receive(:debug).with(\n          \"networks_changed? network settings changed FROM: #{network_settings} TO: #{new_network_settings} on instance #{instance_plan.existing_instance}\"\n        )\n\n      instance_plan.networks_changed?\n    end\n\n``\n. @cppforlife  I dont know why this case will fail, in fact, I could generate the stemcell by using bundle exec rake stemcell:build[softlayer,esxi,ubuntu,trusty,go,bosh-os-images,bosh-ubuntu-trusty-os-image.tgz] with no problem\n. @cppforlife Hi Dmitriy, I am so sorry,  you are right, and it is my mistake because the code in the branch for my PR is a little outdate, I just updated the PR with some new stages for softlayer stemcell builder, for example, install open-iscsi, multipath-tool, as you know, we need use open-iscsi and multipath-tools to handle the works related with persistent disk in softlayer\n. @cppforlife \nThe default value of @enable_virtual_delete_vm is false, so if you did not set @enable_virtual_delete_vm = true explicitly, bosh director will invoke @cloud.delete_vm to delete vm from infrastructure, otherwise, bosh will keep it.\n. @cppforlife \nNot quite correct, the PR is including not only the changes of vm_deleter.rb but also instance_updater.rb. So there are 3 scenarios:\n1. For deployment upgrade / instance update, if enable_virtual_delete_vm=true, bosh director will not invoke delete_vm action of CPI, that is what we want, because we can use os_reload to achieve instance upgrade.\n2. For bosh delete deployment command, even if enable_virtual_delete_vm=true, the command will also delete vms.\n3. For delete_vm resolution by bosh cck,  even if enable_virtual_delete_vm=true, the solution will delete that vm.\nAs a whole, the change will cause no impact to other CPI/infrastructure, such as, aws, openstack, vsphere. For Softlayer, if we want to enable the virtual delete action during instance upgrade, we will set enable_virtual_delete_vms = true in director.yml\n. @cppforlife ,\nThanks for your comments, I had already updated my PR, could you please review it again? Thanks!\n. The result is definitely  true, since the PR aims to ignore the difference \"dns_record_name\" when new_network_settings does not have this property.\n. @mingxiao \nThe content of new_network_settings does not contain key dns_record_name while old_network_settings  had it\nlet(:network_settings) do\n         {\n                   'existing-network' =>{\n                   'type' => 'dynamic',\n                   'cloud_properties' =>{},\n                   'dns_record_name' => '0.job-1.my-network.deployment.bosh',\n                   'dns' => '10.0.0.1',\n               }\n             }\nIn the issue 1360, the difference is caused by different version of bosh-agent,  it will trigger unnecessary recreate_vm. \n. ",
    "cunnie": "Thanks @jianqiu !\nWe have merged your pull-request onto our develop branch. We prefer not to merge against master, so we'd be grateful if in the future you would submit your pull-requests against the develop branch. Thanks.\nhttps://github.com/cloudfoundry/bosh/commit/bc7fb515affd1b564d913e3ad390b38ee483d40e\nAlso, we squash'ed your 9 commits to 1, keeping the comments.\n@wlindner & cunnie\n. Hi @barthy1 & @hashmap ,\ncc: @voelzmo\nThanks!  We've merged and it's going through validation (unit/integration/BATs) now.\n\u2014@cunnie & @wcamarao \n. Hi @xingzhou & @zhang-hua ,\nThanks for the pull request!\n- we liked how you adhered to the standards (e.g. marking messages red \"DESTRUCTIVE OPERATIONS\")\n- we also liked your tests\n- we noticed that you allowed for additional options for the post request. we added inline comments to this PR and we will merge in additional commits to iterate over these points.\n- you don't need to do more work\u2014we'll take it from here!\nThanks,\n@wcamarao & @cunnie\n. Hi xingzhou, \nThanks for sending this PR. \nQuestion: How did you generate the centos_dev_tools_file_list and ubuntu_dev_tools_file_list? We're concerned this list will become outdated.\nBrian & Tyler\n. @voelzmo , We were unable to reproduce your problem. Perhaps you could send us a redacted manifest  and the version of BOSH Director (e.g. release 250) you were using? We were using release 250.\nWe had the following stanza in our test release:\ncompilation:\n  workers: 2\n  network: cf_private\n  reuse_compilation_vms: true\n  cloud_properties:\n    availability_zone: us-east-1c\n    instance_type: c3.large\nAnd during our deploy (of CF) only 2 compilation VMs were spun up (expected behavior, not buggy behavior):\n``\nbosh -t xxxx vms\nActing as user 'admin' on 'xxxx'\nDeploymentcf'\nDirector task 8\nTask 8 done\n+-------------------------------------------------------------------------------------------+---------+-----+---------+--------------+\n| VM                                                                                        | State   | AZ  | VM Type | IPs          |\n+-------------------------------------------------------------------------------------------+---------+-----+---------+--------------+\n| compilation-656d3bb9-2b4e-4513-bb26-e1f78e82918d/0 (e216d84e-47bf-424a-87da-5faf578e3b4f) | running | n/a |         | 10.10.130.11 |\n| compilation-d22dc122-423d-4f1d-8f63-fac1f1bf229f/0 (1d4ae658-0f16-49b8-b1d3-9f6c99052f1f) | running | n/a |         | 10.10.130.10 |\n+-------------------------------------------------------------------------------------------+---------+-----+---------+--------------+\nVMs total: 2\n```\nWe created an integration test to specifically expose this type of problem, although we only committed to a branch not to develop or master: \nWe restricted the IP addresses to 2, which corresponds to the number of compilation worker VMs (i.e. 2). Even with that restriction, we were unable to reproduce the error.  \nhttps://github.com/cloudfoundry/bosh/blob/compilation-112544589/spec/integration/global_networking/compilation_spec.rb#L48-L76\n\u2014@yuzhangcmu & Brian Cunnie\n. Hi @voelzmo ,\nTyler said that this may be related to this tracker story; the problem was fixed in release 248/build 3181:\nhttps://www.pivotaltracker.com/story/show/111108272\n\u2014@yuzhangcmu & Brian Cunnie\n. Merged: https://github.com/cloudfoundry/bosh/commit/3a161c273ed6fe5533a71e33a06e89dd7bf2c8ae\nThank you, liuweichu!\n. Hi Omar,\nThanks for your pull request \u2014 being able to upload stemcells in parallel could be a useful feature.\nSome notes:\n- it appears that your pull request removes the ability to upload a stemcell with the --fix option (an option necessary to overwrite a corrupted stemcell). We'd like to preserve the existing behavior (i.e. --fix replaces the existing stemcell).\n- commenting-out tests, as tempting as it may be, is not something we encourage. If a test is stale or no longer relevant, delete it. In this case, it appears that the tests are still relevant (i.e. uploading an existing stemcell will raise an error, uploading an existing stemcell with --fix replaces an existing stemcell)\n- We'd like to see more coverage on the new feature, specifically corner cases (e.g. what happens when the same stemcell is uploaded simultaneously?). Admittedly writing such a test might not be trivial to write.\n- We prefer fewer commit messages, rather than many. I use git rebase -i ... to squash my [often many] commits before submitting a PR.\nThank you,\n\u2014Brian\n. Thanks @barthy1 , we merged your commit into develop:\nhttps://github.com/cloudfoundry/bosh/commit/e90bc47053c1a43e33e7fd22334cc97e3b0e9e33\n. I think I've seen the same behavior installing a BOSH Director via bosh-init to a t2.nano instance on AWS. I can try to replicate.\nUpdate 6/2/2016: I tried deploying several times, and was unable to replicate failure (i.e. bosh-init deploy succeeded every time)\n. Hi @Petahhh , \nLet me know when you've signed the CLA, thanks.\n. @Petahhh I apologize for not getting back to you -- this one slipped through the cracks. I'll try to be more responsive to PRs in the future.. Hi @rishigits ,\nIf you want a 390x-based Xenial stemcell soon, you may want to try to build your own. A good place to start would be here.\n. Thanks @mingxiao \u2014 that will really help my GCE deployments.\n. @voelzmo thanks. I'm not quite sure #1389 will fix the problem, but I'll follow up with Dmitriy & Evan and close out the ticket if that's that case.\n. Per @cppforlife :\n\nBuild a custom stemcell with a modified /etc/dhcp/dhclient.conf that does not request host-name. If that fixes the problem, submit a PR to BOSH.\n. ### Failure: attempt to fix by setting /etc/dhcp/dhclient.conf to bosh-agent's version\n\nI attempted to fix by modifying dhclient.conf to match bosh-agents's. There were subtle differences, but apparently those differences were not terribly important: \n``` diff\n< #send host-name \"andare.fugue.com\";\n< send host-name = gethostname();\n< #send dhcp-client-identifier 1:0:a0:24:ab:fb:9c;\n< #send dhcp-lease-time 3600;\n< #supersede domain-name \"fugue.com home.vix.com\";\n< #prepend domain-name-servers 127.0.0.1;\n\n\noption rfc3442-classless-static-routes code 121 = array of unsigned integer 8;\nsend host-name \"\";\n23,27c21,24\n<       domain-name, domain-name-servers, domain-search, host-name,\n<       dhcp6.name-servers, dhcp6.domain-search,\n<       netbios-name-servers, netbios-scope, interface-mtu,\n<       rfc3442-classless-static-routes, ntp-servers,\n<       dhcp6.fqdn, dhcp6.sntp-servers;\n\n\n\n          domain-name, domain-name-servers, domain-search, host-name,\n          netbios-name-servers, netbios-scope, interface-mtu,\n          rfc3442-classless-static-routes, ntp-servers;\n\n```\n\nfirstboot.758.txt\n. ## Success: Attempt to fix by setting a DHCP exit hook\nSetting a DHCP exit hook seems to fix the problem almost all the time (99%). Note that this problem only seems to occur if the Google Project's name is 12 characters or longer (e.g. \"BlabberTabber\") (the project name is appended to the dhclient-assigned hostname, which can push the hostname past the 63-character limit (e.g. vm-bad62ff1-640a-40b0-43b8-7467b78ee9b1.c.blabbertabber.internal)\nGoogle's compute-images-packages on occasion will set a DHCP exit hook to specifically deal with this problem.\nI manually set in the stemcell:\ncd /etc/dhcp/dhclient-exit-hooks.d\ncurl -OL https://raw.githubusercontent.com/GoogleCloudPlatform/compute-image-packages/c98f0c89330f547d772d2c25883fe4a8f152b95c/google_configs/bin/set_hostname\nchmod +x set_hostname\nAlthough the deploy command exited non-zero, the deployment was a success:\n```\nUpdating deployment:\n  Getting task state:\n    Performing request GET 'https://bosh-gce.nono.io:25555/tasks/66':\n      Performing GET request:\n        Get https://admin:xxxxxxx@bosh-gce.nono.io:25555/tasks/66: dial tcp 104.154.39.128:25555: i/o timeout\nExit code 1\n```\nfirstboot.sh completed on its first try because the dhclient exit hook had truncated the hostname to 40 characters (this means that we don't need to change firstboot.sh).\nThere's still a race condition, but it's much, much smaller\nWe can see that there's a 50-90 millisecond window between dhclient setting the hostname and the subsequent dhclient-hook truncating the hostname (dhclient is invoked twice: once at startup, and once by bosh-agent (?), and each invocation sets the hostname twice, for a total of four times):\n```\ngrep bin/hostname /var/log/audit/audit.log\ntype=SYSCALL msg=audit(1471610279.524:61): arch=c000003e syscall=170 success=yes exit=0 a0=194d010 a1=40 a2=10 a3=7ffcf2bc4690 items=0 ppid=848 pid=851 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"hostname\" exe=\"/bin/hostname\" key=\"system-locale\"\ntype=SYSCALL msg=audit(1471610279.576:65): arch=c000003e syscall=170 success=yes exit=0 a0=c3e010 a1=27 a2=fffff88000000000 a3=7ffe47f7b0e0 items=0 ppid=848 pid=889 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"hostname\" exe=\"/bin/hostname\" key=\"system-locale\"\ntype=SYSCALL msg=audit(1471610286.488:136): arch=c000003e syscall=170 success=yes exit=0 a0=17f8010 a1=24 a2=fffff8f000000000 a3=7ffeb8479220 items=0 ppid=784 pid=1207 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"hostname\" exe=\"/bin/hostname\" key=\"system-locale\"\ntype=SYSCALL msg=audit(1471610286.572:142): arch=c000003e syscall=170 success=yes exit=0 a0=1cd5010 a1=27 a2=fffff88000000000 a3=7fff4ac00800 items=0 ppid=1262 pid=1302 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"hostname\" exe=\"/bin/hostname\" key=\"system-locale\"\n```\nIn this particular example, there was 6.964 seconds between the 2 invocations of dhclient, which means there was 6.964 seconds of a too-long hostname. By adding the dhclient-exit-hook, we reduced that window to 0.052 milliseconds, a 99.2% reduction. By extrapolating the number of failures we were seeing before this modification (i.e. 87.5%  machine would come up without ssh keys) we could project that the failures would be reduced to ( 87.5% * 0.008 \u2192 ) 0.7%.\nfirstboot.770.txt\n. ### Problems in Paradise: DHCP exit-hook may have unforeseen consequences\nI am not able to upload stemcells to a freshly-redeployed BOSH director with a stemcell with an exit hook. The BOSH director doesn't have the expect bosh-agent-set hostname 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a; instead, it has the hostname set by the exit hook: vm-01acbd50-062d-4c55-6f0d-a9895affbe3a\nThirty five minutes have elapsed since the stemcell was uploaded, and no progress is shown (time of command is 13:57):\n```\nbosh task 72 --debug\nUsing environment 'bosh-gce.nono.io' as user 'admin'\nTask 72\nI, [2016-08-20T13:22:17.551486 #10289] [0x2b27c1bb911c]  INFO -- TaskHelper: Director Version: 1.3262.3.0\nI, [2016-08-20T13:22:17.551582 #10289] [0x2b27c1bb911c]  INFO -- TaskHelper: Enqueuing task: 72\n```\nHere is /etc/hosts:\n```\n127.0.0.1 localhost 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\nThe following lines are desirable for IPv6 capable hosts\n::1 localhost ip6-localhost ip6-loopback 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\n10.128.0.2 vm-01acbd50-062d-4c55-6f0d-a9895affbe3a.c.blabbertabber.internal vm-01acbd50-062d-4c55-6f0d-a9895affbe3a  # Added by Google\n```\nTry setting the hostname manually & uploading stemcell. Not a hang this time, but a failure nonetheless:\n```\nUploading stemcell file:\n  Getting task state:\n    Performing request GET 'https://bosh-gce.nono.io:25555/tasks/73':\n      Performing GET request:\n        Get https://admin:xxxx@bosh-gce.nono.io:25555/tasks/73: dial tcp 104.154.39.128:25555: i/o timeout\nExit code 1\n```\nThis time I did a monit restart all and kicked off another stemcell upload [darn: I should have done a monit restart all without fixing the hostname \u2014 that would have been a good control subject].\nStemcell upload completed.\nTest:\n- set hostname back to old one (vm-01acbd50-062d-4c55-6f0d-a9895affbe3a)\n- monit restart all\n- attempt to upload stemcell again\nAnd it worked. Hmmm, not the result I expected.\nsudo hostname `cat /etc/hostname`\nhostname\n   92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\n\n. ### Problems in Paradise: DHCP exit-hook may have unforeseen consequences\nI am not able to upload stemcells to a freshly-redeployed BOSH director with a stemcell with an exit hook. The BOSH director doesn't have the expect bosh-agent-set hostname 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a; instead, it has the hostname set by the exit hook: vm-01acbd50-062d-4c55-6f0d-a9895affbe3a\nThirty five minutes have elapsed since the stemcell was uploaded, and no progress is shown (time of command is 13:57):\n```\nbosh task 72 --debug\nUsing environment 'bosh-gce.nono.io' as user 'admin'\nTask 72\nI, [2016-08-20T13:22:17.551486 #10289] [0x2b27c1bb911c]  INFO -- TaskHelper: Director Version: 1.3262.3.0\nI, [2016-08-20T13:22:17.551582 #10289] [0x2b27c1bb911c]  INFO -- TaskHelper: Enqueuing task: 72\n```\nHere is /etc/hosts:\n```\n127.0.0.1 localhost 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\nThe following lines are desirable for IPv6 capable hosts\n::1 localhost ip6-localhost ip6-loopback 92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\n10.128.0.2 vm-01acbd50-062d-4c55-6f0d-a9895affbe3a.c.blabbertabber.internal vm-01acbd50-062d-4c55-6f0d-a9895affbe3a  # Added by Google\n```\nTry setting the hostname manually & uploading stemcell. Not a hang this time, but a failure nonetheless:\n```\nUploading stemcell file:\n  Getting task state:\n    Performing request GET 'https://bosh-gce.nono.io:25555/tasks/73':\n      Performing GET request:\n        Get https://admin:xxxx@bosh-gce.nono.io:25555/tasks/73: dial tcp 104.154.39.128:25555: i/o timeout\nExit code 1\n```\nThis time I did a monit restart all and kicked off another stemcell upload [darn: I should have done a monit restart all without fixing the hostname \u2014 that would have been a good control subject].\nStemcell upload completed.\nTest:\n- set hostname back to old one (vm-01acbd50-062d-4c55-6f0d-a9895affbe3a)\n- monit restart all\n- attempt to upload stemcell again\nAnd it worked. Hmmm, not the result I expected.\nsudo hostname `cat /etc/hostname`\nhostname\n   92d9ffc9-c1cd-4a7b-509b-7ceac872d02a\n. ## Setting /etc/hostname to bosh-stemcell fixes the problem\nSetting /etc/hostname to bosh-stemcell (instead of localhost) in the stemcell image fixes the problem of GCE stemcells not booting.\nThis works because /sbin/dhclient-script will set the hostname only if the hostname is set to \"localhost\" or something else.\n- no DHCP exit hook necessary: ~~/etc/dhcp/dhclient-exit-hooks.d/set_hostname~~\n- no need to obscure/hide /bin/hostname to prevent /sbin/dhclient-script from setting it.\n- hostname is quickly set to the one BOSH expects:\n```\nhostname\ncbd46fc8-9bad-4235-aac0-5e9c6a0730b7\n``\n- hostname is only set once (bybosh-agent`?):\n```\ngrep bin/hostname /var/log/audit/audit.log\ntype=SYSCALL msg=audit(1472000504.370:130): arch=c000003e syscall=170 success=yes exit=0 a0=e71010 a1=24 a2=fffff8f000000000 a3=7fff6869bb50 items=0 ppid=765 pid=1204 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"hostname\" exe=\"/bin/hostname\" key=\"system-locale\"\n```\n- firstboot sequence:\n| Time | Event | Provenance |\n| --- | --- | --- |\n| 01:01:35 | dhclient receives IP address | /var/log/daemon.log |\n| 01:01:37 | firstboot.sh begins execution | firstboot.sh output |\n| 01:01:44.370 | hostname set | /var/log/audit/audit.log |\nfirstboot.txt contains the output of firstboot.sh.\n. Closing (I don't know why it's still open \u2014 it was fixed a while ago).\n. Hi tintoy,\nIf I understand you correctly, what you're saying is that all the disks will be ephemeral \u2014 your CPI won't have a concept of a persistent disk.\nI think you might be able to get away with it by simply no-op'ing the disk management api calls (i.e. create_disk, delete_disk, has_disk, attach_disk, detach_disk, get_disk). And also by making sure no manifests you deploy make use of a persistent disk. It may make it impossible to deploy a BOSH director without some underhanded cleverness, but you could probably deploy all your VMs with the BOSH CLI v2.\nDeploying something sophisticated like Cloud Foundry or databases would probably be out of the question (databases tend to depend on persistent data & don't want to lose them due to a re-deploy), but for simple services (e.g. DNS secondary nameservers), not having a persistent disk is fine.. @DRuggeri @hwinkel : We've published http://engineering.pivotal.io/post/bosh-on-ipv6-2/ , the first of several blog posts to describe using IPv6 with BOSH.\n@DRuggeri : Before you get too excited, this blog post describes deploying VMs with both IPv4 & IPv6 addresses, which doesn't address your problem of IPv4 address-exhaustion. We hope to address that in a subsequent post (VMs deployed solely with IPv6).\n@hwinkel : It doesn't address the full chain (BOSH + CFCR), merely BOSH, and only on vSphere. We're still making progress \u2014 stay tuned.. @pivotal-jamil-shamy Sorry, only seeing this now \u2014 what Boz said.. We notice that you allow for options[:headers], but, based on the two cases where the method upload_without_track is called, it is called without options (i.e. options[:headers] will always be nil)\n. We also noticed that although options are allowed here, they are not passed in either existing use case, always defaulting to an empty hash.\n. ). to \u2192 ).to (remove spurious space)\n. before do instead of before { (multi-line)\n. before do instead of before { (multi-line)\n. Okay to re-indent even though you will lose git history (other pairs may feel differently)\n. ",
    "amitkgupta": "Ok, thanks.\n. I can't recall for this specific instance whether it had deployed successfully initially or not.\n. ",
    "giner": "Any updates on this?. Any updates on this?. @cppforlife following the link, is there any specific reason to build from source and not package source?\nWhat I mean is that instead of\n\"wget ...; tar ...; ./configure; ./make; ...\"\nyou might use\n\"apt-get build-dep pkgname; apt-get source pkgname; cd ...\", make some changes if necessary and then \"dpkg-buildpackage\" and dpkg -i pkgfile.deb\nAbout addon.\nomkafka plugin must be used with the same version of rsyslog, librdkafka (and other external dependencies and there are any) it is build against otherwise it might be unstable or crash.\n. Here is what we do now to build against proper versions:\n- packages/librdkafka/packaging\n```\n!/bin/bash\nset -euo pipefail\nAvailable variables\n$BOSH_COMPILE_TARGET - where this package & spec'd source files are available\n$BOSH_INSTALL_TARGET - where you copy/install files to be included in package\nLIBRDKAFKA_VERSION=0.8.6\nTMP=$(mktemp --directory --tmpdir=$BOSH_COMPILE_TARGET)\nLIBRDKAFKA_DIR=$TMP/librdkafka_src/\nBuild and install librdkafka\nmkdir -p $LIBRDKAFKA_DIR\ntar -xf ${BOSH_COMPILE_TARGET}/librdkafka/librdkafka-${LIBRDKAFKA_VERSION}.tar.gz --strip-components=1  --directory $LIBRDKAFKA_DIR\ncd $LIBRDKAFKA_DIR\n./configure --prefix=${BOSH_INSTALL_TARGET}\nmake\nmake install\n```\n- packages/omkafka/packaging\n```\nAvailable variables\n$BOSH_COMPILE_TARGET - where this package & spec'd source files are available\n$BOSH_INSTALL_TARGET - where you copy/install files to be included in package\nabort script on any command that exits with a non zero value\nand report the usage of uninitialized variables\nset -euo pipefail\nDetect rsyslog version which is used for building omkafka module\nOMKAFKA_VERSION=$(rsyslogd -v | head -n1 | grep -o '^rsyslogd 8.[0-9]+.[0-9]+,' | awk -F'[ ,]' '{ print $2 }')\nDetect versions of dependencies\nLIBESTR_VERSION=$(dpkg-query -f '${Version}\\n' -W libestr0 || true)\nLIBJSON_C_VERSION=$(dpkg-query -f '${Version}\\n' -W libjson-c2 || true)\nLIBFASTJSON_VERSION=$(dpkg-query -f '${Version}\\n' -W libfastjson || true)\nInstall detected versions of dependencies to build rsyslog\nDEBS=\"\"\n[ -n \"$LIBESTR_VERSION\" ] && DEBS=\"$DEBS ${BOSH_COMPILE_TARGET}/omkafka/libestr-dev_${LIBESTR_VERSION}amd64.deb\" || true\n[ -n \"$LIBJSON_C_VERSION\" ] && DEBS=\"$DEBS ${BOSH_COMPILE_TARGET}/omkafka/libjson-c-dev${LIBJSON_C_VERSION}amd64.deb\" || true\n[ -n \"$LIBFASTJSON_VERSION\" ] && DEBS=\"$DEBS ${BOSH_COMPILE_TARGET}/omkafka/libfastjson-dev${LIBFASTJSON_VERSION}_amd64.deb\" || true\ndpkg -i $DEBS\nLIBRDKAFKA_DIR=/var/vcap/packages/librdkafka\nTMP=$(mktemp --directory --tmpdir=$BOSH_COMPILE_TARGET)\nDEST=$TMP/dest\nRSYSLOG_DIR=$TMP/rsyslog_src/\nLIBFASTJSON_DIR=$TMP/libfastjson_src/\nBuild rsyslog (we only build it to have omkafka plugin)\nmkdir -p $RSYSLOG_DIR\ntar -xf ${BOSH_COMPILE_TARGET}/omkafka/rsyslog-${OMKAFKA_VERSION}.tar.gz --strip-components=1 --directory $RSYSLOG_DIR\ncd $RSYSLOG_DIR\nexport CPATH=${LIBRDKAFKA_DIR}/include\nexport LIBRARY_PATH=${LIBRDKAFKA_DIR}/lib\n./configure --prefix=/usr --enable-omkafka --disable-uuid --disable-liblogging-stdlog --disable-generate-man-pages\nmake\nmake DESTDIR=$DEST install\nInstall omkafka plugin\ncp $DEST/usr/lib/rsyslog/omkafka.so ${BOSH_INSTALL_TARGET}/omkafka.so\ncp $DEST/usr/lib/rsyslog/omkafka.la ${BOSH_INSTALL_TARGET}/omkafka.la\n```\n. Same happened here. Work around suggested by @tushar-dadlani worked for me. Thank you.. Same happened here. Work around suggested by @tushar-dadlani worked for me. Thank you.. I'm here with another suggestion. How about the following approach?\n\nCreate a new disk and attach to a VM.\nrsync (with --bwlimit set to something reasonable) all files from the current disk to a new one\nStop all jobs\nrsync once again with --delete option\nUnmount both disks -> mount the new one to the right place -> delete the old one\nStart all  jobs\n\nThis should be pretty safe as we don't have to rely on resizing implementation on infrastructure side and at the same time downtime will not be that long as it currently is.. I'm here with another suggestion. How about the following approach?\n\nCreate a new disk and attach to a VM.\nrsync (with --bwlimit set to something reasonable) all files from the current disk to a new one\nStop all jobs\nrsync once again with --delete option\nUnmount both disks -> mount the new one to the right place -> delete the old one\nStart all  jobs\n\nThis should be pretty safe as we don't have to rely on resizing implementation on infrastructure side and at the same time downtime will not be that long as it currently is.. It seems that we've got to the same issue on BOSH upgrade: #2066. It seems that we've got to the same issue on BOSH upgrade: #2066. This looks as a regression and not a change by design.\n\nThis can be fixed by modifying here ...\n\nSimilar check is already done in a different place:\nhttps://github.com/cloudfoundry/bosh/blob/v268.1.0/src/bosh-director/lib/bosh/director/cloud_factory.rb#L48. This looks as a regression and not a change by design.\n\nThis can be fixed by modifying here ...\n\nSimilar check is already done in a different place:\nhttps://github.com/cloudfoundry/bosh/blob/v268.1.0/src/bosh-director/lib/bosh/director/cloud_factory.rb#L48. Pre-start script of cloud controller checks if bosh-dns job exists and waits for DNS to respond. Therefore it fails if bosh-dns is removed (e.g. bosh-dns is stopped) but symlink is not removed.. Pre-start script of cloud controller checks if bosh-dns job exists and waits for DNS to respond. Therefore it fails if bosh-dns is removed (e.g. bosh-dns is stopped) but symlink is not removed.. Talked about this with Dmitriy a while back and it was recognized as a bug.. Talked about this with Dmitriy a while back and it was recognized as a bug.. @h4xnoodle, Also bosh doesn't say what it is doing for so long:\nTask 14915 | 00:31:36 | Preparing deployment: Preparing deployment (00:00:14)\nTask 14915 | 00:33:46 | Preparing package compilation: Finding packages to compile (00:00:00). @h4xnoodle, Also bosh doesn't say what it is doing for so long:\nTask 14915 | 00:31:36 | Preparing deployment: Preparing deployment (00:00:14)\nTask 14915 | 00:33:46 | Preparing package compilation: Finding packages to compile (00:00:00). We worked around the problem by making the second node unresponsive and running bosh cck afterwards. BOSH should allow doing this without non-obvious magic involved.. We worked around the problem by making the second node unresponsive and running bosh cck afterwards. BOSH should allow doing this without non-obvious magic involved.. ",
    "sklevenz": "+1 for saving the manifest always\n\"Parallel\" is not correct. The Jenkin job runs independently and is time triggered. If a deployment runs in parallel then of course it is blocked by the lock. But after a failed deployment the run errand reports HTTP 500.\n. The errand can fail to run but should report a more meaningful error message. \nI was thinking about to add a flag like run-force but honestly I don't have a use case for that. If the root cause for a non running errand is a failed deployment then the deployment needs to be fixed to get the errand run.\nMy proposal is just to improve the error message. \n. I am closing.\nActually this not an issue of Bosh but more an issue of infrastructure. I am getting a 400 because there are snapshots attached to disk. But there are no snapshots. I have to check infra first...\ndebug-dump.txt\n. ",
    "charliewang0104": "I met a similar issue when running \"generate-bosh-lite-dev-manifest\" script to deploy CF. In the script, there is a line \"export BOSH_USE_BUNDLER=true\", bosh_cli can not be run after executing this line. The error on OSX 10.11.3 is -\nWangYudeMacBook-Pro:scripts wangyu$ ./generate-bosh-lite-dev-manifest.test \n/Users/wangyu/.rvm/rubies/ruby-2.1.8/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:in require': cannot load such file -- bundler/setup (LoadError)\n    from /Users/wangyu/.rvm/rubies/ruby-2.1.8/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:inrequire'\n. ",
    "cactis": "thanks!!\n. ",
    "ThadeuBrito": "Thanks!!\n. ",
    "StanleyShen": "OK, thanks for explanation.\n. It it good to mentioned it somewhere in document?\n. Thanks\n. @bosh-cpi-robot Thanks, it's my case. I thought the key used is the one specified in my cf-release manifest, and I didn't notice it's actually the one specified when I deployed the director VM.\n@dpb587-pivotal what you mentioned is not my case, but I did find this answer for such issue when I was googling this issue.\nSo if the key specified when deploying the director VM is used, why do we still need to specify AWS credentials in cf-release manifest, is that useless?\nIf we specified the AWS credentials in cf-release manifest, won't it override the one specified in deploying  director?\nIn my first opinion, I would think the credential we specified in deploying director is only for launching director VM. And in other manifest, we still need to specify AWS credential to launch VMs needed in that deployment.\n. It looks like I am wrong in understanding it.\nthe AWS credential specified in cf-release.yml is not used for launching VMs defined in deployment.\nSo we always use the AWS credential provided in creating the director to launch VMs in deployment.\n. Sorry, I didn't make it clear, all these names are actually the name(tag) shows on AWS console UI.\nWhat shown on bosh CLI is good enough.\nBecause the AWS account may be managed by different guys, in order to let others know who created these VMs and the purpose,  it's much better to put some more useful information to name.\nThat's we are doing, in our case we want to put static string \"reserved - CF 3.0\" before and \" (Creator)\" after the name defined in deployment for job names. (In my case, these VMs created by me, so it's Stanley)\nWe can add these \"reserved - CF 3.0\", \" (Stanley)\" before/after the name manually in AWS console UI, but it will be gone if we update(recreate) these VMs.\nI also don't want to change the jobs to such names, which is not good for maintenance too.\nIt's nice to have some ways in deployment file to achieve this functionality, it's not flexible to put it in director I think.\n. I think in deployment level is more flexible, for example different deployment I can have different prefix/postfix, right?\nI may have two CF deployed on AWS, then 2 instances will have same names for each component.\nIn this case, I will need different postfix to indicate which cf instance, like:\nreserved - CF instance1 access_z1/0 (Creator stanley)\nreserved - CF instance2  api_z1/0/0 (Creator shen)\nIn one deployment, the prefix/postfix should be same for all jobs.\n. Thanks for information, and close it then since we had a way to do that.\n. For my case, the two security groups are defined already.\n. ",
    "Yancey1989": "I ssh to the host that i deploy with the bosh-lite. like \"ssh -p3222 user@host\". Then when i run \"bosh ssh \" it use the port 3222 instead 22.\n. I ssh to the host that i deploy with the bosh-lite. like \"ssh -p3222 user@host\". Then when i run \"bosh ssh \" it use the port 3222 instead 22.\n. ",
    "lwoydziak": "Addressed Maria's comment\n. ",
    "wcamarao": "Hi Tom, ltns.\nYes that's correct, we're taking it from here.\nThanks\nOn Tue, Jan 5, 2016 at 6:59 PM, Tom Xing notifications@github.com wrote:\n\n@wcamarao https://github.com/wcamarao & @cunnie\nhttps://github.com/cunnie\nLong time no see:), this is Tom from IBM, thanks for your review:)\nfor the options comment, you will help to modify the code and merge the PR\nfinally, am I understanding correctly? hehe, thanks for your time:)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1081#issuecomment-169205394.\n. @voelzmo we're getting this fixed for the local vs global level of properties to be redacted, but we're unsure whether the easter egg part means another issue. To clarify, is the key change from \"on\" to \"true\" unexpected?\n\nThanks,\nWagner & @yuzhangcmu \n. @voelzmo thanks for confirming. After looking into the yaml spec, I don't think we should do something about this. Here's why:\n1- Both keys and values in yaml are nodes, meaning keys are not strings (node spec). So, keys could be a boolean for example, in any of its representations. Keys are still expected to be unique but interestingly, keys such as yes and on would result in duplicate keys - read on. :)\n2- Yaml booleans could also be represented as a few different English words (bool type), e.g. on/off, yes/no.\nThis part of the issue seems more related to yaml itself, so I'd suggest that when needed, we wrap such keywords in quotes so they're not evaluated as boolean and instead treated as string. Aye?\n. We investigated further the deployment always failing after changing key from on to true and we weren't able to reproduce it. Any more details you can provide to help reproduce it + a stacktrace?\n. ",
    "linlynn": "@cppforlife  good point , looks like we are missing that\n. Looks like some service is not started for vsphere . ",
    "liuweichu": "Ohh sorry, I forgot to close.\nThank you \ud83d\ude03 \n. Individual CLA sent.\nAlso Rakutentech have signed Corporate CLA.\n. @cppforlife Hi, test case updated.\n. Hello, I'm from a team where is running a company private cfv2 PaaS.\nActually, we are using this ruby version in our own release, in order to avoid spaghetti bash. And that is not worthy a ruby package.\nI am very interested in the details of the possible security issue that caused by this vendor ruby.\nAlso, if it's because ruby1.9.3 is EOL, I want to know if there's a chance we update the version instead of removing the whole ruby.\n. @mattcui\n\bThanks for quick reply. Definitely, as you said, nothing should depend on internal bosh ruby.\nBy the way, as far as I know, the bosh ruby itself can only be called by root. So I'm just curious under what circumstance it would actually cause trouble.\n. @cppforlife @mattcui Thanks for explanation and confirmation. Since it's official decision, we will stop using it.\n. Looking forward to that :D\n. @dpb587-pivotal Hi, we also encountered this problem.\nWe have another problem, that is we have our customized PS1 already but get overridden by 00-bosh-ps1.\nWhen fixing this, could you also update bashrc to check if /etc/profile.d/00-bosh-ps1 exists before sourcing?\nMeans that the last 3 lines in apply.sh change to: \necho \"[ -e /etc/profile.d/00-bosh-ps1 ] && source /etc/profile.d/00-bosh-ps1\" >> $chroot/root/.bashrc\necho \"[ -e /etc/profile.d/00-bosh-ps1 ] && source /etc/profile.d/00-bosh-ps1\" >> $chroot/home/vcap/.bashrc\necho \"[ -e /etc/profile.d/00-bosh-ps1 ] && source /etc/profile.d/00-bosh-ps1\" >> $chroot/etc/skel/.bashrc\n. We have our own job for doing that.\nIt might sound weird, our script is hooked on .bash_aliases.\nTherefore, since 3306, the 00-bosh-ps1 will override our own settings, since it is the last line of .bashrc.\nIf the [ -e ... ] is added, we can safely delete the 00-bosh-ps1 script from our job.. Sure, overwriting/emptying the script could be also the way.\nIMHO, check for existence before sourcing the script is generally more robust.\nFor example, this issue is exactly caused by catting non-existed considered-installed files.. Even it's decided that BOSH doesn't accept integer version anymore (which I tried to search through issues but didn't find), it should be that giving warning at bosh deployment xx.yml, rather than converting it implicitly. @cppforlife Hi, I think this is a BOSH director side behavior.\nWhen it parses the manifest, release/stemcell versions are implicitly converted to string version in BOSH, no matter what the input was.\nnew-cli is shows the same behavior.\n```\nversion is 9 and 10 instead of '9' and '10' in bosh.yml\n$ bosh-gocli deploy bosh.yml\nUsing environment 'http://192.168.50.4:25555' as user 'admin'\nUsing deployment '...'\nreleases:\n  - name: ...\n-   version: '9'\n+   version: '10'\n```\nMy bosh-lite director version is 1.3262.3.0, so this has been around for a while. (We just noticed this because we bumped our director recently from v236 to v260)\nIf this is the expected behavior of BOSH, it's worthy to be mentioned in BOSH doc because input manifest\u2260output manifest is not quite intuitive. IMO, BOSH should provide a way that user can get rendered templates without actually deploying.\nRendering and deploying should be decoupled so that release developers/BOSH admins can get a much faster feedback. (actually I don't know why BOSH haven't provided this yet). Didn't find --dry-run on BOSH 1.3262.24.0. Maybe I should switch to new client.\nBy the way, I'm sure --dry-run can help users to find their templating errors. Does it also provide the rendered results to user? I think that should be very useful.. Would also be nice if new bosh-cli could apply this proposal. We don't delete stemcell by ourselves.\nHowever, it is possible something is triggered unintentionally during deploy and vmdk get deleted.\nBy the way, this is the same issue as https://github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release/issues/22. ",
    "sudhindrarao": "Can you please advice on which stemcell  this is fixed in ?\nI have tried this on  stemcell 3046 and it fails the same way.\n. ",
    "tylerschultz": "@xingzhou and @guoger, this commit shows up as merged, but that is not actually the case (none of the changes ever made it to develop due to a bad merge). Please see @cunnie's comment above, and submit a new pull request with the generator commands (rather than the generated file).\nSorry about that!\n. I believe the fix was in release v255.1.\n. You're absolutely right, the agent will need work as well. It'd need a coordinated fix. I'm going to leave it up to DK as to how this gets prioritized. A PR would probably work too. :-)\n. FYI - I've fixed the test that was failing on Travis. Future Travis runs should work correctly.\n. Tips:\nPlease note that pre-start scripts will not time out, so the deploy will not proceed onto further steps if the pre-start script never exits.\nIf you are running more than 1 instance of your job, you may not want all instances attempting to migrate your database. You can work around this by only migrating on the instances where spec.bootstrap==true. The instance that is bootstrap will always start first. It is best not to depend on index==0. With the introduction of azs in bosh, there may be a case where there is no instance at index 0. \nIf it is important that no other instance in the instance group start before the pre-start script finishes, it may be helpful to take advantage of the update section of your manifest.\n. Hi @dlapiduz,\nWe recreated this commit on the develop branch:\nhttps://github.com/cloudfoundry/bosh/commit/2ed6e15a7e430cb2ef7f06507670c91fe33e3842\nThanks!\n--BOSH team\n. @cppforlife Yes, we (@dpb587 and I) think we understand what's wrong. It is finding the first template/job belonging to the overlapping releases, not the template/job with the matching offending package. \noffending_template1 = templates.find { |t| t.release == release1 }\noffending_template2 = templates.find { |t| t.release == release2 }\nshould probably look something like:\noffending_template1 = templates.find { |t| t.release == release1 && t.model.package_name.include?(package_name) }\noffending_template2 = templates.find { |t| t.release == release2 && t.model.package_name.include?(package_name} }\nThis code is obtuse and should be cleaned up.\n. LGTM, @cppforlife would you please make/prioritize a story to merge it in?\n. LGTM, @cppforlife would you please make/prioritize a story to merge it in?\n. merged! Thank you.\n. merged! Thank you.\n. Hi @Niranjana-5588, \nThis is not a question about BOSH. I'm sorry, I don't know of a better place to ask your question. You may want to try joining https://cloudfoundry.slack.com and asking in the #cf-users channel. Someone should be able to direct you to someone who can help.\n. Hi @Niranjana-5588, \nThis is not a question about BOSH. I'm sorry, I don't know of a better place to ask your question. You may want to try joining https://cloudfoundry.slack.com and asking in the #cf-users channel. Someone should be able to direct you to someone who can help.\n. What are the dns addresses specified in the network subnet for the deployed cf instances? It needs to include the director's ip address.\n. What are the dns addresses specified in the network subnet for the deployed cf instances? It needs to include the director's ip address.\n. The 2 dns entries are expected behavior. One address is the index based address. The other address is the instance uuid based address. The index is something that we'd like to eventually hide away and not expose, as a long term goal. Can you explain why you think having 2 DNS entries is a problem?\nWhat is the contents of /etc/resolv.conf in one of the deployed vms? Does it contain the director's ip address?\nWhat happens when you run dig @<director-ip-address> 48d2b0a4-8533-4b7d-a3c5-b11ab463e910.consul.ccc-bosh-net.cf-installer-team-test.microbosh from inside one of the deployed vms?\nThe cf manifest should have a networks section that the jobs reference. There is likely a dns key/value on each subnet. What is the value seen there?\n. The 2 dns entries are expected behavior. One address is the index based address. The other address is the instance uuid based address. The index is something that we'd like to eventually hide away and not expose, as a long term goal. Can you explain why you think having 2 DNS entries is a problem?\nWhat is the contents of /etc/resolv.conf in one of the deployed vms? Does it contain the director's ip address?\nWhat happens when you run dig @<director-ip-address> 48d2b0a4-8533-4b7d-a3c5-b11ab463e910.consul.ccc-bosh-net.cf-installer-team-test.microbosh from inside one of the deployed vms?\nThe cf manifest should have a networks section that the jobs reference. There is likely a dns key/value on each subnet. What is the value seen there?\n. I think I see the problem. I followed the code through and it automatically adds the power dns server address to the configuration on each instance. I see that your dns.address is 127.0.0.1. This should be <%= $bosh_director_ip %> .\n. I think I see the problem. I followed the code through and it automatically adds the power dns server address to the configuration on each instance. I see that your dns.address is 127.0.0.1. This should be <%= $bosh_director_ip %> .\n. I suspect this is a bug in the uaa release, or a bug in the documentation. Bosh director has supplied a set of variables, from your deployment manifest, to the uaa.yml.erb template found in the uaa release, and the erb template threw a nil reference error when it was evaluated. You may have better luck opening an issue in uaa release repo. You may also find help in the uaa slack channel, or in the cf-users slack channel.\n. I suspect this is a bug in the uaa release, or a bug in the documentation. Bosh director has supplied a set of variables, from your deployment manifest, to the uaa.yml.erb template found in the uaa release, and the erb template threw a nil reference error when it was evaluated. You may have better luck opening an issue in uaa release repo. You may also find help in the uaa slack channel, or in the cf-users slack channel.\n. We've updated the boshcli and bosh-init concourse docker images to include 1.7.1. The latest versions of the boshcli should now be built using 1.7.1. bosh-init will require a new version is published, blocked on PM. \n. We've updated the boshcli and bosh-init concourse docker images to include 1.7.1. The latest versions of the boshcli should now be built using 1.7.1. bosh-init will require a new version is published, blocked on PM. \n. Didn\u2019t write the code, but here\u2019s reasonable guesses based on what it\u2019s doing...\n``` ruby\n<%=\ndef discover_external_ip\n  # convert our OpenStruct into something reliable to iterate over\n  networks = spec.networks.marshal_dump\n# find the first network which is marked as \u201cdefault\u201d\n  # default here is an array which may contain dns and/or gateway\n  # the values for default are documented in http://bosh.io/docs/networks.html#multi-homed\n  _, network = networks.find do |_name, network_spec|\n    # director always fills in default to be at least an empty array\n    # which is truthy... so really the first network is always going to be returned\n    # and since this networks was a hash, ordering is not guaranteed\n# This property is only helpful if you care to know which ip is desired, \n# the ip which is the gateway or the ip that is to be used for dns\n# e.g. network_spec.default.include?(\u2018gateway\u2019)\nnetwork_spec.default\n\nend\n# if there was only a single network, the user didn\u2019t set default, so pick the first one\n  # nowadays, the director fills in default with both to avoid this sort of extra check\n  # I think it was introduced in 257.x+\n  if !network\n    _, network = networks.first\n  end\n# this should never happen... there will always be a network\n  if !network\n    raise \"Could not determine IP via network spec: #{networks}\"\n  end\n# hooray. here\u2019s our network\u2019s IP\n  network.ip\nend\n%>\n```\nThere is a special case of dynamic networks where IPs are not known when we first render templates. In this case, director intentionally fills in network-like values so templates don\u2019t know a difference (i.e. 127.0.0.1). The templates rendered with these values are not installed because after the dynamic VM comes up, we update the spec\u2019s IPs and re-render the templates with new values, and those results are what are sent to the VM.\nThe value of network.ip is whatever value the director/IaaS have assigned the network interface. The templates shouldn\u2019t really care where it comes from, but it will always have an IP value that the templates can use. It\u2019s not possible to derive whether the network was dynamic or manual - that\u2019s not something releases should care about.\nIt\u2019s also probably worth mentioning that there\u2019s a new (not yet released) default of addressable (story) which may be of interest to releases which are trying to broadcast their bound IPs for a service. It\u2019s particularly geared for links.\n/cc @dpb587-pivotal \n. Didn\u2019t write the code, but here\u2019s reasonable guesses based on what it\u2019s doing...\n``` ruby\n<%=\ndef discover_external_ip\n  # convert our OpenStruct into something reliable to iterate over\n  networks = spec.networks.marshal_dump\n# find the first network which is marked as \u201cdefault\u201d\n  # default here is an array which may contain dns and/or gateway\n  # the values for default are documented in http://bosh.io/docs/networks.html#multi-homed\n  _, network = networks.find do |_name, network_spec|\n    # director always fills in default to be at least an empty array\n    # which is truthy... so really the first network is always going to be returned\n    # and since this networks was a hash, ordering is not guaranteed\n# This property is only helpful if you care to know which ip is desired, \n# the ip which is the gateway or the ip that is to be used for dns\n# e.g. network_spec.default.include?(\u2018gateway\u2019)\nnetwork_spec.default\n\nend\n# if there was only a single network, the user didn\u2019t set default, so pick the first one\n  # nowadays, the director fills in default with both to avoid this sort of extra check\n  # I think it was introduced in 257.x+\n  if !network\n    _, network = networks.first\n  end\n# this should never happen... there will always be a network\n  if !network\n    raise \"Could not determine IP via network spec: #{networks}\"\n  end\n# hooray. here\u2019s our network\u2019s IP\n  network.ip\nend\n%>\n```\nThere is a special case of dynamic networks where IPs are not known when we first render templates. In this case, director intentionally fills in network-like values so templates don\u2019t know a difference (i.e. 127.0.0.1). The templates rendered with these values are not installed because after the dynamic VM comes up, we update the spec\u2019s IPs and re-render the templates with new values, and those results are what are sent to the VM.\nThe value of network.ip is whatever value the director/IaaS have assigned the network interface. The templates shouldn\u2019t really care where it comes from, but it will always have an IP value that the templates can use. It\u2019s not possible to derive whether the network was dynamic or manual - that\u2019s not something releases should care about.\nIt\u2019s also probably worth mentioning that there\u2019s a new (not yet released) default of addressable (story) which may be of interest to releases which are trying to broadcast their bound IPs for a service. It\u2019s particularly geared for links.\n/cc @dpb587-pivotal \n. Hi @goupeng212, would you please tell us the director version and stemcell version that you were using when you saw this problem?. Maybe we should add a migration?\n. Ya, this all makes sense. I think the current balancing behavior is wrong too (scenario b). \nThanks for writing the test, that is super helpful.\nI don't think there is a way to move the instances without going through these incantations. Bosh does not move the instances, favoring the prevention of data loss.\n . Ya, this all makes sense. I think the current balancing behavior is wrong too (scenario b). \nThanks for writing the test, that is super helpful.\nI don't think there is a way to move the instances without going through these incantations. Bosh does not move the instances, favoring the prevention of data loss.\n . Thank you for this detailed investigation. \nWe're currently working to build os-images with rsyslog pinned to version 8.22.0-0adiscon1trusty1.\nWe see that a version 8.24 was recently release, but the Changelog for that version makes no mention of a fix for the issue.. Thank you for this detailed investigation. \nWe're currently working to build os-images with rsyslog pinned to version 8.22.0-0adiscon1trusty1.\nWe see that a version 8.24 was recently release, but the Changelog for that version makes no mention of a fix for the issue.. Syslog was pinned back to 8.22 near after the time of this issue, and has\nremained at that version in subsequent stemcell series.\nOn Tue, Oct 10, 2017 at 1:42 PM Paul Nath notifications@github.com wrote:\n\nWhat is the status of this issue? Has this been resolved?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1537#issuecomment-335601590,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAHjFqncro9iYxck7LrWzRAYwJRSGv1tks5sq9a2gaJpZM4LM4XL\n.\n. Syslog was pinned back to 8.22 near after the time of this issue, and has\nremained at that version in subsequent stemcell series.\n\nOn Tue, Oct 10, 2017 at 1:42 PM Paul Nath notifications@github.com wrote:\n\nWhat is the status of this issue? Has this been resolved?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1537#issuecomment-335601590,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAHjFqncro9iYxck7LrWzRAYwJRSGv1tks5sq9a2gaJpZM4LM4XL\n.\n. @harshalk91 You may want to try and post this issue at the https://github.com/cloudfoundry-community/consul-boshrelease repo. They'll be more familiar with how to configure this release.. @harshalk91 You may want to try and post this issue at the https://github.com/cloudfoundry-community/consul-boshrelease repo. They'll be more familiar with how to configure this release.. https://www.pivotaltracker.com/story/show/140068815. https://www.pivotaltracker.com/story/show/140068815. @cnelson Is it possible for you list the indexes you have on the local_dns_blobs table?\n\nSomething like this:\n```\nssh into your director vm and become root\n$ /var/vcap/packages/postgres/bin/psql -U postgres\npostgres=> \\c bosh\npostgres=> select * from pg_indexes where tablename = 'local_dns_blobs';\n``\n. @cnelson Is it possible for you list the indexes you have on thelocal_dns_blobs` table?\nSomething like this:\n```\nssh into your director vm and become root\n$ /var/vcap/packages/postgres/bin/psql -U postgres\npostgres=> \\c bosh\npostgres=> select * from pg_indexes where tablename = 'local_dns_blobs';\n```\n. @cnelson Good grief. Thanks for the detective work.. @cnelson Good grief. Thanks for the detective work.. Update: We're working on a fix to update this broken migration. The fix will be in release version 261.1. The fix to the migration will drop the index only if it exists.\nSorry to all for the inconvenience.. Update: We're working on a fix to update this broken migration. The fix will be in release version 261.1. The fix to the migration will drop the index only if it exists.\nSorry to all for the inconvenience.. I think you'll need to gem uninstall bosh_cli for bundle install to work.. I think you'll need to gem uninstall bosh_cli for bundle install to work.. Seems lovely. When can I click merge?\nIf I understand correctly, this line is why the nginx_ctl line can be deleted:\nhttps://github.com/cloudfoundry/bosh/blob/master/jobs/director/templates/nginx.conf.erb#L5\nAlso, it seems we need to give the same treatment to the blobstore nginx_ctl file:\nhttps://github.com/cloudfoundry/bosh/blob/master/jobs/blobstore/templates/nginx_ctl#L20. Seems lovely. When can I click merge?\nIf I understand correctly, this line is why the nginx_ctl line can be deleted:\nhttps://github.com/cloudfoundry/bosh/blob/master/jobs/director/templates/nginx.conf.erb#L5\nAlso, it seems we need to give the same treatment to the blobstore nginx_ctl file:\nhttps://github.com/cloudfoundry/bosh/blob/master/jobs/blobstore/templates/nginx_ctl#L20. Addressed by https://github.com/cloudfoundry/bosh/pull/1677. Addressed by https://github.com/cloudfoundry/bosh/pull/1677. This was addressed in PR #1680, which superseded #1677. This was released in release version 262.. This was addressed in PR #1680, which superseded #1677. This was released in release version 262.. It occurred to me this change will also require a new version of bosh-template gem is created/published.\n. It occurred to me this change will also require a new version of bosh-template gem is created/published.\n. You're right, the changes I've made here only cover link.address (with no args). I'm not sure what would happen if args were passed to link.address, as it is being treated as a ManualLink by the EvaluationContext. \nDespite this shortcoming, I'd argue there's still value here. This change enables users of link.address with no args to run template tests, which is not possible without this change. Tests fail with 'must be run with director' error when link.address is accessed.\nI think I've convinced Danny to spend some flex time with me to come up with ways we might go about handling arguments to link.address. \nWe can do that as a separate PR, or we can improve this PR. LMK what you'd like.. You're right, the changes I've made here only cover link.address (with no args). I'm not sure what would happen if args were passed to link.address, as it is being treated as a ManualLink by the EvaluationContext. \nDespite this shortcoming, I'd argue there's still value here. This change enables users of link.address with no args to run template tests, which is not possible without this change. Tests fail with 'must be run with director' error when link.address is accessed.\nI think I've convinced Danny to spend some flex time with me to come up with ways we might go about handling arguments to link.address. \nWe can do that as a separate PR, or we can improve this PR. LMK what you'd like.. Seems like we're no longer using build_number. Is there a reason we're pulling the value off the environment (on this task) and not receiving it as an argument?\nI see that it is nice to be able to run build and not have to supply the number. Perhaps then we should remove the parameter from the task?\n. Did we try this without the disabled verification? Is there a reason for us to ignore the issues?\n. Does it make sense to put these debs into a folder /tmp/google_debs dir so that we're not installing a deb in the tmp directory we did not intend to install?\n. This test should check the pre-existing task has an empty string in the new columns, per the test name. It should also check that it's able to add large strings to the new columns.. @outer is not defined anywhere, nor is it assigned from anywhere that I can find in this PR. Is there something I don't understand? \nWhy is method missing overridden?. ",
    "eddiewebb": "@ramonskie I just updated the gemspec file in question locally to use 2.7.1 and it eliminates the error until the gem can be patched properly.\n. ",
    "SamuelMarks": "In the meantime you can also run these two on your httpclient-2.4.0 directory:\nfind -name '*.rb' -type f -exec sed -i 's/ timeout(\\@/ ::Timeout.timeout\\(/g' {} \\;\nfind -name '*.rb' -type f -exec sed -i 's/ timeout\\(([[:digit:]]\\)/ ::Timeout.timeout\\1/g' {} \\;\n. ",
    "sdheepa": "Thanks for the quick response! A follow on question..After making these changes, in order for the director to pick up these changes I would have to build a bosh stemcell and then deploy the bosh director with the updated yaml file that contains the new property. Does that sound correct?\n. ",
    "rnandi": "Merged to develop.\nThanks for your pull request!\n. Merged to develop https://github.com/cloudfoundry/bosh/commit/af16716928e28ef05f644e1b323bdbd40448bdc5\nThanks for the pull request!\n. ",
    "wlindner": "@leizhu PR has been merged e39e8e8\nThanks!\n. @premist merged this PR at a74981b on develop.\nAll active development on BOSH happens on develop, and all pull requests are merged into develop as well. Master on BOSH is representative of the latest bosh-release that is on bosh.io. In the future, please submit pull requests to the develop branch instead of master.\nThank you.\n. Thanks for the PR - we gave it a look and started to merge, but noticed a couple issues when running the full test suites...\n- The travis failure above is due to unit test environment. We fixed it with the following if you want to apply the patch:\n```\ndiff --git a/bosh-director/spec/unit/jobs/attach_disk_spec.rb b/bosh-director/spec/unit/jobs/attach_disk_spec.rb\nindex a01a363..adf21b6 100644\n--- a/bosh-director/spec/unit/jobs/attach_disk_spec.rb\n+++ b/bosh-director/spec/unit/jobs/attach_disk_spec.rb\n@@ -62,7 +62,7 @@ module Bosh::Director\n     it 'marks the pre existing active persistent disk as inactive and orphans it' do\n       attach_disk_job.perform\n\n\nexpect(instance_model.persistent_disks).to_not include(original_disk)\nexpect(instance_model.persistent_disks[0].disk_cid).to eq('fake_disk_cid')\n           original_persistent_disk = Models::PersistentDisk[disk_cid: 'original-disk-cid']\n           expect(original_persistent_disk).to be(nil)\n```\nThis new defaulting of fields within the database does not work in MySQL (it doesn't seem to support defaults of TEXT fields which the cloud_properties_json uses). We get the following error.\n\nBLOB/TEXT column 'cloud_properties_json' can't have a default value (Sequel::DatabaseError)\nIf you're still interested in this PR being merged, please have a look and resolve those issues and then let us know once they have been pushed. Then we can take another look at merging. Thanks!\n. Why is it necessary to restart postgres before restoring the db?\n. ",
    "petergtz": "I think that's not exactly what I want (unless I misunderstood you). Instead, what I want to ask BOSH is e.g.:\n\"Which versions of MyComponent did you deploy in the last 10 days at what time?\"\nIn the end, it's very similar to what bosh tasks recent gives me. But this tells me only when a certain release (including version) was created, but not when this version was deployed. Instead it only gives me:\n| 995  | done | 2016-01-14 11:24:03 UTC | admin | create deployment  | /deployments/MyComponent          |\n| 994  | done | 2016-01-14 10:39:26 UTC | admin | create release     | Created release `MyComponent/26'  |\nTo map incidents that happened in the past to a certain code version, something like this seems like an essential feature for a deployment tool. Is it possible that BOSH does not support this?\nIs it something, that the BOSH team would be happy to see?\n. I think that's not exactly what I want (unless I misunderstood you). Instead, what I want to ask BOSH is e.g.:\n\"Which versions of MyComponent did you deploy in the last 10 days at what time?\"\nIn the end, it's very similar to what bosh tasks recent gives me. But this tells me only when a certain release (including version) was created, but not when this version was deployed. Instead it only gives me:\n| 995  | done | 2016-01-14 11:24:03 UTC | admin | create deployment  | /deployments/MyComponent          |\n| 994  | done | 2016-01-14 10:39:26 UTC | admin | create release     | Created release `MyComponent/26'  |\nTo map incidents that happened in the past to a certain code version, something like this seems like an essential feature for a deployment tool. Is it possible that BOSH does not support this?\nIs it something, that the BOSH team would be happy to see?\n. Nice! This is pretty much what I had in mind.\n. Nice! This is pretty much what I had in mind.\n. > it's definitely possible, not easy though. you may also would have to record entire manifest, cloud-config, etc in the events if you want to see that as part of diffing. if you are just referring to release diffing for now then we already record release version information in the events.\nThe original idea was indeed about release diffing, because I found it often tedious to track down changes I made in different releases and why they probably broke my integration tests. So yes, just this would already help a lot.\nRecording entire manifests and cloud-config would be extremely powerful in the following scenarios: \n1. Production: fast development and an aggressive delivery strategy lead to multiple deployments a day. A bug in the code generates a ticket, but at the time the ticket is looked at new deployments have already taken place. The creation time of the ticket can be used to identify the state of a deployment.\n2. Testing: Sometimes integration tests break not because of code changes, but because of manifest changes. Seeing what manifest changes broke or fixed tests can help a lot in troubleshooting. Concourse helps a lot here already, because we can print/store the manifest before making a deployment. However, sometimes during troubleshooting, it's necessary to do a manual deployment to try things out. Any such manual changes won't be tracked by Concourse. And from a conceptual level, while it's nice that Concourse can be used to store such information, I think BOSH is the right place.\nSo, is my understanding right, that these would be in general desirable features? Or are there potentially other issues (perhaps conceptually) in this proposal?. > it's definitely possible, not easy though. you may also would have to record entire manifest, cloud-config, etc in the events if you want to see that as part of diffing. if you are just referring to release diffing for now then we already record release version information in the events.\nThe original idea was indeed about release diffing, because I found it often tedious to track down changes I made in different releases and why they probably broke my integration tests. So yes, just this would already help a lot.\nRecording entire manifests and cloud-config would be extremely powerful in the following scenarios: \n1. Production: fast development and an aggressive delivery strategy lead to multiple deployments a day. A bug in the code generates a ticket, but at the time the ticket is looked at new deployments have already taken place. The creation time of the ticket can be used to identify the state of a deployment.\n2. Testing: Sometimes integration tests break not because of code changes, but because of manifest changes. Seeing what manifest changes broke or fixed tests can help a lot in troubleshooting. Concourse helps a lot here already, because we can print/store the manifest before making a deployment. However, sometimes during troubleshooting, it's necessary to do a manual deployment to try things out. Any such manual changes won't be tracked by Concourse. And from a conceptual level, while it's nice that Concourse can be used to store such information, I think BOSH is the right place.\nSo, is my understanding right, that these would be in general desirable features? Or are there potentially other issues (perhaps conceptually) in this proposal?. @cppforlife, I've taken a closer look at this, and it seems that introducing a new table/model would make this possible. What I've been thinking about is the following:\nTable: deployment_instances (for lack of a better name at this point)\n| name              | type      | description                                                         |\n|-------------------|-----------|---------------------------------------------------------------------|\n| id                | serial    | This id could exposed to the user as deployment id just like task ids are exposed to the user for further actions. |\n| deployment_id     | integer   | FK to deployment.                                                   |\n| started_at        | timestamp | stores when the deployment was started                              |\n| completed_at      | timestamp | stores when the deployment was completed                            |\n| cloud_config_id   | integer   | identifies the cloud_config used for the deployment                 |\n| manifest_text     | text      | stores the deployment manifest as JSON (or similar)                 |\n| runtime_config_id | integer   | identifies the runtime_config used for the deployment               |\nTable: deployment_instances_release_versions\n|name|type|description|\n|-------|------|----|\n|id|serial||\n|deployment_instance_id|integer| FK to deployment_instance|\n|release_version_id|integer | FK to release_version|\nThe UpdateDeployment job could simply add a new entry to the deployment_instances table for every deployment and associate it with the corresponding deployment and release_versions. That way, all information about a specific deployment is captured.\nWith this kind of addition, a couple of interesting features could be implemented:\n1. bosh diff deployment <deployment_name> <deployment_instance_id1> <deployment_instance_id2> as proposed in my original post.\n2. bosh deployment history <deployment_name> would show past deployments.\n3. bosh rollback deployment <deployment_name> <target_deployment_instance_id> would roll back to a previous deployment.\nI think especially the last one would be an extremely compelling feature for BOSH and incredibly useful for any production environment.. @cppforlife, I've taken a closer look at this, and it seems that introducing a new table/model would make this possible. What I've been thinking about is the following:\nTable: deployment_instances (for lack of a better name at this point)\n| name              | type      | description                                                         |\n|-------------------|-----------|---------------------------------------------------------------------|\n| id                | serial    | This id could exposed to the user as deployment id just like task ids are exposed to the user for further actions. |\n| deployment_id     | integer   | FK to deployment.                                                   |\n| started_at        | timestamp | stores when the deployment was started                              |\n| completed_at      | timestamp | stores when the deployment was completed                            |\n| cloud_config_id   | integer   | identifies the cloud_config used for the deployment                 |\n| manifest_text     | text      | stores the deployment manifest as JSON (or similar)                 |\n| runtime_config_id | integer   | identifies the runtime_config used for the deployment               |\nTable: deployment_instances_release_versions\n|name|type|description|\n|-------|------|----|\n|id|serial||\n|deployment_instance_id|integer| FK to deployment_instance|\n|release_version_id|integer | FK to release_version|\nThe UpdateDeployment job could simply add a new entry to the deployment_instances table for every deployment and associate it with the corresponding deployment and release_versions. That way, all information about a specific deployment is captured.\nWith this kind of addition, a couple of interesting features could be implemented:\n1. bosh diff deployment <deployment_name> <deployment_instance_id1> <deployment_instance_id2> as proposed in my original post.\n2. bosh deployment history <deployment_name> would show past deployments.\n3. bosh rollback deployment <deployment_name> <target_deployment_instance_id> would roll back to a previous deployment.\nI think especially the last one would be an extremely compelling feature for BOSH and incredibly useful for any production environment.. @cppforlife \n\nwhat do you think about relying on events table for such information lookup?\n\nSo far, I looked at events to be more like a log that simply describes what is going on. It can certainly be used to implement what I proposed, but it feels a bit like that would be a misuse of its actual purpose.\nBut to me, what's more important is that it seems\n1. not \"type-safe\", because you would have to store everything in the context and so you suddenly expect typed data in the context in form of certain formats (e.g. JSON) that must formally describe at least release names+versions and manifest.\n2. not entirely right, because deployments to me are a first-class concept in BOSH (just like releases) and therefore deserves to get its own table I believe. Things like creating or deleting VMs to me is not on the same level, because from an abstracted perspective I'm not interested in these details.\nevents seemed to be write only by machine and read only by humans. A deployment_instances table on the other hand would be part of BOSH's fundamental business logic. BOSH would base actions and decisions on it. Again, this is how I looked at it.\n\nbosh -d dep deploy <(bosh interpolate <(bosh event 123 --context) --path /before/manifest)\n\nWhile following the unix style of composing little functions with pipes into powerful commands, I'm a bit worried that such constructs are not very intuitive. One of the best things to me about BOSH is, that I don't have to think about a lot of the commands. Their naming and syntax just comes easy and naturally. The above doesn't follow that style.\n\nregarding cloud config and runtime config, i'm not sure if it would make sense to use older versions, as you might have already changed iaas itself.\n\nGood point. I haven't worked enough with those two things, but if what you are saying is that the cloud config combined with the underlying IaaS config provides an abstraction layer that have to go hand in hand, but are ideally independent of the deployment manifest, then you're probably right, and cloud config shouldn't be rolled back. And after all, my understanding is that cloud config was extracted exactly for the reason that it changes much less frequently and hence, there is probably no good use case for rollbacks.. @cppforlife \n\nwhat do you think about relying on events table for such information lookup?\n\nSo far, I looked at events to be more like a log that simply describes what is going on. It can certainly be used to implement what I proposed, but it feels a bit like that would be a misuse of its actual purpose.\nBut to me, what's more important is that it seems\n1. not \"type-safe\", because you would have to store everything in the context and so you suddenly expect typed data in the context in form of certain formats (e.g. JSON) that must formally describe at least release names+versions and manifest.\n2. not entirely right, because deployments to me are a first-class concept in BOSH (just like releases) and therefore deserves to get its own table I believe. Things like creating or deleting VMs to me is not on the same level, because from an abstracted perspective I'm not interested in these details.\nevents seemed to be write only by machine and read only by humans. A deployment_instances table on the other hand would be part of BOSH's fundamental business logic. BOSH would base actions and decisions on it. Again, this is how I looked at it.\n\nbosh -d dep deploy <(bosh interpolate <(bosh event 123 --context) --path /before/manifest)\n\nWhile following the unix style of composing little functions with pipes into powerful commands, I'm a bit worried that such constructs are not very intuitive. One of the best things to me about BOSH is, that I don't have to think about a lot of the commands. Their naming and syntax just comes easy and naturally. The above doesn't follow that style.\n\nregarding cloud config and runtime config, i'm not sure if it would make sense to use older versions, as you might have already changed iaas itself.\n\nGood point. I haven't worked enough with those two things, but if what you are saying is that the cloud config combined with the underlying IaaS config provides an abstraction layer that have to go hand in hand, but are ideally independent of the deployment manifest, then you're probably right, and cloud config shouldn't be rolled back. And after all, my understanding is that cloud config was extracted exactly for the reason that it changes much less frequently and hence, there is probably no good use case for rollbacks.. Everything you said makes sense.. @cppforlife Out of curiousity, will this go into the tracker for BOSH, or what will be the next steps for this?\nI could possibly implement this and submit a PR, but that would be in my private time, so this could take some time. Still, it sounds like a fun feature to work on.. Started working on this here: https://github.com/petergtz/bosh/tree/add-deployment-revisions\nA few simple cases already work. Complete integration tests are still missing and a lot of details. Will update this issue again, once I think it's ready to be reviewed.. Started working on this here: https://github.com/petergtz/bosh/tree/add-deployment-revisions\nA few simple cases already work. Complete integration tests are still missing and a lot of details. Will update this issue again, once I think it's ready to be reviewed.. @cppforlife I gave this a first shot and I'd be happy if you could have a look at the diff.\nA few notes:\n I debated long with myself if I should call the new entity deployment_revision or audit_log_entry (and similarly history versus audit_log). The latter would emphasize usefulness in contexts where precise audit_logs of production systems are required. OTOH, it's really different revisions of the same deployment. Suggestions welcome.\n I also debated with myself, if I should invent a completely new event type, or if I should simply formalize the existing deployment event. In the end, I decided for a new event type, as I feel like this is a first class concept, which I do not necessarily want to mix with the existing event structure.\n Using object_name as revision number is a bit hacky and I'm not super happy with it.\n The code comes only with a partially complete suite of tests just yet, but I can of course add them, should we decide to go forward with this.\n* I have also not added CLI commands. Also waiting for feedback first.\nSome examples of its usage:\n* curl -k 'https://test:test@localhost:61004/deployments/simple/history' | jq\nyaml\n[\n  {\n    \"deployment_name\": \"simple\",\n    \"revision_number\": 4,\n    \"user\": \"test\",\n    \"task\": \"7\",\n    \"started_at\": \"2017-07-17 23:18:59 +0200\",\n    \"completed_at\": \"2017-07-17 21:19:22 UTC\",\n    \"error\": null,\n    \"manifest_text\": \"---\\nname: simple\\ndirector_uuid: deadbeef\\nreleases:\\n- name: bosh-release\\n  version: latest\\nupdate:\\n  canaries: 2\\n  canary_watch_time: 4000\\n  max_in_flight: 1\\n  update_watch_time: 20\\njobs:\\n- name: foobar\\n  templates:\\n  - name: foobar\\n  resource_pool: a\\n  instances: 3\\n  networks:\\n  - name: a\\n  properties: {}\\n  persistent_disk_pool: disk_a\\n\"\n  },\n  {\n    \"deployment_name\": \"simple\",\n    \"revision_number\": 3,\n    \"user\": \"test\",\n    \"task\": \"5\",\n    \"started_at\": \"2017-07-17 23:18:44 +0200\",\n    \"completed_at\": \"2017-07-17 21:18:53 UTC\",\n    \"error\": null,\n    \"manifest_text\": \"---\\nname: simple\\ndirector_uuid: deadbeef\\nreleases:\\n- name: bosh-release\\n  version: 0.1-dev\\nupdate:\\n  canaries: 2\\n  canary_watch_time: 4000\\n  max_in_flight: 1\\n  update_watch_time: 20\\njobs:\\n- name: foobar\\n  templates:\\n  - name: foobar\\n  resource_pool: a\\n  instances: 3\\n  networks:\\n  - name: a\\n  properties: {}\\n  persistent_disk_pool: disk_a\\n\"\n  },\n  {\n    \"deployment_name\": \"simple\",\n    \"revision_number\": 2,\n    \"user\": \"test\",\n    \"task\": \"4\",\n    \"started_at\": \"2017-07-17 23:18:32 +0200\",\n    \"completed_at\": \"2017-07-17 21:18:41 UTC\",\n    \"error\": null,\n    \"manifest_text\": \"---\\nname: simple\\ndirector_uuid: deadbeef\\nreleases:\\n- name: bosh-release\\n  version: 0.1-dev\\nupdate:\\n  canaries: 2\\n  canary_watch_time: 4000\\n  max_in_flight: 1\\n  update_watch_time: 20\\njobs:\\n- name: foobar\\n  templates:\\n  - name: foobar\\n  resource_pool: a\\n  instances: 2\\n  networks:\\n  - name: a\\n  properties: {}\\n  persistent_disk_pool: disk_a\\n\"\n  },\n  {\n    \"deployment_name\": \"simple\",\n    \"revision_number\": 1,\n    \"user\": \"test\",\n    \"task\": \"3\",\n    \"started_at\": \"2017-07-17 23:18:18 +0200\",\n    \"completed_at\": \"2017-07-17 21:18:28 UTC\",\n    \"error\": null,\n    \"manifest_text\": \"---\\nname: simple\\ndirector_uuid: deadbeef\\nreleases:\\n- name: bosh-release\\n  version: 0.1-dev\\nupdate:\\n  canaries: 2\\n  canary_watch_time: 4000\\n  max_in_flight: 1\\n  update_watch_time: 20\\njobs:\\n- name: foobar\\n  templates:\\n  - name: foobar\\n  resource_pool: a\\n  instances: 1\\n  networks:\\n  - name: a\\n  properties: {}\\n  persistent_disk_pool: disk_a\\n\"\n  }\n]\n\ncurl -k 'https://test:test@localhost:61004/deployments/simple/diff_revisions?revision1=3&revision2=4' | jq\n\nyaml\n{\n  \"manifest\": [\n    [\n      \"releases:\",\n      null\n    ],\n    [\n      \"- name: bosh-release\",\n      null\n    ],\n    [\n      \"  version: 0.1-dev\",\n      \"removed\"\n    ],\n    [\n      \"  version: latest\",\n      \"added\"\n    ]\n  ],\n  \"releases\": {\n    \"added\": [\n      \"bosh-release/0+dev.2\"\n    ],\n    \"removed\": [\n      \"bosh-release/0.1-dev\"\n    ]\n  }\n}\n. That was my first idea too. But that implies having a process somewhere that polls against the director, which has 2 drawbacks:\n\nIt requires an additional failsafe process somewhere (which is not a trivial thing to solve).\nIt adds more load on the director.\n\nI was hoping for something like statsd metrics just like Concourse or most CF components already emit them, that could be monitored somewhere else, not the BOSH director.. > just colocate it with the director. i've been slowly working on https://github.com/cppforlife/bosh-stats-release to export some of stats for general director info.\n\nnot sure what you mean by \"failsafe\".\n\nWell, if you colocate it with the director, what if the director VM dies? You could argue that you don't need metrics anymore. But let's assume you have a director HA setup, where your elastic IP automatically points to a different director VM once the first one dies, and you have the DB and blobstore as external. That way your director survives, but the metrics job doesn't. If the same metrics job runs on all stand-in directors than you'd get metrics multiple times and add up to wrong values.\nIt's a solvable problem, sure, but that's what I meant with \"it's not trivial\". Unless I'm missing something here.. > that would be the case if you have HM produce metrics. all three HMs would be producing metrics then.\nTrue. Okay, bottom line is there is no out of the box solution for resurrection metrics yet. Polling would be a possible solution.\nSince that answers my original question, feel free to close.. @dpb587-pivotal \n\nWould increasing the retention count be sufficient for you?\n\nNot sure. My idea was to increase that value anyway, because I think for events to serve as a proper audit log, it should contain the complete history of past events. My biggest concern with that though is, that it will slow down the overall DB performance, because of its size and eventually slow down the director. And obviously that problem will be worse, when a lot of data is generated by the syslog messages. Do you think that concern is justified?\n\nDo you use any other source for monitoring SSH login events?\n\nI believe we do. But would have to look into how.. @dpb587-pivotal \nSo we've observed this and found that it's unmanageable. When we increase the retention count, the size of the database quickly grows to tens of GBs which then causes a lot of other issues down the line.\nWould you accept a PR that introduces a config value to enable/disable syslog messages from going into the events table?. @dpb587-pivotal Any thoughts on this?. @cppforlife do you see issues with introducing a suppress_ssh_events config? It could be defaulted to false.. @cppforlife After discussing this with @voelzmo it seems that we rather want a generic way to specify which events to keep and which ones to drop. So my suggestion with suppress_ssh_events seems obsolete.\nOTOH, the more I think about this, the more I wonder if we should provide a way to stream these events into a different drain altogether. If we want this as audit log, it'll grow indefinitely and we might not want to keep it in the director's DB then. I found there is log_access_events_to_syslog, but it's purpose is slightly different than bosh events. Any opinions on making bosh events more pluggable, @cppforlife?. @cppforlife It's an excellent initiative and makes perfect sense. I already noticed while implementing my change that part of it is very similar to cloud and runtime config. So I think generalizing these things avoids a lot duplication and definitely makes sense.\nOne thing I'm wondering how we would do it though in your proposal above: in my PR, I'm taking special care of the bosh releases (and stemcells), to make sure a deployment revision knows the exact versions of the bosh releases it deploys. I'm doing this because just having the versioned manifests is not enough, because it might only refer to release versions as latest. To get precise diffs we need those exact release versions. So I'm wondering how we could support this. Suggestions?\nAlso, the notes state what is implemented. Does that mean it's already on master? Or what is the exact state. Maybe @voelzmo can also shed some light on the current state and the intended timeline. Thanks.\n. @pivotal-jamil-shamy Thanks for the prompt response! Will update then.. ",
    "bosh-cpi-robot": "Did you redeploy your Director VM via bosh-init deploy bosh.yml to update\nyour keys?\nOn Thu, Jan 21, 2016 at 10:09 AM, Stanley Shen notifications@github.com\nwrote:\n\nI am trying to deploy cf release to AWS and the manifest file was working,\nthe VMs can be created with my AWS security keys.\nLately I changed my AWS security keys and replaced them in the manifest\nfile.\nBut now the VMs cannot be created due to errors like:\nFailed creating bound missing vms > medium_z1/3: Unknown CPI error\n'Unknown' with message 'AWS was not able to validate the provided access\ncredentials' (00:00:46)\nI also tried to delete the deployment and try again, but I still run into\nthis issue.\nThe new key is correct, I can use it in my AWS cli.\nWhat could be the problem?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1107.\n. yup currently a problem: https://www.pivotaltracker.com/story/show/62354622.\n\nOn Mon, Feb 29, 2016 at 5:52 PM, Christian Ang notifications@github.com\nwrote:\n\nIt looks like BOSH is sometimes improperly overwriting old release scripts\nbefore monit stop is called. We noticed this because one of our stop\nscripts logic changed between releases to account for a package directory\nchange and it failed to stop the old process. We have reproduced this issue\nusing a minimal BOSH deployment.\nReproduction steps:\nDownload the following: bosh-issue.zip\nhttps://github.com/cloudfoundry/bosh/files/152039/bosh-issue.zip\nThe zip contains two release tarballs, two manifests, and a helper script.\nNote: The only difference between the two releases is a test_ctl script\nin the job which echoes its version during monit start and stop to\n/var/vcap/sys/logs/test/test.log.\nTarget a bosh lite director and change the uuid in the manifest to match\nthe director.\nRun\n./deployme 5\nThis script will deploy our two releases 5 times and echo our job\u2019s log\nfile, which contains a record of when the start and stop scripts are called.\nWe expect to see the start and stop script called back to back for each\nrelease:\nv1 start\nv1 stop\n\nv2 start\nv2 stop\n\nv1 start\nv1 stop\n...\nHowever while running this script we will occasionally see that the wrong\nstop script gets called:\n\nv1 start\nv2 stop\n\nor\n\nv2 start\nv1 stop\n\nLet us know if you need anything else.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1154.\n. Story: https://www.pivotaltracker.com/story/show/116220597. We are planning\nto issue v255.6 with a fix shortly.\n\nOn Wed, Mar 23, 2016 at 1:28 AM, Alexander Lomov notifications@github.com\nwrote:\n\nHey, all.\nWe updated BOSH director from 3033 (193) to 3215 (255.5). After that we\ntried to migrate CF from version 192 to 233 and got the following error:\n[2016-03-22 18:04:38 #8042] [] DEBUG -- Director: (0.000142s) SELECT * FROM \"stemcells\" WHERE (\"name\" = 'bosh-aws-xen-hvm-ubuntu-trusty-go_agent')\nE, [2016-03-22 18:04:38 #8042] [] ERROR -- Director: NoMethodError - undefined method deep_merge' for [\"uaa\"]:Array:\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.0/lib/bosh/director/manifest/changeset.rb:21:ininitialize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.0/lib/bosh/director/manifest/changeset.rb:43:in new'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.0/lib/bosh/director/manifest/changeset.rb:43:inblock in diff'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.0/lib/bosh/director/manifest/changeset.rb:32:in each_pair'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.0/lib/bosh/director/manifest/changeset.rb:32:indiff'\nYou can see complete log here\nhttps://gist.github.com/allomov-altoros/575c1cf172b6aef30ec5.\nThe problem occurred in changeset.rb\nhttps://github.com/cloudfoundry/bosh/blob/stable-3215/bosh-director/lib/bosh/director/manifest/changeset.rb#L21\nfile that tried to compute diff between manifests of current deployment and\nthe new manifest. The problem is that old version of CF had the following\nconsul properties in manifest:\n- instances: 1\n  name: uaa_z2\n  networks:\n  - name: cf2\n    properties:\n    consul:\n      agent:\n        services:\n        - uaa\nand the newest version of cf-release switched to using hash instead of\narray in the following way\nhttps://github.com/cloudfoundry/cf-release/blob/v233/example_manifests/minimal-aws.yml#L120-L124\n:\n- instances: 1\n  name: uaa_z1\n  networks:\n  - name: cf1\n    properties:\n    consul:\n      agent:\n        services:\n          uaa: {}\nAs far as I can see, the BOSH fails to find manifest changes when array is\nsubstituted with hash.\nPossible solution in this case is to:\n1. remove consul from initial deployment,\n2. deploy this manifest\n3. update the manifest to use newest version of cf-release\n4. deploy it\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1179\n. Hmm this might be a side effect of string-int conversions. Can you try\nquoting both of the versions so that they are strings for sure?\n\nOn Wed, Mar 23, 2016 at 6:46 AM, Long Nguyen notifications@github.com\nwrote:\n\nBOSH can't deploy same release if used in addons. I'm trying to use addons\nfor my metrics boshrelease and having releases with same name/version isn't\nallowed.\nMetrics Deployments\ncompilation:\n  cloud_properties:\n    name: random\n  network: metrics1\n  reuse_compilation_vms: true\n  workers: 6\ndirector_uuid: 17a45148-1d00-43bc-af28-9882e5a6535a\njobs:\n- instances: 1\n  name: metrics_z1\n  networks:\n  - name: metrics1\n    static_ips:\n    - 10.244.8.2\n      persistent_disk: 4096\n      properties:\n      grafana:\n      admin_password: password\n      admin_username: admin\n      influxdb:\n      database: metrics\n      host: 10.244.8.2\n      password: password\n      username: admin\n      resource_pool: small_z1\n      templates:\n  - name: grafana\n    release: metrics\n  - name: influxdb\n    release: metrics\n    name: metrics-warden\n    networks:\n- name: metrics1\n  subnets:\n  - gateway: 10.244.8.1\n    name: metrics1\n    range: 10.244.8.0/24\n    static:\n    - 10.244.8.2-10.244.8.60\n      type: manual\n      properties: {}\n      releases:\n- name: metrics\n  version: 2\n  resource_pools:\n- cloud_properties:\n  name: random\n  name: small_z1\n  network: metrics1\n  stemcell:\n    name: bosh-warden-boshlite-ubuntu-trusty-go_agent\n    version: latest\n  update:\n  canaries: 1\n  canary_watch_time: 1000-30000\n  max_in_flight: 50\n  serial: false\n  update_watch_time: 1000-30000\nAddon.yml\nreleases:\n- name: metrics\n  version: 2\naddons:\n- name: metrics\n  jobs:\n  - name: telegraf\n    release: metrics\n    properties:\n    influxdb:\n      database: metrics\n      host: 10.244.8.2\n      password: password\n      username: admin\nOutput\n$ bosh -n deploy\nActing as user 'admin' on deployment 'metrics-warden' on 'Bosh Lite Director'\nGetting deployment properties from director...\nDetecting deployment changes\nreleases:\n- name: metrics\n  version: 2\n  addons:\n- name: metrics\n  jobs:\n  - name: telegraf\n    release: metrics\n    properties:\n    influxdb:\n      database: \"\"\n      host: \"\"\n      password: \"\"\n      username: \"\"\nDeploying\nDirector task 13\n  Started preparing deployment > Preparing deployment. Failed: Runtime manifest specifies release metrics' with version as2'. This conflicts with version `2' specified in the deployment manifest. (00:00:00)\nError 530003: Runtime manifest specifies release metrics' with version as2'. This conflicts with version `2' specified in the deployment manifest.\nTask 13 error\nFor a more detailed error report, run: bosh task 13 --debug\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1180\n. this will be in 257.\n\nSent from my iPhone\n\nOn May 9, 2016, at 4:08 AM, Alex Ley notifications@github.com wrote:\n@cppforlife has this been scheduled? Also causing our team an issue.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. For high/critical vulnerabilities we issue stemcells within a day or so.\nFor low and mediums, they are rolled into next stemcell bump automatically,\nso the next stemcell will have it.\n\nOn Thu, Apr 14, 2016 at 11:11 AM, Diego Lapiduz notifications@github.com\nwrote:\n\nThere is a vulnerability in libpcre3_1:8.31-2ubuntu2.1 and it should be\nupdated to libpcre3_1:8.31-2ubuntu2.2.\nWhats the best way of doing that?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1216\n. 255.10 was published (same as 255.8; no code change). sorry for the incovenience. \n\nour intention is to stop publishing stemcell and bosh releases from the same pipeline since these are two separate artifacts. historically it has been done so since there was no bosh-init. now we are splitting our pipeline apart allowing us to ship them separately.\nSent from my iPhone\n\nOn Apr 16, 2016, at 9:47 AM, Dmitriy Kalinin notifications@github.com wrote:\nyup when stemcell was cut from master to update packages, it picked up older revision of bosh release since we have been producing bosh release from 255.x branch most recently. 255.9 bosh release should ve considered yanked and we will release 255.10 from 255.x branch.\nSent from my iPhone\n\nOn Apr 16, 2016, at 7:52 AM, Alexander Lomov notifications@github.com wrote:\nLooks like bosh.io points to commit 8f47e6a7 for bosh version 255.8, this commit contains missing migration files.\nAt the same time bosh release version 255.9 in bosh.io points to 47cafe56, which doesn't have this files.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. \"stopped\" in this context doesnt mean that instances will be preserved through iaas restart (bosh lite effectively is the iaas  here). though you could run 'bosh stop --hard' to stop instances and delete vms, and then later after restart of boslite run 'bost start' which should recreate instances, reattaching persistent disks.\n\n\nSent from my iPhone\n\nOn Apr 26, 2016, at 12:34 AM, Niranjana-5588 notifications@github.com wrote:\n@cppforlife This logs are useful to know why jobs failed. But the main issue is after restarting instance all vms should be in running state not \"unresponsive agent\" state. Came to know the steps to achieve this. First, all bosh vms must be stopped then shut down the instance. When instance started, vms in stopped state which we can start again using bosh start . Followed steps as below mentioned.\nStopped all vms using command bosh stop . All jobs went to 'stopped\" state.\nRestarted the ec2 instance.\nList out vms using command bosh vms cf-warden\nAll vms are in \"unresponsive agent\" state again. (Expected, vms should be in \"stopped\" state as we stopped all vms before shutting down instance)\nIs there any other way to do make bosh vms up and running automatically as running behind log files every time when instance restarted is not seems good solution.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. no-redis-no-cry is not on master. we have not released a version of bosh\nwithout redis. release next week will be the first one.\n\nOn Fri, Apr 22, 2016 at 10:32 AM, Alexander Lomov notifications@github.com\nwrote:\n\nHey @tinygrasshopper https://github.com/tinygrasshopper! Could you tell\nwhat version of BOSH do you use?\nI am sure that the latest version of BOSH got rid of using Redis for locks\nalready. As far as I can see no-redis-no-cry branch is merged to master.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1238#issuecomment-213521603\n. Is there any reason why you dont want to use full stemcell (instead of\nlight stemcells) for AWS on govcloud?\n\nhttps://bosh.io/stemcells/bosh-aws-xen-ubuntu-trusty-go_agent (ones that\ndont say light)\nOn Tue, May 17, 2016 at 10:55 AM, Steven Harms notifications@github.com\nwrote:\n\nEach build copies the AMI image to each region, such as us-east-1. In\norder to deploy in GovCloud, we would like the build process to upload the\nAMI to one additional region: us-gov-west-1 (list of all regions:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\n)\nCurrently this process is shown in the ami.log for other regions:\nI, [2016-05-16T23:39:15.991606 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'sa-east-1'\nI, [2016-05-16T23:39:15.991807 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'eu-west-1'\nI, [2016-05-16T23:39:15.991901 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'eu-central-1'\nI, [2016-05-16T23:39:15.992004 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'ap-southeast-1'\nI, [2016-05-16T23:39:15.992077 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'ap-southeast-2'\nI, [2016-05-16T23:39:15.992185 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'ap-northeast-1'\nI, [2016-05-16T23:39:15.992372 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'ap-northeast-2'\nI, [2016-05-16T23:39:15.992427 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'us-west-1'\nI, [2016-05-16T23:39:15.992490 #25438] INFO -- : Copying AMI\n'ami-25c62e48' from region 'us-east-1' to region 'us-west-2'\nI, [2016-05-16T23:44:41.910298 #25438] INFO -- : [AWS EC2 200 0.502354 0\nretries]\ndescribe_tags(:filters=>[{:name=>\"key\",:values=>[\"Name\"]},{:name=>\"resource-type\",:values=>[\"image\"]},{:name=>\"resource-id\",:values=>[\"ami-25c62e48\"]}])\n\u200b\nI, [2016-05-16T23:44:42.347700 #25438] INFO -- : Finished copying AMI\n'ami-25c62e48' from region 'us-east-1' to AMI 'ami-89e908e8' in region\n'ap-northeast-1'\nI, [2016-05-16T23:44:46.533796 #25438] INFO -- : [AWS EC2 200 0.152518 0\nretries]\ndescribe_tags(:filters=>[{:name=>\"key\",:values=>[\"Name\"]},{:name=>\"resource-type\",:values=>[\"image\"]},{:name=>\"resource-id\",:values=>[\"ami-25c62e48\"]}])\n\u200b\nI, [2016-05-16T23:44:46.795683 #25438] INFO -- : Finished copying AMI\n'ami-25c62e48' from region 'us-east-1' to AMI 'ami-a45d25c4' in region\n'us-west-1'\nI, [2016-05-16T23:44:47.485773 #25438] INFO -- : [AWS EC2 200 0.031049 0\nretries]\ndescribe_tags(:filters=>[{:name=>\"key\",:values=>[\"Name\"]},{:name=>\"resource-type\",:values=>[\"image\"]},{:name=>\"resource-id\",:values=>[\"ami-25c62e48\"]}])\n\u200b\nI, [2016-05-16T23:44:47.800282 #25438] INFO -- : Finished copying AMI\n'ami-25c62e48' from region 'us-east-1' to AMI 'ami-696b9709' in region\n'us-west-2'\nI, [2016-05-16T23:44:55.332455 #25438] INFO -- : [AWS EC2 200 0.080949 0\nretries]\ndescribe_tags(:filters=>[{:name=>\"key\",:values=>[\"Name\"]},{:name=>\"resource-type\",:values=>[\"image\"]},{:name=>\"resource-id\",:values=>[\"ami-25c62e48\"]}])\n\u200b\nI, [2016-05-16T23:44:55.684389 #25438] INFO -- : Finished copying AMI\n'ami-25c62e48' from region 'us-east-1' to AMI 'ami-7abc3416' in region\n'sa-east-1'\nI, [2016-05-16T23:45:07.241101 #25438] INFO -- : [AWS EC2 200 0.079862 0\nretries]\ndescribe_tags(:filters=>[{:name=>\"key\",:values=>[\"Name\"]},{:name=>\"resource-type\",:values=>[\"image\"]},{:name=>\"resource-id\",:values=>[\"ami-25c62e48\"]}])\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1271\n. \n",
    "datianshi": "Is this closed? I am using the bosh release 250 with a bosh deployment on azure. Hit this exact issue\n. ",
    "premist": "Added link to the related issue (#1110) on the description.\n. @wlindner Thanks! I'll make a PR into develop branch in the future.\n. ",
    "craigfurman": "Spoke to Dmitriy, it is indeed still evolving.\nOn Wed, Feb 3, 2016 at 4:30 PM, @dpb587 notifications@github.com wrote:\n\nI don't think the links feature is supported yet - the functionality is\nstill evolving and specifications have changed since v246. Probably easiest\nto wait until it is fully merged in to bosh and more officially documented\non bosh.io?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1118#issuecomment-179325436.\n. Hi @allomov, Thanks for replying! I'm a little confused though: Are you saying that returning 302 when task is submitted for deployments with ongoing tasks is not changing now, or that it should always be the current behaviour?\n\nIMO it's not uncommon for async HTTP APIs to reject long-running operations up-front if it can be determined from initial state that the request is unacceptable, e.g. insufficient permissions to perform the task.\n@cppforlife what do you think?\n. ",
    "dmfallak": "We can't reproduce this on the addons branch. Deployed a test job with a property with value of '~', then deployed again. No changes were printed.\n. ",
    "rwwohl": "I'm also seeing this issue after upgrade to the latest bosh 255.6.  Every null value in my manifest shows up as a change even when nothing is changing:\nproperties:\n  acceptance_tests: \"\"\n  app_ssh: \"\"\n  blobstore:\n    admin_users: \"\"\n    secure_link:\n      secret: \"\"\n    tls:\n      cert: \"\"\n      private_key: \"\"\n....\nThis creates alot of noise making it difficult to see what actually is changing between deploys.\nbosh cli version: 1.3215.0\nbosh version: 255.6\nFor now, I'm dropping down to bosh version 246 which doesnt seem to have this issue\n. Gotcha.  Yeah, that makes sense.  I did some googling, and turns out if I quote the word 'no' , then it does not convert and goes in there as intended.  Thanks for giving clarity on the issue.. ",
    "subhankarc": "Yes it is the same post. Thanks for tagging it. \n. @cppforlife The VMs goes down within 4-5 minutes of deployment, so no question of laptop sleep.\nHere are the agent logs as you asked for below which I could collect after which the VM went down. It is a redis cluster that I deploy. I can connect to redis server till the time it stays alive. Also please see this post where I have pasted the vi /var/vcap/sys/log/health_monitor/health_monitor.log details which shows somehting about unmanaged agent. Looks like HM itself killing the VMs.\n```\n2016-02-05_09:13:13.99684 ****\n2016-02-05_09:13:13.99711 [Action Dispatcher] 2016/02/05 09:13:13 INFO - Running async action run_script\n2016-02-05_09:13:13.99756 [MBus Handler] 2016/02/05 09:13:13 INFO - Responding\n2016-02-05_09:13:13.99885 [MBus Handler] 2016/02/05 09:13:13 DEBUG - Payload\n2016-02-05_09:13:13.99888 ***\n2016-02-05_09:13:13.99918 {\"value\":{\"agent_task_id\":\"f1eeb4a6-44e2-4495-7f64-b0b846439bd6\",\"state\":\"running\"}}\n2016-02-05_09:13:13.99920 **\n2016-02-05_09:13:13.99920 [File System] 2016/02/05 09:13:13 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:13:13.99921 [File System] 2016/02/05 09:13:13 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:13:13.99923 [File System] 2016/02/05 09:13:13 DEBUG - Read content\n2016-02-05_09:13:13.99925 **\n2016-02-05_09:13:13.99925 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:13:13.99931 **\n2016-02-05_09:13:13.99932 [File System] 2016/02/05 09:13:13 DEBUG - Checking if file exists /var/vcap/jobs/redis/bin/pre-start\n2016-02-05_09:13:13.99933 [ParallelScript] 2016/02/05 09:13:13 DEBUG - Did not find 'pre-start' script in job 'redis'\n2016-02-05_09:13:13.99934 [ParallelScript] 2016/02/05 09:13:13 INFO - Will run 0 pre-start scripts in parallel\n2016-02-05_09:13:14.00135 [MBus Handler] 2016/02/05 09:13:14 INFO - Received request with action get_task\n2016-02-05_09:13:14.00137 [MBus Handler] 2016/02/05 09:13:14 DEBUG - Payload\n2016-02-05_09:13:14.00138 **\n2016-02-05_09:13:14.00139 {\"protocol\":2,\"method\":\"get_task\",\"arguments\":[\"f1eeb4a6-44e2-4495-7f64-b0b846439bd6\"],\"reply_to\":\"director.bdfa0a50-23b6-4947-92e1-90b6eef26b8e.696be3d8-e352-460b-b94c-9154ae36774c\"}\n2016-02-05_09:13:14.00140 **\n2016-02-05_09:13:14.00141 [Action Dispatcher] 2016/02/05 09:13:14 INFO - Running sync action get_task\n2016-02-05_09:13:14.00157 [MBus Handler] 2016/02/05 09:13:14 INFO - Responding\n2016-02-05_09:13:14.00188 [MBus Handler] 2016/02/05 09:13:14 DEBUG - Payload\n2016-02-05_09:13:14.00190 **\n2016-02-05_09:13:14.00191 {\"value\":{}}\n2016-02-05_09:13:14.00192 **\n2016-02-05_09:13:14.00508 [MBus Handler] 2016/02/05 09:13:14 INFO - Received request with action start\n2016-02-05_09:13:14.00509 [MBus Handler] 2016/02/05 09:13:14 DEBUG - Payload\n2016-02-05_09:13:14.00510 **\n2016-02-05_09:13:14.00511 {\"protocol\":2,\"method\":\"start\",\"arguments\":[],\"reply_to\":\"director.bdfa0a50-23b6-4947-92e1-90b6eef26b8e.806add20-89c7-4fc3-9b18-c29d6ffa476a\"}\n2016-02-05_09:13:14.00513 **\n2016-02-05_09:13:14.00540 [Action Dispatcher] 2016/02/05 09:13:14 INFO - Running sync action start\n2016-02-05_09:13:14.00561 [File System] 2016/02/05 09:13:14 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:13:14.00578 [File System] 2016/02/05 09:13:14 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:13:14.00597 [File System] 2016/02/05 09:13:14 DEBUG - Read content\n2016-02-05_09:13:14.00599 **\n2016-02-05_09:13:14.00600 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:13:14.00607 **\n2016-02-05_09:13:14.00710 [renderedJobApplier] 2016/02/05 09:13:14 DEBUG - Configuring job {redis 1c917ee3b8d3639bf788777602fa4dc74647dcdf {0a0b6cba58fcb64720ac9ef094787983741fe95e 2dd5fd15-ccdc-446a-a248-981bfecef8ca redis} [{redis-server b53d5357ab95a74c9489cd98a024e6ef6047aba0.1 {3ecef141a5da0f4889d37368f3bc6d844f76feb9 2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4 }}]} with index 0\n2016-02-05_09:13:14.00729 [File System] 2016/02/05 09:13:14 DEBUG - Checking if file exists /var/vcap/data/jobs/redis/1c917ee3b8d3639bf788777602fa4dc74647dcdf-0a0b6cba58fcb64720ac9ef094787983741fe95e\n2016-02-05_09:13:14.00748 [File System] 2016/02/05 09:13:14 DEBUG - Checking if file exists /var/vcap/data/jobs/redis/1c917ee3b8d3639bf788777602fa4dc74647dcdf-0a0b6cba58fcb64720ac9ef094787983741fe95e/monit\n2016-02-05_09:13:14.00766 [File System] 2016/02/05 09:13:14 DEBUG - Reading file /var/vcap/data/jobs/redis/1c917ee3b8d3639bf788777602fa4dc74647dcdf-0a0b6cba58fcb64720ac9ef094787983741fe95e/monit\n2016-02-05_09:13:14.00783 [File System] 2016/02/05 09:13:14 DEBUG - Read content\n2016-02-05_09:13:14.00784 **\n2016-02-05_09:13:14.00785 check process redis\n2016-02-05_09:13:14.00786   with pidfile /var/vcap/sys/run/redis/redis.pid\n2016-02-05_09:13:14.00787   start program \"/var/vcap/jobs/redis/bin/monit_debugger redis_ctl '/var/vcap/jobs/redis/bin/redis_ctl start'\"\n2016-02-05_09:13:14.00788   stop program \"/var/vcap/jobs/redis/bin/monit_debugger redis_ctl '/var/vcap/jobs/redis/bin/redis_ctl stop'\"\n2016-02-05_09:13:14.00789   group vcap\n2016-02-05_09:13:14.00790\n2016-02-05_09:13:14.00792 **\n2016-02-05_09:13:14.00810 [File System] 2016/02/05 09:13:14 DEBUG - Writing /var/vcap/monit/job/0000_redis.monitrc\n2016-02-05_09:13:14.00829 [File System] 2016/02/05 09:13:14 DEBUG - Making dir /var/vcap/monit/job with perm 511\n2016-02-05_09:13:14.00884 [File System] 2016/02/05 09:13:14 DEBUG - Write content\n2016-02-05_09:13:14.00886 **\n2016-02-05_09:13:14.00887 check process redis\n2016-02-05_09:13:14.00888   with pidfile /var/vcap/sys/run/redis/redis.pid\n2016-02-05_09:13:14.00889   start program \"/var/vcap/jobs/redis/bin/monit_debugger redis_ctl '/var/vcap/jobs/redis/bin/redis_ctl start'\"\n2016-02-05_09:13:14.00891   stop program \"/var/vcap/jobs/redis/bin/monit_debugger redis_ctl '/var/vcap/jobs/redis/bin/redis_ctl stop'\"\n2016-02-05_09:13:14.00892   group vcap\n2016-02-05_09:13:14.00893\n2016-02-05_09:13:14.00894 **\n2016-02-05_09:13:14.00929 [File System] 2016/02/05 09:13:14 DEBUG - Glob '/var/vcap/data/jobs/redis/1c917ee3b8d3639bf788777602fa4dc74647dcdf-0a0b6cba58fcb64720ac9ef094787983741fe95e/.monit'\n2016-02-05_09:13:14.00961 [http-client] 2016/02/05 09:13:14 DEBUG - status function called\n2016-02-05_09:13:14.00979 [http-client] 2016/02/05 09:13:14 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:14.00998 [attemptRetryStrategy] 2016/02/05 09:13:14 DEBUG - Making attempt #0\n2016-02-05_09:13:14.01031 [clientRetryable] 2016/02/05 09:13:14 DEBUG - [requestID=1eda963b-d855-4642-54cf-f7e1248b2c77] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:14.01951 [clientRetryable] 2016/02/05 09:13:14 DEBUG - [requestID=1eda963b-d855-4642-54cf-f7e1248b2c77] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:14.02684 [Cmd Runner] 2016/02/05 09:13:14 DEBUG - Running command: monit reload\n2016-02-05_09:13:14.03966 [Cmd Runner] 2016/02/05 09:13:14 DEBUG - Stdout:\n2016-02-05_09:13:14.03968 [Cmd Runner] 2016/02/05 09:13:14 DEBUG - Stderr: Reinitializing monit daemon\n2016-02-05_09:13:14.03969 [Cmd Runner] 2016/02/05 09:13:14 DEBUG - Successful: true (0)\n2016-02-05_09:13:14.03970 [http-client] 2016/02/05 09:13:14 DEBUG - status function called\n2016-02-05_09:13:14.03971 [http-client] 2016/02/05 09:13:14 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:14.03971 [attemptRetryStrategy] 2016/02/05 09:13:14 DEBUG - Making attempt #0\n2016-02-05_09:13:14.03972 [clientRetryable] 2016/02/05 09:13:14 DEBUG - [requestID=1a0193b1-b340-4ba3-67e4-82cc5ba36ef7] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:14.04979 [clientRetryable] 2016/02/05 09:13:14 DEBUG - [requestID=1a0193b1-b340-4ba3-67e4-82cc5ba36ef7] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:14.04980 [monitJobSupervisor] 2016/02/05 09:13:14 DEBUG - Waiting for monit to reload: before=1454663588 after=1454663588\n2016-02-05_09:13:14.08297 2016/02/05 09:13:14 mail from: \"monit@localhost\"\n2016-02-05_09:13:14.12529 [agent] 2016/02/05 09:13:14 DEBUG - Ignored monit event: %!(EXTRA string=Monit instance changed)\n2016-02-05_09:13:14.14790 2016/02/05 09:13:14 mail from: \"monit@localhost\"\n2016-02-05_09:13:14.19110 [NATS Handler] 2016/02/05 09:13:14 INFO - Sending hm message 'alert'\n2016-02-05_09:13:14.19849 [NATS Handler] 2016/02/05 09:13:14 DEBUG - Message Payload\n2016-02-05_09:13:14.19852 ***\n2016-02-05_09:13:14.19854 {\"id\":\"1454663594.2078603955@fa299ad9-f868-481b-a63e-c45522fafe8c\",\"severity\":1,\"title\":\"redis (10.244.2.2) - Does not exist - restart\",\"summary\":\"process is not running\",\"created_at\":1454663594}\n2016-02-05_09:13:14.19859 **\n2016-02-05_09:13:19.06811 [http-client] 2016/02/05 09:13:19 DEBUG - status function called\n2016-02-05_09:13:19.06817 [http-client] 2016/02/05 09:13:19 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:19.06818 [attemptRetryStrategy] 2016/02/05 09:13:19 DEBUG - Making attempt #0\n2016-02-05_09:13:19.06820 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=13785cbc-8626-4fc3-5264-c198c44e685e] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:19.06821 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=13785cbc-8626-4fc3-5264-c198c44e685e] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:19.06823 [http-client] 2016/02/05 09:13:19 DEBUG - status function called\n2016-02-05_09:13:19.06824 [http-client] 2016/02/05 09:13:19 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:19.06825 [attemptRetryStrategy] 2016/02/05 09:13:19 DEBUG - Making attempt #0\n2016-02-05_09:13:19.06826 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=9e269313-9096-4dea-4b68-11ec69d901e2] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:19.06830 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=9e269313-9096-4dea-4b68-11ec69d901e2] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:19.06831 [monitJobSupervisor] 2016/02/05 09:13:19 DEBUG - Starting service redis\n2016-02-05_09:13:19.06832 [http-client] 2016/02/05 09:13:19 DEBUG - Monit request: url='http://127.0.0.1:2822/redis' body='action=start'\n2016-02-05_09:13:19.06834 [attemptRetryStrategy] 2016/02/05 09:13:19 DEBUG - Making attempt #0\n2016-02-05_09:13:19.06836 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=cbfe37ec-86a7-4971-7039-096bc94e8986] Requesting (attempt=1): Request{ Method: 'POST', URL: 'http://127.0.0.1:2822/redis' }\n2016-02-05_09:13:19.09004 [clientRetryable] 2016/02/05 09:13:19 DEBUG - [requestID=cbfe37ec-86a7-4971-7039-096bc94e8986] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:19.09005 [File System] 2016/02/05 09:13:19 DEBUG - Remove all /var/vcap/monit/stopped\n2016-02-05_09:13:19.09009 [MBus Handler] 2016/02/05 09:13:19 INFO - Responding\n2016-02-05_09:13:19.09011 [MBus Handler] 2016/02/05 09:13:19 DEBUG - Payload\n2016-02-05_09:13:19.09013 **\n2016-02-05_09:13:19.09014 {\"value\":\"started\"}\n2016-02-05_09:13:19.09016 **\n2016-02-05_09:13:19.10237 2016/02/05 09:13:19 mail from: \"monit@localhost\"\n2016-02-05_09:13:19.14515 [agent] 2016/02/05 09:13:19 DEBUG - Ignored monit event: %!(EXTRA string=Action done)\n2016-02-05_09:13:19.16941 2016/02/05 09:13:19 mail from: \"monit@localhost\"\n2016-02-05_09:13:19.21869 [agent] 2016/02/05 09:13:19 DEBUG - Ignored monit event: %!(EXTRA string=Exists)\n2016-02-05_09:13:20.09777 [MBus Handler] 2016/02/05 09:13:20 INFO - Received request with action get_state\n2016-02-05_09:13:20.09823 [MBus Handler] 2016/02/05 09:13:20 DEBUG - Payload\n2016-02-05_09:13:20.09825 **\n2016-02-05_09:13:20.09854 {\"protocol\":2,\"method\":\"get_state\",\"arguments\":[],\"reply_to\":\"director.bdfa0a50-23b6-4947-92e1-90b6eef26b8e.9499cc4d-2753-4fed-a0da-36bc46c48bd8\"}\n2016-02-05_09:13:20.09855 **\n2016-02-05_09:13:20.09856 [Action Dispatcher] 2016/02/05 09:13:20 INFO - Running sync action get_state\n2016-02-05_09:13:20.09912 [File System] 2016/02/05 09:13:20 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:13:20.09971 [File System] 2016/02/05 09:13:20 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:13:20.09990 [File System] 2016/02/05 09:13:20 DEBUG - Read content\n2016-02-05_09:13:20.09990 **\n2016-02-05_09:13:20.09990 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:13:20.10044 **\n2016-02-05_09:13:20.10081 [http-client] 2016/02/05 09:13:20 DEBUG - status function called\n2016-02-05_09:13:20.10420 [http-client] 2016/02/05 09:13:20 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:20.11572 [attemptRetryStrategy] 2016/02/05 09:13:20 DEBUG - Making attempt #0\n2016-02-05_09:13:20.11574 [clientRetryable] 2016/02/05 09:13:20 DEBUG - [requestID=f3c12d4e-7552-4b89-53c1-7fe3ff8e9f7c] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:20.11575 [clientRetryable] 2016/02/05 09:13:20 DEBUG - [requestID=f3c12d4e-7552-4b89-53c1-7fe3ff8e9f7c] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:20.11577 [monitJobSupervisor] 2016/02/05 09:13:20 DEBUG - Getting monit status\n2016-02-05_09:13:20.11578 [http-client] 2016/02/05 09:13:20 DEBUG - status function called\n2016-02-05_09:13:20.11579 [http-client] 2016/02/05 09:13:20 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:20.11581 [attemptRetryStrategy] 2016/02/05 09:13:20 DEBUG - Making attempt #0\n2016-02-05_09:13:20.11581 [clientRetryable] 2016/02/05 09:13:20 DEBUG - [requestID=e171be7f-71f4-4de2-7919-143114da6dc8] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:20.11583 [clientRetryable] 2016/02/05 09:13:20 DEBUG - [requestID=e171be7f-71f4-4de2-7919-143114da6dc8] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:20.11804 [File System] 2016/02/05 09:13:20 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:13:20.11810 [File System] 2016/02/05 09:13:20 DEBUG - Reading file /var/vcap/bosh/log/ntpdate.out\n2016-02-05_09:13:20.11815 [MBus Handler] 2016/02/05 09:13:20 INFO - Responding\n2016-02-05_09:13:20.11818 [MBus Handler] 2016/02/05 09:13:20 DEBUG - Payload\n2016-02-05_09:13:20.11821 **\n2016-02-05_09:13:20.11824 {\"value\":{\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"},\"agent_id\":\"fa299ad9-f868-481b-a63e-c45522fafe8c\",\"bosh_protocol\":\"1\",\"job_state\":\"running\",\"processes\":[{\"name\":\"redis\",\"state\":\"running\",\"uptime\":{\"secs\":5},\"mem\":{\"kb\":8764,\"percent\":0.1},\"cpu\":{\"total\":0}}],\"vm\":{\"name\":\"99860a26-62d3-464e-4659-41688467f8d2\"},\"ntp\":{\"message\":\"file missing\"}}}\n2016-02-05_09:13:20.11836 **\n2016-02-05_09:13:55.33726 [agent] 2016/02/05 09:13:55 DEBUG - Building heartbeat\n2016-02-05_09:13:55.33740 [File System] 2016/02/05 09:13:55 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:13:55.33741 [File System] 2016/02/05 09:13:55 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:13:55.33752 [File System] 2016/02/05 09:13:55 DEBUG - Read content\n2016-02-05_09:13:55.33753 **\n2016-02-05_09:13:55.33754 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:13:55.33768 **\n2016-02-05_09:13:55.33769 [monitJobSupervisor] 2016/02/05 09:13:55 DEBUG - Getting monit status\n2016-02-05_09:13:55.33770 [http-client] 2016/02/05 09:13:55 DEBUG - status function called\n2016-02-05_09:13:55.33780 [http-client] 2016/02/05 09:13:55 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:13:55.33781 [attemptRetryStrategy] 2016/02/05 09:13:55 DEBUG - Making attempt #0\n2016-02-05_09:13:55.33782 [clientRetryable] 2016/02/05 09:13:55 DEBUG - [requestID=6cfdbfd8-996b-4e69-7260-55ae097907b8] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:13:55.35516 [clientRetryable] 2016/02/05 09:13:55 DEBUG - [requestID=6cfdbfd8-996b-4e69-7260-55ae097907b8] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:13:55.36056 [File System] 2016/02/05 09:13:55 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:13:55.36058 [NATS Handler] 2016/02/05 09:13:55 INFO - Sending hm message 'heartbeat'\n2016-02-05_09:13:55.36059 [NATS Handler] 2016/02/05 09:13:55 DEBUG - Message Payload\n2016-02-05_09:13:55.36060 **\n2016-02-05_09:13:55.36061 {\"job\":\"redis_leader_z1\",\"index\":0,\"job_state\":\"running\",\"vitals\":{\"cpu\":{\"sys\":\"14.2\",\"user\":\"12.5\",\"wait\":\"0.2\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"20\",\"percent\":\"47\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"system\":{\"inode_percent\":\"20\",\"percent\":\"47\"}},\"load\":[\"2.21\",\"1.69\",\"1.40\"],\"mem\":{\"kb\":\"1886428\",\"percent\":\"31\"},\"swap\":{\"kb\":\"136388\",\"percent\":\"13\"}},\"node_id\":\"\"}\n2016-02-05_09:13:55.36063 **\n2016-02-05_09:14:31.86814 [agent] 2016/02/05 09:14:31 DEBUG - Ignored ssh event: %!(EXTRA string=error: Could not load host key: /etc/ssh/ssh_host_ecdsa_key)\n2016-02-05_09:14:31.86824 [agent] 2016/02/05 09:14:31 DEBUG - Ignored ssh event: %!(EXTRA string=error: Could not load host key: /etc/ssh/ssh_host_ed25519_key)\n2016-02-05_09:14:38.70790 [NATS Handler] 2016/02/05 09:14:38 INFO - Sending hm message 'alert'\n2016-02-05_09:14:38.70793 [NATS Handler] 2016/02/05 09:14:38 DEBUG - Message Payload\n2016-02-05_09:14:38.70794 **\n2016-02-05_09:14:38.70795 {\"id\":\"d2edce92-1530-4d52-6c1a-34725fa0a927\",\"severity\":4,\"title\":\"SSH Login\",\"summary\":\"Accepted password for vcap from 10.244.2.1 port 48318 ssh2\",\"created_at\":1454663678}\n2016-02-05_09:14:38.70798 **\n2016-02-05_09:14:38.71425 [agent] 2016/02/05 09:14:38 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sshd:session): session opened for user vcap by (uid=0))\n2016-02-05_09:14:55.33263 [agent] 2016/02/05 09:14:55 DEBUG - Building heartbeat\n2016-02-05_09:14:55.33445 [File System] 2016/02/05 09:14:55 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:14:55.33463 [File System] 2016/02/05 09:14:55 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:14:55.33625 [File System] 2016/02/05 09:14:55 DEBUG - Read content\n2016-02-05_09:14:55.33628 **\n2016-02-05_09:14:55.33631 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:14:55.33640 **\n2016-02-05_09:14:55.33704 [monitJobSupervisor] 2016/02/05 09:14:55 DEBUG - Getting monit status\n2016-02-05_09:14:55.33775 [http-client] 2016/02/05 09:14:55 DEBUG - status function called\n2016-02-05_09:14:55.33776 [http-client] 2016/02/05 09:14:55 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:14:55.33847 [attemptRetryStrategy] 2016/02/05 09:14:55 DEBUG - Making attempt #0\n2016-02-05_09:14:55.33848 [clientRetryable] 2016/02/05 09:14:55 DEBUG - [requestID=25e2f997-e914-4205-7af5-d7b07c6673d2] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:14:55.34066 [clientRetryable] 2016/02/05 09:14:55 DEBUG - [requestID=25e2f997-e914-4205-7af5-d7b07c6673d2] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:14:55.34108 [File System] 2016/02/05 09:14:55 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:14:55.34196 [NATS Handler] 2016/02/05 09:14:55 INFO - Sending hm message 'heartbeat'\n2016-02-05_09:14:55.34197 [NATS Handler] 2016/02/05 09:14:55 DEBUG - Message Payload\n2016-02-05_09:14:55.34200 **\n2016-02-05_09:14:55.34200 {\"job\":\"redis_leader_z1\",\"index\":0,\"job_state\":\"running\",\"vitals\":{\"cpu\":{\"sys\":\"13.7\",\"user\":\"6.2\",\"wait\":\"0.1\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"20\",\"percent\":\"47\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"system\":{\"inode_percent\":\"20\",\"percent\":\"47\"}},\"load\":[\"2.13\",\"1.74\",\"1.43\"],\"mem\":{\"kb\":\"1891988\",\"percent\":\"31\"},\"swap\":{\"kb\":\"136388\",\"percent\":\"13\"}},\"node_id\":\"\"}\n2016-02-05_09:14:55.34202 **\n2016-02-05_09:15:06.05431 [agent] 2016/02/05 09:15:06 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:auth): authentication failure; logname=vcap uid=1000 euid=0 tty=/dev/pts/0 ruser=vcap rhost=  user=vcap)\n2016-02-05_09:15:10.30830 [agent] 2016/02/05 09:15:10 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:auth): conversation failed)\n2016-02-05_09:15:10.30846 [agent] 2016/02/05 09:15:10 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:auth): auth could not identify password for [vcap])\n2016-02-05_09:15:10.30933 [agent] 2016/02/05 09:15:10 DEBUG - Ignored ssh event: %!(EXTRA string=vcap : 1 incorrect password attempt ; TTY=pts/0 ; PWD=/home/vcap ; USER=root ; COMMAND=/usr/bin/tail -f -n 200 /var/vcap/bosh/log/current)\n2016-02-05_09:15:15.57380 [agent] 2016/02/05 09:15:15 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:auth): authentication failure; logname=vcap uid=1000 euid=0 tty=/dev/pts/0 ruser=vcap rhost=  user=vcap)\n2016-02-05_09:15:22.60187 [agent] 2016/02/05 09:15:22 DEBUG - Ignored ssh event: %!(EXTRA string=vcap : TTY=pts/0 ; PWD=/home/vcap ; USER=root ; COMMAND=/bin/su -)\n2016-02-05_09:15:22.60266 [agent] 2016/02/05 09:15:22 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:session): session opened for user root by vcap(uid=0))\n2016-02-05_09:15:22.60999 [agent] 2016/02/05 09:15:22 DEBUG - Ignored ssh event: %!(EXTRA string=Successful su for root by root)\n2016-02-05_09:15:22.61002 [agent] 2016/02/05 09:15:22 DEBUG - Ignored ssh event: %!(EXTRA string=+ /dev/pts/0 root:root)\n2016-02-05_09:15:22.61053 [agent] 2016/02/05 09:15:22 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(su:session): session opened for user root by vcap(uid=0))\n2016-02-05_09:15:24.93199 [agent] 2016/02/05 09:15:24 DEBUG - Ignored ssh event: %!(EXTRA string=root : TTY=pts/0 ; PWD=/root ; USER=root ; COMMAND=/usr/bin/tail -f -n 200 /var/vcap/bosh/log/current)\n2016-02-05_09:15:24.93247 [agent] 2016/02/05 09:15:24 DEBUG - Ignored ssh event: %!(EXTRA string=pam_unix(sudo:session): session opened for user root by vcap(uid=0))\n2016-02-05_09:15:55.35535 [agent] 2016/02/05 09:15:55 DEBUG - Building heartbeat\n2016-02-05_09:15:55.35641 [File System] 2016/02/05 09:15:55 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:15:55.35643 [File System] 2016/02/05 09:15:55 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:15:55.35645 [File System] 2016/02/05 09:15:55 DEBUG - Read content\n2016-02-05_09:15:55.35646 **\n2016-02-05_09:15:55.35646 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:15:55.35649 **\n2016-02-05_09:15:55.35659 [monitJobSupervisor] 2016/02/05 09:15:55 DEBUG - Getting monit status\n2016-02-05_09:15:55.35661 [http-client] 2016/02/05 09:15:55 DEBUG - status function called\n2016-02-05_09:15:55.35664 [http-client] 2016/02/05 09:15:55 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:15:55.35665 [attemptRetryStrategy] 2016/02/05 09:15:55 DEBUG - Making attempt #0\n2016-02-05_09:15:55.35668 [clientRetryable] 2016/02/05 09:15:55 DEBUG - [requestID=18e8bb01-94ba-4568-6baa-6fead036a83f] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:15:55.36115 [clientRetryable] 2016/02/05 09:15:55 DEBUG - [requestID=18e8bb01-94ba-4568-6baa-6fead036a83f] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:15:55.36268 [File System] 2016/02/05 09:15:55 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:15:55.36412 [NATS Handler] 2016/02/05 09:15:55 INFO - Sending hm message 'heartbeat'\n2016-02-05_09:15:55.36513 [NATS Handler] 2016/02/05 09:15:55 DEBUG - Message Payload\n2016-02-05_09:15:55.36514 **\n2016-02-05_09:15:55.36515 {\"job\":\"redis_leader_z1\",\"index\":0,\"job_state\":\"running\",\"vitals\":{\"cpu\":{\"sys\":\"12.9\",\"user\":\"5.4\",\"wait\":\"0.2\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"20\",\"percent\":\"47\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"system\":{\"inode_percent\":\"20\",\"percent\":\"47\"}},\"load\":[\"1.55\",\"1.65\",\"1.42\"],\"mem\":{\"kb\":\"1897056\",\"percent\":\"31\"},\"swap\":{\"kb\":\"136388\",\"percent\":\"13\"}},\"node_id\":\"\"}\n2016-02-05_09:15:55.36524 **\n2016-02-05_09:16:55.33271 [agent] 2016/02/05 09:16:55 DEBUG - Building heartbeat\n2016-02-05_09:16:55.33676 [File System] 2016/02/05 09:16:55 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:16:55.33818 [File System] 2016/02/05 09:16:55 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:16:55.33871 [File System] 2016/02/05 09:16:55 DEBUG - Read content\n2016-02-05_09:16:55.33873 **\n2016-02-05_09:16:55.33874 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:16:55.33878 **\n2016-02-05_09:16:55.33992 [monitJobSupervisor] 2016/02/05 09:16:55 DEBUG - Getting monit status\n2016-02-05_09:16:55.34037 [http-client] 2016/02/05 09:16:55 DEBUG - status function called\n2016-02-05_09:16:55.34060 [http-client] 2016/02/05 09:16:55 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:16:55.34110 [attemptRetryStrategy] 2016/02/05 09:16:55 DEBUG - Making attempt #0\n2016-02-05_09:16:55.34133 [clientRetryable] 2016/02/05 09:16:55 DEBUG - [requestID=e5e59069-6db0-4673-6b5c-d1753a958cda] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:16:55.35039 [clientRetryable] 2016/02/05 09:16:55 DEBUG - [requestID=e5e59069-6db0-4673-6b5c-d1753a958cda] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:16:55.35114 [File System] 2016/02/05 09:16:55 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:16:55.35125 [NATS Handler] 2016/02/05 09:16:55 INFO - Sending hm message 'heartbeat'\n2016-02-05_09:16:55.35131 [NATS Handler] 2016/02/05 09:16:55 DEBUG - Message Payload\n2016-02-05_09:16:55.35133 **\n2016-02-05_09:16:55.35134 {\"job\":\"redis_leader_z1\",\"index\":0,\"job_state\":\"running\",\"vitals\":{\"cpu\":{\"sys\":\"12.8\",\"user\":\"5.7\",\"wait\":\"0.2\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"20\",\"percent\":\"47\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"system\":{\"inode_percent\":\"20\",\"percent\":\"47\"}},\"load\":[\"1.54\",\"1.62\",\"1.42\"],\"mem\":{\"kb\":\"1895004\",\"percent\":\"31\"},\"swap\":{\"kb\":\"136388\",\"percent\":\"13\"}},\"node_id\":\"\"}\n2016-02-05_09:16:55.35135 **\n2016-02-05_09:17:55.33869 [agent] 2016/02/05 09:17:55 DEBUG - Building heartbeat\n2016-02-05_09:17:55.34277 [File System] 2016/02/05 09:17:55 DEBUG - Checking if file exists /var/vcap/bosh/spec.json\n2016-02-05_09:17:55.34280 [File System] 2016/02/05 09:17:55 DEBUG - Reading file /var/vcap/bosh/spec.json\n2016-02-05_09:17:55.34282 [File System] 2016/02/05 09:17:55 DEBUG - Read content\n2016-02-05_09:17:55.34282 **\n2016-02-05_09:17:55.34282 {\"properties\":{\"logging\":{\"max_log_file_size\":\"\"}},\"job\":{\"name\":\"redis_leader_z1\",\"release\":\"\",\"template\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\",\"templates\":[{\"name\":\"redis\",\"version\":\"1c917ee3b8d3639bf788777602fa4dc74647dcdf\",\"sha1\":\"d6cc4965cbd7ba2eb8ca1932205b4d6bd16ef168\",\"blobstore_id\":\"dceaec35-cf06-43b5-92ff-9d8982d5a82e\"}]},\"packages\":{\"redis-server\":{\"name\":\"redis-server\",\"version\":\"b53d5357ab95a74c9489cd98a024e6ef6047aba0.1\",\"sha1\":\"3ecef141a5da0f4889d37368f3bc6d844f76feb9\",\"blobstore_id\":\"2b9a26df-91d7-46cb-7adc-7a2e13e9a7d4\"}},\"configuration_hash\":\"32fa4ba645f6875371db43a43cb2029898276c92\",\"networks\":{\"redis1\":{\"cloud_properties\":{\"name\":\"random\"},\"default\":[\"dns\",\"gateway\"],\"dns_record_name\":\"0.redis-leader-z1.redis1.redis-warden.bosh\",\"ip\":\"10.244.2.2\",\"netmask\":\"255.255.255.252\"}},\"resource_pool\":{\"cloud_properties\":{\"name\":\"random\"},\"name\":\"small_z1\",\"stemcell\":{\"name\":\"bosh-warden-boshlite-ubuntu-trusty-go_agent\",\"version\":\"3147\"}},\"deployment\":\"redis-warden\",\"index\":0,\"id\":\"\",\"persistent_disk\":4096,\"rendered_templates_archive\":{\"sha1\":\"0a0b6cba58fcb64720ac9ef094787983741fe95e\",\"blobstore_id\":\"2dd5fd15-ccdc-446a-a248-981bfecef8ca\"}}\n2016-02-05_09:17:55.34286 **\n2016-02-05_09:17:55.34344 [monitJobSupervisor] 2016/02/05 09:17:55 DEBUG - Getting monit status\n2016-02-05_09:17:55.34388 [http-client] 2016/02/05 09:17:55 DEBUG - status function called\n2016-02-05_09:17:55.34475 [http-client] 2016/02/05 09:17:55 DEBUG - Monit request: url='http://127.0.0.1:2822/_status2?format=xml' body=''\n2016-02-05_09:17:55.34518 [attemptRetryStrategy] 2016/02/05 09:17:55 DEBUG - Making attempt #0\n2016-02-05_09:17:55.35783 [clientRetryable] 2016/02/05 09:17:55 DEBUG - [requestID=6b72bd78-4426-48e8-527c-abbf9305e685] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://127.0.0.1:2822/_status2?format=xml' }\n2016-02-05_09:17:55.36916 [clientRetryable] 2016/02/05 09:17:55 DEBUG - [requestID=6b72bd78-4426-48e8-527c-abbf9305e685] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n2016-02-05_09:17:55.36968 [File System] 2016/02/05 09:17:55 DEBUG - Checking if file exists /var/vcap/monit/stopped\n2016-02-05_09:17:55.36970 [NATS Handler] 2016/02/05 09:17:55 INFO - Sending hm message 'heartbeat'\n2016-02-05_09:17:55.36971 [NATS Handler] 2016/02/05 09:17:55 DEBUG - Message Payload\n2016-02-05_09:17:55.36972 **\n2016-02-05_09:17:55.36974 {\"job\":\"redis_leader_z1\",\"index\":0,\"job_state\":\"running\",\"vitals\":{\"cpu\":{\"sys\":\"13.9\",\"user\":\"5.4\",\"wait\":\"0.2\"},\"disk\":{\"ephemeral\":{\"inode_percent\":\"20\",\"percent\":\"47\"},\"persistent\":{\"inode_percent\":\"0\",\"percent\":\"0\"},\"system\":{\"inode_percent\":\"20\",\"percent\":\"47\"}},\"load\":[\"1.19\",\"1.50\",\"1.39\"],\"mem\":{\"kb\":\"1894848\",\"percent\":\"31\"},\"swap\":{\"kb\":\"136388\",\"percent\":\"13\"}},\"node_id\":\"\"}\n2016-02-05_09:17:55.36975 *****\n```\n. ",
    "saliceti": "+1\n. +1\nI'm testing with the same versions as @mtekel and I'm experiencing the same issues.\nCreate a new CF deployment. Run delete deployment and it hangs. In the bosh logs I get the errors \"Unresponsive client detected\", Bosh::Registry::InstanceNotFound, etc.\nI could delete it with the \"--force \" option only.\n. @cppforlife: t2.medium (same as @mtekel)\n. ",
    "tomoe": "Here's a discussion that I started in the cf-bosh email list. \nhttps://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/thread/PKKSOPHXECPTXZWLCKNRLGC3EMLBLC6J/\nA relevant knowledge base article is up here:\nhttps://support.pivotal.io/hc/en-us/articles/217922607-Incorrect-system-clock-in-VM\n. I found that this has been merged and included in the release:\nhttps://github.com/cloudfoundry/bosh/releases/tag/stable-3213\nWhat is the policy for closing an issue? \n. ping\n. Closing out as this was my misunderstanding. \nNow I understand cloud check checks at IaaS layer, doesn't care about stuff running on top, which is outside of the scope of the command. \n. @voelzmo Thanks for following up and great to know it's being addressed :-)\n. @voelzmo btw, any plan on fixing in 3146.* series, which is used for internal products (according to https://github.com/cloudfoundry/bosh/issues/1224#issuecomment-214576181) \nThis has relatively a big impact so I'd like to see a fix sooner than later. \ncc @cppforlife \n. Thanks @cppforlife \nI wonder if what the branching/release policy is. Do you maintain release branches and backport bug fixes? Is that documented somewhere? \n. Thanks @dpb587-pivotal for following up. \n\n@cppforlife, perhaps that's something we can document, to help both internal and external expectations.\n\n+1 \n. @cppforlife \n\nwe are trying to deprecate index from the ui of bosh \nIs there a discussion thread or design doc for that change so one can get more details on where it's going? \n. Seems to me that the change would be still useful after the change to uuid from numeric indexing. \nCould this patch be slightly changed and resurrected? \n. LGTM\n. May want to use imperative form (Delete without s at the end) to be consistent with the other messages. \n. \n",
    "beyhan": "Pull request #1152  fixes this issue. \n. Pull request #1152  fixes this issue. \n. Problem resolver task has error state and looks like:\n``\n, [2016-02-25 14:05:53 #13178] [task:55] ERROR -- DirectorJobRunner: Error resolving problem6': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/problem_resolver.rb:48:in apply_resolutions'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:37:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in block in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:30:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:99:in perform_job'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inblock in run'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inrun'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:inblock in work'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:inwork'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in <top (required)>'\n/var/vcap/packages/director/bin/bosh-director-worker:16:inload'\n/var/vcap/packages/director/bin/bosh-director-worker:16:in <main>'\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000478s) SELECT NULL\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000136s) BEGIN\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000690s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2016-02-25 14:05:53.131267+0000', \"description\" = 'scan and fix', \"result\" = 'Error resolving problem6'': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds', \"output\" = '/var/vcap/store/director/tasks/55', \"checkpoint_time\" = '2016-02-25 13:55:00.631683+0000', \"type\" = 'cck_scan_and_fix', \"username\" = 'hm' WHERE (\"id\" = 55)\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.002356s) COMMIT\nI, [2016-02-25 14:05:53 #13178] []  INFO -- DirectorJobRunner: Task took 10 minutes 52.512512077 seconds to process.\nTask 55 error\n```\nbosh tasks recent --no-filter shows:\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| #  | State      | Timestamp               | User  | Description       | Result                                                                         |\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| 65 | error      | 2016-02-25 14:16:51 UTC | hm    | scan and fix      | Error resolving problem `7': Timed out pinging to...                           |\n| 69 | error      | 2016-02-25 14:09:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 68 | error      | 2016-02-25 14:08:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 66 | error      | 2016-02-25 14:06:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 55 | error      | 2016-02-25 14:05:53 UTC | hm    | scan and fix      | Error resolving problem `6': Timed out pinging to...                           |\n. Problem resolver task has error state and looks like:\n``\n, [2016-02-25 14:05:53 #13178] [task:55] ERROR -- DirectorJobRunner: Error resolving problem6': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/problem_resolver.rb:48:in apply_resolutions'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:37:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in block in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:30:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:99:in perform_job'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inblock in run'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inrun'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:inblock in work'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:inwork'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in <top (required)>'\n/var/vcap/packages/director/bin/bosh-director-worker:16:inload'\n/var/vcap/packages/director/bin/bosh-director-worker:16:in <main>'\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000478s) SELECT NULL\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000136s) BEGIN\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000690s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2016-02-25 14:05:53.131267+0000', \"description\" = 'scan and fix', \"result\" = 'Error resolving problem6'': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds', \"output\" = '/var/vcap/store/director/tasks/55', \"checkpoint_time\" = '2016-02-25 13:55:00.631683+0000', \"type\" = 'cck_scan_and_fix', \"username\" = 'hm' WHERE (\"id\" = 55)\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.002356s) COMMIT\nI, [2016-02-25 14:05:53 #13178] []  INFO -- DirectorJobRunner: Task took 10 minutes 52.512512077 seconds to process.\nTask 55 error\n```\nbosh tasks recent --no-filter shows:\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| #  | State      | Timestamp               | User  | Description       | Result                                                                         |\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| 65 | error      | 2016-02-25 14:16:51 UTC | hm    | scan and fix      | Error resolving problem `7': Timed out pinging to...                           |\n| 69 | error      | 2016-02-25 14:09:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 68 | error      | 2016-02-25 14:08:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 66 | error      | 2016-02-25 14:06:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 55 | error      | 2016-02-25 14:05:53 UTC | hm    | scan and fix      | Error resolving problem `6': Timed out pinging to...                           |\n. I created a new one against develop branch.\n. I created a new one against develop branch.\n. Problem resolver task has error state and looks like:\n``\n, [2016-02-25 14:05:53 #13178] [task:55] ERROR -- DirectorJobRunner: Error resolving problem6': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/problem_resolver.rb:48:in apply_resolutions'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:37:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in block in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:30:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:99:in perform_job'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inblock in run'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inrun'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:inblock in work'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:inwork'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in <top (required)>'\n/var/vcap/packages/director/bin/bosh-director-worker:16:inload'\n/var/vcap/packages/director/bin/bosh-director-worker:16:in <main>'\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000478s) SELECT NULL\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000136s) BEGIN\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000690s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2016-02-25 14:05:53.131267+0000', \"description\" = 'scan and fix', \"result\" = 'Error resolving problem6'': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds', \"output\" = '/var/vcap/store/director/tasks/55', \"checkpoint_time\" = '2016-02-25 13:55:00.631683+0000', \"type\" = 'cck_scan_and_fix', \"username\" = 'hm' WHERE (\"id\" = 55)\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.002356s) COMMIT\nI, [2016-02-25 14:05:53 #13178] []  INFO -- DirectorJobRunner: Task took 10 minutes 52.512512077 seconds to process.\nTask 55 error\n```\nbosh tasks recent --no-filter shows:\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| #  | State      | Timestamp               | User  | Description       | Result                                                                         |\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| 65 | error      | 2016-02-25 14:16:51 UTC | hm    | scan and fix      | Error resolving problem `7': Timed out pinging to...                           |\n| 69 | error      | 2016-02-25 14:09:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 68 | error      | 2016-02-25 14:08:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 66 | error      | 2016-02-25 14:06:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 55 | error      | 2016-02-25 14:05:53 UTC | hm    | scan and fix      | Error resolving problem `6': Timed out pinging to...                           |\n. Problem resolver task has error state and looks like:\n``\n, [2016-02-25 14:05:53 #13178] [task:55] ERROR -- DirectorJobRunner: Error resolving problem6': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/problem_resolver.rb:48:in apply_resolutions'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:37:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in block in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock.rb:56:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/lock_helper.rb:13:in with_deployment_lock'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/cloud_check/scan_and_fix.rb:30:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:99:in perform_job'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inblock in run'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.0000.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/job_runner.rb:32:inrun'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/lib/bosh/director/jobs/base_job.rb:10:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/job.rb:227:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:250:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:189:inblock in work'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/resque-1.25.2/lib/resque/worker.rb:166:inwork'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.0000.0/bin/bosh-director-worker:75:in <top (required)>'\n/var/vcap/packages/director/bin/bosh-director-worker:16:inload'\n/var/vcap/packages/director/bin/bosh-director-worker:16:in <main>'\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000478s) SELECT NULL\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000136s) BEGIN\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.000690s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2016-02-25 14:05:53.131267+0000', \"description\" = 'scan and fix', \"result\" = 'Error resolving problem6'': Timed out pinging to c31b4b86-2e7d-4282-b1ba-26a55c4cc348 after 600 seconds', \"output\" = '/var/vcap/store/director/tasks/55', \"checkpoint_time\" = '2016-02-25 13:55:00.631683+0000', \"type\" = 'cck_scan_and_fix', \"username\" = 'hm' WHERE (\"id\" = 55)\nD, [2016-02-25 14:05:53 #13178] [task:55] DEBUG -- DirectorJobRunner: (0.002356s) COMMIT\nI, [2016-02-25 14:05:53 #13178] []  INFO -- DirectorJobRunner: Task took 10 minutes 52.512512077 seconds to process.\nTask 55 error\n```\nbosh tasks recent --no-filter shows:\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| #  | State      | Timestamp               | User  | Description       | Result                                                                         |\n+----+------------+-------------------------+-------+-------------------+--------------------------------------------------------------------------------+\n| 65 | error      | 2016-02-25 14:16:51 UTC | hm    | scan and fix      | Error resolving problem `7': Timed out pinging to...                           |\n| 69 | error      | 2016-02-25 14:09:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 68 | error      | 2016-02-25 14:08:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 66 | error      | 2016-02-25 14:06:59 UTC | hm    | scan and fix      | Unable to get deployment lock, maybe a deployment is in progress. Try again... |\n| 55 | error      | 2016-02-25 14:05:53 UTC | hm    | scan and fix      | Error resolving problem `6': Timed out pinging to...                           |\n. Hi @cppforlife, @xtreme-aaron-jarecki and @pivotal-saman-alv,\nthis pull request is related to change in commit  #9f86d7. We are not sure how the post deployment script should run, if some of the resolutions fail. Currently, it will always run even apply resolution fails to run some of the resolutions and returns an error message [1]. We are not sure what should happen, if some resolutions fail. Should we throw an error and skip the post deployment or it should run [2]?\n[1] https://github.com/cloudfoundry/bosh/pull/1152/files#diff-d2dd38358f14a84c4c3edac663ee320eR44\n[2] https://github.com/cloudfoundry/bosh/pull/1152/files#diff-d2dd38358f14a84c4c3edac663ee320eR48\n. Hi @cppforlife, @xtreme-aaron-jarecki and @pivotal-saman-alv,\nthis pull request is related to change in commit  #9f86d7. We are not sure how the post deployment script should run, if some of the resolutions fail. Currently, it will always run even apply resolution fails to run some of the resolutions and returns an error message [1]. We are not sure what should happen, if some resolutions fail. Should we throw an error and skip the post deployment or it should run [2]?\n[1] https://github.com/cloudfoundry/bosh/pull/1152/files#diff-d2dd38358f14a84c4c3edac663ee320eR44\n[2] https://github.com/cloudfoundry/bosh/pull/1152/files#diff-d2dd38358f14a84c4c3edac663ee320eR48\n. @cppforlife Thanks! \n. @cppforlife Thanks! \n. @zaksoup and @monkeyherder: Thanks for the feedback! We actually saw the need for the proposed refactoring but didn't want to make the change big. It makes absolutely sense to make the proposed refactoring. We are  working on it and will update the pull request.  \nThanks,\nBeyhan\n. @zaksoup and @monkeyherder: Thanks for the feedback! We actually saw the need for the proposed refactoring but didn't want to make the change big. It makes absolutely sense to make the proposed refactoring. We are  working on it and will update the pull request.  \nThanks,\nBeyhan\n. @zaksoup and @monkeyherder: We worked on the refactoring and introduced a new Deployment class. \nWe didn't rebase this pull-request to the current state of develop branch because we wanted to avoid force push and it can still be merged into develop. But we did the rebase locally and ran the BOSH ITs which were all green. \n. Hi @zaksoup and @monkeyherder. We are done with the pull-request. Will be great if you can look into it.\nThanks\nBeyhan\n. Hi @zaksoup and @monkeyherder. We are done with the pull-request. Will be great if you can look into it.\nThanks\nBeyhan\n. We can confirm that the issue is related to expects_vm. The implementation needs to create a deployment plan for this and apparently this operation is to expensive, when the manifests are rather big. We tried a fix where we reduced plan creation from per instance to per deployment, which is a easy optimization but the cpu usage is still high. Tomorrow is a public holiday in Germany and we will continue to work on a solution on Wednesday. So far we don't have an explanation why the issue doesn't occur on AWS.\n. We can confirm that the issue is related to expects_vm. The implementation needs to create a deployment plan for this and apparently this operation is to expensive, when the manifests are rather big. We tried a fix where we reduced plan creation from per instance to per deployment, which is a easy optimization but the cpu usage is still high. Tomorrow is a public holiday in Germany and we will continue to work on a solution on Wednesday. So far we don't have an explanation why the issue doesn't occur on AWS.\n. We introduced a new method for the CPIs calledset_disk_metadata. We found out that different CPIs  behave different in case of a notimplemented/missing method. With this commit:  https://github.com/cloudfoundry/bosh/commit/3d2b2340b241e204e29096396ccd608ece2ab69a we introduced a workaround until its handled properly in the CPIs. We saw that in this pull request you also handle some of the cases. May be you can have a look into the commit.. We introduced a new method for the CPIs calledset_disk_metadata. We found out that different CPIs  behave different in case of a notimplemented/missing method. With this commit:  https://github.com/cloudfoundry/bosh/commit/3d2b2340b241e204e29096396ccd608ece2ab69a we introduced a workaround until its handled properly in the CPIs. We saw that in this pull request you also handle some of the cases. May be you can have a look into the commit.. We use the bosh-cli for interpolation. Our expectation was to get a better error message. Does it make sense to invest time here for a better error message? Also the return code shouldn't be HTTP 500?. We use the bosh-cli for interpolation. Our expectation was to get a better error message. Does it make sense to invest time here for a better error message? Also the return code shouldn't be HTTP 500?. We ran jmeter load tests against both a thin director and a puma director with 3 puma processes querying the GET /info endpoint. This gives us a basic performance comparison between both servers. Setup was:\n 3 jmeter workers\n 500 threads per jmeter worker each haveing a loop count of 1000 =>3 x 500 x 1000 = 1 500 000 requests in total\nPuma has significantly less 502 Bad Gateway errors, lower response times and a higher throughput than Thin.\nThin:\n\nPuma:\n\n. We ran jmeter load tests against both a thin director and a puma director with 3 puma processes querying the GET /info endpoint. This gives us a basic performance comparison between both servers. Setup was:\n 3 jmeter workers\n 500 threads per jmeter worker each haveing a loop count of 1000 =>3 x 500 x 1000 = 1 500 000 requests in total\nPuma has significantly less 502 Bad Gateway errors, lower response times and a higher throughput than Thin.\nThin:\n\nPuma:\n\n. We executed following performance tests:\n* bosh directors:\n  * Latest bosh director from master branch with thin setup and without /deployments endpoint optimization (see https://github.com/cloudfoundry/bosh/pull/1793)\n  * Latest bosh director from master branch with thin setup and with /deployments' endpoint optimization\n  * Latest bosh director from master branch with puma setup with three workers and/deployments` endpoint optimization\n\nall directors had a machine with 4 CPU, 16GB RAM on OpenStack\nwith 10 deployments\ntested against /deployments endpoint\nwe used throughputramp project used by cf-routing team: https://github.com/cloudfoundry-incubator/routing-perf-release. The benchmark started with 10000 request with one thread and incrementally ramped up until 10000 request with 20 threads at a rate limit of 1000. \n\nResults are attached.\n\n\n\n\n\n\n. We executed following performance tests:\n* bosh directors:\n  * Latest bosh director from master branch with thin setup and without /deployments endpoint optimization (see https://github.com/cloudfoundry/bosh/pull/1793)\n  * Latest bosh director from master branch with thin setup and with /deployments' endpoint optimization\n  * Latest bosh director from master branch with puma setup with three workers and/deployments` endpoint optimization\n\nall directors had a machine with 4 CPU, 16GB RAM on OpenStack\nwith 10 deployments\ntested against /deployments endpoint\nwe used throughputramp project used by cf-routing team: https://github.com/cloudfoundry-incubator/routing-perf-release. The benchmark started with 10000 request with one thread and incrementally ramped up until 10000 request with 20 threads at a rate limit of 1000. \n\nResults are attached.\n\n\n\n\n\n\n. @evanphx thank you for the feedback! We changed the binding as suggested. This should be ready now to be merged.. @evanphx thank you for the feedback! We changed the binding as suggested. This should be ready now to be merged.. yes, it is iterating over all non cloud config manifests to get the reserved ips and compute the reserved ranges. Here is an example log for how long it takes (I have removed the reserved ip ranges):\nD, [2017-12-01T12:05:22 #22912] [task:1487595] DEBUG -- DirectorJobRunner: reserved ranges \nD, [2017-12-01T12:05:22 #22912] [task:1487595] DEBUG -- DirectorJobRunner: (0.038069s) SELECT * FROM \"deployments\" WHERE (((\"deployments\".\"id\" NOT IN (SELECT \"deployments_configs\".\"deployment_id\" FROM \"configs\" INNER JOIN \"deployments_configs\" ON (\"deployments_configs\".\"config_id\" = \"configs\".\"id\") WHER\nI, [2017-12-01T12:21:03 #22912] [task:1487595]  INFO -- DirectorJobRunner: Following networks and individual IPs are reserved by non-cloud-config deployments: ...\nD, [2017-12-01T12:21:03 #22912] [task:1487595] DEBUG -- DirectorJobRunner: reserved ranges .... Thanks, it was by accident. Strange that there are two of them :-). We just realized that this has been fixed with: https://www.pivotaltracker.com/story/show/153069939. Steps to reproduce the issue on OpenStack:\n\nDeploy a dummy deployment with persistent disk\nModify the attach_disk CPI method on the director VM to raise an error.\nRedeploy the dummy deployment with --recreate. This will fail with the error specified in step 2.\nReset the changes in the CPI from step 2\nRedeploy the dummy deployment. This will fail with the error from the description in this issue.\n\nWe reproduced the issue on both, CPI api v1 and v2.\n@beyhan && @s4heid . * We couldn't find a reasonable way to avoid the disk mismatch situation. When attach disk fails during a deployment, there is no way to fail the deployment,  in such a state, so that it is resumed from the right place during the next deploy.\n* Bosh cck has the same logic as deploy to find out whether there is a disk mismatch or not. Instances disk information is compared with agents disk information. The logic to fix the situation is like attach_disk_step. Bosh cck calls attach_disk function of a cpi in order to fix the mismatch information. Here is the attach_disk_step which is called during update.\n\nWe also tried to move the handling somewhere else more agent-focused, but we couldn't find such a place, which is better than the current proposed fix.\n\nWe don't have a better proposal at the moment as the proposed fix. @dpb587-pivotal do you have any other ides based on our findings?. * We couldn't find a reasonable way to avoid the disk mismatch situation. When attach disk fails during a deployment, there is no way to fail the deployment,  in such a state, so that it is resumed from the right place during the next deploy.\n* Bosh cck has the same logic as deploy to find out whether there is a disk mismatch or not. Instances disk information is compared with agents disk information. The logic to fix the situation is like attach_disk_step. Bosh cck calls attach_disk function of a cpi in order to fix the mismatch information. Here is the attach_disk_step which is called during update.\n\nWe also tried to move the handling somewhere else more agent-focused, but we couldn't find such a place, which is better than the current proposed fix.\n\nWe don't have a better proposal at the moment as the proposed fix. @dpb587-pivotal do you have any other ides based on our findings?. This is just a spike for discussion. We will definitely reimplement this after we have agreed on the place/way to implement it. The situation in which we need to reattach the disk is:\n1. instance model have a persistent disk cid\n2. instance still requires a persistent disk instance_plan.needs_disk? (the spike doesn't check this)\n3. agent doesn't know anything about persistent disk.\n. This is just a spike for discussion. We will definitely reimplement this after we have agreed on the place/way to implement it. The situation in which we need to reattach the disk is:\n1. instance model have a persistent disk cid\n2. instance still requires a persistent disk instance_plan.needs_disk? (the spike doesn't check this)\n3. agent doesn't know anything about persistent disk.\n. Hi @jfmyers9,\nSorry, I couldn't get back to this issue. It's OK to close it and will reopen it if needed. I still think that the issue exists but didn't observed it for a while. \n. Hi @jfmyers9,\nSorry, I couldn't get back to this issue. It's OK to close it and will reopen it if needed. I still think that the issue exists but didn't observed it for a while. \n. We noticed that we have this failure more often on GCP, because the root disk partition uses the ephemeral disk. When you upload a release to bosh the ephemeral disk is used to temporarily store the uploaded artifacts. Also with a local blobstore setup the ephemeral disk is used by the blobstore as temporary storage during the blob upload. If the ephemeral disk is not big enough you will end up with failures described in this issue.. We noticed that we have this failure more often on GCP, because the root disk partition uses the ephemeral disk. When you upload a release to bosh the ephemeral disk is used to temporarily store the uploaded artifacts. Also with a local blobstore setup the ephemeral disk is used by the blobstore as temporary storage during the blob upload. If the ephemeral disk is not big enough you will end up with failures described in this issue.. I have to think more on this but I can imagine following:\n getting meltdown state: you are right, it is a HM concept. But maybe the BOSH CLI could compute it per deployment based on events generated from HM.\n managing meltdown: could be done in the same way as for resurrection. We can provide config type meltdown which is able to disable or reconfigure the meltdown per deployment. . I have to think more on this but I can imagine following:\n getting meltdown state: you are right, it is a HM concept. But maybe the BOSH CLI could compute it per deployment based on events generated from HM.\n managing meltdown: could be done in the same way as for resurrection. We can provide config type meltdown which is able to disable or reconfigure the meltdown per deployment. . PR https://github.com/cloudfoundry/bosh/pull/2121 is related to this issue. @dpb587-pivotal and @jfmyers9 Dow you know why BOSH cck reattach disk and reboot instance option doesn't execute a disk mount? We find out that this doesn't work without executing an agent mount before the reboot. . @dpb587-pivotal and @jfmyers9 Dow you know why BOSH cck reattach disk and reboot instance option doesn't execute a disk mount? We find out that this doesn't work without executing an agent mount before the reboot. . > Presumably it fails when the agent is coming back up and is running through it's bootstrapping?\nWe haven't observed any failing agents in our test with API V1 and API V2. When the disk information was incorrect the agent started properly but didn't mount the disk. Actually, the disk could be mount also after the reboot but the cck option states: reattach and reboot that is why we mount the disk before the reboot.\n\nDid registry-removal change the behavior of this in any way?\n\nWe observed the same behaviour with and without registry. I think the registry removal did't change anything here. . > Presumably it fails when the agent is coming back up and is running through it's bootstrapping?\nWe haven't observed any failing agents in our test with API V1 and API V2. When the disk information was incorrect the agent started properly but didn't mount the disk. Actually, the disk could be mount also after the reboot but the cck option states: reattach and reboot that is why we mount the disk before the reboot.\n\nDid registry-removal change the behavior of this in any way?\n\nWe observed the same behaviour with and without registry. I think the registry removal did't change anything here. . @dpb587-pivotal and @jfmyers9 this is the PR for attach disk issue we talked about. Also the PR https://github.com/cloudfoundry/bosh/pull/2140 for bosh cck is related.. @dpb587-pivotal and @jfmyers9 this is the PR for attach disk issue we talked about. Also the PR https://github.com/cloudfoundry/bosh/pull/2140 for bosh cck is related.. This is more for documenting reasons and to have it in mind if we change something in the scheduler. There is no functional impact at the moment because as you mentioned monit restarts the process. E.g. we discovered a similar situation in bosh-director-sync-dns, where a combination with EventMachine and Thread spawning shadows the exception and the process doesn't fails. There we proposed this fix. This is more for documenting reasons and to have it in mind if we change something in the scheduler. There is no functional impact at the moment because as you mentioned monit restarts the process. E.g. we discovered a similar situation in bosh-director-sync-dns, where a combination with EventMachine and Thread spawning shadows the exception and the process doesn't fails. There we proposed this fix. ",
    "achawki": "Ok thanks.\n. ",
    "rogeruiz": "Is this functionality any closer to being implemented? From the CloudFoundry Slack, it looks like it might be possible in the future with cloud config and CredHub.. Is this functionality any closer to being implemented? From the CloudFoundry Slack, it looks like it might be possible in the future with cloud config and CredHub.. ",
    "mogul": "Just pinging again on behalf of the cloud.gov team... Is this anywhere in your plans?. ",
    "amulyas": "@dpb587-pivotal I mean io1 \n. it might be running this in old bosh I will try to so this with latest version ad see if that works \n. we can close this ticket for now \n. ",
    "yuanzhao": "Hi,\nPlease find the signed CLA in the attachment. Thank you!\n\n\n. ",
    "timmow": "@cppforlife This doesn't appear to be consistent, some of the tasks will fail with this error, but on re running the delete deployment command they will be able to delete the instance.\n. ",
    "alext": "To add to this, I'm seeing quite strange behaviour when deleting a deployment. I've seen 3 different errors on repeated runs as follows:\n1st run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): uninitialized constant Aws::S3::Errors\n2nd run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): wrong number of arguments (1 for 0)\n3rd run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(116) is still referenced from table \"rendered_templates_archives\".\nContinuing to run it resulted in the number of errors gradually reducing until it succeeded.\n. To add to this, I'm seeing quite strange behaviour when deleting a deployment. I've seen 3 different errors on repeated runs as follows:\n1st run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): uninitialized constant Aws::S3::Errors\n2nd run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): wrong number of arguments (1 for 0)\n3rd run: Failed deleting instances > example_z1/0 (3483c6de-e5a2-4372-bdf2-b6f97e8b385d): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(116) is still referenced from table \"rendered_templates_archives\".\nContinuing to run it resulted in the number of errors gradually reducing until it succeeded.\n. The Travis failure looks to be unrelated to this change (the installed version of bundler doesn't match the Gemfile).\n. The Travis failure looks to be unrelated to this change (the installed version of bundler doesn't match the Gemfile).\n. @voelzmo As far as I can tell, that fix is only included in the 262.x series. It doesn't appear to be included in 265.x. We're currently hitting this issue running v265.2.0.\nI see that https://www.pivotaltracker.com/n/projects/1456570/stories/151434806 is marked as accepted. What needs to happen to get this fix included in 265?\n. I believe this is a significant issue, but perhaps fix doesn't lie with the SHA in the stemcell manifest and instead the bosh director needs to be changed to use different details from the stemcell to generate the key into the compiled package cache.\nAs things stand, if you have the compiled package cache configured in your bosh director, upgrading from one stemcell series to another (eg moving from trusty to xenial) is very likely to result in issues because compiled packages built against trusty will then be used on a xenial machine, which won't necessarily have the same versions of libraries available (as in the capi-release example mentioned above).. Hi, thanks for the response.\nOur use-case for the compiled package cache is purely a performance thing for our development environments. We have a number of slimmed-down development CF deployments, each with their own bosh director. We use the compiled package cache to avoid having to recompile packages for each environment, and each time we tear down and rebuild an environment.\nSo, in short, our use-case is to avoid the repeat compilation times. We don't have any requirements around using the exact same asset etc.\nIn the meantime we're going to investigate how much benefit this feature is actually giving us.. Hi, thanks for the response.\nOur use-case for the compiled package cache is purely a performance thing for our development environments. We have a number of slimmed-down development CF deployments, each with their own bosh director. We use the compiled package cache to avoid having to recompile packages for each environment, and each time we tear down and rebuild an environment.\nSo, in short, our use-case is to avoid the repeat compilation times. We don't have any requirements around using the exact same asset etc.\nIn the meantime we're going to investigate how much benefit this feature is actually giving us.. Hi @belinda-liu,\nIt seems that the output of this discussion is that the compiled package cache feature isn't safe to use when using light stemcells, so we're going to have to disable it. This will have a performance penalty for us in our development environments, but I think we can probably live with that.\nI feel that this is something that should at least be called out in the docs though so that other people don't get bitten by the same issue.. Hi @belinda-liu,\nIt seems that the output of this discussion is that the compiled package cache feature isn't safe to use when using light stemcells, so we're going to have to disable it. This will have a performance penalty for us in our development environments, but I think we can probably live with that.\nI feel that this is something that should at least be called out in the docs though so that other people don't get bitten by the same issue.. ",
    "mbhave": "@mariash @tylerschultz I spoke to @cppforlife and he said pre-start is not meant for this purpose because it does not handle job dependencies. So, if the postgres job wasn't up yet UAA migrations would fail.\n. ",
    "bodymindarts": "@cppforlife why is having cf-release use webdav server a blocker for this?\n. E, [2016-05-11 14:30:59 #15499] [] ERROR -- DirectorJobRunner: Worker thread raised exception: undefined method `disk_cid' for nil:NilClass - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:256:in `rescue in\ncreate_and_attach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:247:in `create_and_attach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:21:in `update_persistent_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater.rb:84:in `block in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater/instance_state.rb:5:in `with_instance_update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater.rb:46:in `update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:111:in `block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_formatter.rb:49:in `with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:109:in `block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:108:in `update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:103:in `block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\nD, [2016-05-11 14:30:59 #15499] [] DEBUG -- DirectorJobRunner: Thread is no longer needed, cleaning up\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: Shutting down pool\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: (0.000384s) SELECT NULL\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: (0.000378s) SELECT \"stemcells\".* FROM \"stemcells\" INNER JOIN \"deployments_stemcells\" ON ((\"deployments_stemcells\".\"stemcell_id\" = \"stemcells\".\"id\") AND (\"deployments_stemcells\".\"deployment_id\" = 66))\nD, [2016-05-11 14:30:59 #15499] [] DEBUG -- DirectorJobRunner: Lock renewal thread exiting\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: Deleting lock: lock:deployment:play-cf-bosh-lite-47\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: Deleted lock: lock:deployment:play-cf-bosh-lite-47\nI, [2016-05-11 14:30:59 #15499] [task:3897]  INFO -- DirectorJobRunner: sending update deployment error event\nD, [2016-05-11 14:30:59 #15499] [task:3897] DEBUG -- DirectorJobRunner: SENT: hm.director.alert {\"id\":\"98f7a54a-47d6-47e0-8d87-8f7e351002b7\",\"severity\":3,\"source\":\"director\",\"title\":\"director - error during update deployment\",\"summary\":\"Error during update deployment for\n'play-cf-bosh-lite-47' against Director 'f635b1ba-5000-48e1-92c8-19471d20e0e4': #<NoMethodError: undefined method `disk_cid' for nil:NilClass>\",\"created_at\":1462977059}\nE, [2016-05-11 14:30:59 #15499] [task:3897] ERROR -- DirectorJobRunner: undefined method `disk_cid' for nil:NilClass\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:256:in `rescue in create_and_attach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:247:in `create_and_attach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/disk_manager.rb:21:in `update_persistent_disk'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater.rb:84:in `block in update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater/instance_state.rb:5:in `with_instance_update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_updater.rb:46:in `update'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:111:in `block (2 levels) in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_formatter.rb:49:in `with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:109:in `block in update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:108:in `update_canary_instance'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/job_updater.rb:103:in `block (2 levels) in update_canaries'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\n. Okay understood. That will do for now.\n. ",
    "fj": "+1 for --redact-properties, but possibly allow a list?\ne.g.:\n- --redact-properties: redact everything\n- --redact-matching-properties=S1,S2,...: redact properties containing any of the substrings S1, S2, ... (e.g. --redact-matching-properties=_SECRET,_KEY,_PASSWORD)\nThe justification is that I may want to redact, say, SECRET_API_PASSWORD but not PUBLIC_SERVICE_NAME.\n. +1 for --redact-properties, but possibly allow a list?\ne.g.:\n- --redact-properties: redact everything\n- --redact-matching-properties=S1,S2,...: redact properties containing any of the substrings S1, S2, ... (e.g. --redact-matching-properties=_SECRET,_KEY,_PASSWORD)\nThe justification is that I may want to redact, say, SECRET_API_PASSWORD but not PUBLIC_SERVICE_NAME.\n. ",
    "Rylon": "Hi @cppforlife do you know if this --no-redact option can be made default, for example by setting something in the BOSH config yml?. Hi @dpb587-pivotal thanks for your response. I understand and agree with the motivation behind making it \"secure by default\", but in this case I'm asking for a way for users to opt-out, by explicitly adding the option to the config file.\nThis seems like it would still satisfy the \"expect secure defaults\" approach, but still give people the option to disable it if they have valid reasons, best of both worlds? :). ",
    "pronoiac": "@jim80net\n. ",
    "ChaosEternal": "I am arranging a detailed test to prove/disprove my idea.\nOn Wed, Mar 16, 2016 at 1:47 AM Dmitriy Kalinin notifications@github.com\nwrote:\n\n@ChaosEternal https://github.com/ChaosEternal bump. thoughts on what\n@dpb587-pivotal https://github.com/dpb587-pivotal said?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/pull/1166#issuecomment-196944592\n. \n",
    "omazhary": "Hello everyone,\nI've tried to get around to the problem myself, and I actually believe I've solved it. You can find it here.\nThe problem is, it breaks the unit-tests that check for stemcell duplicates.\nHow would you propose to proceed?\nRegards,\nOmar\n. Hello everyone,\nI've tried to get around to the problem myself, and I actually believe I've solved it. You can find it here.\nThe problem is, it breaks the unit-tests that check for stemcell duplicates.\nHow would you propose to proceed?\nRegards,\nOmar\n. Hi Dmitriy,\nthanks for your response.\nI've done a little checking. You have a point. I assume this block will cause a problem.\nRegards,\nOmar\n. Hi Dmitriy,\nthanks for your response.\nI've done a little checking. You have a point. I assume this block will cause a problem.\nRegards,\nOmar\n. Hi Dmitriy,\nI've submitted the pull request. I still, however, have not fixed the unit tests. Should I give it a shot as well, or is this something that should be done by the bosh team?\nRegards,\nOmar\n. Hi Dmitriy,\nI've submitted the pull request. I still, however, have not fixed the unit tests. Should I give it a shot as well, or is this something that should be done by the bosh team?\nRegards,\nOmar\n. Thanks for the suggestion!! Removed the commented out line.\n. Thanks for the suggestion!! Removed the commented out line.\n. ",
    "s4heid": "Hi @omazhary,\nthanks for submitting the issue. This commit should fix the problem.\nBest regards,\n@dpb587-pivotal && @s4heid . @bingosummer, \nThanks for submitting this issue. We addressed the bug in this commit. We'll close the issue for now, feel free to reopen if there are any other problems.\nThanks,\n@s4heid && @mikexuu . @srikanth19 thanks for submitting this issue. Unfortunately we need more details in order to help you. If this is a bug, please close and reopen this as a bug report, and follow the template. . Good question. Instead of failing with an error I thought about just being more verbose and giving the operator a hint that his actions have no effect. I see your point that in most cases this behavior pretends a false truth and the operator's original intention wouldn't be fulfilled. On the other hand, I'm a bit concerned that some people may use this command in scripts and rely on the current behavior that bosh doesn't fail. Apart from that I totally agree with you and think this should be the correct behavior of bosh.. ",
    "valenbb": "I will double check for IP conflicts, but if it was/is, bosh would report it. Per item #1, running bosh vms does report unresponsive agents for a few vms that were successfully created. So I will login to one of these vms and check the agent log for reported issues. \nRecreate fails due to IP conflicts being detected. -- I believe this is due to bosh trying to recreate vms that were successfully created from the first deployment (command ran was bosh deploy --recreate).\nThanks for the suggestions and I will continue investigating further and update along the way. \n. I will double check for IP conflicts, but if it was/is, bosh would report it. Per item #1, running bosh vms does report unresponsive agents for a few vms that were successfully created. So I will login to one of these vms and check the agent log for reported issues. \nRecreate fails due to IP conflicts being detected. -- I believe this is due to bosh trying to recreate vms that were successfully created from the first deployment (command ran was bosh deploy --recreate).\nThanks for the suggestions and I will continue investigating further and update along the way. \n. @cppforlife, Thanks again. I setup a new static IP range but it still failed with a timeout for 5 servers. Reran bosh deploy again and this time it reported IP conflicts; i did see some of the dynamic IPs bosh is trying to use was attached to servers in DNS. So I added a new reserved IP range and reran the deployment; this succeeded. I am now able to proceed to the updating process.\nClosing ticket.\n. @cppforlife, Thanks again. I setup a new static IP range but it still failed with a timeout for 5 servers. Reran bosh deploy again and this time it reported IP conflicts; i did see some of the dynamic IPs bosh is trying to use was attached to servers in DNS. So I added a new reserved IP range and reran the deployment; this succeeded. I am now able to proceed to the updating process.\nClosing ticket.\n. Agreed with @dpb587-pivotal. It sounds like your agent is not responding. Give his suggestion a try.. Agreed with @dpb587-pivotal. It sounds like your agent is not responding. Give his suggestion a try.. ",
    "deepakdefender264": "Hi @valenbb \nWe are getting the similar issue on deploying cloud foundry on vSphere environment but its getting failed on bosh deploy.\nBelow is the error and deployment file we have created. Is the error similar to your deployment ?\n\ncf-stub.txt\nbosh.txt\ncf-deployment.txt\n. Thanks @dpb587 and @valenbb we will raise a new issue.. ",
    "Kiemes": "@beyhan\n. Good catch! We have made the env vars an explicit argument. The unit task was missing the DB env var, we fixed that, too.\n. @cppforlife: Do you have any link to a story in tracker for this yet?\n. @cppforlife thanks for the hint. We actually tried both ways, doing a deployment which uses cloud-config and one without cloud-config. In both cases we saw the IP addresses in the instances table in spec_json column. This is even the case regardless of manual or dynamic networks.\n{\n    \"deployment\": \"dummy-legacy\",\n    \"networks\": {\n        \"dummy-dynamic\": {\n            \"type\": \"dynamic\",\n            \"cloud_properties\": {...},\n            \"default\": [\"dns\", \"gateway\"],\n            \"ip\": \"10.0.2.102\",\n            \"netmask\": \"255.255.255.0\",\n            \"gateway\": \"10.0.2.1\"\n        },\n        \"public\": {\n            \"type\": \"vip\",\n            \"ip\": \"192.168.147.159\",\n            \"cloud_properties\": {}\n        }\n    },\n    \"links\": {},\n    \"address\": null,\n    ..\n}\n{\n    \"deployment\": \"dummy-with-cloud-config\",\n    \"networks\": {\n        \"dummy-manual\": {\n            \"ip\": \"10.0.1.12\",\n            \"netmask\": \"255.255.255.0\",\n            \"cloud_properties\": {...},\n            \"default\": [\"dns\", \"gateway\"],\n            \"dns\": [\"172.16.0.1\"],\n            \"gateway\": \"10.0.1.1\"\n        },\n        \"public\": {\n            \"type\": \"vip\",\n            \"ip\": \"192.168.147.160\",\n            \"cloud_properties\": {}\n        }\n    },\n    \"links\": {},\n    \"address\": \"10.0.1.12\",\n}\nOur implementation takes every ip_address from the networks array. So even if the address is set to null in legacy deployments, we would still include every IP. Is this correct or are we missing something?\nBTW the director schema in bosh docs is outdated.\n. @cppforlife you were right, the association 'ip_addresses' is empty in some cases. Therefor we change the behavior to use 'spec' as a fallback. There we showed already that IP addresses are listed in all cases.\nIn contrast to using the agent data as fallback, our current proposal will work for unresponsive agents as well.\nUnits and integration tests are green. Feel free to merge. Unless you have additional feedback.\n. @cppforlife That is basically a cherry-pick of https://github.com/cloudfoundry/bosh/commit/78cc74c5f72601c08361c6c777dd94f368456283 onto 257.x in order to get it released faster.\n. This also applies for bosh start\n. @cppforlife It was just an observation. I did not investigate.\n. This could be a solution https://github.com/cloudfoundry/bosh/commit/4f149918ef7d649a134382958e1ee2072b05a0d5. @cppforlife @voelzmo what do you think?\nThere is one drawback. If the user decides to put the default cpi into the cpi-config as well, stemcells are uploaded twice to the same project. \n\neventually i see us moving away entirely from default configuration on the cpi.\n\nWith this you cannot get rid of the default cpi. This would require special treatment like a consider_default_cpi flag. The name default cpi might be reconsidered then.\n. The dependency comes via https://rubygems.org/gems/bosh_cli/. @tylerschultz thx, that helped. I had an old bosh_cli gem 0.0.0 installed which was just reused by bundler. @cppforlife pipelines were green because you start with a clean gemset.. I now had time to try out the ca_certs job of os-conf.\nThinking about the process, I wonder what happens if you need the additional certs during compilation time? In the create-env case, this is done on the director VM, isn't is? Jobs will be applied after compilation, right?\nThe agent picking up settings.json and handling the certs would happen before compilation I guess.. @cppforlife yes, for example. Or for the compiled packages cache. If you have an own S3 compatible system with your own certificates.. @dpb587-pivotal @cppforlife any further comments?. @mfine30 sorry. But if you know which version was latest on 23 of August last year, I am pretty sure that it was that version as I am mostly up to date :). @voelzmo wanted this as starting point for further discussions. We have a follow-up story to get more numbers. At the moment we can just say that Puma can deal with more load than Thin.\nThere is a test available which fails with Thin but runs green with Puma.. @dpb587-pivotal from the line you are commenting on I guess you ask for instance_id in update_VM_metadata? We explicitly left that tag out as its value is in the metadata tag id. As we are within the context of the VM/instance, I think we should not \"namespace\" id with instance. We are using instance_id in DISK metadata as we are in the context of the disk and we need to namespace the id of the instance.\nNow you could argue why we used instance_group in the context of VM metadata and not just group. We actually were not sure there. instance_group is what people know from the manifest. I guess they would wonder about what group would be.\nIf the outcome of the discussion will be we need to add instance namespace here, then this would also mean instance_name and instance_index. But I hope we do not need the namespace.. ",
    "christianang": "It looks like this issue is because of the ARP cache not being cleared. After manually deleting the entry in the ARP cache we were able to start pinging the machine again.\netcd_z1/0 BEFORE kill: Can ping etcd_z1/1\n```\nroot@b420df28-2036-44cc-9300-1ad1686cbd25:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:00:f0:b5:3a:6d\n          inet addr:10.0.16.4  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::c00:f0ff:feb5:3a6d/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:14619 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:13876 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6698546 (6.6 MB)  TX bytes:1326526 (1.3 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:682 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:682 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:79407 (79.4 KB)  TX bytes:79407 (79.4 KB)\nroot@b420df28-2036-44cc-9300-1ad1686cbd25:~# ping 10.0.16.5\nPING 10.0.16.5 (10.0.16.5) 56(84) bytes of data.\n64 bytes from 10.0.16.5: icmp_seq=1 ttl=64 time=0.336 ms\n64 bytes from 10.0.16.5: icmp_seq=2 ttl=64 time=0.373 ms\n^C\n--- 10.0.16.5 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.336/0.354/0.373/0.026 ms\n```\netcd_z1/1 BEFORE kill: Can ping etcd_z1/0\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:d1:41:e9:f8:9f\n          inet addr:10.0.16.5  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::cd1:41ff:fee9:f89f/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:13379 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:13359 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6597678 (6.5 MB)  TX bytes:1253978 (1.2 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:730 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:730 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:85836 (85.8 KB)  TX bytes:85836 (85.8 KB)\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n64 bytes from 10.0.16.4: icmp_seq=1 ttl=64 time=2.09 ms\n64 bytes from 10.0.16.4: icmp_seq=2 ttl=64 time=0.366 ms\n64 bytes from 10.0.16.4: icmp_seq=3 ttl=64 time=0.340 ms\n64 bytes from 10.0.16.4: icmp_seq=4 ttl=64 time=0.355 ms\n^C\n--- 10.0.16.4 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3001ms\nrtt min/avg/max/mdev = 0.340/0.787/2.090/0.752 ms\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:00:f0:b5:3a:6d REACHABLE\n```\netcd_z1/0 AFTER kill and bosh cck: cannot reach etcd_z1/1\n```\nroot@1361b538-b72a-4864-ac85-d6c18f393492:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:e0:da:db:34:73\n          inet addr:10.0.16.4  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::ce0:daff:fedb:3473/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:8419 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8226 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6230860 (6.2 MB)  TX bytes:681003 (681.0 KB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:627 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:627 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:73000 (73.0 KB)  TX bytes:73000 (73.0 KB)\nroot@1361b538-b72a-4864-ac85-d6c18f393492:~# ping 10.0.16.5\nPING 10.0.16.5 (10.0.16.5) 56(84) bytes of data.\n^C\n--- 10.0.16.5 ping statistics ---\n4 packets transmitted, 0 received, 100% packet loss, time 2999ms\n```\netcd_z1/1 AFTER kill and bosh cck: cannot reach etcd_z1/0\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n^C\n--- 10.0.16.4 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 1999ms\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:00:f0:b5:3a:6d REACHABLE\n```\nOn etcd_z1/1, after clearing entry from arp cache: MAC address for etcd_z1/0 is updated and can now be pinged\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# arp -d 10.0.16.4\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:e0:da:db:34:73 REACHABLE\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n64 bytes from 10.0.16.4: icmp_seq=1 ttl=64 time=0.365 ms\n64 bytes from 10.0.16.4: icmp_seq=2 ttl=64 time=0.334 ms\n64 bytes from 10.0.16.4: icmp_seq=3 ttl=64 time=0.387 ms\n64 bytes from 10.0.16.4: icmp_seq=4 ttl=64 time=0.377 ms\n^C\n--- 10.0.16.4 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 2997ms\nrtt min/avg/max/mdev = 0.334/0.365/0.387/0.030 ms\n```\n. It looks like this issue is because of the ARP cache not being cleared. After manually deleting the entry in the ARP cache we were able to start pinging the machine again.\netcd_z1/0 BEFORE kill: Can ping etcd_z1/1\n```\nroot@b420df28-2036-44cc-9300-1ad1686cbd25:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:00:f0:b5:3a:6d\n          inet addr:10.0.16.4  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::c00:f0ff:feb5:3a6d/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:14619 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:13876 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6698546 (6.6 MB)  TX bytes:1326526 (1.3 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:682 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:682 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:79407 (79.4 KB)  TX bytes:79407 (79.4 KB)\nroot@b420df28-2036-44cc-9300-1ad1686cbd25:~# ping 10.0.16.5\nPING 10.0.16.5 (10.0.16.5) 56(84) bytes of data.\n64 bytes from 10.0.16.5: icmp_seq=1 ttl=64 time=0.336 ms\n64 bytes from 10.0.16.5: icmp_seq=2 ttl=64 time=0.373 ms\n^C\n--- 10.0.16.5 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.336/0.354/0.373/0.026 ms\n```\netcd_z1/1 BEFORE kill: Can ping etcd_z1/0\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:d1:41:e9:f8:9f\n          inet addr:10.0.16.5  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::cd1:41ff:fee9:f89f/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:13379 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:13359 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6597678 (6.5 MB)  TX bytes:1253978 (1.2 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:730 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:730 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:85836 (85.8 KB)  TX bytes:85836 (85.8 KB)\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n64 bytes from 10.0.16.4: icmp_seq=1 ttl=64 time=2.09 ms\n64 bytes from 10.0.16.4: icmp_seq=2 ttl=64 time=0.366 ms\n64 bytes from 10.0.16.4: icmp_seq=3 ttl=64 time=0.340 ms\n64 bytes from 10.0.16.4: icmp_seq=4 ttl=64 time=0.355 ms\n^C\n--- 10.0.16.4 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3001ms\nrtt min/avg/max/mdev = 0.340/0.787/2.090/0.752 ms\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:00:f0:b5:3a:6d REACHABLE\n```\netcd_z1/0 AFTER kill and bosh cck: cannot reach etcd_z1/1\n```\nroot@1361b538-b72a-4864-ac85-d6c18f393492:~# ifconfig\neth0      Link encap:Ethernet  HWaddr 0e:e0:da:db:34:73\n          inet addr:10.0.16.4  Bcast:10.0.16.255  Mask:255.255.255.0\n          inet6 addr: fe80::ce0:daff:fedb:3473/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:8419 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8226 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000\n          RX bytes:6230860 (6.2 MB)  TX bytes:681003 (681.0 KB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:627 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:627 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:73000 (73.0 KB)  TX bytes:73000 (73.0 KB)\nroot@1361b538-b72a-4864-ac85-d6c18f393492:~# ping 10.0.16.5\nPING 10.0.16.5 (10.0.16.5) 56(84) bytes of data.\n^C\n--- 10.0.16.5 ping statistics ---\n4 packets transmitted, 0 received, 100% packet loss, time 2999ms\n```\netcd_z1/1 AFTER kill and bosh cck: cannot reach etcd_z1/0\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n^C\n--- 10.0.16.4 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 1999ms\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:00:f0:b5:3a:6d REACHABLE\n```\nOn etcd_z1/1, after clearing entry from arp cache: MAC address for etcd_z1/0 is updated and can now be pinged\n```\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# arp -d 10.0.16.4\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ip neigh list\n10.0.16.6 dev eth0 lladdr 0e:e8:a3:20:af:c7 REACHABLE\n10.0.16.1 dev eth0 lladdr 0e:fb:85:29:70:e7 REACHABLE\n10.0.16.4 dev eth0 lladdr 0e:e0:da:db:34:73 REACHABLE\nroot@12d68830-e556-48e0-bab6-6080e19e0e47:~# ping 10.0.16.4\nPING 10.0.16.4 (10.0.16.4) 56(84) bytes of data.\n64 bytes from 10.0.16.4: icmp_seq=1 ttl=64 time=0.365 ms\n64 bytes from 10.0.16.4: icmp_seq=2 ttl=64 time=0.334 ms\n64 bytes from 10.0.16.4: icmp_seq=3 ttl=64 time=0.387 ms\n64 bytes from 10.0.16.4: icmp_seq=4 ttl=64 time=0.377 ms\n^C\n--- 10.0.16.4 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 2997ms\nrtt min/avg/max/mdev = 0.334/0.365/0.387/0.030 ms\n```\n. ",
    "zachgersh": "@cppforlife this is a major plus one and huge improvement.\n. @cppforlife this is a major plus one and huge improvement.\n. @wfernandes would it be possible to write an integration test for this? Would be really nice if we could go build a plugin that would be started / receive and maybe echo the events?\nJust a quick glance first idea.. @wfernandes would it be possible to write an integration test for this? Would be really nice if we could go build a plugin that would be started / receive and maybe echo the events?\nJust a quick glance first idea.. This was verified on a bosh-lite deployment by using\nbosh task X --debug\nand looking for the set_vm_metadata call.\nRan the integration/unit tests as well to verify that all was passing.. This was verified on a bosh-lite deployment by using\nbosh task X --debug\nand looking for the set_vm_metadata call.\nRan the integration/unit tests as well to verify that all was passing.. ",
    "evanphx": "@zachgersh I emailed in the CLA. The CLA was probably 3 to 4 times more text input that the actual fix.\n. If you have questions about tuning puma, just ask!\n. @voelzmo Some quick notes on your config:\n\nThe bind and port are mutually exclusive. If you want to bind to localhost like that, you'll need to interpolate the port into the tcp url.\nNo need to change the nginx config when you change the number of puma workers/threads, they're totally independent variables.\nDisconnecting the pool before fork to spawn a new pool per worker is a common pattern, that is fine.\n\nBasically, fix that url/port issue and you're good!. ",
    "ryanmoran": ":100: \n. :100: \n. ",
    "cheeseblubber": "Merged the other two commits: \nhttps://github.com/cloudfoundry/bosh/commit/dcf23d50fe9bc9c17652254b891c950cb21e252f \nhttps://github.com/cloudfoundry/bosh/commit/e30ddaab408599c135ce4810625d90ec5ec198b3\n. Merged with develop https://github.com/cloudfoundry/bosh/commit/7ea217514346154374a7e8bd53bb93e7cc65d1c9\n. ",
    "kalambet": "@voelzmo sure, I'll separate them. thanks!\n. This PR is a first part of separated https://github.com/cloudfoundry/bosh/pull/1203 \n. As discussed with @cppforlife added changes for rsyslog execution in chroot.\n. This PR is a first part of separated https://github.com/cloudfoundry/bosh/pull/1203\n. @cppforlife not sure about that. Last 2 PRs aimed on propper BOSH_MICRO_ENABLED support for ppc64le stemcell build. \nBefore this changes in case this variable is set to 'no' the cache dir for local blobstore is not created, so bosh-agent fails on adding blob to the blobstore. That is the reason for mkdir.\nPlus since this var is a propper way to disable 'bosh-ruby' and 'bosh_micro_go' micro stages I removed exceptions form the stages lists.\nSo current build of ppc64le supported stemcell executes with this var set to 'no'.\nBut all those descriptions are for other bosh PR. In this PR we are changing the way bosh release created for ppc64le support, like nokogiri build and build postgresql.\n. Second commit removes curl for getting config.sub and config.guess for posrgres package and use locally stored versions.\n. ",
    "raindoctor": "@allomov, dynamic networks + cf-231 + any micro bosh with stemcell from 3008 to 3160 is working. The moment I upgrade to 3161, I am getting this error. So, why loggregator templates are fine with other versions of bosh?\n. @allomov, dynamic networks + cf-231 + any micro bosh with stemcell from 3008 to 3160 is working. The moment I upgrade to 3161, I am getting this error. So, why loggregator templates are fine with other versions of bosh?\n. ",
    "chrisrana": "I am also facing same error.I am using 3232.4 with cf-231.Please help resolve this issue\nUnable to render jobs for instance group 'consul'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'cloud_controller'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'api_worker'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'router'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'dea-spare'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'doppler'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'loggregator_trafficcontroller'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n   - Unable to render jobs for instance group 'haproxy'. Errors are:\n     - Unable to render templates for job 'metron_agent'. Errors are:\n       - Error filling in template 'syslog_forwarder.conf.erb' (line 44: undefined method `strip' for nil:NilClass)\n.  bosh --version\nBOSH 1.3071.0\nroot@dev-inception-vm21:/home/ubuntu# bosh status\nConfig\n/root/.bosh_config\nDirector\nName installdev2\nURL https://10.20.0.55:25555\nVersion 1.3155.0 (00000000)\nUser admin\nUUID 3dc7bb73-4fa6-4c83-ab33-e9c870861e05\nCPI openstack\ndns enabled (domain_name: microbosh)\ncompiled_package_cache disabled\nsnapshots disabled\nDeployment\nManifest /opt/installer/tenant-devtest21vik/cf-deploy/cf-devtest21vik.yml\nroot@dev-inception-vm21:/home/ubuntu#\n. What is the latest version of bosh cli .which one I should use\n. Bosh director means stemcell version is it ?\n. Thanks for info.\nI forgot mention I use MicroBosh for deployment because it is Single VM director.I don't use separate bosh release for bosh director.How can I update my microbosh ?\nearlier I was assumption that stemcell has all the required release to create director. I am confused now.\nI do following to install microbosh.\nSTEMCELL_VERSION=3125\ncreate micro_bosh.yml\ncreate micro_bosh.rb\nbosh -n micro deployment micro01\nbosh -n micro deploy  ${BOSH_HOME}/stemcells/$STEMCELL_VERSION\n. I didnt get you .Just to inform you same cf 233 works fine with microbosh.\n I am facing this issue in bosh-init.But problem here is why we have two host entry is mapped with single IP in postgres DB  for a vm instance.I having feeling that because  of this host name is not resolving.PLease refer image pasted there it show bosh vms output\n. I was not aware about 2dns entries thanks for info.\nI checked /etc/resolv.conf It doesnot contain bosh director IP.\ndig b1577113-6e2b-4830-8d96-4f7e58bbd092.cloud-controller.ccc-bosh-net.cf-installer-team-test.microbosh\n```\n; <<>> DiG 9.9.5-3ubuntu0.8-Ubuntu <<>> b1577113-6e2b-4830-8d96-4f7e58bbd092.cloud-controller.ccc-bosh-net.cf-installer-team-test.microbosh\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 60084\n;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;b1577113-6e2b-4830-8d96-4f7e58bbd092.cloud-controller.ccc-bosh-net.cf-installer-team-test.microbosh. IN        A\n;; AUTHORITY SECTION:\n.                       1797    IN      SOA     a.root-servers.net. nstld.verisign-grs.com. 2016091400 1800 900 604800 86400\n;; Query time: 17 msec\n;; SERVER: 127.0.0.1#53(127.0.0.1)\n;; WHEN: Wed Sep 14 18:24:56 UTC 2016\n;; MSG SIZE  rcvd: 203\n```\ndig director_ip\n```\n; <<>> DiG 9.9.5-3ubuntu0.8-Ubuntu <<>> 10.20.0.55\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 15502\n;; flags: qr aa rd ra ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n;; QUESTION SECTION:\n;10.20.0.55.                    IN      A\n;; ANSWER SECTION:\n10.20.0.55.             0       IN      A       10.20.0.55\n;; Query time: 0 msec\n;; SERVER: 10.20.0.2#53(10.20.0.2)\n;; WHEN: Wed Sep 14 18:25:08 UTC 2016\n;; MSG SIZE  rcvd: 44\n```\ncf-template\nJob is referring following setting for networks \n```\nnetworks:\n - name: ccc-bosh-net\n   type: dynamic\n   cloud_properties:\n      net_id: <%= $bosh_net_id %>\n      security_groups:\n        - <%= $new_securitygroup %>\n......\n...\njobs:\n- instances: 1\n  name: cloud_controller\n  networks:\n  - name: ccc-bosh-net\n  persistent_disk: 20000\n```\n. Thanks tylers. It worked like a charm.\n. I see these line .looks like it went in loop .I am repetitively facing this problem.I have 3 dea-spare and all are failing state.\nDrain.log\n```\nI, [2016-09-30T11:03:17.886691 #5446]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:18.012771 #5446]  INFO -- : Hey BOSH, call me back in 5s.\nI, [2016-09-30T11:03:23.028037 #5449]  INFO -- : Drain script invoked with job_check_status hash_unchanged\nI, [2016-09-30T11:03:23.028148 #5449]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:23.028186 #5449]  INFO -- : Hey BOSH, call me back in 5s.\nI, [2016-09-30T11:03:28.044357 #5452]  INFO -- : Drain script invoked with job_check_status hash_unchanged\nI, [2016-09-30T11:03:28.044477 #5452]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:28.044531 #5452]  INFO -- : Hey BOSH, call me back in 5s.\nI, [2016-09-30T11:03:33.061208 #5455]  INFO -- : Drain script invoked with job_check_status hash_unchanged\nI, [2016-09-30T11:03:33.061353 #5455]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:33.061394 #5455]  INFO -- : Hey BOSH, call me back in 5s.\nI, [2016-09-30T11:03:38.077071 #5458]  INFO -- : Drain script invoked with job_check_status hash_unchanged\nI, [2016-09-30T11:03:38.077214 #5458]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:38.077254 #5458]  INFO -- : Hey BOSH, call me back in 5s.\nI, [2016-09-30T11:03:43.092552 #5462]  INFO -- : Drain script invoked with job_check_status hash_unchanged\nI, [2016-09-30T11:03:43.092661 #5462]  INFO -- : Sending signal USR2 to DEA.\nI, [2016-09-30T11:03:43.092813 #5462]  INFO -- : Hey BOSH, call me back in 5s.\n```\ndea_next.log\n```\n54566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/evacuation_handler.rb\",\"lineno\":15,\"method\":\"evacuate!\"}\n{\"timestamp\":1475238152.3749087,\"message\":\"caught SIGUSR2\",\"log_level\":\"warn\",\"source\":\"Dea::Bootstrap\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/signal_handler.rb\",\"lineno\":25,\"method\":\"block (3 levels) in setup\"}\n{\"timestamp\":1475238152.3751335,\"message\":\"Evacuating (first time: false; can shutdown: true)\",\"log_level\":\"info\",\"source\":\"EvacuationHandler\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/evacuation_handler.rb\",\"lineno\":15,\"method\":\"evacuate!\"}\n{\"timestamp\":1475238157.3904114,\"message\":\"caught SIGUSR2\",\"log_level\":\"warn\",\"source\":\"Dea::Bootstrap\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/signal_handler.rb\",\"lineno\":25,\"method\":\"block (3 levels) in setup\"}\n{\"timestamp\":1475238157.3905866,\"message\":\"Evacuating (first time: false; can shutdown: true)\",\"log_level\":\"info\",\"source\":\"EvacuationHandler\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/evacuation_handler.rb\",\"lineno\":15,\"method\":\"evacuate!\"}\n{\"timestamp\":1475238162.425652,\"message\":\"caught SIGUSR2\",\"log_level\":\"warn\",\"source\":\"Dea::Bootstrap\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/signal_handler.rb\",\"lineno\":25,\"method\":\"block (3 levels) in setup\"}\n{\"timestamp\":1475238162.4258277,\"message\":\"Evacuating (first time: false; can shutdown: true)\",\"log_level\":\"info\",\"source\":\"EvacuationHandler\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/evacuation_handler.rb\",\"lineno\":15,\"method\":\"evacuate!\"}\n{\"timestamp\":1475238167.4554849,\"message\":\"caught SIGUSR2\",\"log_level\":\"warn\",\"source\":\"Dea::Bootstrap\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/signal_handler.rb\",\"lineno\":25,\"method\":\"block (3 levels) in setup\"}\n{\"timestamp\":1475238167.4556808,\"message\":\"Evacuating (first time: false; can shutdown: true)\",\"log_level\":\"info\",\"source\":\"EvacuationHandler\",\"data\":{},\"thread_id\":47305138051480,\"fiber_id\":47305154566580,\"process_id\":4601,\"file\":\"/var/vcap/packages/dea_next/lib/dea/lifecycle/evacuation_handler.rb\",\"lineno\":15,\"method\":\"evacuate!\"}\n```\n. I am getting this problem repetitively and continuously.Can you please point me where I should look in to .Is this problem specific to any cf release,bosh director or bosh cli. Is this open stack issue ? every time when i run delete deployment i am facing this issue.\n. @zaksoup If you have have anything to suggest here,the problem is reproducing in my set up.\ntried evacuation_bail_out_time_in_seconds: 115 but no luck.\nI am analysed the issue and found that dea still running in the back ground,monit is showing all job not monitored. drain is keep on sending  USR2 signal to dea.\nI have question how drain utility works,who calls this drain script.\nI want to understand dea and deletion mechanism any link will be useful.\n``\nroot      1453  1452  0 Sep30 ?        00:00:00 svlogd -tt /var/vcap/monit/svlog\nroot      1455  1452  0 Sep30 ?        00:00:26 /var/vcap/bosh/bin/monit -I -c /var/vcap/bosh/etc/monitrc\nroot      5262     1  0 05:26 ?        00:00:00 /bin/bash -exu /var/vcap/jobs/consul_agent/bin/agent_ctl start\nroot      5263     1  0 05:26 ?        00:00:00 /bin/bash -exu /var/vcap/jobs/consul_agent/bin/agent_ctl start\nroot      5264  5262  0 05:26 ?        00:00:00 tee -a /var/vcap/sys/log/consul_agent/consul_agent.stderr.log\nroot      5265  5263  0 05:26 ?        00:00:00 tee -a /var/vcap/sys/log/consul_agent/consul_agent.stdout.log\nroot      5266  5262  0 05:26 ?        00:00:00 logger -p user.error -t vcap.consul-agent\nroot      5267  5263  0 05:26 ?        00:00:00 logger -p user.info -t vcap.consul-agent\nvcap      5273     1  0 05:26 ?        00:01:25 /var/vcap/packages/consul/bin/consul agent -config-dir=/var/vcap/jobs/consul_agent/config -recursor=10.40.1.55 -recursor=10.40.1.2\nroot      5288     1  0 05:27 ?        00:00:17 /var/vcap/packages/ruby-2.2.4/bin/ruby /var/vcap/packages/ruby-2.2.4/bin/rake warden:start[/var/vcap/jobs/dea_next/config/warden.yml]\nroot      5290  5288  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/warden_ctl start\nroot      5291  5290  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5292  5288  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/warden_ctl start\nroot      5293  5290  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5294  5291  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/warden_ctl start\nroot      5295  5292  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5296  5292  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5299  5295  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/warden_ctl start\nroot      5309  5299  0 05:27 ?        00:00:00 logger -p user.error -t vcap.warden_ctl.stderr\nroot      5310  5294  0 05:27 ?        00:00:00 logger -p user.info -t vcap.warden_ctl.stdout\nroot      5465     1  0 05:27 ?        00:00:18 /var/vcap/packages/ruby-2.2.4/bin/ruby /var/vcap/packages/dea_next/bin/dea /var/vcap/jobs/dea_next/config/dea.yml\nroot      5467  5465  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dea_ctl start\nroot      5468  5467  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5469  5465  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dea_ctl start\nroot      5470  5467  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5471  5468  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dea_ctl start\nroot      5472  5469  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5473  5469  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5480  5472  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dea_ctl start\nroot      5487  5471  0 05:27 ?        00:00:00 logger -p user.info -t vcap.dea_ctl.stdout\nroot      5489  5480  0 05:27 ?        00:00:00 logger -p user.error -t vcap.dea_ctl.stderr\nroot      5507     1  0 05:27 ?        00:00:00 /var/vcap/packages/dea_next/go/bin/runner -conf /var/vcap/jobs/dea_next/config/dea.yml\nroot      5509  5507  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dir_server_ctl start\nroot      5510  5509  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5511  5507  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dir_server_ctl start\nroot      5512  5509  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5513  5510  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dir_server_ctl start\nroot      5514  5511  0 05:27 ?        00:00:00 tee -a /dev/fd/63\nroot      5515  5511  0 05:27 ?        00:00:00 awk -W Interactive {lineWithDate=\"echo [date +\\\"%Y-%m-%d %H:%M:%S%z\\\"`] \\\"\" $0 \"\\\"\"; system(lineWithDate)  }\nroot      5517  5514  0 05:27 ?        00:00:00 /bin/bash -e /var/vcap/jobs/dea_next/bin/dir_server_ctl start\nroot      5528  5513  0 05:27 ?        00:00:00 logger -p user.info -t vcap.dir_server_ctl.stdout\nroot      5530  5517  0 05:27 ?        00:00:00 logger -p user.error -t vcap.dir_server_ctl.stderr\nroot      5545     1  0 05:27 ?        00:00:02 /var/vcap/packages/dea_logging_agent/deaagent --config /var/vcap/jobs/dea_logging_agent/config/dea_logging_agent.json\nroot      5547  5545  0 05:27 ?        00:00:00 [dea_logging_age] \nroot      5549  5545  0 05:27 ?        00:00:00 [dea_logging_age] \nvcap      5587     1  0 05:27 ?        00:00:41 /var/vcap/packages/metron_agent/metron --config /var/vcap/jobs/metron_agent/config/metron_agent.json\n```\n. looks like this function is not working properly. Please help understand what this line\n@message_bus.flush { terminate } does here ???\n```\ndef flush_message_bus_and_terminate\n    @logger.info(\"All instances and staging tasks stopped, exiting.\")\n    @message_bus.flush { terminate }\n  end\n```\ndea_next.log\n```\n{\"timestamp\":1475263834.4594078,\"message\":\"Dea started\",\"log_level\":\"info\",\"source\":\"Dea::Bootstrap\",\"data\":{},\"thread_id\":47438794029460,\"fiber_id\":47438810541340,\"process_id\":4624,\"file\":\"/var/vcap/packages/dea_next/lib/dea/bootstrap.rb\",\"lineno\":135,\"method\":\"setup_logging\"}\n{\"timestamp\":1475263834.482458,\"message\":\"nats.connecting\",\"log_level\":\"info\",\"source\":\"Dea::Nats\",\"data\":{\"servers\":[\"nats://nats@0.cloud-controller.ccc-bosh-net.cf-vik-soltest1.microbosh:4222\"]},\"thread_id\":47438794029460,\"fiber_id\":47438810541340,\"process_id\":4624,\"file\":\"/var/vcap/packages/dea_next/lib/dea/nats.rb\",\"lineno\":103,\"method\":\"create_nats_client\"}\n{\"timestamp\":1475263834.4912682,\"message\":\"buildpacks-request.error\",\"log_level\":\"error\",\"source\":\"Dea::Bootstrap\",\"data\":{\"error\":\"unable to resolve server address\"},\"thread_id\":47438794029460,\"fiber_id\":47438810541340,\"process_id\":4624,\"file\":\"/var/vcap/packages/dea_next/lib/dea/bootstrap.rb\",\"lineno\":497,\"method\":\"block in download_buildpacks\"}\n```\n. Thanks @dpb587-pivotal .It worked\n. ",
    "jszroberto": "Facing same issue, is this already resolved? We need to use mandatorily dynamic networks with our IaaS provider.\n. ",
    "kitemongerer": "Loggregator completed a feature to enable dynamic networking.\n. ",
    "jagadish-kb": "@StanleyShen @cppforlife \nI tried the above and renaming breaks resurrection. Bosh resurrection no longer works if we rename the deployment with the above approach. I renamed one of the deployment from service-fabrik-backup-2107-4a6e7c34-d97c-4fc0-95e6-7a3bc8030ba4 to service-fabrik-backup-4a6e7c34-d97c-4fc0-95e6-7a3bc8030ba4. Manually deleted the VM from the horizon console & wanted resurrector to fix the deployment. I was waiting to see if there would be a scan & fix event in bosh tasks. I waited for long and didn't see any. Then i took a peek at health monitor log and below is what i found:\n[2017-12-29T11:30:17.762089 #7277]  INFO : [ALERT] Alert @ 2017-12-29 11:30:17 UTC, severity 4: Notifying Director to recreate instance: 'blueprint/b4957a8c-c6ed-4bc1-a226-e2a0dd25ff09'; deployment: 'service-fabrik-backup-4a6e7c34-d97c-4fc0-95e6-7a3bc8030ba4'; 1 of 1 agents are unhealthy (100.0%)\n[2017-12-29T11:30:17.762220 #7277]  INFO : (Event logger) notifying director about event: Alert @ 2017-12-29 11:30:17 UTC, severity 4: Notifying Director to recreate instance: 'blueprint/b4957a8c-c6ed-4bc1-a226-e2a0dd25ff09'; deployment: 'service-fabrik-backup-4a6e7c34-d97c-4fc0-95e6-7a3bc8030ba4'; 1 of 1 agents are unhealthy (100.0%)\n[2017-12-29T11:30:17.762509 #7277]  WARN : (Resurrector) event did not have deployment, job and id: Alert @ 2017-12-29 11:30:17 UTC, severity 4: Notifying Director to recreate instance: 'blueprint/b4957a8c-c6ed-4bc1-a226-e2a0dd25ff09'; deployment: 'service-fabrik-backup-4a6e7c34-d97c-4fc0-95e6-7a3bc8030ba4'; 1 of 1 agents are unhealthy (100.0%)\nSo this is certainly a dangerous operation to perform and i guess even the above option should not be recommended. Is there any other work around to make even resurrection work post this renaming ? . @cppforlife - Bosh resurrection worked. What had happened was since I had deleted the VM manually from Openstack console, the neutron port was still present and bosh resurrection had failed as it reported that the IP was still in use. I manually deleted even the neutron port and resurrection was able to fix the deployment. \nSo then is it safe to rename deployments? Are there any other tests/gotchas that i need to check before i can start doing this ? Would appreciate your recommendations here.. ",
    "holgerkoser": "@allomov Thanks a lot for your assistance. Your PR looks reasonable to me.\n. ",
    "Infra-Red": "Git diff between 255.8 and 255.9:\ngit diff 8f47e6a7 47cafe56 :\ndiff --git a/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb b/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb\ndeleted file mode 100644\nindex 4d2aa2d..0000000\n--- a/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-require 'json'\n-\n-Sequel.migration do\n-  up do\n-    self[:instances].all do |instance|\n-      unless instance[:spec_json].nil?\n-        spec = JSON.parse(instance[:spec_json])\n-        if !spec['update'].nil? && spec['update'].has_key?('serial') && spec['update']['serial'].nil?\n-          spec['update']['serial'] = false\n-          self[:instances].where(id: instance[:id]).update(spec_json: JSON.generate(spec))\n-        end\n-      end\n-    end\n-  end\n-end\ndiff --git a/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb b/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb\ndeleted file mode 100644\nindex 2e7dd8f..0000000\n--- a/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-Sequel.migration do\n-  up do\n-    self[:stemcells].all do |stemcell|\n-      if stemcell[:operating_system].nil? || stemcell[:operating_system] == ''\n-        self[:stemcells].where(id: stemcell[:id]).update(operating_system: stemcell[:name])\n-      end\n-    end\n-  end\n-end\n. Git diff between 255.8 and 255.9:\ngit diff 8f47e6a7 47cafe56 :\ndiff --git a/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb b/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb\ndeleted file mode 100644\nindex 4d2aa2d..0000000\n--- a/bosh-director/db/migrations/director/20160329201256_set_instances_with_nil_serial_to_false.rb\n+++ /dev/null\n@@ -1,15 +0,0 @@\n-require 'json'\n-\n-Sequel.migration do\n-  up do\n-    self[:instances].all do |instance|\n-      unless instance[:spec_json].nil?\n-        spec = JSON.parse(instance[:spec_json])\n-        if !spec['update'].nil? && spec['update'].has_key?('serial') && spec['update']['serial'].nil?\n-          spec['update']['serial'] = false\n-          self[:instances].where(id: instance[:id]).update(spec_json: JSON.generate(spec))\n-        end\n-      end\n-    end\n-  end\n-end\ndiff --git a/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb b/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb\ndeleted file mode 100644\nindex 2e7dd8f..0000000\n--- a/bosh-director/db/migrations/director/20160331225404_backfill_stemcell_os.rb\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-Sequel.migration do\n-  up do\n-    self[:stemcells].all do |stemcell|\n-      if stemcell[:operating_system].nil? || stemcell[:operating_system] == ''\n-        self[:stemcells].where(id: stemcell[:id]).update(operating_system: stemcell[:name])\n-      end\n-    end\n-  end\n-end\n. 255.10 works fine for me too. @cppforlife @allomov thx for quick response.\n. 255.10 works fine for me too. @cppforlife @allomov thx for quick response.\n. ",
    "medvedzver": "Hi!\nCould you please make a pull request as a single commit?\nThanks!\n. Had to revert commit. Mysql unit tests are failing because mysql can't store fractional datetime.\nThis is the version of mysql we are using \nmysql  Ver 14.14 Distrib 5.5.49, for debian-linux-gnu (x86_64) using readline 6.3\nhttps://dev.mysql.com/doc/refman/5.5/en/fractional-seconds.html\nHowever, when MySQL stores a value into a column of any temporal data type, it discards any fractional part and does not store it.\n. Test fails on Ruby 1.9.3:\nhttps://main.bosh-ci.cf-app.com/pipelines/bosh/jobs/unit-1.9/builds/1020\nLooks like the fix for this would be to add \"# encoding: utf-8\" at the beginning of a cli_spec.rb.\nBut then, this test fails with different error:\nhttps://main.bosh-ci.cf-app.com/builds/131315\n. May we use && instead of and?\nThere is a difference: http://stackoverflow.com/questions/1426826/difference-between-and-and-in-ruby\n. same here\n. @cppforlife How do you know that director supports it? . ",
    "joshnn": "Hi, what I see on my end with bosh instances --ps is:\nconsul_z2/0 (-b362-4b91-9f31-463bd0206de2)*                     | failing | n/a | small_z2  | 10.10.80.37 |\n|   consul_agent                                                             | unknown |     |           |            \n|   metron_agent                                                             | running |     |           |\nWhat I found in consul_agent/consul_agent.stdout.log is agent repeating Graceful leave with an interval of about 60 seconds that end up error 400007. \nPrior to each leave, I see repetitive \"log not in sync\" errors that I do not quite understand how harmful this error would be. If it really is, how can I resolve this out-of-sync situation????\n......\n{\"timestamp\":\"1461944646.744561434\",\"source\":\"confab\",\"message\":\"confab.agent-client.verify-synced.stats.request\",\"log_level\":1,\"data\":{}}\n{\"timestamp\":\"1461944646.745200396\",\"source\":\"confab\",\"message\":\"confab.agent-client.verify-synced.stats.response\",\"log_level\":1,\"data\":{\"commit_index\":\"0\",\"last_log_index\":\"4360\"}}\n{\"timestamp\":\"1461944646.745225668\",\"source\":\"confab\",\"message\":\"confab.agent-client.verify-synced.not-synced\",\"log_level\":2,\"data\":{\"error\":\"log not in sync\"}}\n.....\n{\"timestamp\":\"1461944588.653216839\",\"source\":\"confab\",\"message\":\"confab.controller.configure-server.verify-synced.failed\",\"log_level\":2,\"data\":{\"error\":\"timeout exceeded\"}}\n. ",
    "edwardstudy": "@jbayer It's work for me. But did we have any root cause of this consul agent failed?. @cppforlife Thank you. I asked about the limitations of the SoftLayer tags. The characters of tags string permitted are A-Z, 0-9, whitespace, _ (underscore), - (hypen), . (period), and : (colon), therefore It is not able to use tag names with other characters than the permitted. We are developing CPI so I do not think you need to open an issue again. If the truncation is taken into account, I think that we should improve the softlayer cpi.\nBest regard.\n. @pivotal-jamil-shamy Thank you. I saw https://github.com/cloudfoundry/bosh-notes/blob/master/registry-removal.md in bosh-notes. . Hi, Softlayer meet the same issue on Xenial stemcell. Agent set gpt, but it changed to msdos. I didn't know who changed it?. Hi, @jfmyers9 and @aduffeck \nAbout commit https://github.com/SUSE/bosh-agent/commit/d91a92df6c6d7360af1a456a7d545628bdce837a , do we have a plan to to accept and release it?. @freddesbiens. That's great to remove registry. Can you tell me where bosh-agent to get its settings from without a registry?\nThanks.. ",
    "Niranjana-5588": "@cppforlife Thank you for your reply. I used bosh cck command to recreate vms as shown below. \n```\n[ec2-user@ip-10-0-1-139 deployments]$ bosh cck\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nPerforming cloud check...\nDirector task 246\n  Started scanning 13 vms\n  Started scanning 13 vms > Checking VM states. Done (00:00:11)\n  Started scanning 13 vms > 1 OK, 12 unresponsive, 0 missing, 0 unbound. Done (00:00:00)\n     Done scanning 13 vms (00:00:11)\nStarted scanning 5 persistent disks\n  Started scanning 5 persistent disks > Looking for inactive disks. Done (00:00:00)\n  Started scanning 5 persistent disks > 5 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 5 persistent disks (00:00:00)\nTask 246 done\nStarted         2016-04-21 05:26:36 UTC\nFinished        2016-04-21 05:26:47 UTC\nDuration        00:00:11\nScan is complete, checking if any problems found...\nFound 12 problems\nProblem 1 of 12: etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c) (21b47063-511d-4f9e-4af3-b3c5a8fa8861) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 2 of 12: api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53) (d569b894-7bca-4268-7ce8-e599dec10382) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 3 of 12: consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a) (879d297d-737d-4d76-4282-b6289b79b2a6) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 4 of 12: uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac) (6ded20bf-b960-4044-5edd-802909bbcf5a) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 5 of 12: runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7) (8de415c9-0e79-495d-4477-731556698dc8) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 6 of 12: hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9) (9f031d2a-ce48-4703-5dd1-4f7911e73ff5) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 7 of 12: doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e) (90fb2a07-5560-4db3-663c-9a6322bd7ea7) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 8 of 12: loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) (04579489-4a87-4fd0-5918-65988c86ce37) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 9 of 12: blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7) (2ab46718-74eb-41b3-5771-3488cf156759) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 10 of 12: router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe) (7b265f91-39fd-4b10-49f3-fc498e8617fb) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 11 of 12: postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6) (569234c5-7fb3-45d2-730e-876cc1b723fd) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 12 of 12: ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619) (8f69cd8d-3e5c-4ca6-607d-e2ce689e066a) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nBelow is the list of resolutions you've provided\nPlease make sure everything is fine and confirm your changes\n\n\netcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c) (21b47063-511d-4f9e-4af3-b3c5a8fa8861) is not responding\n     Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'\n\n\napi_z1/0 (126c3764-8d28-4126-99f2-be607e583b53) (d569b894-7bca-4268-7ce8-e599dec10382) is not responding\n     Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'\n\n\nconsul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a) (879d297d-737d-4d76-4282-b6289b79b2a6) is not responding\n     Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'\n\n\nuaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac) (6ded20bf-b960-4044-5edd-802909bbcf5a) is not responding\n     Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'\n\n\nrunner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7) (8de415c9-0e79-495d-4477-731556698dc8) is not responding\n     Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'\n\n\nhm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9) (9f031d2a-ce48-4703-5dd1-4f7911e73ff5) is not responding\n     Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'\n\n\ndoppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e) (90fb2a07-5560-4db3-663c-9a6322bd7ea7) is not responding\n     Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'\n\n\nloggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) (04579489-4a87-4fd0-5918-65988c86ce37) is not responding\n     Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'\n\n\nblobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7) (2ab46718-74eb-41b3-5771-3488cf156759) is not responding\n     Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'\n\n\nrouter_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe) (7b265f91-39fd-4b10-49f3-fc498e8617fb) is not responding\n      Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'\n\n\npostgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6) (569234c5-7fb3-45d2-730e-876cc1b723fd) is not responding\n      Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'\n\n\nha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619) (8f69cd8d-3e5c-4ca6-607d-e2ce689e066a) is not responding\n      Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'\n\n\nApply resolutions? (type 'yes' to continue): yes\nApplying resolutions...\nDirector task 247\n  Started applying problem resolutions\n  Started applying problem resolutions > unresponsive_agent 40: Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'. Done (00:04:03)\n  Started applying problem resolutions > unresponsive_agent 44: Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'. Failed: `api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)' is not running after update. Review logs for failed jobs: consul_agent, cloud_controller_ng, cloud_controller_worker_local_1, cloud_controller_worker_local_2, nginx_cc, cloud_controller_migration, cloud_controller_clock, cloud_controller_worker_1 (00:34:20)\n  Started applying problem resolutions > unresponsive_agent 37: Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'. Done (00:04:05)\n  Started applying problem resolutions > unresponsive_agent 43: Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'. Failed: Action Failed get_task: Task 59d08be6-7de9-47df-4524-b40c146fbd36 result: 1 of 1 post-start scripts failed. Failed Jobs: uaa. (00:19:26)\n  Started applying problem resolutions > unresponsive_agent 46: Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'. Done (00:06:44)\n  Started applying problem resolutions > unresponsive_agent 45: Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'. Done (00:04:27)\n  Started applying problem resolutions > unresponsive_agent 47: Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'. Done (00:00:43)\n  Started applying problem resolutions > unresponsive_agent 48: Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'. Done (00:01:08)\n  Started applying problem resolutions > unresponsive_agent 41: Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'. Done (00:04:27)\n  Started applying problem resolutions > unresponsive_agent 49: Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'. Done (00:04:47)\n  Started applying problem resolutions > unresponsive_agent 42: Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'. Done (00:04:42)\n  Started applying problem resolutions > unresponsive_agent 38: Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'. Done (00:04:42)\n   Failed applying problem resolutions (01:33:34)\nError 100: Error resolving problem 24':api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)' is not running after update. Review logs for failed jobs: consul_agent, cloud_controller_ng, cloud_controller_worker_local_1, cloud_controller_worker_local_2, nginx_cc, cloud_controller_migration, cloud_controller_clock, cloud_controller_worker_1\nError resolving problem `26': Action Failed get_task: Task 59d08be6-7de9-47df-4524-b40c146fbd36 result: 1 of 1 post-start scripts failed. Failed Jobs: uaa.\nTask 247 error\nFor a more detailed error report, run: bosh task 247 --debug\n```\nIf you see above logs from terminal two vms failed to run. Failed vms are api_z1/0 and uaa_z1/0.\nVms states like this\n```\n[ec2-user@ip-10-0-1-139 ~]$ bosh vms cf-warden\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nDirector task 265\nTask 265 done\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n| VM                                                                        | State   | AZ  | VM Type   | IPs          |\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n| api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)                           | failing | n/a | large_z1  | 10.244.0.138 |\n| blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)                     | running | n/a | medium_z1 | 10.244.0.130 |\n| consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)                        | running | n/a | small_z1  | 10.244.0.54  |\n| doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)                       | running | n/a | medium_z1 | 10.244.0.146 |\n| etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)                          | running | n/a | medium_z1 | 10.244.0.42  |\n| ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)                      | running | n/a | router_z1 | 10.244.0.34  |\n| hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)                        | running | n/a | medium_z1 | 10.244.0.142 |\n| loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) | running | n/a | small_z1  | 10.244.0.150 |\n| nats_z1/0 (2f77a0ad-13c3-4794-a73d-266320e1532b)                          | running | n/a | medium_z1 | 10.244.0.6   |\n| postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)                      | running | n/a | medium_z1 | 10.244.0.30  |\n| router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)                        | running | n/a | router_z1 | 10.244.0.22  |\n| runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)                        | running | n/a | runner_z1 | 10.244.0.26  |\n| uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)                           | failing | n/a | medium_z1 | 10.244.0.134 |\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n```\nI reran the same command bosh cck and response is as below \n```\n[ec2-user@ip-10-0-1-139 ~]$ bosh cck\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nPerforming cloud check...\nDirector task 266\n  Started scanning 13 vms\n  Started scanning 13 vms > Checking VM states. Done (00:00:01)\n  Started scanning 13 vms > 13 OK, 0 unresponsive, 0 missing, 0 unbound. Done (00:00:00)\n     Done scanning 13 vms (00:00:01)\nStarted scanning 5 persistent disks\n  Started scanning 5 persistent disks > Looking for inactive disks. Done (00:00:01)\n  Started scanning 5 persistent disks > 5 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 5 persistent disks (00:00:01)\nTask 266 done\nStarted         2016-04-21 07:05:01 UTC\nFinished        2016-04-21 07:05:03 UTC\nDuration        00:00:02\nScan is complete, checking if any problems found...\nNo problems found\n[ec2-user@ip-10-0-1-139 ~]$\n```\nHow to fix this ? Attached log file to refer.\nTask 247 debug.zip\n. @cppforlife Thank you for your reply. I used bosh cck command to recreate vms as shown below. \n```\n[ec2-user@ip-10-0-1-139 deployments]$ bosh cck\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nPerforming cloud check...\nDirector task 246\n  Started scanning 13 vms\n  Started scanning 13 vms > Checking VM states. Done (00:00:11)\n  Started scanning 13 vms > 1 OK, 12 unresponsive, 0 missing, 0 unbound. Done (00:00:00)\n     Done scanning 13 vms (00:00:11)\nStarted scanning 5 persistent disks\n  Started scanning 5 persistent disks > Looking for inactive disks. Done (00:00:00)\n  Started scanning 5 persistent disks > 5 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 5 persistent disks (00:00:00)\nTask 246 done\nStarted         2016-04-21 05:26:36 UTC\nFinished        2016-04-21 05:26:47 UTC\nDuration        00:00:11\nScan is complete, checking if any problems found...\nFound 12 problems\nProblem 1 of 12: etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c) (21b47063-511d-4f9e-4af3-b3c5a8fa8861) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 2 of 12: api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53) (d569b894-7bca-4268-7ce8-e599dec10382) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 3 of 12: consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a) (879d297d-737d-4d76-4282-b6289b79b2a6) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 4 of 12: uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac) (6ded20bf-b960-4044-5edd-802909bbcf5a) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 5 of 12: runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7) (8de415c9-0e79-495d-4477-731556698dc8) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 6 of 12: hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9) (9f031d2a-ce48-4703-5dd1-4f7911e73ff5) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 7 of 12: doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e) (90fb2a07-5560-4db3-663c-9a6322bd7ea7) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 8 of 12: loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) (04579489-4a87-4fd0-5918-65988c86ce37) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 9 of 12: blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7) (2ab46718-74eb-41b3-5771-3488cf156759) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 10 of 12: router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe) (7b265f91-39fd-4b10-49f3-fc498e8617fb) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 11 of 12: postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6) (569234c5-7fb3-45d2-730e-876cc1b723fd) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nProblem 12 of 12: ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619) (8f69cd8d-3e5c-4ca6-607d-e2ce689e066a) is not responding.\n  1. Skip for now\n  2. Reboot VM\n  3. Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'\n  4. Delete VM reference (forceful; may need to manually delete VM from the Cloud to avoid IP conflicts)\nPlease choose a resolution [1 - 4]: 3\nBelow is the list of resolutions you've provided\nPlease make sure everything is fine and confirm your changes\n\n\netcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c) (21b47063-511d-4f9e-4af3-b3c5a8fa8861) is not responding\n     Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'\n\n\napi_z1/0 (126c3764-8d28-4126-99f2-be607e583b53) (d569b894-7bca-4268-7ce8-e599dec10382) is not responding\n     Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'\n\n\nconsul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a) (879d297d-737d-4d76-4282-b6289b79b2a6) is not responding\n     Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'\n\n\nuaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac) (6ded20bf-b960-4044-5edd-802909bbcf5a) is not responding\n     Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'\n\n\nrunner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7) (8de415c9-0e79-495d-4477-731556698dc8) is not responding\n     Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'\n\n\nhm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9) (9f031d2a-ce48-4703-5dd1-4f7911e73ff5) is not responding\n     Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'\n\n\ndoppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e) (90fb2a07-5560-4db3-663c-9a6322bd7ea7) is not responding\n     Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'\n\n\nloggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) (04579489-4a87-4fd0-5918-65988c86ce37) is not responding\n     Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'\n\n\nblobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7) (2ab46718-74eb-41b3-5771-3488cf156759) is not responding\n     Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'\n\n\nrouter_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe) (7b265f91-39fd-4b10-49f3-fc498e8617fb) is not responding\n      Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'\n\n\npostgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6) (569234c5-7fb3-45d2-730e-876cc1b723fd) is not responding\n      Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'\n\n\nha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619) (8f69cd8d-3e5c-4ca6-607d-e2ce689e066a) is not responding\n      Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'\n\n\nApply resolutions? (type 'yes' to continue): yes\nApplying resolutions...\nDirector task 247\n  Started applying problem resolutions\n  Started applying problem resolutions > unresponsive_agent 40: Recreate VM for 'etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)'. Done (00:04:03)\n  Started applying problem resolutions > unresponsive_agent 44: Recreate VM for 'api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)'. Failed: `api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)' is not running after update. Review logs for failed jobs: consul_agent, cloud_controller_ng, cloud_controller_worker_local_1, cloud_controller_worker_local_2, nginx_cc, cloud_controller_migration, cloud_controller_clock, cloud_controller_worker_1 (00:34:20)\n  Started applying problem resolutions > unresponsive_agent 37: Recreate VM for 'consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)'. Done (00:04:05)\n  Started applying problem resolutions > unresponsive_agent 43: Recreate VM for 'uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)'. Failed: Action Failed get_task: Task 59d08be6-7de9-47df-4524-b40c146fbd36 result: 1 of 1 post-start scripts failed. Failed Jobs: uaa. (00:19:26)\n  Started applying problem resolutions > unresponsive_agent 46: Recreate VM for 'runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)'. Done (00:06:44)\n  Started applying problem resolutions > unresponsive_agent 45: Recreate VM for 'hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)'. Done (00:04:27)\n  Started applying problem resolutions > unresponsive_agent 47: Recreate VM for 'doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)'. Done (00:00:43)\n  Started applying problem resolutions > unresponsive_agent 48: Recreate VM for 'loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170)'. Done (00:01:08)\n  Started applying problem resolutions > unresponsive_agent 41: Recreate VM for 'blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)'. Done (00:04:27)\n  Started applying problem resolutions > unresponsive_agent 49: Recreate VM for 'router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)'. Done (00:04:47)\n  Started applying problem resolutions > unresponsive_agent 42: Recreate VM for 'postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)'. Done (00:04:42)\n  Started applying problem resolutions > unresponsive_agent 38: Recreate VM for 'ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)'. Done (00:04:42)\n   Failed applying problem resolutions (01:33:34)\nError 100: Error resolving problem 24':api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)' is not running after update. Review logs for failed jobs: consul_agent, cloud_controller_ng, cloud_controller_worker_local_1, cloud_controller_worker_local_2, nginx_cc, cloud_controller_migration, cloud_controller_clock, cloud_controller_worker_1\nError resolving problem `26': Action Failed get_task: Task 59d08be6-7de9-47df-4524-b40c146fbd36 result: 1 of 1 post-start scripts failed. Failed Jobs: uaa.\nTask 247 error\nFor a more detailed error report, run: bosh task 247 --debug\n```\nIf you see above logs from terminal two vms failed to run. Failed vms are api_z1/0 and uaa_z1/0.\nVms states like this\n```\n[ec2-user@ip-10-0-1-139 ~]$ bosh vms cf-warden\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nDirector task 265\nTask 265 done\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n| VM                                                                        | State   | AZ  | VM Type   | IPs          |\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n| api_z1/0 (126c3764-8d28-4126-99f2-be607e583b53)                           | failing | n/a | large_z1  | 10.244.0.138 |\n| blobstore_z1/0 (707ba852-f09f-4ba5-8cee-a213f90763c7)                     | running | n/a | medium_z1 | 10.244.0.130 |\n| consul_z1/0 (e817a194-77ca-4093-a78d-1f9b74e2a46a)                        | running | n/a | small_z1  | 10.244.0.54  |\n| doppler_z1/0 (a5a39455-b195-4535-9b59-def3a331956e)                       | running | n/a | medium_z1 | 10.244.0.146 |\n| etcd_z1/0 (5a23f8a6-23f9-436e-8739-b05a2573b75c)                          | running | n/a | medium_z1 | 10.244.0.42  |\n| ha_proxy_z1/0 (8cce17f6-62b5-4f0d-a1bc-868210d56619)                      | running | n/a | router_z1 | 10.244.0.34  |\n| hm9000_z1/0 (72a3d178-0682-43d2-98eb-a9ef28f73ed9)                        | running | n/a | medium_z1 | 10.244.0.142 |\n| loggregator_trafficcontroller_z1/0 (fcfa72db-5298-446f-9404-e65902bef170) | running | n/a | small_z1  | 10.244.0.150 |\n| nats_z1/0 (2f77a0ad-13c3-4794-a73d-266320e1532b)                          | running | n/a | medium_z1 | 10.244.0.6   |\n| postgres_z1/0 (7df5c59d-8ae9-4736-bc71-f48239059cc6)                      | running | n/a | medium_z1 | 10.244.0.30  |\n| router_z1/0 (e31d8f10-7642-46e4-b128-e4a2db4aa4fe)                        | running | n/a | router_z1 | 10.244.0.22  |\n| runner_z1/0 (0dd01d57-b510-4a5a-b6df-c0abce335aa7)                        | running | n/a | runner_z1 | 10.244.0.26  |\n| uaa_z1/0 (b6622b43-ed0f-4956-8086-dfaeb61079ac)                           | failing | n/a | medium_z1 | 10.244.0.134 |\n+---------------------------------------------------------------------------+---------+-----+-----------+--------------+\n```\nI reran the same command bosh cck and response is as below \n```\n[ec2-user@ip-10-0-1-139 ~]$ bosh cck\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'cf-warden' on 'Bosh Lite Director'\nPerforming cloud check...\nDirector task 266\n  Started scanning 13 vms\n  Started scanning 13 vms > Checking VM states. Done (00:00:01)\n  Started scanning 13 vms > 13 OK, 0 unresponsive, 0 missing, 0 unbound. Done (00:00:00)\n     Done scanning 13 vms (00:00:01)\nStarted scanning 5 persistent disks\n  Started scanning 5 persistent disks > Looking for inactive disks. Done (00:00:01)\n  Started scanning 5 persistent disks > 5 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)\n     Done scanning 5 persistent disks (00:00:01)\nTask 266 done\nStarted         2016-04-21 07:05:01 UTC\nFinished        2016-04-21 07:05:03 UTC\nDuration        00:00:02\nScan is complete, checking if any problems found...\nNo problems found\n[ec2-user@ip-10-0-1-139 ~]$\n```\nHow to fix this ? Attached log file to refer.\nTask 247 debug.zip\n. @cppforlife This logs are useful to know why jobs failed. But the main issue is after restarting instance all vms should be in running state not \"unresponsive agent\" state. Came to know the steps to achieve this. First, all bosh vms must be stopped then shut down the instance. When instance started, vms in stopped state which we can start again using bosh start <job name>. Followed steps as below mentioned.\n1. Stopped all vms using command bosh stop <job name>. All jobs went to 'stopped\" state.\n2. Restarted the ec2 instance.\n3. List out vms using command bosh vms cf-warden\n4. All vms are in \"unresponsive agent\" state again. (Expected, vms should be in \"stopped\" state as we stopped all vms before shutting down instance)\nIs there any other way to do make bosh vms up and running automatically as running behind log files every time when instance restarted is not seems good solution.\n. @cppforlife This logs are useful to know why jobs failed. But the main issue is after restarting instance all vms should be in running state not \"unresponsive agent\" state. Came to know the steps to achieve this. First, all bosh vms must be stopped then shut down the instance. When instance started, vms in stopped state which we can start again using bosh start <job name>. Followed steps as below mentioned.\n1. Stopped all vms using command bosh stop <job name>. All jobs went to 'stopped\" state.\n2. Restarted the ec2 instance.\n3. List out vms using command bosh vms cf-warden\n4. All vms are in \"unresponsive agent\" state again. (Expected, vms should be in \"stopped\" state as we stopped all vms before shutting down instance)\nIs there any other way to do make bosh vms up and running automatically as running behind log files every time when instance restarted is not seems good solution.\n. any one know how to fix this ?\n. any one know how to fix this ?\n. @bosh-cpi-robot Thank you for your reply. I tried with above commands. Deleting vms and restart vms are time consuming tasks(bosh vms took 1 hour and MySQL vms took  1 hour to start using bosh start). Deployed CF has 13 vms and MySQL has 7 vms and lets consider CF has 2 more third party software services deployed with it. stop, deleting and start all these vms is time consuming tasks. Is there any better way to do this. Is it possible to reduce execution time by providing more resource like cpu cores or reducing number of bosh and MySQL vms ?\n. @bosh-cpi-robot Thank you for your reply. I tried with above commands. Deleting vms and restart vms are time consuming tasks(bosh vms took 1 hour and MySQL vms took  1 hour to start using bosh start). Deployed CF has 13 vms and MySQL has 7 vms and lets consider CF has 2 more third party software services deployed with it. stop, deleting and start all these vms is time consuming tasks. Is there any better way to do this. Is it possible to reduce execution time by providing more resource like cpu cores or reducing number of bosh and MySQL vms ?\n. Any solutions or suggestion \n. Any solutions or suggestion \n. @cppforlife I'm using bosh-lite \n. @cppforlife I'm using bosh-lite \n. Any solutions or suggestions?\n. Any solutions or suggestions?\n. I don't belongs to cloudfoundry.slack.com team. Please add me to this team. It will be helpful to post this issue in #release-integration channel.\n. I don't belongs to cloudfoundry.slack.com team. Please add me to this team. It will be helpful to post this issue in #release-integration channel.\n. ",
    "michaelgrifalconi": "Hello @cppforlife \nI would like to contribute on this topic allowing bosh-init to call a drain script and put bosh in a state where it does not accept new deployments.\nThis would make bosh update more safe, as we can stop concourse pipelines but we cannot stop service brokers to request new services (creating new deployments) while the bosh update is ongoing.\nWhat do you think? Is there something I should keep in mind? \nThanks a lot,\nMichael\n. You mean that if I delete a certain deployment postgres will do some kind of CASCADE delete and take with him all related info? \nIf not, what is the advantage of finding all info to delete or finding all info to extract?\nThe reason is we started with 'one bosh for everything' (infrastructure deployment and on-demand cloud foundry services) and now we would like to have one bosh only for infrastructure deployment and one only for services created by our service broker but we need to move out all services from the first bosh to the second.. Yes, but not in production due to shift of priorities.\nThis is what was done with a starting point of one bosh director with deployments X and Y: \n- deploy a second bosh\n- clone disk and database of the first, attach them to the new one to have 2 copies of the same director\n- from one DB delete deployment X, from the other DB delete deployment Y (*)\n- redeploy Y with the new target to 'tell' Y to send/fetch data from the second director\n(*) draft of the database query (NOTE: logs/events from deleted deployments remain there, did not spent effort to delete them. Also keep in mind the query  is from serveral weeks ago, db structure might have changed) \nbest,\nMichael. Unfortunately not that I am aware of :(. ",
    "tinygrasshopper": "Thanks for reply. So I guess this issue will be solved when https://www.pivotaltracker.com/n/projects/956238/stories/115056593 is complete.\n. This behaviour has one more unintended side effect, one of updates might be lost when multiple users are operating on the same deployment. \nConsider the scenario:\n- No workers is available on the bosh director. \n- User 1 starts a deployment, modifying a important flag, will get the output. \n```\n  Acting as client 'admin' on deployment 'empty_box_deployment' on 'bosh'\n  Getting deployment properties from director...\nDetecting deployment changes\n\nproperties:\n  really_important_flag: \"\"\nDeploying\n\nDirector task 89\n  ```\n- User 2 starts a deployment, modifying another flag, will get the output. \n```\n  Acting as client 'admin' on deployment 'empty_box_deployment' on 'faster-bosh'\n  Getting deployment properties from director...\nDetecting deployment changes\n\nproperties:\n    another_flag: \"\"\nDeploying\n\n```\nNote that User 2 didn't see that really_important_flag was being overriden \n- A worker becomes available and processes both the deploy tasks\n- Both the users see success messages, but user1's changes are lost\nAlso, the above scenario would have a totally different outcome if more than one workers were available.\nI think we can stop issues of this ilk by acquiring a lock on the deployment, when it is received for processing, rather than when its picked up processing, @cppforlife what do you think?\n. @dpb587-pivotal I think #1369 is a different issue, it refers to the old cli in which the you could not upload a blank cloud config (which I was under the impression was not maintained anymore). I am referring to the new CLI in which you can, but it breaks the director with the error I have documented above. . ",
    "jleavers": "@Niranjana-5588 You can sign up here: https://slack.cloudfoundry.org/\n. @Niranjana-5588 You can sign up here: https://slack.cloudfoundry.org/\n. Perhaps these GDS utilities would help here -\nhttp://gds-operations.github.io/vcloud-tools/\nOn 15 September 2016 at 11:56:13, Pierre Oblin (notifications@github.com)\nwrote:\nUnfortunatly, no easy way.\nThe vcloud API is restfull, but very complex to use (SOAP payload).\nI guess you cloud try creating a simple ruby / fog script (ie: create\nindependent disk, attach to an exisiting vm)...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1436#issuecomment-247297415,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ALTbiF4LzCkXZ6BapcxEjzRZGCFrxSs7ks5qqSRNgaJpZM4J7lwY\n.\n. Perhaps these GDS utilities would help here -\nhttp://gds-operations.github.io/vcloud-tools/\nOn 15 September 2016 at 11:56:13, Pierre Oblin (notifications@github.com)\nwrote:\nUnfortunatly, no easy way.\nThe vcloud API is restfull, but very complex to use (SOAP payload).\nI guess you cloud try creating a simple ruby / fog script (ie: create\nindependent disk, attach to an exisiting vm)...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1436#issuecomment-247297415,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ALTbiF4LzCkXZ6BapcxEjzRZGCFrxSs7ks5qqSRNgaJpZM4J7lwY\n.\n. Bosh will need to resolve the hostname of your ESXi host (and connect to it directly) - you can pass in DNS servers with your create-env, e.g.\nbosh create-env bosh-deployment/bosh.yml \\\n    -o bosh-deployment/misc/dns.yml \\\n    --state=state.json \\\n    --vars-store=creds.yml \\\n    -o bosh-deployment/vsphere/cpi.yml \\\n    -v internal_dns=[x.x.x.x,x.x.x.x] \\\n    etc.. Bosh will need to resolve the hostname of your ESXi host (and connect to it directly) - you can pass in DNS servers with your create-env, e.g.\nbosh create-env bosh-deployment/bosh.yml \\\n    -o bosh-deployment/misc/dns.yml \\\n    --state=state.json \\\n    --vars-store=creds.yml \\\n    -o bosh-deployment/vsphere/cpi.yml \\\n    -v internal_dns=[x.x.x.x,x.x.x.x] \\\n    etc.. ",
    "robdimsdale": "Hi @Niranjana-5588 . It looks like your stub is incomplete -- for example, you're missing a section in your stub for networks. The best place to start is with these docs on creating a stub for AWS. If you have any further issues or questions, please open a github issue on cf-release.\n@ljfranklin, feel free to close this issue.\n. ",
    "jiangytcn": "@cppforlife we are using 3192 with microbosh, for now each job instance generated two dns records: \n<instance-index>.<job-name>.<network-name>.<deployment-name>.<bosh-dns-domain>\n<UUID>.<network-name>.<deployment-name>.<bosh-dns-domain>\nand can you tell me in which bosh version involved such UUID dns record, and why generate such record, is there any way to disable generating such dns record but only left <instance-index>.<job-name>.XXXX. thank you so much :)\n. +1 with same issue \n. ",
    "swetharepakula": "@dpb587-pivotal,\nThe issue is not with the resource limit as we can reproduce this issue with jobs that do nothing but sleep as well. Also if you do a monit reload on the vm itself, it is able to restart all the processes properly. As a side note on another machine we had to use a much larger number of processes to experience the same behavior so it seems like it might be some timing issue.\nHere are the logs: https://gist.github.com/DanLavine/79eb6bfb3bc631ccaac141dddb2c64da. I didn't scrub the passwords as it is bosh-lite on my local machine.\n@swetharepakula & @danlavine\nRuntimeOG Team\n. @dpb587-pivotal,\nThe issue is not with the resource limit as we can reproduce this issue with jobs that do nothing but sleep as well. Also if you do a monit reload on the vm itself, it is able to restart all the processes properly. As a side note on another machine we had to use a much larger number of processes to experience the same behavior so it seems like it might be some timing issue.\nHere are the logs: https://gist.github.com/DanLavine/79eb6bfb3bc631ccaac141dddb2c64da. I didn't scrub the passwords as it is bosh-lite on my local machine.\n@swetharepakula & @danlavine\nRuntimeOG Team\n. No, we unfortunately did not find a solution.  We just reduced the number\nof monit processes on the job\nOn May 7, 2016 9:36 PM, \"Matt Cui\" notifications@github.com wrote:\n\n@swetharepakula https://github.com/swetharepakula how is going on with\nthis issue? Have you already found a solution or workaround? Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1245#issuecomment-217690861\n. No, we unfortunately did not find a solution.  We just reduced the number\nof monit processes on the job\nOn May 7, 2016 9:36 PM, \"Matt Cui\" notifications@github.com wrote:\n@swetharepakula https://github.com/swetharepakula how is going on with\nthis issue? Have you already found a solution or workaround? Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1245#issuecomment-217690861\n. @mattcui We did not find a consistent number of processes that triggered the problem. On one job about 10 processes was enough to cause the issue. On another we had to increase the processes significantly to recreate the problem.\n\n@jianqiu We used the bosh stemcell 3147 (bosh-warden-boshlite-ubuntu-trusty-go_agent).\n. @mattcui We did not find a consistent number of processes that triggered the problem. On one job about 10 processes was enough to cause the issue. On another we had to increase the processes significantly to recreate the problem.\n@jianqiu We used the bosh stemcell 3147 (bosh-warden-boshlite-ubuntu-trusty-go_agent).\n. ",
    "combor": "It's still a problem on bosh 255.8 version and CPI 52\nPart of the debug log:\nI, [2016-06-09 12:58:23 #23532] []  INFO -- DirectorJobRunner: Force deleting is set, ignoring exception\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004431s) SELECT NULL\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004455s) BEGIN\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.005101s) SELECT NULL\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.003337s) BEGIN\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.002227s) SELECT * FROM \"ip_addresses\" WHERE (\"ip_addresses\".\"instance_id\" = 59)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.006268s) SELECT * FROM \"ip_addresses\" WHERE (\"ip_addresses\".\"instance_id\" = 43)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004181s) DELETE FROM \"ip_addresses\" WHERE \"id\" = 4\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004003s) DELETE FROM \"ip_addresses\" WHERE \"id\" = 20\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.001525s) DELETE FROM \"instances_templates\" WHERE (\"instance_id\" = 59)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004728s) DELETE FROM \"instances_templates\" WHERE (\"instance_id\" = 43)\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_\ninstance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".: DELETE FROM \"instances\" WHERE \"id\" = 59\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_\ninstance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(43) is still referenced from table \"rendered_templates_archives\".: DELETE FROM \"instances\" WHERE \"id\" = 43\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.005789s) ROLLBACK\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.006121s) ROLLBACK\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: Worker thread raised exception: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constrai\nnt \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".\nAnd stack trace:\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: Worker thread raised exception: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constrai\nnt \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".\n - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `exec'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `block in execute_query'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/logging.rb:37:in `log_yield'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `execute_query'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `block in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:111:in `check_disconnect_errors'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:413:in `_execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block (2 levels) in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:425:in `check_database_errors'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:91:in `hold'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:79:in `execute_dui'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:812:in `execute_dui'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:733:in `with_sql_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1485:in `_delete_without_checking'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1468:in `_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1468:in `_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1000:in `delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1513:in `_destroy_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1500:in `block in _destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:865:in `around_destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1497:in `_destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `block (2 levels) in destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1720:in `block in checked_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:338:in `_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:300:in `block in transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:293:in `transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1720:in `checked_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `block in destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1708:in `checked_save_failure'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:47:in `block in delete_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:19:in `delete_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:55:in `block (3 levels) in delete_instance_plans'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\n. It's still a problem on bosh 255.8 version and CPI 52\nPart of the debug log:\nI, [2016-06-09 12:58:23 #23532] []  INFO -- DirectorJobRunner: Force deleting is set, ignoring exception\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004431s) SELECT NULL\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004455s) BEGIN\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.005101s) SELECT NULL\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.003337s) BEGIN\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.002227s) SELECT * FROM \"ip_addresses\" WHERE (\"ip_addresses\".\"instance_id\" = 59)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.006268s) SELECT * FROM \"ip_addresses\" WHERE (\"ip_addresses\".\"instance_id\" = 43)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004181s) DELETE FROM \"ip_addresses\" WHERE \"id\" = 4\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004003s) DELETE FROM \"ip_addresses\" WHERE \"id\" = 20\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.001525s) DELETE FROM \"instances_templates\" WHERE (\"instance_id\" = 59)\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.004728s) DELETE FROM \"instances_templates\" WHERE (\"instance_id\" = 43)\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_\ninstance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".: DELETE FROM \"instances\" WHERE \"id\" = 59\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_\ninstance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(43) is still referenced from table \"rendered_templates_archives\".: DELETE FROM \"instances\" WHERE \"id\" = 43\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.005789s) ROLLBACK\nD, [2016-06-09 12:58:23 #23532] [] DEBUG -- DirectorJobRunner: (0.006121s) ROLLBACK\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: Worker thread raised exception: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constrai\nnt \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".\nAnd stack trace:\nE, [2016-06-09 12:58:23 #23532] [] ERROR -- DirectorJobRunner: Worker thread raised exception: PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constrai\nnt \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(59) is still referenced from table \"rendered_templates_archives\".\n - /var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `exec'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `block in execute_query'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/logging.rb:37:in `log_yield'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:145:in `execute_query'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `block in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:111:in `check_disconnect_errors'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:132:in `execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:413:in `_execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block (2 levels) in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:425:in `check_database_errors'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `block in execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:91:in `hold'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/adapters/postgres.rb:242:in `execute'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:79:in `execute_dui'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:812:in `execute_dui'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/dataset/actions.rb:733:in `with_sql_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1485:in `_delete_without_checking'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1468:in `_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1468:in `_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1000:in `delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1513:in `_destroy_delete'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1500:in `block in _destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:865:in `around_destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1497:in `_destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `block (2 levels) in destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1720:in `block in checked_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:338:in `_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:300:in `block in transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `block in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/connection_pool/threaded.rb:104:in `hold'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/connecting.rb:236:in `synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/database/query.rb:293:in `transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1720:in `checked_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `block in destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1708:in `checked_save_failure'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/sequel-3.43.0/lib/sequel/model/base.rb:1015:in `destroy'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:47:in `block in delete_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/event_log.rb:97:in `advance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:19:in `delete_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh-director-1.3215.3.0/lib/bosh/director/instance_deleter.rb:55:in `block (3 levels) in delete_instance_plans'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:77:in `block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `loop'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/bosh_common-1.3215.3.0/lib/common/thread_pool.rb:63:in `block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `call'\n/var/vcap/packages/director/gem_home/ruby/2.1.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in `block in create_with_logging_context'\n. This is still happening:\nFailed deleting instances > cc_bridge/0 (818ba950-db80-4afc-9707-416d646204f1): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(406) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n   Failed deleting instances > api_worker/0 (ba4bbf72-51ef-4abc-97b9-fd4407821a44): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(394) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n   Failed deleting instances > router/1 (e9a04847-e44d-4f11-871f-58f033a022e7): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(401) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n. This is still happening:\nFailed deleting instances > cc_bridge/0 (818ba950-db80-4afc-9707-416d646204f1): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(406) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n   Failed deleting instances > api_worker/0 (ba4bbf72-51ef-4abc-97b9-fd4407821a44): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(394) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n   Failed deleting instances > router/1 (e9a04847-e44d-4f11-871f-58f033a022e7): PG::Error: ERROR:  update or delete on table \"instances\" violates foreign key constraint \"rendered_templates_archives_instance_id_fkey\" on table \"rendered_templates_archives\"\nDETAIL:  Key (id)=(401) is still referenced from table \"rendered_templates_archives\".\n (00:01:52)\n. +1 for this. There is no collocated uaa on bosh VM. Update section:\nupdate:\n  canaries: 0\n  max_in_flight: 2\n  canary_watch_time: 30000-600000\n  update_watch_time: 5000-600000\n  serial: false\nIf you are interested in other parts of our manifest you can find it here:\nhttps://github.com/alphagov/paas-cf/tree/17a454ebeb9e52df6edbecd24059b15b430108e3/manifests/cf-manifest/deployments\n. +1 for this. There is no collocated uaa on bosh VM. Update section:\nupdate:\n  canaries: 0\n  max_in_flight: 2\n  canary_watch_time: 30000-600000\n  update_watch_time: 5000-600000\n  serial: false\nIf you are interested in other parts of our manifest you can find it here:\nhttps://github.com/alphagov/paas-cf/tree/17a454ebeb9e52df6edbecd24059b15b430108e3/manifests/cf-manifest/deployments\n. https://github.com/cloudfoundry/bosh/pull/1402 will fix this.\n. https://github.com/cloudfoundry/bosh/pull/1402 will fix this.\n. ",
    "bitops": "Running into this issue as well. \n```\nDirector task 6271\n  Started removing deployment artifacts\n  Started removing deployment artifacts > Detaching stemcells. Done (00:00:00)\n  Started removing deployment artifacts > Detaching releases. Done (00:00:00)\nStarted deleting properties\n  Started deleting properties > Destroying deployment. Failed: PG::Error: ERROR:  update or delete on table \"deployments\" violates foreign key constraint \"placeholder_mappings_deployment_id_fkey\" on table \"placeholder_mappings\"\nDETAIL:  Key (id)=(34) is still referenced from table \"placeholder_mappings\".\n (00:00:00)\nError 100: PG::Error: ERROR:  update or delete on table \"deployments\" violates foreign key constraint \"placeholder_mappings_deployment_id_fkey\" on table \"placeholder_mappings\"\nDETAIL:  Key (id)=(34) is still referenced from table \"placeholder_mappings\".\n```. ",
    "sax": "That might end up being the best solution. I had thought that it would be simpler to provide a cache, but something like BitTorrent Sync wind up being less difficult to manage.\nAlso, I realized that :host is only used when configuring a writable S3 blobstore. I can set :endpoint directly in my configuration to get the behavior that I was expecting.\nThank you!\n. That might end up being the best solution. I had thought that it would be simpler to provide a cache, but something like BitTorrent Sync wind up being less difficult to manage.\nAlso, I realized that :host is only used when configuring a writable S3 blobstore. I can set :endpoint directly in my configuration to get the behavior that I was expecting.\nThank you!\n. Awesome, thank you for the link. I searched around before filing, but obviously not enough. I'll close this issue in favor of #1169.\n. Awesome, thank you for the link. I searched around before filing, but obviously not enough. I'll close this issue in favor of #1169.\n. ",
    "rishigits": "Folks ... do we have any update on this?  Wondering if work has already started to build 16.04 stemcells and if so do we have any release timeline in place.\nThanks.. Folks ... do we have any update on this?  Wondering if work has already started to build 16.04 stemcells and if so do we have any release timeline in place.\nThanks.. @cppforlife I am interested in getting Cloud Foundry running on s390x (Linux on Z systems) but s390x only supports xenial. I already tried building a compatible stemcell but ran into issues midway.  Issues were mostly related to BOSH unable to connect to ssh services running inside 16.04 stemcell.  I did not spend too much time to investigate as I thought if x86 xenial stemcell comes out it will be easier to model it for this platform. . Thanks @cunnie - I am already looking at that and running into some issues specifically related to image_create/image_install_grub stages.  Apparently grub does not work on s390x architecture and code will require some adjustments.  Incidentally, would you or anybody know in what capacity BOSH uses grub.\nMany thanks.\n. Thanks @cunnie - I am already looking at that and running into some issues specifically related to image_create/image_install_grub stages.  Apparently grub does not work on s390x architecture and code will require some adjustments.  Incidentally, would you or anybody know in what capacity BOSH uses grub.\nMany thanks.\n. ",
    "apeschel": "@cppforlife It would be nice to switch to 16.04 to make sure everything is systemd compatible sooner than later. 14.04 is only good until 2019 -- 2 more years from now. This seem like a long time, but the longer it takes to get the 16.04 stemcell, the less time people using Cloud Foundry as a platform have to port all their stuff over and make sure it's compatible.. @cppforlife It would be nice to switch to 16.04 to make sure everything is systemd compatible sooner than later. 14.04 is only good until 2019 -- 2 more years from now. This seem like a long time, but the longer it takes to get the 16.04 stemcell, the less time people using Cloud Foundry as a platform have to port all their stuff over and make sure it's compatible.. ",
    "ldemailly": "+1 on support for Ubuntu 16.04.2 LTS. ",
    "hwinkel": "We are just entering into the CF world. Was a bit a surprise that the latest supported ubuntu release is 14.04. Is there any more recent, 16.04 LTE available soon. Especially with the latest Security updates it would be a good catch to have it.. We are just entering into the CF world. Was a bit a surprise that the latest supported ubuntu release is 14.04. Is there any more recent, 16.04 LTE available soon. Especially with the latest Security updates it would be a good catch to have it.. how far we are with IPv6 now? We have the need to Deploy IPv6 kubernetes Applications.Therefore the whole chain BOSH, CFCR (k8s) and IaaS must support IPv6. . how far we are with IPv6 now? We have the need to Deploy IPv6 kubernetes Applications.Therefore the whole chain BOSH, CFCR (k8s) and IaaS must support IPv6. . @cppforlife Thanks for the feedback. Currently on the IaaS shortlist are a vSphere based Deployment, or eventually Softlayer. There was Azure Planned as well, but seems there is still no IPv6 tin VNET these days.  And finally we are looking to deploy CFCR/K8s on it, which has his own IPv6 Issues. All pretty network related in our deployments!. @cppforlife Thanks for the feedback. Currently on the IaaS shortlist are a vSphere based Deployment, or eventually Softlayer. There was Azure Planned as well, but seems there is still no IPv6 tin VNET these days.  And finally we are looking to deploy CFCR/K8s on it, which has his own IPv6 Issues. All pretty network related in our deployments!. ",
    "MichaelTrestman": "Hi! @jianqiu, thank you for the pr.\nThe unit tests fail when we run them. We think that this line could be the problem. Please confirm that all unit tests pass and then push again, and we'll take another look.\nThanks for contributing,\nMichael and @Kiemes \n. Thank you @barthy1 !! PR has been merged.\n. ",
    "bgandon": "Hello,\nI'm facing the same issue. My BOSH deploys very often hit the 45s timeout. Currently I circumvent the problem  with the following script that patches the director and restarts it.\n```\ncd /var/vcap/packages/director/gem_home/ruby/2../gems/bosh-director-1.*.0 || exit 115\npatch -p0 <<'END_OF_PATCH'\n--- lib/bosh/director/agent_client.rb 2016-05-11 14:13:17.000000000 +0000\n+++ lib/bosh/director/agent_client.rb 2016-05-26 10:13:53.971352833 +0000\n@@ -36,7 +36,7 @@\n       @service_name = service_name\n       @client_id = client_id\n       @nats_rpc = Config.nats_rpc\n-      @timeout = options[:timeout] || 45\n+      @timeout = options[:timeout] || 90\n       @logger = Config.logger\n       @retry_methods = options[:retry_methods] || {}\nEND_OF_PATCH\n/var/vcap/bosh/bin/monit restart director\n```\nHowever I would be happy to find a better way to cure the issue.\nIn the situation above, what solution did you finally identify?\n@cppforlife : how can I know if job start is sync or async in my case? How can I switch it to async?\nThank you for your help.. ",
    "gdean123": "+1 We are experiencing the same issue. We want to avoid having to rewrite the URLs returned by BOSH to include the port.\n. ",
    "mkenyon": "@cppforlife This patch actually makes it nicer for unique id. It's not specific to indexes. There are a few commands that require bosh subcommand job index_or_id and they do not except bosh subcommand job/index_or_id. This patch fixes that inconsistency. The patch itself doesn't care whether the user uses the index or the id. I think that you should merge this.\n. @cppforlife This patch actually makes it nicer for unique id. It's not specific to indexes. There are a few commands that require bosh subcommand job index_or_id and they do not except bosh subcommand job/index_or_id. This patch fixes that inconsistency. The patch itself doesn't care whether the user uses the index or the id. I think that you should merge this.\n. ",
    "gu-bin": "@cppforlife The use scenario is what we have been using in IBM Bluemix for a long time with no problem. The bosh version is\n  Version    1.2751.0 (00000000)\nMaybe it's a incompatibility problem between the new bosh and bosh 1.2751.0?\n. /cc @maximilien @jianqiu @mattcui\n. Add the following parameter to bosh.yml can solve the problem:\ndebug:\n        keep_unreachable_vms: true\n. As we are doing os-reload on Softlayer, we need bosh to continue the deploy on the existing vm after it fails/canceled last time instead of create a new vm to replace the failed/canceled one. \nIf keep_unreachable_vms: true is not set, bosh will delete the failed/canceled vm from bosh db, and next deploy will definitely create a new one to replace it. If we set keep_unreachable_vms: true, the failed/canceled vm record will be kept in bosh db. Next time bosh deploy will depend on the behaviour of CPI create_vm action to either do os-reload or create a new one. But our CPI create_vm action replies on the IP value that bosh director passed to it (valid IP will do os-reload and invalid/empty IP will create a new vm). \nThe director get the vm IP value through get_state method (retrieve the vm's spec.json to get the network information including IP). If at the point of deploy failed/canceled, the vm's spec.json has not been created yet (for example, failed/canceled at os-reload), director can't get the vm IP value through get_state method, then no IP passed to CPI create_vm action, CPI will create a new vm which is not expected. But although director can't get vm IP value through get_state method, the the vm's spec.json information is actually kept in bosh db. Can director just send the IP value in bosh db to CPI in this situation?\nThe issue always happens from bosh v1, but we hacked bosh code to workaround the problem.\n. I've tried on several releases and the same issue exists, so the issue is not related to releases.\n. I didn't see this issue with the community release. \nI found an interesting thing: if run bosh upload release <remote_url> to upload the release from remote server, the name/version info is correct after upload. But if first download the release file to bosh cli and then run bosh upload release <local_file> to upload the release from local, the name/version info is wrong after upload. \n. Any update?\n. @cppforlife We doubled the worker vms' performance to 8*2G cpu and 16G memory but still see this problem. Did you successfully compile the release https://bosh.io/d/github.com/cloudfoundry-community/cf-services-contrib-release on kernel 4.4? What's your worker vm performance configuration? Thanks.\n/cc @maximilien @mattcui \n. ",
    "suhlig": "Nevermind, we just found out that\n$ bosh -u admin -p admin target https://192.168.50.4:25555\ndoes what we want. \nThanks!\n. Nevermind, we just found out that\n$ bosh -u admin -p admin target https://192.168.50.4:25555\ndoes what we want. \nThanks!\n. I have this problem all the time (with the most recent VirtualBox). Whenever my laptop goes to sleep, the director not unaccessible anymore afterwards.\nHaving to go through a create-env after I closed the lid is really annoying and definitely a huge degradation compared to the Vagrant solution we had before (which was rock-solid).. I have this problem all the time (with the most recent VirtualBox). Whenever my laptop goes to sleep, the director not unaccessible anymore afterwards.\nHaving to go through a create-env after I closed the lid is really annoying and definitely a huge degradation compared to the Vagrant solution we had before (which was rock-solid).. > did you configure virtualbox to shut off vms on sleep?\nNot that I am aware of. Other Vagrant-based VMs survive just fine, even the classic bosh-lite. \nBTW, I also see the VM aborting from time to time when doing a brand new bosh create-env.\nIs there anywhere I could look for debugging this?\n$ VBoxManage --version\n5.1.28r117968\nIn case it matters, I am using a 13-inch MacBook Pro (Early 2015) with 16 GiB RAM.. > did you configure virtualbox to shut off vms on sleep?\nNot that I am aware of. Other Vagrant-based VMs survive just fine, even the classic bosh-lite. \nBTW, I also see the VM aborting from time to time when doing a brand new bosh create-env.\nIs there anywhere I could look for debugging this?\n$ VBoxManage --version\n5.1.28r117968\nIn case it matters, I am using a 13-inch MacBook Pro (Early 2015) with 16 GiB RAM.. Just checking in to say that the problem remains with the latest VirtualBox:\n$ VBoxManage --version\n5.2.2r119230. ",
    "DonBower": "Any Resolution?. Hello @ematpl Eric;\nI removed the creds.yml file (the file called out in the --vars-store flag, and viola!\nThanks for your help.\non to configuring stuff.... ",
    "LinuxBozo": "This may be more an issue with the AWS CPI, and definitely an issue in documentation, but the situation we'd like to solve is bootstrapping with bosh-init into AWS GovCloud using either:\n1. A Mac or\n2. Inside a Concourse pipeline\nThe bosh-init documentation doesn't really list any limitations for something like this, but clearly, they exist.\nWhen using a full stemcell, if you try to run on the Mac:\ncreating stemcell (bosh-aws-xen-ubuntu-trusty-go_agent 3232.4):\n    CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Bosh::Clouds::CloudError\",\"message\":\"Timed out reading instance metadata, please make sure CPI is running on EC2 instance\",\"ok_to_retry\":false}\nOk, so we have Concourse on EC2. However, if you try running within a Concourse job (and therefore, garden container):\ncreating stemcell (bosh-aws-xen-ubuntu-trusty-go_agent 3232.4):\n    CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Bosh::Clouds::CloudError\",\"message\":\"Cannot find EBS volume on current instance\",\"ok_to_retry\":false}\nSo, the issue is that using a full stemcell to bootstrap with bosh-init currently requires that it is run within a full EC2 instance. If create_stemcell needs an EC2 instance to build an AMI from stemcell image, shouldn't it provision it, and tear it down when finished, much like compilation vms?\n/cc @sharms @dlapiduz @cnelson\n. This may be more an issue with the AWS CPI, and definitely an issue in documentation, but the situation we'd like to solve is bootstrapping with bosh-init into AWS GovCloud using either:\n1. A Mac or\n2. Inside a Concourse pipeline\nThe bosh-init documentation doesn't really list any limitations for something like this, but clearly, they exist.\nWhen using a full stemcell, if you try to run on the Mac:\ncreating stemcell (bosh-aws-xen-ubuntu-trusty-go_agent 3232.4):\n    CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Bosh::Clouds::CloudError\",\"message\":\"Timed out reading instance metadata, please make sure CPI is running on EC2 instance\",\"ok_to_retry\":false}\nOk, so we have Concourse on EC2. However, if you try running within a Concourse job (and therefore, garden container):\ncreating stemcell (bosh-aws-xen-ubuntu-trusty-go_agent 3232.4):\n    CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Bosh::Clouds::CloudError\",\"message\":\"Cannot find EBS volume on current instance\",\"ok_to_retry\":false}\nSo, the issue is that using a full stemcell to bootstrap with bosh-init currently requires that it is run within a full EC2 instance. If create_stemcell needs an EC2 instance to build an AMI from stemcell image, shouldn't it provision it, and tear it down when finished, much like compilation vms?\n/cc @sharms @dlapiduz @cnelson\n. ",
    "dlapiduz": "@ljfranklin @cppforlife we did a test with very few modifications and it worked like a charm.\nThanks!\n. @ljfranklin @cppforlife we did a test with very few modifications and it worked like a charm.\nThanks!\n. @ljfranklin here is a very small PR for the change https://github.com/cloudfoundry-incubator/aws-light-stemcell-builder/pull/1. Most of the other stuff came as pipeline changes (https://github.com/cloudfoundry-incubator/aws-light-stemcell-builder/compare/master...18F:master?expand=1) and they are not backwards compatible :(\n. ",
    "krishnanrs": "\nI'm submitting another change in the bosh-agent project, if approved would eliminate the need for the sleep statements.\nWasn't aware that cloud-init conflicts with bosh-agent. Will remove that package.\n   Will re-submit a new pull request while just including the gdisk package for ubuntu and centos.\n. The bosh-agent runs growpart (https://github.com/cloudfoundry/bosh-agent/blob/master/platform/linux_platform.go#L283) on the root volume as part of the bootstrap process. On GPT partitioned volumes, gdisk is required for growpart to succeed. The problem is that on linux hosts where growpart exists but fails, then bosh-agent fails the bootstrap process. \n. \n",
    "holgero": "I didn't try monit reload, but after I did a sv restart monit, monit indeed reported the state of all jobs (including the director) as running.\n. I didn't try monit reload, but after I did a sv restart monit, monit indeed reported the state of all jobs (including the director) as running.\n. I created a pull request anyway: #1473\n. I created a pull request anyway: #1473\n. I added a log of level 'warn', but I have no test for it. Is that OK?\n. I added a log of level 'warn', but I have no test for it. Is that OK?\n. Forget my last comment, I added a test for the log message.\n. Forget my last comment, I added a test for the log message.\n. @cppforlife Thank you for the update. I am not sure I got it correctly what you meant when you wrote: \"We've ended up pulling changes in\". Does this mean you pulled my change? Or did you say that you did some other changes that improve only some of the events but still lots of incomplete events are sent?. I attached the output of an experiment with valgrind on rsyslog 8.23 to the issue there.. I attached the output of an experiment with valgrind on rsyslog 8.23 to the issue there.. What I have now to reproduce the problem on bosh-lite:\n A bosh deployment of the syslog release 9 with the stemcell 3312.9 and haproxy from the cf release 249, here is the manifest: manifest.yml. Note that it won't deploy cleanly as the haproxy misses a pem file and doesn't start successfully.\n Manual reconfiguration of the haproxy, here is the changed /var/vcap/jobs/haproxy/config/haproxy.conf. Start the haproxy manually on the vm with /var/vcap/jobs/haproxy/bin/haproxy_ctl start.\nThen on that vm run a command to produce log entries: while : ; do logger hallo $RANDOM ; sleep .5 ; done & and watch the RES size of the rsyslogd increase slowly.... What I have now to reproduce the problem on bosh-lite:\n A bosh deployment of the syslog release 9 with the stemcell 3312.9 and haproxy from the cf release 249, here is the manifest: manifest.yml. Note that it won't deploy cleanly as the haproxy misses a pem file and doesn't start successfully.\n Manual reconfiguration of the haproxy, here is the changed /var/vcap/jobs/haproxy/config/haproxy.conf. Start the haproxy manually on the vm with /var/vcap/jobs/haproxy/bin/haproxy_ctl start.\nThen on that vm run a command to produce log entries: while : ; do logger hallo $RANDOM ; sleep .5 ; done & and watch the RES size of the rsyslogd increase slowly.... ",
    "Petahhh": "@cunnie I have it signed. Should I fax it in?\n. @cunnie I have it signed. Should I fax it in?\n. Hi @cunnie, what can I do to help move this along? Thank you!\n. Hi @cunnie, what can I do to help move this along? Thank you!\n. ",
    "jmcarp": "@cppforlife: thanks, I'll file an issue on s3cli. If s3cli eventually supports these options, would you be open to passing them through in S3cliBlobstoreClient?\n. Replaced by #1299.\n. Sorry about the failing builds earlier, but tests are passing now, and this should be ready for review.\n. @dpb587-pivotal: when do you think this might make it into an official release?\n. Makes sense to me--do you mean something like:\nruby\nmanifest = YAML.load(instance.deployment.manifest || '{}')\nmetadata = metadata.merge(manifest.fetch('tags', {}))\nor\nruby\nmanifest = Manifest.load_from_model(instance.deployment)\nmetadata = metadata.merge(manifest.to_hash.fetch('tags', {}))\nIf we read the tags from the manifest in VmMetadataUpdater, then can we also drop the changed to DeploymentSpecParser? I added those based on discussion in https://github.com/cloudfoundry-incubator/bosh-aws-cpi-release/issues/33#issuecomment-223487970, and my impression was that the purpose of parsing tags in DeploymentSpecParser would be to add them as a new field to the Deployment model.\n. @cppforlife: OK, updated to read tags from the manifest instead of saving to the db.\n. Ping @cppforlife. Is this closer to what you had in mind?\n. Interesting, looks like you all switched back to the implementation I was working with originally, when tags were parsed and stored in their own model: https://github.com/cloudfoundry/bosh/commit/06eb96835d3928cdc13bedece1c7a9902d3ff604.\n. For reference, old work at https://github.com/jmcarp/bosh/commit/d0987072ed9544f68234c67096e549602f17dc28.\n. @cppforlife we're using bosh 265.2.0 from bosh.io, but I think we've been seeing this behavior for a while.. ",
    "huangered": "solved, these three parts need to be put in a single file.\n. @voelzmo hi, we are use the VMware vCenter and VMware NSX to create our cloud foundry.\nwe find in some special environment, we need to adjust the network interface's MTU.\nuse-case 1:\nour layout 3 switch mtu is 1500, the vm is in two host connected by nsx logic switch, which mtu is 1600. the vm in the same host can communicate with each other normally. but when cross-host, they cannot , unless we change to mtu to 1450 . I am not sure this is caused by incorrect nsx or other's . \nbut when we use ubuntu-sphere-stemcell latest version, it will cause this error, ubuntu(14.04)\nI have tested with ubuntu 16, it have no this kind issue.. @dpb587-pivotal great, thanks, let me have a try.. ",
    "arut95": "Those 3 parts should be kept in cloud-config.yml. ",
    "rfreddi": "Hi, the auditd problem occurs also in a clean stemcell based on 3232.4 image.\n```\nbash -x service auditd start\n++ basename service\n+ VERSION='service ver. 0.91-ubuntu1'\n++ basename service\n+ USAGE='Usage: service < option > | --status-all | [ service_name [ command | --full-restart ] ]'\n+ SERVICE=\n+ ACTION=\n+ SERVICEDIR=/etc/init.d\n+ OPTIONS=\n+ '[' 2 -eq 0 ']'\n+ cd /\n+ '[' 2 -gt 0 ']'\n+ case \"${1}\" in\n+ '[' -z '' -a 2 -eq 1 -a auditd = --status-all ']'\n+ '[' 2 -eq 2 -a start = --full-restart ']'\n+ '[' -z '' ']'\n+ SERVICE=auditd\n+ shift\n+ '[' 1 -gt 0 ']'\n+ case \"${1}\" in\n+ '[' -z auditd -a 1 -eq 1 -a start = --status-all ']'\n+ '[' 1 -eq 2 -a '' = --full-restart ']'\n+ '[' -z auditd ']'\n+ '[' -z '' ']'\n+ ACTION=start\n+ shift\n+ '[' 0 -gt 0 ']'\n+ '[' -r /etc/init/auditd.conf ']'\n+ which initctl\n+ grep -q upstart\n+ initctl version\n+ case \"${ACTION}\" in\n+ exec start auditd\nauditd start/running, process 12118\n~# ps -ef | grep 12118\nroot     12219  5537  0 09:53 pts/0    00:00:00 grep --color=auto 12118\n```\nIs there a particular configuration in order to enable auditd service?\nSuggestions?\nThanks in advance\nRemo\n. Hi, the auditd problem occurs also in a clean stemcell based on 3232.4 image.\n```\nbash -x service auditd start\n++ basename service\n+ VERSION='service ver. 0.91-ubuntu1'\n++ basename service\n+ USAGE='Usage: service < option > | --status-all | [ service_name [ command | --full-restart ] ]'\n+ SERVICE=\n+ ACTION=\n+ SERVICEDIR=/etc/init.d\n+ OPTIONS=\n+ '[' 2 -eq 0 ']'\n+ cd /\n+ '[' 2 -gt 0 ']'\n+ case \"${1}\" in\n+ '[' -z '' -a 2 -eq 1 -a auditd = --status-all ']'\n+ '[' 2 -eq 2 -a start = --full-restart ']'\n+ '[' -z '' ']'\n+ SERVICE=auditd\n+ shift\n+ '[' 1 -gt 0 ']'\n+ case \"${1}\" in\n+ '[' -z auditd -a 1 -eq 1 -a start = --status-all ']'\n+ '[' 1 -eq 2 -a '' = --full-restart ']'\n+ '[' -z auditd ']'\n+ '[' -z '' ']'\n+ ACTION=start\n+ shift\n+ '[' 0 -gt 0 ']'\n+ '[' -r /etc/init/auditd.conf ']'\n+ which initctl\n+ grep -q upstart\n+ initctl version\n+ case \"${ACTION}\" in\n+ exec start auditd\nauditd start/running, process 12118\n~# ps -ef | grep 12118\nroot     12219  5537  0 09:53 pts/0    00:00:00 grep --color=auto 12118\n```\nIs there a particular configuration in order to enable auditd service?\nSuggestions?\nThanks in advance\nRemo\n. @mattcui \nHi Matt, I tested the fix in the image created and it works fine.\nAnyway the auditd behavior is changed, so I'm changing my code.\nSo when the stemcell will be updated I need to deploy my new release version.\nThank you very much Matt and all.\n. @mattcui \nHi Matt, I tested the fix in the image created and it works fine.\nAnyway the auditd behavior is changed, so I'm changing my code.\nSo when the stemcell will be updated I need to deploy my new release version.\nThank you very much Matt and all.\n. @dpb587-pivotal \nwe are using the following stemcell:\n| bosh-softlayer-esxi-ubuntu-trusty-go_agent | ubuntu-trusty | 3262.7* | 1282653 |\nThanks a lot for the information!\n@mattcui, please when the new stemcell is available could you update the test environment?\nThanks in advance.\n. @dpb587-pivotal \nwe are using the following stemcell:\n| bosh-softlayer-esxi-ubuntu-trusty-go_agent | ubuntu-trusty | 3262.7* | 1282653 |\nThanks a lot for the information!\n@mattcui, please when the new stemcell is available could you update the test environment?\nThanks in advance.\n. ",
    "0x1mason": "Thanks. I'll do that. It'll take me a bit--swamped at work and I need to jump thru some more hoops to get the lawyers to sign the CLA.\nThe heart of the changes were removing tar and using ruby instead. There is existing platform-specific handling for differences between BSD and GNU tar. My changes make all that unnecessary. . So you can think of this as a \"make it work the same way for everyone regardless of OS\" change.  It's basically just a drop-in for the tar invocations.\n. I have tar on Windows. It works great from the command line. The sys calls from Ruby, not so much.\n. ",
    "baschno": "@cppforlife With the 3262 stemcell releases the issue disappeared and Cassandra is working again...\nSo this issue only appears for all 3232.x releases. \nI guess the new releases are still mounted with noexec which means that this is not the origin.\n. ",
    "mingxiao": "We started looking into this but haven't found the issue yet. It would be helpful to see the full task logs. Do you still have the task logs for the bosh cleanup --all commands?\n. verified https://main.bosh-ci.cf-app.com/builds/108519\n. @jianqiu The issues are already noted. \n- The test on debug output is a false positive\n- Please make an assertion on the result of networks_changed.\nWe also saw that networks_changed? is returning true b/c the test case is configured so that desired_network_plans.any? is true. https://github.com/jianqiu/bosh/blob/533d03117c1a48c1bdad56bd197b0c7bde1fb15b/bosh-director/lib/bosh/director/deployment_plan/instance_plan.rb#L105\nIs that the intended behavior of that test?\n. @jianqiu I think I have enough context now to make the changes.\nWhen I say false positive, I mean it is passing when it should not.\nThe issue with your current test is that new_network_settings, https://github.com/cloudfoundry/bosh/pull/1411/files#diff-c0ae009b7ea6019dc2b271e8c7ef98d0R135,  does not match what is being processed in networks_changed?\nThis is your assert:\nnetworks_changed? network settings changed FROM: {\"existing-network\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{}, \"dns_record_name\"=>\"0.job-1.my-network.deployment.bosh\", \"dns\"=>\"10.0.0.1\"}} TO: {\"existing-network\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{}, \"dns\"=>\"10.0.0.1\"}} on instance job-1/fake-uuid-1 (1)\nAnd this is what being logged:\nnetworks_changed? network settings changed FROM: {\"existing-network\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{}, \"dns_record_name\"=>\"0.job-1.my-network.deployment.bosh\", \"dns\"=>\"10.0.0.1\"}} TO: {\"existing-network\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{}, \"dns\"=>\"10.0.0.1\"}, \"a\"=>{\"ip\"=>\"192.168.1.3\", \"netmask\"=>\"255.255.255.0\", \"cloud_properties\"=>{}, \"default\"=>[\"dns\", \"gateway\"], \"dns\"=>[\"192.168.1.1\", \"192.168.1.2\"], \"gateway\"=>\"192.168.1.1\"}} on instance job-1/fake-uuid-1 (1)\nNotice that your test is missing the new_network_settings['a'] key\n. Fix and pushed onto develop branch, with fix https://github.com/cloudfoundry/bosh/commit/791a8211f8d4d9f6d6b0dfc227381f20fafc83e1\n. @MatthiasWinzeler the cpi-config and update-cpi-config has also been ported over to the go bosh cli\n. We'll clean this up when we merge but we prefer to use parenthesis whenever possible for clarity and consistency. Some exceptions might be in RSpec's idiomatic syntax (expect(subject).to eq(something)) but in this case we'd prefer explicit parents.\n. This tests passes even if network_settings_changed? returns true. The content of the new_network_settings variable does not accurately reflect what is being compared against network_settings. \n. Is this expected to be true or false?\n. consulted with @dpb587 , and we think that the result should be false. Since the network did not change and we are now ignoring dns_record_name, networks_changed? should return false\n. ",
    "friegger": "If the release cannot be deleted, because it is already in use by the deployment, one can do:\nbosh upload release <url> --fix\n. Before the test we saw networks parsed in each create_vm step:\nW, [2018-02-06T15:28:47 #30321] [task:24323]  WARN -- DirectorJobRunner: Finished calculating networks: 11.894892256\nW, [2018-02-06T15:29:00 #30321] [task:24323]  WARN -- DirectorJobRunner: Finished calculating networks: 12.350739324\nW, [2018-02-06T15:29:18 #30321] [task:24323]  WARN -- DirectorJobRunner: Finished calculating networks: 11.328467443\nW, [2018-02-06T15:29:52 #30321] [create_missing_vm(nats_z2/7acb2865-9264-4d83-8b10-f803eb6333c1 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 33.075372001\nW, [2018-02-06T15:30:43 #30321] [create_missing_vm(etcd_z1/393ce582-6502-4cb3-96f2-e1ac48d40c98 (1)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 80.903341315\nW, [2018-02-06T15:30:57 #30321] [create_missing_vm(etcd_z1/51bd5064-e905-4753-b0e3-0b163d5bff55 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 93.77009135\nW, [2018-02-06T15:31:33 #30321] [create_missing_vm(doppler_z2/b38bbc4e-4d74-4f47-8404-95a8d3d301e9 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 114.624604282\nW, [2018-02-06T15:31:37 #30321] [create_missing_vm(consul_z1/e52b7f01-e534-48b0-a695-67a4fcdd0b0b (1)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 120.636838913\nW, [2018-02-06T15:31:44 #30321] [create_missing_vm(uaa_z2/c41e687b-5db9-4086-9980-d9639c1fae2c (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 131.671818551\nW, [2018-02-06T15:32:04 #30321] [create_missing_vm(api_z2/9616410b-0ea8-451d-b1b8-b408f464bb4c (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 136.73622261\nW, [2018-02-06T15:32:17 #30321] [create_missing_vm(clock_z1/e28922d5-e348-437e-9c08-c08cd12997ec (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 138.328126151\nW, [2018-02-06T15:32:42 #30321] [create_missing_vm(router_z1/aa6ffcfc-c58b-4c0e-95c2-a96a31c441d8 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 163.795115488\nW, [2018-02-06T15:33:16 #30321] [create_missing_vm(loggregator_trafficcontroller_z1/d6fa75d8-9c42-40db-b39c-24f94ececdbf (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 175.866320823\nW, [2018-02-06T15:33:23 #30321] [create_missing_vm(etcd_z2/2794a61a-837a-4d27-b963-2db24fca86f7 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 175.666446584\nW, [2018-02-06T15:33:25 #30321] [create_missing_vm(api_z1/0eaad52b-3518-4db5-86c9-00d669a75bd0 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 185.304852686\nW, [2018-02-06T15:33:25 #30321] [create_missing_vm(consul_z1/89d060e5-8064-4338-a10d-3ed064bf68ac (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 173.563625497\nW, [2018-02-06T15:33:33 #30321] [create_missing_vm(doppler_z1/c1147d01-6649-48a9-b878-8e743337a78d (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 198.257133786\nW, [2018-02-06T15:33:46 #30321] [create_missing_vm(nats_z1/9d93cd32-944b-4229-84e2-3a05ea724216 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 188.686175856\nW, [2018-02-06T15:33:50 #30321] [create_missing_vm(consul_z2/88812143-b3fd-4c76-9ed6-9d0b541190ca (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 180.47250213\nW, [2018-02-06T15:33:53 #30321] [create_missing_vm(uaa_z1/bfb575f4-8d4a-4114-8091-41575982f51b (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 178.863657714\nW, [2018-02-06T15:34:02 #30321] [create_missing_vm(clock_z2/cb0beb1a-505b-4dab-bbd0-3b6b2ed5ca95 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 168.141828129\nW, [2018-02-06T15:34:21 #30321] [create_missing_vm(nats_z2/7acb2865-9264-4d83-8b10-f803eb6333c1 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 172.203000815\nW, [2018-02-06T15:34:39 #30321] [create_missing_vm(api_worker_z2/f6aa89f4-0ee5-4319-bafb-b869dc8db0b2 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 180.670593737\nW, [2018-02-06T15:34:46 #30321] [create_missing_vm(loggregator_trafficcontroller_z2/cbed14b8-8832-40fe-bb74-6b4bd56c99c5 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 158.414750466\nW, [2018-02-06T15:34:54 #30321] [create_missing_vm(etcd_z1/51bd5064-e905-4753-b0e3-0b163d5bff55 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 169.777560055\nW, [2018-02-06T15:34:57 #30321] [create_missing_vm(etcd_z1/393ce582-6502-4cb3-96f2-e1ac48d40c98 (1)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 150.031527765\nW, [2018-02-06T15:35:18 #30321] [create_missing_vm(doppler_z2/b38bbc4e-4d74-4f47-8404-95a8d3d301e9 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 168.9090295\nW, [2018-02-06T15:35:45 #30321] [create_missing_vm(consul_z1/e52b7f01-e534-48b0-a695-67a4fcdd0b0b (1)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 150.27966747\nW, [2018-02-06T15:35:58 #30321] [create_missing_vm(uaa_z2/c41e687b-5db9-4086-9980-d9639c1fae2c (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 175.804781371\nW, [2018-02-06T15:36:13 #30321] [create_missing_vm(api_z2/9616410b-0ea8-451d-b1b8-b408f464bb4c (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 162.821008034\nW, [2018-02-06T15:36:13 #30321] [create_missing_vm(router_z1/aa6ffcfc-c58b-4c0e-95c2-a96a31c441d8 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 140.365136805\nW, [2018-02-06T15:36:14 #30321] [create_missing_vm(clock_z1/e28922d5-e348-437e-9c08-c08cd12997ec (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 141.958195706\nW, [2018-02-06T15:36:50 #30321] [create_missing_vm(loggregator_trafficcontroller_z1/d6fa75d8-9c42-40db-b39c-24f94ececdbf (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 124.3717595\nW, [2018-02-06T15:36:50 #30321] [create_missing_vm(api_z1/0eaad52b-3518-4db5-86c9-00d669a75bd0 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 136.262387244\nW, [2018-02-06T15:37:16 #30321] [create_missing_vm(etcd_z2/2794a61a-837a-4d27-b963-2db24fca86f7 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 138.403845851\nW, [2018-02-06T15:37:24 #30321] [create_missing_vm(consul_z1/89d060e5-8064-4338-a10d-3ed064bf68ac (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 133.191968651\nW, [2018-02-06T15:37:29 #30321] [create_missing_vm(consul_z2/88812143-b3fd-4c76-9ed6-9d0b541190ca (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 133.791404509\nW, [2018-02-06T15:37:30 #30321] [create_missing_vm(doppler_z1/c1147d01-6649-48a9-b878-8e743337a78d (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 142.670723492\nW, [2018-02-06T15:37:33 #30321] [create_missing_vm(nats_z1/9d93cd32-944b-4229-84e2-3a05ea724216 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 126.607980134\nW, [2018-02-06T15:37:36 #30321] [create_missing_vm(uaa_z1/bfb575f4-8d4a-4114-8091-41575982f51b (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 116.857172505\nW, [2018-02-06T15:37:37 #30321] [create_missing_vm(clock_z2/cb0beb1a-505b-4dab-bbd0-3b6b2ed5ca95 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 108.961338\nW, [2018-02-06T15:37:39 #30321] [create_missing_vm(loggregator_trafficcontroller_z2/cbed14b8-8832-40fe-bb74-6b4bd56c99c5 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 89.017161769\nW, [2018-02-06T15:37:44 #30321] [create_missing_vm(api_worker_z2/f6aa89f4-0ee5-4319-bafb-b869dc8db0b2 (0)/21)]  WARN -- DirectorJobRunner: Finished calculating networks: 64.653590785\nPossibly mitigates https://github.com/cloudfoundry/bosh/issues/1744. A deployment previously failing with task timeout ran successful with this change.. @avirkar \nBOSH downloads the stemcell and afterwards uploads it to your OpenStack's image service Glance via the OpenStack CPI. Before connecting to the individual OpenStack service clients always contact the identity service Keystone in order to authenticate and discover available services. This call times out, which indicates, that the BOSH VM cannot reach the auth url configured here: https://github.com/cloudfoundry/bosh-deployment/blob/master/openstack/cpi.yml#L68\nIn contrast upload-release does not communicate with the OpenStack API and hence does not yield the same error.\nPlease ensure that the BOSH VM can reach to the OpenStack API. You can find documentation which might help you setting up the infrastructure properly here:\nhttps://bosh.io/docs/init-openstack.html\nClosing this. Please reopen, in case the issue persists.. @jfmyers9 The PR for cck has already been merged, when are you going to have a look at this one. Asking since they are so related and context should be similar.. ",
    "cpraveen412": "We got it. cluster is to be in \"[]\" in yml file\neg: [CF-Cluster]\n. We got it. cluster is to be in \"[]\" in yml file\neg: [CF-Cluster]\n. ",
    "tcnksm": "\nWill re-evaluate in coming months while we improve syslog-release/rsyslog integration (and think about more default logging subsystem).\n\nThat's great \ud83d\udc4d\n\nOut of curiosity how long does it take to compile?\n\nYou mean time of package script ? I can not say exact time right now. But it's not that slow. \n(I will post here next time we run it )\n. ",
    "myminseok": "same here.\nbosh?v=257.3\nbosh-vsphere-cpi-release?v=24\nbosh-vsphere-esxi-ubuntu-trusty-go_agent?v=3262.2\nBOSH cli 1.3262.0\n[WARNING] cannot access director, trying 2 more times...\n[WARNING] cannot access director, trying 1 more times...\ncannot access director (SSL_connect returned=1 errno=0 state=SSLv2/v3 read server hello A: sslv3 alert handshake failure)\nConnection to pcfdemo.net closed.\nbut bosh cli (BOSH 1.3215.3.0) inside of opsmanager works to the same director VM to login.\n. same here.\nbosh?v=257.3\nbosh-vsphere-cpi-release?v=24\nbosh-vsphere-esxi-ubuntu-trusty-go_agent?v=3262.2\nBOSH cli 1.3262.0\n[WARNING] cannot access director, trying 2 more times...\n[WARNING] cannot access director, trying 1 more times...\ncannot access director (SSL_connect returned=1 errno=0 state=SSLv2/v3 read server hello A: sslv3 alert handshake failure)\nConnection to pcfdemo.net closed.\nbut bosh cli (BOSH 1.3215.3.0) inside of opsmanager works to the same director VM to login.\n. ",
    "dineshkumar02": "I'm also getting the similar problem in my OS X.\nbosh target 192.168.50.4 lite\n[WARNING] cannot access director, trying 4 more times...\n[WARNING] cannot access director, trying 3 more times...\n[WARNING] cannot access director, trying 2 more times...\n[WARNING] cannot access director, trying 1 more times...\ncannot access director (SSL_connect returned=1 errno=0 state=SSLv2/v3 read server hello A: unknown protocol)\nruby --version\nruby 2.3.1p112 (2016-04-26 revision 54768) [x86_64-darwin14]\nbosh --version\nBOSH 1.3262.0\n. ",
    "BitRacer": "worked for me as well\n. ",
    "ccemeraldeyes": "+1\nI found the error message \"[WARNING] cannot access director, trying 4 more times...\" unhelpful, in part because I didn't know a more useful message was coming later. I assumed it was a network issue and wasted a few hours debugging from that perspective.\n. +1\nI found the error message \"[WARNING] cannot access director, trying 4 more times...\" unhelpful, in part because I didn't know a more useful message was coming later. I assumed it was a network issue and wasted a few hours debugging from that perspective.\n. ",
    "ChrisMcGowan": "Correct.  Once users where added to bosh.read they could then bosh ssh into their team based deployments.\n. I started with a fresh director deployment using 256.2 and did the initial team1 deployment  while on that release of the director and received the error in question.  I then upgraded the director at that time to 257.1, logged out and back into the director and tried bosh ssh again and same result.  This was when I added the scope bosh.read to the test team11 user.\n. Thank You\n. ",
    "YuPengZTE": "Thank you @voelzmo \n. ",
    "sharms": "Good catch - that was a typo, the pull has been updated\n. We also don't understand the CI Gemfile issue - we added riemann-client to the Gemspec for monitoring\n. Opened against develop as noted in #1337 \n. This issue went away and VMs appeared restored after adding availability_zone\n. ",
    "connected-ndey": "I am experiencing the exact same problem as @YuPengZTE \n. I am experiencing the exact same problem as @YuPengZTE \n. ",
    "carolmorneau": "@eriknelson I'm also seeing  tasks stuck in queue after upload. Have you found a workaround? \nHere's the output I get using the v2 CLI:\n```\nUploading docker bosh\nUsing environment '192.168.50.4' as client 'admin'\n#################################################### 100.00% 59.31 MB/s 2s\nTask 1\nThen it hangs there forever. As shown bellow, the task is stuck in queue.\nbosh -e lite tasks\nUsing environment '192.168.50.4' as client 'admin'\nState   Started At                    Last Activity At              User   Deployment  Description     Result\n1  queued  Thu Jan  1 00:00:00 UTC 1970  Sat Jan 21 19:06:37 UTC 2017  admin  -           create release  -\n1 tasks\nSucceeded\n```\nI'm using bosh-lite commit #6ca2b3824cea596d4e8bfeb075ef458a041a5ae3, \nCLI V2 version 0.0.147-ba602d1-2017-01-11T20:05:06Z\n. Thanks guys, the monit restart works great. If it I need to do that too often I'll reinstall bosh-lite as you suggest. . ",
    "eriknelson": "@carolmorneau Actually yes, SSH into the machine and monit restart the workers. Should get the task started and out of queued.. @cppforlife Thanks for the explanation. I haven't had time to look into it, but is there an existing issue for this or should it be filed?. ",
    "sbaxter": "+1\n. +1\n. ",
    "aalbanes": "Based on my understanding  BoSH action performed to import the release calls the unpack function which uses tar utility:\nhttps://github.com/cloudfoundry/bosh/blob/master/bosh-director/lib/bosh/director/jobs/release/release_job.rb#L58-L69\nTAR  guarantee that when a -r option is used.. the final file will be the latest one: \nhttps://www.gnu.org/software/tar/manual/html_node/append.html#SEC60\nWhat am I missing ?\n. It seems that stemcell contains the /var/log/sysstat folder, but this is \"lost\" when the bosh agent change the  mount point of  /var/log to  /var/vcap/data/root_log file system. \nNotice that the /var/log/sysstat folder is created by dpkg/apt-get during package installation and the sysstat service start script doesn't create that if missing.\nsysstat dir   is created on the wrong filesystem (/ one) and not on  /var/vcap/data/root_log. @mattcui Matt I see, this is another issue.\nThe sysstat folder creation is not impacted by this permission change, that folder is created by root who owns the /var folder tree... as I said in comment#2 the issue is that deb installation via chroot creates the folder in the root filesystem (/), when the agent start it replace the mount point of /var/log to /var/vcap/data/root_log and this lead to sysstat folder to disappear. . ",
    "pivotal-mark-higuera": "This feature is needed. Customers should be able to see the resurrection status without having to change it, especially in a production environment. . ",
    "Freyert": "The problem isn't really that you have to generate keys, the problem is that you have to do it every time.\nWhy not just have BOSH integrate with ssh-agent? That way users can leverage whatever SSH/SCP tooling they prefer and it would allow integrating BOSH with tools like Ansible. Plus SSH and SCP are enormously powerful tools with loads of options and uses that BOSH SSH/SCP will never come close to implementing all of. Also, it would be a waste of time and effort to do that.\nInstead BOSH could just focus on sanely/safely handling key and socket creation/termination. You could set TTL for socket/keys through the command and then plug whatever tool you wanted into the socket.\n\nAnd just as an FYI, most teams that I've spoken with who have created their own releases include steps to either create an SSH user, or change the VCAP password so that they can SSH natively.\nThis is Shadow Ops, and points to the fact that BOSH ssh/scp instead of being features are detriments. Something entirely different may be called for.\n. @poblin-orange do you manage the VMs longterm with BOSH? There seem to be so many things that the notion of a \"job\" does not cover. To me it seems like they've gone a \"one size fits all route\" which is probably more of a reflection on the CPIs.\nAnd BOSH SSH & SCP are a bit worrying to me. --default_password seems kind of nasty. Plus these just aren't at all efficient means of managing a VM over SSH.\nThe agents also go unresponsive all the time. Can't every really figure out why from the logs. It's not that my jobs have failed, they're still running, it's just that I can't manage the VM through BOSH. Which means I have to go through my cloud provider and manually restart the VM. That works about 50% of the time.\nYes, the lack of imperative deployment steps is fairly crippling. It means I can't deploy databases without downtime. Maybe I could do something where I spun up 3 new VMs and connected them to the same persistent store (not sure if you can do that with BOSH).\nI think a lot of the goals that BOSH has are valiant, but there just seem to be much better products out there for doing that.\nInfrastructure as Code for Cloud Deployments -> Terraform\nConfiguration Management & Update -> Ansible, Puppet, Chef, etc.\nVersioning -> Containers (Not really much great out there for that ...)\n. ",
    "cnelson": "Would it be possible to depend on some other well known supported tool (like certstrap?) for generating certs?  I doubt most bosh shops need yet-another-tool that's responsible for cert generation.. +100. I'd love to see this plus an option (--strict ?) that would cause a failure if unused properties exist so it could be used to lint manifests. +100. I'd love to see this plus an option (--strict ?) that would cause a failure if unused properties exist so it could be used to lint manifests. AWS EBS now supports online resizing.  it would be great if bosh made use of this:  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-volume.html\n. AWS EBS now supports online resizing.  it would be great if bosh made use of this:  http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-expand-volume.html\n. @tylerschultz Our db was missing the index, adding it manually (CREATE UNIQUE INDEX blobstore_id_sha1_idx ON local_dns_blobs (blobstore_id, sha1); allowed the migration to continue.\nI believe this issued happened because v257 shipped this version of the db migration: https://github.com/cloudfoundry/bosh/commit/85bb2ecfce08bdc022be31ed8dbfcbfd02568164 and then v258 shipped this version (which added this index): https://github.com/cloudfoundry/bosh/commit/35587c4d58de9998f0e93b79b97860a7cbbc7a96 instead of adding a new migration file to create it.\nSequel tracks which migrations it's applied and will not apply them again, so the updated migration was not run when deploying v258.\nTL;DR This index will not exist for anyone who's on a director that ever ran release v257 \n. @tylerschultz Our db was missing the index, adding it manually (CREATE UNIQUE INDEX blobstore_id_sha1_idx ON local_dns_blobs (blobstore_id, sha1); allowed the migration to continue.\nI believe this issued happened because v257 shipped this version of the db migration: https://github.com/cloudfoundry/bosh/commit/85bb2ecfce08bdc022be31ed8dbfcbfd02568164 and then v258 shipped this version (which added this index): https://github.com/cloudfoundry/bosh/commit/35587c4d58de9998f0e93b79b97860a7cbbc7a96 instead of adding a new migration file to create it.\nSequel tracks which migrations it's applied and will not apply them again, so the updated migration was not run when deploying v258.\nTL;DR This index will not exist for anyone who's on a director that ever ran release v257 \n. This would be very useful for having jobs know if other jobs like metron are locally installed and available for use!\nDreaming about bosh providing unix sockets for jobs on the same host to communicate with each other :) . This would be very useful for having jobs know if other jobs like metron are locally installed and available for use!\nDreaming about bosh providing unix sockets for jobs on the same host to communicate with each other :) . ",
    "domdom82": "I can confirm that 3262.5 contains kernel 4.4.0-31.\nI think I will use that one for now. I just wonder why later versions switched back to the older kernel 3.19?\n. hi I just stumbled over this exact issue. Confused the heck out of me until I realized it had nothing to do with my stemcells section but with missing  cloud-config upload to director. The message should really point that out.. @drnic feel free to update my PR :). ",
    "amuessig": "What's the timeline for this feature? For me, this sounds quite important.. That also applies for stemcells. I just experienced that. However, for stemcells the stemcell is still shown in bosh stemcells and it is also available in vcenter. I had to re-upload it bosh upload-stemcell <name> --fix.\nBOSH Version: 268.4.0\nvsphere cpi: 52. ",
    "Prospecta": "Hi,\nAttempted it again with an exact vapp_storage_profile but still receiving the same error.  I have attached the debug log in case there's anything obvious in there.\nThanks\nbosh.log.zip\n. Hi,\nAttempted it again with an exact vapp_storage_profile but still receiving the same error.  I have attached the debug log in case there's anything obvious in there.\nThanks\nbosh.log.zip\n. Hi,\nAny idea how I can determine that the problem is related to independent disks if I can't create them from the vCloud portal?\nThanks\n. Hi,\nAny idea how I can determine that the problem is related to independent disks if I can't create them from the vCloud portal?\nThanks\n. Hi,\nSo I've been engaging with VMware support this week and they suggested we switch off SDRS for the datastore cluster that Bosh is being deployed to.  Apparently there is an SDRS placement issue which affects version 8.0.1 of vCloud.\nI have attempted the deployment of Bosh again (with SDRS turned off) and it has run successfully.\nI have just posed the question to VMware to understand the impact of turning off this feature when deploying additional VMs to the datastore cluster.  I'll let you know what they come back with.\nCheers\n. Hi,\nSo I've been engaging with VMware support this week and they suggested we switch off SDRS for the datastore cluster that Bosh is being deployed to.  Apparently there is an SDRS placement issue which affects version 8.0.1 of vCloud.\nI have attempted the deployment of Bosh again (with SDRS turned off) and it has run successfully.\nI have just posed the question to VMware to understand the impact of turning off this feature when deploying additional VMs to the datastore cluster.  I'll let you know what they come back with.\nCheers\n. ",
    "gdenn": "I just encountered the same problem. In our case we have an automation system that does BOSH deployments in parallel (three at the time). All of my three deployments used a new release of logstash and two of them failed with the get lock error message. The first deployments succeeded.\nOn our vSphere system with our compilation VMS we experience that compilation times for single releases often are above 15 minutes.. ",
    "gossion": "Having some issues with this PR, close it before the issues are fixed.\n. Having some issues with this PR, close it before the issues are fixed.\n. Hi @cppforlife, can you help to merge this PR? it is for Azure multiple network interfaces.\n. Hi @cppforlife, can you help to merge this PR? it is for Azure multiple network interfaces.\n. Hi @ljfranklin  and @cunnie, we found the issue on Azure and the fix is only on Azure. See more detail in link. . It is not a bosh issue. \n; is a sh command seperator, I should ' or \" to wrap the string. like \nbosh -e azure -d cf attach-disk nats/d4afe240-ec01-4d27-9b9b-aeac38cf64f5 'caching:None;disk_name:bosh-disk-data-91bf7a1b-ada2-47d3-80bd-b07696bb60bf;resource_group_name:guwe1205'\nclose this issue.. It is not a bosh issue. \n; is a sh command seperator, I should ' or \" to wrap the string. like \nbosh -e azure -d cf attach-disk nats/d4afe240-ec01-4d27-9b9b-aeac38cf64f5 'caching:None;disk_name:bosh-disk-data-91bf7a1b-ada2-47d3-80bd-b07696bb60bf;resource_group_name:guwe1205'\nclose this issue.. Thanks for response, @luan .\nThe run.log is for another issue, sorry for misleading. \nI have renamed folder in that github repo to avoid confusion. New folder is this case is https://github.com/gossion/logs/tree/master/issue-2043/, all logs in /var/vcap/sys/log are uploaded, and /var/vcap/monit/ is also uploaded to this folder.\nFor this case, we did not have a run.log (BOSH_LOG_PATH was not set). \nmonit summary shows that everything is running, see logs as below:\n```\nazureuser@devbox-8-28-7-34-11:~$ bosh -e azure tasks -r\nUsing environment '10.0.0.4' as client 'admin'\nID  State   Started At                    Last Activity At              User   Deployment  Description      Result\n7   queued  Thu Jan  1 00:00:00 UTC 1970  Tue Aug 28 15:08:54 UTC 2018  admin  -           create release   -\n6   done    Tue Aug 28 15:09:17 UTC 2018  Tue Aug 28 15:09:37 UTC 2018  admin  -           create release   Created release 'bpm/0.9.0'\n5   done    Tue Aug 28 15:08:25 UTC 2018  Tue Aug 28 15:09:18 UTC 2018  admin  -           create release   Created release 'silk/2.10.0'\n4   done    Tue Aug 28 15:09:18 UTC 2018  Tue Aug 28 15:09:21 UTC 2018  admin  -           create release   Created release 'bosh-dns-aliases/0.0.2'\n3   done    Tue Aug 28 15:08:25 UTC 2018  Tue Aug 28 15:09:17 UTC 2018  admin  -           create release   Created release 'loggregator-agent/2.0'\n2   done    Tue Aug 28 15:04:57 UTC 2018  Tue Aug 28 15:17:19 UTC 2018  admin  -           create stemcell  /stemcells/bosh-azure-hyperv-ubuntu-trusty-go_agent/3586.36\n1   done    Tue Aug 28 15:04:39 UTC 2018  Tue Aug 28 15:04:56 UTC 2018  admin  -           create release   Created release 'bosh-dns/1.7.0'\n7 tasks\nSucceeded\nazureuser@devbox-8-28-7-34-11:~$ ./connect_director_vm.sh\nUnauthorized use is strictly prohibited. All access and activity\nis subject to logging and monitoring.\nWelcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-128-generic x86_64)\n\nDocumentation:  https://help.ubuntu.com/\nLast login: Wed Aug 29 07:37:16 UTC 2018 from 10.0.0.100 on pts/0\nLast login: Thu Aug 30 01:36:19 2018 from 10.0.0.100\nbosh/0:~$ sudo su\nbosh/0:/home/jumpbox# monit summary\nThe Monit daemon 5.2.5 uptime: 1d 10h 36m\n\nProcess 'nats'                      running\nProcess 'postgres'                  running\nProcess 'blobstore_nginx'           running\nProcess 'director'                  running\nProcess 'worker_1'                  running\nProcess 'worker_2'                  running\nProcess 'worker_3'                  running\nProcess 'worker_4'                  running\nProcess 'director_scheduler'        running\nProcess 'director_sync_dns'         running\nProcess 'director_nginx'            running\nProcess 'health_monitor'            running\nProcess 'registry'                  running\nProcess 'uaa'                       running\nProcess 'credhub'                   running\nSystem 'system_localhost'           running\nbosh/0:/home/jumpbox# monit status\nThe Monit daemon 5.2.5 uptime: 1d 10h 36m\nProcess 'nats'\n  status                            running\n  monitoring status                 monitored\n  pid                               36883\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  7932\n  memory kilobytes total            7932\n  memory percent                    0.1%\n  memory percent total              0.1%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'postgres'\n  status                            running\n  monitoring status                 monitored\n  pid                               36922\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          34\n  memory kilobytes                  13760\n  memory kilobytes total            357140\n  memory percent                    0.1%\n  memory percent total              5.0%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'blobstore_nginx'\n  status                            running\n  monitoring status                 monitored\n  pid                               36984\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          2\n  memory kilobytes                  4852\n  memory kilobytes total            20612\n  memory percent                    0.0%\n  memory percent total              0.2%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'director'\n  status                            running\n  monitoring status                 monitored\n  pid                               37029\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          3\n  memory kilobytes                  53916\n  memory kilobytes total            252624\n  memory percent                    0.7%\n  memory percent total              3.5%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'worker_1'\n  status                            running\n  monitoring status                 monitored\n  pid                               38186\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  57956\n  memory kilobytes total            57956\n  memory percent                    0.8%\n  memory percent total              0.8%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'worker_2'\n  status                            running\n  monitoring status                 monitored\n  pid                               38203\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  58468\n  memory kilobytes total            58468\n  memory percent                    0.8%\n  memory percent total              0.8%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'worker_3'\n  status                            running\n  monitoring status                 monitored\n  pid                               38217\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  58348\n  memory kilobytes total            58348\n  memory percent                    0.8%\n  memory percent total              0.8%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'worker_4'\n  status                            running\n  monitoring status                 monitored\n  pid                               38236\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  58760\n  memory kilobytes total            58760\n  memory percent                    0.8%\n  memory percent total              0.8%\n  cpu percent                       0.4%\n  cpu percent total                 0.4%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'director_scheduler'\n  status                            running\n  monitoring status                 monitored\n  pid                               37126\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  64544\n  memory kilobytes total            64544\n  memory percent                    0.9%\n  memory percent total              0.9%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'director_sync_dns'\n  status                            running\n  monitoring status                 monitored\n  pid                               37167\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  64988\n  memory kilobytes total            64988\n  memory percent                    0.9%\n  memory percent total              0.9%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'director_nginx'\n  status                            running\n  monitoring status                 monitored\n  pid                               37193\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          2\n  memory kilobytes                  4604\n  memory kilobytes total            20488\n  memory percent                    0.0%\n  memory percent total              0.2%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'health_monitor'\n  status                            running\n  monitoring status                 monitored\n  pid                               40348\n  parent pid                        1\n  uptime                            1d 10h 19m\n  children                          0\n  memory kilobytes                  37968\n  memory kilobytes total            37968\n  memory percent                    0.5%\n  memory percent total              0.5%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  port response time                0.001s to localhost:25923/healthz [HTTP via TCP]\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'registry'\n  status                            running\n  monitoring status                 monitored\n  pid                               37286\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          0\n  memory kilobytes                  31920\n  memory kilobytes total            31920\n  memory percent                    0.4%\n  memory percent total              0.4%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'uaa'\n  status                            running\n  monitoring status                 monitored\n  pid                               37320\n  parent pid                        1\n  uptime                            1d 10h 35m\n  children                          10\n  memory kilobytes                  415456\n  memory kilobytes total            430332\n  memory percent                    5.8%\n  memory percent total              6.0%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  port response time                0.000s to localhost:8443/healthz [HTTP via TCP]\n  data collected                    Thu Aug 30 01:36:26 2018\nProcess 'credhub'\n  status                            running\n  monitoring status                 monitored\n  pid                               38398\n  parent pid                        1\n  uptime                            1d 10h 34m\n  children                          12\n  memory kilobytes                  630636\n  memory kilobytes total            645668\n  memory percent                    8.8%\n  memory percent total              9.0%\n  cpu percent                       0.0%\n  cpu percent total                 0.0%\n  data collected                    Thu Aug 30 01:36:26 2018\nSystem 'system_localhost'\n  status                            running\n  monitoring status                 monitored\n  load average                      [0.06] [0.12] [0.05]\n  cpu                               0.8%us 0.3%sy 0.8%wa\n  memory usage                      1972724 kB [27.6%]\n  swap usage                        30028 kB [0.4%]\n  data collected                    Thu Aug 30 01:36:26 2018\nbosh/0:/home/jumpbox#\n```\nThe error happens several times recently, but is not always reproduced. Last time we saw the issue, we redeployed the director, and then the issue was gone. . Thanks @jfmyers9 . \nUnfortunately, the test environment was deleted. I will keep monitoring the error, and get delayed_jobs for you if it happens again.. ",
    "evandbrown": "An important note:\nThe stemcell build fails for me when applied to the develop branch (though I'm submitting it to this branch per contribution guidelines). Specifically, the /dev/null socket that rsyslog is expected to create never appears.\nWhen cherrypicked onto master, the stemcell builds and performs as expected.\n. This stage must be run for the Google stemcell to provide a correct /etc/init/rsyslog.conf\n. This declaration is removed as /var/log never appears on the Google stemcell. There was discussion that bosh-agent may be expected to mount this location to ephemeral disk, which does not exist on GCE.\n. Prevent the Google daemon from setting SSH host keys, which races with BOSH's own configuration.\n. ",
    "Victor-Paschenko": "Thanks for answering @dpb587-pivotal , I will give it a try. \n. Thanks for answering @dpb587-pivotal , I will give it a try. \n. ",
    "johnsonj": "You can also try using a recent release of the 3262 stemcell, eg: https://bosh.io/d/stemcells/bosh-google-kvm-ubuntu-trusty-go_agent?v=3262.19.\nWe've updated our cloudfoundry docs in the cpi to properly reflect this. \n. I'm seeing MBR formatted persistent disks from bosh by default on GCP (bosh-google-kvm-ubuntu-trusty-go_agent-3468.13):\n```\nmaster/8679c9bb-d655-4e6c-8a53-0468599f7c50:~$ sudo parted -l\nModel: Google PersistentDisk (scsi)\nDisk /dev/sda: 21.5GB\nSector size (logical/physical): 512B/4096B\nPartition Table: msdos\nNumber  Start   End     Size    Type     File system     Flags\n 1      32.3kB  3071MB  3071MB  primary  ext4\n 2      3071MB  6951MB  3880MB  primary  linux-swap(v1)\n 3      6951MB  21.5GB  14.5GB  primary  ext4\nModel: Google PersistentDisk (scsi)\nDisk /dev/sdb: 5369MB\nSector size (logical/physical): 512B/4096B\nPartition Table: msdos\nNumber  Start  End     Size    Type     File system  Flags\n 1      512B   5364MB  5364MB  primary  ext4\n```\n@cunnie is this different from your expectations?\n@ansd It looks like there's a few options to force the agent to use parted instead of sfdisk today:\n- Use a persistent disk over 2TB (src). This will certainly provide a GPT formatted disk. \n- Use the Ubuntu Xenial stemcell (src). This may work, assuming parted has a different set of defaults. \n. ",
    "xyloman": "Great questions and thanks for the response. This is an on premise installation of openstack. The Operations team for openstack where going to upload the image manually during the proof of concept phase. After which time we hope to leverage the bodh automation for updating the stem cells. The long term goal is to go with the setup similar to what is documented. However, I could still see a need to reference an image that might have already been uploaded wouldn't we still need the light stemcell in that case? \n. Great questions and thanks for the response. This is an on premise installation of openstack. The Operations team for openstack where going to upload the image manually during the proof of concept phase. After which time we hope to leverage the bodh automation for updating the stem cells. The long term goal is to go with the setup similar to what is documented. However, I could still see a need to reference an image that might have already been uploaded wouldn't we still need the light stemcell in that case? \n. On a new Ubuntu 16.04 (ami-1ee65166) spun up on AWS.  Installed the dependencies \nsudo apt-get install -y build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt-dev libxml2-dev libssl-dev libreadline6 libreadline6-dev libyaml-dev libsqlite3-dev sqlite3 \nand received the following error \n```\ncreating stemcell (bosh-aws-xen-hvm-ubuntu-trusty-go_agent 3468.13):\n  CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Unknown\",\"message\":\"cannot load such file -- openssl\",\"ok_to_retry\":false}\n```\nHowever, when I leverage a Ubuntu 14.04 (ami-0c9f2974) I am able to successfully perform the bosh create-env such as https://github.com/cloudfoundry/bosh-deployment/ \nbosh create-env /tmp/bosh-deployment/bosh.yml \\\n    --state=bosh-state.json \\\n    --vars-store=bosh-creds.yml \\\n    -o /tmp/bosh-deployment/aws/cpi.yml \\\n    -o /tmp/bosh-deployment/aws/iam-instance-profile.yml \\\n    -o /tmp/bosh-deployment/uaa.yml \\\n    -o /tmp/bosh-deployment/credhub.yml \\\n    -o /tmp/bosh-deployment/jumpbox-user.yml \\\n    -v iam_instance_profile=${iam_instance_profile} \\\n    -v access_key_id=${access_key_id} \\\n    -v secret_access_key=${secret_access_key} \\\n    -v director_name=bosh \\\n    -v internal_cidr=${internal_cidr} \\\n    -v internal_gw=${internal_gw} \\\n    -v internal_ip=${internal_ip} \\\n    -v region=${region} \\\n    -v az=${az} \\\n    -v default_key_name=${default_key_name} \\\n    -v default_security_groups=[${default_security_groups}] \\\n    --var-file private_key=bosh.pem \\\n    -v subnet_id=${subnet_id} \\\n    -v credhub_encryption_password=${credhub_encryption_password}. On a new Ubuntu 16.04 (ami-1ee65166) spun up on AWS.  Installed the dependencies \nsudo apt-get install -y build-essential zlibc zlib1g-dev ruby ruby-dev openssl libxslt-dev libxml2-dev libssl-dev libreadline6 libreadline6-dev libyaml-dev libsqlite3-dev sqlite3 \nand received the following error \n```\ncreating stemcell (bosh-aws-xen-hvm-ubuntu-trusty-go_agent 3468.13):\n  CPI 'create_stemcell' method responded with error: CmdError{\"type\":\"Unknown\",\"message\":\"cannot load such file -- openssl\",\"ok_to_retry\":false}\n```\nHowever, when I leverage a Ubuntu 14.04 (ami-0c9f2974) I am able to successfully perform the bosh create-env such as https://github.com/cloudfoundry/bosh-deployment/ \nbosh create-env /tmp/bosh-deployment/bosh.yml \\\n    --state=bosh-state.json \\\n    --vars-store=bosh-creds.yml \\\n    -o /tmp/bosh-deployment/aws/cpi.yml \\\n    -o /tmp/bosh-deployment/aws/iam-instance-profile.yml \\\n    -o /tmp/bosh-deployment/uaa.yml \\\n    -o /tmp/bosh-deployment/credhub.yml \\\n    -o /tmp/bosh-deployment/jumpbox-user.yml \\\n    -v iam_instance_profile=${iam_instance_profile} \\\n    -v access_key_id=${access_key_id} \\\n    -v secret_access_key=${secret_access_key} \\\n    -v director_name=bosh \\\n    -v internal_cidr=${internal_cidr} \\\n    -v internal_gw=${internal_gw} \\\n    -v internal_ip=${internal_ip} \\\n    -v region=${region} \\\n    -v az=${az} \\\n    -v default_key_name=${default_key_name} \\\n    -v default_security_groups=[${default_security_groups}] \\\n    --var-file private_key=bosh.pem \\\n    -v subnet_id=${subnet_id} \\\n    -v credhub_encryption_password=${credhub_encryption_password}. ",
    "dennisjbell": "This is also happening in the latest bosh-lite available (https://github.com/cloudfoundry/bosh-lite.git , commit:566dbe0, last updated Aug 31)\nIn this case, using cf-245.  If bosh-lite cannot support this, what is the latest version of cf that is compatible with the current state of bosh-lite?\n. ",
    "MatthiasWinzeler": "@cppforlife Good idea, I will change the default to \"\" then.\n@tjvman My cpi config looked like this (omitted the third cpi for better readability):\ncpis:\n- name: openstack1\n  type: openstack\n  properties:\n    openstack:\n      auth_url: https://api.openstack1:5000/v2.0/tokens\n      tenant: openstack1\n      username: openstack1-user\n      api_key: openstack1-password\n      connection_options:\n        ssl_verify_peer: true\n        read_timeout: 300\n        write_timeout: 300\n      config_drive: disk\n      use_dhcp: false\n      default_key_name: cloudfoundry\n      private_key: /home/ubuntu/bosh-workspace/ssh/cloudfoundry\n      default_security_groups: []\n      human_readable_vm_names: true\n      endpoint_type: publicURL\n      state_timeout: 300\n      boot_from_volume: false\n      stemcell_public_visibility: false\n      wait_resource_poll_interval: 5\n      ignore_server_availability_zone: false\n- name: openstack2\n  type: openstack\n  properties:\n    openstack:\n      auth_url: https://api.openstack2:5000/v2.0/tokens\n      tenant: openstack2\n      username: openstack2-user\n      api_key: openstack2-password\n      connection_options:\n        ssl_verify_peer: true\n        read_timeout: 300\n        write_timeout: 300\n      config_drive: disk\n      use_dhcp: false\n      default_key_name: cloudfoundry\n      private_key: /home/ubuntu/bosh-workspace/ssh/cloudfoundry\n      default_security_groups: []\n      human_readable_vm_names: true\n      endpoint_type: publicURL\n      state_timeout: 300\n      boot_from_volume: false\n      stemcell_public_visibility: false\n      wait_resource_poll_interval: 5\n      ignore_server_availability_zone: false\nSome of the properties are just defaults of the openstack cpi. They are now picked up by default (see @voelzmo 's comment above) and could most likely be omitted.\n. Thanks for reviewing the changes! \nI pushed most of the requested changes and will complete the remaining open points in the coming days.\n. > Always using the newest cloud_config when performing CPI calls breaks bosh recreate functionality (may break other things). We would like to preserve the versioning functionality for the CloudConfig, but from your initial PR comments it seemed like always using the latest CloudConfig was necessary. Can you give us some more context on why the newest CloudConfig has to be used so we can work together and figure out a solution that allows both the new PR and desired pre-existing functionality to work?\nIt's best explained when looking at this integration test which tests the switch from non-cpi-config to cpi-config:\n- Deploy without cpi-config\n- Create cpi-config\n- Update cloud-config (add CPIs to AZs)\n- Deploy\nMy assumption was that this works out of the box. Without my ugly fix, this did not work - BOSH used the older cloud config while deploying, so it did not know which AZ uses which CPI and thus deployed to the director cpi.\nYou can easily reproduce this by removing my hack and run the integration test above.\nMaybe this is some other bug?\n. Pushed some last improvements - it's ready now for another review - and merged current develop. integration-1.9-postgres has some broken specs, but it's the same on develop.\n. @mfine30 this is not longer an issue for us, since we were able to migrate it by scaling down/up before. Feel free to close it if it's ok for you.. Could we fix this by using the pid_guard helper function that is used in a lot of bosh releases?\ni.e. https://github.com/cloudfoundry/cf-release/blob/ef70b3f389a44bd5065745b7b30bfce45a8acccd/jobs/collector/templates/collector_ctl.erb#L16 / https://github.com/cloudfoundry/cf-release/blob/690e8155510acac067193f1076c74ce6312c0479/src/common/utils.sh#L7\nThis would ensure that the PID of the ctlis not writing its PID when nginx is already running (the exact error case Lafunamor describes).. We're aware that - with the current implementation - we're able to do our migration by moving one zone at a time. However, it would be easier to do this in one task (since we have to do it for 30 deployments...). @cppforlife Each zone has its own, dedicated network subnet. IPs will change, but we're cautiously optimistic that most software we use does not rely on it. From the main CF ecosystem (i.e. CF, Diego, etcd, consul, cf-mysql et al) the only one I know of that breaks if IPs change is rabbitmq.\nTimeline: We'll start migrating end of this year.. Makes sense. I removed the check for context in the openstack cpi: https://github.com/swisscom/bosh-openstack-cpi-release/commit/77970370f428447586925001414c4a84cb892d6d\n. No, there's not. I changed it.\n. It's not only cpi, but also cid (see further below in the spec). Should I add both as separate arguments? I prefer the params way since it's more readable.\n. I kept it a generic runtime exception on purpose - it should always happen during develop time and bubble up to the developer (it means he didn't bind the model before using cid_for_az). \nWhat's the preferred way in such situations? Should we still introduce a more specific error? Does it need to be put in errors.rb (even though it should never leave the system)?\n. ",
    "jutkko": "It's possible that, when there's a \"latest\" version, bosh treats the actual version as more recent. To find out, export the actual manifest (bosh download manifest <some-release> > <temporary-file>) and compare the versions to those in your manifest file. If there's a difference, that's what bosh is complaining about. It would be better if bosh told us which value in the manifest it was unhappy about.. ",
    "henryaj": "Thanks!\n. cc @kirederik. I don't think that addresses our issue. Configuring the ssh_tunnel cause shouldn't cause the deployment to fail (and this wasn't happening on previous versions of the Director + CPI). . Ah, okay - thanks @dpb587-pivotal, that makes sense.. ",
    "luan": "The commit above fixes bosh start on errand instances. The described issue will still occur unless you bosh start your errand. That will be the supported way to get out of this state on the next version of BOSH.\nThe reason for this is that we found it to be surprising for a user who has bosh stop --hard an errand instance to see that same instance run the errand later with bosh run-errand. Especially when the errand is located in multiple instances. The behavior when you bosh stop --hard is to not re-create that instance on a deploy, so it's natural that we wouldn't do that when bosh run-errand either.\n. The commit above fixes bosh start on errand instances. The described issue will still occur unless you bosh start your errand. That will be the supported way to get out of this state on the next version of BOSH.\nThe reason for this is that we found it to be surprising for a user who has bosh stop --hard an errand instance to see that same instance run the errand later with bosh run-errand. Especially when the errand is located in multiple instances. The behavior when you bosh stop --hard is to not re-create that instance on a deploy, so it's natural that we wouldn't do that when bosh run-errand either.\n. closing this since cloudfoundry/bosh-linux-stemcell-builder#70 was merged.. closing this since cloudfoundry/bosh-linux-stemcell-builder#70 was merged.. Hi @mattcui,\nWhile we haven't specifically seen this happen, it seems like that change would potentially cause what you're seeing. But even though that's true, we expect users of BOSH to be upgrading their stemcells more often than their directors. Moreover, going from 263 to 267 is a huge leap and it will often contain enough changes that making guarantees about not updating the jobs interface becomes really hard.\nSo, while we agree that this is something that caused all your jobs to be updated, we don't intend to focus efforts on guaranteeing that this isn't going to happen on this version or in the future. Thanks for reporting!\nBest,\n@luan && @jfmyers9, CF BOSH Team. Hi @mattcui,\nWhile we haven't specifically seen this happen, it seems like that change would potentially cause what you're seeing. But even though that's true, we expect users of BOSH to be upgrading their stemcells more often than their directors. Moreover, going from 263 to 267 is a huge leap and it will often contain enough changes that making guarantees about not updating the jobs interface becomes really hard.\nSo, while we agree that this is something that caused all your jobs to be updated, we don't intend to focus efforts on guaranteeing that this isn't going to happen on this version or in the future. Thanks for reporting!\nBest,\n@luan && @jfmyers9, CF BOSH Team. Looks like your problem was resolved @pulpham? Thanks @voelzmo for the help.\nFeel free to re-open if this is still an issue!. Hi @andyliuliming,\nI'm not sure we completely understand you proposal. Could you show us in which methods that property is passed in the CPI? We see a group that is passed to create_vm here.\nAs for the \"contract\" goes, do you mean it should be documented in https://bosh.io/docs/cpi-api-v1-method/create-vm/ as something that will be present on create_vm calls? If that's the case, we agree that documenting that would be good if we intend to keep that property around (and we do).\nBest,\n@luan & @mikexuu, CF BOSH Team\n. Hi @gossion,\nWe see some interesting looking errors in your run.log and the agent logs on the director that don't look normal to us:\n2018-02-03_13:02:19.75449 panic: runtime error: invalid memory address or nil pointer dereference\n2018-02-03_13:02:19.75450 [signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x850e40]\n2018-02-03_13:02:19.75450 \n2018-02-03_13:02:19.75450 goroutine 9 [running]:\n2018-02-03_13:02:19.75474 github.com/cloudfoundry/bosh-agent/infrastructure.(*MultiSourceMetadataService).GetPublicKey(0xc4201409f0, 0xc4200d1b80, 0xc57020, 0xc420010af0, 0xc5cbc0)\n2018-02-03_13:02:19.75475   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/infrastructure/multi_source_metadata_service.go:17 +0x30\n2018-02-03_13:02:19.75475 github.com/cloudfoundry/bosh-agent/infrastructure.ComplexSettingsSource.PublicSSHKeyForUsername(0xc5c200, 0xc4201409f0, 0xc513e0, 0xc420059020, 0x9d0b81, 0x15, 0xc5d280, 0xc420017a90, 0x9c22a4, 0x4, ...)\n2018-02-03_13:02:19.75475   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/infrastructure/complex_settings_source.go:31 +0x31\n2018-02-03_13:02:19.75475 github.com/cloudfoundry/bosh-agent/infrastructure.(*ComplexSettingsSource).PublicSSHKeyForUsername(0xc420142680, 0x9c22a4, 0x4, 0xc58660, 0xc420142480, 0x0, 0x0)\n2018-02-03_13:02:19.75476   <autogenerated>:10 +0x7c\n2018-02-03_13:02:19.75500 github.com/cloudfoundry/bosh-agent/settings.(*settingsService).PublicSSHKeyForUsername(0xc42012b520, 0x9c22a4, 0x4, 0xc420140960, 0x25, 0xc420140960, 0x25)\n2018-02-03_13:02:19.75501   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/settings/service.go:59 +0x4e\n2018-02-03_13:02:19.75501 github.com/cloudfoundry/bosh-agent/agent.bootstrap.Run(0xc5f260, 0xc420142200, 0xc625e0, 0xc420083040, 0x9c6867, 0x9, 0xc59660, 0xc42012b520, 0xc5d280, 0xc420017a90, ...)\n2018-02-03_13:02:19.75502   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/agent/bootstrap.go:50 +0x1b5\n2018-02-03_13:02:19.75502 github.com/cloudfoundry/bosh-agent/agent.(*bootstrap).Run(0xc420010be0, 0xc420083040, 0x9c6867)\n2018-02-03_13:02:19.75502   <autogenerated>:16 +0x5c\n2018-02-03_13:02:19.75502 github.com/cloudfoundry/bosh-agent/app.(*app).Setup(0xc42000ab00, 0x0, 0x0, 0x7ffeb6038f22, 0x6, 0x9c6867, 0x9, 0x9c2c3d, 0x5, 0x7ffeb6038f2c, ...)\n2018-02-03_13:02:19.75504   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/app/app.go:112 +0xd2c\n2018-02-03_13:02:19.75505 main.runAgent.func1(0xc5d280, 0xc420017a90, 0x0, 0x0, 0x7ffeb6038f22, 0x6, 0x9c6867, 0x9, 0x9c2c3d, 0x5, ...)\n2018-02-03_13:02:19.75505   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/main/agent.go:30 +0x226\n2018-02-03_13:02:19.75505 created by main.runAgent\n2018-02-03_13:02:19.75505   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/main/agent.go:43 +0xb9\nAnd \n[CLI] 2018/02/03 13:05:37 ERROR - Deploying: Creating instance 'bosh/0': Updating instance disks: Updating disks: Deploying disk: Mounting disk: Sending 'get_task' to the agent: Agent responded with error: Action Failed get_task: Task 0dd59011-9dfa-45c6-6f16-b6c046d87dfe result: Persistent disk with volume id 'caching:None;disk_name:bosh-disk-data-bae04939-b5dd-4330-b36e-8cc1e65505ef;resource_group_name:ICA-RG-ManagedCF-2-3-4-38-57' could not be found\nCould you see in that director VM if the jobs are healthy and running? A monit status output would be helpful here. Also, watch monit summary for a bit to see if there are any jobs that are failing intermittently.\nIt seems like it could be related to some credential/key configuration on your director. Are you able to reproduce this reliably by creating a new director on Azure?\nThanks,\n@luan & @jaresty, CF BOSH. Hi @gossion,\nWe see some interesting looking errors in your run.log and the agent logs on the director that don't look normal to us:\n2018-02-03_13:02:19.75449 panic: runtime error: invalid memory address or nil pointer dereference\n2018-02-03_13:02:19.75450 [signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x850e40]\n2018-02-03_13:02:19.75450 \n2018-02-03_13:02:19.75450 goroutine 9 [running]:\n2018-02-03_13:02:19.75474 github.com/cloudfoundry/bosh-agent/infrastructure.(*MultiSourceMetadataService).GetPublicKey(0xc4201409f0, 0xc4200d1b80, 0xc57020, 0xc420010af0, 0xc5cbc0)\n2018-02-03_13:02:19.75475   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/infrastructure/multi_source_metadata_service.go:17 +0x30\n2018-02-03_13:02:19.75475 github.com/cloudfoundry/bosh-agent/infrastructure.ComplexSettingsSource.PublicSSHKeyForUsername(0xc5c200, 0xc4201409f0, 0xc513e0, 0xc420059020, 0x9d0b81, 0x15, 0xc5d280, 0xc420017a90, 0x9c22a4, 0x4, ...)\n2018-02-03_13:02:19.75475   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/infrastructure/complex_settings_source.go:31 +0x31\n2018-02-03_13:02:19.75475 github.com/cloudfoundry/bosh-agent/infrastructure.(*ComplexSettingsSource).PublicSSHKeyForUsername(0xc420142680, 0x9c22a4, 0x4, 0xc58660, 0xc420142480, 0x0, 0x0)\n2018-02-03_13:02:19.75476   <autogenerated>:10 +0x7c\n2018-02-03_13:02:19.75500 github.com/cloudfoundry/bosh-agent/settings.(*settingsService).PublicSSHKeyForUsername(0xc42012b520, 0x9c22a4, 0x4, 0xc420140960, 0x25, 0xc420140960, 0x25)\n2018-02-03_13:02:19.75501   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/settings/service.go:59 +0x4e\n2018-02-03_13:02:19.75501 github.com/cloudfoundry/bosh-agent/agent.bootstrap.Run(0xc5f260, 0xc420142200, 0xc625e0, 0xc420083040, 0x9c6867, 0x9, 0xc59660, 0xc42012b520, 0xc5d280, 0xc420017a90, ...)\n2018-02-03_13:02:19.75502   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/agent/bootstrap.go:50 +0x1b5\n2018-02-03_13:02:19.75502 github.com/cloudfoundry/bosh-agent/agent.(*bootstrap).Run(0xc420010be0, 0xc420083040, 0x9c6867)\n2018-02-03_13:02:19.75502   <autogenerated>:16 +0x5c\n2018-02-03_13:02:19.75502 github.com/cloudfoundry/bosh-agent/app.(*app).Setup(0xc42000ab00, 0x0, 0x0, 0x7ffeb6038f22, 0x6, 0x9c6867, 0x9, 0x9c2c3d, 0x5, 0x7ffeb6038f2c, ...)\n2018-02-03_13:02:19.75504   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/app/app.go:112 +0xd2c\n2018-02-03_13:02:19.75505 main.runAgent.func1(0xc5d280, 0xc420017a90, 0x0, 0x0, 0x7ffeb6038f22, 0x6, 0x9c6867, 0x9, 0x9c2c3d, 0x5, ...)\n2018-02-03_13:02:19.75505   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/main/agent.go:30 +0x226\n2018-02-03_13:02:19.75505 created by main.runAgent\n2018-02-03_13:02:19.75505   /tmp/build/9674af12/gopath/src/github.com/cloudfoundry/bosh-agent/main/agent.go:43 +0xb9\nAnd \n[CLI] 2018/02/03 13:05:37 ERROR - Deploying: Creating instance 'bosh/0': Updating instance disks: Updating disks: Deploying disk: Mounting disk: Sending 'get_task' to the agent: Agent responded with error: Action Failed get_task: Task 0dd59011-9dfa-45c6-6f16-b6c046d87dfe result: Persistent disk with volume id 'caching:None;disk_name:bosh-disk-data-bae04939-b5dd-4330-b36e-8cc1e65505ef;resource_group_name:ICA-RG-ManagedCF-2-3-4-38-57' could not be found\nCould you see in that director VM if the jobs are healthy and running? A monit status output would be helpful here. Also, watch monit summary for a bit to see if there are any jobs that are failing intermittently.\nIt seems like it could be related to some credential/key configuration on your director. Are you able to reproduce this reliably by creating a new director on Azure?\nThanks,\n@luan & @jaresty, CF BOSH. Sorry for accidentally closing and re-opening!. Sorry for accidentally closing and re-opening!. @shausy it would also be good to see a complete stack trace from bosh task 23191 --debug. Also, would you be able to provide us with what steps you took to get to this state? A sequence of commands from bosh deploy until you got to the error would be great.. @shausy it would also be good to see a complete stack trace from bosh task 23191 --debug. Also, would you be able to provide us with what steps you took to get to this state? A sequence of commands from bosh deploy until you got to the error would be great.. Closing this based on commit above. Should be good when we release the next version.. Closing this based on commit above. Should be good when we release the next version.. We merged this on https://github.com/cloudfoundry/bosh/commit/3d7708860dc2db720c071c1ddd548041718b6482. Not sure why github didn't pick up on the merge.. We merged this on https://github.com/cloudfoundry/bosh/commit/3d7708860dc2db720c071c1ddd548041718b6482. Not sure why github didn't pick up on the merge.. It's not. Sorry. This semaphore started out supporting multiple waits at a time and I forgot to remove that part.\n. Looks reasonable to me at first sight. Are you working on getting it merged?\nOn Thu, Dec 18, 2014 at 4:41 PM, Stev Witzel notifications@github.com\nwrote:\n\n\n\n@mon.synchronize do\n@uwait.wait while @max > 0 and @count == @max\n@dwait.signal if @count == 0\n@count += 1\nend\nend\nend\n  +\ndef signal(number = 1)\nif (number > 1)\nnumber.times { down!(1) }\ncount\nelse\n@mon.synchronize do\n@dwait.wait while @count == 0\n@uwait.signal if @count == @max\n  @luan: What do you think about something like this ...\n\ndef wait\n      @mon.synchronize do\n        @uwait.wait_while do\n          @max > 0 and @count == @max\n        end\n        @count += 1\n        count\n      end\n    end\n    def signal\n      @mon.synchronize do\n        @count -= 1\n        @uwait.signal\n        count\n      end\n    end\nSeems cleaner to me.\nReply to this email directly or view it on GitHub:\nhttps://github.com/cloudfoundry/bosh/pull/684/files#r22083155\n. Yeah. I'm on vacation today otherwise I could patch it up. Thanks\n\n\nOn Thu, Dec 18, 2014 at 4:51 PM, Stev Witzel notifications@github.com\nwrote:\n\n\n\n@mon.synchronize do\n@uwait.wait while @max > 0 and @count == @max\n@dwait.signal if @count == 0\n@count += 1\nend\nend\nend\n  +\ndef signal(number = 1)\nif (number > 1)\nnumber.times { down!(1) }\ncount\nelse\n@mon.synchronize do\n@dwait.wait while @count == 0\n@uwait.signal if @count == @max\n  Yep. It's a cool feature!\n\n\nReply to this email directly or view it on GitHub:\n  https://github.com/cloudfoundry/bosh/pull/684/files#r22083611\n. Note to merger: rename this migration to be the most recent one (use current date).. Could you address this TODO? Feels like a specific error would be helpful here.. Didn't see a unit test for this when the network doesn't exist. Do you mind adding one?. Regardless of when the update is actually implemented, I think we'd benefit from having a specific error here that is specifically raised and tested by a unit test in the case of the subnet not being in the database to avoid undefined behavior.. Didn't go through every single possibility of this class here, but it feels like the complexity of perform isn't being exercised enough by only these few tests. Do you mind taking another look and making sure we have coverage for at least most of the logical branches (conditionals, rescues, etc) in CreateNetworkStage?. Minor style thing, but since I'm already requesting changes here might as well mention it: we usually have an empty line before and around describes its, befores and multi-line lets.. Looks like some of these lets ended up not being used?. Could you move these into a top-level describe like describe 'network lifecycle then have the enabled/disabled as sub-contexts of that? I find it easier to browse spec files when the have a single top-level describe block that does the shared setup (like with_reset_sandbox_before_each(networks: { 'enable_cpi_management' => enable_cpi_management }) where enable_cpi_management is a let managed by the contexts).. Minor thing again: would you mind using a more descriptive name such as netmask_bits? I feel like nm can mean a lot of different things in this context and I actually had to browse around to expand the acronym in my head when reading this.. I didn't see any unit tests added for the changes made here, do you mind adding a few?. I didn't see any unit tests added for the changes made here, do you mind adding a few?. \n\n",
    "warroyo": "@dpb587 that is exactly what I am looking for, thank you for the information. I will keep an eye on the new cli.\n. ok, so i found that WEBDAV is supported(https://github.com/cloudfoundry/bosh/blob/c6c419f9eff844c35c8c7307e977427f05a23a51/src/bosh_cli/spec/assets/config/dav/config/final.yml). however there does not seem to be an option to trust a self signed cert or ignore it. my Artifactory uses a self signed cert.\nwhen i run bosh upload blobs it fails due to the self signed cert. is there an embedded cert store I can add my certificate to?\nupdate:\nfound the setting here : https://github.com/cloudfoundry/bosh/blob/c6c419f9eff844c35c8c7307e977427f05a23a51/src/blobstore_client/lib/blobstore_client/dav_blobstore_client.rb#L18\n. ok so I got this figured out. artifactory does work as on onsite blobstore. you have to use the webdav provider for the blobstore. also there is an option for ssl_verify_none that is not documented anywhere .\nhere are my configs:\nfinal.yml\n````\nblobstore:\n  provider: dav\n  options:\n    endpoint: https://myartifactory.com/artifactory/some-repo\n    ssl_no_verify: true\nfinal_name: my-release\n````\nprivate.yml\n````\nblobstore:\n  dav:\n    endpoint: https://myartifactory.com/artifactory/some-repo\n    user: **\n    password: *\n````\n@cppforlife any chance this could be added to the docs for future reference. . ",
    "h4xnoodle": "I think this should be enabled by default @mfine30 since there isn't a reason for it not to be. \nTo opt into this lifecycle event you must do two things: script and enable the flag. If it were on by default, omitting the script would be the same as the default being disabled.\nAdditionally, bosh-deployment ships with enable_post_deploy: true.. I think this should be enabled by default @mfine30 since there isn't a reason for it not to be. \nTo opt into this lifecycle event you must do two things: script and enable the flag. If it were on by default, omitting the script would be the same as the default being disabled.\nAdditionally, bosh-deployment ships with enable_post_deploy: true.. I believe this fixes your issue: https://github.com/cloudfoundry/bosh/commit/5d0e4832cf665c8b2b04996b7f79c8d0bcb0b001\nWe had the same, and we noticed it wasn't happening anymore. However, having blank plans still feels incorrect!. I believe this fixes your issue: https://github.com/cloudfoundry/bosh/commit/5d0e4832cf665c8b2b04996b7f79c8d0bcb0b001\nWe had the same, and we noticed it wasn't happening anymore. However, having blank plans still feels incorrect!. I really like this idea. \nIn addition to the suggestion to add bosh check-certificates it may also be a good idea to report any cert expiry dates after a bosh deploy, whether they are new or the same certs as a previous deploy. \nThe reason I would want to add this after a deploy is that this command could be the only one an operator uses, then leaves the system running.\nIf using healthmonitor with email set up, it may also be nice to receive an email about expiring certificates. . I really like this idea. \nIn addition to the suggestion to add bosh check-certificates it may also be a good idea to report any cert expiry dates after a bosh deploy, whether they are new or the same certs as a previous deploy. \nThe reason I would want to add this after a deploy is that this command could be the only one an operator uses, then leaves the system running.\nIf using healthmonitor with email set up, it may also be nice to receive an email about expiring certificates. . @voelzmo Hi Marco, thanks for your report.\nIt seems that BOSH is behaving by design here. \nAssuming we understand the issue: the only CF deployment attempted has failed. There have been link provider intents created. BOSH leaves any created intents behind during a failed deployment, to assist in debugging. A subsequent successful deployment will delete any unused links/providers at the end. Another deployment wishing to consume links from a failed deployment, will have no choice but to use the created links in the failed provider deployment.\nWe don't expect reliable behaviour for a cross-deployment consumer when the provider deployment has failed. \nWas there a successful deployment of cf after the failed one, and before attempting to consume again? \nWhat version of the director is being used?. @voelzmo Hi Marco, thanks for your report.\nIt seems that BOSH is behaving by design here. \nAssuming we understand the issue: the only CF deployment attempted has failed. There have been link provider intents created. BOSH leaves any created intents behind during a failed deployment, to assist in debugging. A subsequent successful deployment will delete any unused links/providers at the end. Another deployment wishing to consume links from a failed deployment, will have no choice but to use the created links in the failed provider deployment.\nWe don't expect reliable behaviour for a cross-deployment consumer when the provider deployment has failed. \nWas there a successful deployment of cf after the failed one, and before attempting to consume again? \nWhat version of the director is being used?. Yes @voelzmo as of 266.x we changed the behaviour. We felt that continuing to choose a set of links from a previous successful deployment could potentially hide more subtle problems with attempting to use old links. This behaviour will continue in future releases until otherwise.\nIn your case, the provider has failed, and there hasn't been a subsequent deployment that has succeeded. Would you still want to use the previously successful links, potentially incorrectly?. Yes @voelzmo as of 266.x we changed the behaviour. We felt that continuing to choose a set of links from a previous successful deployment could potentially hide more subtle problems with attempting to use old links. This behaviour will continue in future releases until otherwise.\nIn your case, the provider has failed, and there hasn't been a subsequent deployment that has succeeded. Would you still want to use the previously successful links, potentially incorrectly?. Closing as this is currently by design.. Closing as this is currently by design.. Hi @APShirley \nThe command bosh releases does not need the deployment in order to list all releases. The command's intention is to show you all releases bosh knows about, which could be applicable to one or more deployments (or none at all).\nReference for more information:\nBosh Director API: https://bosh.io/docs/director-api-v1/#list-releases\nBosh CLI: https://bosh.io/docs/cli-v2/#release-mgmt. Hi @APShirley \nThe command bosh releases does not need the deployment in order to list all releases. The command's intention is to show you all releases bosh knows about, which could be applicable to one or more deployments (or none at all).\nReference for more information:\nBosh Director API: https://bosh.io/docs/director-api-v1/#list-releases\nBosh CLI: https://bosh.io/docs/cli-v2/#release-mgmt. Hi @abh1kg, thanks for giving this a try. The reason this doesn't work is because each instance group will end up providing the exact same link. Any release attempting to consume will need to somehow choose which one to use across all instance groups. \nAt this time we don't support resolving links from addons. One work-around might be to include the addon as a job within the manifest for each instance group, and alias the providers for each. See the last part about resolving ambiguity here: https://bosh.io/docs/links/#overview\nWe will consider this in our next releases for BOSH. . Hi @abh1kg, thanks for giving this a try. The reason this doesn't work is because each instance group will end up providing the exact same link. Any release attempting to consume will need to somehow choose which one to use across all instance groups. \nAt this time we don't support resolving links from addons. One work-around might be to include the addon as a job within the manifest for each instance group, and alias the providers for each. See the last part about resolving ambiguity here: https://bosh.io/docs/links/#overview\nWe will consider this in our next releases for BOSH. . Do you have any orphaned disks that could potentially be the lost database?\nTry bosh disks --orphaned on your parent bosh. If there is an orphaned disk, this could be your child bosh's disk. Then you could try bosh attach-disk to reattach it. . Do you have any orphaned disks that could potentially be the lost database?\nTry bosh disks --orphaned on your parent bosh. If there is an orphaned disk, this could be your child bosh's disk. Then you could try bosh attach-disk to reattach it. . Hi @giner, what's happening here is that Bosh is validating the input manifest, and determining if there are changes to make. We agree it should be a bit better at detecting change, considering there are no changes to any input.. Hi @giner, what's happening here is that Bosh is validating the input manifest, and determining if there are changes to make. We agree it should be a bit better at detecting change, considering there are no changes to any input.. That's correct. We have shown some of what bosh is doing as non-debug output, but not all. We could add all events to account for all time taken, or remove the misleading (00:00:14). We're always looking for ways to improve :) Thanks for your feedback!. That's correct. We have shown some of what bosh is doing as non-debug output, but not all. We could add all events to account for all time taken, or remove the misleading (00:00:14). We're always looking for ways to improve :) Thanks for your feedback!. @voelzmo We currently have this story which will likely be revised upon later: https://www.pivotaltracker.com/story/show/162558368. @voelzmo We currently have this story which will likely be revised upon later: https://www.pivotaltracker.com/story/show/162558368. Closing as it's an individual's setup issue. We hope it's resolved now :). Closing as it's an individual's setup issue. We hope it's resolved now :). Hi @jamesjoshuahill, we're tracking in this story: https://www.pivotaltracker.com/story/show/163197005 and once @mfine30 can accept then we'll cut a release, and the next teams will be notified/pick it up automatically. . Hi @jamesjoshuahill, we're tracking in this story: https://www.pivotaltracker.com/story/show/163197005 and once @mfine30 can accept then we'll cut a release, and the next teams will be notified/pick it up automatically. . Hey @aeijdenberg thanks for taking a look at this and submitting the PR!\nWould you mind adding a unit test for this? Let me know if you'd like help with this. \nTaking a closer look, we will also need to make sure that the @links_provder_intents and @dns_encoder.id_for_group_tuple() are deterministic. \n@xtreme-behrouz-soroushian and @degaurab are currently taking a look as well. \n\nRebecca, BOSH. Hi @scottschulthess \n\nWhen you deployed on step #8, were any options like --skip-drain used? \nWhat I suspect could be happening here is that redeploying your deployment would cause the current one to drain. The drain process uses job templates, which are rendered again from the current deployment state, not the target deployment state you're aiming for.\nWould you be able to give this a try with --skip-drain and see what happens?\n\nRebecca, BOSH, Pivotal. What is the benefit or problem solved by having it on the deployment level? \n\nWould this deployment-level flag override the director flag?. Hi @surazzarus could you please post sanitized output and the curl command you're running? What do you get when you use the CLI for these tasks?. Hi @surazzarus could you please post sanitized output and the curl command you're running? What do you get when you use the CLI for these tasks?. ",
    "paolostivanin": "Output from ltrace -f -c -p PID:\n```\n% time     seconds  usecs/call     calls      function\n\n48.45   23.708106         149    158096 malloc_usable_size\n 17.30    8.465422         150     56274 free\n 11.55    5.652662         151     37407 malloc\n  8.11    3.969524         150     26362 memcpy\n  6.85    3.353998         149     22416 realloc\n  6.00    2.935627         149     19582 calloc\n  1.71    0.836083         149      5591 memmove\n  0.03    0.012940         148        87 memset\n\n100.00   48.934362                325815 total\n```\n. After some minutes I did another check with strace:\n```\n% time     seconds  usecs/call     calls    errors syscall\n\n81.00    0.004017          10       413           select\n 10.93    0.000542           0      7075           poll\n  2.90    0.000144           0     23614           clock_gettime\n  2.38    0.000118           0      5668           sendto\n  1.11    0.000055           0      8396       743 recvfrom\n  0.97    0.000048           0      5936        40 stat\n  0.71    0.000035           0      5696           write\n  0.00    0.000000           0        86        34 read\n  0.00    0.000000           0        80           close\n  0.00    0.000000           0        40           writev\n  0.00    0.000000           0        40           socket\n  0.00    0.000000           0        78        38 accept\n  0.00    0.000000           0       160           recvmsg\n  0.00    0.000000           0        40         6 shutdown\n  0.00    0.000000           0        40           bind\n  0.00    0.000000           0        40           getsockname\n  0.00    0.000000           0        40           getpeername\n  0.00    0.000000           0        40           setsockopt\n  0.00    0.000000           0        40           uname\n  0.00    0.000000           0        80           fcntl\n  0.00    0.000000           0       906           gettimeofday\n\n100.00    0.004959                 58508       861 total\n``\n. @dpb587-pivotal no. Unfortunately, there's nothing interesting inside the logs. The following is the output ofps -e -o %cpu,%mem,rss,vsz,command | grep director`\n89.6  1.3 110592 212276 ruby /var/vcap/packages/director/bin/bosh-director -c /var/vcap/jobs/director/config/director.yml\n 0.5  0.6 51640 151640 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_1.yml -i 1\n 0.5  0.6 51712 151516 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_2.yml -i 2\n 0.5  0.6 51580 151516 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_3.yml -i 3\n 0.5  0.6 51664 151528 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_4.yml -i 4\n 0.5  0.6 51564 151464 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_5.yml -i 5\n 0.5  0.7 59156 158912 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_6.yml -i 6\n 0.5  0.6 51508 151524 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_7.yml -i 7\n 0.5  0.6 51416 151612 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_8.yml -i 8\n 0.5  0.6 51552 151596 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_9.yml -i 9\n 0.5  0.6 51804 151592 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_10.yml -i 10\n 0.5  0.6 51580 151612 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_11.yml -i 11\n 0.6  0.6 51500 151612 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_12.yml -i 12\n 0.5  0.6 51460 151612 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_13.yml -i 13\n 0.5  0.6 51312 151540 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_14.yml -i 14\n 0.5  0.6 51604 151516 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_15.yml -i 15\n 0.5  0.6 51536 151512 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_16.yml -i 16\n 0.5  0.6 51404 151540 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_17.yml -i 17\n 0.5  0.6 51456 151596 ruby /var/vcap/packages/director/bin/bosh-director-worker -c /var/vcap/jobs/director/config/worker_18.yml -i 18\n 0.4  0.6 52228 218908 ruby /var/vcap/packages/director/bin/bosh-director-scheduler -c /var/vcap/jobs/director/config/scheduler.yml\n@cppforlife we are running on Suse Cloud 6 on our own datacenter. The stemcell version is 3263.8 (also tested on a 3263.7 stemcell). If you need more info, just ask me :)\n. @JamesClonk just for my curiosity, on which iaas are you running?\n. @tushar-dadlani thanks! Imma try it on our AWS env and I'll let you know! If @JamesClonk is also using Openstack. That could mean it's something  related to it...\n. I have just tested bosh 258 on our AWS deployment and it is working super fine. No problem at all on aws.\nSo this narrow down the issue on Openstack.\n@tushar-dadlani do you have an OS env with which you could try bosh 258?\n. @cppforlife to answer your slack request :) better pasting code here than there!\n[pid 30015] select(17, [10 12 15 16], [15 16], [], {0, 0}) = 4 (in [15 16], out [15 16], left {0, 0})\n[pid 30015] writev(15, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(15, \"\", 16384)         = 0\n[pid 30015] writev(16, [{\"HTTP/1.1 200 OK\\r\\nContent-Type: t\"..., 324}, {\"[{\\\"name\\\":\\\"admin-ui\\\",\\\"releases\\\":[\"..., 4677}], 2) = 5001\n[pid 30015] read(16, \"\", 16384)         = 0\n[pid 30015] shutdown(15, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(15)                   = 0\n[pid 30015] shutdown(16, SHUT_WR)       = -1 ENOTCONN (Transport endpoint is not connected)\n[pid 30015] close(16)                   = 0\n[pid 30015] gettimeofday({1477554939, 730315}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 730363}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 820718}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 820801}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554939, 911154}, NULL) = 0\n[pid 30015] gettimeofday({1477554939, 911242}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 1821}, NULL) = 0\n[pid 30015] gettimeofday({1477554940, 1900}, NULL) = 0\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] gettimeofday({1477554940, 92300}, NULL) = 0\nI truncated the output but there were like tons of select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout) and then something changed:\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 1 (in [12], left {0, 18368})\n[pid 30015] select(14, [10 12 13], [], [], {29, 999992}) = 1 (in [13], left {29, 999990})\n[pid 30015] select(14, [10 12 13], [13], [], {0, 0}) = 1 (out [13], left {0, 0})\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 1 (in [12], left {0, 75870})\n[pid 30015] select(14, [10 12 13], [], [], {29, 999993}) = 1 (in [13], left {29, 999991})\n[pid 30015] select(14, [10 12 13], [13], [], {0, 0}) = 1 (out [13], left {0, 0})\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\n[pid 30015] select(13, [10 12], [], [], {0, 90000}) = 0 (Timeout)\nlsof -p 30015 -ad 10,12\nCOMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF   NODE NAME\nruby    30015 vcap   10r  FIFO   0,10      0t0 803493 pipe\nruby    30015 vcap   12u  IPv4 803494      0t0    TCP localhost:25556 (LISTEN)\n. Nothing interesting on the access log, just a bunch o:\n\"GET /deployments HTTP/1.1\" 200 4677 \"-\" \"EventMachine HttpClient\" 0.055 0.055 .\n\"GET /deployments/cf-diego/instances HTTP/1.1\" 200 1317 \"-\" \"EventMachine HttpClient\" 0.020 0.020 .\n\"GET /deployments/cf HTTP/1.1\" 200 145994 \"-\" \"HTTPClient/1.0 (2.7.1, ruby 2.3.1 (2016-04-26))\" 0.008 0.008 .\n\"GET /info HTTP/1.1\" 200 356 \"-\" \"HTTPClient/1.0 (2.7.1, ruby 2.3.1 (2016-04-26))\" 0.002 0.002 .\nInside thedebug log file there are only some SQL stuffs, nothing interesting...\n. @cppforlife good to know :) dunno if it could be helpful but currently we have 18 VMs running on Diego on our test system and 50 on our prod system\n. @JamesClonk we are using psql on both OS and AWS!\n. Even though the better approach would be to have it configurable.... Ok! I'll do it :) I close this PR and will open a new one!. bosh v260.1 didn't solve the problem for us. The only way to have bosh deployable is by setting a higher timeout.... @voelzmo no, everything works fine after our debugging session!. The command should return the output (or an aggregate of the output) of the monit status of the bosh vm processes. Like bosh instances --ps but for bosh itself.. PS: this is happening on OpenStack, Azure and GCP (postgres within the director vm).\nEDIT: on Amazon (using RDS and not local postgres) it works fine. . We are using 4 VCPUs, 8GB ram machine on all IaaSes (more or less, it's 7.5GB on amazon, 15GB gcp).. @cppforlife After multiple tries, I can confirm that setting pgpool to 80 solved our issue on Azure, OpenStack and GCP.\nL Error: timeout: 60.0, elapsed: 63.44875963\n14:16:02 | Creating missing vms: consul_z2/d533090f-cbba-49ef-bea2-4fb1b1fa1658 (0) (00:01:35)\n            L Error: timeout: 60.0, elapsed: 61.342340354\n14:15:47 | Creating missing vms: doppler_z1/2a95cd3a-5a9b-447e-a441-2fcafa5f9a4e (0) (00:01:20)\n            L Error: timeout: 60.0, elapsed: 63.651453009\n14:16:26 | Creating missing vms: router_z1/511d5279-71e2-4f1e-8130-26876246fab0 (0) (00:01:59). @cppforlife pool_timeout defined here: https://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh&version=262.3#p=director.db.connection_options. timeout of 160, the issue is still happening\nDirector: PG::Error: server closed the connection unexpectedly\n        This probably means the server terminated abnormally\n        before or while processing the request.: SELECT NULL. @cppforlife we were able to solve the issue by increasing the task checkpoint interval.\nHere the task is sent into timeout if the lock is not renewed after 90s. We patched the director, and set the sentinel value to 300s. This solved our issue.\nBefore the change:\n```\nTask 86352 Started  Tue Sep 19 14:43:17 UTC 2017\nTask 86352 Finished Tue Sep 19 14:44:22 UTC 2017\nTask 86352 Duration 00:01:05Updating deployment:\n  Expected task '86352' to succeed but state is 'timeout'\nExit code 1\ndebug:3004:D, [2017-09-19 14:43:17 #12123] [task:86352] DEBUG -- DirectorJobRunner: Acquiring lock: lock:deployment:cf\ndebug:12459:D, [2017-09-19 14:45:18 #12123] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\n```\nAfter the change (Openstack):\n4046:D, [2017-09-20 07:18:38 #30286] [task:86486] DEBUG -- DirectorJobRunner: Acquired lock: lock:release:cf\n9687:D, [2017-09-20 07:20:19 #30286] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\n10707:D, [2017-09-20 07:22:45 #30286] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\n17724:D, [2017-09-20 07:25:15 #30286] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\n27038:D, [2017-09-20 07:27:16 #30286] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\nAfter the change (Azure):\n2783:D, [2017-09-20 08:58:29 #49720] [task:22428] DEBUG -- DirectorJobRunner: Acquired lock: lock:deployment:cf\n3849:D, [2017-09-20 08:59:00 #49720] [task:22428] DEBUG -- DirectorJobRunner: Deleted lock: lock:release:cf uid: 0d37cb9b-093e-41cb-a823-b80cbbac4ba4\n10451:D, [2017-09-20 09:00:30 #49720] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf\n20439:D, [2017-09-20 09:03:51 #49720] [] DEBUG -- DirectorJobRunner: Renewing lock: lock:deployment:cf. ",
    "cjcjameson": "I suppose we aren't sure. We didn't do anything to delete the VM, so we assumed it was still there. Thanks for pointing out that method of searching the logs -- that will be helpful in future situations. For this case, we had:\n```\n$:\u2192 bosh task 16350 --debug | grep changes\nActing as user 'admin' on 'my-bosh'\nD, [2016-10-31 22:12:15 #31826] [] DEBUG -- DirectorJobRunner: Need to update instance 'db/0 (af89e3fb-2031-48a0-8266-8a8f8ce55723)', changes: \"packages, persistent_disk, configuration, state\"\nI, [2016-10-31 22:12:15 #31826] [canary_update(db/0 (af89e3fb-2031-48a0-8266-8a8f8ce55723))]  INFO -- DirectorJobRunner: Updating instance db/0 (af89e3fb-2031-48a0-8266-8a8f8ce55723), changes: \"packages, persistent_disk, configuration, state\"\n$:\u2192 bosh task 16350 --debug | grep create_vm\nActing as user 'admin' on 'my-bosh'\nD, [2016-10-31 22:10:37 #31826] [create_missing_vm(db/0 (af89e3fb-2031-48a0-8266-8a8f8ce55723)/1)] DEBUG -- DirectorJobRunner: External CPI sending request: {\"method\":\"create_vm\",\"arguments\":[\"3011fd28-75fa-40a2-aae7-5bb3f1668a98\",\"ami-1bfe5a7b light\",{\"availability_zone\":\"us-west-2b\",\"instance_type\":\"c4.large\",\"ephemeral_disk\":{\"size\":30000,\"type\":\"gp2\"}},{\"private\":{\"ip\":\"10.32.120.12\",\"netmask\":\"255.255.255.240\",\"cloud_properties\":{\"subnet\":\"subnet-b1f977d5\",\"security_groups\":[\"sg-3d439c5b\"]},\"default\":[\"dns\",\"gateway\"],\"dns\":[\"10.32.120.202\"],\"gateway\":\"10.32.120.1\"}},[],{}],\"context\":{\"director_uuid\":\"6aaaf9cc-dcd1-4dd3-8890-62cfb3e0b05e\"}} with command: /var/vcap/jobs/aws_cpi/bin/cpi\n```\nIf that means that this is a non-issue, feel free to close the issue. Thanks!\n. Oh, I think it was a persistent disk -- I probably said the wrong thing.. Ok -- we assume that website might be closed source, so it's reasonable that we wouldn't see that code anywhere, right?. ",
    "zhutao11": "It's not the openstack's issue.  The response from openstack will be handled at Web Server,and the UTF-8 in header comes from Tomcat.\nI think the UTF-8 contributes to response analysis,  but should not be a limit.\nThank you very much!\n. ",
    "istvanballok": "https://github.com/cloudfoundry/bosh/pull/1505\n. ",
    "manno": "@cppforlife for us it will just fail on EFI systems since the grub2 installation is missing the EFI modules.\n. @cppforlife for us it will just fail on EFI systems since the grub2 installation is missing the EFI modules.\n. Oh, you mean if there is a grub installation which is able to install EFI? Right, a test for that would be useful, since it probably won't boot.\nI would check if /boot/efi was created and bail out. I'll try to add it to this PR later today.\n. Are BOSH instance groups applied in the order that is given in the manifest? The bosh create-env might depend on the custom certificate.\nIf that works it would be nice to remove native trusted certs from the bosh release.. Are BOSH instance groups applied in the order that is given in the manifest? The bosh create-env might depend on the custom certificate.\nIf that works it would be nice to remove native trusted certs from the bosh release.. @dpb587-pivotal Thanks for the quick reply. Yes, compiling from source works fine, so we're just using the local bosh release ops file for now.. @barthy1 we successfully used something like Task.where(:id => task.id).update(description: Sequel.join([@column_name, \"asdfasdf\"])).\nRefreshing the task model should not be necessary, we don't expect any code to read from the new logging columns while the task is running.. ",
    "StBurcher": "Hi,\nthank you that worked. I have change the time. . Hi,\nwe found out that the problem is in the IAAS level. A solution is not found yet. So, I will close this issue. . I'm facing a problem again deploying CF240. During the perparing package compilation, the error Timing out pinging to  occurs. \nI'm able to ssh into the VMs. The agent log is full of the following messages. \n> 2017-01-16_15:13:52.76117 [ConfigDriveMetadataService] 2017/01/16 15:13:52 WARN - Failed to load config from /dev/disk/by-label/CONFIG-2 - Reading files on config drive: Failed to get file contents, disk path '/dev/disk/by-label/CONFIG-2' does not exist\n\n> 2017-01-16_15:13:52.76117 [ConfigDriveMetadataService] 2017/01/16 15:13:52 WARN - Failed to load config from /dev/disk/by-label/config-2 - Reading files on config drive: Failed to get file contents, disk path '/dev/disk/by-label/config-2' does not exist\n\nand\n> 2017-01-16_15:13:49.14392 [File System] 2017/01/16 15:13:49 DEBUG - Checking if file exists /sys/class/net/lo/device\n\n> 2017-01-16_15:13:49.14395 [File System] 2017/01/16 15:13:49 DEBUG - Stat '/sys/class/net/lo/device' 2017-01-16_15:13:49.14396 [Cmd Runner]\n\n> 2017/01/16 15:13:49 DEBUG - Running command 'ifup --no-act eth0'\n\n> 2017-01-16_15:13:49.14550 [Cmd Runner] 2017/01/16 15:13:49 DEBUG -> Stdout: 2017-01-16_15:13:49.14552 [Cmd Runner] 2017/01/16 15:13:49 DEBUG - Stderr: ifup: interface eth0 already configured\n\n> 2017-01-16_15:13:49.14553 [Cmd Runner] 2017/01/16 15:13:49 DEBUG - Successful: true (0) 2017-01-16_15:13:49.14554 [attemptRetryStrategy]\n\n> 2017/01/16 15:13:49 DEBUG - Making attempt #0\n\n> 2017-01-16_15:13:49.14554 [clientRetryable] 2017/01/16 15:13:49 DEBUG - [requestID=515d5afb-4b24-4cf3-63e4-399f1e1ef2d7] Requesting (attempt=1): Request{ Method: 'GET', URL: 'http://169.254.169.254/latest/user-data' } 2017-01-16_15:13:49.32663\n\n> [clientRetryable] 2017/01/16 15:13:49 DEBUG - [requestID=515d5afb-4b24-4cf3-63e4-399f1e1ef2d7] Request succeeded (attempts=1), response: Response{ StatusCode: 200, Status: '200 OK' }\n\n> 2017-01-16_15:13:49.90153 [settingsService] 2017/01/16 15:13:49 ERROR - Failed loading settings via fetcher: Unmarshalling settings wrapper: invalid character '<' looking for beginning of value\n\n> 2017-01-16_15:13:49.90162 [File System] 2017/01/16 15:13:49 DEBUG - Reading file /var/vcap/bosh/settings.json 2017-01-16_15:13:49.90163\n\n> [settingsService] 2017/01/16 15:13:49 ERROR - Failed reading settings from file Opening file /var/vcap/bosh/settings.json: open /var/vcap/bosh/settings.json: no such file or directory\n\n> 2017-01-16_15:13:49.90164 [main] 2017/01/16 15:13:49 ERROR - App setup Running bootstrap: Fetching settings: Invoking settings fetcher: Unmarshalling settings wrapper: invalid character '<' looking for beginning of value 2017-01-16_15:13:49.90175 [main] 2017/01/16\n\n> 15:13:49 ERROR - Agent exited with error: Running bootstrap: Fetching settings: Invoking settings fetcher: Unmarshalling settings wrapper: invalid character '<' looking for beginning of value\n> 2017-01-16_15:13:50.91835 [main] 2017/01/16 15:13:50 DEBUG - Starting agent\n\nAny Idea?\nUpdate:\nIf I using only 2 works, bosh compiles more packages as with 5 workers. \nUpdate 2:\nOne agent is there on not???\n> +-------------------------------------------------------------------------------------------+--------------------+-------+---------+-----------+ | VM                                                                  \n> | State              | AZ    | VM Type | IPs       |\n> +-------------------------------------------------------------------------------------------+--------------------+-------+---------+-----------+ | compilation-66f44067-17cd-4d87-9a0d-568edf77dfa5/0\n> (34a657ad-9273-4856-92d1-3e82aa5f78f6) | running            | INDIA |\n> xlarge  | 10.0.0.47 | |\n> compilation-f03b6ee0-02f3-4e14-b320-1b9439daa937/0\n> (9df21f0e-8cb7-4bbd-84c9-c664e86717eb) | unresponsive agent | INDIA | \n> |           |\n> +-------------------------------------------------------------------------------------------+--------------------+-------+---------+-----------+\n\nThe IaaS was not the problem. . @voelzmo  Thank you for the tip. I found the problem in the manifest. The registration endpoint was not correct configured. ",
    "andyliuliming": "made my Azure organization public now. so CLA not need to be signed here : ). here's one pull request I sent to sequel library, and Jeremy suggests us to use the TrueClass because it's more genernal\nhttps://github.com/jeremyevans/sequel/pull/1264. @dpb587-pivotal Hi Danny, I'm thinking about to support the mssql for the bosh.\nso prepare the database migration for that first.\nin case you don't mind making it more general : ). @dpb587-pivotal Hi Danny, I'm thinking about to support the mssql for the bosh.\nso prepare the database migration for that first.\nin case you don't mind making it more general : ). @luan yes, it's that \"group\" you mentioned.\nit would be very nice if it can be document, and then we can use it without worrying about it would be removed or changed suddenly.\nThanks,\nAndy. ",
    "Lafunamor": "since @MatthiasWinzeler is on vacation I'll answer here.\nWhen shrinking down to one instance and increasing back up to 3 instances BOSH will behave as expected.\nNevertheless (as you mentioned) this is depending on the deployed software and not always possible.. @tylerschultz correct. Line 5 defines the PID location.\nI've removed the line from blobstore nginx as well.. @dpb587-pivotal I disagree. A start script should never write the PID file for an appllication unless the application cannot do itself.\nAs mentioned before writing the PID file in the start script causes the problem that wenn the start script is run while nginx is still running, the start script will overwrite the PID file. This will then result in monit not being able to monitor the process and fail after trying to start  nginx multiple times.\nThe system will not recover unless an operator reacts to the problem.. I've added pidguard as @MatthiasWinzeler suggested. Please let me know if this is better.. I just ran in this issue. It seems like errands can not provide links or consume link provided by themselves.\nI suggest adding at least a meaningful error message for this case. \"Can't generate network settings without an IP\" is very difficult to trouble shoot.\nI generally don't see the need for errand to be able to provide links as they are short lived. If an errand has a dependency on an other errand, they probably should be merged into one errand.. ",
    "Samze": "Some questions:\n\nIs varchar(255) suitable for the new context_id column? with non-null, default ''? We think a UUID of some kind would be sufficient, so it could be shorter.\nShould we add an index for the new context_id column?\nIs any user input santisation necessary for the X-Bosh-Context-Id header?\n\nShould the request be rejected if the header is larger than the database field?. Some questions:\n\n\nIs varchar(255) suitable for the new context_id column? with non-null, default ''? We think a UUID of some kind would be sufficient, so it could be shorter.\n\nShould we add an index for the new context_id column?\nIs any user input santisation necessary for the X-Bosh-Context-Id header?\nShould the request be rejected if the header is larger than the database field?. Thanks @cppforlife, see latest commit with improvements. . Thanks @cppforlife, see latest commit with improvements. . Hey @cppforlife, thanks for looking at this so quickly.\n\nThat was the original idea, however after looking at the directors job spec for config_server we noticed that you can only specify a single url. Are there other reasons to have an array of one?. @cppforlife Changed to urls. ",
    "zankich": "@barthy1 can you please resolve these merge conflicts, thanks!. @barthy1 can you also resolve the merge conflicts please, thanks!. It seems like it would be better to change the behavior of this test, rather than removing it completely, so that we're testing that we now return the errand_result object rather than the description.. Is this require necessary?. These new assertions should be in a different it block. This test currently mixes valid stemcells and invalid stemcells, so the expected behavior is testing two different things. There should be a new test which tests the unsupported stemcells explicitly.\nSo there should be one test which successfully uploads a stemcell and we can see the stemcells in the stemcell table, and a new test which uploads a stemcell, with the wrong format, and the stemcells table should not have this new stemcell.. The descriptions of theit blocks in this context should be updated to convey the assertion in the test. Currently they read like nested context, and give no clear indication of what the test is trying to prove. . For readability, can you please switch this to use an if instead of an unless.. The ternary operator is unnecessary here, you can simple negate the value of empty? and achieve the same behavior.. It looks like you could move the is_supported? check to this location instead of nesting the supported logic throughout this method. If you did an if !is_supported? at this location, you could fast fail out of this method. There you could log the cpi format error, raise StemcellNotSupported and no other modifications to this method are necessary.. Is locking this gem version necessary?. Is locking this gem version necessary?. @barthy1 You can create a default value using the before_create method.\nHere's an example of before_create in the cloud config model\nhttps://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/models/cloud_config.rb#L5-L7. ",
    "davidelang": "does the memory usage continue to increase over time? or is it stable, but just much higher than before?\nwhat's the rsyslog.conf file?. loading the impstats module will show you the queue status.\neven a short connect/disconnect to haproxy can have rsyslog send some messages \n(and with TCP, it won't know that they don't get through, you need to use relp \nto be sure of delivery), so this would look very differently with different \nvolumes of logs being sent.\n. ",
    "stephanme": "Our 00-forwarder.conf contains:\n```\n$ModLoad imuxsock                       # local message reception (rsyslog uses a datagram socket)\n$MaxMessageSize 10240 # default is 2k\n$WorkDirectory /var/vcap/sys/rsyslog/buffered # where messages should be buffered on disk\nForward vcap messages to the aggregator\n\n$ActionResumeRetryCount -1              # Try until the server becomes available\n$ActionQueueType LinkedList             # Allocate on-demand\n$ActionQueueFileName agg_backlog        # Spill to disk if queue is full\n$ActionQueueMaxDiskSpace 32m            # Max size for disk queue\n$ActionQueueLowWaterMark 2000           # Num messages. Assuming avg size of 512B, this is 1MiB.\n$ActionQueueHighWaterMark 8000          # Num messages. Assuming avg size of 512B, this is 4MiB. (If this is reached, messages will spill to disk until the low watermark is reached).\n$ActionQueueTimeoutEnqueue 0            # Discard messages if the queue + disk is full\n$ActionQueueSaveOnShutdown on           # Save in-memory data to disk if rsyslog shuts down\nListen for logs over UDP\n\n$ModLoad imudp\n$UDPServerAddress 127.0.0.1cd\n$UDPServerRun 514\ntemplate(name=\"CfLogTemplate\" type=\"list\") {\n        constant(value=\"<\")\n        property(name=\"pri\")\n        constant(value=\">\")\n        property(name=\"timestamp\" dateFormat=\"rfc3339\")\n      constant(value=\" 10.0.65.66 \")\n\n    property(name=\"programname\")\n    constant(value=\" [job=dummy_z1 index=0] \")\n    property(name=\"msg\")\n\n}\n. @@10.0.68.3:5514;CfLogTemplate\nLog vcap messages locally, too\n$template VcapComponentLogFile, \"/var/log/%programname:6:$%/%programname:6:$%.log\"\n$template VcapComponentLogFormat, \"%timegenerated% %syslogseverity-text% -- %msg%\\n\"\n:programname, startswith, \"vcap.\" -?VcapComponentLogFile;VcapComponentLogFormat\nPrevent them from reaching anywhere else\n:programname, startswith, \"vcap.\" ~\n```\nwhich is basically what metron agent configures. I.e. we send logs via TCP.\nWe executed the following script on 2 VMs with stemcell 3312.7 and 3312:\nwhile :\ndo\n  logger $RANDOM'TESTTESTSTESTTEST{\"timestamp\":1481736578.1486158,\"message\":\"(0.000462s) SELECT * FROM \\\"apps\\\" WHERE (\\\"apps\\\".\\\"guid\\\" = a7887e60-6535-4b27-af62-93b11f3f8701) LIMIT 1\",\"log_level\":\"info\",\"source\":\"cc.db\",\"data\":{\"request_guid\":\"2f5c02f9-aaae-4052-4106-57dfe0548273::8d753b9f-953c-4613-9b7f-71aa9f0b4b49\"},\"thread_id\":47186750830220,\"fiber_id\":47186770028100'\ndone\nAdditional observation: our remote TCP log endpoint (an ELK) is not stable but seems to reset connections quite often. \nUsing stemcell 3312.7 (rsyslogd 8.23.0) we see increasing memory consumption and the following message in /var/log/syslog:\nDec 15 14:22:11 localhost rsyslogd: action 'action 0' resumed (module 'builtin:omfwd') [v8.23.0 try http://www.rsyslog.com/e/2359 ]\nDec 15 14:22:11 localhost rsyslogd: action 'action 0' resumed (module 'builtin:omfwd') [v8.23.0 try http://www.rsyslog.com/e/2359 ]\nDec 15 14:22:11 localhost rsyslogd: action 'action 0' resumed (module 'builtin:omfwd') [v8.23.0 try http://www.rsyslog.com/e/2359 ]\nDec 15 14:22:11 localhost rsyslogd: action 'action 0' resumed (module 'builtin:omfwd') [v8.23.0 try http://www.rsyslog.com/e/2359 ]\nDec 15 14:22:11 localhost rsyslogd: action 'action 0' resumed (module 'builtin:omfwd') [v8.23.0 try http://www.rsyslog.com/e/2359 ]\nOn stemcell 3312 (rsyslogd 8.22.0), memory consumption is stable and we don't see the messages above in /var/log/syslog.\nTherefore our conclusion is that there is a memory leak in rsyslog 8.23.0 when the remote syslog endpoint is not stable.. ",
    "DenverOps": "What is the status of this issue?  Has this been resolved?. What is the status of this issue?  Has this been resolved?. ",
    "anEXPer": "We'd like to propose releasing said version pin.\nWe (the team currently maintaining syslog-release need features only present in v8.25 and higher. Since rsyslog's stable version is up to v8.32 now, there's every chance that whatever issue originally provoked the pinning is gone. We attempted the replication described above in order to check, but, unfortunately, have concluded that the replication case is invalid.\nThe issue is that \"slow increase in allocated memory\" isn't necessarily a memory leak, especially if the memory allocation:\n- has bounded growth\n- is released according to business/configuration rules\nThis is what we believe was actually happening in the replication case using HA Proxy above. HA Proxy without a functioning backend accepts and then quickly kills TCP connections, never actually accepting data. When we ran the replication case for longer than ten minutes, the memory usage stabilized and held steady at around 1.4% of available memory. Rsyslog's configured to enqueue actions that haven't been completed in a linked list. If all the replication shows is the growth of action queues, we'd expect the memory reservation to drop as soon as a single good connection was made - and when we performed that experiment, that's exactly what we saw.\nNot only that, but the issue \"replicates\" exactly the same way using the reported method with both the \"before/unaffected\" v8.22, the originally reported v8.23, and the latest v8.32. Which is good, since it reflects correct functioning of the program.. We'd like to propose releasing said version pin.\nWe (the team currently maintaining syslog-release need features only present in v8.25 and higher. Since rsyslog's stable version is up to v8.32 now, there's every chance that whatever issue originally provoked the pinning is gone. We attempted the replication described above in order to check, but, unfortunately, have concluded that the replication case is invalid.\nThe issue is that \"slow increase in allocated memory\" isn't necessarily a memory leak, especially if the memory allocation:\n- has bounded growth\n- is released according to business/configuration rules\nThis is what we believe was actually happening in the replication case using HA Proxy above. HA Proxy without a functioning backend accepts and then quickly kills TCP connections, never actually accepting data. When we ran the replication case for longer than ten minutes, the memory usage stabilized and held steady at around 1.4% of available memory. Rsyslog's configured to enqueue actions that haven't been completed in a linked list. If all the replication shows is the growth of action queues, we'd expect the memory reservation to drop as soon as a single good connection was made - and when we performed that experiment, that's exactly what we saw.\nNot only that, but the issue \"replicates\" exactly the same way using the reported method with both the \"before/unaffected\" v8.22, the originally reported v8.23, and the latest v8.32. Which is good, since it reflects correct functioning of the program.. ",
    "ericpromislow": "bosh-stemcell-3312.7-warden-boshlite-ubuntu-trusty-go_agent.tgz\nI can't tell what's going on with keytool, but it looks like it's hanging.  Stranger that after\nI kill it it hangs around as  -- maybe no parent is waiting on it?. Still a problem, on my osx box only.  Here's some more weirdness from ps on the bosh VM (running virtualbox Version 5.0.30 r112061:\nvagrant@bosh-lite:~$ ps auxww | grep key\nroot      7357 99.6  0.0      0     0 ?        Z<l  22:57  45:48 [keytool] <defunct>\nroot     30387  0.0  0.0    196    36 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     30414  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: ......................................................................rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ./run: line 20: success: command not found ./run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? ...\nroot     30489  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     30738  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     31080  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     31276  0.0  0.0    196    36 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nvagrant  31321  0.0  0.0  11752   784 pts/0    S+   23:43   0:00 grep --color=auto key\nroot     31564  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     31975  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: ......................................................................rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ./run: line 20: success: command not found ./run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? ...\nroot     32003  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: ......................................................................rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ./run: line 20: success: command not found ./run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? ...\nroot     32059  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: /run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? rsyslogd: module 'imuxsock' already in this config, cannot be added  [v8.23.0 try http://www.rsyslog.com/e/2221 ] rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ...\nroot     32453  0.0  0.0    196    40 ?        S<   22:55   0:00 runsvdir -P /etc/service log: ......................................................................rsyslogd: Could not open output pipe '/dev/xconsole':: No such file or directory [v8.23.0 try http://www.rsyslog.com/e/2039 ] ./run: line 20: success: command not found ./run: line 40: success: command not found Could not load host key: /etc/ssh/ssh_host_ecdsa_key? Could not load host key: /etc/ssh/ssh_host_ed25519_key? ...\nvagrant@bosh-lite:~$ ps auxww | grep keytool\nroot      7357 99.6  0.0      0     0 ?        Z<l  22:57  45:50 [keytool] <defunct>\nvagrant  31336  0.0  0.0  11752   796 pts/0    S+   23:43   0:00 grep --color=auto keytool\nAlso, several jobs are running out of non-existent directories:\nvagrant@bosh-lite:~$ ps auxww | grep pre-start \nvagrant   1261  0.0  0.0  11748   936 pts/0    R+   23:49   0:00 grep --color=auto pre-start\nroot      7297  0.0  0.0  18008  2972 ?        S<   22:57   0:00 /bin/bash /var/vcap/jobs/uaa/bin/pre-start\nroot     11167  0.0  0.0  19584  1452 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11168  0.0  0.0  19584  1476 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11178  0.0  0.0  19584  1852 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11181  0.0  0.0  19584  1864 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11187  0.0  0.0  19588  1932 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11196  0.0  0.0   5916  1832 ?        S<   22:59   0:00 logger -p user.info -t vcap.cloud_controller_ng.pre-start.stdout\nroot     11199  0.0  0.0  19588  1932 ?        S<   22:59   0:00 bash /var/vcap/jobs/cloud_controller_ng/bin/pre-start\nroot     11205  0.0  0.0   5916  1968 ?        S<   22:59   0:00 logger -p user.error -t vcap.cloud_controller_ng.pre-start.stderr\nvagrant@bosh-lite:~$ l /var/vcap/jobs\nblobstore@  director@  garden@  health_monitor@  nats@  postgres@  redis@  warden_cpi@\nThe /var/vcap/jobs/{uaa,cloud_controller_ng} dirs are gone, but there are still commands running there.bash` is kind of live -- it you change a bash file while running the script, strange things can happen, like bash knows what the original coordinates in the script are, and tries to use them while running the script to get the next command (seems strange, but PHP 1.0 worked that way,\namong other interpreters). So maybe the script is hanging, and/or has passed keytool bad data which is causing it to hang.. Running current versions of everything:\nvirtualbox: 5.0.30r112061\nbosh-cli: 1.3262.24.0\nbosh-lite commit: d916aecb0d8fe8a9f8a23e641696504442a9c3fe. ",
    "Adamjmb": "Is there a solution for this problem? I have replicated this issue, and there's no resolution. Thanks.. Ok, I give the manifest a good look over... I'm curious to know exactly what you changed to correct the yml syntax. My manifest is an exact copy/paste from the bosh docs, similar to your manifest above, with my passwords.. Here is my Manifest:\n. @voelzmo Yes I did that. Thanks. My Manifiest again. Any help would be appreciated. Thanks.\nboshmanifest.txt\n. @nunodio I'm sorry, not sure where these 2 spaces are in the line. Can you point them out?. I found the issue. @nunodio is correct dealing with the 2 spaces on:\nmbus: \"https://mbus:mbus-password@<MY_IPD>:6868\" # <--- Uncomment & change\nThe line should be 2 spaces out, like:\nmbus: \"https://mbus:mbus-password@<MY_IPD>:6868\" # <--- Uncomment & change'\nI think this issue is now resolved. Thanks. ",
    "nunodio": "In that case the problem is related with the indentation. I figured out later. Analyse the indentation with eager eye for all attributes in your manifest file.  Sometimes the yml syntax is correct but the attributes aren't in the correct list.. In that case the problem is related with the indentation. I figured out later. Analyse the indentation with eager eye for all attributes in your manifest file.  Sometimes the yml syntax is correct but the attributes aren't in the correct list.. Please remove 2 spaces at the beginning of this 2 lines and test it:\n```\nagent: {mbus: \"nats://nats:nats-password@10.10.0.6:4222\"} # <--- Uncomment & change\nmbus: \"https://mbus:mbus-password@:6868\" # <--- Uncomment & change\n. Please remove 2 spaces at the beginning of this 2 lines and test it:\nagent: {mbus: \"nats://nats:nats-password@10.10.0.6:4222\"} # <--- Uncomment & change\nmbus: \"https://mbus:mbus-password@:6868\" # <--- Uncomment & change\n```. At the beginning. To shift left the lines.. At the beginning. To shift left the lines.. ",
    "geekyzk": "thanks,i try \nOn 01/12/2017 02:16\uff0c Alexander Lomovnotifications@github.comwrote\uff1a\nhey @geekyzk.\nI created a sample project with UAA token retrieval in java. You might find it useful to check it out: https://github.com/allomov/bosh-uaa-client-test\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.. @allomov thinks, it is successful. @allomov thanks,i know ,  Is there any way to get all the data?. thanks,successfully solved. ",
    "AmberAlston": "@cppforlife do you have an estimate for when this might be merged in? Had trouble locating the backlog story to answer the question myself. Thank you!. ",
    "harshalk91": "Yes!! I was trying with external network afterwards i tried with private network as well. It worked. But i needed to manually assign static_ips in cf deployment manifest. . Yes!! I was trying with external network afterwards i tried with private network as well. It worked. But i needed to manually assign static_ips in cf deployment manifest. . @voelzmo After assigning static ips in cf deployment manifest i am able to perform bosh deploy. and launch 16 vms successfully.  So slowly but steadily moving forward towards cloud foundry deployment with your help \ud83d\udc4d \nAppreciate your knowledge . @voelzmo After assigning static ips in cf deployment manifest i am able to perform bosh deploy. and launch 16 vms successfully.  So slowly but steadily moving forward towards cloud foundry deployment with your help \ud83d\udc4d \nAppreciate your knowledge . @voelzmo Can you please help me out. . @voelzmo Can you please help me out. . ",
    "univ0298": "D, [2017-01-24 08:50:39 #2595] [task:429433] DEBUG -- DirectorJobRunner: networks_changed? network settings changed FROM: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"], \"dns_record_name\"=>\"0.database-lon02.default.eu-gb-prod-diego-lon02.microbosh\", \"ip\"=>\"159.8.128.218\", \"netmask\"=>\"255.255.255.0\", \"gateway\"=>\"159.8.128.1\"}} TO: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"]}} on instance database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85)\nD, [2017-01-24 08:50:39 #2595] [task:429433] DEBUG -- DirectorJobRunner: configuration_changed? changed FROM: 99a53472557c7d1cc25617c928bb6b297092f0d5 TO: b89e64785f0f3c19790a7bb2eefb32834e09f1ce on instance database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85)\nD, [2017-01-24 08:50:39 #2595] [task:429433] DEBUG -- DirectorJobRunner: trusted_certs_changed? changed FROM:  TO: da39a3ee5e6b4b0d3255bfef95601890afd80709\nD, [2017-01-24 08:50:39 #2595] [canary_update(database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85))] DEBUG -- DirectorJobRunner: networks_changed? network settings changed FROM: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"], \"dns_record_name\"=>\"0.database-lon02.default.eu-gb-prod-diego-lon02.microbosh\", \"ip\"=>\"159.8.128.218\", \"netmask\"=>\"255.255.255.0\", \"gateway\"=>\"159.8.128.1\"}} TO: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"]}} on instance database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85)\nD, [2017-01-24 08:50:40 #2595] [canary_update(database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85))] DEBUG -- DirectorJobRunner: networks_changed? network settings changed FROM: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"], \"dns_record_name\"=>\"0.database-lon02.default.eu-gb-prod-diego-lon02.microbosh\", \"ip\"=>\"159.8.128.218\", \"netmask\"=>\"255.255.255.0\", \"gateway\"=>\"159.8.128.1\"}} TO: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"]}} on instance database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85)\nD, [2017-01-24 08:50:40 #2595] [canary_update(database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85))] DEBUG -- DirectorJobRunner: networks_changed? network settings changed FROM: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"], \"dns_record_name\"=>\"0.database-lon02.default.eu-gb-prod-diego-lon02.microbosh\", \"ip\"=>\"159.8.128.218\", \"netmask\"=>\"255.255.255.0\", \"gateway\"=>\"159.8.128.1\"}} TO: {\"default\"=>{\"type\"=>\"dynamic\", \"cloud_properties\"=>{\"security_groups\"=>[\"default\", \"cf\"]}, \"dns\"=>[\"159.8.128.250\", \"10.0.80.11\", \"10.0.80.12\"], \"default\"=>[\"dns\", \"gateway\"]}} on instance database_lon02/0 (fa0b2f77-84b3-4253-8291-6180f424fa85). ",
    "cwlbraa": "here's some straightforward slack evidence that this solves a real user problem:\nhttps://cloudfoundry.slack.com/archives/C07C04W4Q/p1537887050000100. here's some straightforward slack evidence that this solves a real user problem:\nhttps://cloudfoundry.slack.com/archives/C07C04W4Q/p1537887050000100. The load balancers are used by the cloud foundry deployment that you'll be wanting to deploy onto that bosh director. \n\nbbl-env-sa-db7a86b-cf-router-lb is going to load balance http[s] requests through the gorouter to your cloud foundry application containers pushed by cf push $APP.\nbbl-env-sa-db7a86b-cf-ssh-lb is going to load balance ssh tunnels through the diego brain ssh proxy to your cloud foundry application containers when you run cf ssh $APP.\n\nbbl-env-sa-db7a86b-cf-tcp-lb is going to load balance tcp requests through the tcp router to your cloud foundry application containers pushed by cf push $APP.. The load balancers are used by the cloud foundry deployment that you'll be wanting to deploy onto that bosh director. \n\n\nbbl-env-sa-db7a86b-cf-router-lb is going to load balance http[s] requests through the gorouter to your cloud foundry application containers pushed by cf push $APP.\n\nbbl-env-sa-db7a86b-cf-ssh-lb is going to load balance ssh tunnels through the diego brain ssh proxy to your cloud foundry application containers when you run cf ssh $APP.\nbbl-env-sa-db7a86b-cf-tcp-lb is going to load balance tcp requests through the tcp router to your cloud foundry application containers pushed by cf push $APP.. \n",
    "fdhex": "Trying to use the sysstat plugin from Telegraf in a BOSH vm and having a similar issue, would you have any update?. Trying to use the sysstat plugin from Telegraf in a BOSH vm and having a similar issue, would you have any update?. ",
    "ntdt": "Good guess, my bosh-init was out dated. Update to new version of bosh-init fix this problem.. ",
    "antonsoroko": "@pivotal-jamil-shamy \nHi.\nThank you for your response.\nI'm afraid not.\nEven if bosh links work for bosh addons, I need access to a large number of properties, so I will need to create a lot of provides properties for every interesting job (and then maintain these new properties, for example, for cf-release; furthermore in case of PCF this is even impossible, I believe) and then add all these properties as consumes to bosh agent. And I guess I will need to mark all these links as optional even if they aren't mine, otherwise, the agent cannot be deployed everywhere (as I understand).. @cppforlife \nHi.\n\nwhat exactly is your job doing?\n\nMonitoring :-)\nI want to deploy my agent everywhere, and then dynamically load necessary modules depending on environment.\ne.g.\n<% if p(\"consul.agent.mode\") == 'server' %>\nsome_code_to_load_consul_module\n<% end %>. @mfine30 \nHi.\nI am not sure that links are useful in this particular case, but okay.. The same for 261.. @cppforlife \nThus, there is no way to upgrade version of my agent other than through bosh rutime config?\nAnd if I want to add my agent explicitly to VM - I must to specify the same version as in bosh rutime config, right?. @cppforlife \nHi.\nThank you for your response.\nMy case is more connected with #1585 - I want to install agent to all VMs, so but becasue of #1585 I started thinking about possible workarounds. So in this case I was thinking about adding new version of my agent to some specific VM (via releases and templates sections) so that my agent will have access to properties of this VM (and to its own custom properties, defined in properties section of job).. @cppforlife \nHi.\nActually, deployment level addons feature sounds interesting for me too. Is there any ETA for it?). @cppforlife \nHi.\nAs I understand you talked about this story https://www.pivotaltracker.com/n/projects/956238/stories/118799931, right?. @dpb587-pivotal \n@cppforlife \nHi. \nUnfortunately, \"deployment level addons\" feature does not solve this issue because we are talking about \"top level\" addons (in this particular case).\nAlso exclude section in addons also does not solve this issue, because release versions check is triggered before exclude rules.\nAlso we have discussed this issue with @barthy1 , and she have some tech ideas/details.\nThanks in advance.. @dpb587-pivotal \n@cppforlife \nHi. \nUnfortunately, \"deployment level addons\" feature does not solve this issue because we are talking about \"top level\" addons (in this particular case).\nAlso exclude section in addons also does not solve this issue, because release versions check is triggered before exclude rules.\nAlso we have discussed this issue with @barthy1 , and she have some tech ideas/details.\nThanks in advance.. @cppforlife \nHi. \nI forgot to mention that I used bosh-init version 0.0.99-1c660f1-2016-11-11T23:51:52Z\nWith bosh-init version 0.0.100-a5c1605-2017-02-14T23:33:52Z update has been successfully completed.\nThanks for the tip.. @cppforlife \nHi. \nI forgot to mention that I used bosh-init version 0.0.99-1c660f1-2016-11-11T23:51:52Z\nWith bosh-init version 0.0.100-a5c1605-2017-02-14T23:33:52Z update has been successfully completed.\nThanks for the tip.. @cfmagnum \nHi. Have you tried to update your bosh-init/bosh-cli?. @cfmagnum \nHi. Have you tried to update your bosh-init/bosh-cli?. @cfmagnum \nHi. Try to upgrate to any of these versions:\nhttps://github.com/cloudfoundry/bosh/issues/1589#issuecomment-280273526\nI solved problem by upgrading my bosh-init version to 0.0.100+:\nhttps://github.com/cloudfoundry/bosh/issues/1589#issuecomment-280348697. @cfmagnum \nHi. Try to upgrate to any of these versions:\nhttps://github.com/cloudfoundry/bosh/issues/1589#issuecomment-280273526\nI solved problem by upgrading my bosh-init version to 0.0.100+:\nhttps://github.com/cloudfoundry/bosh/issues/1589#issuecomment-280348697. @barthy1 \nHi!\nThanks for this useful feature.\nIs there any ETA for \"deployments exclusions\" feature?\nThanks in advance.. @barthy1 \nHi!\nThanks for this useful feature.\nIs there any ETA for \"deployments exclusions\" feature?\nThanks in advance.. @barthy1 \nActually exclude sections works, my problem was in release version check.\nThanks.\n. @barthy1 \nActually exclude sections works, my problem was in release version check.\nThanks.\n. @dpb587-pivotal \nHello.\nIn my case it is more about PCF. It is really boring to create unique exclude rules for every foundation (e.g. p-mysql-12345, p-mysql-98765 and so on and so forth).\nThanks.. @cppforlife \nHi.\n1) I do not see any teams in PCF\n2) I do not really understand how it could help to exclude deployments (maybe because I need an example, but I can't get one because of point \u21161)\nThanks!. @cppforlife \nHi.\n1) I do not see any teams in PCF\n2) I do not really understand how it could help to exclude deployments (maybe because I need an example, but I can't get one because of point \u21161)\nThanks!. ",
    "hkumarmk": "+1\nI would like to add consul-agent as runtime config with a list of consul-services which would be populated depending upon the releases an instance is loaded. This would be a nice feature to support bunch of other scenarios where we would need customized runtime config according to the releases/applications installed on instances.. ",
    "x6j8x": "Server version:     5.7.11-log MySQL Community Server (GPL)\n. Server version:       5.7.11-log MySQL Community Server (GPL)\n. It's an out-of-the box MySQL 5.7 RDS instance with standard parameter set. . It's an out-of-the box MySQL 5.7 RDS instance with standard parameter set. . I had to manually remove the remains of the failed migration (variable_sets & variables table), but now it works like a charm! \ud83d\udc4d . I had to manually remove the remains of the failed migration (variable_sets & variables table), but now it works like a charm! \ud83d\udc4d . I'll try the new bosh-cli version (had version 0.0.99 in our ci image). \n0.0.99 worked perfectly up until bosh 260.6 & stemcell 3312.8. I'll try the new bosh-cli version (had version 0.0.99 in our ci image). \n0.0.99 worked perfectly up until bosh 260.6 & stemcell 3312.8. The updated bosh-cli helped! @dpb587-pivotal Thanks! \ud83d\udc4d . The updated bosh-cli helped! @dpb587-pivotal Thanks! \ud83d\udc4d . ",
    "cfmagnum": "WorkAround !!!\nI tried to deploy my proto bosh on bosh-openstack-kvm-ubuntu-trusty-go_agent/3363 stemcell and facing same issue. I switch back to bosh-openstack-kvm-ubuntu-trusty-go_agent/3312.15 and it is running fine for me.It seems bosh-openstack-kvm-ubuntu-trusty-go_agent/3363 stemcell contain bugs in it.. @antonsoroko \nMy Bosh CLI version is BOSH 1.3262.4.0\nAnd bosh-init version is 0.0.81-775439c-2015-12-09T00:36:03Z\n. Hey thanks @antonsoroko \nI have updated my bosh cli to 1.3262.26.0 and bosh-init to 0.0.101 . \nAnd it's working fine. I  am using bosh-openstack-kvm-ubuntu-trusty-go_agent/3363.12 stemcell.. ",
    "dk": "wrong number?. sorry I meant you're probably dialing the wrong 'dk' here. ",
    "kieron-pivotal": "@mfine30 We've switched to using the BOSH CLI go library now, so this is no longer an issue for us. Thanks.. ",
    "David-Pivotal": "Thank you for your comments. Could you suggest how I can configure UAA to eliminate bosh.admin?. ",
    "aclevername": "Bump. Bump. Still attempting to test when the internal db is configured with ::1, see here https://pivotal.slack.com/archives/C3M0B3TV3/p1547134391134800\nPlease do not merge yet.. Still attempting to test when the internal db is configured with ::1, see here https://pivotal.slack.com/archives/C3M0B3TV3/p1547134391134800\nPlease do not merge yet.. A issue has been opened regarding the inability to configure ::1 here: https://github.com/cloudfoundry/bosh/issues/2113\n. A issue has been opened regarding the inability to configure ::1 here: https://github.com/cloudfoundry/bosh/issues/2113\n. After talking to the BOSH team we are happy that it will work correctly for ::1.. After talking to the BOSH team we are happy that it will work correctly for ::1.. ",
    "JohannesAtGit": "Closed here. Reopened at right place: https://github.com/cloudfoundry-incubator/bosh-workspace/issues/126. ",
    "emirozer": "@tyyko Hi, I am also curious, have you actually attempted to do this?. Thank you very much Michael!. Just out of curiosity is there an official way of doing this migration?\n. ",
    "freddesbiens": "This has been fixed and will appear in our next release. The revised error message is: \nError: Deployment manifest contains 'stemcells' section, but it can only be used with cloud-config enabled on your bosh director.\nThank you for your patience!. This has been fixed and will appear in our next release. The revised error message is: \nError: Deployment manifest contains 'stemcells' section, but it can only be used with cloud-config enabled on your bosh director.\nThank you for your patience!. Closing this issue as the proposed workaround was useful and the new feature requested could have been a security risk.. @JamesClonk We implemented the first of the two stories mentioned by @xtreme-andrew-su above. The second one is much more complicated, however, and it will probably stay buried in our backlog forever since stories with more value with always take precedence. A possible workaround is to leverage co-located errands; the errand is guaranteed to have an IP address then, since it is paired with a running instance.\nClosing this issue for now. Thank you for your patience.. I was able to reproduce it on BOSH v268.2.x. Closing and re-opening to have a story created in our tracker.. Thank you for your patience. After careful consideration, we will not implement the requested feature. The reason for this is that it would make it very easy to expose confidential properties outside the job without the operator realizing it. We think the security risks outweigh the gains in productivity.. Hi @mikomraz.\nNormally, the contents of the VM metadata records are meant to be consumed by the BOSH agent. The agent will use the IP address (169.254.169.254) directly in order to bootstrap the VM. What's the use case for the DNS name in your situation? . Closing this issue and re-opening it to create a tracker story. . Closing and re-opening to get a Tracker story created.. @voelzmo What is the status on this, Marco? I see you accepted the spike story, but the issue is still open... Thanks!. Closing and reopening for the bot to create a Tracker story.. Closing and re-opening this issue to have a Tracker story created.. Closing and re-opening this issue to have a Tracker story created.. Looks good to me, Marco.\nI am not sure if it would be easier to implement one log per worker or a\nlog for all workers, since I am still unfamiliar with the code base. I\nsuppose one log per worker would simplify troubleshooting in large scale\ninstalls, considering it is now possible for a director to control remote\nworkers as well. What do you think?\nBest,\nFr\u00e9d\u00e9ric Desbiens\nProduct Manager | Cloud Foundry BOSH\n\nOn Fri, Jul 20, 2018 at 4:24 AM Marco Voelz notifications@github.com\nwrote:\n\nI've created https://www.pivotaltracker.com/story/show/159191961 for now,\nlet me know what your think.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1999#issuecomment-406526602,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AkpnJ4WiBf1cp8ELUATq6Vh5NzdN2kq_ks5uIZOfgaJpZM4VBSDd\n.\n. Closing and re-opening this issue to have a Tracker story created.. We definitely need to add validations when operators upload a cloud config. \nClosing and re-opening this issue to have a Tracker story created.. We recently merged changes that enable registry-less operation for BOSH. \n\nThe changes can be found in the following versions of our components:\n- v268.3.0+ of the director\n- v170.6+ of the Xenial stemcells\n- v5.4.0+ of the BOSH CLI\nYou need as well a compatible version of your CPI for this to work. We released the AWS one recently and the OpenStack one is not far behind. Azure will come at a later date.\nClosing this for now. Please let us know if you have further problems.. We are addressing this though this epic: https://www.pivotaltracker.com/epic/show/4121599. Closing the issue. . @aeijdenberg Thank you for your comment. Although the epic's description does not mention it, it contains stories focused on the director's certificates as well:\n\nAPI to check the status of the director's certificates: https://www.pivotaltracker.com/story/show/160199794\nCLI command to print the status of the director's certificates: https://www.pivotaltracker.com/story/show/158631742. @dpb587-pivotal The change we implemented is to support more recent key formats, but we do nothing to normalize the inputs. @JunoJunho Normally, our recommendation would be to normalize the key yourself before passing it to BOSH. Is there any particular reason why you are using those \"one liner\" keys?. Hi Marco.\n\nWe currently have a story to investigate the upgrade to the latest NATS in the Toronto private tracker. Engineers should be able to pick it up in the near future. We could work on this issue at the same time.\nThanks for submitting this!. Thank you for your patience. The BOSH team carefully considered and discussed this issue. However, our intent is for operators using CPI config to explicitly define a default CPI for each of their availability zones. As mentioned in my comment on #2066, we will update our documentation to make this explicit. . Thank you for your patience. After careful consideration, we decided not to implement a fix for this. However, we will update our documentation to mention that operators using cpi-config must explicitly define a default CPI for each of their availability zones.. Hi @aegershman.\nWhile working on this, we have realized it would be difficult for the director itself to expose metrics in the way you suggest. This is because the various processes making up the director run under BPM. We cannot easily access hardware properties from a process running inside BPM. \nWe will report director VM metrics through Health Monitor in the future. Our engineers are currently working on the implementation and you can follow their progress in this Tracker story: https://www.pivotaltracker.com/story/show/161762467\nGenerally speaking, you should leverage Health Monitor for anything that has to do with the healthiness of your BOSH environment. While you can obtain some information through the APIs, doing so will have an influence on the director's performance and resource usage. \nWe may eventually give operators a way to access metrics through the command line. What is  your specific use case? Most production environments use Prometheus or Healthwatch.... ",
    "janaurka": "@dpb587-pivotal: already tried. Since the vm does not exist anymore in vSphere it fails:\n```\n[root@pcf-bosh01-p logsearch]# bosh start elasticsearch_data/3 --force\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'logsearch' on 'bosh-infra-dev'\nYou are about to start elasticsearch_data/3\nDetecting deployment changes\nReleases\nNo changes\nCompilation\nNo changes\nUpdate\nNo changes\nResource pools\nNo changes\nDisk pools\nNo changes\nNetworks\nNo changes\nJobs\nNo changes\nProperties\nNo changes\nStemcells\nNo changes\nInstance groups\nNo changes\nStart elasticsearch_data/3? (type 'yes' to continue): yes\nPerforming 'start elasticsearch_data/3'...\nDirector task 235845\n  Started preparing deployment > Preparing deployment. Failed: Timed out sending 'get_state' to 7af77eae-3581-46c5-b630-26635be3b759 after 45 seconds (00:02:15)\nError 450002: Timed out sending 'get_state' to 7af77eae-3581-46c5-b630-26635be3b759 after 45 seconds\nTask 235845 error\nFor a more detailed error report, run: bosh task 235845 --debug\n```\nIf bosh cck only works when the vm is supposed to be powered on, what happens, if the Iaas (in this case vSphere) somehow bricks the VMs and they cannot be started anymore. How can I tell bosh to recreate those VMs?\nIn other words, since we do not rely on the data of the persistent disk (although it\u2019s already there and we could mount and cp the content), how do we make bosh aware of that this VM is simply gone?. @dpb587-pivotal: already tried. Since the vm does not exist anymore in vSphere it fails:\n```\n[root@pcf-bosh01-p logsearch]# bosh start elasticsearch_data/3 --force\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on deployment 'logsearch' on 'bosh-infra-dev'\nYou are about to start elasticsearch_data/3\nDetecting deployment changes\nReleases\nNo changes\nCompilation\nNo changes\nUpdate\nNo changes\nResource pools\nNo changes\nDisk pools\nNo changes\nNetworks\nNo changes\nJobs\nNo changes\nProperties\nNo changes\nStemcells\nNo changes\nInstance groups\nNo changes\nStart elasticsearch_data/3? (type 'yes' to continue): yes\nPerforming 'start elasticsearch_data/3'...\nDirector task 235845\n  Started preparing deployment > Preparing deployment. Failed: Timed out sending 'get_state' to 7af77eae-3581-46c5-b630-26635be3b759 after 45 seconds (00:02:15)\nError 450002: Timed out sending 'get_state' to 7af77eae-3581-46c5-b630-26635be3b759 after 45 seconds\nTask 235845 error\nFor a more detailed error report, run: bosh task 235845 --debug\n```\nIf bosh cck only works when the vm is supposed to be powered on, what happens, if the Iaas (in this case vSphere) somehow bricks the VMs and they cannot be started anymore. How can I tell bosh to recreate those VMs?\nIn other words, since we do not rely on the data of the persistent disk (although it\u2019s already there and we could mount and cp the content), how do we make bosh aware of that this VM is simply gone?. oh, totally forgot this, sorry :( We solved it somehow, I have just asked the person which did solve it to tell me how, so I can a) give a solution for my problem here and b) close the bug.. oh, totally forgot this, sorry :( We solved it somehow, I have just asked the person which did solve it to tell me how, so I can a) give a solution for my problem here and b) close the bug.. ",
    "mrferris": "Had the same problem just now. Fixed it by directly modifying the Bosh Director's internal DB:\nAfter accessing the director, use https://github.com/drnic/bosh-getting-started/blob/master/troubleshooting/bosh-database-cleaning.md to access the DB. Had to use postgres as the user instead of bosh, and then connect to the bosh database.\nFirst, find the offending phantom VM (via its UUID) from the instances table and note its instance id.\nselect * from instances;\nIt'll be an integer, for us it was 147.\nThis is the row you want to delete, but it's referenced as a foreign key by some other tables so you have to delete those first (make sure you use instance_id, not id! and replace 147 with the id of your instance):\ndelete from ip_addresses where instance_id=147;\ndelete from instances_templates where instance_id = 147;\ndelete from rendered_templates_archives where instance_id = 147;\nThen, delete the instance entry:\ndelete from instances where id = 147;\nAfter doing this, bosh vms no longer showed the problematic VM and all was right in the world. \nI do not doubt that there is an easier solution, if you even want to call this a solution. . @janaurka I am very interested in hearing how you solved it. Thanks!. ",
    "nagelpat": "@mrferris we ended up in this state due to a file locking issue on the persistent disk of the elastic search data node (I can't say if it was because of elasticsearch or the vSphere datastore itself).\n```\n[root@pcf-bosh01-p ~]# bosh tasks recent --no-filter 1000 | grep error | less\n[root@pcf-bosh01-p ~]# bosh task 231023\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on 'bosh-infra-dev'\nDirector task 231023\n  Started applying problem resolutions > Inconsistent mount information:\nRecord shows that disk 'disk-47261bd9-70c4-448d-a63b-77c27cd1fb63' should be mounted on vm-7f80660c-e2a8-47f3-b849-7c2897d8399f.\nHowever it is currently :\n    Not mounted in any VM (mount_info_mismatch 58): Reattach disk and reboot instance. Failed: Unknown CPI error 'Unknown' with message 'Datei konnte nicht gesperrt werden' in 'reboot_vm' CPI method (00:02:10)\nError 100: Error resolving problem '29': Unknown CPI error 'Unknown' with message 'Datei konnte nicht gesperrt werden' in 'reboot_vm' CPI method\n```\nSo, we simply deleted the no longer existing VM from the internal DB via:\n```\n[root@pcf-bosh01-p ~]# bosh delete vm vm-7f80660c-e2a8-47f3-b849-7c2897d8399f\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nAre you sure you want to delete vm 'vm-7f80660c-e2a8-47f3-b849-7c2897d8399f'? (type 'yes' to continue): yes\nDirector task 237191\nTask 237191 done\n```\nA bosh deploy did then recreate the missing VM and attached the persistent disk successfully.\n```\nAre you sure you want to deploy? (type 'yes' to continue): yes\nDirector task 237210\n  Started preparing deployment > Preparing deployment. Done (00:00:01)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:00)\nStarted creating missing vms > elasticsearch_data/ccfc3266-ff20-468f-b263-5c3df745457e (3). Done (00:02:14)\nStarted updating instance elasticsearch_data\n  Started updating instance elasticsearch_data > elasticsearch_data/ccfc3266-ff20-468f-b263-5c3df745457e (3) (canary\n```\n. @mrferris we ended up in this state due to a file locking issue on the persistent disk of the elastic search data node (I can't say if it was because of elasticsearch or the vSphere datastore itself).\n```\n[root@pcf-bosh01-p ~]# bosh tasks recent --no-filter 1000 | grep error | less\n[root@pcf-bosh01-p ~]# bosh task 231023\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on 'bosh-infra-dev'\nDirector task 231023\n  Started applying problem resolutions > Inconsistent mount information:\nRecord shows that disk 'disk-47261bd9-70c4-448d-a63b-77c27cd1fb63' should be mounted on vm-7f80660c-e2a8-47f3-b849-7c2897d8399f.\nHowever it is currently :\n    Not mounted in any VM (mount_info_mismatch 58): Reattach disk and reboot instance. Failed: Unknown CPI error 'Unknown' with message 'Datei konnte nicht gesperrt werden' in 'reboot_vm' CPI method (00:02:10)\nError 100: Error resolving problem '29': Unknown CPI error 'Unknown' with message 'Datei konnte nicht gesperrt werden' in 'reboot_vm' CPI method\n```\nSo, we simply deleted the no longer existing VM from the internal DB via:\n```\n[root@pcf-bosh01-p ~]# bosh delete vm vm-7f80660c-e2a8-47f3-b849-7c2897d8399f\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nAre you sure you want to delete vm 'vm-7f80660c-e2a8-47f3-b849-7c2897d8399f'? (type 'yes' to continue): yes\nDirector task 237191\nTask 237191 done\n```\nA bosh deploy did then recreate the missing VM and attached the persistent disk successfully.\n```\nAre you sure you want to deploy? (type 'yes' to continue): yes\nDirector task 237210\n  Started preparing deployment > Preparing deployment. Done (00:00:01)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:00)\nStarted creating missing vms > elasticsearch_data/ccfc3266-ff20-468f-b263-5c3df745457e (3). Done (00:02:14)\nStarted updating instance elasticsearch_data\n  Started updating instance elasticsearch_data > elasticsearch_data/ccfc3266-ff20-468f-b263-5c3df745457e (3) (canary\n```\n. ",
    "fghorbel": "ok thanks.. ok thanks.. Thanks!. Thanks!. Hi guys, \nI tried this\n```\nresource_pools:\n- name: bosh\n  env:\n    bosh:\n      password: jihngfur;gtweurhpgtiorehwterouthnrwepteironh\n``\nI saw the password got changed in /etc/shadow but when i try to dosu vcapand enter the password, i gotpermission denied` in the new stemcell 3363.9\n. Hi guys, \nI tried this\n```\nresource_pools:\n- name: bosh\n  env:\n    bosh:\n      password: jihngfur;gtweurhpgtiorehwterouthnrwepteironh\n``\nI saw the password got changed in /etc/shadow but when i try to dosu vcapand enter the password, i gotpermission denied` in the new stemcell 3363.9\n. Yes i used mkpasswd -s -m sha-512. As i said, i saw the password got changed in /etc/shadow, but i can not still access. . Yes i used mkpasswd -s -m sha-512. As i said, i saw the password got changed in /etc/shadow, but i can not still access. . yes.. yes.. ",
    "oozie": "Apologies for a delay in responding @cppforlife \nI've been trying to figure out how to exclude: only from errand machines, rather than particular jobs/deployments. What I want to avoid is maintaining a blacklist of errands under exclude.\nIf you have any hints on how to accomplish excluding from errands that would be great.. ",
    "staylor14": "Moar +1'ing for compiled-for.. ",
    "m-mayran": "In development environments, it happens quite often to have to do updates. In certain cases where several jobs happen consequentially, that can lead to delays of several dozens of minutes. . ",
    "miyer-pivotal": "I'm hitting the exact same issue with a broker-deregistrar errand - since the last update, were you able to figure out the root cause?\n. ",
    "wfernandes": "We ran into this issue on the notifications-release. We were trying to deploy with this PR which includes bosh-links.\nI'm assuming this issue has not yet been resolved.. @cppforlife Just wanted an update on this PR. We'd like to get this in as soon as possible as it is vital for the smooth operation of the bosh-system-metrics-server. Thanks.. We are closing this issue because it is fixed in later versions of bosh.\nTested with Bosh 263.. ",
    "subhashgunda": "pl help me on this..i can't get around this issue.. I ended up by removing the quotes in elb_names of router which was a point of entry (not haproxy) \n \"router\": {\"instance_type\": {\"id\": \"automatic\"},\"instances\": 1,\"internet_connected\": false,\"elb_names\": [\"\"]},\nI am not sure how i missed that.  removing quotes fixed the issue.\nthanks for you reply though. appreciate it.\n. ",
    "asherbar": "My bad: the manifest indeed contains a networks: null section, it just wasn't showing up in the log. . ",
    "claudio-benfatto": "hello @Amit-PivotalLabs I'll try to present a different use case. I originally used an example from the same configuration provided in this document to simplify my explanation without introducing implementation details. But I understand it can be useful to do it now :)\nOur use case is the following: we are developing a metrics boshrelease (based on the golang implementation of graphite) where we have a carbon-c-relay (https://github.com/grobian/carbon-c-relay) job which acts as a load balancer for several worker clusters (the worker job is called go-carbon (https://github.com/grobian/carbon-c-relay) ).\nThe user api is taken care by a job called carbonzipper (https://github.com/dgryski/carbonzipper).\nIn our implementation, we run several separate go-carbon clusters whose communication boundary are the instances which belong to the same cluster. The idea is to run a separate cluster per az and load balance them via the relay.\nThe carbon-c-relay job needs to access the links for the go-carbon instances in each cluster in a separate configuration stanza. The configuration file which will be created via the ERB template looks like:\n....\ncluster <name>\n    < <forward | any_of | failover> [useall] |\n      <carbon_ch | fnv1a_ch | jump_fnv1a_ch> [replication <count>] >\n        <host[:port][=instance] [proto <udp | tcp>]> ...\n    ;\ncluster <name>\n    < <forward | any_of | failover> [useall] |\n      <carbon_ch | fnv1a_ch | jump_fnv1a_ch> [replication <count>] >\n        <host[:port][=instance] [proto <udp | tcp>]> ...\n    ;\n...\nthis is the component which would benefit from a different links implementation.\nActually what we need is not a fully dynamic set of go-carbon clusters, but just a way to avoid hard-coding their references in the template code when resolving the links , leaving to the user the freedom and flexibility to choose how many to add.\nThanks!. hello @Amit-PivotalLabs I'll try to present a different use case. I originally used an example from the same configuration provided in this document to simplify my explanation without introducing implementation details. But I understand it can be useful to do it now :)\nOur use case is the following: we are developing a metrics boshrelease (based on the golang implementation of graphite) where we have a carbon-c-relay (https://github.com/grobian/carbon-c-relay) job which acts as a load balancer for several worker clusters (the worker job is called go-carbon (https://github.com/grobian/carbon-c-relay) ).\nThe user api is taken care by a job called carbonzipper (https://github.com/dgryski/carbonzipper).\nIn our implementation, we run several separate go-carbon clusters whose communication boundary are the instances which belong to the same cluster. The idea is to run a separate cluster per az and load balance them via the relay.\nThe carbon-c-relay job needs to access the links for the go-carbon instances in each cluster in a separate configuration stanza. The configuration file which will be created via the ERB template looks like:\n....\ncluster <name>\n    < <forward | any_of | failover> [useall] |\n      <carbon_ch | fnv1a_ch | jump_fnv1a_ch> [replication <count>] >\n        <host[:port][=instance] [proto <udp | tcp>]> ...\n    ;\ncluster <name>\n    < <forward | any_of | failover> [useall] |\n      <carbon_ch | fnv1a_ch | jump_fnv1a_ch> [replication <count>] >\n        <host[:port][=instance] [proto <udp | tcp>]> ...\n    ;\n...\nthis is the component which would benefit from a different links implementation.\nActually what we need is not a fully dynamic set of go-carbon clusters, but just a way to avoid hard-coding their references in the template code when resolving the links , leaving to the user the freedom and flexibility to choose how many to add.\nThanks!. @Amit-PivotalLabs the need to separate the clusters is due to an architectural choice, based on the way the relay works when distributing the metrics across nodes. The end result is an improved resilience to faults and the reduction of the impact of node losses across the cluster (the rationale of this is partly discussed in this presentation: https://fosdem.org/2017/schedule/event/graphite_at_scale/).\nA few management tools we'd like to use are furthermore only covering the replication=1 configuration.\nFor now, I've solved the problem by adding some logic to the templates: https://github.com/SpringerPE/go-graphite-boshrelease/blob/buckytools/jobs/carbon-c-relay/templates/config/carbon-c-relay.conf#L5-L37\nBut ofc would be better and more elegant if we leveraged some internal links functionalities instead.\nThanks!. @Amit-PivotalLabs the need to separate the clusters is due to an architectural choice, based on the way the relay works when distributing the metrics across nodes. The end result is an improved resilience to faults and the reduction of the impact of node losses across the cluster (the rationale of this is partly discussed in this presentation: https://fosdem.org/2017/schedule/event/graphite_at_scale/).\nA few management tools we'd like to use are furthermore only covering the replication=1 configuration.\nFor now, I've solved the problem by adding some logic to the templates: https://github.com/SpringerPE/go-graphite-boshrelease/blob/buckytools/jobs/carbon-c-relay/templates/config/carbon-c-relay.conf#L5-L37\nBut ofc would be better and more elegant if we leveraged some internal links functionalities instead.\nThanks!. ",
    "bpicode": "Thanks for the reply. Actually I did sign the CLA, it seems @cfdreddbot didn't notice?. ",
    "dmlemos": "In fact today we hit DB limits. Although state servers it is a totally different story, there is plenty of use cases out there :). I have updated the comments with benefits and suggest to use the cloud provider offer.. I wonder if it is possible to check the status for the bosh director snapshots. Those are also unmaintainable.. I would like to see this as well. We changed load balancers in AWS and bosh cck didn't detect the changes.. Thanks. Please feel free to move the issue.. @cdutra Just for you to know I have tested v62 with reached the same result. I don't remember the problem being this old.. ",
    "smahesh3": "Any update on this? Any plans to include this feature in your roadmap?. ",
    "anoop2811": "Would be great feature to add to scale out deployments and keep the operators happy. Any updates or plans of doing this feature?. Would be great feature to add to scale out deployments and keep the operators happy. Any updates or plans of doing this feature?. ",
    "gavinbunney": "This would be a huge help with CI systems as well, so we could scale out Concourse as needed for larger automation suites and scale it back down automatically.\nFor operating K8S, being able to use the ClusterAutoscaler would be great as well: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md\nPerhaps BOSH could manage/use an AutoscalingGroup (in AWS' case) rather than individual instances?. ",
    "tintoy": "Thanks @cunnie - that's exactly the kind of info I was after! :). Hi - sorry for the late reply; I missed the notification email.\nI work for Dimension Data, and our cloud API doesn't currently expose virtual disks as stand-alone devices. At this stage, I think the CPI option is a non-starter since the client in question wants to run Cloud Foundry. We'll therefore probably host it for them on the underlying (VMWare / Hyper-V) infrastructure. If we do add persistent disk support, I'd love to revisit this and will probably hit you up for some advice when that happens :). Thanks, @dpb587-pivotal :). ",
    "pulkitpivotal": "@cppforlife Could we close this out soon. How can we help?. ",
    "danjahner": "Is that plan going forward to require an explicit job name that includes a version, e.g. postgres-9.4, postgres-9.6, etc.?. ",
    "kevinzurek": "Much prettier, thank you @dpb587-pivotal . I was still using the old CLI. \ud83d\udc4d . ",
    "bxlkm": "I am also the same problem, the current I set the configuration works is one, openstack VMS has been successful.\n04:36:31 | Deprecation: Ignoring cloud config. Manifest contains 'networks' section.\n04:36:37 | Preparing deployment: Preparing deployment (00:00:52)\n04:46:11 | Preparing package compilation: Finding packages to compile (00:00:19)\n04:46:30 | Compiling packages: cli-network-policy-plugin/330209a6db78c64b3983e94a46b2586c05087bcc (00:15:10)\n            L Error: Timed out pinging to 78b5b3e9-ee5d-40cc-b117-9e7ee18a6a6d after 600 seconds\n05:01:41 | Error: Timed out pinging to 78b5b3e9-ee5d-40cc-b117-9e7ee18a6a6d after 600 seconds\nCapturing task '16' output:\nExpected task '16' to succeed but was state is 'error'\nExit code 1\nlogs\uff1a\nERROR -- DirectorJobRunner: Worker thread raised exception: Timed out pinging to bf8f9250-2cfd-4b09-8b66-4e127182242b after 600 seconds - /var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/agent_client.rb:211:in rescue in wait_until_ready'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/agent_client.rb:217:inwait_until_ready'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/vm_creator.rb:66:in create_for_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:144:increate_instance'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:185:in instance'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:16:inwith_reused_vm'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:115:in prepare_vm'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:66:inblock in compile_package'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock_helper.rb:48:in block in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock.rb:53:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock_helper.rb:48:in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:59:incompile_package'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:204:in block (2 levels) in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/event_log.rb:99:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:202:in block in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:198:in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:162:inblock (4 levels) in compile_packages'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\n////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n bosh/src/bosh-director/lib/bosh/director/agent_client.rb--source code:\n line 198:   def wait_until_ready(deadline = 600) ...\nIf the VMS boot time is longer, is this parameter set externally\uff1f\nI think\uff0cIf no parameters can be set, only the source code can be modified\u3002\n. thanks.. Hi @ivandavidov \nI adjusted the machine to 8 core 16G memory does not exist this problem, I deployed above the openstack, but the following error occurred:\nI judge that the virtual machine is too slow to start\u3002\n[2017-05-11 08:05:02 [task:11] ERROR - DirectorJobRunner: out pinging 937620d4-b525-4808-b9a1-c53586d77256 after 600 seconds #30097] Timed (to)\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/agent_client.rb:211:in rescue in wait_until_ready'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/agent_client.rb:217:inwait_until_ready'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/vm_creator.rb:66:in create_for_instance_plan'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:144:increate_instance'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:185:in instance'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/compilation_instance_pool.rb:16:inwith_reused_vm'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:115:in prepare_vm'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:66:inblock in compile_package'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock_helper.rb:48:in block in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock.rb:53:inlock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/lock_helper.rb:48:in with_compile_lock'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:59:incompile_package'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:204:in block (2 levels) in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/event_log.rb:99:inadvance_and_track'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:202:in block in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:198:in process_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-261.4.0/lib/bosh/director/deployment_plan/steps/package_compile_step.rb:162:inblock (4 levels) in compile_packages'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:77:in block (2 levels) in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:63:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-261.4.0/lib/common/thread_pool.rb:63:in block in create_thread'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\n[2017-05-11 08:05:02 #30097] []  INFO -- DirectorJobRunner: Task took 20 minutes 50.841256392000105 seconds to process.\nTask 11 error\nCapturing task '11' output:\n  Expected task '11' to succeed but was state is 'error'\nExit code 1\n. ",
    "jacko1118": "dear dpb587-pivotal,\nthanks so much for your reply, I go thru my bosh.yml file and use the default password to login to director with user vcap.\nThe main purpose to login to director is to see if it can access the new created VMs during running command bosh deploy.\nI also have a look at the  bosh task --debug 5  result but cannot find any valuable information.\nI will keep you update with my finding.\nThanks so much for your help!!\nregards\nJacko\nresource_pools:\n- name: test_pool\n  network: private\n  stemcell:\n    url: https://bosh.io/d/stemcells/bosh-vsphere-esxi-ubuntu-trusty-go_agent?v=3363.15\n    sha1: d5571cd8e13d1daca99dc821e4fb751f4cdd42f8\n  cloud_properties:\n    cpu: 2\n    ram: 8_192\n    disk: 80_000\n  env:\n    bosh:\n      # c1oudc0w is a default password for vcap user\n      password: \"$6$4gDD3aV0rdqlrKC$2axHCxGKIObs6tAmMTqYCspcdvQXh3JJcvWOY2WGb4SrdXtnCyNaWlrf3WEqvYR2MYizEGp3kMmbpwBC6jsHt0\"\nvcap@10.1.1.159's password:\nWelcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-72-generic x86_64)\n\nDocumentation:  https://help.ubuntu.com/\n\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\nLast failed login: Fri May  5 01:09:33 UTC 2017 from 10.1.1.104 on ssh:notty\nThere was 1 failed login attempt since the last successful login.\nLast login: Fri May  5 01:09:41 2017 from 10.1.1.104\nTo run a command as administrator (user \"root\"), use \"sudo \".\nSee \"man sudo_root\" for details.\n/:~$\n/:~$\n/:~$\n/:~$ df -h\n. Dear dpb587,\nduring the temperary Vm creation, I saw the VM's ip from vsphere console is 10.1.1.1 to 10.1.1.6.\nbut the hostname /dns name of these vms are unreachable, error is the log we saw in my post.\nSo the problem is the director cannot access the hostname of the vm , even they are in the same VLAN.\nPls advise.\nthanks.\nregards\njacko. by the way , you mentioned dynamic network, where shall I config this? during create bosh director or create CF?\nThanks.. Hi,\nI would like to ssh to the temporary VM duing deployment and have a look at the log, but I cannot access it via director;\n/:~# ssh 10.1.1.2\nThe authenticity of host '10.1.1.2 (10.1.1.2)' can't be established.\nECDSA key fingerprint is 69:6a:e7:be:f5:26:dd:ed:d2:3f:8b:da:5e:5b:d8:02.\nAre you sure you want to continue connecting (yes/no)? no\nHost key verification failed.\n/:~# ssh 10.1.1.2\nThe authenticity of host '10.1.1.2 (10.1.1.2)' can't be established.\nECDSA key fingerprint is 69:6a:e7:be:f5:26:dd:ed:d2:3f:8b:da:5e:5b:d8:02.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added '10.1.1.2' (ECDSA) to the list of known hosts.\nUnauthorized use is strictly prohibited. All access and activity\nis subject to logging and monitoring.\nroot@10.1.1.2's password:\npls advise.\nthanks.. It\u2019s recommended to start a deploy again and SSH into one of the VMs and look at the Agent logs while the Director waits for VMs to become accessible. See director.debug.keep_unreachable_vms property to let Director know to leave unreachable VMs for easier debugging.\ncould you tell me how to access the VM via SSH?. Check the Agent log on the VM that timed out for a \u201chandshake\u201d connection between the BOSH Director and the Agent\n1.Use your IaaS virtualization console to open a terminal window on the VM that timed out and log in as root.\ndo you know what is the root password?\nThanks.\nRegards\nJacko. Last login: Thu May 11 17:16:35 2017 from 10.1.1.104\nroot@ubuntu6:~# bosh target 10.1.1.159\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nTarget set to 'my-bosh'\nroot@ubuntu6:~# bosh vms\nRSA 1024 bit CA certificates are loaded due to old openssl compatibility\nActing as user 'admin' on 'my-bosh'\nDeployment 'sodec-vsphere-demo'\nDirector task 14\nTask 14 done\n+-------------------------------------------------------------------------------------------+--------------------+-----+---------+-----+\n| VM                                                                                        | State              | AZ  | VM Type | IPs |\n+-------------------------------------------------------------------------------------------+--------------------+-----+---------+-----+\n| compilation-04c149c3-78ea-4d8d-ae2b-0294e5480a71/0 (98b36b67-cc0c-439b-aef6-1fa55ec769de) | unresponsive agent | n/a |         |     |\n| compilation-1bca6bc7-8bd4-4d57-9f1b-d75b6558f14c/0 (838ad2a5-b624-4793-bb92-f4e6511f340f) | unresponsive agent | n/a |         |     |\n| compilation-3c94e779-4044-4190-bc18-65d637134e32/0 (e0f2fe95-d1c5-4217-9d78-63296c2a0d87) | unresponsive agent | n/a |         |     |\n| compilation-7201bbfb-1884-466e-a06b-8ed9d67b7fd6/0 (00d2b54b-c8e8-4694-9bab-b408a93c5658) | unresponsive agent | n/a |         |     |\n| compilation-b781632f-7fb9-4d02-bb80-dda2be0b0e78/0 (ccb6e3dc-a518-43b6-859d-9fdd10c1ef89) | unresponsive agent | n/a |         |     |\n| compilation-c88c3c53-c691-40d4-9303-8e09e409c2c4/0 (b33ec5d4-00fa-4312-8cc2-af57a95d9213) | unresponsive agent | n/a |         |     |\n+-------------------------------------------------------------------------------------------+--------------------+-----+---------+-----+\nVMs total: 6\nI still get the sameerror.. Dear dpb587,\n yes you  are right, the exsi is our of resource and I  have to ask for more resource.\nthanks for your kindly reply.\nRegards\nJacko. ",
    "utako": "We've also hit this problem working on Release Engineering. Has there been any work done to address this? \nThanks,\n@utako and @davewalter. This is introducing quite a bit of flakiness into our CI pipelines. Will this be prioritized at some point?\nThanks.. user error. closing.. ",
    "ansd": "In this case, the provider is required to specify all individual properties like\n```\nprovides:\n- name: broker\n  type: redis_broker\n  properties:\n  - redis.broker.protocol\n  - redis.broker.name\n  - redis.broker.something\nproperties:\n  redis.broker.protocol:\n    default: https\n    description: fooo\n  redis.broker.name:\n    description: baar\n  redis.broker.something:\n    description: some description\n    default: else\n```\n(see 621d26817cbaee4afb05a2b1b5cc2fbcb5e35648 for integration test)\nThere should currently be no new feature which would allow the provider to specify only the top-level property redis.broker and use the defaults of the leaves (redis.broker.protocol and redis.broker.something) because that would be backwards-incompatible since existing deployments may rely on this existing behavior.. We experience the same issue deploying MongoDB 3.4 on GCP.\nAccording to the MongoDB documentation:\n\nWith the WiredTiger storage engine, use of XFS is strongly recommended to avoid performance issues that may occur when using EXT4 with WiredTiger.\n\nAs this seems to be a general issue, blocking the recommended way of deploying MongoDB 3.4+, is it possible to update the agent to use GPT formatted disks by default? . @BeckerMax and I just noticed that by parallelising apply_resolutions, when running bosh cck, not only are VMs resurrected in parallel, but also disks get reattached in parallel since the CLI sends a single request to the /problems endpoint.\nThat's a side effect we didn't intend to do by this PR. But it also shouldn't harm? Are there cases when we don't want to apply resolutions in parallel when executing bosh cck?. ",
    "Deekshit51": "I am having the same issue can you please check?. ",
    "sarneaud": "We're having the reverse problem: a job with optional linking that's picking up an unwanted link through implicit linking.  We need to explicitly nil out a consumer link.. We're having the reverse problem: a job with optional linking that's picking up an unwanted link through implicit linking.  We need to explicitly nil out a consumer link.. I was just confused by the documentation and didn't realise it was possible.  (Taking another look, there are examples where a consumer link is set to nil, but the main text doesn't say anything about explicitly setting links to nil so it's easy to miss if you don't know the feature exists.). I was just confused by the documentation and didn't realise it was possible.  (Taking another look, there are examples where a consumer link is set to nil, but the main text doesn't say anything about explicitly setting links to nil so it's easy to miss if you don't know the feature exists.). ",
    "jaikanthm": "Hi @dpb587 \nThanks a ton for the detailed reply. That clarified the question and threw the right amount of light. \nThanks again. Thanks @schmidtsv \nSo in this case Canary, is also dependent on the original number (number of instances) of VMs or containers.\nFor canary to actually be effective, there  must be more than one instance deployed right ?. Thanks again for the prompt response. \nOne last question, you mentioned that when we do a OS/stemcell upgrade the persistent disk gets detached and a new VM gets created with the upgraded stemcell. Does the same process happen when we do a Application release upgrade ? say if mysql is getting an upgrade, are the steps similar ?. Thank you very much guys. That was very helpful\nWhen using canaries, does the deployment success or failure comes from the test scripts. To rephrase how does a canary deployment is deemed as success or a failure ?. ",
    "ivandavidov": "Hi @bxlkm,\nThis looks very much like a performance issue. I had similar problem on my laptop which is an almost entry level machine (8GB RAM, Pentium N3700 CPU).\nWhat are the specs on the machine where you do your deployment?\n. Thank you, @allomov!\nCurrently the Docker image is the closest and fastest thing to what I'm trying to achieve.\nFrom what I see, the Docker image versions are following the same pattern as the Vagrant based bosh-lite VM. I know for sure that bosh-lite is not going to have new updates (in favor of the new bosh 2 CLI which manages the VM), which makes me think that the Docker image won't get updates either. Nevertheless, this Docker image might be just what I need.\nI consider the issue resolved since I got the initial \"push\" which I need in order to investigate this area further.. @cppforlife - thank you for the additional information.\nI already ran into trouble with the bosh v1 docker image. Seems like I have to explicitly configure the docker environment to use non-default network because I got these errors while I tried to deploy cf-release:\n```\nDeploying\n\nAre you sure you want to deploy? (type 'yes' to continue): yes\nDirector task 4\nDeprecation: Ignoring cloud config. Manifest contains 'networks' section.\nStarted preparing deployment > Preparing deployment. Done (00:00:01)\nStarted preparing package compilation > Finding packages to compile. Done (00:00:00)\nStarted compiling packages\n  Started compiling packages > cli-network-policy-plugin/330209a6db78c64b3983e94a46b2586c05087bcc\n  Started compiling packages > cli/6e22370bfb380059b856a1b397bc14ae9ec1633c\n  Started compiling packages > haproxy/ad6e9e08a6d7a0f8951322f9a2ce394105df7fcf\n  Started compiling packages > nginx/e77a07866c05979b49389fe36d66846411ba08a2\n  Started compiling packages > ruby-2.3/3a224c0fb813a20b92de640ac779e1754a2748b4\n  Started compiling packages > libpq/7e51aa75bcabc47002eccc38be269de63fc6df7d\n   Failed compiling packages > cli/6e22370bfb380059b856a1b397bc14ae9ec1633c: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID 'a7c7d25b-ecd6-48a3-ab96-943d446e8a90': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\n   Failed compiling packages > nginx/e77a07866c05979b49389fe36d66846411ba08a2: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID '01d5b66b-f929-476c-8e67-e8ff484a2501': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\n   Failed compiling packages > libpq/7e51aa75bcabc47002eccc38be269de63fc6df7d: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID '2ab2a6ae-6047-4640-802e-7e008ceee570': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\n   Failed compiling packages > cli-network-policy-plugin/330209a6db78c64b3983e94a46b2586c05087bcc: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID 'd0f930af-4d6b-4355-aa83-e2c28d15489e': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\n   Failed compiling packages > haproxy/ad6e9e08a6d7a0f8951322f9a2ce394105df7fcf: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID '9b2bf8e0-52e4-4f1e-98c5-be3e51e0f737': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\n   Failed compiling packages > ruby-2.3/3a224c0fb813a20b92de640ac779e1754a2748b4: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID 'f21a1f1f-87eb-4bec-bb61-0a7075a81070': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method (00:00:00)\nError 100: CPI error 'Bosh::Clouds::CloudError' with message 'Creating VM with agent ID 'a7c7d25b-ecd6-48a3-ab96-943d446e8a90': Creating container: Post http://api/containers: dial tcp 127.0.0.1:7777: getsockopt: connection refused' in 'create_vm' CPI method\nTask 4 error\nFor a more detailed error report, run: bosh task 4 --debug\n```\nI'm pretty sure the above error is due to some docker misconfiguration. I ran the docker container like this:\nsudo docker run --privileged=true --publish 25555:25555 -i -t bosh/bosh-lite bash\n/usr/bin/start-bosh\nSince there is active Docker support in the bosh v2 infrastructure, I guess it is better for me to focus my efforts in this direction instead of trying to figure out what the problem is with my local machine/docker.\nOnce again - thank you for the hint! It is really useful!. ",
    "ships": "We have concluded that this issue only appears in pre-cloud-config deployments, and therefore doesn't require a fix as we expect general migration to cloud config manifests. \nThis issue occurs particularly when compilation VMs are required for a deploy which at the same time expects to remove existing instances.\nPossible workarounds for who is hitting the issue on their non-cloud-config deployments are:\n- Move compilation VMs into a different network (as suggested by @voelzmo).\n- Deploy in two steps, where the first one compiles any new packages without deleting existing instances, then remove old instances in separate step.. @voelzmo yep, that'll do -- missed that gem. Thanks!. Hi Barthy1\nWe wouldn't expect to be aware of Sequel interface at this point -- can we move this parameter into a hash component in the call to filter_by, and rely on the InstanceLookup to implement the spec_json search in DB? This seems like a high level of exposure of the DB layout/contract.\nwhat do you think?\neve. Additionally, it looks like L#28-30 can be pushed inside the instance lookup also?. Following above comment: if we isolate the abstraction as described there, wouldn't we be able to have this error be swallowed neatly, so that we don't need to start handling this case.. ",
    "Everlag": "My bad, I misinterpreted the documentation. Sorry about that.. ",
    "magnetnoc": "Thank you for your response.\nI re-did the upload command like this:\nbosh upload-stemcell https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3421.6\nSo that it would be retrieved directly, so there would be no download issues (on my side anyway)\nsame error occurs .... Ok I think you may have hit on the primary issue.  I am trying to install the Cloud Foundry system onto AWS, and I am not making it past this step - I'm not sure how to fix the issue you mentioned above, but I am definitely trying to deploy on AWS.  I have been following the docs at https://docs.cloudfoundry.org/deploying/aws/deploy-aws.html but after a few tries I am unable to progress, so any help would be appreciated.. ",
    "schmidtsv": "The OS upgrade is not an \"upgrade\" in the sense of using apt-get. When upgrading a stemcell, the Director will stop the Jobs on the Instance, detach the persistent disk, if present, then terminate the vm. It will then create a new VM with the new image, attach the disk, copy over the Jobs and tries to start them. \nThe Canary in this case is the first VM to be upgraded, and if it runs all other machines of those instance groups go through the same process, this time the amount of instances that do that is limited by max_in_flight.. If you want a no-downtime upgrade you always need to use something with either Clustering or Replication. In case of MySQL, you need a cluster which fails over when the master shuts down for upgrade. If you just have 1 instance in an instance group the whole group will be down during upgrade. (You should also consider, that a node may fail, so for production you should always have some form of failover/duplication). If you did not manually assign a password, the password will be random (beginning on stemcell 3363). You should take a look at https://github.com/cloudfoundry/bosh-deployment/blob/master/jumpbox-user.yml to create a jumpbox user with ssh key auth.. What confuses me, according to the Docs and my experience  you most likely have an error in your cloud-config or director config.\nIn the AZ config, You set a datastore pattern and a persistent_datastore pattern, although both are not in the documentation. datastore_pattern and persistent_datastore_pattern both go into the CPI config. You add the Datastores to Disks but not to VMs (which can lead to problems.\nAdditionally you added the general CPI config into the cloud config, where it has no effect, as it belongs to the director that deploys.\nYour networks seems wierd, the distributed port group in z1 is AZ1-vlan-17-IAAS-PCF-Infra and in z2 is vlan-17-IAAS-PCF-Infra. You sure that is right? Because in all setups I have seen the Port groups usually enumerate (If you get some wierd port errors, it would be a Port group misnaming)\nMy first guess would be, that it either is unable to place a VM into a datastore because your VMs have no ephemeral disk configuration (unless your CPI config is also in the director, in which case defining a datastore_pattern is a string there and it expects a regex matching a storage that is reachable from the cluster (check if the datastore is attached to the cluster). Alternatively the datastores are not reachable from the cluster. Can you check if you can manually start a VM in the desired cluster using one of the datastores you linked?. You do not need the CPI config if you are using it. I was referring to the one in the Director manifest. (The one on the bosh vm, not in the cloud_provider section). Can you just either link me your create-env call with the variables in it (-the password) or take your create-env call, replace create-env with interpolate with the added option --path /instance_groups/name=bosh/properties/vcenter/datacenters/0/clusters and paste me the clusters that come out?. Ok, your problem is simple. In the director manifest you only mention one cluster. For technical reasons BOSH wants you to add all clusters it can deploy to there. you try to deploy to a different cluster. One way to fix that is creating an ops file with that snippet and add it as ops file to the end of your command \nyaml\n- type: replace\n  path: /instance_groups/name=bosh/properties/vcenter/datacenters/0/clusters\n  value:\n  - cvg_iaas_internal:\n    resource_pool: bosh-utility-1\n  - cvg_iaas_internal_1:\n    resource_pool: bosh-utility-1\nThen deploy the director, and you should be able to deploy on both Cluster without all the data store clutter in the cloud-config. I think I forgot an indentation layer in front of resource_pool, aka 2 blanks, and he complains that you defined nil where he expected an array. You could also write it like this\n- cvg_iaas_internal: {resource_pool: bosh-utility-1}\nFor vars, we use a so called iaas_config file for that. Just make a single level yml that looks like this and use it with -l:\ndirector_name: overbosh\nOr generally\n<-v variable>: value. ",
    "zhoutiekui": "@dpb587-pivotal   thank you so much.\nWe had encountered the error because of our iaas problem leading the \"check_instance_ips(remote_ip, instance_id) if remote_ip\" code failure. and then the error response is also \"timed out pinging....\".\nAnother question, I want to know how to confirm the agent status is normal and  the agent service is running ? and what conditions are needed to make sure the agent service can be started ?\n. ",
    "prashantgnet": "Yes, I have schema v1 based deployments which uses the same network which are also assigned to compilation_network. so issue is due to this. Thanks a lot Danny!. I am facing this issue on vSphere environment, getting following error messages under log file. I have set pool_timeout value to 60. will try with larger value.\nD, [2017-11-14 11:16:02 #26383] [task:151] DEBUG -- DirectorJobRunner: (0.000184s) COMMIT\nD, [2017-11-14 11:16:02 #26383] [task:151] DEBUG -- DirectorJobRunner: [network-configuration] Reserved ip '10.122.51.205' for bosh_services_network as static\nD, [2017-11-14 11:16:02 #26383] [task:151] DEBUG -- DirectorJobRunner: [network-configuration] Found subnet with azs 'z1' for 10.122.51.205. Reserved as static network reservation.\nE, [2017-11-14 11:16:02 #26383] [task:151] ERROR -- DirectorJobRunner: PG::Error: server closed the connection unexpectedly\n        This probably means the server terminated abnormally\n        before or while processing the request.: SELECT NULL\nD, [2017-11-14 11:16:02 #26383] [task:151] DEBUG -- DirectorJobRunner: (0.000158s) SET standard_conforming_strings = ON. ha director support will be helpful for us. we are looking for \"multi vm director\" for performance perspective.\nThanks,\nPrashant   . We are right now creating full bosh environment (using normal deployment, YML file) under single-vm bosh-init based director. Full bosh environment contains following six vms with respective jobs under it along with max threads set to 32 and workers set to 8.\nfull-bosh-1-blobstore\nfull-bosh-1-director\nfull-bosh-1-health_monitor\nfull-bosh-1-nats\nfull-bosh-1-postgres\nfull-bosh-1-registry\nDue to https://github.com/cloudfoundry/bosh/issues/1715 issue we are not able to place jobs on separate vms.. hmm...I might be wrong! I am thinking, we will get good performance if we place each job on separate vm along with max threads and workers. I have not compare the actual performance between single vm and multi vm bosh director but you may close this issue as we can fine tune any performance parameters using single vm bosh director. ha support looks very good.\nThanks,\nPrashant. Yes, right now we are using https://github.com/cloudfoundry/bosh-deployment based bosh director and will update any performance issue, if any. . Thanks for the updates! Closing it.. ",
    "lnhrdt": "@dpb587-pivotal Thanks Danny! You're right.\nName      Bosh Lite Director\nUUID      63ddb6a6-d6fe-40e1-b16b-2ff6b2034c79\nVersion   260.0.0 (00000000)\nCPI       warden_cpi\nFeatures  compiled_package_cache: enabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\nMaybe BOSH Lite could get an update. We're using the latest master.. ",
    "jochenehret": "We've just noticed that another issue has been raised which is most likely a duplicate of this one: #1712 \nYou can close one of the tickets, if duplication has been confirmed.. The issue is under discussion in [https://github.com/cloudfoundry-incubator/bosh-google-cpi-release/issues/199], so closing this one now.. ",
    "penrod": "I've seen this when trying to deploy using the bosh create-env command. The\ndeploy is expecting only one job, so the parser throws an error.\nWhat is the deploy command you are using when you get this error?\nOn Thu, Jun 22, 2017 at 6:04 PM, abhilash07 notifications@github.com\nwrote:\n\nHi is any-one deploy full bosh using any kind of cpi. I mean i create the\nfull bosh yaml file to deploy on openstack using openstack cpi. but i am\ngetting this error {Command 'deploy' failed: Validating deployment\nmanifest: jobs must be of size 1 }. Is any one have idea about full bosh\ndeployment. For your reference i am attaching my yaml file.\nfull-bosh-init.txt\nhttps://github.com/cloudfoundry/bosh/files/1096452/full-bosh-init.txt\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1715, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADS2SWY58CGUjGMlxgKoB7X22KABQss1ks5sGwEigaJpZM4OC_JC\n.\n. I've seen this when trying to deploy using the bosh create-env command. The\ndeploy is expecting only one job, so the parser throws an error.\n\nWhat is the deploy command you are using when you get this error?\nOn Thu, Jun 22, 2017 at 6:04 PM, abhilash07 notifications@github.com\nwrote:\n\nHi is any-one deploy full bosh using any kind of cpi. I mean i create the\nfull bosh yaml file to deploy on openstack using openstack cpi. but i am\ngetting this error {Command 'deploy' failed: Validating deployment\nmanifest: jobs must be of size 1 }. Is any one have idea about full bosh\ndeployment. For your reference i am attaching my yaml file.\nfull-bosh-init.txt\nhttps://github.com/cloudfoundry/bosh/files/1096452/full-bosh-init.txt\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/1715, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADS2SWY58CGUjGMlxgKoB7X22KABQss1ks5sGwEigaJpZM4OC_JC\n.\n. I suspected that may be the case but the part that didn't make sense to me was that I was able to update bosh from 263.3 to 264.2 with no problem. If I attempted to update 264.2 to 264.7, I would then get the name collision error. \n\nIf you are curious, we can dig deeper on this but at this point I am going to close this as I ran into issues with the nats certs needed for the 264.1 update and will not be updating the bosh versions until I can figure that one out.  I'll re-open it if the issues persists once I update the deploying bosh to 264.2. \nThanks. I suspected that may be the case but the part that didn't make sense to me was that I was able to update bosh from 263.3 to 264.2 with no problem. If I attempted to update 264.2 to 264.7, I would then get the name collision error. \nIf you are curious, we can dig deeper on this but at this point I am going to close this as I ran into issues with the nats certs needed for the 264.1 update and will not be updating the bosh versions until I can figure that one out.  I'll re-open it if the issues persists once I update the deploying bosh to 264.2. \nThanks. ",
    "abhilash07": "hi @penrod @dpb587-pivotal , thanks for your reply, i use bosh-init deploy --.yml.  Bt the problem is as we all know micro-bosh vm is single vm, which take take cares release engineering and auto-recovery etc, of deployment vm's. But the problem is some times director vms goes down it is difficult to manage the deployment vms. Is there any solutions to deploy bosh-init vm in to multi vm's ?. hi @penrod @dpb587-pivotal , thanks for your reply, i use bosh-init deploy --.yml.  Bt the problem is as we all know micro-bosh vm is single vm, which take take cares release engineering and auto-recovery etc, of deployment vm's. But the problem is some times director vms goes down it is difficult to manage the deployment vms. Is there any solutions to deploy bosh-init vm in to multi vm's ?. ",
    "poy": "@cppforlife We actually did try the --force flag and still experienced errors. We resolved the problem by running the delete-deployment several times and each time more VMs were successfully deleted.\nThat being said, there does not appear to be an option to skip the drain (much like the available flag --skip-drain in the deploy command). Should there not be one?. @dpb587-pivotal I don't think we have an opinion spec.az being embedded.. ",
    "tscolari": "\ud83c\udd99 \nIs there any update on this bug? . ",
    "Callisto13": "hello @dpb587-pivotal @cunnie sorry we did not respond sooner.\nI just quickly verified that the problem still exists, and yes it does seem to happen consistently.\nHere is the simple manifest I was using:\n```\n\ninstance_groups:\n- azs:\n  - z1\n  instances: 1\n  jobs:\n  - name: garden\n    properties:\n      garden:\n        debug_listen_address: 0.0.0.0:17013\n        graph_cleanup_threshold_in_mb: 10240\n        listen_address: 0.0.0.0:7777\n        listen_network: tcp\n        log_level: debug\n    release: garden-runc\n  name: garden\n  networks:\n  - name: default\n  stemcell: stemcell\n  vm_type: n1-standard-1\n  persistent_disk: 10000\n  env:\n    persistent_disk_fs: xfs\nname: xfs-persistent-test\nreleases:\n- name: garden-runc\n  version: 1.10.0\nstemcells:\n- alias: stemcell\n  os: ubuntu-trusty\n  version: '3468.17'\nupdate:\n  canaries: 1\n  canary_watch_time: 1000-240000\n  max_in_flight: 3\n  update_watch_time: 1000-240000\nand the error is still the same:\nL Error: Action Failed get_task: Task a755a26d-196b-47a0-66b9-cb67f91405c8 result: Mounting persistent disk: Formatting partition with xfs: Shelling out to mkfs.xfs: Running command: 'mkfs.xfs /dev/sdb1', stdout: '', stderr: 'warning: device is not properly aligned /dev/sdb1\nUse -f to force usage of a misaligned device\n': exit status 1\n```. ",
    "erjohnso": "@cunnie @dpb587-pivotal - we have confirmation that >2TB disks will force parted and unlocks using XFS on GCP.. That's great @cppforlife, is there a rough ETA for when they'd be available on bosh.io?. ",
    "GETandSELECT": "Hey\nSwisscom guy here.\nThe encrypted_docker_password column in ccdb.packages is varchar(255).\nWe verified the reported problem with different length passwords and received ERROR 1406 (22001): Data too long for column 'encrypted_docker_password' at row 1. The password is not stored in clear text, there is much chars overhead for the encryption.\n@wmnnd  thank you very much for reporting to upstream. Great investigation and report. Can you please report the bug in https://github.com/cloudfoundry/cloud_controller_ng. \nBosh is not involved in Docker Registry.\nHere the the error:\n{\"timestamp\":1499406631.9314291,\"message\":\"exception not translated: Sequel::DatabaseError - Mysql2::Error: Data too long for column 'encrypted_docker_password' at row 1:\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/mysql2-0.4.5/lib/mysql2/client.rb:120:in `_query'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/mysql2-0.4.5/lib/mysql2/client.rb:120:in `block in query'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/mysql2-0.4.5/lib/mysql2/client.rb:119:in `handle_interrupt'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/mysql2-0.4.5/lib/mysql2/client.rb:119:in `query'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/adapters/mysql2.rb:136:in `block in _execute'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/logging.rb:48:in `log_connection_yield'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/adapters/mysql2.rb:131:in `_execute'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/adapters/utils/mysql_mysql2.rb:36:in `block in execute'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `block in synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/connection_pool/threaded.rb:103:in `hold'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/adapters/utils/mysql_mysql2.rb:36:in `execute'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/adapters/mysql2.rb:76:in `execute_insert'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/dataset/actions.rb:986:in `execute_insert'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/dataset/actions.rb:338:in `insert'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1997:in `_insert_raw'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1979:in `_insert'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2041:in `block (2 levels) in _save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1243:in `around_create'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2038:in `block in _save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1243:in `around_save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2034:in `_save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1684:in `block (2 levels) in save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2188:in `block in checked_transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:186:in `block in transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `block in synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/connection_pool/threaded.rb:103:in `hold'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:156:in `transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2188:in `checked_transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1684:in `block in save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:2176:in `checked_save_failure'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/model/base.rb:1684:in `save'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/package_create.rb:24:in `block in create'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:186:in `block in transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `block in synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/connection_pool/threaded.rb:103:in `hold'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:156:in `transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/package_create.rb:23:in `create'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/package_create.rb:34:in `create_without_event'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/v2/app_create.rb:71:in `create_lifecycle'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/v2/app_create.rb:19:in `block in create'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:215:in `_transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:190:in `block in transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `block in synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/connection_pool/threaded.rb:107:in `hold'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/connecting.rb:280:in `synchronize'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sequel-4.41.0/lib/sequel/database/transactions.rb:156:in `transaction'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/app/actions/v2/app_create.rb:11:in `create'\\\\n/var/vcap/data/packages/cloud_controller_ng/853a1d8b1c7be39a15e72854ed8b929688d87c9e/cloud_controller_ng/app/controllers/runtime/apps_controller.rb:261:in `create'\\\\n/var/vcap/data/packages/cloud_controller_ng/853a1d8b1c7be39a15e72854ed8b929688d87c9e/cloud_controller_ng/app/controllers/base/base_controller.rb:79:in `dispatch'\\\\n/var/vcap/data/packages/cloud_controller_ng/853a1d8b1c7be39a15e72854ed8b929688d87c9e/cloud_controller_ng/lib/cloud_controller/rest_controller/routes.rb:16:in `block in define_route'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1610:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1610:in `block in compile!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:in `block (3 levels) in route!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:994:in `route_eval'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:975:in `block (2 levels) in route!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1015:in `block in process_route'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:in `catch'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1013:in `process_route'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:973:in `block in route!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:in `each'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:972:in `route!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1085:in `block in dispatch!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `block in invoke'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `catch'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `invoke'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1082:in `dispatch!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:in `block in call!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `block in invoke'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `catch'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:1067:in `invoke'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:907:in `call!'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:895:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/xss_header.rb:18:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/path_traversal.rb:16:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/json_csrf.rb:18:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/base.rb:49:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-protection-1.5.3/lib/rack/protection/frame_options.rb:31:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/nulllogger.rb:9:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/head.rb:13:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:182:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/sinatra-1.4.7/lib/sinatra/base.rb:2013:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/urlmap.rb:66:in `block in call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/urlmap.rb:50:in `each'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/urlmap.rb:50:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/request_logs.rb:22:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/security_context_setter.rb:19:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/vcap_request_id.rb:15:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/cors.rb:49:in `call_app'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/cors.rb:14:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/middleware/request_metrics.rb:12:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/rack-1.6.5/lib/rack/builder.rb:153:in `call'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/thin-1.7.0/lib/thin/connection.rb:86:in `block in pre_process'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/thin-1.7.0/lib/thin/connection.rb:84:in `catch'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/thin-1.7.0/lib/thin/connection.rb:84:in `pre_process'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/thin-1.7.0/lib/thin/connection.rb:50:in `block in process'\\\\n/var/vcap/packages/cloud_controller_ng/cloud_controller_ng/vendor/bundle/ruby/2.3.0/gems/eventmachine-1.0.9.1/lib/eventmachine.rb:1067:in `block in spawn_threadpool'\",\"log_level\":\"warn\",\"source\":\"cc.api\",\"data\":{\"request_guid\":\"93f6a3b5-c5db-4c43-5a03-72b89db5b66f::d8ab52e8-f9cf-4fd7-b38c-98dc3a27e923\"},\"thread_id\":47357287403320,\"fiber_id\":47357303324800,\"process_id\":6524,\"file\":\"/var/vcap/data/packages/cloud_controller_ng/853a1d8b1c7be39a15e72854ed8b929688d87c9e/cloud_controller_ng/app/controllers/base/base_controller.rb\",\"lineno\":305,\"method\":\"translate_and_log_exception\"}. ",
    "loewenstein": "@cppforlife @dpb587 This is not yet ready to be merged.\nI reworked DiskManager to get the DB up to date prior to sending out DNS blobs.\nThe change does solve the problem for the k8s CPI.\nHowever, there are a couple of open points:\n- Write a unit test for the new behaviour (all units are green, just no assertion on the new things happening)\n- Fix the failing integration tests\nrspec ./spec/gocli/integration/cli_ignore_spec.rb:387 # ignore/unignore-instance when there are ignored instances and a deploy happens when the existing instances is larger than the desired ones when the ignored instances is equal to desired ones deletes all non-ignored vms and leaves the ignored alone without updating them\nrspec ./spec/gocli/integration/cli_ignore_spec.rb:440 # ignore/unignore-instance when there are ignored instances and a deploy happens when the existing instances is larger than the desired ones when the ignored instances are fewer than the desired ones should keep the ignored instances untouched and adjust the number of remaining functional instances\nrspec ./spec/gocli/integration/cli_ignore_spec.rb:957 # ignore/unignore-instance when ignoring all of the instances in a zone when using static IPs balances the vms correctly, and it errors if removing an az containing ignored vms, and it errors if removing static IP assigned to an ignored VM\n- more places that have to update DB & broadcast DNS. @dpb587-pivotal @cppforlife \nI would evolve this PR to\n- concentrate all disk operations inside DiskManager, i.e. nowhere should cpi.attach_disk / cpi.detach_disk be called directly\n- conditionally handle the \"changing IP\" behaviour inside DiskManager by\n  - wait for agent to start\n  - apply_initial_vm_spec -> get_state -> update DB spec with 'networks' from agent state, render templates (1)\n  - broadcast dns (2)\nI propose to have explicit director configuration for when/if this actions should be triggered. For certain CPIs (for now I only know of one) this will always be required, for others it will never be required, e.g. update_dns_on_disk_operation.\nThis will involve creating a deployment plan in places where this was not yet necessary and might lead to parts of these operations called multiple times (e.g. instance updater calls disk_manager.update_persistent_disks, which will broadcast dns, and itself broadcasts dns).\nWDYT?\n1)\n```\ndef apply_initial_vm_state(instance_plan)\n       instance_plan.instance.apply_initial_vm_state(instance_plan.spec)\n   unless instance_plan.instance.compilation?\n     # re-render job templates with updated dynamic network settings\n     @logger.debug(\"Re-rendering templates with updated dynamic networks: #{instance_plan.spec.as_template_spec['networks']}\")\n     JobRenderer.render_job_instances_with_cache([instance_plan], @template_blob_cache, @dns_encoder, @logger)\n   end\n end\n\n```\n2)\n@dns_state_updater.update_dns_for_instance(instance_plan.instance.model, instance_plan.network_settings.dns_record_info). ",
    "jaresty": "merge story #152973240. merge story #152973200. merge story #152973162. merge story #152973098. Merge story #152972966. Merge story #152972966. @jsievers @voelzmo Does this PR solve the problem it intends to?  In reading the conversation from afar, it isn't clear whether the PR should be merged quickly because it solves your issue, or whether it does not go far enough to solve the problem (and therefore should be ignored in favor of waiting for a full fix).  Can you please clarify?. Merging story: #152972674. This was manually merged to master.  There does not appear to be a merge commit, but this commit is now in the history of the master branch here. @bingosummer Can you please provide your cloud config as well?. @bingosummer Can you please provide your cloud config as well?. @ishustava I didn't see it in the issue description but maybe I missed it -- can you clarify if this failure occurred while turning create-swap-delete on for the first time or if the previous deployment (which succeeded) also had it turned on?. @ishustava I didn't see it in the issue description but maybe I missed it -- can you clarify if this failure occurred while turning create-swap-delete on for the first time or if the previous deployment (which succeeded) also had it turned on?. Any chance you share the contents of the rendered nats.conf with credentials redacted? I'm surprised to see any IP addresseses in that file since after applying the crate swap delete ops file the director should be filling in link addresseses as DNS names instead. . Any chance you share the contents of the rendered nats.conf with credentials redacted? I'm surprised to see any IP addresseses in that file since after applying the crate swap delete ops file the director should be filling in link addresseses as DNS names instead. . I'm not sure I'm understanding correctly-is this what you are seeing?\n\nDeploy->upgrade deploy and fail->deploy which would succeed => rendered nats template has old ip address\n\nVs.\n\n\nDeploy->upgrade deploy succeeds => rendered nats template has new ip address. I'm not sure I'm understanding correctly-is this what you are seeing?\n\n\nDeploy->upgrade deploy and fail->deploy which would succeed => rendered nats template has old ip address\n\n\nVs.\n\nDeploy->upgrade deploy succeeds => rendered nats template has new ip address. That is interesting.  Do you mind including the upgrade output from a successful upgrade for comparison?. That is interesting.  Do you mind including the upgrade output from a successful upgrade for comparison?. @jfmyers9 I backfilled tests for the orphaned network cleanup and saw some results that seemed surprising.  It's possible that I made a mistake in my test setup, but it looks like we are removing unmanaged networks if they are on the deployment model (where being on means that the network names match). That behavior sounds incorrect to me, but maybe I'm wrong. . @jfmyers9 I backfilled tests for the orphaned network cleanup and saw some results that seemed surprising.  It's possible that I made a mistake in my test setup, but it looks like we are removing unmanaged networks if they are on the deployment model (where being on means that the network names match). That behavior sounds incorrect to me, but maybe I'm wrong. . Hi @alext,\n\nWe looked into this bug report and discovered that in fact it is an expected behavior for light stemcells to share a shasum -- in fact, that shasum is the shasum of the empty string \"\".  We have chatted with our PM about it and don't plan to change this behavior for now.  Please give us feedback if you have an argument why this is important to change.\nClosing the issue for now-thanks,\n@jaresty | @luan | @pivotal-mp . Yep.. I thought it should be working as is, but the tests showed it was not actually working.  Without this change, I was unable to get the cloud_config_ids to result in cloud_config objects passed to the test doubles.. I see the issue now - find_by_ids is actually a custom find method defined on our model (I had assumed it was provided by Sequel).  I'll investigate a bit further to see why it's not finding the models we're creating for test.. We were stubbing its return value in the test.  I removed the stub and restored the original code.  Now this PR is just adding some new test coverage.. The original code used a deployment_name local variable that was set to manifest_hash['name'] so it was helpful to extract this method as a crutch while I was refactoring.  I considered memoizing it but since the manifest_hash method is already memoized I didn't see the need to do so.  Inlining the manifest_hash['name'] into all the methods that are now calling deployment_name could work, but I think deployment_name still communicates the purpose of this value better than manifest_hash['name'] would, so I'm not clearly in favor of moving in that direction.. Removed. Removed. ",
    "martjarvi": "Hi. In the bosh.yml vmware manifest I have this:\nresource_pools:\nenv:\n   bosh:\n      password: \"yourhash\". The password in the manifest must be a sha-512 hash. Check https://lists.cloudfoundry.org/archives/list/cf-bosh@lists.cloudfoundry.org/thread/RT6MCSF2PF2DMHCTTCKXQIW7IAZO3URY/\n\nOn 25. juuli 2017, at 18:15, cwb124 notifications@github.com wrote:\nHere is what my resource_pools section looks like:\nresource_pools:\nname: vms\nnetwork: default\nenv:\nbosh:\npassword: password\nmbus:\ncert: ((mbus_bootstrap_ssl))\nI assume this sets the vcap password to 'password'? I have done this but can't ssh vcap@director_ip using that password.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "KaiHofstetter": "We found an easy way to reproduce this issue. See the commit here: https://github.com/cloudfoundry/bosh/commit/7c25f7cdb8d2a9b3e1d9dd0a4eef740d7529f617\nThis commit adds high cpu usage to the bosh vms endpoint, which is executed by the BOSH threadpool. We saw the following:\n* When task threads from the BOSH threadpool generate high cpu load:\n  * the TaskCheckPointer thread, which updates the checkpoint time in the DB, is scheduled infrequently\n  * SQL requests to update the checkpoint time are very slow \n  * this can cause the task timeout\nInterpretation\n\nthe TaskCheckPointer thread uses a lot of IO (e.g. db calls, log outputs), which means it passes control back to the scheduler\nthe task threads from the threadpool on the other hand use rarely or don't use IO, which means they rarely pass control back to the scheduler and rely on the scheduler to pause them\n\nObservation\nWhen we modify the commit mentioned above to 100_000_000.times { |i| i * i; Thread.pass }, the checkpoint column is updated in time and much faster. This means that the TaskCheckPointer thread doesn't get enough CPU time.\nWith Thread.pass\nD, [2018-02-05T09:14:08 #465] [task:1915-checkpoint] DEBUG -- DirectorJobRunner: (0.002087s) (conn: 47260528670420) SELECT NULL              \nD, [2018-02-05T09:14:08 #465] [task:1915-checkpoint] DEBUG -- DirectorJobRunner: (0.000827s) (conn: 47260528670420) BEGIN          \nD, [2018-02-05T09:14:08 #465] [task:1915-checkpoint] DEBUG -- DirectorJobRunner: (0.001032s) (conn: 47260528670420) UPDATE \"tasks\" SET \"checkpoint_time\" = '2018-02-05 09:14:08.348115+0000' WHERE (\"id\" = 1915)                                                                        \nD, [2018-02-05T09:14:08 #465] [task:1915-checkpoint] DEBUG -- DirectorJobRunner: (0.003577s) (conn: 47260528670420) COMMIT\nWithout Thread.pass\nD, [2018-02-02T16:40:32 #31869] [task:1914-checkpoint] DEBUG -- DirectorJobRunner: (2.604226s) (conn: 47338503403260) SELECT NULL\nD, [2018-02-02T16:40:35 #31869] [task:1914-checkpoint] DEBUG -- DirectorJobRunner: (1.101875s) (conn: 47338503403260) BEGIN\nD, [2018-02-02T16:40:37 #31869] [task:1914-checkpoint] DEBUG -- DirectorJobRunner: (1.301775s) (conn: 47338503403260) UPDATE \"tasks\" SET \"checkpoint_time\" = '2018-02-02 16:40:30.165886+0000' WHERE (\"id\" = 1914)\nD, [2018-02-02T16:40:41 #31869] [task:1914-checkpoint] DEBUG -- DirectorJobRunner: (2.604388s) (conn: 47338503403260) COMMIT\nThe bosh deployments task generates high cpu usage e.g. in this case https://github.com/cloudfoundry/bosh/issues/1847 which ends with task cancelled because of timeout.\nAlthough Thread.pass is not the desirable solution, it shows that there is an thread scheduling issue and not a thread locking problem.\nDo you have any better ideas on how to approach the issue?. ",
    "miguelverissimo": "This was merged and later reverted since it breaks the integration tests.\nWill cherry-pick the commit and fix the integration tests in the story https://www.pivotaltracker.com/story/show/160186667 . ",
    "ngocvanho": "director.debug.log.txt\ndirector.stderr.log.txt\nHi @dpb587-pivotal,\nPlease help me see the files that i attach above. I watched it but i understand errors.\nThank so much.\nVan\n. Hi dpb587-pivotal\nThank for your attention.\nMy OpenStack,which was deployed by Mirantis, is running on 3 slave node(however, I must run 1 master node follow mirantis model) and all of node is VM of virtual box, which is running on PC with i7 processor, 32GB memory.\nSo I think because of too many layer of virtualization is the reason for OpenStackVM can't used up capacity of hardware. \nAfter I sent a reply yesterday, I ran bosh-init again with new flavor(4core, 8GB memory) but it take almost 11 hours to print out the failed like before(last flavor only take 6 hours).\nSo now, only 1 thing I can do is increase *_watch_time follow your suggestion. Hope it work\nOne again, thank a lots for your help.\nVan. Hi dpb587-pivotal,\nI set  *_watch_time in bosh.yml as following:\nupdate:\n  canaries: 1\n  max_in_flight: 10\n  canary_watch_time: 5000-300000\n  update_watch_time: 5000-300000\n\u25a0\u3000The result is the same, but deployment time now cost 9 hours instead of 6 hours as before.\nStarted deploying\n  Creating VM for instance 'bosh/0' from stemcell '4feb6615-8db2-439d-9c7d-f38cecc57da0'... Finished (00:01:05)\n  Waiting for the agent on VM 'afeff020-615f-4c94-b859-f1f254842f29' to be ready... Finished (00:06:09)\n  Creating disk... Finished (00:00:22)\n  Attaching disk '404dc9c4-5de5-48d3-852b-d5f4797360c2' to VM 'afeff020-615f-4c94-b859-f1f254842f29'... Finished (00:00:54)\n  Rendering job templates... Finished (00:01:19)\n  Compiling package 'ruby/c1086875b047d112e46756dcb63d8f19e63b3ac4'... Finished (01:24:03)\n  Compiling package 'ruby_openstack_cpi/6576c0d52231e773f4ad53f5c5a0785c4247696a'... Finished (01:23:08)\n  Compiling package 'mysql/b7e73acc0bfe05f1c6cbfd97bf92d39b0d3155d5'... Finished (00:17:21)\n  Compiling package 'libpq/661f5817afe24fa2f18946d2757bff63246b1d0d'... Finished (00:19:39)\n  Compiling package 'health_monitor/e9317b2ad349f019e69261558afa587537f06f25'... Finished (00:46:07)\n  Compiling package 'postgres/3b1089109c074984577a0bac1b38018d7a2890ef'... Finished (01:12:34)\n  Compiling package 'nginx/2ec2f63293bf6f544e95969bf5e5242bc226a800'... Finished (00:23:58)\n  Compiling package 'davcli/5f08f8d5ab3addd0e11171f739f072b107b30b8c'... Finished (00:00:07)\n  Compiling package 'nats/63ae42eb73527625307ff522fb402832b407321d'... Finished (00:04:08)\n  Compiling package 's3cli/bb1c1976d221fdadf13a6bc873896cd5e2433580'... Finished (00:00:08)\n  Compiling package 'bosh_openstack_cpi/918abecbb3015ee383d5cb2af23e8dbfed6392d1'... Finished (00:00:33)\n  Compiling package 'verify_multidigest/8fc5d654cebad7725c34bb08b3f60b912db7094a'... Finished (00:00:05)\n  Compiling package 'registry/d81865cf0ad85fd79cb19aeb565bf622f2a17a83'... Finished (00:47:52)\n  Compiling package 'director/50af678ba068312e5de229b0558775ebae8d0892'... Finished (00:56:13)\n  Compiling package 'postgres-9.4/ded764a075ae7513d4718b7cf200642fdbf81ae4'... Finished (01:13:58)\n  Updating instance 'bosh/0'... Finished (00:01:10)\n  Waiting for instance 'bosh/0' to be running... Failed (00:12:27)\nFailed deploying (09:13:33)\nStopping registry... Finished (00:00:00)\nCleaning up rendered CPI jobs... Finished (00:00:00)\nCommand 'deploy' failed:\n  Deploying:\n    Received non-running job state: 'failing'\nAfter the deployment ended with failure, I ssh to the bosh vm and run \"monit start\"\n/:/home/vcap# monit summary\nThe Monit daemon 5.2.5 uptime: 8h 49m \nProcess 'nats'                      running\nProcess 'postgres'                  running\nProcess 'blobstore_nginx'           running\nProcess 'director'                  Execution failed\nProcess 'worker_1'                  running\nProcess 'worker_2'                  running\nProcess 'worker_3'                  running\nProcess 'director_scheduler'        running\nProcess 'director_nginx'            running\nProcess 'health_monitor'            running\nProcess 'registry'                  running\nSystem 'system_localhost'           running\nThen I  start the directory manually, it seems being started normally\n/:/home/vcap# /var/vcap/jobs/director/bin/director_ctl start\n/etc/sudoers: parsed OK\n/etc/sudoers.d/README: parsed OK\n/etc/sudoers.d/director: parsed OK\n/:/home/vcap# monit summary\nThe Monit daemon 5.2.5 uptime: 8h 53m \nProcess 'nats'                      running\nProcess 'postgres'                  running\nProcess 'blobstore_nginx'           running\nProcess 'director'                  running\nProcess 'worker_1'                  running\nProcess 'worker_2'                  running\nProcess 'worker_3'                  running\nProcess 'director_scheduler'        running\nProcess 'director_nginx'            running\nProcess 'health_monitor'            running\nProcess 'registry'                  running\nSystem 'system_localhost'           running\nQuestion : \n\nShould I increase max-time for *_watch_time to 9 hours (the total of deployment time)? \nIs the \"Waiting for instance 'bosh/0' to be running... Failed (00:12:27)\"  the last step of \"bosh-init deploy\"?\n   If the director can come up by manual start, is it fine to continue use it? \n\nThanks,\nVan. ",
    "GarfieldIsAPhilosopher": "Fixed the has_applicable_team? logic, and merged it to master. Hi @kreamyx ,\nThanks for your PR updates. We have a question about enabling this feature when there are already existing manual networks. \nSince enable_cpi_management is a global property on the director, how are we differentiating a new network that should be managed versus an existing manual network that should not be managed? Our concern is that the managed property is not stored on the network table, but rather is applied to all networks that are parsed and configured to be true or false based on the config spec. \nThanks\n@GarfieldIsAPhilosopher && @mikexuu, BOSH team. which component will be calling delete_network?It might be a good idea to add network lock before deleting the network to avoid race condition where the same network is being reused by create_networks function. ",
    "joerodriguez": "@cppforlife This is the work that you had suggested during our conversation at CF Summit.. @cppforlife This is the work that you had suggested during our conversation at CF Summit.. ",
    "jtarchie": "@zachgersh, we'd love to. We've had some troubles with testing multiple loops with EventMachine (ie we don't know how to). Is anyone available to pair on it next week?. Only one of the values are ints. It was nice to keep it consistent, especially when parsing the values for a golang struct.. ",
    "nitish29": "Hi @dpb587-pivotal, apparently in our case it was a space quota issue. We ran out of space. Thanks for the help, much appreciated.. ",
    "mcwumbly": "I don't think the director was under high load in terms of RAM or CPU usage from what we could tell, but this theory from @jraqula in the slack thread seems plausible:\n\nhow's this for a theory: listing/checking a task will cause the state to be set to timeout if it's outside the 90s\n\nOur tests poll the director to check the task status on an interval (that is 5 seconds by default).. We've run into this again recently in our CF distribution as we're looking to co-locate the BOSH CLI on VMs that require it using this package, but some of those jobs are already vendoring a version of that package so they are no conflicting.\n\nIIRC, there should be a story which allows packages of the same name from different releases even if fingerprints aren't the same. \n\nIs there something else we can follow to stay in the loop on any progress here? Or should it be sufficient to just follow this github issue?\n. ",
    "JCL38-ORANGE": "Hello,\nA quick feedback about the issue.\nFinally, the syntax documented in https://github.com/cloudfoundry/bosh-notes/blob/master/errands-v2.md is fine.\nThe steps followed in order to use collocated errands : \nDelete the uploaded release in old BOSH Director\nUpload the release with the BOSH Director that supports collocated errands (up to v263)\nDeploy the release and apply the syntax (described in bosh-notes)\nThe command \"bosh -d foo errands\" returns all errands (collocated ones and others) \nThe issue is now close.\nThanks and regards,\nJC LOSAPIO.\n. ",
    "dannysullivan": "@dpb587-pivotal thanks for the heads up - I'll do that now.. ",
    "deniseirvin": "(I just filled out the contributors agreement, and was asked to close and reopen this pr). ",
    "pbakre": "Please check the yml file which gets generated while executing bosh int . Update the 'gateway' and 'range' sections in it. Update the cloud config again with this updated yml file. Upload the stemcell again and it should pick up the values.. ",
    "ramyasg": "Thanks for your reply. We were able to resolve the issue. Closing the issue. . ",
    "calebwashburn": "Looks as if this line might be source of different ordering.\nhttps://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/models/runtime_config.rb#L18. ",
    "jsievers": "@voelzmo we keep seeing hung registry processes in production...\n. done\n. done\n. converted ctl script to template\n. done\n. added default explicit values for syslog event forwarder for now\n. done\n. done\n. actually it's spec.job.name, see https://github.com/cloudfoundry/bosh/blob/master/bosh-director-core/lib/bosh/director/core/templates/job_template_renderer.rb#L19. Done.\n. ",
    "jraverdy-orange": "Open this issue on bosh-linux-stemcell-builder. ",
    "deadpoet6": "thanks @dpb587-pivotal I had something similar in my .yml file before but I have also tried with your property but still, I have the similar error.\n\nAlso, I have tried to modify code in openstack.rb file but I got to know that while compiling it would change the parameters back to the actual state i.e., 300. Anyother way I could accomplish this?. ",
    "Fydon": "Hi @bendalby82.\nWindows support for bosh-cli is still a work in progress and I'm also looking into Windows support for the virtualbox-cpi. Please follow those issues for updates. For now unfortunately you'll need to use  bosh-lite using Vagrant if you want to use BOSH on Windows, even though it is not recommended..",
    "parthasarathi204": "@cppforlife : Any comments please ?. @dpb587-pivotal I have made only the following change in director(attached). After login to to director I can see director.yml is correct configuration for s3. But deploying learn-bosh  creates issue.\ndiffs.txt. I have cloned the latest bosh-deployment.\n. @dpb587-pivotal Yes. Absolutely no issue with  access/secret key . I am able to do bosh upload-blobs using the same  access/secret key . Sorry I could not make that work . The whole purpose of this exercise was to  support another new external blobstore . In actual environment I am able to do that . I have made the necessary changes in cpi, director, cli as well as stemcell . I can see the bosh-blobstore-bmc gets invoked .  I have one issue there. Will appreciate if you help me to resolve that . s3cli/gcs cli put all the credentials in the config.json But here the private key(credential)  is referring to a path in bosh director which is very much available in the bosh director.\nBut while deploying when it it is in the compilation VM , the path obviously won't exist . Hence the following error.  How can I get rid of this ? How can I refer the filepath or copy the file to the newer vms so that this goes through\nTask 13 | 04:36:22 | Compiling packages: ruby/aeb9acaed6ce2aa86ce1ce9f4017528be0e2041e\n(00:02:33)\n                  L Error: Action Failed get_task: Task 02df66c1-f692-469d-61de-2400789505dd result: Compiling package ruby: Fetching package ruby: Fetching package blob bce57f0f-f881-48d4-b999-aa868313992a: Getting blob from inner blobstore: Getting blob from inner blobstore: Shelling out to bosh-blobstore-bmc cli: Running command: 'bosh-blobstore-bmc -c /var/vcap/bosh/etc/blobstore-bmc.json get bce57f0f-f881-48d4-b999-aa868313992a /var/vcap/data/tmp/bosh-blobstore-externalBlobstore-Get094615252', stdout: 'Error setting up bmc client\nopen /var/vcap/packages/bosh-bmccli/bin/apikey.pem: no such file or directory\n', stderr: '2018/01/04 04:38:54 open /var/vcap/packages/bosh-bmccli/bin/apikey.pem: no such file or directory\nWill be great if you can help on this.. @dpb587-pivotal , Any thought on the issue ?. @dpb587-pivotal Thats true it is different from the original issue . Yes I have made changes in stemcell/bosh-director/bosh-cli . As well as implemented the bosh-bmccli , The changes are inline with s3cli and gcscli .\nI am not sure what changes have to be made for the agent .  The issue I am facing as the key is in a file as the sdk for bosh-bmccli consumes apikey file. Not the key . So the director and and the cli able to work with bosh-bmccli as the key file is present . But in case of new VM like the compiler VM  the same is not copied .\nI am looking for idea how to pass the key-content to the  newer vm from the bosh-director ? If you can help me to pass through this , I am done . Yes I am aware that the changes definitely will be passed to the upstream repositories.. The whole struggle is \"you'll have to use file content as a value and before calling underlying client\".  I could not not figure out where to write this mechanism , The file is available  with bosh director . I can read the content from the bosh director before invoking  , but not sure how to pass the the content to the compiler . Any sample or usage in the doc/code/referral ?\n. ",
    "jiaoyan2017": "@cppforlife yes, I did.\n[root@ip-172-31-20-93 ~]# history | grep yajl-ruby\n   97  gem install yajl-ruby\n  182  history | grep yajl-ruby\n[root@ip-172-31-20-93 ~]# history | grep \"yum install gcc gcc-c++ ruby\"       \n   96  yum install gcc gcc-c++ ruby ruby-devel mysql-devel postgresql-devel postgresql-libs sqlite-devel libxslt-devel libxml2-devel patch openssl\n  183  history | grep \"yum install gcc gcc-c++ ruby\"\nThe Amazon Machine Image is Amazon Linux 2017.09.0 (Community AMI ami-0797ea64).\n[root@ip-172-31-20-93 ~]# uname -a\nLinux ip-172-31-20-93 4.9.58-18.51.amzn1.x86_64 #1 SMP Tue Oct 24 22:44:07 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. ",
    "eljuanchosf": "Hey guys! I think is some sort of incompatibility between Ruby 2.3.x and the OpenSSL version in Ubuntu 16.04. I suffered this condition and tried Ruby 2.2 and it worked.. ",
    "saedalavinia": "It's not an incompatibility, but rather the fact that openssl-devel (or equivalent in apt-get) needs to be installed before ruby is installed. Either remove ruby, install openssl-devel, and reinstall ruby, or follow documentaion on how to resolve this issue without having to remove ruby.. ",
    "Steve-Hand": "A field value like \"month: '2,5,8,11'\" gets deployed as \"month: 25811\". Hi.  We're having problems with volumes being formatted with the MSDOS partitioning scheme. As such, they cannot be expanded beyond 2TB. As I understand it, parted can also be used to create MSDOS partitions. So, your comments using 'parted' does not quite tell me that all partitions involved, albeit on the ephemerial disk or volumes created by bosh, have the GUID partitioning table.  The reference commit is, at least to me, ambiguous fro the same reason. \nPlease clarify whether bosh has actually be changed to use the GUI partition table by default.. ",
    "jacknewberry": "We have identified the issue as being caused by this method:\nhttps://github.com/cloudfoundry/bosh/blob/1a6d637ddc8f15107407e581884f75224ed15a61/src/bosh-director/lib/bosh/director/models/template.rb#L70-L75\nWhich is eventually called from here: \nhttps://github.com/cloudfoundry/bosh/blob/391e66347870f782964aac7cc5a7e63576b638c3/src/bosh-director/lib/bosh/director/api/controllers/deployments_controller.rb#L458\nInstance-group errands have a different discovery path so they are not affected.\nBoth job and instance-group errands seem to be templated the same whether they specify the template path as bin/run or /bin/run.. We have identified the issue as being caused by this method:\nhttps://github.com/cloudfoundry/bosh/blob/1a6d637ddc8f15107407e581884f75224ed15a61/src/bosh-director/lib/bosh/director/models/template.rb#L70-L75\nWhich is eventually called from here: \nhttps://github.com/cloudfoundry/bosh/blob/391e66347870f782964aac7cc5a7e63576b638c3/src/bosh-director/lib/bosh/director/api/controllers/deployments_controller.rb#L458\nInstance-group errands have a different discovery path so they are not affected.\nBoth job and instance-group errands seem to be templated the same whether they specify the template path as bin/run or /bin/run.. ",
    "sauravmndl": "@AbelHu  and @cppforlife Thank you very much for your help so far. I have given a acceptable name () in attach command, but giving me following error now \nDirectorJobRunner: CPI error 'Bosh::Clouds::CloudError' with message 'Unknown disk caching xxxxxxxxxx' in 'attach_disk' CPI method\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/cloud/external_cpi.rb:195:inhandle_error'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/cloud/external_cpi.rb:99:in invoke_cpi_method'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/cloud/external_cpi.rb:61:inattach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/disk_manager.rb:76:in attach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:78:inhandle_new_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:31:in block in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:136:in_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:110:in block in transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/connecting.rb:251:inblock in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/connection_pool/threaded.rb:105:in hold'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/connecting.rb:251:insynchronize'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:99:in transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/transactor.rb:5:inblock in retryable_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:28:in block in retryer'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:26:inloop'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:26:in retryer'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/common.rb:118:inretryable'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/transactor.rb:4:in retryable_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:29:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:96:in perform_job'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:32:inblock in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:49:in with_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:32:inrun'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:85:in block (3 levels) in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:1046:inblock in spawn_threadpool'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:in block in create_with_logging_context'\nD, [2017-11-27 12:17:42 #5561] [task:820] DEBUG -- DirectorJobRunner: (0.000084s) SELECT NULL\nD, [2017-11-27 12:17:42 #5561] [task:820] DEBUG -- DirectorJobRunner: (0.000194s) SELECT * FROM \"tasks\" WHERE (\"id\" = 820) LIMIT 1\nD, [2017-11-27 12:17:42 #5561] [task:820] DEBUG -- DirectorJobRunner: (0.000122s) SELECT NULL\nD, [2017-11-27 12:17:42 #5561] [task:820] DEBUG -- DirectorJobRunner: (0.000053s) BEGIN. Hi @AbelHu , thanks for the help. Now I have found the correct command. Bosh cli is accepting the command but throwing following error saying volume not found. But it is actually present in Azure. Following is the error detail. \n[2017-11-27 18:33:11 #13053] [task:869] ERROR -- DirectorJobRunner: Action Failed get_task: Task b5b3835f-b379-44c0-6f25-9ad693e5be0d result: Persistent disk with volume id 'disk_name:bosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71ae;caching:None;resource_group_name:rg-sample-azure-dev1' could not be found\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:301:inhandle_method'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:356:in handle_message_with_retry'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:58:inmethod_missing'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:412:in get_task_status'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:206:inwait_for_task'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:376:in send_message'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/agent_client.rb:106:inmount_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/disk_manager.rb:169:in mount_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/disk_manager.rb:78:inattach_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:78:in handle_new_disk'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:31:inblock in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:136:in _transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:110:inblock in transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/connecting.rb:251:in block in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/connection_pool/threaded.rb:105:inhold'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/connecting.rb:251:in synchronize'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/sequel-4.32.0/lib/sequel/database/transactions.rb:99:intransaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/transactor.rb:5:in block in retryable_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:28:inblock in retryer'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:26:in loop'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/retryable.rb:26:inretryer'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/common.rb:118:in retryable'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/transactor.rb:4:inretryable_transaction'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/attach_disk.rb:29:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:96:inperform_job'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:32:in block in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:32:in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:in block in perform'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:85:inblock (3 levels) in run'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/eventmachine-1.0.4/lib/eventmachine.rb:1046:in block in spawn_threadpool'\n/var/vcap/packages/director/gem_home/ruby/2.3.0/gems/logging-1.8.2/lib/logging/diagnostic_context.rb:323:inblock in create_with_logging_context'\nD, [2017-11-27 18:33:11 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.000094s) SELECT NULL\nD, [2017-11-27 18:33:11 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.000237s) SELECT * FROM \"tasks\" WHERE (\"id\" = 869) LIMIT 1\nD, [2017-11-27 18:33:11 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.000133s) SELECT NULL\nD, [2017-11-27 18:33:11 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.000063s) BEGIN\nD, [2017-11-27 18:33:11 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.000414s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2017-11-27 18:33:11.981151+0000', \"description\" = 'attach disk ''disk_name:bosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71ae;caching:None;resource_group_name:rg-sample-azure-dev1'' to ''blueprint_z1/c6df4272-3efc-44ee-a186-ce8101fb12db''', \"result\" = 'Action Failed get_task: Task b5b3835f-b379-44c0-6f25-9ad693e5be0d result: Persistent disk with volume id...', \"output\" = '/var/vcap/store/director/tasks/869', \"checkpoint_time\" = '2017-11-27 18:32:47.015496+0000', \"type\" = 'attach_disk', \"username\" = 'admin', \"deployment_name\" = 'service-fabrik-1047-a8ff8ce0-9577-4506-be04-3483ddd2f2f2', \"started_at\" = '2017-11-27 18:30:46.968654+0000', \"event_output\" = '{\"time\":1511807591,\"error\":{\"code\":450001,\"message\":\"Action Failed get_task: Task b5b3835f-b379-44c0-6f25-9ad693e5be0d result: Persistent disk with volume id ''disk_name:bosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71ae;caching:None;resource_group_name:rg-sf-azure-dev1'' could not be found\"}}\n', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 869)\nD, [2017-11-27 18:33:12 #13053] [task:869] DEBUG -- DirectorJobRunner: (0.025650s) COMMIT\nI, [2017-11-27 18:33:12 #13053] []  INFO -- DirectorJobRunner: Task took 2 minutes 25.041717989999995 seconds to process.\nTask 869 error\nCapturing task '869' output:\n  Expected task '869' to succeed but state is 'error'\nExit code 1. @AbelHu  I am confirming again that the disk is present in the resource group. Can there be any problem with agent?. @AbelHu \nI can see the disks section of /var/vcap/bosh/setings.json contains following  but/dev/sddis not included inlsblk` command\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0     3G  0 disk\n\u2514\u2500sda1   8:1    0     3G  0 part /\nsdb      8:16   0    16G  0 disk\n\u2514\u2500sdb1   8:17   0    16G  0 part\nsdc      8:32   0    30G  0 disk\n\u251c\u2500sdc1   8:33   0   7.8G  0 part [SWAP]\n\u2514\u2500sdc2   8:34   0  22.2G  0 part /var/vcap/data\nsr0     11:0    1   632K  0 rom\n\"disks\": {                                                                                                                 \n   \"raw_ephemeral\": null,                                                                                                   \n   \"persistent\": {                                                                                                          \n     \"caching:None;disk_name:bosh-disk-data-0a8f47bc-e934-4a7f-90c5-ec775d5b71ae;resource_group_name:rg-sample-azure-dev1\": {   \n       \"path\": \"/dev/sdd\",                                                                                                  \n       \"lun\": \"1\",                                                                                                          \n       \"host_device_id\": \"{f8b3781b-1e82-4818-a1c3-63d806ec15bb}\"                                                           \n     }                                                                                                                      \n   },                                                                                                                       \n   \"ephemeral\": {                                                                                                           \n     \"path\": \"/dev/sdc\",                                                                                                    \n     \"lun\": \"0\",                                                                                                            \n     \"host_device_id\": \"{f8b3781b-1e82-4818-a1c3-63d806ec15bb}\"                                                             \n   },                                                                                                                       \n   \"system\": \"/dev/sda\"                                                                                                     \n },\n. @AbelHu  Did you check whether the disk is attached to the VM in Azure portal? ===> Not attached\nCould you see sdd by sudo fdisk -l inside the VM? ==> sdd is not listed\nI have observed the behaviour: The disk gets attached for some moment and doesn't get mounted. Then detached. Then shows the error --> volume not found.. I see the issue is to some extent similar to https://github.com/cloudfoundry-incubator/bosh-azure-cpi-release/issues/227 \nThe vm type I am using is: D2S_V3\nAnd bosh stemcell is : bosh-azure-hyperv-ubuntu-trusty-go_agent  3468 . Reopening it as seems to be bosh issue. Creating a ticket also and will update here.. Same as https://github.com/cloudfoundry/bosh/issues/1857. Thanks for opening this.. ",
    "jhedman2": "Unfortunately no, this was a brand new build i started today...  i'll look at the release notes though, thank you.. Absolutely, apologize.  Here's the document i was following: https://bosh.io/docs/init-vsphere-v1.html\n. ",
    "EleanorRigby": "@shawnho1018 : Did you figure out what happened? I am getting same error.. ",
    "shawnho1018": "@EleanorRigby  I resolved this issue. Make sure you checked your jumpbox has access to both vCenter and ESXi hosts. Upload stemcell requires connection to both instead of only vCenter. . ",
    "williammartin": "I am also seeing this on:\n```\nUsing environment '192.168.50.6' as client 'admin'\nName      Bosh Lite Director\nUUID      b6b85589-82b8-476d-9341-b51062feb96d\nVersion   264.1.0 (00000000)\nCPI       warden_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: disabled\n          dns: disabled\n          snapshots: disabled\nUser      admin\nSucceeded\n```\nbosh task --debug output:\n```\nUsing environment '192.168.50.6' as client 'admin'\nTask 120\nI, [2017-12-14T11:51:23.360134 #2910] [0x2b1ab95d2f4c]  INFO -- TaskHelper: Director Version: 264.1.0\nI, [2017-12-14T11:51:23.360178 #2910] [0x2b1ab95d2f4c]  INFO -- TaskHelper: Enqueuing task: 120\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Looking for task with task id 120\nD, [2017-12-14T11:51:23 #650271] [] DEBUG -- DirectorJobRunner: (0.000337s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Found task #120, :state=>\"processing\", :timestamp=>2017-12-14 11:51:23 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/120\", :checkpoint_time=>2017-12-14 11:51:23 UTC, :type=>\"update_deployment\", :username=>\"admin\", :deployment_name=>\"cf\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Running from worker 'worker_3' on director/3cde8d74-66c1-4fdc-5850-0a770078f2e8 (127.0.0.1)\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Starting task: 120\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Creating job\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000271s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000269s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Performing task: #120, :state=>\"processing\", :timestamp=>2017-12-14 11:51:23 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/120\", :checkpoint_time=>2017-12-14 11:51:23 UTC, :type=>\"update_deployment\", :username=>\"admin\", :deployment_name=>\"cf\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000205s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000371s) UPDATE \"tasks\" SET \"state\" = 'processing', \"timestamp\" = '2017-12-14 11:51:23.648408+0000', \"description\" = 'create deployment', \"result\" = NULL, \"output\" = '/var/vcap/store/director/tasks/120', \"checkpoint_time\" = '2017-12-14 11:51:23.648534+0000', \"type\" = 'update_deployment', \"username\" = 'admin', \"deployment_name\" = 'cf', \"started_at\" = '2017-12-14 11:51:23.648481+0000', \"event_output\" = '', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000730s) COMMIT\nI, [2017-12-14T11:51:23 #650271] [task:120]  INFO -- DirectorJobRunner: Reading deployment manifest\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000676s) SELECT * FROM \"tasks\" WHERE \"id\" = 120\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000346s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000489s) INSERT INTO \"events\" (\"parent_id\", \"timestamp\", \"user\", \"action\", \"object_type\", \"object_name\", \"error\", \"task\", \"deployment\", \"instance\", \"context_json\") VALUES (NULL, '2017-12-14 11:51:23.655477+0000', 'admin', 'update', 'deployment', NULL, 'no implicit conversion of nil into String', '120', NULL, NULL, '{}') RETURNING *\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000713s) COMMIT\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000312s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000357s) UPDATE \"tasks\" SET \"event_output\" = (\"event_output\" || '{\"time\":1513252283,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n') WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000593s) COMMIT\nE, [2017-12-14T11:51:23 #650271] [task:120] ERROR -- DirectorJobRunner: no implicit conversion of nil into String\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:377:in parse'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:377:inparse_stream'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:325:in parse'\n/var/vcap/packages/ruby-2.4/lib/ruby/2.4.0/psych.rb:252:inload'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:28:in perform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:99:inperform_job'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in block in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:49:inwith_thread_name'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:inperform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:in block in perform'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:83:inblock (3 levels) in run'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/eventmachine-1.2.5/lib/eventmachine.rb:1076:in block in spawn_threadpool'\n/var/vcap/packages/director/gem_home/ruby/2.4.0/gems/logging-2.2.2/lib/logging/diagnostic_context.rb:474:inblock in create_with_logging_context'\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000182s) SELECT * FROM \"tasks\" WHERE (\"id\" = 120) LIMIT 1\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000202s) BEGIN\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.000348s) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2017-12-14 11:51:23.662943+0000', \"description\" = 'create deployment', \"result\" = 'no implicit conversion of nil into String', \"output\" = '/var/vcap/store/director/tasks/120', \"checkpoint_time\" = '2017-12-14 11:51:23.648534+0000', \"type\" = 'update_deployment', \"username\" = 'admin', \"deployment_name\" = 'cf', \"started_at\" = '2017-12-14 11:51:23.648481+0000', \"event_output\" = '{\"time\":1513252283,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 120)\nD, [2017-12-14T11:51:23 #650271] [task:120] DEBUG -- DirectorJobRunner: (0.001053s) COMMIT\nI, [2017-12-14T11:51:23 #650271] []  INFO -- DirectorJobRunner: Task took 0.019790606 seconds to process.\n```\nWhen I do bosh manifest, the task reports it succeeded but I don't get the manifest back.. No, it failed. I'm just reiterating that this issue (the error message returned) is not so good. Thanks.. This is reproducible by following the standard bosh-deployment and cf-deployment instructions for a local bosh deployment from current master :(\nhttps://bosh.io/docs/bosh-lite/\nhttps://github.com/cloudfoundry/cf-deployment/blob/master/deployment-guide.md. ",
    "jamesjoshuahill": "@cppforlife, what's the release cadence for the bosh-template gem? This issue came up during https://github.com/pivotal-cf/syslog-migration-release/pull/5. Awesome! Thank you @dpb587-pivotal \ud83d\ude03. Hi hi\nThank you for your comments @dpb587 and @mfine30.\n@miguelverissimo we have added the else you suggested. This skip message will be visible in bbr output when the --debug flag is set.\nJosh and @alamages . Hi @h4xnoodle \nThank you for merging our PR. How can we track the next release of v266.x? Do you know when the next patch release is due?\nJosh and @alamages . ",
    "damzog": "Did you find a way to repair the bosh config?. ",
    "fatbearinc": "For both Meltdown and Spectre, I am very much interested in knowing when the stemcells will be available for:\n\nbosh-openstack-kvm-centos-7-go_agent\nbosh-vsphere-esxi-centos-7-go_agent\n\nIs there any goal date for this?. To make the question concrete, consider the following part of a manifest.MF file inside of a release.tgz file that contains an embedded deploy.tgz file:\njobs:\n- fingerprint: 17b3eeebd5ecbb578705b6b083e930ff6ff66a9d\n  name: deploy\n  sha1: ecd4e3bfa8c1a07ed774645f6901a3e8b550d6ab\n  version: 17b3eeebd5ecbb578705b6b083e930ff6ff66a9d\nSuppose that I modify corresponding deploy.tgz and want to rebuild the BOSH release.\n\nI can use sha1sum to update the sha1 field above. If not, what?\nHow can I update the fingerprint field?\nHow can I update the version field?\n\nThanks,\nSteve. ",
    "xtreme-andrew-su": "Closing and reopening to create tracker story.. Closing and reopening to create tracker story.. @ramonskie with the completion of https://www.pivotaltracker.com/story/show/157571931 and https://www.pivotaltracker.com/story/show/157569428 perhaps your specific usecase is handled? I'm closing this issue for now, please feel free to reopen if the issue still persists.. @ramonskie with the completion of https://www.pivotaltracker.com/story/show/157571931 and https://www.pivotaltracker.com/story/show/157569428 perhaps your specific usecase is handled? I'm closing this issue for now, please feel free to reopen if the issue still persists.. Closing as there was documentation updates\n\nYou must ensure that your drain script exits in one of following ways:\n\n\nexit with a non-0 exit code to indicate drain script failed\n\n\nexit with 0 exit code and also print an integer followed by a newline to stdout (nothing else must be printed to stdout):\n\n\nstatic draining: If the drain script prints a zero or a positive integer, BOSH sleeps for that many seconds before continuing.\ndynamic draining: If the drain script prints a negative integer, BOSH sleeps for that many seconds, then calls the drain script again.. Closing as there was documentation updates\nYou must ensure that your drain script exits in one of following ways:\n\n\nexit with a non-0 exit code to indicate drain script failed\n\n\nexit with 0 exit code and also print an integer followed by a newline to stdout (nothing else must be printed to stdout):\n\n\nstatic draining: If the drain script prints a zero or a positive integer, BOSH sleeps for that many seconds before continuing.\ndynamic draining: If the drain script prints a negative integer, BOSH sleeps for that many seconds, then calls the drain script again.. Hi @Benjamintf1 you can take a look at https://bosh.io/docs/pre-start/ for doing malformed and invalid setting checks. This provides authors of their own releases to manage the preconditions of their application. The logs would still live on the machine and it should be the operator's choice to check these for more information. They can also leverage bosh logs command to get the logs back.. Hi @Benjamintf1 you can take a look at https://bosh.io/docs/pre-start/ for doing malformed and invalid setting checks. This provides authors of their own releases to manage the preconditions of their application. The logs would still live on the machine and it should be the operator's choice to check these for more information. They can also leverage bosh logs command to get the logs back.. Hey @JamesClonk, I want to confirm, are there any other changes to your manifest between your usage on BOSH 265 and 266?\n\nFrom the stacktrace, the error is coming from the provider.\nFrom our testing, we see that this only happens when you meet the following conditions:\nProvider must be on manual network + is an errand\nIn 265.x we also see this happen under the similar conditions:\nProvider must be on manual network + is an errand + shared\nor\nProvider must be on manual network + is an errand + consumed\nWe were not able to reproduce this from the details provided above:\nProvider is on a manual network and not an errand.\nConsumer is an errand which consumes the above provider\nSome information we'd like to gather for better diagnosis:\n- Any changes that were made to your manifest.\n- Any changes that were made to your cloud-config.\n- Whether your mysql instance group has lifecycle: errand defined in the manifest.\n- The network types (eg. manual, dynamic, vip) for mysql instance group.\n- The network types (eg. manual, dynamic, vip) for bootstrap instance group.\n. Hey @JamesClonk, I want to confirm, are there any other changes to your manifest between your usage on BOSH 265 and 266?\nFrom the stacktrace, the error is coming from the provider.\nFrom our testing, we see that this only happens when you meet the following conditions:\nProvider must be on manual network + is an errand\nIn 265.x we also see this happen under the similar conditions:\nProvider must be on manual network + is an errand + shared\nor\nProvider must be on manual network + is an errand + consumed\nWe were not able to reproduce this from the details provided above:\nProvider is on a manual network and not an errand.\nConsumer is an errand which consumes the above provider\nSome information we'd like to gather for better diagnosis:\n- Any changes that were made to your manifest.\n- Any changes that were made to your cloud-config.\n- Whether your mysql instance group has lifecycle: errand defined in the manifest.\n- The network types (eg. manual, dynamic, vip) for mysql instance group.\n- The network types (eg. manual, dynamic, vip) for bootstrap instance group.\n. From your example, I created a sample test-release. \nCreating a simple bosh-lite warden director using the latest bosh-deployment v266.2.0 then deployed using this manifest which as far as I can tell is the same as your example.\nI was not able to reproduce this issue.\nI've included a few other manifest which shows the issue you're seeing but only for providers that are errands.\nYou can reach out to me on https://cloudfoundry.slack.com/messages/bosh for @asu.. From your example, I created a sample test-release. \nCreating a simple bosh-lite warden director using the latest bosh-deployment v266.2.0 then deployed using this manifest which as far as I can tell is the same as your example.\nI was not able to reproduce this issue.\nI've included a few other manifest which shows the issue you're seeing but only for providers that are errands.\nYou can reach out to me on https://cloudfoundry.slack.com/messages/bosh for @asu.. @JamesClonk FYI\nRelated stories:\n- https://www.pivotaltracker.com/story/show/158444025\n- https://www.pivotaltracker.com/story/show/158444050\n. @JamesClonk FYI\nRelated stories:\n- https://www.pivotaltracker.com/story/show/158444025\n- https://www.pivotaltracker.com/story/show/158444050\n. I think that's the next step once we have the appropriate stemcell defined in the cpi.yml\nafaik, version xenial 170.2+ of heavy stemcells is registry-less ready, the light stemcells will be any version after 170.6+. Once those are out, I'm all for adding a new ops-file.\nGoing to close this issue for now. Added a story: https://www.pivotaltracker.com/story/show/162017526. I think that's the next step once we have the appropriate stemcell defined in the cpi.yml\nafaik, version xenial 170.2+ of heavy stemcells is registry-less ready, the light stemcells will be any version after 170.6+. Once those are out, I'm all for adding a new ops-file.\nGoing to close this issue for now. Added a story: https://www.pivotaltracker.com/story/show/162017526. Overall it looks good and I do like the change into smaller components. Some parts feel like they can still be broken down or even moved to their own class.. Overall it looks good and I do like the change into smaller components. Some parts feel like they can still be broken down or even moved to their own class.. This comment can be removed.. Similarly, this can also be removed.. ",
    "andrearagao": "What happens if that single director crashes? . ",
    "chirangaalwis": "@cppforlife I am running create-env from outside of this vpc. I am not using a jumpbox, in this case.. @cppforlife I got it working. Missed out on the documentation related to exposing environment on a public IP. Yet, I would like to suggest minor improvements to the original documentation.\nThe command used is as follows:\nbosh create-env bosh-deployment/bosh.yml \\\n    --state=state.json \\\n    --vars-store=creds.yml \\\n    -o bosh-deployment/aws/cpi.yml \\\n    -o bosh-deployment/external-ip-with-registry-not-recommended.yml \\\n    -v director_name=aws \\\n    -v internal_cidr=10.0.0.0/24 \\\n    -v internal_gw=10.0.0.1 \\\n    -v internal_ip=10.0.0.6 \\\n    -v access_key_id=<some_value> \\\n    -v secret_access_key=<some_value> \\\n    -v region=us-east-2 \\\n    -v az=us-east-2a \\\n    -v default_key_name=bosh \\\n    -v default_security_groups=[bosh] \\\n    --var-file private_key=<path>/bosh.pem \\\n    -v subnet_id=subnet-xxxxxxxx \\\n    -v external_ip=<AWS_Elastic_IP>\nIn section Step 2: Deploy, it would be better if a mapping is provided in terms the terminology (e.g. the created AWS Elastic IP is referred as the external-ip in this documentation). And better if explanations are provided in terms of what is happening as a 'dumb' user may wonder what exactly happens to the AWS resources they create in Step 1.. @cppforlife can you elaborate on this further? Plus, is there any alternative way to achieve this?. @cppforlife my effort is to share the directory across the job VMs (cloud servers). A cloud server could be a container, VM or even a physical machine (as defined here), depending on the infrastructure. In the case of BOSH Lite, we get containers.\nHence, I don't see how I can achieve the target by simply mounting the directory to the Director VM in the case of BOSH Lite. Please correct me if I am wrong.. @dpb587-pivotal, @benmoss I got this working. I was misguided by the Using environment '18.218.240.11' as client 'admin' thinking that I had already logged in. Realized that I was not logged out when checked with bosh env.\nBTW, sorry for the late response.. ",
    "cdutra": "@dmlemos That's actually a CPI issue on starting on v63. We did a refactor and it caused a regression to only override if auto_assign_public_ip is true. In your case you're using string but you should use boolean instead.\nThe working version you mentioned is most likely AWS CPI v62 or earlier.\nWe added a bug in our backlog to fix that.\ncc @jaresty . Hi @kreamyx,\nThank you for submitting the PR. We're in the process of reviewing it and we left some comments.\nAlso we noticed that the commits could be merged and the commit message could better capture the proposed changes.\nIn the PR you mentioned you should have a meeting with @cppforlife about changing the interface for create_network call. @cppforlife suggested we should allow users to have the ability specify only netmask in case the user doesn't care about the actual network address, only the size of it; this way the cpi could pick one based on the IaaS current state. Did you you all come to a conclusion?\ncc @belinda-liu . We tested on director 268.5.0 and it's fixed, thanks! . What is managed = managed? Can you just pass managed to the method?. Could you move this to a separate component like a NetworkOrphaner? So the Deployment deleter doesn't need to know how to orphan networks.. It seems odd to have a setter for managed property. You can have managed defaulting to false in the constructor.. def initialize(name, subnets, logger, managed = false). new(name, subnets, logger, managed). az_cloud_properties.merge!(availability_zone.cloud_properties). Could you clarify why the response is different from what is described in the doc.. Could you add a test to cover the case of adding a subnet to the network that already exists and will be used by the deployment. According to the code it should fail.. ",
    "K7king": "Thank You.. ",
    "nsharmacovs": "Thanks, i was looking for part second of your answer .. Thanks, i was looking for part second of your answer .. ",
    "VincenzoDo": "@cppforlife I've tried with different users (ldap user and local administrator) and still get the same error. And as you mentioned, the error isn't clear at all.. @ktchen14 here you go: https://pastebin.ubuntu.com/p/NNJqyKbQnR/. @ktchen14 this is the create-env (it's the default taken form the official tutorial):\nbosh create-env bosh-deployment/bosh.yml \\\n    --state=state.json \\\n    --vars-store=creds.yml \\\n    -o bosh-deployment/vsphere/cpi.yml \\\n    -v director_name=bosh-1 \\\n    -v internal_cidr=10.0.0.0/24 \\\n    -v internal_gw=10.0.0.1 \\\n    -v internal_ip=10.0.0.2 \\\n    -v network_name=\"MyNet\" \\\n    -v vcenter_dc=datacenter1 \\\n    -v vcenter_ds=disk_1 \\\n    -v vcenter_ip=vcenterIP \\\n    -v vcenter_user='administrator'\\\n    -v vcenter_password='xxxxxx' \\\n    -v vcenter_templates=bosh-1-templates \\\n    -v vcenter_vms=bosh-1-vms \\\n    -v vcenter_disks=bosh-1-disks \\\n    -v vcenter_cluster=Compute\nValues are adapted to my environment. I've tried to put quotes (single and double) around both vcenter_user and password but nothing changed, I've also tried different formats of usernames (local and ldap) for example: 'administrator', 'administrator@vsphere.local' or 'domain\\domainuser' nothing seems to work. As for the password I tried escaping the special characters, still nothing.\nDo you have any other suggestion? I'm I doing something wrong? Is there a way to test just user auth.?. Not really since I stopped using bosh. What I can say is that it wasn't a password problem because I've tried with simpler ones and it still failed.. ",
    "ktchen14": "The debug log is no longer available. @VincenzoDo would you mind uploading them again? Thanks.. @VincenzoDo according to the logs vCenter is returning a 403 Forbidden when we attempt to connect to it and issue the RetrieveServiceContent call. This is the first call that any vCenter client must issue when connecting to vCenter.\nCan I see the exact bosh create-env command that you're issuing? Make sure that if you're passing the vSphere username and password as variables to be interpolated (using -v) that they're properly quoted so that bash isn't mangling them.. ",
    "muralisc": "@cppforlife Took a stab at this issue. Can you take a look and let me know the comments.. @cppforlife i have exposed sap-cloudfoundry on my profile.. @dpb587-pivotal hi, i have updated the visibility.. @cppforlife The issue we are trying to address is not only an orphaned disk, but an arbitrary disk created by infrastructure. Initial solution was to read the details of the new disk from the infrastructure and create persistent_disk entry.\nThe issue link with the AWS CPI prs and comments : https://github.com/cloudfoundry/bosh/issues/1900\n. Closing this and raising https://github.com/cloudfoundry/bosh/pull/2015. @voelzmo , updated https://github.com/cloudfoundry/bosh-cli/pull/459 as well.. Sure, Updated the the PR with changes. ",
    "Benjamintf1": "Yeah, we were actually using pre-start scripts at the time. The idea is perhaps you, in some cases, should be louder then bosh logs(which are often not looked at unless an error occurs), or quieter then causing failed deploys(especially given it can place you in a half deployed state). \nHere's a few examples:\n1) The operator deploys the product with insecure settings(should be warned that settings are insecure)\n2) The product is able to execute some features, but not other features\n3) The product, some set of features, or some settings used are deprecated and due to be removed on the next major release. \nThis would perhaps appear besides other warnings and log lines in the bosh deploy process. \nTask 8104 | 16:23:53 | Compiling packages: package/guid (00:01:57)\nTask 8104 | 16:28:03 | Creating missing vms: vm/id (0)\nTask 8104 | 16:29:29 | Updating instance redis: vm/id (0) (canary) (00:01:19)\nTask 8104 | 16:30:29 | WARNING: job.property will be deprecated in the next release. Use job.different_name\nTask 8104 | 16:30:30 | WARNING: job.key is possibly an insecure key. Consider increasing the key-size. Stale issue, still a problem. . ",
    "ywysuibian": "@voelzmo thank you for your kindly response.\nI am not sure about multiple networks, maybe there is some misunderstanding in my previous description. \nI was using this kind of configuration for almost two years, and I just upgraded bosh director from a very lower version to 263.3, then upgraded cf from 253 to 270. During cf upgrading, all VMs were recreated (maybe due to stemcell upgrading),  and then bosh vms cannot display private ip of VMs which also configured with an floating ip.\nMy deployment yml looks like this\nyaml\ninstance_groups:\n...\n- instances: 1\n  jobs:\n  - name: aaaa\n    release: aaaa-release\n  - name: bbbb\n    release: bbbb-release\n  - name: cccc\n    release: cccc-release\n  name: xxxx\n  networks:\n  - default:\n    - dns\n    - gateway\n    name: default\n  - name: floating\n    static_ips:\n    - xxx.xxx.xxx.ipb\nIn the response from bosh_agent to director, I foud\n```json\n{\n...\n  \"networks\": {\n    \"default\": {\n      \"cloud_properties\": {\n  },\n  \"default\": [\n    \"dns\",\n    \"gateway\"\n  ],\n  \"dns\": [\n    \"xxx.xxx.xxx.xxx\",\n    \"xxx.xxx.xxx.ipa\"\n  ],\n  \"gateway\": \"xxx.xxx.xxx.1\",\n  \"ip\": \"xxx.xxx.xxx.ipa\",\n  \"netmask\": \"255.255.255.0\",\n  \"type\": \"dynamic\"\n},\n\"floating\": {\n  \"cloud_properties\": {\n\n  },\n  \"ip\": \"xxx.xxx.xxx.ipb\",\n  \"type\": \"vip\"\n}\n\n}\n...\n}\n```\nBut in bosh director database, I can only find one record in ip_addresses table related to this VM. After I delete this record, bosh vms became to display both ips.. @dpb587-pivotal I am using cloud-config for the first time.. I worked around by manually delete the address record in bosh postgres database. \nClose this issue.. ",
    "lordcf": "HI,\nwe tried using job postgres-9.4, now facing the error as shown below.\nError 80011: Package name collision detected in instance group 'bosh': job 'bosh/postgres-9.4' depends on package 'bosh/postgres-9.4', job 'shield/shield-agent' depends on 'shield/postgres-9.4'. BOSH cannot currently collocate two packages with identical names from separate releases.\n. @dpb587 : Thanks for response. After adding 'migrated_from' in bosh.yml , The Error is resolve and bosh update successfully but we loose our previous deployment which was running on bosh .All deployment went to orphaned now. any suggestion for this..\n . Hi @dpb587 : yes. BOSH lost its database.\nWe have capture the disc mounted on instance before update and after update.\nBOSH Before update :\ndf -h                                                                                         \nFilesystem      Size  Used Avail Use% Mounted on                                                \nudev            7.9G  4.0K  7.9G   1% /dev                                                      \ntmpfs           1.6G  292K  1.6G   1% /run                                                      \n/dev/xvda1      2.9G  1.2G  1.6G  42% /                                                         \nnone            4.0K     0  4.0K   0% /sys/fs/cgroup                                            \nnone            5.0M     0  5.0M   0% /run/lock                                                 \nnone            7.9G  4.0K  7.9G   1% /run/shm                                                  \nnone            100M     0  100M   0% /run/user                                                 \n/dev/xvdb2       13G  664M   11G   6% /var/vcap/data                                            \ntmpfs           1.0M   56K  968K   6% /var/vcap/data/sys/run                                    \n/dev/xvdf1       20G   90M   19G   1% /var/vcap/store     \nAfter Update BOSH:\nFilesystem      Size  Used Avail Use% Mounted on                                                    \nudev            7.9G     0  7.9G   0% /dev                                                          \ntmpfs           1.6G   13M  1.6G   1% /run                                                          \n/dev/xvda1      2.9G  1.5G  1.3G  55% /                                                             \ntmpfs           7.9G     0  7.9G   0% /dev/shm                                                      \ntmpfs           5.0M     0  5.0M   0% /run/lock                                                     \ntmpfs           7.9G     0  7.9G   0% /sys/fs/cgroup                                                \n/dev/xvdb2      1.5G  1.3G  115M  92% /var/vcap/data                                                \ntmpfs           1.0M   60K  964K   6% /var/vcap/data/sys/run  \nAfter update we are not able to see  the /var/vcap/store.\n is it default behaviour or we are missing some configuration?\n. @dpb587 and @h4xnoodle : Thanks for suggestion there was an persistence disc misconfiguration in manifest. after correct, it worked. . Closing this issue. This error because of subnet mismtach.\n. ",
    "challa": "thanks for quick reply.\nwhen you say \"teams are automatically created within the director when they are necessary\"  how does bosh know when to create a team and with what name.\nsay i want to create  teams \"team1\", \"team2\", \"Team3\", how do i create that. is it possible to create this way.\nbut for me once teams are created (in my local set up i update in DB directly) i am able create to deployments into a certain team by setting the corresponding team scope to the UAA client.\nStill i am not able to figure out how to create teams with my preferred name. \n. @cppforlife  Thanks for the providing the necessary steps. ",
    "abh1kg": "@cppforlife : Can this be merged?. ",
    "ashishjain14": "@cppforlife @dpb587-pivotal  this is an important fix for us and would help if we an expedite the review and merge.. @cppforlife @dpb587-pivotal  this is an important fix for us and would help if we an expedite the review and merge.. ",
    "MattSurabian": "I'm a little behind the times and haven't used BBR yet. I've had success rebuilding directors using only a backup of the director's postgres database and bosh cck to restore deployments.\nYour mileage may vary on that since I'm also not using UAA or CredHub though. Curious to see what Bosh maintainers have to say.. Yah I wouldn't want to add NOPASSWD sudo on the director. Unfortunately I've not been using bosh backup or bbr so my experience won't be of any use on that error. I've only connected to postgres and dumped the database used by the director \"manually\" (scripted but not using any tooling other than regular psql). So doing a backup totally out of band. Like I said, probably not what you should be looking at doing, but it has worked, so there's that as a last resort.. I should also mention that I've moved all statefull things off the director like the registry so the director has nothing locally to persist.. It seems like things will work as expected provided you have a latest credhub and director version. The latest bosh director always sends the parameter and the latest credhub respects it. If you run latest credhub with an older bosh director version that doesn't send the parameter things won't be so ideal.\nI probably should make a PR to update the docs but just haven't found the time yet. . Though the more I've thought about it the more I think it is the director's job to issue the GET first.. ",
    "keithkroeger": "Hi @MattSurabian I assume you're using the bosh v1 cli bosh backup [https://bosh.io/docs/director-backup.html] for this work, correct.\nWe are attempting bbr, but receiving finding scripts failed on bosh/0: sudo: no tty present and no askpass program specified - exit code 1  in response to ./bbr director --key openstack/id_rsa  --username vcap --host $BOSH_ENVIRONMENT pre-backup-check\nOur bosh director was deployed using create-env\nI know i can connect to my bosh director and add vcap to the list of sudo-ers with NOPASSWD, but this would have to be done manually.. Hi @MattSurabian I assume you're using the bosh v1 cli bosh backup [https://bosh.io/docs/director-backup.html] for this work, correct.\nWe are attempting bbr, but receiving finding scripts failed on bosh/0: sudo: no tty present and no askpass program specified - exit code 1  in response to ./bbr director --key openstack/id_rsa  --username vcap --host $BOSH_ENVIRONMENT pre-backup-check\nOur bosh director was deployed using create-env\nI know i can connect to my bosh director and add vcap to the list of sudo-ers with NOPASSWD, but this would have to be done manually.. Thank you @cppforlife \nIn that case, do you see a solution to our error finding scripts failed on bosh/0: sudo: no tty present and no askpass program specified - exit code 1?\nIt appears we have to edit the SUDOERS to not require a password for vcap.\nWe've done this manually to get a backup to run, but this isn't sustainable as we'd need to do this every time the bosh director is re-created.. Thank you @cppforlife \nIn that case, do you see a solution to our error finding scripts failed on bosh/0: sudo: no tty present and no askpass program specified - exit code 1?\nIt appears we have to edit the SUDOERS to not require a password for vcap.\nWe've done this manually to get a backup to run, but this isn't sustainable as we'd need to do this every time the bosh director is re-created.. Thank you, @mfine30.  That helps a lot!. Thank you, @mfine30.  That helps a lot!. ",
    "jhohiii": "Apparently, I don't have a cpi-config:\n[ jhoh@cvglppcfutl01 ~ ] $ bosh cpi-config\nUsing environment '10.51.17.15' as client 'admin'\nNo CPI config\nExit code 1\nWhere do I configure that?\nAll of these VMs get successfully created in either AZ (Cluster) on their corresponding local only storage (one VxRail-.* datastore per cluster).  The issue comes when persistent disk is being created (before it is attached).  The networking is correct.  We have a single VLAN across both clusters, but with different portgroup names.  They are just labels.  I will change the name in the second AZ - prepending 'AZ2-' to match the prepend of 'AZ1-'.\nI'm really interested in how I update my CPI config.\n@schmidtsv . @schmidtsv \n I could have sworn that I added the bootstrap script...oh well, here it is...\n```\nbosh create-env bosh-deployment/bosh.yml \\\n    --state=state.json \\\n    --vars-store bosh-utility-1/creds.yml \\\n    -o bosh-deployment/vsphere/cpi.yml \\\n    -o bosh-deployment/vsphere/resource-pool.yml \\\n    -o bosh-deployment/misc/dns.yml \\\n    -v director_name=utility-bosh-1 \\\n    -v internal_ip=class-b.17.15 \\\n    -v internal_gw=class-b.16.1 \\\n    -v internal_dns=[class-b2.144.177,class-b3.144.17] \\\n    -v internal_cidr=class-b.16.0/22 \\\n    -v network_name=\"vlan-17-IAAS-PCF-Infra\" \\\n    -v vcenter_dc=cvg_iaas \\\n    -v vcenter_ds=\"VxRail-Virtual-SAN-Datastore-.*\" \\\n    -v vcenter_ip=class-b.4.22 \\\n    -v vcenter_user= \\\n    -v vcenter_password= \\\n    -v vcenter_templates=utility-1-templates \\\n    -v vcenter_vms=utility-1-vms \\\n    -v vcenter_disks=utility-1-disks \\\n    -v vcenter_rp=bosh-utility-1 \\\n    -v vcenter_cluster=cvg_iaas_internal\n```. @schmidtsv \n```\n- cvg_iaas_internal:\n    resource_pool: bosh-utility-1\nSucceeded\n```\nAt the --path /instance_groups/name=bosh/properties/vcenter level:\ndatacenters:\n- clusters:\n  - cvg_iaas_internal:\n      resource_pool: bosh-utility-1\n  datastore_pattern: VxRail-Virtual-SAN-Datastore-.*\n  disk_path: utility-1-disks\n  name: cvg_iaas\n  persistent_datastore_pattern: VxRail-Virtual-SAN-Datastore-.*\n  template_folder: utility-1-templates\n  vm_folder: utility-1-vms. That works and makes sense when you describe it that way.  I'd much rather deploy bosh with a complete manifest instead of all those \"-v\" command line arguments.  \nAnyway, now it is failing:\n```Deployment manifest: '/home/jhoh/git/utility-bosh/bosh-deployment/bosh.yml'\nDeployment state: 'state.json'\nStarted validating\n  Downloading release 'bosh'... Skipped [Found in local cache] (00:00:00)\n  Validating release 'bosh'... Finished (00:00:00)\n  Downloading release 'bosh-vsphere-cpi'... Skipped [Found in local cache] (00:00:00)\n  Validating release 'bosh-vsphere-cpi'... Finished (00:00:00)\n  Validating cpi release... Finished (00:00:00)\n  Validating deployment manifest... Finished (00:00:00)\n  Downloading stemcell... Skipped [Found in local cache] (00:00:00)\n  Validating stemcell... Finished (00:00:02)\nFinished validating (00:00:04)\nStarted installing CPI\n  Compiling package 'ruby-2.4-r3/8471dec5da9ecc321686b8990a5ad2cc84529254'... Finished (00:00:00)\n  Compiling package 'vsphere_cpi/3049e51ead9d72268c1f6dfb5b471cbc7e2d6816'... Finished (00:00:00)\n  Compiling package 'iso9660wrap/82cd03afdce1985db8c9d7dba5e5200bcc6b5aa8'... Finished (00:00:00)\n  Installing packages... Finished (00:00:00)\n  Rendering job templates... Finished (00:00:00)\n  Installing job 'vsphere_cpi'... Finished (00:00:00)\nFinished installing CPI (00:00:01)\nStarting registry... Finished (00:00:00)\nUploading stemcell 'bosh-vsphere-esxi-ubuntu-trusty-go_agent/3468.21'... Skipped [Stemcell already uploaded] (00:00:00)\nStarted deploying\n  Waiting for the agent on VM 'vm-8baa895f-1c30-4b93-a368-da1dca45ca97'... Finished (00:00:00)\n  Stopping jobs on instance 'unknown/0'... Finished (00:00:00)\n  Unmounting disk 'disk-b15c8465-ea45-4562-8822-ec5a14d86317'... Finished (00:00:01)\n  Deleting VM 'vm-8baa895f-1c30-4b93-a368-da1dca45ca97'... Finished (00:00:10)\n  Creating VM for instance 'bosh/0' from stemcell 'sc-98d32778-2acb-4899-af74-dc27c94c3047'... Finished (00:00:12)\n  Waiting for the agent on VM 'vm-45f8a435-c10a-4544-a31e-665770120752' to be ready... Finished (00:00:27)\n  Attaching disk 'disk-b15c8465-ea45-4562-8822-ec5a14d86317' to VM 'vm-45f8a435-c10a-4544-a31e-665770120752'... Finished (00:00:15)\n  Rendering job templates... Failed (00:00:03)\nFailed deploying (00:01:22)\nStopping registry... Finished (00:00:00)\nCleaning up rendered CPI jobs... Finished (00:00:00)\nDeploying:\n  Building state for instance 'bosh/0':\n    Rendering job templates for instance 'bosh/0':\n      Rendering templates for job 'vsphere_cpi/7ef406ee1d230b5322d8dc8ccce5da6bbbdabe7c':\n        Rendering template src: cpi.json.erb, dst: config/cpi.json:\n          Rendering template src: /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/bosh-release-job368514904/templates/cpi.json.erb, dst: /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/rendered-jobs340009586/config/cpi.json:\n            Running ruby to render templates:\n              Running command: 'ruby /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/erb-renderer041303363/erb-render.rb /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/erb-renderer041303363/erb-context.json /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/bosh-release-job368514904/templates/cpi.json.erb /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/rendered-jobs340009586/config/cpi.json', stdout: '', stderr: '/home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/erb-renderer041303363/erb-render.rb:189:in rescue in render': Error filling in template '/home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/bosh-release-job368514904/templates/cpi.json.erb' for vsphere_cpi/0 (line 44: #<NoMethodError: undefined methodinject' for nil:NilClass>) (RuntimeError)\n        from /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/erb-renderer041303363/erb-render.rb:175:in render'\n        from /home/jhoh/.bosh/installations/bf01fd76-af1d-46a1-587a-6b0d93900a5d/tmp/erb-renderer041303363/erb-render.rb:200:in'\n':\n                exit status 1\nExit code 1\n```\nbosh create-env bosh-deployment/bosh.yml \\\n    --state=state.json \\\n    --vars-store bosh-utility-1/creds.yml \\\n    -o bosh-deployment/vsphere/cpi.yml \\\n    -o bosh-deployment/vsphere/resource-pool.yml \\\n    -o bosh-deployment/misc/dns.yml \\\n    -o cluster-2.yml \\\n<same -v options as above>\ncluster-2.yml\n- type: replace\n  path: /instance_groups/name=bosh/properties/vcenter/datacenters/0/clusters\n  value:\n  - cvg_iaas_internal:\n    resource_pool: bosh-utility-1\n  - cvg_iaas_internal_1:\n    resource_pool: bosh-utility-1. @schmidtsv \nThat completely solved my issue.\nThank you for the education.  Is there a more complete resource for documentation?  It seems like some of the more common enterprise deployment options are not documented, but are communicated through Q&A.\n. ",
    "bandesz": "@dpb587-pivotal oh wow, thanks, this is a hilarious mistake :D I didn't even realise it until now that we accidentally used <%=. And because of the starting # it didn't cause any issues in the yaml file.\nSolved :). I did some digging and the Nginx error log file is configured as /var/vcap/sys/log/director/error.log and Nginx is able to write to it. I wasn't able to find why is the other error.log being created.. ",
    "aeijdenberg": "FWIW I'm seeing the same issue in Ubuntu 18.04. The error message I see when adding --debug appears to be:\nE, [2018-10-24T18:23:59.164206 #10707] [task:11] ERROR -- DirectorJobRunner: Downloading remote release from https://bosh.io/d/github.com/cloudfoundry/binary-buildpack-release?v=1.0.27 failed: #<SocketError: Failed to open TCP connection to bosh.io:443 (getaddrinfo: Temporary failure in name resolution)>. Managed to resolve eventually by recreating the env a bunch of times, and updating VirtualBox to the latest patch level VirtualBox 5.2.20 and mucking around with some resolvconf voodoo I found on the internet.\nI suspect update to latest patch level made the difference.. We recently contributed a PR that allows for certificate rotation within a bosh deployment (not the bosh certs itself) here: https://github.com/cloudfoundry/bosh-cli/pull/490\nThe linked README includes a variant of our bash scripts we run as part of our CD that stops the deploy if it detects certificates (in either credhub or creds.yml) that are expiring with 30 days.. We recently contributed a PR that allows for certificate rotation within a bosh deployment (not the bosh certs itself) here: https://github.com/cloudfoundry/bosh-cli/pull/490\nThe linked README includes a variant of our bash scripts we run as part of our CD that stops the deploy if it detects certificates (in either credhub or creds.yml) that are expiring with 30 days.. @freddesbiens - the linked epic refers to rotations of certificates declared in bosh manifests.\nThe start of this issue talks about certificates managed by the director itself - this is probably quite distinct.\ne.g. bosh agent certificates. @freddesbiens - the linked epic refers to rotations of certificates declared in bosh manifests.\nThe start of this issue talks about certificates managed by the director itself - this is probably quite distinct.\ne.g. bosh agent certificates. It seems as though with each deploy bosh is generating a different .bosh/links.json file inside of each job directory.\nI ran a successful deploy with v268.5.0 and saved a copy of config for a job, then ran a deploy again and noted the only difference was this file.\nIt would appear that this file shared the same logical content, however the order of the array elements differs with each deploy.\ne.g.\n```bash\nFiles have different content\n$ cat a/.bosh/links.json | shasum\n32437ce93d8e583b0c6a9bd2c263ef662512de6e  -\n$ cat b/.bosh/links.json | shasum\n70fed0b61c23250c8e5d3b558aff00eab2ba18b6  -\nBut if you deterministically sort the elements, they are actually the same\n$ cat a/.bosh/links.json | jq -c .[] | sort | shasum\n9867a2e904ef55702c7c44d7a441dc25c7d0d9d5  -\n$ cat b/.bosh/links.json | jq -c .[] | sort | shasum\n9867a2e904ef55702c7c44d7a441dc25c7d0d9d5  -\n```. I think the bug is that this array isn't sorted before output https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director-core/lib/bosh/director/core/templates/job_template_renderer.rb#L108\nLooks like it never has been sorted, so technically this bug may have always been present, but perhaps didn't present itself until now because of the way that map was generated changed in https://github.com/cloudfoundry/bosh/commit/39748765525aa13f084a27f0903c65671f978ff8?. 2nd attempt with correct Ruby syntax appears to work when applied to our bosh director.\nNote that the first deploy after updating the director will cause all instances to be updated, but at least after that this part will be stable, and subsequent deploys will be clean.\nIt may be possible with a more complex patch to preserve the order returned in prior version of bosh director, but I'm not going to attempt that, and it didn't seem to be well defined - rather it seems to be deterministic by accident.. Have verified a 2nd deploy to the same directory of the same manifest now correctly does not attempt to update any of the instances.. ",
    "wilfredwilly": "yes thanks a lot. Thank a lot @cppforlife @genevieve @cwlbraa \n. ",
    "apsraps": "Thanks for the response. Yes, this is a development environment. And we had run into some issues with hard stop few days ago and could not resurrect with bosh start, bosh cck etc and had to do the setup all over again. We can try this again now and see if we run into that issue again.. ",
    "genevieve": "Hey @wilfredwilly. We can also answer you over on https://github.com/cloudfoundry/bosh-bootloader\n\n\nFor AWS, there are 3 instances created with their own purpose. \n\n\nbosh/0 is your BOSH director\n\njumpbox/0 is your bastion/jumpbox that lies in front of your BOSH director that you have to proxy commands through and allows us to limit the ports and incoming traffic on the director instead of exposing it to the whole internet\n\nbbl-env-saimaa-2018-04-02t10-13z-nat is the NAT box required on AWS\n\n\nYou can reach bosh/0 with this document: https://github.com/cloudfoundry/bosh-bootloader/blob/master/docs/howto-ssh.md#to-the-bosh-director\n\n\nThere are three load balancers each for different kinds of traffic. . \n\n",
    "pivotal-mp": "We'll close this for now. Please let us know if you want to re-open it.\nBest,\n@pivotal-mp && @dpb587-pivotal . Hi @shausy,\nHave you ever successfully deployed this deployment? There's a known behavior where an instance operation (e.g. stop, start, recreate) will fail for deployments that have never succeeded. See also: https://github.com/cloudfoundry/bosh/issues/1854.\nLet us know if that helps,\n@pivotal-mp && @jfmyers9, CF BOSH. We think this commit (https://github.com/cloudfoundry/bosh/commit/9142f3583dd6fd7b8c2dd3744e715bf10686cebd) could fix the issue. But since we could never successfully reproduce the bug, we can't be sure. Could you try it out and let us know if you still see the error?\nBest,\n@pivotal-mp && @jrussett, CF BOSH. We reproduced this error using a test: specifically, if you have a link provider with an unresponsive agent, subsequent deployments that consume that link may have a template filled in with the unresponsive IP address before the unresponsive VM is recreated.  However, this is not a problem if you are using DNS, because the address will be a DNS address which will be constant (which we also verified with a test).  \nUsing create-swap-delete with IP addresses is not supported and not guaranteed to work. We recommend using DNS whenever you are using create-swap-delete.\n@pivotal-mp && @jaresty. Thanks, we've added a note to the docs to indicate that os is the preferred way of specifying the stemcell.\nRegarding your second question, if you use os and version, it'll automatically pick the correct stemcell version for each IaaS.. Sounds good, we'll close. Let us know if you experience this again!\nThanks!\n@pivotal-mp && @luan, BOSH team. Hi @forexamplelyr,\nThis sounds like a UAA related issue. Could you check with their team (https://github.com/cloudfoundry/uaa) on how to debug this?\nBest,\n@pivotal-mp && @dpb587-pivotal . ",
    "simonjjones": "Hi @madamkiwi,\nApologies for the delay in getting back to you. We can't see anything glaringly obvious that would cause this sort of behaviour, and it's not a flake we've seen during our AWS testing.\nI appreciate it's been some time, but we could continue the investigation better if we were able to see the agent logs. If you see this issue again could you please let us know and pass on the logs?\nThanks\nSimon. ",
    "pc-jedi": "Hey Marco,\nYeah that helps, but what if both jobs depend on different versions of the package?\nAnd will there be a solution for package re-use in the future? Because I personally hate duplicating code/blobs/etc.. ",
    "mkuratczyk": "We just run into this issue as well. Our release contained golang package and everything worked fine in our test environments. However, it was failing in another env. It turned out an add-on which also contained golang package was installed in that env and on the deployed VM the package was actually coming from the add-on. It contained the same Golang version but their packaging script was different which led to a different path for go binary.\nWe've just renamed our package to avoid this conflict but this behaviour leads to some very hard to diagnose issues so it would be great to fix that.. Given that it's a breaking change to support multiple packages, could we instead get the collision error in all cases? As I mentioned above - we had two Golang packages with the same Go version and the same package name (one from our tile, one from an add-on we didn't know would be collocated) and our deployment got broken because the folder structure inside /var/vcap/packages/golang was different than expected (the other package \"won\" and its packaging script was different). I think BOSH should prevent situations like that when an addon can break the deployment.. @dpb587 we've solved this problem by renaming our golang package for now and we are thinking about switching to golang-release in the future but just to explain - we use Golang and ginkgo to run smoke tests (RMQ tile). I'm sure we could do it differently but for the time being that's how we execute smoke tests errand and therefore we need Golang as a job dependency, not for compilation.. ",
    "kreamyx": "@cdutra Thanks. Appreciate your feedback and working on it. I will also squash the commits and tidy up the commit messages.\nYes, we already had the meeting with @cppforlife and we agreed to include netmask_bits in the subnet definition. It is already included in the PR\nThanks. @jrussett , @cdutra , and @jaresty Thank you very much for your feedback and your comments. The PR was updated to address all your comments. I also squashed the commits into one.. Hi @GarfieldIsAPhilosopher and @mikexuu \nThanks for your question.\nFor a network to be managed, two things have to happen:\n\nThe global enable_cpi_management has to be enabled (it is disabled by default)\nThe network spec in the cloud config has to have the managed property set to true (it is false by default)\n\nSo enabling the global enable_cpi_management in itself won't do anything as no networks will be managed after enabling the feature unless you explicitly specify that you want a network to be managed in the cloud config.\nAnother thing, the networks table in the database only stores managed networks so there is no need to store the managed property in the table.\nLet me know if you have further questions\n. @jrussett I updated my PR to address your comments\n@jfmyers9 , @belinda-liu With the exception of the 5-day default value for cleaning up orphaned networks, all of your comments have been addressed in an updated PR. One major change was to introduce an orphaned query parameter to the Get method of the /networks endpoint. This change is also mirrored in https://github.com/cloudfoundry/bosh-cli/pull/473\nThe reason for having a 5-day default value was to be consistent with the default value for cleaning up orphaned disks.\nPlease let me know if you want to reduce the 5-day default value to one day and if any further actions are required. Thanks!. Oh this is a case of updating the network which we have decided to address later (we had discussed that with the rest of the team and @cppforlife).. Sounds good! Thanks. The doc is outdated. The CPI response is:\n[id, {}, cloud_props]\nIt was changed after the discussions we had with @cppforlife to accommodate for the netmask_bits scenario. Agreed.. @jrussett Can you please give me a scenario where the subnet name can change? Are you referring to a cloud config update that would update the subnet name? If yes, then all cases involving subnet changes, adding subnets, or deleting subnets are not handled by this PR as these actions are related to network update. We don't have an agreement yet on how to handle updates but we believe that we might need an additional CPI method (update_network for example). OK. Sounds good!. @GarfieldIsAPhilosopher For this PR, no components are calling delete network. Ina future PR, it will be called. Good point about locking!. @jfmyers9 No, there is no specific reason for the 5 days default value. We were just trying to be consistent with the default value for orphaned disks which is 5 days. Should I change this to 1 day instead of 5?. Sounds good! I will remove it. My bad. I will change this to orphaned_networks in an updated PR. This is a very good point. I will have to make the changes here and on the CLI side as well.. @jrussett Thanks. I have modified the unit test to mock out the OrphanNetworkManager instead. @jfmyers9 I added the validation. Currently, the CPI doesn't give any hints regarding why a failure could have occurred. I was initially thinking of removing the subnet from the IaaS as a best effort attempt. Meaning that Bosh will attempt to remove the subnet, if it fails for any reason, Bosh would remove this subnet from the database and stop managing it and then it will be left to the cloud operator to clean things up. But you are right, another approach could be to leave it in the database. This way, Bosh will attempt to remove it again the next time the deployment is updated. I have no strong opinions either way.. Commented below.. One problem I see in the second approach is:\n1- deployment created with a managed network with two subnets\n2- cloud config updated, and the second subnet is removed from the config\n3- IaaS returns an error \"can not delete subnet\" and the deployment update proceeds\n4- The cloud operator manually cleans up the no longer used subnet\n5- any further update of the deployment will attempt to remove the subnet from the iaas (which no longer exists) but will fail\nIn this scenario, the subnet will never be removed from the database.\nI have another PR ready to be submitted for this approach if this is the desired behavior.. ",
    "vijayb47": "Happened to what version of bosh/cpi? I am still getting same error on a shared VPC/Subnet with bosh 268.5.0/ CPI 29.. ",
    "nvprakashreddy": "Not remembering the bosh version, i remember the issue was created due to incorrect configurations in bosh server.. ",
    "surazzarus": "@dpb587-pivotal thank you for the reply. I would like to get the deployments of bosh using the Director REST API. bosh deployments give me the deployments after i am logged in. But how about using curl.. What are the additional headers required?\nI tried:\ncurl -s -k https://admin:password@bosh-director-ip:25555/deployments\nI get same not authorized. I did check the uaa/UAA/Token and still couldn't figure it out about getting the access token. I am still trying though.\nI am making a service that calls the Director HTTP API to get the list of my deployments, so that I don't have to login into the bosh-cli to get the details about my deployments. . ",
    "Gerg": "@oozie. @oozie. ",
    "belinda-liu": "Hey Amit,\nWe tried to reproduce the scenario but didn't see the same failure. Below is the output, you can see that after redeploying all three instances are running successfully. We experimented with various configurations with different azs, ignoring different instances, and updating the reserved IPs in the cloud config and still weren't able to reproduce the issue. \n```\njm+bl $ bosh -d zookeeper instances --details\nUsing environment '192.168.50.6' as client 'admin'\nTask 12. Done\nDeployment 'zookeeper'\nInstance                                        Process State  AZ  IPs         State    VM CID                                VM Type  Disk CIDs                             Agent ID                              Index  Resurrection  Bootstrap  Ignore\n                                                                                                                                                                                                                          Paused\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  running        z1  10.244.0.2  started  9805f023-63f6-4cfa-7691-62209f292238  default  fd3abd2d-04cc-467a-4a20-e99bdba0016c  244eb05e-ac27-48e8-ba3c-d04a8b324b63  0      false         true       false\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  running        z3  10.244.0.4  started  e92d9289-2afd-4500-56ef-0584cd5f7f08  default  ffcc3e28-7bcb-47fa-7f04-b42db6dabe39  13e1034b-0e06-49d4-9bfc-4234dbe767f3  2      false         false      false\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running        z2  10.244.0.3  started  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  6ad2c63d-fd54-42d1-6ec4-5a06ba5dcf37  85114aa4-7429-4972-8316-06815a3e0d0d  1      false         false      false\n3 instances\nSucceeded\n2018-11-20 13:47 ~/deployments/vbox\njm+bl $ bosh vms\nUsing environment '192.168.50.6' as client 'admin'\nTask 13. Done\nDeployment 'zookeeper'\nInstance                                        Process State       AZ  IPs         VM CID                                VM Type  Active\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  unresponsive agent  z1  10.244.0.2  9805f023-63f6-4cfa-7691-62209f292238  default  true\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  unresponsive agent  z3  10.244.0.4  e92d9289-2afd-4500-56ef-0584cd5f7f08  default  true\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running             z2  10.244.0.3  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  true\n3 vms\nSucceeded\n2018-11-20 13:48 ~/deployments/vbox\njm+bl $ bosh -d zookeeper ignore zookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'zookeeper'\nSucceeded\n2018-11-20 13:48 ~/deployments/vbox\njm+bl $ bosh -d zookeeper deploy zookeeper.yml --fix\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'zookeeper'\nRelease 'zookeeper/0.0.9' already exists.\nContinue? [yN]: y\nTask 14\nTask 14 | 21:48:50 | Preparing deployment: Preparing deployment (00:02:15)\nTask 14 | 21:51:05 | Warning: You have ignored instances. They will not be changed.\nTask 14 | 21:51:05 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 14 | 21:51:15 | Updating instance zookeeper: zookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac (0) (canary) (00:00:28)\nTask 14 | 21:51:43 | Updating instance zookeeper: zookeeper/59debd80-5367-418d-bf69-e1ecbad7363a (2) (00:00:17)\nTask 14 Started  Tue Nov 20 21:48:50 UTC 2018\nTask 14 Finished Tue Nov 20 21:52:00 UTC 2018\nTask 14 Duration 00:03:10\nTask 14 done\nSucceeded\n2018-11-20 13:52 ~/deployments/vbox\njm+bl $ bosh vms\nUsing environment '192.168.50.6' as client 'admin'\nTask 15. Done\nDeployment 'zookeeper'\nInstance                                        Process State  AZ  IPs         VM CID                                VM Type  Active\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  running        z1  10.244.0.2  b2733259-660f-4a7f-67ff-ea38b1cb7bed  default  true\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  running        z3  10.244.0.4  ac27805c-5a29-442f-58f7-2cc33be1d7f1  default  true\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running        z2  10.244.0.3  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  true\n3 vms\nSucceeded\n```\nWe've fixed a couple of issues around ignored instances since this issue was opened, so the behavior in this situation may have changed. We're going to close this issue, feel free to reopen if it happens again.\nThanks,\n@belinda-liu && @jfmyers9, CF BOSH . Hey Amit,\nWe tried to reproduce the scenario but didn't see the same failure. Below is the output, you can see that after redeploying all three instances are running successfully. We experimented with various configurations with different azs, ignoring different instances, and updating the reserved IPs in the cloud config and still weren't able to reproduce the issue. \n```\njm+bl $ bosh -d zookeeper instances --details\nUsing environment '192.168.50.6' as client 'admin'\nTask 12. Done\nDeployment 'zookeeper'\nInstance                                        Process State  AZ  IPs         State    VM CID                                VM Type  Disk CIDs                             Agent ID                              Index  Resurrection  Bootstrap  Ignore\n                                                                                                                                                                                                                          Paused\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  running        z1  10.244.0.2  started  9805f023-63f6-4cfa-7691-62209f292238  default  fd3abd2d-04cc-467a-4a20-e99bdba0016c  244eb05e-ac27-48e8-ba3c-d04a8b324b63  0      false         true       false\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  running        z3  10.244.0.4  started  e92d9289-2afd-4500-56ef-0584cd5f7f08  default  ffcc3e28-7bcb-47fa-7f04-b42db6dabe39  13e1034b-0e06-49d4-9bfc-4234dbe767f3  2      false         false      false\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running        z2  10.244.0.3  started  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  6ad2c63d-fd54-42d1-6ec4-5a06ba5dcf37  85114aa4-7429-4972-8316-06815a3e0d0d  1      false         false      false\n3 instances\nSucceeded\n2018-11-20 13:47 ~/deployments/vbox\njm+bl $ bosh vms\nUsing environment '192.168.50.6' as client 'admin'\nTask 13. Done\nDeployment 'zookeeper'\nInstance                                        Process State       AZ  IPs         VM CID                                VM Type  Active\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  unresponsive agent  z1  10.244.0.2  9805f023-63f6-4cfa-7691-62209f292238  default  true\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  unresponsive agent  z3  10.244.0.4  e92d9289-2afd-4500-56ef-0584cd5f7f08  default  true\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running             z2  10.244.0.3  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  true\n3 vms\nSucceeded\n2018-11-20 13:48 ~/deployments/vbox\njm+bl $ bosh -d zookeeper ignore zookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'zookeeper'\nSucceeded\n2018-11-20 13:48 ~/deployments/vbox\njm+bl $ bosh -d zookeeper deploy zookeeper.yml --fix\nUsing environment '192.168.50.6' as client 'admin'\nUsing deployment 'zookeeper'\nRelease 'zookeeper/0.0.9' already exists.\nContinue? [yN]: y\nTask 14\nTask 14 | 21:48:50 | Preparing deployment: Preparing deployment (00:02:15)\nTask 14 | 21:51:05 | Warning: You have ignored instances. They will not be changed.\nTask 14 | 21:51:05 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 14 | 21:51:15 | Updating instance zookeeper: zookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac (0) (canary) (00:00:28)\nTask 14 | 21:51:43 | Updating instance zookeeper: zookeeper/59debd80-5367-418d-bf69-e1ecbad7363a (2) (00:00:17)\nTask 14 Started  Tue Nov 20 21:48:50 UTC 2018\nTask 14 Finished Tue Nov 20 21:52:00 UTC 2018\nTask 14 Duration 00:03:10\nTask 14 done\nSucceeded\n2018-11-20 13:52 ~/deployments/vbox\njm+bl $ bosh vms\nUsing environment '192.168.50.6' as client 'admin'\nTask 15. Done\nDeployment 'zookeeper'\nInstance                                        Process State  AZ  IPs         VM CID                                VM Type  Active\nzookeeper/438fd82a-7241-4df7-b4ea-6fb62891f9ac  running        z1  10.244.0.2  b2733259-660f-4a7f-67ff-ea38b1cb7bed  default  true\nzookeeper/59debd80-5367-418d-bf69-e1ecbad7363a  running        z3  10.244.0.4  ac27805c-5a29-442f-58f7-2cc33be1d7f1  default  true\nzookeeper/c2dac497-de71-4f29-b8eb-68b7f1970a16  running        z2  10.244.0.3  df7fbaa1-5dec-4bc0-56dc-e6119e7ef434  default  true\n3 vms\nSucceeded\n```\nWe've fixed a couple of issues around ignored instances since this issue was opened, so the behavior in this situation may have changed. We're going to close this issue, feel free to reopen if it happens again.\nThanks,\n@belinda-liu && @jfmyers9, CF BOSH . Hi @ndhanushkodi,\nThis theoretically can happen when multiple useradd commands run in parallel. We tried to reproduce this in various ways but couldn't get an error. Are you still seeing this happen?\nThanks,\n@belinda-liu && @jfmyers9, CF BOSH. Hi @ndhanushkodi,\nThis theoretically can happen when multiple useradd commands run in parallel. We tried to reproduce this in various ways but couldn't get an error. Are you still seeing this happen?\nThanks,\n@belinda-liu && @jfmyers9, CF BOSH. Hi @sjolicoeur,\nDid you have any further questions? Let us know, otherwise we will close this issue due to inactivity.\nThanks,\n@belinda-liu && @luan, CF BOSH. Hi @sjolicoeur,\nDid you have any further questions? Let us know, otherwise we will close this issue due to inactivity.\nThanks,\n@belinda-liu && @luan, CF BOSH. Hi @sjolicoeur,\nWe're closing this issue due to inactivity. Feel free to reopen if this problem comes up again.\nThanks,\n@belinda-liu && @langered, CF BOSH. Hi @sjolicoeur,\nWe're closing this issue due to inactivity. Feel free to reopen if this problem comes up again.\nThanks,\n@belinda-liu && @langered, CF BOSH. Hi @gossion,\nWe're going to close this issue for now to clear up our open issues. Please reopen if and when the error happens again, and we can take another look.\nThanks,\n@belinda-liu && @mikexuu, CF BOSH. Hi @gossion,\nWe're going to close this issue for now to clear up our open issues. Please reopen if and when the error happens again, and we can take another look.\nThanks,\n@belinda-liu && @mikexuu, CF BOSH. Hi @shausy,\nCan you show us what the output of bosh manifest -d {deployment-name}? We want to confirm it's not the known issue mentioned above.\nAlso, you can find the create deployment task by running bosh tasks --recent=50 -d {deployment-name}. The create deployment task should be listed in the output (and you can toggle the number in the recent flag to see older tasks). Once you have the number for the create deployment task, can you show us the output when you run bosh tasks {task number} --debug?\nThanks,\n@belinda-liu & @luan, CF BOSH . Hi @shausy,\nCan you show us what the output of bosh manifest -d {deployment-name}? We want to confirm it's not the known issue mentioned above.\nAlso, you can find the create deployment task by running bosh tasks --recent=50 -d {deployment-name}. The create deployment task should be listed in the output (and you can toggle the number in the recent flag to see older tasks). Once you have the number for the create deployment task, can you show us the output when you run bosh tasks {task number} --debug?\nThanks,\n@belinda-liu & @luan, CF BOSH . Hi @shausy,\nIs this still an issue? If not, we are going to close this due to inactivity.\nThanks,\n@belinda-liu && @langered, CF BOSH. Hi @shausy,\nIs this still an issue? If not, we are going to close this due to inactivity.\nThanks,\n@belinda-liu && @langered, CF BOSH. Hi @shausy,\nWe are closing this issue for now. Feel free to reopen if necessary.\nThanks,\n@belinda-liu && @s4heid, CF BOSH. Hi @shausy,\nWe are closing this issue for now. Feel free to reopen if necessary.\nThanks,\n@belinda-liu && @s4heid, CF BOSH. Hey @carlhejiayu, \nDid you have any other questions? If not, we are going to close this issue. \nThanks,\n@belinda-liu && @pivotal-mp, CF BOSH team. Hey @carlhejiayu, \nDid you have any other questions? If not, we are going to close this issue. \nThanks,\n@belinda-liu && @pivotal-mp, CF BOSH team. Looks much better to Max and me! We are big fans. Dragging it into the prioritization section of the backlog to give other people a chance to look at it and a pair to properly run integration tests on it.. Looks much better to Max and me! We are big fans. Dragging it into the prioritization section of the backlog to give other people a chance to look at it and a pair to properly run integration tests on it.. Hi @iainsproat,\nIs this still an issue? Otherwise, we are going to close this due to inactivity.\nThanks!\n@belinda-liu && @pivotal-mp . Hi @iainsproat,\nIs this still an issue? Otherwise, we are going to close this due to inactivity.\nThanks!\n@belinda-liu && @pivotal-mp . Hi @forexamplelyr,\nWe're not too sure what you're experiencing. If you're looking to debug the deploy from your latest message, it might be helpful to run bosh task 191 --debug. In general, while CF Deployment is quite large, it does not need 100+ G to deploy. Your original output suggested more of a networking issue than a size issue.\nRegardless, it sounds like your questions might be better answered by CF Deployment (https://github.com/cloudfoundry/cf-deployment). Hope that helps.\nThanks,\n@belinda-liu && @pivotal-mp, CF BOSH team. Hi @forexamplelyr,\nWe're not too sure what you're experiencing. If you're looking to debug the deploy from your latest message, it might be helpful to run bosh task 191 --debug. In general, while CF Deployment is quite large, it does not need 100+ G to deploy. Your original output suggested more of a networking issue than a size issue.\nRegardless, it sounds like your questions might be better answered by CF Deployment (https://github.com/cloudfoundry/cf-deployment). Hope that helps.\nThanks,\n@belinda-liu && @pivotal-mp, CF BOSH team. Hey @alext,\nWas there anything else you wanted to discuss? It sounds like you were going to do some investigating on your end, not sure how much else we can help. \nIf there's nothing else, we'd like to close this issue.\nThanks,\n@belinda-liu && @xtreme-behrouz-soroushian. Hey @alext,\nWas there anything else you wanted to discuss? It sounds like you were going to do some investigating on your end, not sure how much else we can help. \nIf there's nothing else, we'd like to close this issue.\nThanks,\n@belinda-liu && @xtreme-behrouz-soroushian. Hi @langered,\nDid you get a chance to try out that fix? If there's no other problems, we'd like to close this issue.\nThanks,\n@belinda-liu  && @xtreme-behrouz-soroushian, CF BOSH Team. Hi @langered,\nDid you get a chance to try out that fix? If there's no other problems, we'd like to close this issue.\nThanks,\n@belinda-liu  && @xtreme-behrouz-soroushian, CF BOSH Team. Hi @tvs,\nDid you try using a newer director? If there's no other problems, we'd like to close this issue.\nThanks,\n@belinda-liu && @xtreme-behrouz-soroushian, CF BOSH team. Hi @tvs,\nDid you try using a newer director? If there's no other problems, we'd like to close this issue.\nThanks,\n@belinda-liu && @xtreme-behrouz-soroushian, CF BOSH team. Hi @tvs,\nWe're going to close this issue for now because of inactivity. Feel free to reopen this if you continue to have this issue.\nThanks!. Hi @tvs,\nWe're going to close this issue for now because of inactivity. Feel free to reopen this if you continue to have this issue.\nThanks!. Hi @surazzarus,\nAre you still experiencing this problem? If not, we'd like to close this issue.\nThanks!. Hi @surazzarus,\nAre you still experiencing this problem? If not, we'd like to close this issue.\nThanks!. ",
    "scottgai": "got same problem with \"bosh delete-vm\" as well. But the VM CID in my case looks like \"agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d;resource_group_name:tmca-cloudfoundry\". \nIf VM CID is not enclosed with quote, the command output is like\n```\nubuntu@lab13:~$ bosh -e lab13 -d cf-b8a0cd3a767cd0eeca66 delete-vm agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d;resource_group_name:tmca-cloudfoundry\nUsing environment '10.193.78.11' as user 'director' (bosh..read, openid, bosh..admin, bosh.read, bosh.admin)\nUsing deployment 'cf-b8a0cd3a767cd0eeca66'\nContinue? [yN]: y\nTask 1063\nTask 1063 | 10:36:51 | Delete VM: agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d\nTask 1063 | 10:36:51 | Delete VM: VM agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d is successfully deleted (00:00:00)\nTask 1063 Started  Fri Aug  3 10:36:51 UTC 2018\nTask 1063 Finished Fri Aug  3 10:36:51 UTC 2018\nTask 1063 Duration 00:00:00\nTask 1063 done\nSucceeded\n-bash: resource_group_name:tmca-cloudfoundry: command not found\nubuntu@lab13:~$\n```\nIf enclosed VM CID with quote or escape semicolon with backslash (agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d\\;resource_group_name:tmca-cloudfoundry):\n```\nubuntu@lab13:~$ bosh -e lab13 -d cf-b8a0cd3a767cd0eeca66 delete-vm \"agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d;resource_group_name:tmca-cloudfoundry\"\nUsing environment '10.193.78.11' as user 'director' (bosh..read, openid, bosh..admin, bosh.read, bosh.admin)\nUsing deployment 'cf-b8a0cd3a767cd0eeca66'\nContinue? [yN]: y\nTask 1064\nTask 1064 | 10:37:20 | Delete VM: agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d\nTask 1064 | 10:37:20 | Delete VM: VM agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d is successfully deleted (00:00:00)\nTask 1064 Started  Fri Aug  3 10:37:20 UTC 2018\nTask 1064 Finished Fri Aug  3 10:37:20 UTC 2018\nTask 1064 Duration 00:00:00\nTask 1064 done\nSucceeded\n```\nThough the response message is \"Succeeded\", actually the vm was not deleted at all in both cases. task debug logs show only the part of VM CID before semicolon was used to query \"vms\" table in bosh database. \nubuntu@lab13:~$ bosh task 1064 --debug  |grep vms\nD, [2018-08-03T10:39:51.719778 #31059] [task:1065] DEBUG -- DirectorJobRunner: (0.000472s) (conn: 47269830134640) SELECT * FROM \"vms\" WHERE ((\"cid\" = 'agent_id:3dc13f1f-d695-41f4-bccc-e13399b54a7d') AND (\"active\" IS TRUE)) LIMIT 1\nExcept problem of recognizing semicolon in VM CID, should the task output also be enhanced to give some error (or warning) message instead of \"Succeeded\"?. @dpb587-pivotal \nIt's \"BOSH Director for Azure\" v2.2-build 296. \nI don't have such env on Azure so checked on \"BOSH Director for vSphere\" v2.2-build 296 and got following info. \n```\nubuntu@opsmgr-env8-v2-2:~$ bosh env\nUsing environment '172.18.0.11' as client 'ops_manager'\nName      p-bosh\nUUID      aad3cf0c-3075-4adb-9526-024d93f302fe\nVersion   266.6.0 (00000000)\nCPI       vsphere_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: enabled\n          dns: disabled\n          snapshots: disabled\nUser      ops_manager\n```\nand CPI release is as following for this OpsMgr version according to release note.\nAzure CPI | 35.2\nvSphere CPI | 50\n. Issue could be reproduced on PCF 2.2 system not 2.1. Here is bosh env info from one 2.2 system.\n```\n$ bosh env\nUsing environment '10.0.8.10' as client 'ops_manager'\nName      p-bosh\nUUID      3155ac05-43a1-4027-a37b-658a96ebea98\nVersion   266.9.0 (00000000)\nCPI       azure_cpi\nFeatures  compiled_package_cache: disabled\n          config_server: enabled\n          dns: disabled\n          snapshots: disabled\nUser      ops_manager\n```. ",
    "alex-slynko": "@mikomraz In CFCR we use 169.254.169.254 or  as DNS in cloud config. It resolves metadata.google.internal correctly.. ",
    "mikomraz": "@pivotal-jamil-shamy I haven't got the chance to check on ubuntu-trusty.\n@alex-slynko I understand. But in this case I haven't used CFCR, but installed kubeadm separately.\nSo it may be worth to update the default cloud-config in bosh-deployment, or better yet, check why the daemons haven't done their job.. Yes. I confirmed the issue on ubuntu-trusty too.. Hello @freddesbiens,\nThe use case is that the package kubeadm looks for metadata.google.internal and fails to resolve it, consecutively failing to start a cluster.\nI am under the impression that the record 169.254.169.254 metadata.google.internal # Added by Google is standard on GCP compute machines, meant to be consumed by everybody - and not BOSH agent specific. Note that on standard provisioned GCP compute the record always exists, and also that on BOSH stemcells after rebooting (allowing kubeadm to succeed in starting a cluster).\nTherefore I think it's a bug that the record is missing in the first place.. ",
    "aegershman": "The more I think about it, the more I think using events might not be the best approach to what I actually need to get done.\nWhat I need to do is pull the total number of VMs which were created / destroyed over the history of a deployment (across all deployments). Every single one. I know this sounds a bit silly, as the total number of VMs churned over doesn't really matter that much. But I could really use the data to impress some of the more, ah, \"old school\" members of the corporation I work for. I fully understand that in BOSH land, \"total VMs churned\" is not particularly impressive, but considering there are people I have to debate with who are doing manual VM provisioning/upgrading, to them this seems like an impressive metric & it would be nice to be able to shove it in their face.\ntl;dr, this is for demonstrating the value. We do use both Prometheus and PCF healthwatch in our critical systems, and so will presumably be ready to ingest that information if the metrics are accessible/reported through health monitor. \nBut to your question about specific CLI use case-- it would be the same rationale for why any other VM's vitals in a bosh deployment are accessible via the --vitals flag: \n\nsome (hopefully non-critical) environments aren't hooked up to third-party monitoring systems\nit's more convenient when triaging/debugging is necessary (e.g., especially if third-party support needs a convenient way to request the customer to share the status of their foundation using CLI commands)\nif the information is present (and doesn't incur needless code complex to implement), why not make it accessible via the CLI as it is for other VMs?\n\nThanks for your time. ",
    "s-matyukevich": "Thanks a lot @cppforlife !\nYour suggestion to use different subnets indeed solves the main problem. There is, however, a second problem: what if for some instance group I want to consume combined link from both data centers, and for another instance group I need a link from a single data center? Using mysql as an example one more time, it is a valid scenario to have 2 different proxies in each data center, and configure them in such a way that each proxy redirects traffic only to local mysql instances. In this case, we still need 2 different mysql instance groups. \nYou can take this into account when prioritizing this issue. . ",
    "xtreme-bozhidar-lenchov": "Sorry about the delay @pivotal-jamil-shamy; Brian rolled off a team a few weeks ago and we've forgotten to follow up on his issue. Spoke with Jesse a bit and we agree, it was surprising to see the diff shown.\nWe did not provide run.pivotal.io as a variable through CLI flags. We are loading a few other variables from credhub into a vars-file, but these do not include ((system_domain)). \nWe do a deploy dry-run with the no-redact flag to get a deployment diff (to verify manually and put in our communication), before kicking off the deployment itself. The deployment gets kicked off by a bosh-deployment-resource in CI, and we don't seem to provide any properties other than target, client, client_secret, ca_cert, deployment and manifest.\nMentioning dry-run in case it's the damp run rearing its head again.. ",
    "antonioaguilar": "@dpb587-pivotal I'm using bosh-lite to do a cf-deployment and unfortunately the $HOME environment variable only works for the .bosh folder but not for the .bosh_virtualbox_cpi. The .bosh_virtualbox_cpi is the folder that actually grows larger when doing multiple installs of CF locally. Any suggestions on how to get Bosh to respect the $HOME variable for both .bosh and  .bosh_virtualbox_cpi folders?. ",
    "BeckerMax": "^ @cppforlife @dpb587-pivotal @jfmyers9 . Fixed with https://github.com/cloudfoundry/bosh/commit/eb59b8678f2cb98a87c494628acc0f9086ffafc5. @h4xnoodle Unfortunately, I can't open the referenced story. Is there another story where we can track the progress?. Hi @gaurav987654321 ,\nin general this error means that the director could not communicate with the agent. See here for troubleshooting pinged out agent.\nFirst steps could be to investigate the debug log with bosh task 5 --debug or checking for network connectivity between agent and director possibly by ssh'ing onto the deployed VM.. Did you change anything in the manifest or infrastructure that led to the first error go away and the second one to appear?\nThe error output tells the problem: Bosh wants to use an ip for deploying that is already in use by another vm.\nFirstly, you can check in horizon or the openstack cli which server uses this ip currently. Secondly, this could also be a configuration issue of the networks section in your deployment manifest. If you share the redacted deployment manifest/cloud-config this could give more hints.\nIn general, it seems that your questions are about the general usage of bosh rather than a bug report or feature request. In these cases I think using our official community slack would give you faster responses and allow a more direct communication.\nSlack: #bosh on https://slack.cloudfoundry.org. @dpb587-pivotal @charleshansen \nWe implemented a test which will check whether a new member is added to the EvaluationContext class which is not yet part of EvaluationContext.==. With this kind of test, members which are intentionally not part of EvaluationContext.== need to be listed explicitly in the test. We think the test provides additional value but are unsure if it is worth the added complexity. What is your opinion?. ",
    "xtreme-conor-nosal": "Another delete deployment failed with a more specific timeout:\n```\nTask 38359\nTask 38359 | 21:19:19 | Deleting instances: ipsec_2/c6491b21-9697-400b-855a-d19cb7135c97 (0) (00:02:19)\n                     L Error: Timed out sending 'run_script' to 62a1e465-b0fb-446c-9623-f7a2782e25e8 after 45 seconds\nTask 38359 | 21:25:32 | Error: Timed out sending 'run_script' to 62a1e465-b0fb-446c-9623-f7a2782e25e8 after 45 seconds\nTask 38359 Started  Tue Jul  3 21:17:00 UTC 2018\nTask 38359 Finished Tue Jul  3 21:25:32 UTC 2018\nTask 38359 Duration 00:08:32\nTask 38359 error\nDeleting deployment 'ipsec-optional_proposals-windows-2012R2':\n  Expected task '38359' to succeed but state is 'error'\nExit code 1\n```\nIn this case the task status was error instead of timeout. Does that imply this timeout came from the director whereas the previous one came from the CLI?. Yes, we needed to scale up our director - concourse pipelines were kicking off many deployments in parallel.\nCould the first error message be explicit (e.g. timed out waiting for response from director or director timed out waiting for worker)?. @pivotal-mp @belinda-liu\nI wanted to resurface this. The docs were updated to hint that 'os' is preferred, but there seems to be a larger issue that 'name' and 'os' can't be mixed.\nIf I have a VM deployed on stemcell bosh-google-kvm-ubuntu-xenial-go_agent/250.4 I would expect an addon specifying:\n- include:\n    stemcell:\n    - os: ubuntu-xenial\nto apply to my VM, regardless of whether my manifest specified name or OS.. ",
    "henrytk": "@voelzmo ah yes, good spot. Closing.. ",
    "ashwin-venkatesh": "More examples from access.log:\n10.0.0.5 - - [20/Jul/2018:20:46:40 +0000] \"GET /tasks/15/output?type=event HTTP/1.1\" 206 502 \"-\" \"Go-http-client/1.1\" 0.006 0.006 .\n10.0.0.5 - - [21/Jul/2018:16:37:42 +0000] \"GET /tasks/21 HTTP/1.1\" 502 166 \"-\" \"Go-http-client/1.1\" 81.716 81.716 .\n10.0.0.5 - - [22/Jul/2018:14:49:46 +0000] \"GET /tasks/25/output?type=event HTTP/1.1\" 502 166 \"-\" \"Go-http-client/1.1\" 188.190 188.190 .\n10.0.0.5 - - [22/Jul/2018:21:31:20 +0000] \"GET /tasks/26/output?type=event HTTP/1.1\" 502 166 \"-\" \"Go-http-client/1.1\" 438.182 438.182 .\n10.0.0.5 - - [25/Jul/2018:08:29:24 +0000] \"GET /tasks/37/output?type=event HTTP/1.1\" 502 166 \"-\" \"Go-http-client/1.1\" 74.330 74.330 .\n10.0.0.5 - - [25/Jul/2018:08:34:55 +0000] \"POST /stemcells?fix=true HTTP/1.1\" 502 166 \"-\" \"Go-http-client/1.1\" 266.577 107.849 .. ",
    "xtreme-jason-smith": "Hey,\nSo we spent some more time trying to get to the bottom of this, and I think this might be a problem with resource sharing of sorts. When we try to upload our ~120GB heavy azure stemcell, tar -d and gzip start running in the director VM which are CPU intensive and can ensure other processes get throttled for the CPU resource. This led to the puma webserver seemingly dropping request. Bumping our director VM from a 2 core to an 8 core seemed to rid us of this problem. \nMaybe ensuring the puma process has a high priority might have helped. This might be a problem unique to our team considering the size of the stemcells we have to work with on Azure, but we figured this would be helpful to know in general and it might be something others observe if they run CPU intensive tasks on the director VM.\nFeel free to close this issue if that does not seem like as much of a concern.\n/cc @ashwin-venkatesh @jfmyers9 @dpb587 @mfine30 \nCheers!!. Hey,\nSo we spent some more time trying to get to the bottom of this, and I think this might be a problem with resource sharing of sorts. When we try to upload our ~120GB heavy azure stemcell, tar -d and gzip start running in the director VM which are CPU intensive and can ensure other processes get throttled for the CPU resource. This led to the puma webserver seemingly dropping request. Bumping our director VM from a 2 core to an 8 core seemed to rid us of this problem. \nMaybe ensuring the puma process has a high priority might have helped. This might be a problem unique to our team considering the size of the stemcells we have to work with on Azure, but we figured this would be helpful to know in general and it might be something others observe if they run CPU intensive tasks on the director VM.\nFeel free to close this issue if that does not seem like as much of a concern.\n/cc @ashwin-venkatesh @jfmyers9 @dpb587 @mfine30 \nCheers!!. ",
    "benjaminguttmann-avtq": "Just as an additional information, since v2.1.0 credhub provides the possibility to request certificates that will expire within a certain amount of days (https://github.com/pivotal-cf/credhub-release/releases/tag/2.1.0). ",
    "techie20122018": "The bosh director is already deployed . uploading the stemcell is creating below exception,\nError: Unknown CPI error 'Unknown' with message 'getaddrinfo: Name or service not known (hostname:443)' in 'create_stemcell' CPI method\nTask 13 | 18:05:15 | Error: Unknown CPI error 'Unknown' with message 'getaddrinfo: Name or service not known (hostname:443)' in 'create_stemcell' CPI method. yes. this is in vsphere . I just replaced the actual hostname.. Not sure , where you want to update the dns entries. \ndo you want me to update the below entry in the file dns.yml , something like this\n\ntype: replace\n  path: /networks/name=default/subnets/0/dns\n  value: (10.1.2.3,10.5.1.2). I change the dns entry in cloud config.yml and update it .still no success.. yes . I got that . is there any way to change those parameters in already created bosh director.. why i am getting the cluster ip as 10.100.200.1  , even I am getting the same\n\n$ kubectl get all\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.100.200.1           443/TCP   17m. ",
    "aleliaert": "@dpb587-pivotal - Your tip certainly helped us put this issue out to pasture.  We no longer have a beef with our vcap account.  Farewell cow.  Thanks!. @dpb587-pivotal - Your tip certainly helped us put this issue out to pasture.  We no longer have a beef with our vcap account.  Farewell cow.  Thanks!. ",
    "JunoJunho": "No, not this error\nWhen the .pem file does not fit to format. I faced the issue that if the .pem file did not have line feed properly, ssh connection is not successfully established.. ",
    "pulpham": "Hi @voelzmo \nsrc.tgz the stemcell I donwloaded.\nAnd target image_id is id of image I have created above.\nYes, all modifications only apply to that image itself.. ",
    "jhvhs": "At Cloudops EU, we're also struggling with the blobstore being included in director backups. We are running continuous upgrades, and very quickly, a PCF foundation with CF/redis/mysql and a few other tiles results in a 130Gb encrypted tarball, which takes up to 10 hours to produce and upload. And it takes longer just to download, extract and decrypt the backup, let alone actually use it, than it takes to re-create this whole foundation from scratch, given that we have separate backups for cf and opsman.\nWe were wondering about the reasoning behind including the blobstore. As we understand it, it's just an ephemeral cache that does not belong to the backup altogether. Is there any particular reason for including it?. @dpb587 @voelzmo \nWhat we see in a long-running environment is that the blobstore gets really bloated\nwhen running updates in a timely fashion. For example, when inspecting the diego \nrelease together with engineers from BBR (@aclevername, @terminatingcode),\nafter performing a bosh clean-up --all on our long-running environment we see\nthe following output:\n```\n$ bosh stemcells\nName                                    Version   OS\nbosh-google-kvm-ubuntu-trusty-go_agent  3541.52  ubuntu-trusty\n~                                       3541.4   ubuntu-trusty\nbosh-google-kvm-ubuntu-xenial-go_agent  170.30   ubuntu-xenial\n~                                       97.57    ubuntu-xenial\n~                                       97.19*    ubuntu-xenial\n```\n```\n$ bosh inspect-release diego/2.22.1\nJob\nauctioneer/f20aa450b4f682b3af1c636e3403aa02926c9139b3c2ee256e3eadf162efe64f\n... more jobs, nothing special ...\n12 jobs\nPackage                                                                                   Compiled for\n?>> auctioneer/e935376138903524c5df577fafd1a61f6d5591b4c3e88955ac7f4c6069c0d421           ubuntu-xenial/170.15 <<?\n~                                                                                         ubuntu-xenial/170.25\n~                                                                                         (source)\n... some more packages along the same lines ...\ncertsplitter/fe1cfa89d614a8438e4cbcff5a7b5d56d333e19278a992a433c596ddad9b737d             (source)\n~                                                                                         ubuntu-xenial/170.15\n~                                                                                         ubuntu-xenial/170.25\n?>> ~                                                                                     ubuntu-xenial/97.53 <<?\n... more package with at least two 170 blobs and an optional 97 blob ...\n```\nWe inspected the compiled_releases table on the bosh director, and have got the following report:\nbosh => select count(*), stemcell_version from compiled_packages group by stemcell_version order by stemcell_version;\n count | stemcell_version\n-------+------------------\n   285 | 170.15\n     4 | 170.19\n     4 | 170.21\n    88 | 170.24\n   282 | 170.25\n    39 | 170.30\n     5 | 3312.20\n     6 | 3363.20\n     4 | 3363.24\n     7 | 3363.25\n     6 | 3363.26\n     4 | 3363.27\n     8 | 3363.29\n     7 | 3363.30\n     8 | 3363.31\n    13 | 3363.37\n    10 | 3363.42\n    10 | 3363.44\n     1 | 3363.53\n     1 | 3363.61\n     9 | 3421.18\n    14 | 3421.19\n    14 | 3421.20\n     4 | 3421.24\n    15 | 3421.26\n    15 | 3421.32\n    13 | 3421.34\n    13 | 3421.35\n    11 | 3421.37\n    11 | 3421.38\n     8 | 3421.44\n     8 | 3421.46\n    14 | 3421.9\n    11 | 3445.11\n    14 | 3445.16\n     8 | 3445.17\n    15 | 3445.19\n    11 | 3445.21\n    12 | 3445.22\n    12 | 3445.23\n    11 | 3445.24\n     5 | 3445.30\n     3 | 3445.32\n     8 | 3445.42\n     5 | 3445.48\n     6 | 3445.51\n     2 | 3468.13\n     2 | 3468.16\n    14 | 3468.21\n     2 | 3468.22\n    42 | 3468.25\n    12 | 3468.27\n    31 | 3468.28\n    15 | 3468.30\n    27 | 3468.42\n    14 | 3468.46\n    15 | 3468.51\n    20 | 3468.54\n    29 | 3468.64\n    11 | 3468.67\n    38 | 3468.69\n    11 | 3468.71\n    49 | 3468.73\n    11 | 3468.78\n    40 | 3541.12\n    37 | 3541.25\n    47 | 3541.30\n    78 | 3541.34\n    31 | 3541.36\n    29 | 3541.37\n     9 | 3541.4\n    14 | 3541.44\n    14 | 3541.46\n    14 | 3541.48\n    14 | 3541.49\n    16 | 3541.52\n    10 | 3541.57\n    31 | 3541.59\n    12 | 3541.60\n    10 | 3541.61\n     1 | 3586.16\n     1 | 3586.24\n     2 | 3586.25\n    73 | 3586.26\n    75 | 3586.27\n    23 | 3586.36\n    23 | 3586.40\n    79 | 3586.42\n    95 | 3586.43\n    96 | 3586.46\n    96 | 3586.52\n    24 | 3586.54\n    24 | 3586.56\n    91 | 3586.57\n    17 | 3586.60\n     2 | 3586.65\n     2 | 3586.66\n    11 | 97\n     3 | 97.16\n    14 | 97.17\n    57 | 97.19\n    59 | 97.28\n    12 | 97.31\n    74 | 97.32\n    61 | 97.34\n   133 | 97.39\n    22 | 97.41\n    22 | 97.42\n    44 | 97.43\n    38 | 97.47\n    38 | 97.49\n   187 | 97.53\n    15 | 97.57\n(113 rows)\nWe can't help, but notice that the number of compiled packages for 170.15 is almost equal to\nthe number of compiled packages from 170.25.\nIncidentally, seeing the 3363 stemcell stuff lying around, we decided to see if it actually is on the disk\nbosh => select id, blobstore_id from compiled_packages where stemcell_version='3363.42' limit 3;\n  id  |             blobstore_id\n------+--------------------------------------\n 3276 | 7d316dc8-4f5a-42e9-750c-d10c9d803d24\n 3281 | 89c78c9d-545a-4e4f-747c-acd6d6ae0cd8\n 3312 | e6e83827-ddfc-4754-76c6-5a04fa10a448\n(3 rows)\nThen we have indeed located the physical files on the director VM:\nbosh/0:/var/vcap/store/blobstore/store# tree -D | grep 89c78c9d\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Dec  5  2017]  89c78c9d-545a-4e4f-747c-acd6d6ae0cd8\nOur pain point that the blob store on our director VM is bloated up to the 86Gb size with those and similar files,\nwhich prevents us from consistently and regularly backing up the director using BBR.. The above is an investigation we ran as part of a follow-up of the issues we have mentioned in https://github.com/cloudfoundry/bosh/issues/2032#issuecomment-435469995. According to https://github.com/cloudfoundry/bosh/blob/31e25224d914aa87c6e8bba2e921091356c2a5eb/src/bosh-director/lib/bosh/director/jobs/helpers/stemcell_deleter.rb#L9-L29, the stemcell deletion process only triggers the CPI cleanup, but does not take care of anything else.. ",
    "mrosecrance": "We've gone forward with introducing a job property on the blobstore job called blobstore.bbr.enabled. We've also taken your suggestion and put in a marker file in the backup artifact when it's disabled.. Could you please fill out the bug report form? There isn't enough information here for us to see if this is a reproducible issue or something to do with configuration.. Changing the agent Disk CID without updating the record in the Director means that the Director can no longer send disk changes to the agent. I wonder if getting these CID's to match is a better solution than skipping the checking altogether. Without them matching none of the behavior here is supported although we'd need to think about what data migration on an encrypted disk looks like. Currently it seems like recreates of the VM will lose track of the disk.\nIs the desired behavior here that the bosh director do nothing with the disk and it's managed completely by the operator?\nAs for comments on the code, test's would be great. Also nesting a not if in a not if is not very elegant. Could it be rewritten as if !current_packages[\"xcrypt\"] || !has_multiple_persistent_disks?(instance_plan)\n. I suppose from my perspective there are potentially a number of places you could get stemcells (bosh.io being only one of them). It wouldn't be a large change to include in the output message but so far we haven't mentioned bosh.io in any cli output as far as I know.\nDownloading a stemcell automatically would require more thought. To download it for you we would have to assume you:\n1. Have an outbound internet connection\n2. Know your IAAS that you need the stemcell for (which since multi-cpi is a thing may mean all the IAAS's).\n. ",
    "mikexuu": "Hi @sjolicoeur ,\nThanks for posting the issue. \nWe had questions about how this error occurs in the first place, what do you mean by \"use xenial pre-compiled binaries in a deploy that had both xenial and trusty deployed\"? Are you updating your windows cells that refer to Xenial pre-compiled releases?\nAlso regarding the solution that you mentioned, I don't quite follow what you mean by \"a binary install\". Please correct me if I'm wrong, but my current understanding is you were able to successfully deploy by not referencing these Xenial pre-compiled releases on your windows cells?\nThanks,\n@mikexuu & @GarfieldIsAPhilosopher \n. Hi @acrmp ,\nThanks for the issue. We've addressed the hash mismatch in this commit. Closing the issue for now, feel free to reopen if there are any other concerns.\nBest,\n@jaresty && @mikexuu , CF BOSH Team. Can you please do an earlier return rather than wrapping this method in an if block? i.e. returning if resurrection_config is nil || is emtpy? . Can you please reword this context to be more along the lines of \"when the deployment is neither included nor excluded' ?. Similarly, can you please reword this context to be more along the lines of \"when the instance group is neither included nor excluded' ?. Same as the context comments above, thanks. Likewise, same as above. Maybe a misnamed method here? Did y'all mean handle_not_detached_instance_plan ? . Is there a better name for finish_up? It looks like the method is handling a lot of different responsibilities (recreation, networking, dns, links, state, etc.) Maybe this is a sign to break up finish_up even further? The refactor in RecreateHandler.perform looks great and very easy to follow. I can see similar pattern being used in finish_up. What are your thoughts?. It looks like some of the logic in handle_detached_instance is repeated in finish_up (i.e. releasing network plans, links, dns, updating variable set, etc.). Is there a way we can only have this in one place?. ",
    "sjolicoeur": "@mikexuu Apologies I got my terms confused. I meant compiled releases we were trying to use:  experimental/use-compiled-releases-xenial-stemcell.yml; by not making use of the ops file the install was able to proceed. \nHowever the solution I would like to see is not so much a fix to allow the compiled release to work, but  a clearer error message that is more actionable. As had I not been pairing with someone that was well versed in the caveats of the platform this would have taken a long time to debug. . ",
    "jrussett": "As part of https://www.pivotaltracker.com/story/show/160070506, we did an investigation into this issue and found it difficult to find anything problematic. We're going to go ahead and close this issue for now. Please feel free to reopen this if this bug happens again and you're able to get more information from it by way of logs.\nThanks,\n@jrussett && @simonjjones, CF BOSH. I went over this with @jaresty when we were community pair and talked through some of the design decisions and agreed with them. I really like the idea of extracting smaller portions of logic into digestible functions and then making the \"main\" function essentially a large composition of helper methods, especially considering so many of these dense classes are difficult to reason about and keep a working mental model of their logic.\nI don't feel comfortable merging this PR with him because I think other people on the team should have a chance to look at it and ask questions if they have any, so I'll let someone else merge it in.. We noticed that we are raising an unhandled exception here in the case when the desired network subnet does not match the subnet name in the database. Are we confident that a subnet name will not change in between deploys? We imagine that the name could change, especially since we are recovering orphaned networks (see line 148-151).   . Should these tests read something like this instead?\ncontext 'when deploying a manifest with a managed network' do\n  it 'should fail when not configured with a subnet name' do\n. I believe the call to the constructor can read:\nnew(network_name, range, gateway, name_servers, cloud_properties, netmask, availability_zone_names, restricted_ips, static_ips, sn_name, nm_bits) \nAs the last two arguments are purely positional; there's no need for assignment.. We should probably have a unit-level test to drive out the behavior of create_network being idempotent; specifically, we should probably have a test that verifies it will re-use a pre-existing network by name.. We think it might make more sense if, in this file, instead of mocking out the CPI and testing the side effects of the DeleteOrphanNetworks, we instead mock out the OrphanNetworkManager, ensuring that it receive multiple calls to delete_network. The justification being that since the DeleteOrphanNetworks does not have a direct dependency on the CPI, it's unnecessary/not a good idea to test those side effects at this level. Some of these helper methods seem to clearly be helpful in understanding the true nature/type of an object, e.g. there is a more generic variable named @manifest_text but the function raw_manifest_text better conveys the intent of the object. Why was this one useful?. ",
    "bloeys": "Thank you @voelzmo for your reply. I am sadly away for this week and won't be able to update until next week.\nHowever, I wanted to clarify how to gain sudo access on these machines. As far as I have tried, I'm unable to view the logs of a compilation machine or if the machine didn't start correctly (as in this case). Okay so setting the password, SSH into a compilation Xenial VM, I found the following set on the ntpserver file: time1.google.com time2.google.com time3.google.com time4.google.com\nThis is the same as is set on working Trusty stemcells, and can be reached as I tested using curl, where a 302 \"The document has moved\" response is returned.\nI got the logs which you can find (removed repetitive parts) here. The main error seems to be here:\n[PartedPartitioner] 2018/09/17 11:19:30 ERROR - Failed with an error: Running command: 'parted -s /dev/vdb unit B mkpart bosh-partition-0 1048576 3872391167', stdout: '', stderr: 'parted: invalid token: bosh-partition-0\n2018-09-17_11:19:30.15345 Error: Expecting a file system type.\nIt fails when trying to create a partition. Any ideas on this one?. Sounds good then!\nThank you for the help. I'll keep the issue open for any updates and can close once its resolved.. This makes sense, thanks for the work. I will really be looking forward to the coming release as it has been keeping us from updating some things.. ",
    "shausy": "Yes the deployment is successful. Im trying to created failure scenarios for kubernetes cluster during that operation this issue been observed.. @luan Deployment is taken care internally using the PKS as part of cluster creation.\nPost cluster creation, I have tried to do this operation and it fails\n```\nCli:/home/pksadmin# bosh task 23191 --debug\nUsing environment '172.28.248.101' as client 'ops_manager'\nTask 23191\nI, [2018-09-21T12:34:10.278142 #28] [0x2b10e6dc568c]  INFO -- TaskHelper: Director Version: 266.11.0\nI, [2018-09-21T12:34:10.278191 #28] [0x2b10e6dc568c]  INFO -- TaskHelper: Enqueuing task: 23191\nI, [2018-09-21T12:34:10.608792 #23837] []  INFO -- DirectorJobRunner: Looking for task with task id 23191\nD, [2018-09-21T12:34:10.609743 #23837] [] DEBUG -- DirectorJobRunner: (0.000264s) (conn: 47042397571780) SELECT * FROM \"tasks\" WHERE \"id\" = 23191\nI, [2018-09-21T12:34:10.610735 #23837] []  INFO -- DirectorJobRunner: Found task #23191, :state=>\"processing\", :timestamp=>2018-09-21 12:34:10 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/23191\", :checkpoint_time=>2018-09-21 12:34:10 UTC, :type=>\"update_deployment\", :username=>\"ops_manager\", :deployment_name=>\"service-instance_f0ef4e53-2a6f-4b6f-a854-b67c4d1894f1\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nI, [2018-09-21T12:34:10.610808 #23837] []  INFO -- DirectorJobRunner: Running from worker 'worker_4' on director/39cc9961-2a18-47ab-7748-d8a26e5832e8 (127.0.0.1)\nI, [2018-09-21T12:34:10.610865 #23837] []  INFO -- DirectorJobRunner: Starting task: 23191\nI, [2018-09-21T12:34:10.610931 #23837] [task:23191]  INFO -- DirectorJobRunner: Creating job\nD, [2018-09-21T12:34:10.611404 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000168s) (conn: 47042397571780) SELECT * FROM \"tasks\" WHERE \"id\" = 23191\nD, [2018-09-21T12:34:10.612298 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000177s) (conn: 47042397571780) SELECT * FROM \"tasks\" WHERE \"id\" = 23191\nI, [2018-09-21T12:34:10.615072 #23837] [task:23191]  INFO -- DirectorJobRunner: Performing task: #23191, :state=>\"processing\", :timestamp=>2018-09-21 12:34:10 UTC, :description=>\"create deployment\", :result=>nil, :output=>\"/var/vcap/store/director/tasks/23191\", :checkpoint_time=>2018-09-21 12:34:10 UTC, :type=>\"update_deployment\", :username=>\"ops_manager\", :deployment_name=>\"service-instance_f0ef4e53-2a6f-4b6f-a854-b67c4d1894f1\", :started_at=>nil, :event_output=>\"\", :result_output=>\"\", :context_id=>\"\"}>\nD, [2018-09-21T12:34:10.615982 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000083s) (conn: 47042397571780) BEGIN\nD, [2018-09-21T12:34:10.617491 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000322s) (conn: 47042397571780) UPDATE \"tasks\" SET \"state\" = 'processing', \"timestamp\" = '2018-09-21 12:34:10.615163+0000', \"description\" = 'create deployment', \"result\" = NULL, \"output\" = '/var/vcap/store/director/tasks/23191', \"checkpoint_time\" = '2018-09-21 12:34:10.615396+0000', \"type\" = 'update_deployment', \"username\" = 'ops_manager', \"deployment_name\" = 'service-instance_f0ef4e53-2a6f-4b6f-a854-b67c4d1894f1', \"started_at\" = '2018-09-21 12:34:10.615293+0000', \"event_output\" = '', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 23191)\nD, [2018-09-21T12:34:10.618292 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000659s) (conn: 47042397571780) COMMIT\nI, [2018-09-21T12:34:10.618411 #23837] [task:23191]  INFO -- DirectorJobRunner: Reading deployment manifest\nD, [2018-09-21T12:34:10.618989 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000167s) (conn: 47042397571780) SELECT * FROM \"tasks\" WHERE \"id\" = 23191\nD, [2018-09-21T12:34:10.620258 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000074s) (conn: 47042397571780) BEGIN\nD, [2018-09-21T12:34:10.621227 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000492s) (conn: 47042397571780) INSERT INTO \"events\" (\"parent_id\", \"timestamp\", \"user\", \"action\", \"object_type\", \"object_name\", \"error\", \"task\", \"deployment\", \"instance\", \"context_json\") VALUES (NULL, '2018-09-21 12:34:10.619593+0000', 'ops_manager', 'update', 'deployment', NULL, 'no implicit conversion of nil into String', '23191', NULL, NULL, '{}') RETURNING *\nD, [2018-09-21T12:34:10.625738 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.004195s) (conn: 47042397571780) COMMIT\nD, [2018-09-21T12:34:10.626630 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000064s) (conn: 47042397571780) BEGIN\nD, [2018-09-21T12:34:10.627311 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000285s) (conn: 47042397571780) UPDATE \"tasks\" SET \"event_output\" = (\"event_output\" || '{\"time\":1537533250,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n') WHERE (\"id\" = 23191)\nD, [2018-09-21T12:34:10.628045 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000615s) (conn: 47042397571780) COMMIT\nE, [2018-09-21T12:34:10.628206 #23837] [task:23191] ERROR -- DirectorJobRunner: no implicit conversion of nil into String\n/var/vcap/data/packages/ruby-2.4-r4/5fa1496fb6d4278aa2e385cf20602f54872fc6a4/lib/ruby/2.4.0/psych.rb:377:in parse'\n/var/vcap/data/packages/ruby-2.4-r4/5fa1496fb6d4278aa2e385cf20602f54872fc6a4/lib/ruby/2.4.0/psych.rb:377:inparse_stream'\n/var/vcap/data/packages/ruby-2.4-r4/5fa1496fb6d4278aa2e385cf20602f54872fc6a4/lib/ruby/2.4.0/psych.rb:325:in parse'\n/var/vcap/data/packages/ruby-2.4-r4/5fa1496fb6d4278aa2e385cf20602f54872fc6a4/lib/ruby/2.4.0/psych.rb:252:inload'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/update_deployment.rb:28:in perform'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:99:inperform_job'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in block in run'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh_common-0.0.0/lib/common/thread_formatter.rb:52:inwith_thread_name'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/job_runner.rb:34:in run'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/base_job.rb:10:inperform'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:36:in block in perform'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/bosh-director-0.0.0/lib/bosh/director/jobs/db_job.rb:83:inblock (3 levels) in run'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/eventmachine-1.2.5/lib/eventmachine.rb:1076:in block in spawn_threadpool'\n/var/vcap/data/packages/director/3a8a83074d698ec5bd1b812bb5bee53ebcfe87ae/gem_home/ruby/2.4.0/gems/logging-2.2.2/lib/logging/diagnostic_context.rb:474:inblock in create_with_logging_context'\nD, [2018-09-21T12:34:10.628769 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000191s) (conn: 47042397571780) SELECT * FROM \"tasks\" WHERE \"id\" = 23191\nD, [2018-09-21T12:34:10.629798 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000064s) (conn: 47042397571780) BEGIN\nD, [2018-09-21T12:34:10.630590 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000260s) (conn: 47042397571780) UPDATE \"tasks\" SET \"state\" = 'error', \"timestamp\" = '2018-09-21 12:34:10.629423+0000', \"description\" = 'create deployment', \"result\" = 'no implicit conversion of nil into String', \"output\" = '/var/vcap/store/director/tasks/23191', \"checkpoint_time\" = '2018-09-21 12:34:10.615396+0000', \"type\" = 'update_deployment', \"username\" = 'ops_manager', \"deployment_name\" = 'service-instance_f0ef4e53-2a6f-4b6f-a854-b67c4d1894f1', \"started_at\" = '2018-09-21 12:34:10.615293+0000', \"event_output\" = '{\"time\":1537533250,\"error\":{\"code\":100,\"message\":\"no implicit conversion of nil into String\"}}\n', \"result_output\" = '', \"context_id\" = '' WHERE (\"id\" = 23191)\nD, [2018-09-21T12:34:10.631264 #23837] [task:23191] DEBUG -- DirectorJobRunner: (0.000554s) (conn: 47042397571780) COMMIT\nI, [2018-09-21T12:34:10.631420 #23837] []  INFO -- DirectorJobRunner: Task took 0.020464458 seconds to process.\nTask 23191 error\nCapturing task '23191' output:\n  Expected task '23191' to succeed but state is 'error'\nExit code 1\nroot@New-PksCli:/home/pksadmin# bosh task 23191 --debug\n```. ",
    "srikanth19": "Thanks for the information.\nI have deployed the bosh director using the bosh bootloader and I am able to connect using bosh CLIV2 and deployed CF, but now I cannot change the SAS Manifest file so I have to go with it. \nI have now installed the bosh CLIV1\n[root@bosh bbl]# boshv1 --version\nBOSH 1.3262.26.0\nbut when I try to connect to bosh director using CLIV1 commands, I am facing below error.\n[root@bosh bbl]# boshv1 target https://34.203.223.237:25555\nTarget set to 'bosh-bbl-env-winnipeg-2018-09-27t21-06z'\n/usr/local/share/gems/gems/cf-uaa-lib-3.2.5/lib/uaa/http.rb:173:in rescue in net_http_request': error: Connection timed out - connect(2) (CF::UAA::BadTarget)\n        from /usr/local/share/gems/gems/cf-uaa-lib-3.2.5/lib/uaa/http.rb:159:innet_http_request'\n        from /usr/local/share/gems/gems/cf-uaa-lib-3.2.5/lib/uaa/http.rb:147:in request'\n        from /usr/local/share/gems/gems/cf-uaa-lib-3.2.5/lib/uaa/token_issuer.rb:77:inrequest_token'\n        from /usr/local/share/gems/gems/cf-uaa-lib-3.2.5/lib/uaa/token_issuer.rb:264:in client_credentials_grant'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/client_token_issuer.rb:18:inaccess_info'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/client.rb:34:in block in access_info'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/client.rb:50:inwith_save'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/client.rb:34:in access_info'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/token_provider.rb:47:inclient_access_info'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/token_provider.rb:33:in get_access_info'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/uaa/token_provider.rb:16:intoken'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/client/credentials.rb:14:in authorization_header'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/base_command.rb:72:inlogged_in?'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/commands/misc.rb:119:in set_target'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/command_handler.rb:57:inrun'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/lib/cli/runner.rb:59:in run'\n        from /usr/local/share/gems/gems/bosh_cli-1.3262.26.0/bin/bosh:19:in'\n        from /usr/local/bin/boshv1:23:in load'\n        from /usr/local/bin/boshv1:23:in'\nCan you help me to connect to bosh director using the CLI1 commands.. Hi,\nNow I am using cliv2 supported Manifest file but I am facing the below error. I have attached manifest and bosh release output command file.\nError: Unable to render instance groups for deployment. Errors are:\n  - Unable to render jobs for instance group 'cas_controller'. Errors are:\n    - Unable to render templates for job 'cas_controller'. Errors are:\n      - Error filling in template 'cas_grid_vars' (line 14: undefined method ip' for nil:NilClass)\n      - Error filling in template 'properties.sh.erb' (line 29: undefined methodip' for nil:NilClass)\n  - Unable to render jobs for instance group 'microservices'. Errors are:\n    - Unable to render templates for job 'sas-microservices-deploy'. Errors are:\n      - Failed to find variable '/bosh-bbl-env-winnipeg-2018-09-27t21-06z/sas-vdmml-ubuntu/sas' from config server: HTTP Code '404', Error: 'The request could not be completed because the credential does not exist or you do not have sufficient authorization.'\nThanks in advance.\nmanifest.txt\nreleases.txt\n. Any update. Anyone can help me out . Hi,\nThanks for responding.\nI am trying to deploy SAS Viya on cloud foundry. The deployment has started and has thrown the below error:\nError: 'cas_controller/6f220e5f-f2ac-4fbf-8c78-d41538c6dc02 (0)' is not running after update. Review logs for failed jobs: java, numa, dpkg-installer, cas_controller, tmp_chmod, bosh-dns, bosh-dns-resolvconf, bosh-dns-healthcheck\nTask 137 | 12:24:47 | Error: 'cas_controller/6f220e5f-f2ac-4fbf-8c78-d41538c6dc02 (0)' is not running after update. Review logs for failed jobs: java, numa, dpkg-installer, cas_controller, tmp_chmod, bosh-dns, bosh-dns-resolvconf, bosh-dns-healthcheck\nI logged in the cas_controller and checked the /var/vcap logs and found below error(not sure if this is the real culprit) in the logs monit/cas-controller.err.log \nls: cannot access /var/vcap/packages//lib: No such file or directory\nls: cannot access /var/vcap/packages///.jar: No such file or directory\n/var/vcap/jobs/cas_controller/bin/cas_controller_ctl: line 34: hash: numactl: not found\nBut I was able to locate numactl in /var/vcap/packages folder.\nI am not sure if this could help or not, I have also observed diego cell was in failing state after the above error was encountered.\nThanks in advance.\n. ",
    "sunshineperi": "This is the thread on slack:\nhttps://cloudfoundry.slack.com/archives/C02HPPYQ2/p1540190522000100\nhttps://cloudfoundry.slack.com/archives/C02HPPYQ2/p1539845673000100\nperi [1 day ago]\n@h4xnoodle, from the code, it seems that default cpi should be used, cpi_name is nil or empty is checked almost everywhere, except the stemcell.rb,  after adding the check,  it worked! So I assume this is something that is missing?  You can find the code fix at the bottom of the issue.\nh4xnoodle [14 hours ago]\nif we do end up checking elsewhere, we'll make sure this still makes sense. Do you have insight on how your CPI config had an empty CPI defined? How come the CPI was not defined in cloud_properties of the az?\nemm,   do you have insight?  how come? . ",
    "ishustava": "Hey @mfine30,\nSorry for the delay!\nIt fails roughly twice per week across 5 directors. As I've mentioned in the issue, the error goes away on a re-run, but we think it may contribute to growing bosh director persistent disk size. My theory is that when the above error occurs, director may leave behind some blobs that won't be picked up on the next bosh clean-up task since they are no longer in the database. \nAdditionally, it causes some confusion for the new team members. It's been on our known issues board for a while and we know to re-run this job if error occurs. We'd like to get it fixed eventually, but it's not a blocker.. Hey @mfine30,\nSorry for the delay!\nIt fails roughly twice per week across 5 directors. As I've mentioned in the issue, the error goes away on a re-run, but we think it may contribute to growing bosh director persistent disk size. My theory is that when the above error occurs, director may leave behind some blobs that won't be picked up on the next bosh clean-up task since they are no longer in the database. \nAdditionally, it causes some confusion for the new team members. It's been on our known issues board for a while and we know to re-run this job if error occurs. We'd like to get it fixed eventually, but it's not a blocker.. @jaresty both deployments had it enabled and, I believe, it was enabled before the first deployment started.. @jaresty both deployments had it enabled and, I believe, it was enabled before the first deployment started.. I don't think that feature will work if a job template intentionally gets the IP, which is the case for nats job. As a side note, the logic there is kind of complicated and probably today the line I linked can just be \nnet: <%= spec.ip %> as I have found out from #1476.\nFWIW, cf upgrades with create-swap-delete work in the happy case scenario (if the above network error doesn't happen).. I don't think that feature will work if a job template intentionally gets the IP, which is the case for nats job. As a side note, the logic there is kind of complicated and probably today the line I linked can just be \nnet: <%= spec.ip %> as I have found out from #1476.\nFWIW, cf upgrades with create-swap-delete work in the happy case scenario (if the above network error doesn't happen).. Hey @jaresty @mfine30, saw your conversation in Tracker and decided to reply here.\nI might be missing something, but I think the fact that the nats job specifically asks for IP addresses might be a red herring. \nWe've done a few upgrades of CF successfully with create-swap-delete strategy turned on if VM creation doesn't fail due to network flakiness. I think this might be because both route_registrar and route_emitter consume nats addresses via instances[i].address from the nats link. NATs servers are not aware that they are in the cluster and, instead, NATs clients are cluster aware\nWhat looks suspicious to me is the output of step 3, where only the packages for the doppler instance group were downloaded. In a happy case scenario, we're seeing that packages for all VMs are downloaded after VM creation completes successfully.. Hey @jaresty @mfine30, saw your conversation in Tracker and decided to reply here.\nI might be missing something, but I think the fact that the nats job specifically asks for IP addresses might be a red herring. \nWe've done a few upgrades of CF successfully with create-swap-delete strategy turned on if VM creation doesn't fail due to network flakiness. I think this might be because both route_registrar and route_emitter consume nats addresses via instances[i].address from the nats link. NATs servers are not aware that they are in the cluster and, instead, NATs clients are cluster aware\nWhat looks suspicious to me is the output of step 3, where only the packages for the doppler instance group were downloaded. In a happy case scenario, we're seeing that packages for all VMs are downloaded after VM creation completes successfully.. Sorry for the confusion. We are seeing both.\nIn my previous comment I meant to say that we saw other upgrades (not including the one reported in this issue) succeed. In those cases, NATS job has the new IP in the template.\nThis bug described in my original post will fit scenario #1 you\u2019ve mentioned. Scenario #2 describes other successful upgrades we have done with create swap delete turned on. I mentioned the successful cases to highlight the fact that we\u2019ve seen successful upgrades with create-swap-delete turned on and NATS/NATS consumers being the way they are, so I think the issue is not NATS-specific, that seems like a red herring.. Sorry for the confusion. We are seeing both.\nIn my previous comment I meant to say that we saw other upgrades (not including the one reported in this issue) succeed. In those cases, NATS job has the new IP in the template.\nThis bug described in my original post will fit scenario #1 you\u2019ve mentioned. Scenario #2 describes other successful upgrades we have done with create swap delete turned on. I mentioned the successful cases to highlight the fact that we\u2019ve seen successful upgrades with create-swap-delete turned on and NATS/NATS consumers being the way they are, so I think the issue is not NATS-specific, that seems like a red herring.. Sure, here it is:\n```\nUsing environment 'https://10.0.0.6:25555' as client 'admin'\nTask 31121\nTask 31121 | 19:11:52 | Preparing deployment: Preparing deployment (00:00:28)\nTask 31121 | 19:14:34 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 31121 | 19:14:34 | Compiling packages: bitsgo/daf8d36bc46450c6ecdda2cdfecd616203453a91 (00:02:00)\nTask 31121 | 19:17:09 | Creating missing vms: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0)\nTask 31121 | 19:17:09 | Creating missing vms: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0)\nTask 31121 | 19:17:09 | Creating missing vms: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1)\nTask 31121 | 19:17:09 | Creating missing vms: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0)\nTask 31121 | 19:17:09 | Creating missing vms: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1)\nTask 31121 | 19:17:09 | Creating missing vms: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2)\nTask 31121 | 19:17:09 | Creating missing vms: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:17:09 | Creating missing vms: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:17:09 | Creating missing vms: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0)\nTask 31121 | 19:17:09 | Creating missing vms: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0)\nTask 31121 | 19:17:09 | Creating missing vms: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1)\nTask 31121 | 19:17:09 | Creating missing vms: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0)\nTask 31121 | 19:17:09 | Creating missing vms: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2)\nTask 31121 | 19:17:09 | Creating missing vms: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1)\nTask 31121 | 19:17:09 | Creating missing vms: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:17:09 | Creating missing vms: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1)\nTask 31121 | 19:17:09 | Creating missing vms: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0)\nTask 31121 | 19:17:09 | Creating missing vms: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0)\nTask 31121 | 19:17:09 | Creating missing vms: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:17:09 | Creating missing vms: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0)\nTask 31121 | 19:17:09 | Creating missing vms: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0)\nTask 31121 | 19:17:09 | Creating missing vms: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0)\nTask 31121 | 19:17:09 | Creating missing vms: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1)\nTask 31121 | 19:17:09 | Creating missing vms: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0)\nTask 31121 | 19:17:09 | Creating missing vms: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:17:09 | Creating missing vms: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0)\nTask 31121 | 19:17:09 | Creating missing vms: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:17:09 | Creating missing vms: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0)\nTask 31121 | 19:19:11 | Creating missing vms: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (00:02:02)\nTask 31121 | 19:19:12 | Creating missing vms: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:19:16 | Creating missing vms: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:02:07)\nTask 31121 | 19:19:16 | Creating missing vms: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0)\nTask 31121 | 19:20:02 | Creating missing vms: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:02:53)\nTask 31121 | 19:20:04 | Creating missing vms: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (00:02:55)\nTask 31121 | 19:20:05 | Creating missing vms: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:02:56)\nTask 31121 | 19:20:12 | Creating missing vms: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (00:03:03)\nTask 31121 | 19:20:19 | Creating missing vms: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:03:10)\nTask 31121 | 19:20:31 | Creating missing vms: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (00:03:22)\nTask 31121 | 19:20:32 | Creating missing vms: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:03:23)\nTask 31121 | 19:20:39 | Creating missing vms: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:03:30)\nTask 31121 | 19:20:43 | Creating missing vms: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:03:34)\nTask 31121 | 19:20:42 | Creating missing vms: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (00:03:33)\nTask 31121 | 19:20:52 | Creating missing vms: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (00:03:43)\nTask 31121 | 19:21:02 | Creating missing vms: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (00:03:53)\nTask 31121 | 19:21:14 | Creating missing vms: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (00:04:05)\nTask 31121 | 19:21:19 | Creating missing vms: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:04:10)\nTask 31121 | 19:21:19 | Creating missing vms: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:04:10)\nTask 31121 | 19:21:23 | Creating missing vms: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (00:04:14)\nTask 31121 | 19:21:24 | Creating missing vms: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (00:02:08)\nTask 31121 | 19:21:27 | Creating missing vms: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:04:18)\nTask 31121 | 19:21:27 | Creating missing vms: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (00:04:18)\nTask 31121 | 19:21:27 | Creating missing vms: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:02:15)\nTask 31121 | 19:21:28 | Creating missing vms: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (00:04:19)\nTask 31121 | 19:21:32 | Creating missing vms: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (00:04:23)\nTask 31121 | 19:21:32 | Creating missing vms: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:04:23)\nTask 31121 | 19:21:36 | Creating missing vms: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (00:04:27)\nTask 31121 | 19:21:37 | Creating missing vms: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:04:28)\nTask 31121 | 19:21:38 | Creating missing vms: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:04:29)\nTask 31121 | 19:21:40 | Creating missing vms: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:04:31)\nTask 31121 | 19:21:43 | Creating missing vms: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:04:34)\nTask 31121 | 19:21:53 | Creating missing vms: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:04:44)\nTask 31121 | 19:21:54 | Creating missing vms: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (00:04:45)\nTask 31121 | 19:21:55 | Creating missing vms: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (00:04:46)\nTask 31121 | 19:21:56 | Creating missing vms: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:04:47)\nTask 31121 | 19:21:56 | Downloading packages: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0)\nTask 31121 | 19:21:56 | Downloading packages: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0)\nTask 31121 | 19:21:56 | Downloading packages: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1)\nTask 31121 | 19:21:56 | Downloading packages: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1)\nTask 31121 | 19:21:56 | Downloading packages: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1)\nTask 31121 | 19:21:56 | Downloading packages: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2)\nTask 31121 | 19:21:56 | Downloading packages: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0)\nTask 31121 | 19:21:56 | Downloading packages: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1)\nTask 31121 | 19:21:56 | Downloading packages: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0)\nTask 31121 | 19:21:56 | Downloading packages: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0)\nTask 31121 | 19:21:56 | Downloading packages: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0)\nTask 31121 | 19:21:56 | Downloading packages: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:21:56 | Downloading packages: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0)\nTask 31121 | 19:21:56 | Downloading packages: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0)\nTask 31121 | 19:21:56 | Downloading packages: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0)\nTask 31121 | 19:21:56 | Downloading packages: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1)\nTask 31121 | 19:21:56 | Downloading packages: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:21:56 | Downloading packages: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:21:56 | Downloading packages: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1)\nTask 31121 | 19:21:56 | Downloading packages: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:21:56 | Downloading packages: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2)\nTask 31121 | 19:21:56 | Downloading packages: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:21:56 | Downloading packages: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0)\nTask 31121 | 19:21:56 | Downloading packages: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1)\nTask 31121 | 19:21:56 | Downloading packages: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0)\nTask 31121 | 19:21:56 | Downloading packages: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1)\nTask 31121 | 19:21:56 | Downloading packages: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0)\nTask 31121 | 19:21:56 | Downloading packages: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1)\nTask 31121 | 19:21:56 | Downloading packages: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:22:30 | Downloading packages: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:00:34)\nTask 31121 | 19:22:30 | Downloading packages: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:00:34)\nTask 31121 | 19:22:30 | Downloading packages: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:22:30 | Downloading packages: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0)\nTask 31121 | 19:22:31 | Downloading packages: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (00:00:35)\nTask 31121 | 19:22:31 | Downloading packages: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (00:00:35)\nTask 31121 | 19:22:33 | Downloading packages: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:00:37)\nTask 31121 | 19:22:33 | Downloading packages: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:00:37)\nTask 31121 | 19:22:34 | Downloading packages: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (00:00:38)\nTask 31121 | 19:22:36 | Downloading packages: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:00:40)\nTask 31121 | 19:22:37 | Downloading packages: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (00:00:41)\nTask 31121 | 19:22:37 | Downloading packages: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:00:41)\nTask 31121 | 19:22:38 | Downloading packages: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (00:00:42)\nTask 31121 | 19:22:39 | Downloading packages: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (00:00:43)\nTask 31121 | 19:22:40 | Downloading packages: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:00:44)\nTask 31121 | 19:22:42 | Downloading packages: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:00:46)\nTask 31121 | 19:22:43 | Downloading packages: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:00:47)\nTask 31121 | 19:22:46 | Downloading packages: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:00:50)\nTask 31121 | 19:22:48 | Downloading packages: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (00:00:52)\nTask 31121 | 19:22:49 | Downloading packages: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (00:00:53)\nTask 31121 | 19:22:54 | Downloading packages: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (00:00:24)\nTask 31121 | 19:23:06 | Downloading packages: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (00:01:10)\nTask 31121 | 19:23:16 | Downloading packages: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:00:46)\nTask 31121 | 19:23:36 | Downloading packages: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:01:40)\nTask 31121 | 19:23:41 | Downloading packages: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (00:01:45)\nTask 31121 | 19:23:41 | Downloading packages: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:01:45)\nTask 31121 | 19:23:42 | Downloading packages: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (00:01:46)\nTask 31121 | 19:23:44 | Downloading packages: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (00:01:48)\nTask 31121 | 19:23:46 | Downloading packages: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:01:50)\nTask 31121 | 19:23:46 | Downloading packages: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (00:01:50)\nTask 31121 | 19:23:48 | Downloading packages: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:01:52)\nTask 31121 | 19:23:49 | Downloading packages: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (00:01:53)\nTask 31121 | 19:23:49 | Downloading packages: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:01:53)\nTask 31121 | 19:23:52 | Downloading packages: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:01:56)\nTask 31121 | 19:23:56 | Downloading packages: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:02:00)\nTask 31121 | 19:24:01 | Downloading packages: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (00:02:05)\nTask 31121 | 19:24:01 | Updating instance adapter: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (canary)\nTask 31121 | 19:24:01 | Updating instance nats: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (canary) (00:00:44)\nTask 31121 | 19:24:45 | Updating instance nats: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:00:18)\nTask 31121 | 19:25:43 | Updating instance adapter: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (canary) (00:01:42)\nTask 31121 | 19:25:43 | Updating instance adapter: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:01:16)\nTask 31121 | 19:26:59 | Updating instance database: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (canary) (00:01:43)\nTask 31121 | 19:28:42 | Updating instance database: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:01:33)\nTask 31121 | 19:30:15 | Updating instance database: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:01:32)\nTask 31121 | 19:31:47 | Updating instance diego-api: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (canary) (00:00:45)\nTask 31121 | 19:32:32 | Updating instance diego-api: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:00:36)\nTask 31121 | 19:33:08 | Updating instance api: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (canary)\nTask 31121 | 19:33:08 | Updating instance bits: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (canary)\nTask 31121 | 19:33:08 | Updating instance cc-worker: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (canary)\nTask 31121 | 19:33:08 | Updating instance uaa: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (canary)\nTask 31121 | 19:33:54 | Updating instance bits: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (canary) (00:00:46)\nTask 31121 | 19:33:54 | Updating instance bits: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:33:54 | Updating instance cc-worker: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (canary) (00:00:46)\nTask 31121 | 19:33:54 | Updating instance cc-worker: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:34:14 | Updating instance bits: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:00:20)\nTask 31121 | 19:34:16 | Updating instance cc-worker: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:00:22)\nTask 31121 | 19:34:43 | Updating instance api: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (canary) (00:01:35)\nTask 31121 | 19:34:43 | Updating instance api: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:35:11 | Updating instance uaa: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (canary) (00:02:03)\nTask 31121 | 19:35:11 | Updating instance uaa: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:36:11 | Updating instance api: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:01:28)\nTask 31121 | 19:37:19 | Updating instance uaa: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:02:08)\nTask 31121 | 19:37:19 | Updating instance router: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (canary) (00:02:51)\nTask 31121 | 19:40:10 | Updating instance router: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:02:55)\nTask 31121 | 19:43:05 | Updating instance tcp-router: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (canary) (00:00:58)\nTask 31121 | 19:44:03 | Updating instance tcp-router: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:01:03)\nTask 31121 | 19:45:06 | Updating instance scheduler: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (canary) (00:00:48)\nTask 31121 | 19:45:54 | Updating instance scheduler: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:00:55)\nTask 31121 | 19:46:49 | Updating instance diego-cell: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (canary)\nTask 31121 | 19:46:49 | Updating instance doppler: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (canary) (00:00:51)\nTask 31121 | 19:47:40 | Updating instance doppler: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:00:40)\nTask 31121 | 19:48:20 | Updating instance doppler: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:48:26 | Updating instance diego-cell: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (canary) (00:01:37)\nTask 31121 | 19:48:26 | Updating instance diego-cell: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:49:00 | Updating instance doppler: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:00:40)\nTask 31121 | 19:49:00 | Updating instance doppler: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:00:40)\nTask 31121 | 19:50:03 | Updating instance diego-cell: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:01:37)\nTask 31121 | 19:50:03 | Updating instance log-api: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (canary) (00:01:43)\nTask 31121 | 19:51:46 | Updating instance log-api: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:01:18)\nTask 31121 | 19:53:04 | Updating instance nozzle: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (canary)\nTask 31121 | 19:53:04 | Updating instance credhub: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (canary) (00:01:05)\nTask 31121 | 19:54:09 | Updating instance credhub: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:54:24 | Updating instance nozzle: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (canary) (00:01:20)\nTask 31121 | 19:55:14 | Updating instance credhub: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:01:05)\nTask 31121 Started  Wed Oct 31 19:11:52 UTC 2018\nTask 31121 Finished Wed Oct 31 19:55:14 UTC 2018\nTask 31121 Duration 00:43:22\nTask 31121 done\nSucceeded\n. Sure, here it is:\nUsing environment 'https://10.0.0.6:25555' as client 'admin'\nTask 31121\nTask 31121 | 19:11:52 | Preparing deployment: Preparing deployment (00:00:28)\nTask 31121 | 19:14:34 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 31121 | 19:14:34 | Compiling packages: bitsgo/daf8d36bc46450c6ecdda2cdfecd616203453a91 (00:02:00)\nTask 31121 | 19:17:09 | Creating missing vms: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0)\nTask 31121 | 19:17:09 | Creating missing vms: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0)\nTask 31121 | 19:17:09 | Creating missing vms: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1)\nTask 31121 | 19:17:09 | Creating missing vms: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0)\nTask 31121 | 19:17:09 | Creating missing vms: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1)\nTask 31121 | 19:17:09 | Creating missing vms: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2)\nTask 31121 | 19:17:09 | Creating missing vms: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:17:09 | Creating missing vms: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:17:09 | Creating missing vms: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0)\nTask 31121 | 19:17:09 | Creating missing vms: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0)\nTask 31121 | 19:17:09 | Creating missing vms: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1)\nTask 31121 | 19:17:09 | Creating missing vms: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0)\nTask 31121 | 19:17:09 | Creating missing vms: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2)\nTask 31121 | 19:17:09 | Creating missing vms: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1)\nTask 31121 | 19:17:09 | Creating missing vms: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:17:09 | Creating missing vms: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1)\nTask 31121 | 19:17:09 | Creating missing vms: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0)\nTask 31121 | 19:17:09 | Creating missing vms: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0)\nTask 31121 | 19:17:09 | Creating missing vms: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:17:09 | Creating missing vms: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0)\nTask 31121 | 19:17:09 | Creating missing vms: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0)\nTask 31121 | 19:17:09 | Creating missing vms: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0)\nTask 31121 | 19:17:09 | Creating missing vms: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1)\nTask 31121 | 19:17:09 | Creating missing vms: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0)\nTask 31121 | 19:17:09 | Creating missing vms: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:17:09 | Creating missing vms: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0)\nTask 31121 | 19:17:09 | Creating missing vms: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0)\nTask 31121 | 19:17:09 | Creating missing vms: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:17:09 | Creating missing vms: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0)\nTask 31121 | 19:19:11 | Creating missing vms: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (00:02:02)\nTask 31121 | 19:19:12 | Creating missing vms: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:19:16 | Creating missing vms: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:02:07)\nTask 31121 | 19:19:16 | Creating missing vms: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0)\nTask 31121 | 19:20:02 | Creating missing vms: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:02:53)\nTask 31121 | 19:20:04 | Creating missing vms: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (00:02:55)\nTask 31121 | 19:20:05 | Creating missing vms: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:02:56)\nTask 31121 | 19:20:12 | Creating missing vms: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (00:03:03)\nTask 31121 | 19:20:19 | Creating missing vms: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:03:10)\nTask 31121 | 19:20:31 | Creating missing vms: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (00:03:22)\nTask 31121 | 19:20:32 | Creating missing vms: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:03:23)\nTask 31121 | 19:20:39 | Creating missing vms: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:03:30)\nTask 31121 | 19:20:43 | Creating missing vms: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:03:34)\nTask 31121 | 19:20:42 | Creating missing vms: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (00:03:33)\nTask 31121 | 19:20:52 | Creating missing vms: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (00:03:43)\nTask 31121 | 19:21:02 | Creating missing vms: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (00:03:53)\nTask 31121 | 19:21:14 | Creating missing vms: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (00:04:05)\nTask 31121 | 19:21:19 | Creating missing vms: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:04:10)\nTask 31121 | 19:21:19 | Creating missing vms: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:04:10)\nTask 31121 | 19:21:23 | Creating missing vms: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (00:04:14)\nTask 31121 | 19:21:24 | Creating missing vms: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (00:02:08)\nTask 31121 | 19:21:27 | Creating missing vms: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:04:18)\nTask 31121 | 19:21:27 | Creating missing vms: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (00:04:18)\nTask 31121 | 19:21:27 | Creating missing vms: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:02:15)\nTask 31121 | 19:21:28 | Creating missing vms: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (00:04:19)\nTask 31121 | 19:21:32 | Creating missing vms: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (00:04:23)\nTask 31121 | 19:21:32 | Creating missing vms: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:04:23)\nTask 31121 | 19:21:36 | Creating missing vms: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (00:04:27)\nTask 31121 | 19:21:37 | Creating missing vms: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:04:28)\nTask 31121 | 19:21:38 | Creating missing vms: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:04:29)\nTask 31121 | 19:21:40 | Creating missing vms: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:04:31)\nTask 31121 | 19:21:43 | Creating missing vms: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:04:34)\nTask 31121 | 19:21:53 | Creating missing vms: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:04:44)\nTask 31121 | 19:21:54 | Creating missing vms: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (00:04:45)\nTask 31121 | 19:21:55 | Creating missing vms: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (00:04:46)\nTask 31121 | 19:21:56 | Creating missing vms: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:04:47)\nTask 31121 | 19:21:56 | Downloading packages: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0)\nTask 31121 | 19:21:56 | Downloading packages: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0)\nTask 31121 | 19:21:56 | Downloading packages: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1)\nTask 31121 | 19:21:56 | Downloading packages: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1)\nTask 31121 | 19:21:56 | Downloading packages: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1)\nTask 31121 | 19:21:56 | Downloading packages: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2)\nTask 31121 | 19:21:56 | Downloading packages: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0)\nTask 31121 | 19:21:56 | Downloading packages: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1)\nTask 31121 | 19:21:56 | Downloading packages: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0)\nTask 31121 | 19:21:56 | Downloading packages: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0)\nTask 31121 | 19:21:56 | Downloading packages: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0)\nTask 31121 | 19:21:56 | Downloading packages: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:21:56 | Downloading packages: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0)\nTask 31121 | 19:21:56 | Downloading packages: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0)\nTask 31121 | 19:21:56 | Downloading packages: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0)\nTask 31121 | 19:21:56 | Downloading packages: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1)\nTask 31121 | 19:21:56 | Downloading packages: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:21:56 | Downloading packages: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:21:56 | Downloading packages: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1)\nTask 31121 | 19:21:56 | Downloading packages: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:21:56 | Downloading packages: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2)\nTask 31121 | 19:21:56 | Downloading packages: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:21:56 | Downloading packages: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0)\nTask 31121 | 19:21:56 | Downloading packages: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1)\nTask 31121 | 19:21:56 | Downloading packages: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0)\nTask 31121 | 19:21:56 | Downloading packages: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0)\nTask 31121 | 19:21:56 | Downloading packages: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1)\nTask 31121 | 19:21:56 | Downloading packages: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0)\nTask 31121 | 19:21:56 | Downloading packages: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1)\nTask 31121 | 19:21:56 | Downloading packages: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:22:30 | Downloading packages: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:00:34)\nTask 31121 | 19:22:30 | Downloading packages: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:00:34)\nTask 31121 | 19:22:30 | Downloading packages: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:22:30 | Downloading packages: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0)\nTask 31121 | 19:22:31 | Downloading packages: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (00:00:35)\nTask 31121 | 19:22:31 | Downloading packages: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (00:00:35)\nTask 31121 | 19:22:33 | Downloading packages: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:00:37)\nTask 31121 | 19:22:33 | Downloading packages: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:00:37)\nTask 31121 | 19:22:34 | Downloading packages: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (00:00:38)\nTask 31121 | 19:22:36 | Downloading packages: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:00:40)\nTask 31121 | 19:22:37 | Downloading packages: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (00:00:41)\nTask 31121 | 19:22:37 | Downloading packages: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:00:41)\nTask 31121 | 19:22:38 | Downloading packages: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (00:00:42)\nTask 31121 | 19:22:39 | Downloading packages: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (00:00:43)\nTask 31121 | 19:22:40 | Downloading packages: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:00:44)\nTask 31121 | 19:22:42 | Downloading packages: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:00:46)\nTask 31121 | 19:22:43 | Downloading packages: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:00:47)\nTask 31121 | 19:22:46 | Downloading packages: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:00:50)\nTask 31121 | 19:22:48 | Downloading packages: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (00:00:52)\nTask 31121 | 19:22:49 | Downloading packages: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (00:00:53)\nTask 31121 | 19:22:54 | Downloading packages: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (00:00:24)\nTask 31121 | 19:23:06 | Downloading packages: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (00:01:10)\nTask 31121 | 19:23:16 | Downloading packages: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:00:46)\nTask 31121 | 19:23:36 | Downloading packages: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:01:40)\nTask 31121 | 19:23:41 | Downloading packages: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (00:01:45)\nTask 31121 | 19:23:41 | Downloading packages: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:01:45)\nTask 31121 | 19:23:42 | Downloading packages: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (00:01:46)\nTask 31121 | 19:23:44 | Downloading packages: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (00:01:48)\nTask 31121 | 19:23:46 | Downloading packages: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:01:50)\nTask 31121 | 19:23:46 | Downloading packages: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (00:01:50)\nTask 31121 | 19:23:48 | Downloading packages: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:01:52)\nTask 31121 | 19:23:49 | Downloading packages: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (00:01:53)\nTask 31121 | 19:23:49 | Downloading packages: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:01:53)\nTask 31121 | 19:23:52 | Downloading packages: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:01:56)\nTask 31121 | 19:23:56 | Downloading packages: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:02:00)\nTask 31121 | 19:24:01 | Downloading packages: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (00:02:05)\nTask 31121 | 19:24:01 | Updating instance adapter: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (canary)\nTask 31121 | 19:24:01 | Updating instance nats: nats/2a3323f3-6889-49ad-8e2f-ce5b1b794525 (0) (canary) (00:00:44)\nTask 31121 | 19:24:45 | Updating instance nats: nats/060195cb-5efd-4896-bce4-bc749f01cceb (1) (00:00:18)\nTask 31121 | 19:25:43 | Updating instance adapter: adapter/318f900f-7462-4d90-a997-c373f9c57b68 (0) (canary) (00:01:42)\nTask 31121 | 19:25:43 | Updating instance adapter: adapter/94f6f903-225b-45e1-85f4-dd87b1c1a559 (1) (00:01:16)\nTask 31121 | 19:26:59 | Updating instance database: database/8dcc0ce0-c6a6-4c38-b254-88485ada485b (0) (canary) (00:01:43)\nTask 31121 | 19:28:42 | Updating instance database: database/ef31c418-d1a6-422e-ade0-8f841f043bbf (1) (00:01:33)\nTask 31121 | 19:30:15 | Updating instance database: database/5282d1aa-719a-4afc-b387-7cbb52eccdac (2) (00:01:32)\nTask 31121 | 19:31:47 | Updating instance diego-api: diego-api/b9347e4f-0928-4e4c-a07a-9436d1cfa1ad (0) (canary) (00:00:45)\nTask 31121 | 19:32:32 | Updating instance diego-api: diego-api/68882cea-be64-49e9-94fb-ec6b167d4974 (1) (00:00:36)\nTask 31121 | 19:33:08 | Updating instance api: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (canary)\nTask 31121 | 19:33:08 | Updating instance bits: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (canary)\nTask 31121 | 19:33:08 | Updating instance cc-worker: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (canary)\nTask 31121 | 19:33:08 | Updating instance uaa: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (canary)\nTask 31121 | 19:33:54 | Updating instance bits: bits/1216c5ff-3346-44da-9670-601c83ccde4a (0) (canary) (00:00:46)\nTask 31121 | 19:33:54 | Updating instance bits: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1)\nTask 31121 | 19:33:54 | Updating instance cc-worker: cc-worker/481cbbe9-70f5-4de8-b6f8-cce8b683b172 (0) (canary) (00:00:46)\nTask 31121 | 19:33:54 | Updating instance cc-worker: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1)\nTask 31121 | 19:34:14 | Updating instance bits: bits/a838c388-ee19-4a6d-9eb4-d48a49848cc2 (1) (00:00:20)\nTask 31121 | 19:34:16 | Updating instance cc-worker: cc-worker/c106bad4-d35b-4a56-9c2b-e73d12e17933 (1) (00:00:22)\nTask 31121 | 19:34:43 | Updating instance api: api/b62f8597-b0ed-4978-8bd3-f7e7bf44b8d1 (0) (canary) (00:01:35)\nTask 31121 | 19:34:43 | Updating instance api: api/5f648cd8-28ca-4776-a136-453c8313e94e (1)\nTask 31121 | 19:35:11 | Updating instance uaa: uaa/ee43f3a5-1a34-46ca-a58f-7aec6174db7a (0) (canary) (00:02:03)\nTask 31121 | 19:35:11 | Updating instance uaa: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1)\nTask 31121 | 19:36:11 | Updating instance api: api/5f648cd8-28ca-4776-a136-453c8313e94e (1) (00:01:28)\nTask 31121 | 19:37:19 | Updating instance uaa: uaa/0f607b25-08cf-4798-8682-25a58c552db9 (1) (00:02:08)\nTask 31121 | 19:37:19 | Updating instance router: router/0199ce00-0c80-46f1-905d-6111896bbd08 (0) (canary) (00:02:51)\nTask 31121 | 19:40:10 | Updating instance router: router/6fd5053f-7045-4a84-b4dc-3e44b21d1354 (1) (00:02:55)\nTask 31121 | 19:43:05 | Updating instance tcp-router: tcp-router/0008d728-a7bb-4e97-b6bd-56c92e1fec07 (0) (canary) (00:00:58)\nTask 31121 | 19:44:03 | Updating instance tcp-router: tcp-router/bef5c6ca-039e-4d5f-a616-3336f355ac46 (1) (00:01:03)\nTask 31121 | 19:45:06 | Updating instance scheduler: scheduler/2117a22c-a670-40fd-be06-676255ae5c75 (0) (canary) (00:00:48)\nTask 31121 | 19:45:54 | Updating instance scheduler: scheduler/11ddf194-ae27-430c-be61-eb96d08156d1 (1) (00:00:55)\nTask 31121 | 19:46:49 | Updating instance diego-cell: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (canary)\nTask 31121 | 19:46:49 | Updating instance doppler: doppler/0ba8ea77-a31a-4a08-8f1f-7fc463221147 (0) (canary) (00:00:51)\nTask 31121 | 19:47:40 | Updating instance doppler: doppler/90a9c3a5-10f5-4e3e-a9d5-0b62577624d7 (2) (00:00:40)\nTask 31121 | 19:48:20 | Updating instance doppler: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3)\nTask 31121 | 19:48:26 | Updating instance diego-cell: diego-cell/3128a3bf-0b26-4c8f-aa95-bd21f92f2e35 (0) (canary) (00:01:37)\nTask 31121 | 19:48:26 | Updating instance diego-cell: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1)\nTask 31121 | 19:49:00 | Updating instance doppler: doppler/8b3d6e73-02d0-4b17-9a48-e918b7c199f5 (3) (00:00:40)\nTask 31121 | 19:49:00 | Updating instance doppler: doppler/bcda4994-6d3a-4c5e-bb5f-12b9a68ee491 (1) (00:00:40)\nTask 31121 | 19:50:03 | Updating instance diego-cell: diego-cell/2f378dd6-fcbf-4c52-bf44-209573e13bfa (1) (00:01:37)\nTask 31121 | 19:50:03 | Updating instance log-api: log-api/2c085ced-2901-48ea-aed0-53d268b5dd57 (0) (canary) (00:01:43)\nTask 31121 | 19:51:46 | Updating instance log-api: log-api/825b6027-3c6e-4f4d-a6b2-5687d54224be (1) (00:01:18)\nTask 31121 | 19:53:04 | Updating instance nozzle: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (canary)\nTask 31121 | 19:53:04 | Updating instance credhub: credhub/1552a7d5-bb4a-4d01-9d6f-6bcc16e88eb5 (0) (canary) (00:01:05)\nTask 31121 | 19:54:09 | Updating instance credhub: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1)\nTask 31121 | 19:54:24 | Updating instance nozzle: nozzle/a31b05e8-926e-4fe8-93f1-e6e2f09f0bfb (0) (canary) (00:01:20)\nTask 31121 | 19:55:14 | Updating instance credhub: credhub/2501cfdb-564b-4e63-8916-fef4c9c218f3 (1) (00:01:05)\nTask 31121 Started  Wed Oct 31 19:11:52 UTC 2018\nTask 31121 Finished Wed Oct 31 19:55:14 UTC 2018\nTask 31121 Duration 00:43:22\nTask 31121 done\nSucceeded\n```. Hey @jfmyers9 & @dpb587-pivotal,\nThanks for looking into it. We have only seen this issue once and have since disabled it in our pipelines for other reasons.\nI've attached the two debug logs:\n30998_debug.log\n31003_debug.log\n. Hey @jfmyers9 & @dpb587-pivotal,\nThanks for looking into it. We have only seen this issue once and have since disabled it in our pipelines for other reasons.\nI've attached the two debug logs:\n30998_debug.log\n31003_debug.log\n. ",
    "ctaymor": "Btw, we don't see this problem in trusty stemcells, only on xenial.. ",
    "mxplusb": "I've also seen similar networking issues with create-swap-delete, you can find my deployment logs here: https://gist.github.com/9ad197b11dff26116d9b292b6cba0c36.. //cc @thomas1206 @bingosummer @jastev. @pivotal-mp & @jaresty can you clarify what IP addresses is not supported and not guaranteed to work means? Is this in reference to static IPs or something else?. @pivotal-mp & @jaresty can you clarify what IP addresses is not supported and not guaranteed to work means? Is this in reference to static IPs or something else?. ",
    "betht1220": "Please let me know if follows are correct if I want to add an additional instance for a group. Thanks\n1. delete the existing deployment, change the instance key for the group, set the deployment, and bosh deploy?\n2. This means all data for this deployment will be lost?. Is this how to download the manifest \"bosh manifest -d cf\"? \nI've used that and modify the instances and do \"bosh deploy\", I got error with \"file not found\".\nI've deleted all url in releases, and \"bosh deploy\", I run in the same error.\nLooks like my way of getting manifest is not correct. Thanks for the comments aemengo.. ",
    "aemengo": "Hi again^^ @betht1220. You're correct, deleting the existing deployment will mean that all the data will get lost. But you don't need to delete it. BOSH allows you to make these kinds of changes without losing your data.\nSimply download the manifest, edit the appropriate key and then redeploy and BOSH should be able to identity that you only want the number of instances changed.. @betht1220 I suspect that there might be a syntax issue. Try the following:\n```\n$ bosh -d cf manifest > ./cf.yml\nEdit the ./cf.yml in place, remove all the urls\n$ bosh -d cf deploy ./cf.yml\n```\nYou can find out more information here: https://bosh.io/docs/deploying/. Closed in favor of: https://github.com/cloudfoundry/bosh-cli/issues/518. ",
    "rosenhouse": "I considered exposing a single user-configured string-value to hold something like env-name.  I also considered threading the director name through directly under a well-named tag like director-name.\nI implemented this one thinking that the added flexibility was cheap and might have some benefit, e.g.   {region: eu}, {env: staging} might be better than director-name: eu-staging.\nBut I don't have a strong preference amongst the three options.  Any of them would satisfy our use-case.  . @mfine30 did this get prioritized?  routing team is interested in using the feature.  . Thanks!. ",
    "flyinprogrammer": "\nAll pull request submitters and commit authors must have a Contributor License Agreement (CLA) on-file with us. Please have everyone sign the appropriate CLA (individual or corporate), and send to: contributors@cloudfoundry.org.\n\nYa'll should consider Docusign - it's a much better solution.. ",
    "deniseyu": "hey @giner, thanks for raising this. @h4xnoodle's assessment is spot-on - there are a sequence of database calls that have to happen before BOSH can determine whether any changes need to be applied during the templating process, which takes around 0.3 seconds per instance depending on your network. we're going to look into improving the UX rather than investing in code optimization at this stage. would it help if the output were more granular, such as adding Preparing deployment: Rendering templates (00:01:59):\n```\nUsing environment '192.168.50.6' as client 'admin'\nTask 8\nTask 8 | 21:38:44 | Preparing deployment: Preparing deployment (00:00:13)\nTask 8 | 21:38:57 | Preparing deployment: Rendering templates (00:01:59)\nTask 8 Started  Mon Dec 17 21:38:44 UTC 2018\nTask 8 Finished Mon Dec 17 21:40:56 UTC 2018\nTask 8 Duration 00:02:12\nTask 8 done\nSucceeded\n```\n(cc @jfmyers9). ",
    "carlhejiayu": "We were able to successfully deploy fim addon with what was suggested by @voelzmo\nHowever, can it be updated in documentation to specify this? (Or if it is documented, please let us know where)\nAnd should BOSH director be updated to implicitly figure out the corresponding os from the name stemcell alias.. Something of concern is if there are multiple types of \"ubuntu-trusty\" stemcells uploaded of the same version number?\nFor example, there is a \"google-kvm\" and \"aws\" stemcell when the deployment supports multi CPI?. ",
    "NautiluX": "Hi @dpb587, thanks for the clarification. While I still think the output is a bit confusing and should maybe list those two \"groups\" of errands separately, I think this issue itself can be closed.. Hi @belinda-liu, it seems the issue still exists on https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-xenial-go_agent?v=250.9. We reproduced it on AWS using a dummy deployment and migrating it from r4.xlarge to r5.large.\ncc @KaiHofstetter . @dpb587 We ended up not changing the behavior of the current implementation. Templates still render in isolation, meaning if one template changes the spec then the next one gets the unmodified version.\nAs it is no breaking change, we will merge this PR. Feel free to reach out to us if you have further concerns.. Hi @dpb587-pivotal, thanks for your comment!\nUnfortunately, I think it's common in ruby to implement == yourself if you need it for your class and need to keep its existence in mind when adding properties to a class. We added a test to catch changes to the public interface. We will create a follow-up PR later.\nWe didn't consider adding tests for == as the behaviour of the whole thing shouldn't change, but I think you're right, it's part of the public interface of EvaluationContext. We will add those.\nAnd yes - I think we just overlooked @properties.. This change fixes this related issue where it director-sync-dns doesn't stop if an exception occurs.. ",
    "iainsproat": "We\u2019ve not been able to recreate this issue, so I guess it can be closed. We\nwere able to workaround the original problem by redeploying.\nOn Tue 8 Jan 2019 at 18:05, belinda-liu notifications@github.com wrote:\n\nHi @iainsproat https://github.com/iainsproat,\nIs this still an issue? Otherwise, we are going to close this due to\ninactivity.\nThanks!\n@belinda-liu https://github.com/belinda-liu && @pivotal-mp\nhttps://github.com/pivotal-mp\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/2103#issuecomment-452395195,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAEMMYcpOCEGPgUtFGHMkjBHdfceOHjeks5vBN3rgaJpZM4ZReO3\n.\n. We\u2019ve not been able to recreate this issue, so I guess it can be closed. We\nwere able to workaround the original problem by redeploying.\n\nOn Tue 8 Jan 2019 at 18:05, belinda-liu notifications@github.com wrote:\n\nHi @iainsproat https://github.com/iainsproat,\nIs this still an issue? Otherwise, we are going to close this due to\ninactivity.\nThanks!\n@belinda-liu https://github.com/belinda-liu && @pivotal-mp\nhttps://github.com/pivotal-mp\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/cloudfoundry/bosh/issues/2103#issuecomment-452395195,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAEMMYcpOCEGPgUtFGHMkjBHdfceOHjeks5vBN3rgaJpZM4ZReO3\n.\n. \n",
    "forexamplelyr": "@pivotal-mp  hi\uff01now I increased the size of my diskc to 150G\nwhen i deploy cf\uff0ci meet the issue\uff1a\n```\nTask 191 | 14:22:09 | Compiling packages: util-linux/6310f68e8294c980de62481bced2fc915f449e3a2cc2d13a7a2a49c4a612d825 (00:33:16)\nTask 191 | 14:22:10 | Compiling packages: file_server/be9b94de40a859edb9bb9859f4ab869b21347519\n(00:03:16)\nL Error: Action Failed get_task: Task with id 5e9185cd-7060-4729-4a9f-2a9ee4472a4f could not be found\nTask 191 | 14:27:01 | Compiling packages: auctioneer/69a1050832879e8c477a2a63c57fc5ce3e6993fe (00:29:38)\nTask 191 | 14:30:48 | Compiling packages: routing-api/d2cec651ce5287068b1b11959579af17c4248fde59f4aff4e43fb6097c94b633 (00:23:36)\nTask 191 | 14:31:05 | Compiling packages: ssh_proxy/779e429a52abaf0a9ca1fabd04eb8570aee8e121 (00:25:53)\nTask 191 | 14:46:29 | Compiling packages: healthcheck/57004bfd97bd8fa59e2920744bc1c4a39af381a8 (00:27:51)\nTask 191 | 14:55:47 | Compiling packages: diego-sshd/cb7bc74d3e357d48f2ee309b3c3ff35e8a6a95b5 (00:42:35)\nTask 191 | 14:55:55 | Error: Action Failed get_task: Task with id 5e9185cd-7060-4729-4a9f-2a9ee4472a4f could not be found\nTask 191 Started Sat Dec 22 13:29:56 UTC 2018\nTask 191 Finished Sat Dec 22 14:55:55 UTC 2018\nTask 191 Duration 01:25:59\nTask 191 error\nUpdating deployment:\nExpected task '191' to succeed but state is 'error'\nError: Action Failed get_task: Task with id 5e9185cd-7060-4729-4a9f-2a9ee4472a4f could not be found\n```\nis there sometihing wrong with it\uff1f. @dpb587-pivotal \nThank you for your answer. There were no ERROR level log in /var/vcap/sys/log/uaa.log \nMy friends tried to deploy vbox on a physical machine and then deployed cf, which was successful. So I suspect that the reason for these problems is because deploying cf can't re-vbox on a virtual machine with sufficient resources, and then deploy cf, so it always appears issue like: Action Failed get_task: Task c8f3bfb0-5b26-4c03-7cf4-d9031e20c571 result : Updating certificates with retries or Action Failed get_task: Task with id 5e9185cd-7060-4729-4a9f-2a9ee4472a4f could not be found\nthis is a related url\uff1ahttps://github.com/cloudfoundry/cf-deployment/issues/681. ",
    "mllives": "@dpb587-pivotal \nThanks for your advice! \nBut I still cannot solve this problem. I cannot  figure out where it is wrong.\nI checked monit information from /var/vcap/monit/monit.log and other information from /var/vcap/sys/log/*/ files. I found nothing. \nInformation in /var/vcap/monit/monit.log  as blow, it goes through the  \n\u201c info     : 'health_monitor' process is running with pid  XXX\n  error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP\n   info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n   error    : 'health_monitor' process is not running\n   info     : 'health_monitor' trying to restart\n   info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl  \u201d\ncycle endlessly.\n1 [UTC Feb 21 08:50:35] info     : monit: generated unique Monit id 50acf259a8fe945311dd3626be2c4744 and stored to '/root/.monit.id'\n      2 [UTC Feb 21 08:50:35] info     : Starting monit daemon with http interface at [127.0.0.1:2822]\n      3 [UTC Feb 21 08:50:35] info     : Starting monit HTTP server at [127.0.0.1:2822]\n      4 [UTC Feb 21 08:50:35] info     : monit HTTP server started\n      5 [UTC Feb 21 08:50:35] info     : 'system_localhost' Monit started\n      6 [UTC Feb 21 08:50:35] error    : Cannot open a connection to the mailserver 'localhost:2825' -- Transport endpoint is not connected\n      7 [UTC Feb 21 08:50:35] error    : No mail servers are available\n      8 [UTC Feb 21 08:50:35] error    : Cannot open a connection to the mailserver 'localhost:2825' -- Transport endpoint is not connected\n      9 [UTC Feb 21 08:50:35] error    : No mail servers are available\n     10 [UTC Feb 21 08:50:35] error    : Alert handler failed, retry scheduled for next cycle\n     11 [UTC Feb 21 08:55:46] info     : Starting monit daemon with http interface at [127.0.0.1:2822]\n     12 [UTC Feb 21 08:55:46] info     : Starting monit HTTP server at [127.0.0.1:2822]\n     13 [UTC Feb 21 08:55:46] info     : monit HTTP server started\n     14 [UTC Feb 21 08:55:46] info     : 'system_localhost' Monit started\n     15 [UTC Feb 21 11:48:53] info     : Starting monit daemon with http interface at [127.0.0.1:2822]\n     16 [UTC Feb 21 11:48:53] info     : Starting monit HTTP server at [127.0.0.1:2822]\n     17 [UTC Feb 21 11:48:53] info     : monit HTTP server started\n     18 [UTC Feb 21 11:48:53] info     : 'system_localhost' Monit started\n     19 [UTC Feb 21 11:49:11] info     : Starting monit daemon with http interface at [127.0.0.1:2822]\n     20 [UTC Feb 21 11:49:11] info     : Starting monit HTTP server at [127.0.0.1:2822]\n     21 [UTC Feb 21 11:49:11] info     : monit HTTP server started\n     22 [UTC Feb 21 11:49:11] info     : 'system_localhost' Monit started\n     23 [UTC Feb 21 11:49:11] error    : 'nats' process is not running\n     24 [UTC Feb 21 11:49:12] info     : start service 'registry' on user request\n     25 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     26 [UTC Feb 21 11:49:12] info     : start service 'health_monitor' on user request\n     27 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     28 [UTC Feb 21 11:49:12] info     : start service 'director_nginx' on user request\n     29 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     30 [UTC Feb 21 11:49:12] info     : start service 'director_sync_dns' on user request\n     31 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     32 [UTC Feb 21 11:49:12] info     : start service 'director_scheduler' on user request\n     33 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     34 [UTC Feb 21 11:49:12] info     : start service 'worker_4' on user request\n     35 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     36 [UTC Feb 21 11:49:12] info     : 'nats' trying to restart\n     37 [UTC Feb 21 11:49:12] info     : 'nats' start: /var/vcap/jobs/bpm/bin/bpm\n     38 [UTC Feb 21 11:49:12] info     : start service 'worker_3' on user request\n     39 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     40 [UTC Feb 21 11:49:12] info     : start service 'worker_2' on user request\n     41 [UTC Feb 21 11:49:12] info     : monit daemon at 19414 awakened\n     42 [UTC Feb 21 11:49:13] info     : start service 'worker_1' on user request\n     43 [UTC Feb 21 11:49:13] info     : monit daemon at 19414 awakened\n     44 [UTC Feb 21 11:49:13] info     : start service 'director' on user request\n     45 [UTC Feb 21 11:49:13] info     : monit daemon at 19414 awakened\n     46 [UTC Feb 21 11:49:13] info     : start service 'blobstore_nginx' on user request\n     47 [UTC Feb 21 11:49:13] info     : monit daemon at 19414 awakened\n     48 [UTC Feb 21 11:49:13] info     : start service 'postgres' on user request\n     49 [UTC Feb 21 11:49:13] info     : monit daemon at 19414 awakened \n50 [UTC Feb 21 11:49:13] info     : start service 'nats' on user request\n     51 [UTC Feb 21 11:49:13] info     : monit daemon at 19414 awakened\n     52 [UTC Feb 21 11:49:21] info     : 'postgres' start: /var/vcap/jobs/bpm/bin/bpm\n     53 [UTC Feb 21 11:49:28] info     : 'postgres' start action done\n     54 [UTC Feb 21 11:49:30] info     : 'blobstore_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n     55 [UTC Feb 21 11:49:43] info     : 'blobstore_nginx' start action done\n     56 [UTC Feb 21 11:49:44] info     : 'director' start: /var/vcap/jobs/bpm/bin/bpm\n     57 [UTC Feb 21 11:49:55] info     : 'worker_1' start: /var/vcap/jobs/director/bin/worker_ctl\n     58 [UTC Feb 21 11:49:58] info     : 'worker_2' start: /var/vcap/jobs/director/bin/worker_ctl\n     59 [UTC Feb 21 11:50:01] info     : 'worker_3' start: /var/vcap/jobs/director/bin/worker_ctl\n     60 [UTC Feb 21 11:50:05] info     : 'worker_4' start: /var/vcap/jobs/director/bin/worker_ctl\n     61 [UTC Feb 21 11:50:10] info     : 'director' start action done\n     62 [UTC Feb 21 11:50:16] info     : 'worker_1' start action done\n     63 [UTC Feb 21 11:50:24] info     : 'worker_2' start action done\n     64 [UTC Feb 21 11:50:32] info     : 'worker_3' start action done\n     65 [UTC Feb 21 11:50:40] info     : 'worker_4' start action done\n     66 [UTC Feb 21 11:50:45] info     : 'director_scheduler' start: /var/vcap/jobs/bpm/bin/bpm\n     67 [UTC Feb 21 11:51:11] info     : 'director_scheduler' start action done\n     68 [UTC Feb 21 11:51:15] info     : 'director_sync_dns' start: /var/vcap/jobs/bpm/bin/bpm\n     69 [UTC Feb 21 11:51:42] info     : 'director_sync_dns' start action done\n     70 [UTC Feb 21 11:51:47] info     : 'director_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n     71 [UTC Feb 21 11:52:17] error    : 'director_nginx' failed to start\n     72 [UTC Feb 21 11:52:22] info     : 'director_nginx' start action done\n     73 [UTC Feb 21 11:52:27] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n     74 [UTC Feb 21 11:52:36] info     : 'health_monitor' start action done\n     75 [UTC Feb 21 11:52:43] info     : 'registry' start: /var/vcap/jobs/bpm/bin/bpm\n     76 [UTC Feb 21 11:53:14] error    : 'registry' failed to start\n     77 [UTC Feb 21 11:53:21] info     : 'registry' start action done\n     78 [UTC Feb 21 11:53:24] info     : Awakened by User defined signal 1\n     79 [UTC Feb 21 11:53:27] info     : 'nats' start action done\n     80 [UTC Feb 21 11:53:29] info     : 'nats' process is running with pid 19445\n     81 [UTC Feb 21 11:53:33] error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP\n     82 [UTC Feb 21 11:53:41] info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n     83 ^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@[UTC Feb 21 11:59:05] info     : St     83 arting monit daemon with http interface at [127.0.0.1:2822]\n     84 [UTC Feb 21 11:59:05] info     : Starting monit HTTP server at [127.0.0.1:2822]\n     85 [UTC Feb 21 11:59:05] info     : monit HTTP server started\n     86 [UTC Feb 21 11:59:05] info     : 'system_localhost' Monit started\n     87 [UTC Feb 21 11:59:05] error    : Cannot open a connection to the mailserver 'localhost:2825' -- Transport endpoint is not connected\n     88 [UTC Feb 21 11:59:05] error    : No mail servers are available\n     89 [UTC Feb 21 11:59:05] error    : Cannot open a connection to the mailserver 'localhost:2825' -- Transport endpoint is not connected\n     90 [UTC Feb 21 11:59:05] error    : No mail servers are available\n     91 [UTC Feb 21 11:59:05] error    : Alert handler failed, retry scheduled for next cycle\n     92 [UTC Feb 21 11:59:05] error    : 'nats' process is not running\n     93 [UTC Feb 21 11:59:05] error    : Cannot open a connection to the mailserver 'localhost:2825' -- Transport endpoint is not connected\n     94 [UTC Feb 21 11:59:05] error    : No mail servers are available\n     95 [UTC Feb 21 11:59:05] info     : 'nats' trying to restart\n     96 [UTC Feb 21 11:59:05] info     : 'nats' start: /var/vcap/jobs/bpm/bin/bpm\n     97 [UTC Feb 21 11:59:36] error    : 'nats' failed to start\n98 [UTC Feb 21 11:59:36] error    : 'postgres' process is not running\n     99 [UTC Feb 21 11:59:37] info     : 'postgres' trying to restart\n    100 [UTC Feb 21 11:59:37] info     : 'postgres' start: /var/vcap/jobs/bpm/bin/bpm\n    101 [UTC Feb 21 12:04:37] error    : 'postgres' failed to start\n    102 [UTC Feb 21 12:04:37] error    : 'blobstore_nginx' process is not running\n    103 [UTC Feb 21 12:04:38] info     : 'blobstore_nginx' trying to restart\n    104 [UTC Feb 21 12:04:38] info     : 'blobstore_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n    105 [UTC Feb 21 12:05:09] error    : 'blobstore_nginx' failed to start\n    106 [UTC Feb 21 12:05:09] error    : 'director' process is not running\n    107 [UTC Feb 21 12:05:10] info     : 'director' trying to restart\n    108 [UTC Feb 21 12:05:10] info     : 'director' start: /var/vcap/jobs/bpm/bin/bpm\n    109 [UTC Feb 21 12:05:40] error    : 'director' failed to start\n    110 [UTC Feb 21 12:05:40] info     : 'worker_1' start: /var/vcap/jobs/director/bin/worker_ctl\n    111 [UTC Feb 21 12:05:42] info     : 'worker_2' start: /var/vcap/jobs/director/bin/worker_ctl\n    112 [UTC Feb 21 12:05:44] info     : 'worker_3' start: /var/vcap/jobs/director/bin/worker_ctl\n    113 [UTC Feb 21 12:05:49] info     : 'worker_4' start: /var/vcap/jobs/director/bin/worker_ctl\n    114 [UTC Feb 21 12:05:55] error    : 'director_scheduler' process is not running\n    115 [UTC Feb 21 12:06:04] info     : 'director_scheduler' trying to restart\n    116 [UTC Feb 21 12:06:04] info     : 'director_scheduler' start: /var/vcap/jobs/bpm/bin/bpm\n    117 [UTC Feb 21 12:06:34] error    : 'director_scheduler' failed to start\n    118 [UTC Feb 21 12:06:40] error    : 'director_sync_dns' process is not running\n    119 [UTC Feb 21 12:06:45] info     : 'director_sync_dns' trying to restart\n    120 [UTC Feb 21 12:06:45] info     : 'director_sync_dns' start: /var/vcap/jobs/bpm/bin/bpm\n    121 [UTC Feb 21 12:07:15] error    : 'director_sync_dns' failed to start\n    122 [UTC Feb 21 12:07:21] error    : 'director_nginx' process is not running\n    123 [UTC Feb 21 12:07:25] info     : 'director_nginx' trying to restart\n    124 [UTC Feb 21 12:07:25] info     : 'director_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n    125 [UTC Feb 21 12:07:55] error    : 'director_nginx' failed to start\n    126 [UTC Feb 21 12:08:02] error    : 'health_monitor' process is not running\n    127 [UTC Feb 21 12:08:07] info     : 'health_monitor' trying to restart\n    128 [UTC Feb 21 12:08:07] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    129 [UTC Feb 21 12:08:11] error    : 'registry' process is not running\n    130 [UTC Feb 21 12:08:20] info     : 'registry' trying to restart\n    131 [UTC Feb 21 12:08:20] info     : 'registry' start: /var/vcap/jobs/bpm/bin/bpm\n    132 [UTC Feb 21 12:08:51] error    : 'registry' failed to start\n    133 [UTC Feb 21 12:09:34] error    : 'nats' process is not running\n    134 [UTC Feb 21 12:09:34] info     : 'nats' trying to restart\n    135 [UTC Feb 21 12:09:34] info     : 'nats' start: /var/vcap/jobs/bpm/bin/bpm\n    136 [UTC Feb 21 12:10:05] error    : 'nats' failed to start\n    137 [UTC Feb 21 12:10:05] error    : 'postgres' process is not running\n    138 [UTC Feb 21 12:10:05] info     : 'postgres' trying to restart\n    139 [UTC Feb 21 12:10:05] info     : 'postgres' start: /var/vcap/jobs/bpm/bin/bpm\n    140 [UTC Feb 21 12:10:34] info     : 'postgres' started\n    141 [UTC Feb 21 12:10:47] error    : 'blobstore_nginx' process is not running\n    142 [UTC Feb 21 12:10:47] info     : 'blobstore_nginx' trying to restart\n    143 [UTC Feb 21 12:10:47] info     : 'blobstore_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n    144 [UTC Feb 21 12:11:17] error    : 'blobstore_nginx' failed to start\n    145 [UTC Feb 21 12:11:17] error    : 'director' process is not running\n146 [UTC Feb 21 12:11:17] info     : 'director' trying to restart\n147 [UTC Feb 21 12:11:17] info     : 'worker_1' stop: /var/vcap/jobs/director/bin/worker_ctl\n    148 [UTC Feb 21 12:11:25] info     : 'worker_2' stop: /var/vcap/jobs/director/bin/worker_ctl\n    149 [UTC Feb 21 12:11:32] info     : 'worker_3' stop: /var/vcap/jobs/director/bin/worker_ctl\n    150 [UTC Feb 21 12:11:37] info     : 'worker_4' stop: /var/vcap/jobs/director/bin/worker_ctl\n    151 [UTC Feb 21 12:11:44] info     : 'director' start: /var/vcap/jobs/bpm/bin/bpm\n    152 [UTC Feb 21 12:12:04] info     : 'director' started\n    153 [UTC Feb 21 12:12:07] info     : 'worker_1' start: /var/vcap/jobs/director/bin/worker_ctl\n    154 [UTC Feb 21 12:12:10] info     : 'worker_2' start: /var/vcap/jobs/director/bin/worker_ctl\n    155 [UTC Feb 21 12:12:13] info     : 'worker_3' start: /var/vcap/jobs/director/bin/worker_ctl\n    156 [UTC Feb 21 12:12:18] info     : 'worker_4' start: /var/vcap/jobs/director/bin/worker_ctl\n    157 [UTC Feb 21 12:12:22] error    : 'director_scheduler' process is not running\n    158 [UTC Feb 21 12:12:22] info     : 'director_scheduler' trying to restart\n    159 [UTC Feb 21 12:12:22] info     : 'director_scheduler' start: /var/vcap/jobs/bpm/bin/bpm\n    160 [UTC Feb 21 12:12:52] info     : 'director_scheduler' started\n    161 [UTC Feb 21 12:13:02] error    : 'director_sync_dns' process is not running\n    162 [UTC Feb 21 12:13:02] info     : 'director_sync_dns' trying to restart\n    163 [UTC Feb 21 12:13:02] info     : 'director_sync_dns' start: /var/vcap/jobs/bpm/bin/bpm\n    164 [UTC Feb 21 12:13:34] error    : 'director_sync_dns' failed to start\n    165 [UTC Feb 21 12:13:34] error    : 'director_nginx' process is not running\n    166 [UTC Feb 21 12:13:34] info     : 'director_nginx' trying to restart\n    167 [UTC Feb 21 12:13:34] info     : 'director_nginx' start: /var/vcap/jobs/bpm/bin/bpm\n    168 [UTC Feb 21 12:14:08] error    : 'director_nginx' failed to start\n    169 [UTC Feb 21 12:14:08] info     : 'health_monitor' process is running with pid 1522\n    170 [UTC Feb 21 12:14:18] error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP\n    171 [UTC Feb 21 12:14:29] info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    172 [UTC Feb 21 12:14:29] error    : 'registry' process is not running\n    173 [UTC Feb 21 12:14:29] info     : 'registry' trying to restart\n    174 [UTC Feb 21 12:14:29] info     : 'registry' start: /var/vcap/jobs/bpm/bin/bpm\n    175 [UTC Feb 21 12:15:01] error    : 'registry' failed to start\n    176 [UTC Feb 21 12:15:13] info     : 'nats' process is running with pid 1595\n    177 [UTC Feb 21 12:15:21] info     : 'postgres' process is running with pid 1653\n    178 [UTC Feb 21 12:15:26] info     : 'blobstore_nginx' process is running with pid 1725\n    179 [UTC Feb 21 12:15:38] info     : 'director' process is running with pid 1875\n    180 [UTC Feb 21 12:15:47] info     : 'director_scheduler' process is running with pid 1972\n    181 [UTC Feb 21 12:15:51] info     : 'director_sync_dns' process is running with pid 2017\n    182 [UTC Feb 21 12:16:02] info     : 'director_nginx' process is running with pid 2061\n    183 [UTC Feb 21 12:16:10] error    : 'health_monitor' process is not running\n    184 [UTC Feb 21 12:16:22] info     : 'health_monitor' trying to restart\n    185 [UTC Feb 21 12:16:22] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    186 [UTC Feb 21 12:16:29] info     : 'registry' process is running with pid 2122\n    187 [UTC Feb 21 12:16:49] info     : 'health_monitor' process is running with pid 2187\n    188 [UTC Feb 21 12:17:03] error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP\n    189 [UTC Feb 21 12:17:03] info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    190 [UTC Feb 21 12:17:15] error    : 'health_monitor' process is not running\n    191 [UTC Feb 21 12:17:24] info     : 'health_monitor' trying to restart\n    192 [UTC Feb 21 12:17:24] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    193 [UTC Feb 21 12:17:43] info     : 'health_monitor' process is running with pid 2215\n    194 [UTC Feb 21 12:17:47] error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP\n    195 [UTC Feb 21 12:17:47] info     : 'health_monitor' exec: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n196 [UTC Feb 21 12:17:59] error    : 'health_monitor' process is not running\n    197 [UTC Feb 21 12:18:06] info     : 'health_monitor' trying to restart\n    198 [UTC Feb 21 12:18:06] info     : 'health_monitor' start: /var/vcap/jobs/health_monitor/bin/health_monitor_ctl\n    199 [UTC Feb 21 12:18:25] info     : 'health_monitor' process is running with pid 2235\n    200 [UTC Feb 21 12:18:29] error    : 'health_monitor' failed, cannot open a connection to INET[localhost:25923/healthz] via TCP. @dpb587-pivotal \nThanks a lot!\nI checked logs in /var/vcap/sys/log/health_monitor/*. There are two logs in /var/vcap/sys/log/health_monitor/. But they are both empty.\nI checked values in /var/vcap/jobs/health_monitor/config/*. There seems to be no problem.\nI used bosh-deployment to deploy Director VM and Bosh on OpenStack.. ",
    "gaurav987654321": "Hi @BeckerMax\nthanks for reply \nI checked into network connectivity now  when i am  trying  a deployment i am getting error (IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590)\nTask 10 | 04:57:39 | Preparing deployment: Preparing deployment (00:00:01)\nTask 10 | 04:57:40 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 10 | 04:57:41 | Creating missing vms: learn-server/a7a46bdf-05a2-413d-b609-b99dbb1ebd71 (0) (00:00:08)\n                   L Error: CPI error 'Bosh::Clouds::CloudError' with message 'OpenStack API Conflict Expected([201]) <=> Actual(409 Conflict)\nexcon.error.response\n  :body          => \"{\\\"NeutronError\\\": {\\\"message\\\": \\\"IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590\\\", \\\"type\\\": \\\"IpAddressAlreadyAllocated\\\", \\\"detail\\\": \\\"\\\"}}\"\n  :cookies       => [\n  ]\n  :headers       => {\n    \"Content-Length\"         => \"168\"\n    \"Content-Type\"           => \"application/json\"\n    \"Date\"                   => \"Wed, 09 Jan 2019 04:57:49 GMT\"\n    \"X-Openstack-Request-Id\" => \"req-e39fc11d-e238-4b0c-bb96-d8bb69fd8158\"\n  }\n  :host          => \"public-c02.cni.de.atos.net\"\n  :local_address => \"10.0.1.10\"\n  :local_port    => 45050\n  :path          => \"/v2.0/ports\"\n  :port          => 13696\n  :reason_phrase => \"Conflict\"\n  :remote_ip     => \"185.60.37.113\"\n  :status        => 409\n  :status_line   => \"HTTP/1.1 409 Conflict\\r\\n\"\n (IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590).\nCheck task debug log for details.' in 'create_vm' CPI method (CPI request ID: 'cpi-254577')\nTask 10 | 04:57:50 | Error: CPI error 'Bosh::Clouds::CloudError' with message 'OpenStack API Conflict Expected([201]) <=> Actual(409 Conflict)\nexcon.error.response\n  :body          => \"{\\\"NeutronError\\\": {\\\"message\\\": \\\"IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590\\\", \\\"type\\\": \\\"IpAddressAlreadyAllocated\\\", \\\"detail\\\": \\\"\\\"}}\"\n  :cookies       => [\n  ]\n  :headers       => {\n    \"Content-Length\"         => \"168\"\n    \"Content-Type\"           => \"application/json\"\n    \"Date\"                   => \"Wed, 09 Jan 2019 04:57:49 GMT\"\n    \"X-Openstack-Request-Id\" => \"req-e39fc11d-e238-4b0c-bb96-d8bb69fd8158\"\n  }\n  :host          => \"public-c02.cni.de.atos.net\"\n  :local_address => \"10.0.1.10\"\n  :local_port    => 45050\n  :path          => \"/v2.0/ports\"\n  :port          => 13696\n  :reason_phrase => \"Conflict\"\n  :remote_ip     => \"185.60.37.113\"\n  :status        => 409\n  :status_line   => \"HTTP/1.1 409 Conflict\\r\\n\"\n (IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590).\nCheck task debug log for details.' in 'create_vm' CPI method (CPI request ID: 'cpi-254577')\nTask 10 Started  Wed Jan  9 04:57:39 UTC 2019\nTask 10 Finished Wed Jan  9 04:57:50 UTC 2019\nTask 10 Duration 00:00:11\nTask 10 error\nUpdating deployment:\n  Expected task '10' to succeed but state is 'error'\nExit code 1\nTask 10 | 04:57:39 | Preparing deployment: Preparing deployment (00:00:01)\nTask 10 | 04:57:40 | Preparing package compilation: Finding packages to compile (00:00:00)\nTask 10 | 04:57:41 | Creating missing vms: learn-server/a7a46bdf-05a2-413d-b609-b99dbb1ebd71 (0) (00:00:08)\n                   L Error: CPI error 'Bosh::Clouds::CloudError' with message 'OpenStack API Conflict Expected([201]) <=> Actual(409 Conflict)\nexcon.error.response\n  :body          => \"{\\\"NeutronError\\\": {\\\"message\\\": \\\"IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590\\\", \\\"type\\\": \\\"IpAddressAlreadyAllocated\\\", \\\"detail\\\": \\\"\\\"}}\"\n  :cookies       => [\n  ]\n  :headers       => {\n    \"Content-Length\"         => \"168\"\n    \"Content-Type\"           => \"application/json\"\n    \"Date\"                   => \"Wed, 09 Jan 2019 04:57:49 GMT\"\n    \"X-Openstack-Request-Id\" => \"req-e39fc11d-e238-4b0c-bb96-d8bb69fd8158\"\n  }\n  :host          => \"public-c02.cni.de.atos.net\"\n  :local_address => \"10.0.1.10\"\n  :local_port    => 45050\n  :path          => \"/v2.0/ports\"\n  :port          => 13696\n  :reason_phrase => \"Conflict\"\n  :remote_ip     => \"185.60.37.113\"\n  :status        => 409\n  :status_line   => \"HTTP/1.1 409 Conflict\\r\\n\"\n (IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590).\nCheck task debug log for details.' in 'create_vm' CPI method (CPI request ID: 'cpi-254577')\nTask 10 | 04:57:50 | Error: CPI error 'Bosh::Clouds::CloudError' with message 'OpenStack API Conflict Expected([201]) <=> Actual(409 Conflict)\nexcon.error.response\n  :body          => \"{\\\"NeutronError\\\": {\\\"message\\\": \\\"IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590\\\", \\\"type\\\": \\\"IpAddressAlreadyAllocated\\\", \\\"detail\\\": \\\"\\\"}}\"\n  :cookies       => [\n  ]\n  :headers       => {\n    \"Content-Length\"         => \"168\"\n    \"Content-Type\"           => \"application/json\"\n    \"Date\"                   => \"Wed, 09 Jan 2019 04:57:49 GMT\"\n    \"X-Openstack-Request-Id\" => \"req-e39fc11d-e238-4b0c-bb96-d8bb69fd8158\"\n  }\n  :host          => \"public-c02.cni.de.atos.net\"\n  :local_address => \"10.0.1.10\"\n  :local_port    => 45050\n  :path          => \"/v2.0/ports\"\n  :port          => 13696\n  :reason_phrase => \"Conflict\"\n  :remote_ip     => \"185.60.37.113\"\n  :status        => 409\n  :status_line   => \"HTTP/1.1 409 Conflict\\r\\n\"\n (IP address 10.2.1.2 already allocated in subnet 24b9eb5b-5aeb-4fa0-9ad3-cae7d2593590).\nCheck task debug log for details.' in 'create_vm' CPI method (CPI request ID: 'cpi-254577')\nTask 10 Started  Wed Jan  9 04:57:39 UTC 2019\nTask 10 Finished Wed Jan  9 04:57:50 UTC 2019\nTask 10 Duration 00:00:11\nTask 10 error\nUpdating deployment:\n  Expected task '10' to succeed but state is 'error'\nExit code 1\n. ",
    "gmrodgers": "Hi @dpb587-pivotal \nHow do you see BBR users dealing with configuring database backups?\nNot quite sure about this question.  The current external database backups produced by the restore script in this version are invalid and will restore the director to an unhealthy state when used. We would like to remove this attempt to backup external databases as it was not promised by bbr until the BBR SDK was introduced in BOSH v267.2.  External databases would have to be backed up outside of bbr.\nShould they need to know to differentiate between internal and external databases?\nYes as the internal database is backed up using bbr but the external was never backed up for this version we are PRing into.\nShould they be expected to add bbr-database-X backup jobs if they're using an external database? How does that get communicated?\nThe external database backup feature was never promised or communicated to the consumer of the BOSH release until the usage of the BBR SDK release in BOSH v267.2.  If we were to communicate this, we believe something similar to this matrix would be useful.  We believe that they would have to backup the external database outside of using bbr and therefore not create a new bbr-database-X job\nShould the backup/restore functionality instead be removed from director and into the database-specific jobs which provide the local postgres database?\nNot sure about this question either with regards to the PR.  This is up to the BOSH team as bbr will run the backup and restore scripts if they are found in the bin/bbr/ directory of any job so the functionality should be similar if you decide to implement this.  The fix would be good regardless we are giving a false impression to the consumers of BOSH that we back up external databases for this version.\nShould the current director backup/restore functionality be fixed to work for both local and remote databases?\nNo as this is not a fix for backing up external databases but a fix to stop it looking like we back up external databases.\nThanks Glen && @MModhaPivotal. ",
    "alamages": "Hello @dpb587-pivotal, is there any update on this?. ",
    "charleshansen": "@voelzmo thanks for the PR, it seems like a good idea and we ended up just re-implementing it after thinking about it for a bit 6a09e3cd61852c2ca595e8a1ed86934865192411. It is basically the same as your PR. We verified that it now has fewer calls to the database on a small deployment, but did not try anything against 1600 VMs like you were talking about.\nLet us know if this looks good to you and if this improves your performance. Thanks!\n--\n@charleshansen @dpb587-pivotal \n. Looking through this issue myself for the first time. It looks like the core issue is that there is no way to clean up old compiled packages from the blobstore. I'd vote for bosh clean-up --all having logic to delete blobs that don't match major versions. I'm more skeptical of directly linking the cleanup to deleting a stemcell. Do you also expect bosh delete-stemcell to trigger this cleanup behavior?. Not fully understanding the situation all the way, It looks to me like you want to use eager here \nhttps://sequel.jeremyevans.net/rdoc/classes/Sequel/Model/Associations/DatasetMethods.html#method-i-eager\n(We it for similar reasons in some complicated lookups in cloud controller, like https://github.com/cloudfoundry/cloud_controller_ng/blob/63fa0c5815ab536aaa2cb5baed13452ca14ced6c/lib/cloud_controller/copilot/sync.rb#L70)\nI'll try to take a closer look at the data model here if you think this is a useful direction.. I looked into the query with @xtreme-behrouz-soroushian \nThe existing query is more complicated than it needs to be, doing sql calculations across multiple tables. It should actually be both faster and simpler to do the filtering in memory instead of using sql queries. Here is our crack at it (all the unit tests pass)\n```\n    def filter_instances(vm_cid_to_exclude)\n      instances_from_db = Models::Instance\n        .where(compilation: false)\n        .eager(:vms)\n        .all\n  instances_from_db.select do |instance|\n    instance.vms.any? do |vm|\n      vm.active && vm.cid != vm_cid_to_exclude\n    end\n  end\nend\u0000\n\n```\nOne complicating factor: instance.agent_id goes through instance.active_vm which appears to go around anything we could do with eager loading. If active_vm used (self.)vms.find instead of Vm.first, it would work, but it has slightly different caching behavior and I'm not sure if this is a safe change to make.\n. We looked at it a little more, and tried to fix the active_vm reloading behavior (https://github.com/cloudfoundry/bosh/commit/e53000862b4df58c7e7442125be202a967780f83), but this breaks a number of integration tests.\nMy recommendation at this point is to directly use instance.vms.find(&:active).agent_id instead of instance.agent_id if you need caching behavior. It might be worth it to add an instance method to Models::Instance, something like cached_agent_id or cached_active_vm. If this doesn't work, let me know.. On master, we changed filter_instances to the function we wrote above and the unit tests all passed. We got the query slightly wrong originally and the tests caught it, so I'm pretty sure this is ultimately the same function. I personally find it much easier to understand without all the extra sql logic.\nAs far as validating \"faster\", we didn't run any performance tests or anything, it was just more of a gut feeling. I've found in optimizing other queries that Ruby in memory is normally just as fast as sql unless you are using really large tables (~million rows) or if you are searching for something and utilizing database indexing. We aren't indexing on the \"active\" field in the vms, and I've never heard of anything approaching a million VMS in a bosh deployment, so it seems safe to me. \nThat said, If you have a performance environment set up, I'd like to know how it fares. You would still need to stop using instance.agent_id so that we don't hit the database for every instance. I didn't try running it, so I'm not 100% sure this will fix it.. ",
    "jontours": "I've discovered the same behavior while migrating CF secrets to CredHub.  . @MattSurabian did you find a way to ensure this mode parameter gets set? . ",
    "degaurab": "/cc @charleshansen \nHi @langered\nSeems like the issue might be fixed on bosh-agent (master) and it might be in the latest 250.x line of stemcells.\nreference bosh-agent commit : https://github.com/cloudfoundry/bosh-agent/commit/bc536e3ee481281a992942eb4f24ca173691f0bb\nSo can you use the 250.x line and let us know if the issue persists.\nThanks\n. /cc @charleshansen \nHi @tvs \nWe think this issue is fixed in latest version of BOSH director. Based on this commit: https://github.com/cloudfoundry/bosh/commit/444de6a2bed256adee98df0695151698a6023378\nSo please try using 268.6+ release and let us know if the issue persists.\nThanks.. Hi @pivotal-jamil-shamy\nCan you share job logs related to the failure. As most of the authentication/authorization is done using UAA it might be causing failure upstream. Also if DB is not up before UAA start talking to it and other component will cause major failures.\nThanks. replacing cancel_sync_dns with direct rpc call kind of confusing as it don't explain what we are actually canceling until you look around the code. For maintainability aspect the code it look more cleaner approach, but kind of hiding the intent as the rpc method is very generic (as the agent method is just a wrapper around the rpc call). Just a view... . ",
    "jdesulme": "Hey @belinda-liu - I think this is still an issue. However, In my case instead of using the bosh director I had tried to deploy this instance type using a deployment manifest and it failed with a similar error. My original goal was for a T3 instance but I noticed this request as well. So I opened #2135 to investigate since it's a similar type of error. . Hey @belinda-liu - I think this is still an issue. However, In my case instead of using the bosh director I had tried to deploy this instance type using a deployment manifest and it failed with a similar error. My original goal was for a T3 instance but I noticed this request as well. So I opened #2135 to investigate since it's a similar type of error. . ",
    "langered": "Hey @belinda-liu && @xtreme-behrouz-soroushian,\nwe did not try the fix yet but we will do anytime soon.\nWe'll provide feedback here as soon as we have tried it out.\nThanks,\n@langered . ",
    "aad": "close as we have to destroy the test env. ",
    "h0nIg": "one additional expected behaviour: the old producer is resolvable as long as the new one has not been deployed. ",
    "cghsystems": "It ensures that the ''config/templates'\u2019 folder is imported in to the gem. Maybe a single config/* entry would suffice. Adding config/templates didn't seem to work. I will look into it. \n. That can easily be done. I was trying to gauge whether anybody out there saw this feature as useful before I added much more. What do you recon, should I go ahead and add it now? \n. Makes sense once I add more templates. Shall I go ahead and add the other cpis now? I was trying to keep the first commit light but easily add in this feature if there is the demand. \n. I think this makes sensed for the current functionality. I.e only one type of template managed. Once we add more types then this would need to be more carefully handled. Probably do a check at the CPI level of directories. \n. Well I would use this and I'm in the mood partly because of client feedback.  Hopefully that will help persuade the powers that be!! I'll go ahead and add the functionality discussed then. \n. ",
    "cf-frontend": "should expect that returned value matches (e.g. ...list_locks.should == parsed_response)\n. should expect on the actual content of the response\n. ",
    "ipoddar-ibm": "Hello cppforlife, I have added a commit to enable bosh micro and also addressed your comments for removing the parens according to ruby style. It seems that the Travis build is failing with an error from the go agent unit tests. I haven't touched the go agent in this commit, so I don't understand why the unit tests are failing. Can you please review? thanks I.P\n. ",
    "DennisDenuto": "let's avoid pulling down existing value of the column when updating it. Instead we should be able to use sequel concat). Should the test name also mention result? Also the new result output behavior of persisting to the db is not being tested.. ",
    "dalewick": "I wouldn't use an inner_join, since ruby sequel makes a magic foreign key variable called: instance.vms: https://github.com/cloudfoundry/bosh/blob/master/src/bosh-director/lib/bosh/director/models/instance.rb#L9\nIf you have a given instance, you can just say: instance.vms.each to iterate through the matching vms for the instance, or instance.vms.all for an array of the vms.  And it should be super quick.  Alternately maybe an index would make the query resolve quicker if that isn't an option.\nFrom the snippet I can't tell if that would help or not though.. It seems unusual to attempt to remove the subnet, and then if it fails, remove it from the bosh database, and never retry deleting it.  \nMaybe it shouldn't destroy it from the bosh database in the failure state, so that the next time it does a cleanup, bosh can try again.. It seems unusual to attempt to remove the subnet, and then if it fails, remove it from the bosh database, and never retry deleting it.  \nMaybe it shouldn't destroy it from the bosh database in the failure state, so that the next time it does a cleanup, bosh can try again.. Does the CPI result give a hint of whether the IaaS isn't responding versus the subnet is already deleted?  Maybe that could be the difference of whether the subnet db model should be purged or not.. "
}
{
    "burr86": "Yes please! Makes this useful for Apache log files too. :)\n. Oops, sorry, I didn't see this note from you five months ago... here's some sample access logs where the fields are delimited by whitespace, except for the fields that are quoted:\nhttps://www.dropbox.com/s/5r6mqpgwmli7fmb/accesslog.txt\n. ",
    "harelba": "Hi @burr86 - can you send me an small extract of your data so i can develop it on \"live data\" and not on artificial data? even 10-20 rows would be enough.\n. @burr86 - Can you send me a small amount of live data so i can work with it?\n. Thanks a lot, that's great.\n. Planning on working on it in the coming days. Will update.\n. Finished. Will be committed soon.\n. Done. The current version, 1.2.0, supports quoted fields (default behavior)\nI'll close the bug once I see that people are using it without problems.\n. Quoted CSVs are now supported. Closing. If there's an issue with it, please open a more specific bug.\n. RPM Packaging is finished (currently created using a script called create-rpm). Still need to provide some hosting mechanism for the RPMs. Will update.\n. RPM Packaging is in beta. See README for download link.\n. didn't mean to close it. Can it be reopened or should i open a new issue?\n. You're absolutely right... The original idea was to skip header lines which contain field names and such. I'll fix that soon.\nthanks for the input.\n. Fixed and pushed changes - Thanks @fuckinghuarong \n. Hi, yes, you're right it is a bug. I'll try to fix it as soon as possible.\nBtw, which os are you using?\nOn Sep 24, 2013 6:47 PM, \"Thomas Riccardi\" notifications@github.com wrote:\n\nI haven't found a way to use files with spaces in their name: quoting with\n\", ', or ` doesn't work; escaping spaces with a backslash doesn't work\neither.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/issues/4\n.\n. @kc14 yeah, filenames with spaces are currently a limitation of q. I have plans to have a solution for it for a long time now, but haven't gotten to it.. That would be great! Thanks for helping with this. I'll create a tag as\nsoon as I am near my computer.\n On Oct 11, 2013 10:04 PM, \"Stuart Carnie\" notifications@github.com wrote:\nThe issue is they won't include it unless it comes for the official repo.\nSee here: mxcl/homebrew#23192https://github.com/mxcl/homebrew/pull/23192\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/issues/5\n.\n. Hi Stuart,\n\nI've just added the tag \"1.0\" with the first official release. I'm not sure what homebrew expects/requires, so if this is not good enough and you need me to do anything else, i'd be glad to do it, just drop me a line.\n. Hi, thanks for helping with this, i was also working on a good solution for this. I tried your code and it fails for the simple use-case of q \"select ... from 'filename' \". Also it doesn't fix the special \")\" case that comes afterwards. I'm still taking a look at your changes, maybe only a small change is required.\n. yeah, but it needs to be consistent regardless of the content of the table name. For example, if someone has a command line which has the table name as a parameter, not knowing if the parameter will contain spaces: q \"select c1 from '$1' where ...\". I my view, it needs to work smoothly. What do you think?\n. No proper solution for this for now. In the mean time, a possible workaround can be to pipe the output of the file to q. For example: cat \"my file with spaces\" | q \"select * from -\". I'll update once I have a consistent solution for this.\n. I agree. This is something I've started to deal with (related to full parsing of the sql). \nNow that version 1.3.0 has gone out with lots of additions such as column name/type detection, my plan is to solve this problem as well. Will update on twitter and in github when I do.\nSorry for the temporary inconvenience\nHarel\n. There's a beta version ready which provides a relaxed parsing mode will allow specifying the number of columns in the command line, combining all the rest of the data in each row to the last column. \nThe beta version will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\nUse relaxed mode and -c X in order to make all extra data on the line merge into column cX.\n. Added in version 1.3.0. Use -c <column-count> in order to specify the column count. Extra data in each row will be added to the last column.\nMake sure to use relaxed mode and not strict mode (relaxed mode is the default).\n. Closing - #39 describes things better. \nFeature will be completed soon.\n. Beta version with this feature will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\nNote that this is a breaking change, since now -H is a flag and not a parameter. e.g. there is no \"-H 3\" or anything like that. Just -H or no -H.\nIt was stupid to allow multiple header rows in the first place... :-) I believe no one really used it.\n. Added new feature in version 1.3.0.\nUse -H in order to specify that there is a header row. Column names will be determined automatically from the header row. \nIf you want to see the column name/type analysis results, run q with -A.\n. Working on it. Will be released soon.\n. Beta version with this feature will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Added new feature in version 1.3.0. Happens automatically during the reading of the input. If you want to see the column name/type analysis results, run q with -A\n. Exactly :)\nHi, sorry for the late reply. Been offline for a couple of days.\nThanks a lot, I'll take a deeper look at your tip and see if I can find some trick to make the invalidation fast enough (was planning on cksum, perhaps a sampled cksum or something, with an option to be stricter and slower through a command line parameter).\nHarel\n. I've created an API which will allow q to be used from python code as a module. The changes also inherently include the possibility to reuse previously loaded data (e.g. running multiple queries against the same loaded data).\nAlpha version of the new API will be committed into the main branch in a couple of days.\n. Alpha branch of the python api has been committed to https://github.com/harelba/q/tree/expose-as-python-api. \nThe python api supports reuse of already-loaded data, and this capability is exposed to the command line by allowing the user to write multiple queries in the same q execution - E.g. q \"select ...\" \"select ...\" \"select ...\" .... Running q like that will load the data only once for each file, even if it's used in multiple queries. In the future, I'll probably add an interactive REPL for this as well.\nAny input would be helpful and appreciated.\nHarel\n. Forgot to write - The readme file of the branch contains the required information about the API.\n. This capability is now fully supported internally, and exposed partially by running multiple queries on the same command line (Every invocation of q reuses the data between multiple queries that are being run).\nThis issue will be closed when the feature is fully exposed.\n. One example of a limitation that the tool currently has due to not having real sql parsing is limited support for sub queries.\n. thanks ritchie, i'll give it a look - looks promising!\n. Two of the most common error cases have been handled to provide clearer reporting. Additional problematic error reports will have their own issue opened.\n. Sorry for the late reploy. Definitely no reason not to do that, you're right, although i'm not sure what is required for this to happen. I'll start digging and send updates.\n. I'm doing this as part of the bigger refactoring process for q's code, separating between \"qtextasdata\" python module, and a command line frontend which will be called q.\nThis will allow installing and reusing the python module using pip, separately from the command line.\nHarel\n. Hi @webmaven I'm not sure it will be available for the upcoming 1.5.0 version, but it's planned for the next one, 1.6.0.\nIf i manage to squeeze it into 1.5.0 (due in a couple of weeks, i believe), I will update here.\n. I've created a python API so q can be used as a module from python code. I will commit an alpha version of it to the master branch in a couple of days.\nOnce the API is ok, I'll add the pip related stuff.\n. Alpha branch of the python api has been committed to https://github.com/harelba/q/tree/expose-as-python-api. \nIf you have any ideas on how to make this module a full fledged pypi module without breaking any existing q installation code, that would be great.\nAny input would be helpful and appreciated.\nHarel\n. Forgot to write - The readme file of the branch contains the required information about the API.\n. That's what probably I'll do eventually if there is no other option, but that's exactly what I wanted to prevent - Doing that would require fixing all the installation code for homebrew/rpm/debian/arch, and I would really like to find a good way to work around doing that, since it will postpone the time the API support will be released.\n. Closing this issue and merging it with https://github.com/harelba/q/issues/24 so the dicussions will happen only in one location.\n. Forgot to close it - Duplicate of #24 \n. Thanks for pointing it out! It's a bug in the ls flags i've provided. fixing now.\n. Fixed. The ls command should include the -d flag: \n$ ls -ltrd * | q \"select c1,count(1) from - group by c1\"\nNot using the -d flag causes the ls output to descend to directories and be non-tabular in nature. Besides, I need to make error display better (already have a ticket on this).\nI've just pushed the fix, but if you're using homebrew in order to download it, then you'll still be getting the old version. I'll bump the version numbers in the homebrew formula.\n. homebrew formula updated to version 1.1, which includes this fix - Will update when homebrew will integrate the pull request.\n. Pull request for homebrew formula has been merged. q v1.1 will now be installed when using homebrew, fixing the issue.\n@bmayer0122 Thanks for noting this.\n. fixed.\n. fix is part of version 1.1.1\n. Thanks a lot for that fix - It's definitely a needed one.\n. As for the other fixes, i'd be more than happy to help and fix them or integrate your pull requests when you send them.\nAny other comments and general impressions regarding the use of the tool will be greatly appreciated.\nThanks again\nHarel\n. btw - i'm bumping up the homebrew formula version, so the fix will be there soon, if you're using mac.\n. Hi JKrag - Thanks a lot for the comments and insights.  Let's continue this discussion in email and see what can be done.\n. Merged it, with some minor changes such as changing the output delimiter flag to -D (so it's similar to the input delimiter -d) + some added comments.\nI'd appreciate if you can make future pull request on a separate branch.\nThanks!\n. Feature has been added to version 1.1.4, and homebrew updated to install this version.\n. Looks like a nice addition, although I'm not sure about one thing - If it's an sqlite version with the regexp operator installed, then does it really need this addition?\nAnyway, it looks nice and I'll add it once i have a few minutes to bump the homebrew version along with it.\nHarel\n. btw - I'm usually mixing q and grep in such cases, but I can see the benefit of having it inside q, in order to easily limit the regexp matching to one field.\n. I see... Great, thanks for the info. I'll merge the pull request soon. Really appreciate it\n. Merged the pull request and send a pull request to homebrew to bump up q's version.\nThanks!\n. done. thanks.\nOn Mon, Jan 13, 2014 at 6:38 PM, Nick Beeuwsaert\nnotifications@github.comwrote:\n\nSQLite3 defines a REGEXP operator, but it doesn't provide a function for\nit, so you can do Y REGEXP X and sqlite will try and execute the\nundefined function regexp(X,Y)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/pull/19#issuecomment-32224473\n.\n. Checked it out, it's a bug, thanks. Going to integrate the fix into the next release. In the mean time, you can add a WHERE clause which says columnX is not null, and it will prevent it from happening.\n\nSorry for the temporary inconvenience.\nHarel\n. Going to remove the .qrc file for the coming version.\n. Thanks! that's nice. I'll fix the readme.\n. Hi, \nThanks for noting that. I'll merge and create a new version.\n. Acked. \nAdding a license file soon.\nThanks.\n. btw - Can you tell me where did you hear about q?\n. Added LICENSE file to source control. License is GPL.\n. I'm hardly a license expert. The main reason beyond choosing GPL was that I designed q to be a Linux command line tool, and as such, I thought that it would be best to use the same license as other command line tools available. \nI'm not sure this is a good reason though. I debated this with myself a lot, since I wanted everyone who needs it to use it. I would appreciate any insights you have regarding this choice and the choice of a license for q in general.\nThanks for the input\nHarel\n. Acked. I have plans for it in the near future. \nThanks\n. I see there is high demand for it :)\nI'll try to make room for it so it wlil go into the upcoming 1.5.0 version (due in a couple of weeks).\nI'll update here.\nHarel\n. I've created an API which will allow working with q from inside python programs. I'll upload an alpha version to the master branch in the coming day or two. \nHopefully, it'll be good enough to be released as part of the upcoming 1.5.0 version. Once it is good enough, I'll also finish the job of making qtextasdata a proper pypi module so it can be installed through pip.\nAny comments regarding it will be most appreciated.\n. Alpha branch of the python api has been committed to https://github.com/harelba/q/tree/expose-as-python-api. \nHopefully, I'll find a good way to make this a proper pypi package without needing to replace all the installation code. If I manage to do that, then this addition will come out in the coming 1.5.0 version.\nAny input would be helpful and appreciated.\nHarel\n. Forgot to write - The readme file of the branch contains the required information about the API.\n. Writing the same response here, for other people as well.\nThat's what probably I'll do eventually if there is no other option, but that's exactly what I wanted to prevent - Doing that would require fixing all the installation code for homebrew/rpm/debian/arch, and I would really like to find a good way to work around doing that, since it will postpone the time the API support will be released.\n. Let's decide that all the discussions regarding this subject will continue here on this issue's comments.\n@webmaven , this might be a good idea regardless of the issue at hand, but it won't prevent the need to rewrite all installations. \nOne of the complications in it being a python module is that you need pip to be installed on the user's machine. Another idea would be to have the module code reside in some private directory which only q (the command line interface) would know about.\nI'm starting to think that rewriting the installations would be a must. In that case, I would prefer to try again to provided a binary compiled version of q. However, when I tried it in the past it \"almost worked\". Everything worked well, but there were some GLIBC versioning issues with old Linux machines.\nIf you have experience with compiling python for old linux machines using PyInstalled, any information about it would be great.\nHarel\n. Maybe I'm missing a part of your point, @webmaven, but there's another implicit requirement for this - The need not to duplicate any code. I don't want the executable \"q\" to contain the same code of the qtextasdata python module. Does what you're suggesting cover this?\n. Thanks, I believe I see your point. I'll take a look at it.\n. As part of the 1.5.0 release that I've released today, the q command line now uses a full-blown python module and API behind the scenes. \nExposing it as a PyPI python module will be a separate phase, after reviews by the community of the new API.\nYou can take a look at the python API here. Any comments would be most appreciated.\n. Hi - Good catch :) Thanks.\n. Hi Conrad, \nThanks for all your RPM related issues. I'll fix all of them. \nHarel\n. Fixed in version 1.3.0. Closing.\n. Fixed in version 1.3.0. Closing.\n. Fixed in version 1.3.0. Closing.\n. Have some questions regarding this, so it's not changed yet. Will ask offline and post results here.\n. @cemeyer A question about this - When using a local tarball, I'm currently running rpmbuild --ta <tarname> in order to create the rpm. When I change Source0 to the official tarball (the github archive of the relevant tag, which is far better than using a local tarball, I agree), how shoul I run rpmbuild in order to properly create the RPM?\n. thanks\n. Seems that rpmbuild is not able to download the sources on its own.\nI'm changing the create-rpm script to download it manually using the version tag, and generating the necessary files before creating the rpm.\nWill be part of version 1.4.0\n. Hi,\nCan you please send an example in which it doesn't work?\nI've tried the following and it seems to return the correct answers:\nharel@harel-laptop:~/dev/github/q$ ./q -H 0 \"select * from exampledatafile\" | wc -l\n248\nharel@harel-laptop:~/dev/github/q$ ./q -H 1 \"select * from exampledatafile\" | wc -l\n247\nharel@harel-laptop:~/dev/github/q$ ./q -H 2 \"select * from exampledatafile\" | wc -l\n246\n. Oh, yes, you're right. It's a bug in the -t parameter. I'll fix that in the next revision. In the mean time, you can just use -d $'\\t' instead of -t to get the tab delimition (The need for the $ before the '\\t' is some linux quirk that was the reason i created the -t parameter in the first place). If you combine -d and -H they will both work. Only the -t \"shortcut\" is broken.\n. Beta version providing column name detection through the header will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Fixed in 1.3.0. Closing.\n. Hi, I'm definitely planning on doing it. However, there are some more pressing issues which i'm already working on (such as column-type inference, header name detection, some bugs etc.). They're almost done, and i'll do the PEP8-ing right after that, as part of a major code refactoring that i'm planning.\nThanks a lot for the help and the inputs.\nHarel\n. Done as part of 1.4.0. Closing.\n. Hi, thanks for the issue, and for the gist with the data. The code currently does have a hacky quick-fix for this kind of cases, since it's a very common one. So, it's odd that you get this error.\nI've downloaded the gist and run the following: q \"select * from gistfile1.txt\", and got the data below (no error), which seems to be the correct result. I would like to investigate this further, so we can understand the root cause. Can you please send the command you run? Can you please check that the gist contains exactly the input to q? I'm currently working on a real solution for this, which will either allow to set the number of columns manually, taking the \"slack\" afterwards as one big column (useful for log files and for ls). I would like to know that the solution would work on your example use-case, so any information would be great.\n-rw-r--r-- 1 duane staff 6938 Sep 13 14:29 001.create_books.sql       \ndrwx------+ 54 duane staff 1836 Jan 29 09:30 Desktop       \ndrwx------+ 17 duane staff 578 Jan 28 16:03 Documents       \ndrwx------+ 65 duane staff 2210 Feb 21 14:15 Downloads       \ndrwx------@ 17 duane staff 578 Feb 21 15:31 Dropbox       \ndrwx------@ 61 duane staff 2074 Dec 3 09:21 Library       \ndrwx------+ 3 duane staff 102 Jan 30 2013 Movies       \ndrwx------+ 4 duane staff 136 Jan 30 2013 Music       \ndrwx------+ 6 duane staff 204 Aug 9 2013 Pictures       \nlrwxr-xr-x 1 duane staff 29 Oct 15 2012 Projects -> /Users/duane/Dropbox/Projects     \ndrwxr-xr-x+ 4 duane staff 136 Jan 30 2013 Public       \ndrwxr-xr-x 4 duane staff 136 Oct 2 2012 Scripts       \ndrwxr-xr-x+ 3 duane staff 102 Jan 30 2013 Sites       \ndrwx------ 3 duane staff 102 Nov 5 10:06 VirtualBox VMs\n. Hi Jan,\nI totally agree, I'm currently working on that exact area in the code, in relation to type/name inference, and as you possibly saw, there are additional issues opened exactly related to this. \nI completely agree with the three modes that you've described, and they fit exactly to what I've started doing in that area. So this is the direction I'm going to push towards. Will update when it's done.\nThanks a lot for the input and all the suggestions.\nHarel\n. beta version which handles this issue by providing strict/relaxed mode and specifying column count in the command line will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\nUse -m relaxed for relaxed mode and -c X in order to limit the number of columns. All data beyond X columns will be part of column cX.\n. Fixed in 1.3.0. Use -m relaxed along with -c X in order to limit the number of columns. Any extra columns will be merged into the last column.\nClosing.\n. Hi, thanks.\nNo, the project is not the same. There are similarities, but q supports full sql queries, including joins across files, and in general multiple files (for example, you can write q \"select ... from file1+file2+file3 ...\", or q \"select ... from file1 t1 join file2 t2 on ...). q also fully supports any file encoding, both in input and in output (not sure about textql on this subject, though).\ntextql seems to be faster than q, which is to be expected since it's using a compiled language, but q can provide more than adequate speed for most use cases people have been talking to me about.\ntextql is a very nice project. I'm preparing a full comparison between the two, which i'll publish once it's finished.\n. I'd have to say that I have not saved any of the results of the comparison i did a long while ago.\nPerhaps I'll do another comparison at some point and publish it.. Thanks. Will merge it to the next version.\n. Hi, you're obviously right. This is a leftover from the old engine (which I changed to v2 in version 1.2.0). I'm in the process of fixing it now. Will update as soon as it's fixed.\n. beta version which fixes this issue will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Thanks both for the input. It's really important to me and i'm glad that you find the time to provide it.\nI'm currently in the middle of adding column type inference (almost finished), which will allow to take empty int/float values and treat them as NULLs when inserting to the db. I think this will solve this issue.\nFor string types, I might add a configuration option to decide whether you want empty values as nulls or as empties.\nSorry for the inconvenience, i'll try to fix all this as soon as I can.\nHarel\n. beta version which handles this issue will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Two things - on the incorrect line, you're obviously right. Another bug related to the move from engine v1 to v2. I'll be fixing that quick and update soon.\nAbout the extra ; part, like I said in the other bug you opened. It's in the works and will soon be fixed.\nThanks, and sorry for the inconvenience.\n. Hi, I have a change which includes type inference, fixes the extra delimiters at the end, and solves the bug reported here. Also, it provides a cleaner \"parsing mode\" parameter, as described in #33, which provides for a cleaner logic.\nI'm still testing it to make sure everything works fine, but I would be glad if you're interested in doing some more sanity testing when I finish testing it myself before i actually release it.\nHarel\n. beta version with fixes for this will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. @alexprengere Beta version 1.3.0b is out - Would you mind giving it some testing when you have some time?\nThanks a lot for your help.\nHarel\n. @alexprengere thanks a lot for all the testing you've done.\nAbout that bug - you're right. It's related to the fact that the separator is a space (or more correctly a whitespace character). Take a look at this:\nharel@harel-laptop:~/dev/github/q$ cat csv-with-spaces\na 1 0\nb 2 0\nc  0\nharel@harel-laptop:~/dev/github/q$ cat csv-with-spaces | ./q \"select * from -\" -D ';'\na;1;0\nb;2;0\nc;0;\nharel@harel-laptop:~/dev/github/q$ cat csv-with-commas \na,1,0\nb,2,0\nc,,0\nharel@harel-laptop:~/dev/github/q$ cat csv-with-commas | ./q \"select * from -\" -D ';' -d ','\na;1;0\nb;2;0\nc;;0\nWhen the separator is a whitespace, it causes the problem you describe. When the separator is a comma (or any non-whitespace char for that matter), it works properly.\nThe reason for that is that q is currently configured to skip initial whitespace at the beginning of the values (part of the csv dialect used). When the separator is a whitespace such as a space, then it gets confused and treats this wrongly.\nTwo possible solutions I see:\n- When the separator is a whitespace, turn off the \"skip initial whitespace\" flag.\n- Allow the user to set this flag using a command line parameter.\nI'm currently thinking about combining both solutions (e.g. default the flag to True, unless the delimiter is whitespace, and allow the user to override it in a command line parameter).\nIf you have any more ideas and comments regarding this, i'd appreciate it.\nHarel\n. by the way, the reason for setting the skip initial whitespace flag to true was to support simple use cases such as ls -l, in which the extra whitespace is redundant.\nPerhaps, treat it differently when using strict mode vs. relaxed mode?\n. Tried to change the logic to something like this:\nharel@harel-laptop:~/dev/github/q$ cat csv-with-spaces | ./q \"select * from -\" -D ';'\na;1;0\nb;2;0\nc;0;\nharel@harel-laptop:~/dev/github/q$ cat csv-with-spaces | ./q \"select * from -\" -D ';' -m strict\na;1;0\nb;2;0\nc;;0\nIn relaxed mode, it'll still give you the same problem as before. But in strict mode, it'll work properly.\nSince relaxed mode is currently the default, I'm not sure that it's the most intuitive behavior, but actually it depends on the person's use-case. \nAny comments would be helpful.\n. yeah, i agree. Adding the flag to the coming 1.3.0 release. Thanks for the comments.\nHarel\n. Added -k parameter for keeping leading whitespace in values. Will handle trailing whitespace separately afterwards, as current behavior is to keep trailing whitespace.\n. Fixed in 1.3.0. Use -k in order to keep leading whitespace. Not setting this as the default, in order to preserve current (and simple) behavior. Trailing spaces will be handled on a separate feature.\nClosing as fixed.\n. Yes, i'll remove #8 and leave this one open. I'm in the middle of writing it as part of the column type inference.\nWill update when it's done.\n. beta version with support for this will be released later on today.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Alpha branch of the python api has been committed to https://github.com/harelba/q/tree/expose-as-python-api.\nMajor code cleaning and refactoring has been done in the process of creating this API.\nAny input would be helpful and appreciated.\nHarel\n. Done as part of the newly released 1.5.0 version.\n. Hi @petrux. Thanks for wanting to help.\nI completely agree. I'm in a tight two weeks stretch at work, so things have slowed down a bit. Will merge it at the end of the week and then you'll be able to do your thing.\nCan you maybe provide some information about your plans regarding the folder-structure change? So we'll see that we're in sync before you spent any significant time?\n. Sorry for the delay, I'm in a long vacation traveling, and without a real\ncomputer connection most of the time.\nI'll be in touch as soon as I get back.\nThanks\nOn Mar 28, 2014 2:14 AM, \"petrux\" notifications@github.com wrote:\n\nHi there. Have you read my proposal? Any news?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/issues/41#issuecomment-38840776\n.\n. Done as part of version 1.4.0, which just got out. Closing.\n. Hi, I have some new things in testing which might be able to solve most of these issue.\n. Thanks, I'll integrate it into the next version.\n. Beta version 1.3.0b released with this fix. Will close when the version moves out of beta.\n. Fixed in 1.3.0\n. You're right. Thanks for the ticket.\n\nI've fixed that and it will be added to the coming 1.3.0 release.\nPlease note that you'll have to use back-ticks in order to reference the column names themselves in the query (sqlite style). See below. \nharel@harel-laptop:~/dev/github/q$ ./q -H -t 'select a,`b c` from -' < test.tsv \n1   2\n. Fixed in 1.3.0.\n. Thanks, will fix.\n. fixed in 1.4.0\n. Using the replace function, along with -c 1 achieves that easily, and this seems to be relatively rare, so there is no need to support it specifically.\nHere is an example for comma separated lines:\nq -d ',' \"select replace(c1,',','') from myfile\" -c 1\nIn order to eliminate tab separated lines, you need to use the more elaborate sqlite syntax for writing a tab (X'09' needs to be used in order to signify a tab literal):\nq -t \"select replace(c1,X'09','') from -\" -c 1\n. Fixed in 1.4.0. Closing.\n. @davewbrown Sorry for the late reply. In a stretching couple of weeks at work, so things here go a bit more slowly than usual. \nI really thank you for your time invested in doing this. \nI'm planning on pulling this change as soon as I have some time to go over it and fix conflicts (Need to take a deeper look at it, since it's not an automatic merge and I'm not familiar with PEP8 stuff myself...).\n. Hi Again @davewbrown. Tried a quick manual merge a few minutes ago, but it seems that the pull request is based on the 1.2.0 version and not the latest 1.3.0, in which I made a lot of changes. This causes lots of conflicts when I'm trying to perform the merge that I can't say safely that will not break anything.\nIs it possible (and easy and quick...) for you to provide the pull request again, but using HEAD? I'm not sure how quick is it for you (is it mainly a tool you're running or is it a set of manual changes?). Hope it's not too much trouble..\nIf you know of another way in which I can quickly perform a clean manual merge of the pull request, that would be even better.\nAnyway, if it's ok with you, i'd appreciate it if you can provide the pull request on a branch and not on the master branch. That would make multiple pull requests easier.\nThanks a lot again for the effort,\nHarel\n. Done as part of the 1.4.0 release.\n@davewbrown Sorry for not integrating your merge request regarding this issue when it was possible.\n. Not sure if you can see my previous message just minutes ago, but scratch it if you do... Forgot the details (too many production issues in my day job...).\nI would love to add this one, and also learned something new about getting the metadata from the cursor... :)\nI agree that -j is confusing and I wish we could use -h/-H for that, but we can't. I'll add this soon.\nOther suggestions for which flag to use are welcome.\n. @bonki I like this idea. Need to play with it a bit and add some tests, see that it makes sense in most standard use-cases. If I see it doesn't conflict with anything, then I might add it in the coming 1.4.0 version.\nThis does mean that if you want to have header in the output going to a file/pipe, then you need to use a full-name flag and not a one-letter flag, but I believe that this is something that can be considered reasonable.\nHarel\n. After some thinking, I agree with @jeroenjanssens. unwrapping lines, if needed, should be done by other tools and not by q itself, regardless of whether the output is a tty or not. There are enough tools which can do it much better and smarter than q would ever be able to accomplish without missing its original purpose.\nI'm planning on fixing this for the upcoming 1.4.0 release.\n. 1.4.0 Released and contains this feature. No \"tty detection tricks\" - Just -O for adding an output header as needed. \n. :)\nI'm hardly a licensing expert, but as far as I know, there is no need to notify the FSF directly about any new software using GPL. I am planning on adding it to the Free Software Directory soon.\nRegarding the years, I've been under the impression that the permission text needs to be constant and is part of the standard FSF copyright text.\nThanks for the input, I'll fix that in all files. It should read 2012-2014.\nHope you find the tool useful nonetheless,\nHarel\n. So I need to investigate this further and fix the things you're mentioning. I've been advised that writing the FSF as the owner is just a standard procedure for making the tool licensed as GPL, allowing easier enforcement than if I would have set the owner as myself.\nAny more information about this would be greatly appreciated.\nI would dig deeper myself in order to see what needs to be done.\nThanks\nHarel\n. Fixed the year issues in the files.\nHowever, the transfer of ownership is something which I'm not sure I fully understand how to fix.\nAny information and advice would be appreciated.\nHarel\n. @Xender thanks a lot for the information, and sorry for the late reply.\nThe coming version, 1.4.0, will solve this issue.\nHarel\n. 1.4.0 Released - Copyright is Harel Ben-Attia now, fixed years, and still GPL.\n. 1.4.0 Released and removes the need for this fix (temp table ids are sequential).\n. Hi @Gasol. Thanks for the pull request, I'll take a look at it as soon as I can. Seems like it can do a lot of good.\nThanks for contributing!\n. Hi Again @Gasol.\nI've checked your change and it seems great in terms of performance!\nThere's only one thing - One of the tests fails after the change (./run-tests).\nIt's related to handling of null values in fields which are integers. This might be related to the \"?\" parameter change in your pull request.\nI'm investigating on how to fix that. If you know what could be the reason, that would be great.\nWill update.\nHarel\n\nFAIL: test_handling_of_null_integers (main.ParsingModeTests)\nTraceback (most recent call last):\n  File \"./run-tests\", line 500, in test_handling_of_null_integers\n    self.assertEquals(o[0],'1.5')\nAssertionError: '1.0' != '1.5'\n. No problem, It's actually my fault - I haven't gotten around yet to fixing the folder structure, so things have become a bit too crowded.\nI'll also add some words on it soon on the readme file.\n. Thanks @Gasol !\nI've fixed a small bug in your latest changes related to different column types. Will merge the pull request later on.\n. Hi @Gasol I'm merged the pull request manually, with some changes related to what we've discussed. \nI tried to make sure that this will still be considered your pull request, but I'm not sure that it worked.\nI'm closing the pull request - Please contact me and tell me if it's counted as your pull request or not, and I'll try to fix it if not.\n. Fixed in 1.4.0\n. You're right. Didn't think about it. I agree that -T is the proper parameter name. \nAlready committed to my working master. Will be part of the coming version, 1.4.0.\nThanks\n. @bonki - Thanks for helping with the comment. \n. Done, with -T. Will be integrated into the coming 1.4.0 version.\nI decided to make the -t and -T flags stronger than their manual -d/-D counterparts. Meaning, that if there is one of the T flags, it will cause any D flags to be ignored.\nThe reason for that is that if someone is using the .qrc file, they will be able to override it with a T flags if needed.\nHarel\n. Just released 1.4.0. Fixed.\n. Hi @PreXident  Sorry for the late response.\nThanks a lot for the data and information regarding this problem.\nAs much as I would like to provide support for files with spaces, I believe that adding more complication to the hack-of-hack won't be a good direction :) It will also break existing behavior, which is something that will make a lot of users angry at me :)\nRegarding your additions to support other encodings in the queries, it seems interesting - I'm taking a deeper look at it and will integrate the relevant changes to the upcoming 1.4.0 release.\n. I totally agree that this could help for lots of situations. Maybe I should have thought about something like that from the beginning, but for now I believe that waiting for full SQL parsing will be the best way.\nRegarding the encoding part, I just managed to recreate your issue on my terminal. I had to change the actual encoding of the terminal in order to get the same errors (e.g. your queries worked for me out of the box...).\nAnyway, I'll update on this.\nThanks again\n. about the read from file part - I will arrange a parameter for that in the upcoming release. \nDidn't think it would be an issue, since in Linux you can always use backticked cat filename to inject stuff into the command line.\n. @PreXident From the example screenshot you sent above, I see that you're not using the -H parameter and will cause the \"no such column\" error.\nFrom my attempts, when using the IBM852 encoding, which is your encoding, things seem to work, but there might be some more issues.\nCan you try to run the same experiments above, but with the -H -d \\; parameters and let me know the results?\nThanks\n. @PreXident Reproducing this properly is very challenging, as it requires me to actually generate the byte-sequences for each encoding manually (copy-pasting between terminals of different encodings actually seems to convert the encoding automatically...). \nIn addition, it is not clear what is the encoding of the .sql file you provided - It seems to be utf-8 and not  your native encoding.\nAnyway, I'd be glad to continue pursuing this, but i need more information from you.\nIn the mean time, I'll provide a way to read the query from a file, and provide a query encoding parameter. \n. @PreXident Added query encoding (-Q) + reading query from file using -q (query encoding affects it), and also a separate flag for setting output encoding (-E).\nIt's pushed to the latest head. If you'd like to test it with your windows setup and locale settings, that would be great.\nThese additions will be part of the upcoming 1.4.0 release.\nThanks\nHarel\n. Added feature for 1.4.0, which just got released. You can read the query from a file using -q, and you can provide a query encoding using -Q if needed.\nAdded automatic behavior to work properly with the user's terminal encoding, but these two flags provide the needed control and flexibility if there is any problem.\nImportant: Please note that the header name encoding (which is using the input encoding) can be different from the query encoding (which is using the query/terminal encoding). This can lead to different encodings for the same column name. Any required conversion is handled automatically by q.\n. @jeroenjanssens You're right, and not only that, there is already a mechanism for holding this kind of mapping in order to be able to reuse data and tables. I'll fix that in the upcoming 1.4.0 release.\nThanks\nHarel\n. Fixed - no need for any random ids or anything. \nAlready committed and will be part of the 1.4.0 release.\n. Just released 1.4.0. Fixed.\n. You're right, I have a bug there. This is gonna be fixed in the upcoming 1.4.0 version.\n. @mikaelgrave As I mentioned in #74, please send me an email and I'll send you a modified version as soon as I'm done with the fix (before the version is released).\nThe next version (It's 1.5.0, not 1.4.0, sorry, it missed it in the 1.4.0 release, and didn't update here) will be available in the coming three weeks I estimate.\n. A modified version which fixes this one already exists, but has not been committed to the master branch yet, since I wanna do some more heavy testing for it.\nIf you want the modified version in the mean time, please send me an email to harelba@gmail.com and I'll send it to you.\nHarel\n. Fix has been pushed into the master branch, with added parameters -w and -W for determining the input/output quoting. Will be released as part of the coming version 1.5.0.\n. That's great. Thanks for the input.\nOn Mon, Nov 10, 2014 at 2:45 PM, Mika\u00ebl Grav\u00e9 notifications@github.com\nwrote:\n\nThank you @harelba https://github.com/harelba. I've just tried that\nlast version on a 9,000-row CSV file, using comma as delimiter, and\ncontaining many cells with commas. It all worked great. The output does now\nenclose those fields within double.\nNote: I did not use the -w or -W parameters (yet). The default behavior\nworked well for me once I had specify -d ,\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harelba/q/issues/56#issuecomment-62442455.\n. Full support for quoting input and output fields are part of the newly released 1.5.0 version.\n\nSupported quoting methods: none, non-numeric, minimal (when-needed), all.\n. Thanks a lot @StreakyCobra ! \nSome quick questions:\n1. Why was there a need to replace the shabang line in q?\n2. I'm not familiar with PKGBUILDs and ArchLinux procedures. Can you tell me what is the best practice for updating the version there once a new version is available?\nHarel\n. Thanks a lot @StreakyCobra ! that was really helpful.\nThanks for your efforts. Hope you find q useful for your purposes.\n. @StreakyCobra you're quick! I just saw that you've created the Arch Linux packages for 1.5.0!\nThanks a lot !!!\n. Didn't know there is one :) Now I do...\nI've always seen that github issues a new release for each of the tags I create, so I didn't really think it's possible to create my own released manually.\nI'm playing around with creating the actual release, and not just the tag, not sure how github will handle it, so be prepared for another atom message :)\n. Hi @pmarchwiak , sorry for the late reply.\nThe way to do it is to wrap the column name with back-ticks, as is the standard in sqlite for column names with special characters (the period in the column name is considered a special character since a.id might indicate also column name id in table a).\nharel@harel-laptop:~$ cat example\na.id a.name\n1 bill\n2 bob\nharel@harel-laptop:~$ q -H 'select `a.name` from example'\nbill\nbob\nharel@harel-laptop:~$\nPlease note that in order to use the back-ticks inside the select statement, you need to enclose the statement with a single quote and not in a double quote. The reason for this is that back-ticks are interpreted as special characters in the linux command line, and using the single-quotes prevents Linux from interpreting them, allowing q to get them properly.\n. Hi @barsnick, sorry for the late reply.\nI would love a pull request for this. That would be great.\nThanks a lot for the kind words. I'm really glad that you find q useful.\nHarel\n. Fixed lots of stuff in the RPM creation mechanism, including delegation of all preprocessing work to the SPEC file. Closing this issue. \nI would love to hear about any more RPM or packaging related issues - Please open new issues as needed.\n. Fixed in the coming 1.5.0 release.\n. Hi @vi (nice name), \nYou're right. It's currently a limitation that q works only when there is a FROM clause.\nI looked at it and fixed it. I'll integrate the fix into the coming version.\nIf you are using Linux/Mac, then I can send you the fixed version over email if you want, so you can work with it until the new version comes out. (In Windows I cannot provide the modified version without performing a full release).\nPlease tell me if you want the fixed version in the email.\nHarel\nharelba@gmail.com\n. I've pushed the fix for this into the master branch.\nWill close the ticket once I release the next official version.\n. Solved in the newly released 1.5.0 version. \nq \"SELECT 12+32\" is now supported, and actually, since multiple queries on the same command-line are also supported now, so things similar to q \"select 3+54\" \"select 100*200+23.5/3\" \"...\" are supported as well.\nClosing.\n. You're right... Most of my testing was on either pipes or files. It's a bit hard to create an \"actual terminal\" automatic test... Anyway, I'll take a look at it and analyze it.\nThanks for the input.\n. hi, sorry for the late reply. Will take a close look on this. thanks for the input, @bitti and @vi \n. Hi,\nI dug a bit into that. Seems to be related to buffering issues. This seems to be an issue in the python csv module. When reading directly from stdin, everything works fine, so it's not a python issue. I think it's related to the fact that the actual reads are not being done in the python csv module itself, but rather in the _csv native module (hence the difference between mac and linux).\nAnyway, I'm not sure that it's a large enough issue to work on currently. I'd give it a go later on again to see if I can find a workaround (Found a related workaround, but it seems to make the data reading line-buffered, which could hurt performance and has some quirks to it).\nHarel\nregular-python-stdin-iteration.py (works fine)\n```\n!/usr/bin/python\nimport sys,os\nline = sys.stdin.readline()\nwhile line:\n        line = line[:-1]\n        print line\n        line = sys.stdin.readline()\n```\ndirect-csv-iteration.py (requires double eof):\n```\n!/usr/bin/python\nimport sys,csv\nfor row in csv.reader(sys.stdin):\n        print row\n```\nline-buffered-csv-iteration.py (works ok, but has some quirks which might affects stability and performance):\n```\n!/usr/bin/python\nimport sys,csv\nfor row in csv.reader(iter(sys.stdin.readline, '')):\n        print row\n```\n. I agree. I'll open a ticket for python's csv module.\n. Closing\n. Hi, @Fil . Sorry for the late reply. I'll take a deep look at it during the weekend and see what can be done to make it more standard.\n. Hi @Fil . I've checked in a proper fix for these issues to the master branch. \nNew behavior is that both double double-quotes and escaping by backslashes should be working out of the box.\nSince it's a breaking change (although a welcome one...), I've introduced two new flags for disabling the double double-quoting and escaped double-quoting capability. These will provide backward compatibility for anyone who might need it for some strange reason (--disable-double-double-quoting and --disable-escaped-double-quoting)\nI would appreciate it if you could take the q executable from the master branch and test the changes when you have a few minutes and provide some input on the result.\nThanks a lot for the input and the help\nHarel\n. thanks for testing!\nI know, i couldn't think of a reason that someone would want to keep the wrong parsing, except for people who already ran q on their own stuff, not detecting this problem, and might want to run some more queries on the same data, without being afraid of any inconsistencies.\nSince it's a really crazy scenario, I did change the default behavior - The flags only revert q to the old behavior.\nI would probably remove these flags in the next-next version.\nAgain, thanks for the help with this\nHarel\n. Hi. \nI agree with @bitti here. I don't want q to become some kind of \"linux shell\" of some sort. My purpose is that q will be a complementary tool in the linux toolset, and @bitti's example shows the standard linux way to do such a thing (or pipe it and run q with \"FROM -\").\nAppreciate the input and the help nonetheless.\nHarel\n. yeah, you're right... Something like q -d , \"select a.*,b.* from \"<(cat fileA...)\" a join \"<(cat fileB....)\" b\" ....\n. of course :) It was just an example. The headers idea is great :) I'll use it - Thanks\n. again, sorry for the late reply (too much day work... :))\nI'll take a deep look at that during the weekend.\n. Hi @codeplayer2org and @Fil \nI'm trying to reproduce it on my machine and getting correct answers (using the official 1.4.0 and the latest version with the fixes for issue #66 ). From your first q command above, it's obvious there is some kind of issue with determining the typeid column name, so I would request if possible that you run the same command again, but this time with the -A parameter. This will dump the detected column names and types.\nSo let's do the following:\n1. Run the following command and send the output here. \nq -H -O -d ,  'select * from ./dailytasks.csv where `limit` = 2' -A\n1. Tell me which OS version you're using\n2. Could you zip the above csv file and the q executable and send them to me by email?\nThanks\nHarel\nbtw - Currently q doesn't recognize the second row of data types (is it a standard csv thing?). The effect of it is that q's auto column type detection would recognize all fields as strings (See the -A output). If you remove that second row and run q with -A again, you'd see that it detects the types properly. I will consider adding such a feature of setting the column types explicitly in the future.\nMy attempt at reproducing the issue with the official q version\nharel@harel-laptop2:~/dev/github/q/bin$ cat dailytasks.csv \n\"typeid\",\"limit\",\"apcost\",\"date\",\"checkpointId\"\n\"int\",\"int\",\"int\",\"string\",\"string\"\n\"1\",\"2\",\"5\",\"1,2,3,4,5,6,7\",\"3000,3001,3002\"\n\"2\",\"2\",\"5\",\"1,2,3,4,5,6,7\",\"3003,3004,3005\"\nharel@harel-laptop2:~/dev/github/q/bin$ q -H -O -d ,  'select * from ./dailytasks.csv where \"limit\" = 2'\ntypeid,limit,apcost,date,checkpointId\n1,2,5,1,2,3,4,5,6,7,3000,3001,3002\n2,2,5,1,2,3,4,5,6,7,3003,3004,3005\nharel@harel-laptop2:~/dev/github/q/bin$ q -H -O -d ,  'select * from ./dailytasks.csv where typeid = 1' \ntypeid,limit,apcost,date,checkpointId\n1,2,5,1,2,3,4,5,6,7,3000,3001,3002\nharel@harel-laptop2:~/dev/github/q/bin$\n. Found the bug... :)\nI've done the relevant fix in q. However, it seems like a risky bug so I need to test it and see how it affects backward compatibility.\nThe reason for that is that the csv module does not know about encodings, so q handles the encoding on its own. The bug is related to the way that q opens the file, which causes beginning-of-file data to have problems in such cases.\n@codeplayer2org - I'll send you the modified version in the email so you can continue working in the mean time.\n. You need to use -e utf-8-sig. The encoding is utf-8-sig (the one which can accept BOM entries).\nharel@harel-laptop2:~/dev/github/q$ bin/q -H -O -d , 'select * from ./dailytasks.csv where typeid = 1' -A -e utf-8-sig\nTable for file: ./dailytasks.csv\n  `typeid` - text\n  `limit` - text\n  `apcost` - text\n  `date` - text\n  `checkpointId` - text\nharel@harel-laptop2:~/dev/github/q$\nAs for the data types, the reason they are all detected as strings is because of the second row of the data types. If you remove it, the relevant fields will be detected as ints.\nI'm working on making the fix work properly with data from stdin as well, which will allow you to remove the second row in a streaming fashion (sed '2d') instead of changing the file.\n. Look at the email I sent. It's a further modified version which supports the proper encoding of the BOM when the data is streamed from stdin. This will allow you to remove the 2nd row of the file as it's fed into q, instead of changing the csv file itself. Removing that line will allow q to detect the type of each column automatically (Currently, the detection mechanism sees values such as \"int\" in the second row, inferring that the columns are all strings).\ntry running\ncat dailytasks.csv | sed '2d' | q \"...... FROM -\" -A -e utf-8-sig\n. i'm leaving this open until it will be officially release in the coming 1.5.0 version.\n. Turns out that the fix for supporting utf-8 with BOM conflicts with two bugs in the python csv module (see original bug http://bugs.python.org/issue7185 and there's another one which I will open to the csv module, which prevents reading non-ascii characters by the csv module, even when the file is being opened using the proper codecs.open(...,encoding=...) method.\nFor now, this bug of not being able to skip a utf-8 BOM message is of a lesser priority than continuing to fully support non-ascii characters in the data, so the latest fix into the master branch does not solve this issue.\nI will continue to try to find another workaround to support both, but in the mean time, please use one of the following workarounds:\ncat file-with-bom | awk '{ if (NR==1) sub(/^\\xef\\xbb\\xbf/,\"\"); print }' | q \"....\"\nor\ncat file-with-bom | sed '1 s/^\\xef\\xbb\\xbf//' | q \"....\"\nIf someone find a proper workaround for support this, while not failing any of the tests in the test-suite (run test/test-all), I would be glad to hear about it and integrate it.\nHarel\n. One more thing regarding someone finding a proper workaround - \nThe test for checking proper utf-8-with-BOM is included in the test-suite, but it's muted using an immediate return (the method name is test_utf8_with_bom_encoding). You can re-enable it during your tests in order to make sure your fix works.\n. workaround is implemented in the newly released 1.5.0 version.\nUse -e utf-8-sig as the encoding and things will work properly.\n. Actually, relaxed mode, which is the default mode, is more suitable for this use case.\nIn relaxed mode, if you provide a column count using -c, then any remaining data after the last column becomes part of the last column.\nSo to do it you just need to run q with -c 1. See the example below:\nharel@harel-laptop2:~/dev/github/q/bin$ cat one-liner-file \na b c d e f g h i j k l m n\nharel@harel-laptop2:~/dev/github/q/bin$ q -c 1 \"select c1 from one-liner-file\" \nWarning: column count is one - did you provide the correct delimiter?\na b c d e f g h i j k l m n\nharel@harel-laptop2:~/dev/github/q/bin$\nJust for completeness - Using strict mode will actually show an error if the column count is not identical to the column count provided/detected.\nAs for the warning message - Warnings/errors are always printed to stderr, allowing 2>/dev/null as is the standard in many linux tools. Maybe I'll add some quiet mode in the future, but this is low priority.\nHarel\n. Definitely true @vi. I'll fix.\n. Added warning suppression when the user provides a specific column count of 1 to the master branch.\nWill be released as part of the coming 1.5.0 version.\nHarel\n. Warning suppression when providing a specific column count is part of the newly released 1.5.0 version.\nProviding a -c 1 -m relaxed (-m relaxed is the default behavior) would treat the entire row as one column. If you really like to eliminate the output delimiter, you can do that through the SQL itself.\nHere is an example for comma separated lines:\nq -d ',' \"select replace(c1,',','') from myfile\" -c 1\nIn order to eliminate tab separated lines, you need to use the more elaborate sqlite syntax for writing a tab (X'09' needs to be used in order to signify a tab literal):\nq -t \"select replace(c1,X'09','') from -\" -c 1\n. Yeah, this is definitely a required feature, although I'm not sure of how it's best to implement it. Two things - one is that I'm not sure it should clutter the query itself or be part of the parameters to q, and the other is that adding it to the query itself requires a full blown sqlite3 parse tree generator, which I haven't found a good one yet in python which actually does the job (Having such a parse tree generator would allow much more good stuff to happen).\nAlso, I'm not sure about the frequency that this kind of problem happens to users. I'm planning on adding a \"feedback mode\" to q, which will allow to send feedback to my email directly from the command line. Maybe then I'll have more stats regarding what is more frequent and what isn't.\n. I've created a new python-level API for q, which will allow q to be used from inside python code, as a module. The API level fully supports per-file input parameters. Still not sure how to expose it to the command line level, though, so we'll see how to expose it.\nAn alpha version of the new API will be committed into the master branch in a couple of days. Any feedback on it will be greatly appreciated.\nHarel\n. Alpha branch of the python api has been committed to https://github.com/harelba/q/tree/expose-as-python-api. \nAny input would be helpful and appreciated.\nHarel\n. The's definitely the idea :) the python API is only meant to be a backend\nfor the command line - Nothing will change from the command line\npoint-of-view.\nI'll try to provide a proper way to provide separate parameters for each\nfile in the command line.\nOn Sun, Nov 23, 2014 at 9:10 PM, Vitaly Shukela notifications@github.com\nwrote:\n\nI think the main advantage of q is ability to use it as a one-liner,\nwithout any \"environment\". I don't want new features (this issue, the\ncaching) to be API-only and command-line interface to \"stagnate\".\nFor-programmer q analogue can be a bit like F#'s type providers\nhttp://msdn.microsoft.com/en-us/library/hh156509.aspx, bringing\ninformation about rows in a CSV file into tab completion in Ipython or an\nIDE. Such library can be a backbone for q, not a \"mode\" of q.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harelba/q/issues/70#issuecomment-64145802.\n. @vi I believe that the notion of your idea of providing aliases to the files, along with the relevant modifiers would work well.\n\nOnce the python API is commented upon and finalized, I will get to exposing that on the command line.\n. @vi The python API is now the driver for the newly released 1.5.0 version, and I have a development branch which describes the API. Your input will be welcome. https://github.com/harelba/q/blob/expose-as-python-api/README.markdown\n. I have been thinking about adding modification queries, and this is part of the reason why the command is q and not select. In addition, I didn't want select to be case sensitive, which would have been the case if the command itself would have been called \"SELECT\".\nOne of the reasons adding modification commands is stalled is because this requires much smarter parsing of the queries, for which I haven't found a good solution yet.\nWhat do you think about the popularity of such a feature?\nHarel\n. I've released version 1.5.0, which internally is a full python API.\nDoesn't support any modification of files, but might be relevant for some of the use cases. Your input on the API will be most welcome (Take a look at the API development branch in https://github.com/harelba/q/blob/expose-as-python-api/README.markdown)\nPlease note that providing a full-blown modification API is not a small feat... :)\n. This is an easy one to add, although the part which feels a bit hacky for me was the fact that you still need to wrap the rest of the command line with the quotes. I've been thinking in the past of allowing multiple parameters to q, joining them in the code, but it felt hacky as well, since special shell chars would need to be wrapped anyway.\nAny thoughts about this would be appreciated.\n. I have to say that none of the solutions seem clean unforunately. I do not wish to pollute linux's namespace with SQL commands such as select and update, and the need to do hueristics in order to try to make things \"easier\" usually makes things harder for people. \nI prefer the stricter approach on that subject. If there is a concrete suggestion for a clean solution, I will definitely consider it.\n. Thanks for the pull request. \nThis change tries to make the pip installation work with too many breaking changes (This will break a lot of existing installations). In my view, pip should be used for installing python modules, and not main executables.\nI really want to make q installable by pip, but only as part of separating q to two parts - one is a reusable python module (installable by pip) and the other is a \"main\" command line frontend. \nI'm doing it as part of a larger refactoring of the q code base. \nThanks a lot for the effort!\n. @mikaelgrave @Fil you're right, this is one of the main things for the coming version 1.5.0. \nI'm sorry this is inconvenient for you... Send me an email to harelba@gmail.com if you want, and I'll send you a modified q version once it's done (before the new version is released).\n. thanks :) That's a funny typo :)\n. thanks!\n. I have a fix for it in the coming 1.5.0 version. I'll get back home in an\nhour or two. Please provide your email address and I'll send you the\nunofficial version which fully supports it.\nHarel.\n\u05d1\u05ea\u05d0\u05e8\u05d9\u05da 8 \u05d1\u05d3\u05e6\u05de' 2014 21:48, \u200f\"Max Harlow\" notifications@github.com \u05db\u05ea\u05d1:\n\nI'm querying a CSV which has a column whose (quoted) values contain\nnewline characters. The output of q doesn't display those quotes, so when\nI bring the resulting CSV into Excel it thinks each row is a new record.\nExample row:\n123,Something,\"A\nrecord\nwith\nnew\nlines\"\nIs there any way around this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harelba/q/issues/78.\n. Merging into #56 .  Same issue.\n\nFixed for the new 1.5.0. Will close #56 after releasing.\n. Hi,\nCan you send the output of file data.csv? I wanna check that your file is not using the \"UTF-8 with BOM\" encoding - See the bug https://github.com/harelba/q/issues/68\nRegardless, I would appreciate it if you could send me the actual output of the initial part of the file, if possible (run head -50 > sample.csv and send it to me by email harelba@gmail.com). Testing by recreating the actual data on my machine works, so this quite possibly encoding related.\nSorry for the inconvenience, \nHarel\n. @oderwat - btw, if the output of file says that it's a UTF-8 with BOM encoding, then you can use the workaround I described in bug #68 in the mean time (and tell me whether it works or not).\n. Just checked your file and this is exactly the same issue as #68 . The files are encoded using \"utf-8 with BOM\" encoding, which the internal csv python module has problems with.\nI thought it's a rare case (see #68 discussion), but since you have it as well (there was one more person), I will provide support for this special case in the coming version 1.5.0. For clarity I would like to say that q natively supports encodings, but this specific encoding (\"utf-8-sig\") collides with a bug in one of the libraries I use, and causes this issue.\nIn the mean time, there is a relatively simple workaround which you can use (Wouldn't work in Windows, sorry...):\nRun your queries as follows:\ncat sql.csv | sed '1 s/^\\xef\\xbb\\xbf//' | q -H -d , \"select la_id from -\"\nThe idea is to remove the first four bytes of the file (four implicit bytes, that you can't really see), since they are the ones causing the issue.\nDepending on your use-case and file sizes, you could write the modified file to another file and work on the modified file in the mean time, in order to avoid writing the workaround every time you use the file:\n```\ncat sql.csv | sed '1 s/^\\xef\\xbb\\xbf//' > fixed-sql.csv\nq ..... \"select .... from fixed-sql.csv\"\n```\nHope that helps for now. The coming version 1.5.0 is delayed a bit, but will be available soon I hope.\nHarel\n. I've integrated a work-around the bug to the newly released 1.5.0.\nBut regardless, if you can remove the BOM part, that would be even better :)\n. 1.5.0 has been released and fixes this problem. Use -e utf-8-sig when using UTF8 with BOM files\n. Ah... That's an interesting one :)\nCan you provide an example file - 10-20 lines will be enough, so I can analyze what is needed?\n. @LeDom22 Didn't seem to get your sample file by email. Can you please send it?\n. Definitely an interesting one :)\nTwo bugs here actually:\n- First,the column type is not detected properly (the column is analyzed as text and not as a float/decimal. Run with -A and you'll see what I mean).\n- Second, semi-related of course, is that performing numeric operations on these columns does not work properly.\nAs for the second part, there is a simple (however crude workaround), which will work for you in the mean time. You can use the function replace to replace the commas with a dot before making calculations.\n$ q -d ';' 'select sum(replace(`Euro line amount`,\",\",\".\")) from Cs.csv' -e cp1252 -H\n124\n11900\n12\n23\n33\n55\n66\n77\n17983.75\n...\nPlease note that in the example file you sent there are cases where the amount is some kind of code and not a real amount so sum(X) without filtering for the proper lines will not provide a correct money amount.\nAbout the first bug, I will have to take a deeper look at it and see how q can solve this (sqlite does not support decimals out of the box, but q can - and needs - to abstract that from the user).\nHarel\n. Hi @haozh ,\nYour case even makes this issue much more critical, since it becomes impossible to get the correct data.\nCurrently, there's no way to specify data types manually. \nI've created a temporary modified version which can force all fields to be text fields. If you'd like it, please send me an email and I'll send it to you so you can continue your work, although it will obviously impair your work with the other fields as non-text data. \nIf you don't want the modified version, there's one ugly workaround I can suggest - If you have a header row in your file, you could stop using the -H parameter - This will lead to a warning, but will include the header data as part of the actual data, leading to inferring the columns as text columns. Using this trick will require two things: You'd need to use the c1,c2,cN field names instead of the real ones, and you'd also have to filter-out the header line using a WHERE clause. If you don't have a header row in the file, you can just add a dummy one, and use the same trick as I described above.\nIf that doesn't solve your immediate problem, tell me and I'll work on a modified version which will allow you to force all the column types.\nHarel\n. python3 is great, and q will probably support it in the future. However, making q use multiple cores is not related to python3 or the language in general, but would require a much different logical effort in term of the actual logic. \nI have plans on creating a concurrent version of q, but it will probably not be in python.\n. One of the options, yes, not sure yet. Researching way to provide a streaming solution. We'll see :)\n. q exposes the same capabilities as sqlite, which means that you can use the predefined column \"ROWID\" for getting the row number.\nq \"select ROWID,...... from ....\"\n. As for the your original request, @bitti is definitely right in that case, since ROWID > 3 will not make q skip actually reading the first three lines, it will only select the rows after the third line. There is a difference, since column name and type detection will happen right from row 1, and this is probably not what you want.\nIf you just want to skip returning the first three rows, then using ROWID > 3 is fine.\n. The delimiter of du is a tab, not a space. q has a shorthand parameter for tab-delimited - -t You can use it, or you can use the more elaborate way -d $'\\t' (The strange way to describe a tab in the linux command line in general).\nharel@harel-laptop2:~/dev/outbrain/graphitus/magniphus$ du -x --max-depth=1 ../ | q -t \"select c2 from -\"\n../css\n../js\n../data\n../images\n../magniphus\n../.svn\n../grafana\n../fonts\n../\n. The reason is that the c1/1024.0 expression is not a string, but a number. As in regular sqlite3, you need to cast the number to a string in order to use the || operator and concatenate them:\n$ du -x --max-depth=1 ./  | q   -t \" select cast(c1/1024.0 as string) || 'K' from - order by c1 desc\"\n2.9921875K\n2.0703125K\n0.85546875K\n0.015625K\n. If you're using linux, then using its power would give it to you easily without the need for q to support such a thing (which would be trying to stretch q beyond it's scope, and Linux have much better tools for such things). Just use tr in order to replace one of the delimiters with the other before feeding it to q:\ncat myfile | tr $'\\t' ' ' | q -d ' ' \"select ..... from - where ....\"\nBtw, the crazy dollar-quote-baclslash-t-quote thingy is the standard Linux way to describe a tab on the command line...\nIf it's on Windows you're talking, then I'm sorry that I cannot provide a good solution for that. One possible solution would be to use q itself in order to \"trick\" the data and replace the tabs with spaces. I can be done by piping the data into q twice - A first time with the entire row as one column, just to replace the tabs with spaces (using the function replace), and then pipe the output to a second q , which will actually run the query you need. \nIn essence, you would be implementing an elaborate and crude version of Linux's \"tr\" command by using q. Something like that:\nq.exe -c 1 \"select replace(c1,X'09',' ') from myfile\" | q.exe -d ' ' \"select .... from ... where ....\"\n. Hi, @Hunter-Github, \nThanks for taking the time to comment.\nI quite agree that q is not suitable for every use case there is, and in my view it is not meant to be. It's another tool in the Linux toolset, which allows people to do a better or a more readable/maintainable work where suitable. \nI use many of the standard Linux tools quite extensively (too much, some would say), and I love them (grep is an amazing piece of technology, both conceptually and performance-wise, and it has 20 years of evolution behind it, which makes it even more amazing in my eyes).\nI would never expect someone (nor myself) to use q for everything. On the contrary - I would encourage them to combine the usage of the best tools they know in order to provide the most efficient solution. q is another tool in that toolset, and I constantly find myself piping output from grep to q, to sed, and then to q again, in order to solve the problems I need to solve efficiently.\nI would only hope that many people are finding it useful and seamless enough in order to be used naturally as another Linux tool. If that is the case, I would be really happy.\nRegardless, I am always thinking about ways of improving q's performance, including thoughts about a streaming solution which I've started researching lately. I would really love getting concrete ideas about how to improve its performance, and I'd be glad if they lead to more users finding q beneficial for their purposes.\nThere's some more related stuff in my rationale page of q's site - http://harelba.github.io/q/rationale.html .\n. By the way, internally, q knows to reuse data which has already been loaded. In the next version, q will support an interactive mode which will maximize the value of this capability by allowing the user to perform multiple queries and analyze the data in multiple ways, without having to reload the data for every query.\n. Hi, sorry for the late reply. Too much stuff in my day job the last days...\nI'd be glad to take a deeper look on your changes later on this week.\nThanks for the effort!\n. Hi @kmehkeri,\nThanks for opening the bug. It is a bug. I've fixed it and pushed the fix into the master branch. It will be released as part of the next version.\nIn the mean time you can take the q executable from here, or send me your email address and I'll send you the executable.\nPlease tell me if there are still any issues with the newer executable, of if you encounter any others.\nHarel\nI'll close the issue after the next version is released.\n. Fixed in v1.6.3. Hi, you need to use -H in order to tell q that there is a header row, and also use -d , to signify that that delimiter is a comma. \nPlease tell me if things still don't work for some reason. \n. Hi @ralhei sorry for the late reply,\nThanks a lot for your input and your efforts. \nIn regard to installing using pip install - While it's another easy way to install q, it collides with my plans regarding q and python. I plan to provide q as a python module so q's capabilities will also be exposed as an API (e.g. import qtextasdata ...), and I treat the command-line variation as \"non python specific\", meaning that the fact that q is written in python is incidental.\nUsing pip install for installing the command line variation, although useful, contradicts it. For example, I'm not sure how \"uninstalls\" are supposed to be performed by pip for q. Another point is that in all other installation forms, q is installed on /usr/bin/q and using pip it seems to be installed in /usr/local/bin/q.\nWhen I make the python module public, I will definitely consider using what you're suggesting here so the q command line can be installed alongside with it.\nIn regard to the license, you're right that it's not exposed properly. The code does say which license is it, but it's not exposed in the README/site. The license is GPLv3, similar to many command line tools. \nHope you understand the reasoning behind my decision,\nHarel\n. Hi Ralph,\nI see your point about the virtualenv installation. \nAlso, in regard to my plan of providing a python module - The plan is definitely to provide a pip install method of installing q, but my plan is to change the project structure so it becomes a \"standard python module\" and not just add setup.py. However, adding a setup.py, perhaps in a subfolder so it doesn't clutter the root project folder, can provide the \"pip instlal git://\" capability without hurting almost anything. \nAlso, I'm hardly an expert on python packaging, but i believe that providing an uninstall is probably just adding some stuff to setup.py. I'll take a look at that.\nI'll think about it some more and decide what to do.\nThanks for the input\nHarel\n. You're right. It seems that the default \"minimal\" output wrapping format does not cover newlines...\nYou can use \"-W all\" in the mean time in order to get coherent output. I'll make sure to fix this as soon as possible.\nHarel\n. @maxharlow Hi, can you please confirm that the workaround works for you? or is it?\n. +1 for this. You're absolutely right. I know it's annoying.\n. Obviously, I will take a look at it to see how this can be done :)\n. +1 for this as well.  Will take a deeper look.\nThe issue with adding -O automatically is related to backward compatibility. For example, if there are automated scripts which people created, then it might break them. \nHowever, I do agree that it might be worth considering a breaking change in this aspect.\n. @mahiki regardless of the nice suggestion of mixing between c1..cN and still using the -H, please see @bitti 's solution for the example you've written. When using double-quotes, linux still executes backtick-enclosed code, but using single-quotes, linux will not bother you with it. Using single-quotes would require, though, using double-quotes inside the query itself, or escaping any single-quotes inside the query parameter.\nThanks @bitti for the help.\nRegarding the feature request itself, I'd bump it in priority.\nHarel\n. thanks @bitti that's true, I will change the examples to single quotes.. @bitti sorry, this fell off through the cracks... I'll make note to change it.. Hi @dkvasnicka \nThis is indeed an issue, and requires adding full support for decimal types. \nI already have an intermediate temporary version which allows to force all column types to strings. This has been helpful in one related use case. However, it won't be helpful in your case, unless you're writing the output fields without any computation whatsoever.\nIf you want it, I'd be glad to send it to you. Just drop me a line over email.\nI'm working on abstracting the decimal values issue away (sqlite does not have a real notion of a decimal, so it needs to be handled by q).\nHarel\n. Hi @Vel0x , sorry fro the late reply.\nYou're right, and I have plans to do this. I've made some preparations for this in the latest 1.5.0 version, and hopefully it will be part of the next version. \nIn the mean time, there is partial support for something which might help you - You can run multiple queries on the same command line and the data will be loaded only once:\nq \"select .... from A\" \"select .... from B\" \"select ... from A\"\nIn this example, A and will will be loaded only once - The third query will run almost instantly since A is already loaded.\nBy the way, I'm also planning on an interactive mode for the next version, which will make use of this capability and will allow to run multiple queries on the same data without reloading.\nHarel\n. sorry for the late reply,\nq has a variation on this - You can ask it to save the data to an sqlite database file, and then analyze it with sqlite command line.\n-S SAVE_DB_TO_DISK_FILENAME, --save-db-to-disk=SAVE_DB_TO_DISK_FILENAME\n                        Save database to an sqlite database file\nRegardless, if you're dealing with huge files, then you'd probably need to use some distributed solution and not q. I once had done some steps of making q's engine work with apache spark behind the scenes, and a POC was indeed working, but I eventually didn't pursue this to become full fledged production grade.\n. Hi,\nChecked the _csv C source code and it seems that there should no performance/memory impact for changing this. Also did some performance tests and it seems that there is no real impact.\nHowever, there might be cross platform issues with sys.maxint, and @bitti is right that in most cases it's probably a delimiter issue and not a real field value.\nI will catch this kind of error and issue a warning so people can check whether it's a wrong delimiter issue or not, and will either add it as a configurable value or set it to a platform-robust appropriate value.\n@ShinNoNoir If you need a modified version in the mean time, please tell me and i'll make one and send it to you over email. Also, it would be very interesting for me to see this kind of data, just so see the actual use-case. If you could send me one or two rows of data over email, that would be great.\nHarel\n. @ShinNoNoir Thanks, if it's too much effort, then it's ok, i wanted to see it mainly out of curiosity.\n. Fixed in v1.6.3 - The size is now dynamic and can be passed as a parameter. Also, in case of an error the offending line number is being shown. \n. Hi @Fil , sorry for the late reply.\nThanks for opening this. It does seem like a problem. I'm investigating.\n. It's due to a bug in python's csv module. It doesn't support encodings, and the standard way that python suggests to override it does not support NULs (q follows this suggestion currently). \nI've seen some ideas on how to overcome this, but I need to check them out, to see not only that they are working, but whether or not they impose a significant performance penalty.\nI'll take a further look and try to see if this change can be made.\nIn the mean time, I'm sorry I don't have another solution, but you'll need to use iconv or something similar. If there's anything intermediate which you think will help you, please tell me.\n. Thanks a lot! I'm glad you find q useful.\nI've already started working on a REPL, and it will come out in the next version of q.\nPart of its features is that it will be able to reuse already-loaded data, so multiple queries on the same data would be immediate. The underlying API for supporting that has already been implemented in version 1.5.0.\nAny additional input would be most welcome,\nHarel\n. no update unfortunately, i'll see if i can squeeze it in and package a new version. Hi @jungle-boogie , sorry for the late reply,\nonly python 2 is supported for now. I haven't checked how far the incompatibility goes, perhaps I'll be able to take a look before releasing the next version, and see if I can make it compatible with python 3 without breaking stuff.\nHarel\n. Hi,\nSubqueries are supported currently only in the where clause.\n@bitti 's solution (Thanks!) solves the problem using standard linux bash - The <(cmd) contruct tells bash to run this command, and replace it with a temporary filename (actually a file descriptor) which will contain the data.\nIf you're still having any issues, please contact me.\nHarel\n. Closing this one as it's a subset of #12 \nFixes regarding it will be notified in #12.\n. @knightwu - @bitti 's solution (thanks!) should be working well.\nPlease note that if you already have column names which start with a number, such as 19_cost, you can use them by wrapping the column names with backticks (Identical to sqlite's way of escaping identifiers). Be aware, though, that in order to pass backticks to q, you'd need to use single-quotes around the query itself and not double quotes, otherwise linux will interpret the backticks on its own, and this is not what you want. Example below.\nIf you still have a problem, I'd be glad to hear about it.\nHarel\nq -H -O 'select avg(a.cost) as `19_cost`, avg(b.cost) as 0_cost, avg(a.qcost) as 19_qcost, avg(b.qcost) as 0_qcost from ... where `19_cost` > 1000'\n. Thanks a lot @bfontaine !\nMerged.\n. Added flag for universal newline support (currently only for regular files, not supporting .gz or direct-stdout). It's not in the new released version, but it's in master. Glad if people could give it a go before I issue a new version.. Hi @chuanconggao sorry for the late reply, been on a vacation.\nThanks for notifying on this, I'll take a deeper look.\nHarel\n. fixed on master.\nWill close it when the new version is released.\nThanks a lot @jungle-boogie for finding and help with this!\n. thanks @jungle-boogie !\n. No problem @garethrees , happens to me once in a while as well :)\nbtw, a useful parameter might be to use -A to see the column names and types:\nharel@harel-laptop2:~/dev/github/q$ q \"SELECT * FROM 55c5d2646373760e9f00002b.csv\" -H -d , -A\nTable for file: 55c5d2646373760e9f00002b.csv\n  `Transaction Date` - text\n  `Transaction Type` - text\n  `Sort Code` - text\n  `Account Number` - int\n  `Transaction Description` - text\n  `Debit Amount` - float\n  `Credit Amount` - float\n  `Balance` - float\nharel@harel-laptop2:~/dev/github/q$\n. note that sqlite (and q) will require you to wrap your field names with a backtick, similar to the output above, in cases where a space is part of the column name.\n. @zaxebo1 Thanks for the information. I didn't know about six - I'll take a look and try to integrate it in the next version.\n. @NDevox sorry for the late reply. \nThe real block is making sure that the distribution logic remains compatible and working properly after the changes.\n. Hi, sorry for the large delay in providing a response.\n@NDevox @zaxebo1 : You're both right, q's end-to-end testing would definitely cover that.\nThe main roadblock (aside from my having some free time...) is related to packaging. Ideally, I would have wanted a fully packaged single-executable for all OSs, preventing the users' need to actually care which python installation is used behind the scenes. Currently, however, this is not the case, and rpm/deb users need to be aware of it. In Windows I'm already using PyInstaller in order to provide a single-executable. Making the relevant changes so windows remains a single-executable is the main challenge, apart from the obvious changes in q's code (which I'm assuming are relatively minor).\nSo the shortest path to making it happen is providing a 2.7/3.x mutually compatible version of q, and then rebuilding the packages and making minimal changes to the packaging for all OSs. Hopefully, the changes for the windows version creation would be minimal as well - Replacing/Upgrading PyInstaller would be a larger effort that we would want to prevent.\nIf you have other suggestions for easy packaging that would provide a short path to it, I'd be glad to hear them. A PR from master with the relevant changes would go a great length towards this, as free time is scarce these days :) \nbtw, note that according to q's Google Analytics 35% of users are Windows users, so we cannot neglect this OS :). Hi @NDevox ! Thanks for your interest. \nRegarding the packaging, I'm not an expert in deb/rpm packaging, but I believe the changes there should be minimal, if any. For PyInstaller, it's supposed to be supported, but the it's a very delicate software unfortunately. For example, in the latest q version, some people are experiencing a pyinstaller warning every time they're running q in Windows. Making it package q with python 2.7 executables (in Windows only) could probably be a minimal to reasonable effort, but making it package q for windows with python 3 might prove a larger effort. \nRegarding q being inside a single file - This is actually an artifact of python's packaging being notoriously bad, as you mentioned. The idea was to provide people with an easy capability to download a \"single-file executable\" in linux/mac environments without resorting to deb/rpm, if they want to. I would probably do it differently today, exposing q as python module that can be pip installed, and a frontend q binary that just uses it, but this is a larger effort, that requires more attention, and is unrelated to the python 3 porting.\nBtw, the test-suite is essentially a set of system tests that actually run the q executable. It is quite comprehensive and completely external to the code - it tests the command line interface itself. These tests allowed me to do large scale cross-cutting changes with relative confidence.\nI'm sorry I don't have time to invest in fully porting this myself. I'm too tied up with other projects at the moment, but I'd be glad to push this through and help as much as I can, if there's some help from the community.\n. Added flag for universal newline support (currently only for regular files, not supporting .gz or direct-stdout). The solution is based on this PR, but it adds backward compatibility through an explicit flag that needs to be provided, and some clearer error messages.\nThe addition is integrated into master (not part of the new 1.6.3 release - I'll issue another version after some people test these changes). Thanks a lot @serima !! \nI will create a way to switch between the languages. Merging it.\n. @serima It's online! \nhttp://harelba.github.io/q/ja/\nThanks again for the effort, and for supporting q in general\nHarel\n. Hi @jitbit , sorry for the late reply.\nq supports standard SQL comments (it actually supports all sqlite constructs, so you can just search the net for sqlite stuff as needed).\nThis means that you can embed comments inside the queries using \"/.../\" and using \"-- .... \\n\" syntaxes. \n@bitti thanks :)\nHarel\n. ah... I see :)\nNeed to think about it. Adding special semantics above the layer of the SQL itself can be a rabbit hole, but I won't say that there can't be any exceptions.\nThe standard way to achieve something like that without resorting to preprocessing such as sed, would be to add another condition to the WHERE part of the sql (something like \"WHERE c1 not like '#%'\"). That would do the trick.\nI'll give it some thought.\nHarel\n. @jitbit Thanks for the kind words. You're right about the issue with the comment's formatting. I actually don't usually see logs with comments in them, but perhaps it's just bias from my regular day job stuff.\nIn terms of parsing wrongly-formatted rows such as comments, q does provide \"relaxed\" mode (which is actually the default in order to be able to easily handle output from many linux commands which don't provide strict tabular output), but commented lines can cause issues in column-inference in that regard (e.g. thinking that a column is a string instead of being numeric, since the comment text will be considered as data), so it won't be fully helpful.\nI am aware that in the context of Windows, many users are using q as kind of a command line replacement for many of the standard linux tools, but I do agree with @bitti that in that regard q cannot be a full replacement for all the needs. Again, need to think about it - I got a request for removing comment rows from two more users, so perhaps it might be something that needs to be considered as an exception.\n. Unfortunately, the way I parse the query in q prevents it from supporting the (legitimate) sqlite implicit join syntax. Perhaps I'll fix that in the future, but for the forseen future, this is one of q's limitations, and I should probably add it to the list of limitations in the web site.\nThanks for the pull request!\n. Hi @ryanmjacobs - This is supposed to be fixed, and I can't get this error in the latest version 1.5.0.\nharel@Harels-MBP:~:$ cat file1\nid age\n1 78\n2 45\n3 25\nharel@Harels-MBP:~:$ q -H \"select * from file1 where age = 26\"\nharel@Harels-MBP:~:$\nWhich version are you using? (q --version)\n. Sorry for the inconvenience. \nI'll take a look.\n. hi sorry for the late reply.\n@ernestohs thanks for the input - this means that the fix is working well... great :)\n. @rsalmei Hi, sorry for the late reply, I'm on a long vacation mostly offline.\nI'm not sure I can understand the original problem yet. I'm using version 1.5.0 myself on a mac and it doesn't crash even with the -HO. Can you please send me \"file1\" over email so I can check? Please send it zipped so gmail won't modify it or anything.\nThanks\nHarel\n. Fixed in v1.6.3.. @mikaelgrave Hi, sorry for the late reply, I'm out on vacation.\nI'll take a deeper look and update here.\nHarel\n. @mikaelgrave Indeed an important bug, thanks a lot! \nFixed it, but the issue implicates a bug in one of the tests as well (meaning that the change will break current behaviour), so I wanna test it some more before checking it in.\nIf you're not on Windows and want the un-checkedin version, ping me over email harelba@gmail.com and I'll be glad to send it to you in the mean time.\nHarel\n. btw, if there's anyone who would be kind enough to test the modified version on their stuff, just to make sure that nothing important is broken, I'd be glad to provide it.\n. @mikaelgrave - Sent over email. Thanks a lot!\n. Fix integrated into master.\nThanks @mikaelgrave for finding the problem and helping to test the fix!\nTicket will be closed after the next version is released. If someone needs the fix earlier, please use the master trunk or contact me.\n. Fixed in v1.6.3.. Fixed in master. Will be closed once a new version comes out.\n. Fixed in v1.6.3. Hi all sorry for the late reply, I'm on a long vacation in south east Asia and Internet here is intermittent :)\nAnyway, I'll probably implement the automatic delimiter detection right after I come back from the vacation, shouldn't be too hard. The original reason for space being the default delimiter was actually because many Linux tools output their table-like data using spaces, and I didn't want to break compatibility by changing the default.\nThank you all for the input.\nHares\n. Hi sorry for the late reply, i'm on a long vacation...\nThere is a limitation on the data size that q can process in that case, since on windows it is currently packages with the 32 bit version of python.\nI'm working on an experimental version which will be using Apache Spark behind the scenes and will allow to work around this kind of limitation (and provide much more speed in many use cases). Once it's out, this issue will no longer happen obviously.\nI'm really sorry for the inconvenience and that I don't have a good solution for your needs.\n. This indeed seems like a bug, since it's only meant to be a warning thrown to stderr, and not fail anything.\nI'll dig into it.\nThanks a lot @nduc !\n. Fixed in master. Will go out as part of the next version. \n@nduc I'd appreciate it if you could download the master version and check that it works for your use case. Please send me information here or through email if there are still any issues.\nbtw, it turns out that this exposes another issue in which q was not issuing an error even if the file was completely empty (e.g. the actual header is missing but expected). Fixed that as well, but it's kind of a breaking change, meaning that there might be people out there that relied on the broken behaviour.\nSorry for the inconvenience.\nHarel\n. Reopened until it will go out in the next official version (Didn't mean to close it - the checkin did that automatically).\n. It seems related to the python version you're using. q requires python 2.6+\nCan you check the python version you running? \nbtw - Which OS are you running on?\n. also - python 3 is not supported yet, perhaps it might be related as well.\n. Interesting - Is it a 64 bit system? \nAlso, please tell me what do you mean \"rebuild\" q? How do you run it?\nRun python REPL, and inside it run:\nimport sys\nprint sys.maxsize\nI suspect it might be related to the fact that it's a 64 bit installation (When I provide q.exe on windows, it's compiled with the python 32 bit binaries, regardless of the target OS 32/64 bit configuration).\nSeveral options:\n1. Hack the q python code (and hope that it's the only incompatibility) - Change line 53 \"sys.maxsize\" to 131072 and try to run it.\n2. Download a side-version of python 2.7 32bit, extract it to a separate folder, and run q with that python (don't override the system python or anything, just run python using the full path of this extracted folder)\n3. Wait for the next official version (in which I provide a full-blown q.exe file).\nSorry for the inconvenience. Wish I could provide you with a better experience.\nHarel\n. Hi, thanks a lot for the input. \nI'll be near my computer tomorrow morning Israel time, and take a deeper look. \nI suspect that these issue is addressed through the vagrant instance that I'm using in order to create the build. \n. Btw, I have an idea that it might be related to a small issue in the fix I introduced today... As I mentioned, I'll take a look at it first thing in the morning. \nHarel \n. Sorry for the late reply.\nIt seems like a deeper thing, I'll have to continue my investigation. Sorry for the inconvenience\nHarel\n. Original issue fixed in v1.6.3.. Hi @schlauergerd , sorry for the late reply.\nq follows the sqlite syntax, which does not support short-from joins. The way to write it is using the \"cross join\":\nq \"select * from file1 f cross join file2 g\"\nPlease tell me if you still have any issues.\nHarel\n. Thanks @schlauergerd !\n. Hi, sorry for the late reply, i'm on a long vacation...\nMost sqlite capabilities are exposed through q without any special needs. However, CTE is not one of them. I'll take a deeper look and see if it's possible to add it.\nThanks\nHarel\n. Of course, i'll definitely add this to the limitations if I can't come up with something good.\n. the syntax is matching sqlite, so you can do the following in order to get random records. Take into account that this is not a light operation if dealing with very large files.\nseq 1 1000000 | q \"select * from - order by random() limit 5\"\nDon't hesitate to contact me if you need more assistance.\nHarel\n. @eddified Sorry, missed that issue a while back. Probably not relevant anymore, but just for completeness and for other people. \nThe root cause of this is that the last line contains a malformed field content in terms of double-quoting - Seems that the rm_html_file_name field contains a partial json, and hence partial escaping of double-quotes. This causes q to misdetect the column types and the number of columns (detecting 86 columns for this row instead of 81). When deleting other lines from the file, q's autodetection changes (since the autodetection is according to the actual data), and this is the reason why the error was gone/changed.\nHere's the content of the partial field (row with 1334565323 in it):\n\"4|{\"\"ad_body\"\": \"\"Special savings on our online marketing tools & custom services to get you up & running.\"\", \"\"ad_id\"\": \"\"6010875463996\"\", \"\"viewtag\"\": \"\"https://altfarm.mediaplex.com/ad/ck/14302-167341-17235-399?mpt=[CACHEBUSTER]\"\">Click Here\"\"\"\"\"\nThis problem in turn leads to exposing a bug in q, related to \"relaxed mode\" string concatenation handling. I've fixed this in the master branch, and pushed it. \n. Fixed in v1.6.3.. Yes, it's supposed to (semantics is identical to sqlite), however, I haven't tested for this thoroughly.\nYou can force querying by the order of insertion by adding an ORDER BY ROWID if needed (similar to sqlite).\n. Fixed in v1.6.3. \n@svenXY I'm sorry, the arch linux version is still 1.5.0 - Someone else created the arch linux version for 1.5.0, and I'm not familiar with the process. If you still require it, ping me, and I'll try to dig deeper.. @svenXY sorry for the late reply - True, it's a bug, I've fixed it and pushed into master. Will close this issue when version 1.6.0 is officially out.\n. Fixed in v1.6.3. Hi @swsien127 \nYes, q supports unions. In general, all sqlite3 constructs are supported. See example below:\n$ cat a.data\n1 2 3\n4 5 6\n7 8 9\n$ cat b.data\n10 20 30\n40 50 60\n70 80 90\n$ q \"select * from a.data a where a.c1 = 1 union select * from b.data b where b.c3 = 90\"\n1 2 3\n70 80 90\n$\n. Sorry that i don't have a proper solution for this - q supports integers up to 2^63 (similar to sqlite). Numbers higher than that are treated as doubles, and hence are displayed as such.\nThe master branch's latest contains an additional parameter --as-text, which makes q treat all fields as text - You can download and use this version with this parameter and see if it can help with your specific use case.\nIf anyone has any good idea on how to handle this better, it'd be great.\n. Hi @hazemkmammu \nYou're right of course, it's a bug. I'm sorry but I don't have a quick solution for it - It's directly related to #12 , for which I haven't found a proper solution yet.\nThe number of issues related to #12 was quite low in terms of impact on existing users, so I haven't invested enough time in finding a good enough solution. However, your use case exposes the problem to the fullest, so I'll take a deeper look and see if something can be done.. @Unode Thanks! I'll take a deeper look during the weekend and merge it.\n. no, it hasn't actually. . Hi sorry for the late reply, thanks a lot for this!. merged it, awesome! thanks again, @serima !. This is interesting, it's related to the fact that q doesn't support universal newlines.\nThe root reason is the fact that rows are delimited by \\r (0x0d) instead of being delimited by \\n (0x0a). With universal newlines support, this would have been dealt with ok. However, it would mean  trouble in terms of supporting multiple encodings, and in performance.\nOne workaround, if you're using linux, is as follows:\n$ sed 's/\\x0d/\\x0a/g' TechCrunchcontinentalUSA.csv | q -d \",\" -H \"select company,numemps from - limit 5\"\nLifeLock,\nLifeLock,\nLifeLock,\nMyCityFaces,7\nFlypaper,\n$\nIf you're using a mac, then the standard sed command syntax is different, so the solution would just be to run brew install gsed, and enter the same command above, but with gsed:\ngsed 's/\\x0d/\\x0a/g' TechCrunchcontinentalUSA.csv | q -d \",\" -H \"select company,numemps from -\"\nIf you're using Windows, then it might be a bit more complicated, dependingn on the tools/editors that you have. I'm not familiar with anything specific, but perhaps one of the text editors might have a \"convert newlines\" capability which might help.\nHope that helps, please contact me again if you need more assistance.\nHarel\n. @EwanSadie - Added support for universal newlines. It's in master (not part of the newly released 1.6.3 version), but I'd be glad if you could test it and provide feedback. I'll be releasing a new version with this addition once several people take a look at it and provide feedback.. Thanks for the proposal @sweentown . However, I tend to agree with @serima that the pairing is better, and the cost of it is minimal. This might lead to complications in more complex queries, such as when joining stdin with some other table, and would require \"tricks\" in order to do right. The cost of optimizing for the simplest case is relatively high vs the benefits in my view.. Hi all, thank you for caring and for the discussion :)\nMy view on this is that it might be a nice convenience feature, but the fact that it kind of conflicts with a regular SQL semantics in which no-FROM means no-table, then I believe that the value it will be provide is not high enough to implement.\nHarel\n. Hi @aghasemi . Sorry, selecting from subqueries is not supported. As @serima noted (thanks!), the other issue might help.\nI'm considering adding support for this capability in the upcoming version.\n. hi, sorry for the late reply.\nAs @bitti wrote (thanks...), this is indeed an issue related to not autodetecting the column type as float. This is related to an old issue I've opened a long time ago but never got to solve ( #57 ). \nI will take a deeper look to see how this can be fixed asap.\nThanks,\nHarel\nBtw, obviously not a good enough workaround, but writing 205.0 in the additional row would have worked around the problem as well.\nIn general, checking which types q detected on a certain file can be done by adding -A to the command line, as follows:\n$ q -d , -H -A \"select * from textfile1.csv.txt\"\nTable for file: textfile1.csv.txt\n  `Value` - float\n  `Timestamp` - text\n  `MetricId` - text\n  `Unit` - text\n  `Description` - text\n  `Entity` - text\n  `EntityId` - text\n  `IntervalSecs` - int\n  `Instance` - text\n$ q -d , -H -A \"select * from textfile2.csv.txt\"\nTable for file: textfile2.csv.txt\n  `Value` - text\n  `Timestamp` - text\n  `MetricId` - text\n  `Unit` - text\n  `Description` - text\n  `Entity` - text\n  `EntityId` - text\n  `IntervalSecs` - int\n  `Instance` - text\n$\n. Sorry, it's not possible to provide a hint for the datatype. . @databasedude - Added flag for universal newline support (currently only for regular files, not supporting .gz or direct-stdout). It's not in the new released version, but it's in master and I'd be glad if you can test it and provide feedback.. True. There are two parts to it actually: The first part is detecting problematic column names, which exists, but the duplicate-column check was implicit (I've just pushed a change to the master which makes it more explicit and managed). The second part is what you're referring to, allowing to provide the column names externally. This is indeed a good feature.. Hi, I've released a new version for windows 1.7.4 which solves this extra warning issue. Just download the new windows version from the site (remember to refresh the page properly, so the new link will be used) and install it. The setup version should be setup-1.7.4.exe.. Hi @nilpix , as @bitti notes, this is an sqlite3 thing. If enough people would complain about this, I might add a custom round function to q which will work around this.. We're trying to push towards making python 3 compatibility (#109), so this will not be a problem.\n. Thank you all for the discussion. \nRegarding limiting to reading from stdout - While autocompletion is definitely valuable, i believe it is not supposed to be the driving force behind the decision to support real filenames as opposed to only stdin.\nI do think that supporting both options benefits more people than supporting only one, not to mention use cases where reading from stdin is not enough anyway.\nRegarding python2 vs python - I'm not familiar with the standard - Is \"python2\" executable supposed to exist in all python2 installations?\n. @bitti @fxcoudert -I'd be glad to fix it. I'm not sure I fully understand what needs to be done specifically. IIUC, then there are two options. Please confirm/correct me:\n\nMake q fully python compatible - in that case, the shabang should remain as it is\nChange the shabang to something else as long as q only works with python2.\n\nAm I correct that these are the relevant options?\n. @fxcoudert @bitti please confirm/comment on #178 . I'm already adding several extension functions: regexp(), sha1(), and percentile().\nI don't mind adding several others, but i'd be glad to do it in one go, since i'm currently involved in other projects, and the overhead of issuing a version is not trivial (mainly because of the windows version actually...).\nLet's make this issue a placeholder for all the functions that people require, and then i'll add them.\nJust add a comment with your request.\n. @bitti About your suggestion to split the windows and linux version releases - The q site google analytics shows that 30% of the people that arrive to the site run on windows machines. While this doesn't necessarily indicates that this is the percentage of users, it does lead me to think that there is a relatively large base of users which are windows-based. This is why i would prefer the versions not to diverge.. Hi, q automatically supports the standard sqlite3 functions (see inside https://www.sqlite.org/). In addition, I've added user-function support for regexp and sha1.\nI do agree that the list of those user functions needs to be available, however listing all the available functions would mean duplicating sqlite3's documentation, and this is something that I don't think would be beneficial.\nI will change this ticket description to add an option to list all user functions.. thanks!. If i'm not missing something, then you need to add -H to the command line so the header row will be used to name the columns. If i'm missing something, please send the first few rows of the two files as an example, or some small dummy data, so i can understand the issue better.\nYou can also run the query using -A - This will analyze the data and show you the resulting \"table\" structure that q has determined, along with column names and types.. Nice, the temporary database option seems pretty interesting. \nI'll try to arrange that the next version provides a flag for it. If it proves to be stable enough and fast enough, then I'll make it the default in the next one.\nThanks!\n. This is interesting, since i have a lot of windows users that report much faster speeds (am i assuming correctly that you're using windows because of the reference to LogParser?). Can you provide more details? Machine Type, hard disk type, number of rows and columns in the file, perhaps a \"demo\" of the first 200 lines or so would help (if possible of course in terms of privacy. You could also send it to my email so it's not here in public). \nI have done some tests in the past regarding temporary sqlite files, but it's much slower than in-memory. Btw, the newest version of q has the ability to dump the parsed output into an sqlite db file (-S or --save-db-to-disk), so you can process the data inside sqlite itself if needed.\nbtw, i've created a file with 1M rows and 48 columns on each row (315MB), and querying it in q takes 1 minute and 5 seconds on my laptop (macbook pro). I'm attaching the file here, it would be interesting if you download it and measure several runs with this file, so we can have some rough comparison. The file is zipped for upload/download convenience, unzip it before checking.\nbtw, i'm getting lots of positive responses from users regarding q's relative speed, but obviously the tool is more optimized for convenience rather than speed currently. I've started fiddling around with replacing the processing mechanism in q to work with Spark, but it's not productizable yet.\nlarge-file.zip\n. awesome! thanks @simonw . btw, @simonw - Regarding \"-\" instead of \"stdin\" - You're absolutely right, but I do agree that it's not worth the trouble for now, since in many cases, backticks will be needed on the table names anyway, and I prefer the consistency over the clarity in this case.. Thanks @shigemk2 !. Hi, sorry for the late reply.\nSpaces are supposed only in column names with back-ticks, but you're right, this issue is the same as #4, spaces in table names and file names are not supported. There's no plan to add support for it now, as it's a relatively high effort for a use-case that can be easily worked-around in most cases (cat \"./Big Fat Disk/hashes.complete.txt\" | q \"select * from -\")\nI'll add this to the limitations, and close this as it's a duplicate of #4.. Hi, sorry for the late reply, and thanks for the kind words.\nThis error is actually a bug in pyinstaller for windows, which requires a workaround and new build of q in order to solve. I've already heard of some people encountering this error, but the fact that the output of this line is in stdout and not in stderr makes it much more severe.\nI'll try to arrange a windows machine for building q again without this error :)\n. Hi, I've released a new version for windows 1.7.4 which solves this extra warning issue. Just download the new windows version from the site (remember to refresh the page properly, so the new link will be used) and install it. The setup version should be setup-1.7.4.exe.. Hi sorry for the late reply, and for the inconvenience... \nI'm aware of this error, it's caused by a bug in the tool that packages q into a windows executable. I need to issue a new version work around that bug. This will happen in the next two weeks . Hi, I've released a new version for windows 1.7.4 which solves this extra warning issue. Just download the new windows version from the site (remember to refresh the page properly, so the new link will be used) and install it. The setup version should be setup-1.7.4.exe.. Hi,\nPart of q's concept is to integrate well with the linux command-line ecosystem, so using the relevant tools for filtering unneeded lines is preferred in my view. Here's an example with w's output:\nw | sed '1d' | q -H \"select TTY from -\"\nsed is used in this case in order to delete row number 1. If you need to delete several rows, you can pass a range to sed using '1,10d' for example. For osx users, download gsed (the gnu-sed) and use it instead. If you're a window user, then i fully understand that there is no good-enough solution for this, but i do not believe that going towards adding those capabilities into q itself would be the best approach.\n. Hi @dclong , I'm glad you find q useful. \nI agree with @bitti regarding fitting the unix philosophy, I believe that data cleanup/formatting should be done by using other, more suitable, linux tools. q doesn't attempt to replace the entire linux command-set, quite the contrary, at least from my viewpoint.\nRegarding the automatic column name, I don't really believe that there's a significant difference between choosing cN and _N, both are short enough and anyone can get used to either without any implications.\nAs for \"auto-completing\" column names, I believe that something like that would be less predictable and stable. For example, think about a query which uses a column prefix (e.g. phone) that runs on a csv file. If the user will run the same query on a newer version of the csv which contains another column with the same prefix (e.g. phone2), then the query would either break or provide incorrect results. Such behavior is not something I would expect from a tool such as q, and the benefits of such auto-completion are marginal in my view.\nThe main advantages of q from my point of view are:\n1. It can auto-detect column types and handle encodings properly, treating text as actual data and not just as \"characters\"\n2. It provides a very familiar interface - sqlite3's SQL is very powerful and yet very mature\n3. It integrates well with the inherent composability of linux tools.\nI'm not expecting q's traits to be suitable for every use case. All I can hope is that some users will find it useful for them.\nHarel\n. Hi @jameshfisher sorry for the late reply.\nActually, the file is a dummy file which I've used in order to show how to work with the tool. They examples weren't made to be executable (the original file actually contained sensitive data, so publishing it would not have been possible).\nI agree that it would be very beneficial if the examples were fully executable. I was under the impression that anyone looking at them would already have their own csv/tsv file that would want to tinker with as part of learning the tool.\n. While I understand the issue with kubectl's output, I agree with @bitti on this. Adding such a capability to q would be cumbersome, probably error prone, and a step in the wrong direction. kubectl effectively has a bug in its output, since it does provide multiple output formats, but doesn't fully provide a table format that is machine-parsable.\nI would take another approach and create a more generic preprocessing capability that converts the visual table structure to a more logical one. \nHere's a short script I've written to convert visual-space-delimited fields into logical ones. It uses the header row as a hint on how to split the other rows. You can use it as follows (notice that i've added the -d , since i've made the csv-size script hardcoded to create comma separated fields).\nkubectl get ingress | ./csv-ize | q -HOb -d , 'select * from -'\nBelow is the code for csv-ize. Obviously it could benefit from parameterizing some stuff and making it more robust to errors.\n```\n!/usr/bin/env python\nimport os,sys\nimport re\nimport csv\nw = csv.writer(sys.stdout,delimiter=',',quotechar='\"',quoting=csv.QUOTE_ALL)\nh = sys.stdin.readline()[:-1]\npositions = [0] + [x.start()+1 for x in re.finditer(' [^ ]',h)] + [len(h)]\ndef split_line(l,positions,trim=True):\n    do_format = lambda v: v.strip() if trim else v\n    return [do_format(l[p0:p1]) for p0,p1 in zip(positions,positions[1:])]\nheader = split_line(h,positions)\nw.writerow(header)\nline = sys.stdin.readline()\nwhile line:\n    line = line[:-1]\nw.writerow(split_line(line,positions))\n\nline = sys.stdin.readline()\n\n```\n. q accepts sqlite3 syntax constructs. Concatenating using sqlite3 is done by using \"||\":\n$ q 'select \"A\"||\"/\"||\"1\"'\nA/1\n$\n. thanks @bitti and @avilella for handling :)\n. @bitti would #!...python2 be available on all platforms? If yes, then it might be a great idea.\nAnyway, when I have a moment or two, i'll try to finish the make-q-python-3-compatible.... @Berkmann18 please confirm that your python is indeed 3.x. \nHopefully we'll soon support both python3 and python2, and i'll issue a new version that supports it.\nIf you wanna hack it for now, and you have python2 installed on your machine as well, you can change the first line in q's executable to #!/usr/bin/env python2 instead of just ...python.\nI'll update here when a new version will come out.. Hopefully, we'll have a python 3 compatible version soon. Until then, see my workaround above if applicable in your case.\nSorry for the inconvenience.. master is fully python2+3 compatible now. I'll try to issue a new release soon.. q has an output delimiter parameter that you can use (-D). Run q --help for details if needed. \nbtw, you could also control output for this specific case by using SQL string concatenation (e.g. q \"SELECT firstname||' '||lastname from ....\"). see sqlite's string concatenation info.. In order for q to use the header for the column names, you need to specify -H in the command line: q -H \"select ....\". i'd be glad to create a package for this, but i'm unfamiliar with the entire flow of releasing such packages. If you could elaborate on this, that would be great.. hi, yeah q supports glob, and actually also un-gzips .gz files provided to it as table names.\nThe bug itself is interesting, i'll definitely take a deeper look.. Tried some stuff, this is odd, since it seems that q is handling the file closing properly. \nWhen you test @bitti 's variation, using cat, you don't get any error, right?\nAlso, how many files are there in the folder?\nand can you send the results of cat /proc/<process id>/limits while q is running? You can find the process id of q through ps.\n. @peylight would you mind adding the needed information? I'm gonna be releasing a new version soon, and I want to add an fixes for this bug as well if possible.. @peylight kind reminder about this issue.. @peylight Would be glad to get your exact command line, and some screen shot/output of the files in your folder. thanks! indeed now i've managed to reproduce the problem, i did create lots of files, but i probably missed something in setting the file limit, since it wasn't reproduced earlier.\nThis happens because q opens the files in parallel, i'll take a deeper look to see how to optimize this so no one will get this error.\nThanks again\n. thanks for the kind words :)\ni always wanted to, but haven't got to it. It would require adding a python package to the current installation formats, since there are many people relying on it in Windows platforms as well (30% percent of users are on windows).\nI'll take another look at it. Thanks! I'll merge it.. Sorry for the delay, thanks a lot! I'll merge it. Wouldn't it be better to remove the allow_failures? That way the PR branch tests will fail until python3 tests are really passing (travis takes the .travis.yaml file on a per-branch basis, so the master should not fail).. sorry for the late reply, too many projects at once \ud83d\ude03 \nI agree that 2+3 compatibility would be great, although I haven't done any 2-to-3 production project.\nThe main issue to ensure is unicode conversion and testing, so q's multi-encoding capability will be fully preserved.\nI've started playing around with a python3-only branch, just for checking out how deep is the rabbit hole in that regard. I'll push it so you can take a look. The idea is to reduce the risk in that regard, and then do the changes in a way that will allow 2+3 compatibility.\nI'd appreciate any ideas or experience related to making the code 2-and-3 compatible with relation to encoding.\n. Sorry for the late responses. I really appreciate your interest and your well intent. I'm just too occupied with other stuff, but i'm very much interested in making this move, and i will definitely find some burst-time to invest in taking this forward.\nMerging the PR.\n. same here, thanks a lot, i'll merge it. btw, the main concern i have about moving to python3 is whether it will require modifying my (not good) packaging flow. But i guess it's time to move forward on this.. yeah, i agree, thanks for adding the python 3 tests in travis.. Turns out the major challenge with q with regard to python3 is to modify all the encoding related code to match the different python3 logic. Not sure it will be easy to provide a version that supports both 2 and 3, but I've started working on it on a separate branch called python3. I'll change the travis config so it runs python3.6 on the matching PR.\nYour advice and help would be very much welcome.\n. seems like tests are failing after the change.. Scraping this in favor of trying the incremental 2+3 migration approach. thanks @cclauss . Closing this since #195 contains full python2 and python3 compatibility.\nThanks @thomasgoirand . Hi, sorry for the late reply.\nI think it depends on your \"native\" encoding. In my terminal both work well, but i believe that this is because my native encoding it utf-8, so the copy-paste i did from your message above was utf-8 as well. \nI would need two things in order to try to help:\n1. Upload of the actual file that causes you issues (send it as a zip file, so there won't be any incidental encoding/re-encoding)\n2. Information about your computer - OS, native encoding, etc.\n3. the output of file X on this file on your terminal (assuming you're on osx/linux...)\nHarel. @rseemann kind reminder to this issue. I'd be glad to help if needed.. This seems like a delimiter issue. For historical reasons, q's default delimiter is not a tab, but a space.\nif your file is tab delimited, then just use -t or -d $'\\t' as a parameter, in order to tell q that it's a tab delimited file.\nOne important way to analyze the  column/type inference of q is to run it with -A, which shows the structure that it has detected on the data files.\n. just saw this as well \ud83d\ude04 pushing the fix now. @cclauss do you happen to know any best-practices regarding the float/double precision changes between py2 and py3?. Thanks a lot @cclauss ! i'll perform the fixes.\nAlso, forget about the float/double thing - It's actually a bug in q because of __str__() vs __repr__() differences that does not happen in py3... I believe I'll just fix the bug itself.\n. > https://github.com/harelba/q/blob/master/test/test-suite#L1999 file() was removed in Python 3 (there are other issues too).\nThe link you've send is pointing to master - i haven't merged the branch yet.. everything now works, with some minor breaking change which i'm not sure is critical - py3's universal newline behavior is such that it automatically succeeds to detect it even if it's not explicitly provided, so the breaking change is that even if someone doesn't provide the -U flag, things will go smoothly.\nOdd - now i'm seeing that python 3.6 passed for universal newlines, but 3.7 fails on it. checking.. @cclauss actually i did some. Nothing too scientific yet, but it seems that py3 is significantly better than py2. I've also done similar comparisons with textql, to see the differences between them in terms of performance.\nInteresting results, i'm still working on it, to make sure that results are consistent. Will publish when available.. Code is now fully compatible with both python versions, with a possible significant performance improvement. The next challenge is now to create proper packaging for it. I'll work on this in a separate PR after merging. Afterwards, I'll do some project structure changes in order to export q as a python module as well.. Regarding the performance gains, I'm performing a benchmark and will update once results are final.. Merging this. Note that there's one \"breaking change\" in terms of packaging - One cannot anymore download the q python file straight from the master and just run it. The reason is that now external python dependencies are required. I am deprecating the \"one-exectuable-file\" option, so I believe that this is a reasonable decision. The plan is in line with exposing q as a python module as well, which i'll address soon.\n. suggestion\n            print(\"Cannot encode data. Error:%s\" % e, file=f_err). suggestion\n    if (t == str or t == six.text_type) and ((output_delimiter in v) or ('\"' in v)):\nThis is a bug that i've found when experimenting with python 3 changes. If fixes the evaluation order to the expected one (unrelated specifically to p3). saw your important note regarding six.b and six.u not being used anymore. Figured that I'll change everything to be consistent, and then do a find-replace sweep and change all of those to the regular b and u.\n. the cursor returns byte strings for py2, and already-decoded strings for py3.. \ud83d\udc4d . \ud83d\udc4d decided not to do it for now, like you said in most cases this should be a non issue. I'll do some benchmarks later on about the difference between py2 and py3 behavior, and fix this if needed.. Thanks. Will do a complete sweep over all of those after everything works.. true. decided not to change it now, will do later on.. removed the unicode(x, encoding) code, thanks for noticing. As for the long/unicode checking, do you happen to have a suggestion on how to do that using feature-detection?. it's nice, but for some reason i use it only in other languages, and not enough with python \ud83d\ude04 \n. ",
    "yotamoron": "Well, I don't know :-(\nOn Fri, Feb 21, 2014 at 6:59 PM, Harel Ben-Attia\nnotifications@github.comwrote:\n\ndidn't mean to close it. Can it be reopened or should i open a new issue?\n\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/pull/2#issuecomment-35749898\n.\n\n\nAlways VSRE http://vsre.info/.\nTo infinity - and beyond!\n. ",
    "fuckinghuarong": "never mind, I misunderstand header_skip.\n. Don't you think --header-skip=HEADER_SKIP should NOT takes those lines into account in terms of structure?\n. ",
    "Niluge-KiWi": "Among other things Ubuntu 10.04.\nThanks.\n. ",
    "kc14": "In standard ansi SQL you can use backticks to quote table and column names. Would be great if you could do that in q, too. E.g.:\nq \"SELECT * FROM `a file with spaces.csv`\". ",
    "stuartcarnie": "Ahh, cheers mate \u2013 I'll update the formula to refer to your official tag now\n. ",
    "haidahaha": "If there is no space between filename, there is no need to put quotes around it, isn't it?\n. ",
    "thomas-riccardi": "Bump.\nThis is really limiting because the proposed workaround only works with one input file, and one major feature of \"q\" is to be able to JOIN files.\nI currently make symbolic links with compatible filenames as a workaround, but it's still not ideal.\nI think \"q\" should at least support back-ticks quoting like it's done for fields (in issue #44).\n. Thank you, I confirm it now works.\n. ",
    "Fil": "On a similar note I was wondering how one could reuse the generated db.\nChanging :memory: to q.sqlite and ending with db.conn.commit() instead of table_creator.drop_table() did the trick.\nCaching the data is not obvious, as you need to check if it's the same (could be the file's md5sum), and have some sort of garbage collection.\n. Also see #1 and #56\n. Yes, everything works!\n(I don't understand how anyone would need to keep a transformation such as \\\"a => \\a)\nclosed by ddf52cb04b2bc2afa530396097a2e89b29b94941\n. might be linked to the way q (mis)handles quotes (see #66 )\n. I can't reproduce the bug, neither with q1.4.0 nor with q1.5.0 (Mac OS X 10.9.5).\n. agree \u2014 but a duplicate of #56\n. Amen\n. $ hexdump -C utf16le.csv\n00000000  ff fe 75 00 69 00 64 00  09 00 6e 00 61 00 6d 00  |..u.i.d...n.a.m.|\n00000010  65 00 0a 00 e9 00 74 00  f4 00 6e 00 6e 00 f9 00  |e.....t...n.n...|\n00000020  09 00 68 00 69 00 0a 00  29 06 4a 06 28 06 31 06  |..h.i...).J.(.1.|\n00000030  39 06 44 06 27 06 09 00  68 00 61 00 0a 00 2d 4e  |9.D.'...h.a...-N|\n00000040  fd 56 09 00 68 00 6f 00  0a 00                    |.V..h.o...|\n0000004a\nAs you can see utf16-le has a two-bytes BOM \\xff\\xfe. Skipping it (as with utf-8) does not solve the problem.\n. For the record if I process it through iconv first, it works:\niconv -f UTF-16LE -t UTF-8 utf16le.csv | q -H -t -e utf-8-sig \"select uid from -\"\n. Thank you @harelba. I'm fine with using iconv, was just reporting this\nissue as a suggestion for the project.\nMaybe q should not be responsible for wild charset conversions, but detect\nsuch cases and offer instead an error message with useful suggestions -- or\njust open a pipe to iconv?\n-- Fil\nOn Thu, Mar 26, 2015 at 9:22 AM, Harel Ben-Attia notifications@github.com\nwrote:\n\nIt's due to a bug in python's csv module. It doesn't support encodings,\nand the standard way that python suggests to override it does not support\nNULs (q follows this suggestion currently).\nI've seen some ideas on how to overcome this, but I need to check them\nout, to see not only that they are working, but whether or not they impose\na significant performance penalty.\nI'll take a further look and try to see if this change can be made.\nIn the mean time, I'm sorry I don't have another solution, but you'll need\nto use iconv or something similar. If there's anything intermediate which\nyou think will help you, please tell me.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/harelba/q/issues/98#issuecomment-86394426.\n. \n",
    "bitti": "Yeah, as we all know, the 2 most difficult problems in computer science are cache invalidation, naming things and off by one errors.\n. qlib? But the confusion with Qt related libs might be too great. So qapi?\n. Option -T is for specifying TAB as an output separator. To specify it as an input separator you have to use -t. But is docker images really using TAB delimiters? Your quoted output only contains spaces. If not I would suggest replacing two or more spaces with a TAB before parsing with q. E.g.\ndocker images | sed 's/  \\+/\\t/g' | q -t -H 'select `IMAGE ID` from -'. @dclong I don't know what you mean by \"bracket\". And double quote strings works perfectly fine in sqlite (for which `q` is basically a wrapper for). But even if you want to use single quotes you can just escape them.\n\nOf course if you use double quotes for the q query parameter you have to be aware that backticks will trigger shell evaluations in bash (and probably some other shells). That's why I suggested to @harelba a while ago to use single quotes in the examples to avoid confusions which he did: https://github.com/harelba/q/issues/94#issuecomment-276610049.. There is already an -f output formatting option. So why not add an h parameter for it? Since it contains no equal sign it doesn't clash with the column formatting options.\n. Works fine for me in Mac OS X, but I experience the same issue under Linux.\nIn Mac OS X I encounter another minor edgecase when not actually selecting any fields:\n$ q 'select 5 from -'\n1 2 3\n^D\n5D\nSo it appends and additional D to the output for whatever reasons (doesn't happen in Linux though).\nThis (or piping via echo) works fine in either Linux and Mac OS:\nq 'select 5 from -' <<<\"1 2 3\"\n5\n. I took another look. Actually the D appending is also happening when selecting one field:\n$ q 'select c1 from -'\n1 2 3\n^D\n1D\nBut it disappears when selecting at least two fields:\n$ q 'select c1, c3 from -'\n1 2 3\n^D\n1 3\nSame thing happens when piping stdin through cat, so it doesn't seem to be related to the tty issue under Linux. So you might open a separate issue ticket for this.\n. Hm, seems you're right. When I redirect the output to a file the D doesn't appear, so it seems merely a display issue. I'm still wondering why it's only happening on Mac and not under Linux.\n. Works fine for me with\nq \"SELECT * FROM \"<(find . -ls)\nI strongly suggest not implementing features which unix provides out of the box.\n. That doesn't seem to be unusual for a unix tool. Why not use -d$'\\n'if you want to make sure that the whole line is used as a column? The only thing is that there is no option to suppress the resulting Warning: column count is one - did you provide the correct delimiter? but even that can be achieved with a 2>/dev/null redirection.\n. > When I see select, I first think about the select(2) syscall. When I see SELECT, I immediately think about SQL.\nBut SQL is case insensitive, so harelba has a point. I also don't like the idea of spoiling the namespace with all kinds of common keywords. ImageMagick did that mistake and we get annoyed dearly by that to this day.\n. That request doesn't sound very intriguing. You could just write a bash function or a wrapper script to achieve this. E.g.\nfunction SELECT { \n    options=()\n    while [[ $# > 0 ]]\n    do \n        param=$1\n        shift\n        case $param in \n            -*) options+=($param) ;; \n            *) query=\"select $param\" ;; \n        esac\n    done\n    q ${options[*]} \"$query\"\n}\nEven though there are quite a few unix tools which work differently when called with a different alias, it seems to be a mere syntactic shortcut in this case. \nMaybe an interactive mode would be more interesting? It couldn't work together with stdin as an input source though.\n. I think the \"UNIX\" way would be to use the LC_NUMERIC environment variable? Python also has an API to access the corressponding settings: https://docs.python.org/2/library/locale.html#locale.localeconv. But maybe q should provide an option to enable using that, because it would be a big change in behaviour.\n. Pipe the input through sed 1,3d. It's not UNIX philosophy to try to do everything in one tool.\n. I want to point out that there is a conflict between the \"don't be smart\" predictable behaviour of unix tools and the \"do what I mean\" approach. I don't think we should conflate them and sacrifice one for the other, because then we wouldn't get the advantage of either. E.g. a DWIM tool could be a simple wrapper around q which calls it with the \"right\" parameters. In this way the fuzzy logic parts would also be clearly separated.\nThe same comment applies for the header detection proposition.\n. Use single quotes for the outer quotes and replace inner single quotes with double quotes. That prevents shell evaluations. E.g.\n$ q -Ht 'select xsan from datafile.txt where `Product Group Description`=\"gl_boss\"  limit 5'\nA00KI83. Sometimes shell evaluations is what one wants. But it can be argued that in this case the user probably knows what he or she is doing and should know when double quotes are appropriate. So maybe using single quotes for the readme examples would be better?\n\nBtw. you even can use shell evaluations when using single quoted strings, just by simple concatenation. E.g. let's say you want to filter weekend_day by a shell variable $day but still using single quotes, you could do:\n$ q -Ht 'select * from datafile.txt where weekend_day=\"'$day'\" limit 5'\n\nOn the other hand with double quotes it would get a little simpler in this case:\n$ q -Ht \"select * from datafile.txt where weekend_day='$day' limit 5\"\n\n. I think this is in fact related to ticket https://github.com/harelba/q/issues/171. The solution is to use at least version 1.6.0 and providing the --as-text option.. A few GB might be still feasible, but if you get into the TB range I doubt that q is the right tool for that. I always found q a nice postprocessing tool, e.g. when querying the results of some mapreduce jobs. It certainly is not suited to do the processing of large files itself.. What's more common? That someone really want's to read a CSV which such big fields or that one happens to read a CSV which is wrongly quoted and would rather prefer to get an error message?\n. Just nest it like this:\nq \"select * from \"<(q \"select * from ./tmp.txt\")\" a\"\n. You can use -O for output headers and -H for input header detection:\nq -H 'select avg(a.cost) as cost_19, avg(b.cost) as cost_0, avg(a.qcost) as qcost_19, avg(b.qcost) as qcost_0 \nfrom \n'<(q -O 'select c37 as url, avg(c3) as cost, avg(c10) as qcost from 19.csv group by url')' a \njoin \n'<(q -O 'select c37 as url, avg(c3) as cost, avg(c10) as qcost from 0.csv group by url')' b on a.url == b.url where \na.cost > b.cost'\nBtw. 19_cost etc are not valid qualifiers since they start with a number.\n. why would you want that in q when you just can pipe your input through sed '/^#/d' therefore keeping the tools orthogonal according to UNIX philosophy?\n. I don't know how windows is bearable without cygwin, but even stock-windows can do something trivial like filtering a text file by a pattern: https://technet.microsoft.com/en-us/library/bb490907.aspx.\n. Even though the term \"CSV\" comes from \"Comma Separated Values\" the file extension \"csv\" doesn't define the delimiter. From https://en.wikipedia.org/wiki/Comma-separated_values:\n\nIn popular usage, however, the term CSV may denote some closely related delimiter-separated formats, which use a variety of different field delimiters. \n\nThat's also the reason you have to specify the delimiter when you import a .csv into a spreadsheet application.\n. That being said: there is already a feature request for automatic delimiter detection (https://github.com/harelba/q/issues/93). Even though I argued against it I guess it would be fine to have an explicit flag to have at least a detection for the most simple cases.\n. I don't really know about CTEs but it seems their usecase which can not be achieved with subqueries is in recursive queries. If you don't need recursive queries you should be fine with using subqueries, which are not supported by q in the from clause but there is a workaround: https://github.com/harelba/q/issues/102. If you don't care (or know) about the exact number and just want to have a certain fraction you also could do something like this (here with a sample rate of about 1%):\nseq 1 1000000 | q \"select * from - where random() % 100 = 0\"\nThat's also slightly faster in my measurements.\n. @sweentown: If you really want to join stdin with itself you need to alias it, e.g.\nselect * from - join - x\n\nBut I think @harelba was referring to the more common case of joining stdin with another file.\nIf using simple selects on stdin is a common usecase for you, you can define a simple script or bash function like this:\nqq() { q ${@:1:$#-1} \"select ${!#} from -\"; }\n\nAs a bonus this example also allows you to drop the select prefix.\n@serima: Sure, but if you leave out from that also doesn't imply stdin as table but simply no table at all. That can be useful for queries like select date() (or just to test some sqlite functions) and is already working fine in q. So trying to imply stdin here would change that semantic and would contradict the SQL standard.. @sweentown: In fact the argument should be the other way around: if leaving out \"from\" would not be part of the SQL standard, we could think about giving it a special interpretation since there should be no conflict with existing usage (except that some might rely on it to fail).. It seems q gets confused when integers and floats are mixed in the same column and it falls back to string interpretation in this case (7 is the biggest first character in your example and hence 7.33 the \"maximum\" string). To force numeric interpretation you can just modify your query a little bit though:\nselect Timestamp,Entity,max(Value+0) from .\\textfile2.csv where Unit='%' and Entity='CHBARRYCAPP1' group by Entity, Timestamp. That doesn't seem like a q issue, but like a sqlite issue:\n\n$ sqlite3\nsqlite> select round(32655.5265, 3);\n32655.526\n\nIn fact that might be pretty normal, since decimal fractions can not be exactly represented as floats, the internal representation of 32655.5265 might be less than 32655.5265. Since I don't see any way to cast a value to a decimal in sqlite I guess the easiest workaround is to add a small fraction before the rounding to mitigate any float inaccuracies. I.e. round(avg(c3)+1e-10,3).. @patatetom I don't understand the suggestion. Your want to disallow from filename syntax just to force everyone to use stdin? how do you even join different tables then?. I don't even understand how this is a q issue. Autocompletion is a feature of the shell. And in bash it just works fine for me if I use enter instead of space before the filename:\n$ cat >filename\nsome test\nfor autocompletion\n$ q 'select * from\n> filename ' # Pressed <tab> after fi\nsome test\nfor autocompletion\n\nThat works with either single or double quotes. For zsh you probably have to configure it, which should be possible considering its much better autocomplection capabilities.. @patatetom If using selects on stdin is your standard usecase I provided a script at https://github.com/harelba/q/issues/139#issuecomment-276828149. Enhancing it to support where and order clauses would be more involved but certainly possible.. @fxcoudert I mentioned that quote already some months ago: https://github.com/harelba/q/issues/172#issuecomment-403627334. But maybe @harelba will hear us now?. I don't think there is really an option. Nr. 2 has to be done since Nr. 1 more involved and can't be done right away (even though it's not that complicated). On my system I had to edit the shebang line manually to keep q running, which is an unfortunate state.. Does every release has to include a Windows build? And how many people are actually waiting for a Windows release compared to a Linux/Unix release? Why not make more frequent releases for Unix and some infrequent releases with a Windows build?. I cannot reproduce this. Can you provide the output of cat -t myfile.tsv, q -t 'SELECT * FROM myfile.tsv'|cat -t and sqlite3 <<<.show?. I see, I only tested with q 1.4.0. With q 1.6.0 I can reproduce the problem as well so it seems to be a regression.. It still doesn't make sense, since your example doesn't use a range but a single date only. Why not just filter by where c1 = '2017-07-20'?. I never saw this + syntax, is that a sqlite feature? Anyway, q -d, \"select * from h.csv+d.csv\" seems to work fine, no need to provide column names explicitly. You can also use q -d, -O \"select c1 as f1, c2 as f2 from d.csv\" to avoid creating an intermediate header file.\nBut why don't you want to use cat? Its exactly made fore usecases like that and you don't even need q for it.. 1. q follows UNIX philosophy, therefore it's supposed to be predictable rather than trying to be smart. That also means it works great together with other UNIX tools. If a tool breaks that contract by outputting superfluous ornaments you need to write a special filter for them to massage the output into a proper format. But even such tools often have a flag to produce a machine-readable output, therefore you should study their documentation carefully before investing time in writing a wrapper.\n\n\nWhy would that be an advantage? You could as well have a table with _1, _2 etc as column names. And if you have column names you might as well read them in with -H.. The b option is included for convenience but in fact violates Unix philosophy. You can get a similar result by piping q output through column -t, so it's not strictly needed. Therefore I think adding another option on the input side would be a step in the wrong direction. Instead I suggest making an input transformation to distinguish between column separators and literal spaces:\nkubectl get ingress | sed 's/  +/\\t/g' | q -tHOb 'select * from -'\n\n\nOr instead of -b use the suggested column command:\nkubectl get ingress | sed 's/  \\+/\\t/g' | q -tHO 'select * from -' | column -t -s$'\\t'\n\nThis also leads to a slightly different spacing (which might be preferable).\n. It seems you need to use at least version 1.6.0 of q, which supports an --as-text option. If you also have columns which should actually be interpreted as ints that would make things more complicated, but at least a workaround which casts should work then.\nIf you have a very simple query you could also consider the pragmatic (but technically wrong) approach of leaving out the -O and -H parameter and therefore force a text interpretation of all columns by q (assuming your header column names are all text). Of course then you would need to use c1 etc. to reference columns in the query.. Works fine for me:\n```\n$ brew install q\nUpdating Homebrew...\n==> Auto-updated Homebrew!\n[...]\n==> Downloading https://github.com/harelba/q/archive/1.7.1.tar.gz\n==> Downloading from https://codeload.github.com/harelba/q/tar.gz/1.7.1\n################################################################## 100.0%\n\ud83c\udf7a  /usr/local/Cellar/q/1.7.1: 4 files, 81.5KB, built in 2 seconds\n```\nWhat does python --version yields for you? I get the system default version Python 2.7.15. Note that q is not Python 3 compatible yet, when I change my virtualenv to a Python 3 environment I get the same error as you. That can be considered a bug, since users of q shouldn't have to care if it's implemented in Python.\n@harelba: I suggest to replace the shebang line #!/usr/bin/env python with #!/usr/bin/env python2 till Python 3 compatibility is achieved. . What do you mean by \"dumped\"? Deleted /usr/bin/python? I think Mac OS doesn't really rely on the system Python being available, although there might be a few old obscure scripts which do (like /usr/bin/smtpd.py). There might be some 3rd party apps tough which rely on the system python though.\nBut I guess by \"dumped\" you mean you just changed the default path python, since modifying /usr/bin would require disabling the system integrity protection under a recent Mac OS. Indeed I just rechecked and noticed that my Python 2.7.15 is coming from a brew installation, the actual system python in /usr/bin/python is Python 2.7.10. So I have the same situation as you, my default path python is not using the system Python with the difference that it's still a version 2 python.\nSo if you insist to have Python 3 as a your default python I guess the only option is to either write a wrapper for q which changes the default environment to python 2 or to directly change the shebang line of the /usr/local/bin/q executable (which has the disadvantage of being overwritten on the next brew upgrade when the issue is still not fixed in the next q release).. There is no guarantee, but PEP-0394 clearly states\n\nUnix-like software distributions (including systems like Mac OS X and Cygwin) should install the python2 command into the default path whenever a version of the Python 2 interpreter is installed\n\nSo even though according to the spec the python binary should point to a python 2 interpreter if available, this can not be expected anymore since quite a while, therefore:\n\npython should be used in the shebang line only for scripts that are source compatible with both Python 2 and 3.\n\nOn Mac OS I can not see a python2 executable in the default system path /usr/bin though, only a link for python2.7 (which probably gets installed with Xcode). But this could be easily remedied by the Homebrew people by adding a python 2 as dependency for the q package (which then also installs a python2 executable as per spec).\nBut no matter how we slice and dice it, the current shebang line is broken according to this PEP unless q is python 2 and python 3 compatible (which of course would be the best solution if it could be implemented in a reasonable time).\n. The stuff in /usr/local is user installed and not part of the system default installation. In my case it's just a link to a homebrew installed Python, probably in yours too:\n$ ll /usr/local/bin/python2\nlrwxr-xr-x  1 david  admin  37 Jun 11 17:12 /usr/local/bin/python2@ -> ../Cellar/python@2/2.7.15/bin/python2. You know q but don't know about jq!? \ncurl -s https://api.github.com/repos/harelba/q/releases/latest \\\n| jq -r '.assets[].browser_download_url | scan(\".*deb$\")' \\\n| wget -qi -\nSeems simpler and less fragile.. Don't use single quotes to reference columns or you just get literal strings. Just leave the quotes off or use backticks instead if you have column names with special characters. Also see https://www.sqlite.org/lang_select.html. How is this q related? Can you give an example commandline which is triggering this?. Interesting, I didn't know q supports fileglobs. But that's probably because I usually would never use them and do an explicit cat instead. E.g. in your example\ncat ./path/to/myfiles_* | q \"SELECT COUNT(*) FROM - WHERE c1 == 'sample'\"\n\nor\nq \"SELECT COUNT(*) FROM \"<(cat ./path/to/myfiles_*)\" WHERE c1 == 'sample'\"\n\nI'm sure cat is smart enough to avoid opening all input files at once. Still maybe @harelba is able to optimize the supported glob?. Yes long living side branches are seldom a good idea. Integrate as early as possible. At least there should be no hindrance to merge these changes since they are Python 2 compatible.. Why not delete the image on this occasion?. @mbrukman the image might be screenshot from that site. On the site itself I see only text.. The solution is not q specific but works in general in UNIX like systems. Just combine with another tool, here tr:\n$ echo \"x,1;y,2;z,3\" | tr ';' '\\n' | q -d \",\" \"select * from -\"\nx,1\ny,2\nz,3\n. ",
    "ritchielincoln": "@harelba have you looked at this? https://github.com/andialbrecht/sqlparse/\n. ",
    "thenoviceoof": "The only thing I don't know how to do is to auto-make a user-specific configuration file, but everything else should be pretty straightforward.\n. ",
    "juanpabloaj": "+1\n. @thenoviceoof I started this setup.py file for local installation with pip.\nhttps://github.com/harelba/q/pull/73\n. ok, I understand.\nfor now, I pushed this branch \nhttps://github.com/juanpabloaj/q/tree/from_github\nfor install q with pip from github with\npip install git+https://github.com/juanpabloaj/q.git@from_github\n. in package directory, with\npip install -e .\nyou can make a local installation.\n. @harelba maybe is interesting for you this posts about of how submit python package to pypi \nhttps://hynek.me/articles/sharing-your-labor-of-love-pypi-quick-and-dirty/\nhttp://peterdowns.com/posts/first-time-with-pypi.html\n. ",
    "webmaven": "@harelba Any idea when qtextasdata will be pip-intsallable, then?\n. :+1: \n. What if you split the q python package into a pypi installable package (although q is taken), and then made it a dependency of the system installable q-text-as-data?\n. OK, how about this then:\nSplit the q python package into a pypi installable package, and then pull it in here as a git submodule, rather than as a system package or Python dependency?\n. :+1: \n. What if you split the q python package into a pypi installable package (although q is taken), and then made it a dependency of the system installable q-text-as-data?\n. OK, how about this then:\nSplit the q python package into a pypi installable package, and then pull it in here as a git submodule, rather than as a system package or Python dependency?\n. A python distributable package is just a package directory (or module) nested in another directory with a bunch of extra stuff, which can easily be ignored.\nSo if the package is in it's own repo, I think that making it a submodule of this one should prevent most rewriting. It certainly won't need to be pip installed.\n. A git submodule is basically transcluded, you need ever only edit the python code once, in either location, but you would be publishing it twice, as both an exectuable command and as  an installable python module.  Which is your concern?\n. @Flimm: \n\n... too late, looks like someone already has grabbed the name q.\n\nYes, I noted that upthread: https://github.com/harelba/q/issues/24#issuecomment-64205247\n. Any suggestions for a different package name?\n. qball, quirk, qlever, qore, etc.?\n. > What do you think about the popularity of such a feature?\nI think it would be very popular. I would like to build and distribute small example apps with example data, and having SQLAlchemy immediately able to call into CSV data via q (or qtextasdata) instead of having to first load the example data into a db like SQLite would make for a much nicer experience out of the box.\n. ",
    "JKrag": "Hi Harel\nI hope this is a viable return address where I can reach you.\nIf you prefer, my direct email is jankrag@gmail.com\nI am actually just hacking on the next improvement right now, or at least\ntesting an idea.\nIt seems to me that it is not always useful that the output uses the same\ndelimiter as the input, so I am trying to add an --outputdelimiter option\nas well.\nRegarding the more complex \"bugs\" that I might not be up for attacking\nmyself right now, the following two stick out:\n1) Handling \"delimiter\" chars within quoted strings.\nThe test data I am \"attacking\" was generated by a little webscaper script I\nwrote during christmas in Python, and uses Pythons csv package to write the\ndata to csv. I don't know if this is configurable, but the way I used it,\nit used comma as delimiter, and quoted any strings containing comma or\nother \"weird\" chars.\nAfter scraping 200,000 cats from an online DB (our cat club), I found your\ntool and started using it. (my plan is to dump the data in Neo4J, but right\nnow I am just looking at the data for my own fun).\nUsing q, I found out that it doesn't handle the quoted strings at all, i.e.\nit finds far too many columns in some of the data lines.\nReading your code, this is not the first place that I feel comfortable to\ndeep dive :-)\nI solved my own problem by converting my data file to pipe-delimited,\nwithout quoted strings, and verifying that all 200K rows now have exactly 7\ndelimiters.\nThis conversion was not easy either, and I ended up using the long\ncsv2pipe.awk script found on this page:\nhttp://www.unix.com/shell-programming-scripting/100633-convert-csv-file-double-quoted-strings-pipe-delimited-file.html\nwhich just shows that it is not a trivial problem, but might give you some\ninspiration :-(\nWhile writing this, I have just started thinking, that maybe Pythons csv\nmodule could also solve the reading/parsing problem? Or have you already\nexplored this route?\nAn example from my original comma-separated file could be:\n192176,M,\"Wishstar Triumph, DM\",PER n 03 23,13-07-1996,CFA 1142-1086866\nV1298,163224,192169\nThis is not the typical case, but a \"bad\" line including quoted string and\nextra comma.\nThe same line in my pipe converted version is:\n192176|M|Wishstar Triumph, DM|PER n 03 23|13-07-1996|CFA 1142-1086866\nV1298|163224|192169\n2) The second problem I have, which I am seriously debugging right now, is\nthat sometimes my output gets really weird when writing any output that\nuses the \"last\" column in my data (c8), or using SELECT * FROM.\nIn the simplest case, where I only output numerical data and static text,\nit decides to move the last char in each line to the beginning of the line.\nI have attached a little sample file with the first 1000 lines of my data.\nTo try the simplest possible example, I wrote the query:\nq  \"SELECT '(' || c7 || ')' FROM p1k.csv \"\nwhich works fine:\n()\n(168756)\n(173119)\n(173119)\n(779)\nbut if I just try using column c8 instead:\nq  \"SELECT '(' || c8 || ')' FROM p1k.csv \"\ni get:\n)147\n)187004\n)122427\n)193551\n)\n)516\n???????\nOnce I use more complex queries, with multiple columns, I get really\nstrange behaviour :-D\nq  -b \"SELECT * FROM p1k.csv WHERE c4 LIKE 'PER a%'\"\ngives something like:\n|    |    |    |    |    |    |                 |PER a 03    |...\n|NRR RX 32107    |150313|152031\n|    |    |    |    |    |    |    Clouds       |PER a       |30-08-1996|FD\nLO 89243     |149618|190099\n|    |    |    |    |    |    |    oon          |PER a 02 62\n|05-08-1997|SVERAK LO 148954|149618|190062\n |    |    |    |    |    |    |                |PER a       |...\n|NRR LO 133261   |871   |10340\n   |    |    |    |    |    |    |              |PER a 33    |...\n|NRR LO 131052   |262   |972\n    |    |    |    |    |    |    |             |PER as 22 62|...\n|FHK LH 2009-14  |12640 |18\n |    |    |    |    |    |    |                |PER a 03 23 |31-12-2000|FD\nLO 112920    |187957|13818\n   |    |    |    |    |    |    |    ttle Girls|PER a\n|28-06-2002|NRR LO 133483   |134932|678\n |    |    |    |    |    |    |                |PER a 01 62\n|24-05-2000|(N)NRR LO 127750|129842|24552\n|    |    |    |    |    |    |                 |PER a\n|17-04-1998|NRR LO 122730   |136287|187191\n|    |    |    |    |    |    |                 |PER a 22\n |28-05-1997|NRR LO 119694   |167738|174757\n  |    |    |    |    |    |    |               |PER a 33    |...\n|SVERAK LO 109548|124381|9647\nI think I have tried at least a hundred different queries in an attempt to\nget a systematical grip on the behaviour, but haven't reached a conclusion\nyet.\nMaybe you can figure it out knowing the code much better?\n3) Oh, and on a side note, i figured that it might be useful to have a \"raw\noutput\" option that uses the select statement to select the relevant lines,\nbut just dumps the original line back out. This could be very useful\n(piping output to a new file) for filtering large csv files, to get smaller\nsample files.\nIn a perfect world I guess this feature wouldn't be needed. Ideally, q\n\"SELECT * FROM n.csv\"  should return the exact original data if the same\ndelimiter is used for input and output :-D   (for a well formed file)\nOn the other hand it would be perfect for extracting good debugging samples\n(e.g. to send to you :-)).\nI might try to tackle this one myself once I work my way through the\n\"output delimiter\" feature.\nSorry for the very long mail (I decided to go with thorough issue\ndescriptions and samples :-) ) and thanks again for a very nice tool, and a\nchance to learn some more Python.\nI guess I could have reported these as issues on GitHub (and I will gladly\ndo so if you think it will inspire other users to help).\nRegards Jan\nOn 9 January 2014 20:59, Harel Ben-Attia notifications@github.com wrote:\n\nAs for the other fixes, i'd be more than happy to help and fix them or\nintegrate your pull requests when you send them.\nAny other comments and general impressions regarding the use of the tool\nwill be greatly appreciated.\nThanks again\nHarel\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/harelba/q/pull/17#issuecomment-31969751\n.\n\n\nJan Krag\njankrag@gmail.com\nTwitter: @jankrag\n. Forgot to update the README. This is fixed now in the new commit.\n. I'm not a license expert either, but during a webinar a few months ago, by Brent Beer from GitHub recommended: http://choosealicense.com/ to help choosing. (and as a followup, http://addalicense.com/ to make adding it easy).\nLast time I was involved in a discussion about adding a license (to the project gitall), I recommended the MIT license, as it is probably the simplest possible one, and therefore a good choice if you just want to let people use your code :-) without religious feelings).\n. Just a thought, connected to this suggestion:\nI have had a few cases where I actually wanted the opposite behaviour, i.e. i have sometimes been bitten by the rather loose column-count handling.\nMy thought was that we might make a case for at least the following \"modes\":\n1. loose: (default) The current loose implementation (as far as I remember its something like: test first line, then add 7 extra cols for safety)\n2. hungry: Specify number of columns, and have the last col. gobble up the rest of line. Perfect for log-file handling\n3. strict: infer number of columns from first line (or specify), and then fail if remaining data does not fit. This mode would be much better for \"normal\" csv data, and would provide a better fail-fast experience instead of just giving \"weird\" results/output.\nMode 2 (and optionally 3) would also need an added col-count parameter.\nWhat do you think of this Harel?\n. I think I can at least provide some input/answer to this one.\nAs you probably know, q works by storing the csv contents temporarily in a SQLite DB in memory. \nThe documentation for SQLite's avg() function states:\n\nThe avg() function returns the average value of all non-NULL X within a group. \nString and BLOB values that do not look like numbers are interpreted as 0. \nThe result of avg() is always a floating point value as long as at there is at least one non-NULL input even if all inputs are integers. \nThe result of avg() is NULL if and only if there are no non-NULL inputs.\n\nAs there is no advanced type-inference in q yet, all data is stored as strings as default. This means that an empty value is stored as an empty string and not as null. According to the SQLite documentation above, these values are therefore treated as 0 instead of null.\nThis will probably change once q gets type inference, but I guess you could temporarily work around the problem with some casting. \nAn alternative would be to add a temporary improvement that stores empty values as null instead of empty strings, but I guess this would have to be configurable to avoid breaking existing usage.\nThis should be reasonably easy to add. (I might even give it a go if I can find the time.)\n. I guess this is a duplicate, but better described version, of https://github.com/harelba/q/issues/8\n. ",
    "NickBeeuwsaert": "SQLite3 defines a REGEXP operator, but it doesn't provide a function for it, so you can do Y REGEXP X and sqlite will try and execute the undefined function regexp(X,Y)\n. ",
    "zackp30": "Hmm, this seems to now produce a user defined function raised exception error.\n. ",
    "anantasty": "good catch @zhanxw \n. ",
    "dsuch": "Hello @harelba - it was linked to on Planet Python http://planet.python.org/\n. Thanks for adding it @harelba but this is a very interesting choice for such a low-level library. I guess it will severely limit its usage, i.e. only other GPL software packages will be able to use it.\nOut of curiosity, what made you pick this particular license? Why not MIT, BSD, Python or LGPL?\n. Cheers @harelba - my take is simply to use BSD/MIT/LGPL for projects that can be used by other programmers and GPL for end-user products.\nIn this particular case I'd like to possibly use q in Zato https://zato.io/docs/ which is LGPL. \nThe context is that I'd like to run SQL queries against incoming CSV data posted over HTTP.\nSo like you see, this is quite far from your original intended usage but your project has potential to be reusable in many places. Command line usage is but one thing among many others.\n. ",
    "h5rdly": "so.. what's the license?. ",
    "hannes-brt": "+1\n. ",
    "Flimm": "I haven't looked over it in detail, but I would recommend going ahead and putting on Pypi before someone else grabs the name q.\n... too late, looks like someone already has grabbed the name q.\n. I have the same issue, and I could invest the time needed to fix it if you want.\n. ",
    "michaeljoseph": ":+1:\n. ",
    "cemeyer": "Also you don't need BuildRoot: anymore.\n. Also 'Requires: python-libs' should probably be dropped... the python interpreter already requires python-libs.\nMight want to take a look at https://fedoraproject.org/wiki/Packaging:Python .\n. rpmbuild -ba q.spec (\"build all\").\n. Correct. Sounds good to me.\n. ",
    "dalemyers": "So I've looked into it further. It doesn't seem to matter the file. What causes it to break is when I use the '-t' flag to use tab as the delimiter. \n. That sounds pretty awesome. I was planning on having a look into doing it my self this weekend, but it seems like you are already way ahead of me!\nThanks\n. Just going through my old issues and this is still open. I don't need this functionality any more, but I'm curious if it was ever added? . At the time, I was working with files a few hundred MB. It's enough that caching is nice if you are going back and forth multiple times in a script. . That sounds good enough for what I need(ed)!. ",
    "microamp": "+1\nAlso,\n\nLimit all lines to a maximum of 79 characters. (PEP8)\n\nwould make it more readable in my opinion.\n. ",
    "kannan83": "hi harelba, \nthanks a lot for your response. i really appreciate it. \nas you said, textql could be a little bit faster than q since yes it is written in compiled language, go. \nand supporting joins on files is on the way. \nanyway, 2 nice projects ... :+1: \n. ",
    "mikaello": "Do you have any plans to publish the comparison between textql and q? That would be very interesting to see :-)\nSince this issue was created textql has been updated, and now supports multiple files / directories. Are these projects even closer now than 4 years ago?. ",
    "chid": "+1 works well for me.\n. ",
    "alexprengere": "This works perfectly now:\n``` bash\n$ cat test.csv\na,1,0\nb,2,0\nc,,0\n$ cat test.csv| ./q -d',' \"select * from -\"\na,1,0\nb,2,0\nc,,0\n``\n. I completely understand the rationale. I think storing empty values as _null_ would really make sense, since there is no other way of providing _null_ values toq`.\n. This works perfectly now:\n``` bash\n$ cat test.csv\na,1,0\nb,2,0\nc,,0\n$ cat test.csv| ./q -d',' \"select avg(c2) from -\"\n1.5\n```\n. Yes I would be glad to do some testing when you release it. You could also push your development work in separate branches for earlier reviews and tests :wink:.\nI am very interested in the project, and did some similar work around \"queries-on-text-files\" with GeoBases. By the way, about the line parsing, if you want to support single/multiple/empty character(s) as delimiter (empty string to split on every character), you may want to take a look at this piece of code.\n. Huge improvements have been made with the latest commits, but the bug described in this issue is still there (problem with whitespace delimiter):\n``` bash\n$ cat test.csv\na 1 0\nb 2 0\nc  0\n$ cat test.csv| ./q \"select * from -\" -D ';'\na;1;0\nb;2;0\nc;0;\n``\n. I did some other tests, and indeed setting theskipinitialspacetoFalsefixes the problem described here. As you said, you could default that flag toTrue, unless the delimiter is whitespace, but that would break thels -l` use cases, where we actually want the spaces to be \"collapsed\" to have a proper column count.\nMy view on this is that:\n- simple things should be simple, so the ls -l proper parsing should not require to set an extra flag\n- the behavior of q should be consistent accross delimiters, to avoid confusing users\nSo my advice would be to always set skipinitialspace to True, even for the whitespace delimiter (which is the default one, so breaking changes should not be made lightly). Now, the CLI should allow to disable the skipinitialspace flag, since this changes what is actually loaded into sqlite, and users may want to have their content \"untouched\" in tge DB. Such a flag would also fix the current issue, for example:\nbash\n$ cat csv-with-spaces | ./q \"select * from -\" --keep-spaces -D ';' \na;1;0\nb;2;0\nc;;0\n. This works now:\n``` bash\n$ cat test.csv\nname,value\na,1\nb,2\na,5\n$ cat test.csv | ./q -d',' \"select name,avg(value) from - group by name\" -H\na,3.0\nb,2.0\n```\n. ",
    "petrux": "I'd like to contribute and do this stuff. As this would impact deeply on the project, wouldn't be better to merge the PEP8 pull request before? Thanks.\n. Hi @harelba. I was thinking of:\n- use a standard project structure as pointed out here\n- move all the .markdown files into a /doc directory\n- create an /examples directory for examples  \nFor all the building/packaging/distribution stuff I'm a bit ignorant, so I'm open to any suggestion. But I'd like to start with source code, tests, docs and examples. What do you think?  \nEDIT: during the weekend I'll be off, so I think I'll start working on this issue on Sun. evening or Monday. Have a nice week... and weekend, of course.\n. Hi there. Have you read my proposal? Any news? \n. ",
    "yunfan": "okay , will wait for that.\ni like this tool, it made us use the data based on logic not on some details\nalso the jq tool is another good one, i hope there are one for xlsx and output to xlsx\nwhich is the most final format of us programmer working for business department\n. ",
    "dclong": "I'm still experience an issue using q of version 1.7.1. I tried to use q to find outdated Docker images (from the output of docker images) but got a warning message. \n$ docker images\nREPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE\ndclong/jupyter-beakerx     latest              f56abf68f4a3        10 hours ago        1.55 GB\ndclong/jupyter             latest              80e8b5d0beba        24 hours ago        786 MB\n<none>                     <none>              d0ca316710aa        24 hours ago        963 MB\ndclong/jupyter             <none>              20f490cf873c        33 hours ago        963 MB\ndclong/jupyter-beakerx     <none>              8b7cbf8cd567        33 hours ago        1.73 GB\ndclong/jupyterhub-ds       latest              c3366002a3d8        47 hours ago        4.67 GB\ndclong/jupyterhub-tdodbc   latest              bfb4b244a144        2 days ago          1.87 GB\ndclong/jupyter             <none>              cdc06da454d2        10 days ago         1.18 GB\ndclong/python              latest              6f52f723366e        10 days ago         694 MB\nubuntu                     18.04               02f9d6707661        5 weeks ago         88.3 MB\n``\n$ docker images | q -T -H 'selectIMAGE ID` from -'\nquery error: no such column: IMAGE ID\nWarning - There seems to be a \"no such column\" error, and -H (header line) exists. Please make sure that you are using the column names from the header line and not the default (cXX) column names\n```. @bitti Thank you, bitti!\nI think it's best to use bracket to escape columns as\n\n\nIt best to quote queries using double quotes as string is quoted using single quotes in SQL. For this reason, double quotes to escape columns should be avoid.\n\n\nBacktilde is interpreted as shell commands in double quotes.. Thank you, Harel and David! This is very helpful!\n. Great, thanks a lot!\n. \n\n",
    "theomega": "Before merging we should discuss the default (off or on) and the naming of the switch \"-j\" is not the best choice, I'd like to use \"-h\" but that is used by the \"--help\".\n. ",
    "bonki": "This doesn't seem to have been pulled yet, what's the status on this? I'd also like to see this added but vote for keeping it off by default.\nAs for the short option, I agree that -j might not be the best choice especially since we already have -z for dealing with gzip'd files and we might want to add -j for bzip2 support later (not that we couldn't use another option for that, but I think it's sane to use the same as tar).\n. I have given this some more thought and propose another solution similar to grep --color:\n1. --header=never: Never display header.\n2. --header=always: Always display header.\n3. --header=auto: Display header if stdin is a tty, otherwise don't.\nIntroducing auto as sane default would also maintain backwards compatibility for scripts (that's also grep\u2018s default).\n. @harelba Yay! I agree, I don't think a single-letter option is needed since almost always showing the header in a terminal should be beneficial (or at least not annoying). The one circumstance I can come up with where it could be slightly annoying is if your data is brief (in terms of column width) but the attribute names are quite long possibly resulting in a few lines of header but very little actual data being output to the terminal depending on the query.\nE.g.:\nthis_is_an_attribute;this_is_another_attribute;and_another_fine_attribute_here;and_a_fourth_attribute_here;this_header_might_eventually_become_quite_long\n1;2;3;foo;bar\n4;5;6;qux;quux\nWe could work around that by having auto still skip the header if it is longer (in terms of characters) than (arbitrary number ahead) e.g. 150% the terminal width so you'd never see a wrapped header longer than one and a half lines, but this should probably be possible to override using an environment variable and/or .qrc to accommodate everyone's preferences. What do you think?\nAs for scripts, if you do want to force the header - the one-time additional overhead of a few added characters is negligible (and the long option helps with readability/maintenance, which is good), which is probably also why GNU grep chose not to have a short option for --color.\n. What you are looking for is -D $'\\t' which works as expected.\n. ",
    "jeroenjanssens": "Being able to output the header would be great! I'm personally not a big fan of @bonki's suggestion to also let Q deal with too wide headers. I can come up with two existing solutions if you don't want have long lines wrapped:\n1. Display on the first, say, 80 characters, using cut:\nbash\n$ q \"...\" | cut -c1-80\n1. View the output using less and chop long lines:\nbash\n$ q \"...\" | less -S\nThe second option also allows you to scroll horizontally. In any case, I believe it's important that the output should be predictable and not depend on the width of the terminal.\n. Perhaps I'm missing something, but could someone explain why the temporary table names have to be random? As far as I understand they only have to be unique. Wouldn't it be possible to just incrementing a counter when generating a temporary table name? \nOtherwise, in addition to generating a random name (either using a random number or UUID), it would be good to keep a list of generated table names and check whether the new candidate name is not already in that list.\n. :+1: Looking forward to 1.4.0!\n. ",
    "Vifon": "The ownership is a much bigger issue than the dates. You've stated in the copyright files that this program is a property of Free Software Foundation, which I doubt you meant.\n. ",
    "Xender": "Of course you don't need to notify FSF when your software is using GPL. However, transfer of ownership is something bigger, like Vifon said.\nFSF states that transfer of ownership is required when you want your program to be part of GNU project (I think other criteria must be met too).\n. By default, you have all the rights to the program, because you are creator of it.\nTransfer of those rights/ownership is by no means necessary for licensing you program/code under GPL.\nIt's necessary only if you want your program to be part of the GNU Project or an FSF project.\nIt's therefore perfectly fine to write your name as a copyright holder.\nOr, if you want to make Q an FSF project, I think you should start with reading their FAQs and follow all needed steps.\nAlso, by common sense - they should know that you give them the ownership rights, because what they need them for is intervening in case of GPL violation.\nI think these links could help:\nhttps://www.gnu.org/licenses/licenses.html\nhttps://www.gnu.org/licenses/why-assign.html\nhttps://en.wikipedia.org/wiki/List_of_GNU_software#What_it_means_to_be_a_GNU_package\nI hope that helps ;)\n. ",
    "Gasol": "Sorry, i didn't notice that has test to run.\n. I has just fixed, Could you please check again?\n. By the way, i don't clean up unused method.\n. ",
    "PreXident": "This screenshots summarises my problems with q on Windows:\n\nMaybe it would be enough to provide option to pass file with query as an argument and possibility to specify its encoding?\nI tried a little different approach in my fork and this is what I ended with (KDratio.bat creates KDratio.csv from tabulky.csv using q.bat and KDratio.sql). I am no pythonist, so there may be some fundamental mistakes. Names of columns by @theomega are already included and the filename lookup is also changed as I needed complicated nested queries which are not supported by current hack of hack :-)\n. I agree that backward compatibility is very important and I do not insist on using my changes, I just wanted to show other possible solution. Also this does not only support names with spaces but more complicated queries like 'SELECT * from (SELECT * from test.csv UNION SELECT * from test2.csv)' which I could not make to work with your hack. This will of course be no problem once proper SQL parser/analyzer is used.\nAlso do you think that support for queries stored inside files could be useful? It can be a workaround for command line encoding which seems to be quite a hell on Windows.\n. Great, thank you. A lot of things is so much easier in Linux :-)\n. ",
    "mikaelgrave": "Any idea when 1.4.0 will be released?\n. Thank you @harelba. I've just tried that last version on a 9,000-row CSV file, using comma as delimiter, and containing many cells with commas. It all worked great. The output does now enclose those fields within double.\nNote: I did not use the -w or -W parameters (yet). The default behavior worked well for me once I had specify -d ,\n. Thank you!\n. @harelba Thanks a lot! I'll gladly test it. I've just emailed you.\n. Thanks @harelba for fixing this so quickly.\n. ",
    "StreakyCobra": "With pleasure!\n1. To have both python 2 and python 3 installed on the same system, Archlinux team decided to rename the python 2 interpreter to python2 and keep python as python 3 interpreter. So packages that need to use python 2 need to be launched by /usr/bin/python2 and not /usr/bin/python. This is the common way to write PKGBUILD for python 2 packages. For instance official package ipython2 is doing the same.\n2. (Brief version) Archlinux packaging works as follows: There is 3 official repositories (named core, extra and community) managed by devs and trusted users. For unsupported packages, there is a central unofficial repository called AUR (for Arch User Repository) where users can share their packages. All repositories use the same build system based on PKGBUILD files, except that official ones provide directly binary packages (already compiled and packaged) and AUR\u00a0provides only the PKGBUILD files (the packages are compiled and packaged on the user's system).\nSo except if a trusted user or a dev want to support your package, it will stay in the AUR for the moment. Sometime TU or dev adopt AUR packages that interest them (or are widely used). But the quality of packages in AUR is really good and every Arch user is really used to build packages from here.\nSo to answer your question: unless a dev or trusted user want to adopt it, I'll manage the PKGBUILD for q in AUR (at least as long as I have the time ;-) ). If later I can not continue, the package will be orphaned and another guy can adopt and maintain it. When you release a new version, either A) I'll notice it because I have starred your project in git, or B) You create an account on the AUR\u00a0website, go to the q package page and press \"Flag package out-of-date\": I'll receive an email, or C) Just send me an email (it is on the top of the PKGBUILD). I'll do the job to update the PKGBUILD\u00a0accordingly.\n(not so brief finally!)\n. My pleasure @harelba. Thanks to the GitHub's RSS\u00a0stream for new releases I get noticed right away ;-)\n. ",
    "barsnick": "Hi Harel,\nOn Sat, Aug 16, 2014 at 13:15:14 -0700, Harel Ben-Attia wrote:\n\nHi @barsnick, sorry for the late reply.\n\nNo hurry! This is not our day job, right? ;)\n\nI would love a pull request for this. That would be great.\n\nDone. Thanks!\nMoritz\n. ",
    "vi": "$ printf '1 2 3\\r2 3 4\\r\\x04' | socat - exec:'bin/q SELECT\\\\\\ *\\\\\\ FROM\\\\\\ -',pty,echo=0 \n$ printf '1 2 3\\r2 3 4\\r\\x04\\x04' | socat - exec:'bin/q SELECT\\\\\\ *\\\\\\ FROM\\\\\\ -',pty,echo=0 \n1 2 3\n2 3 4\n$\n. @bitti, Probably \"1D\" means \"^D overritten by 1\". \"^D overritten by 1 3 is just 1 3, as it is longer.\n. I think the issue is of low priority. You can forward the issue to Python \"csv\" module maintainers and stop on that.\n. Oh, I forgot that you can just use Bash'es one for it by temporarily exiting the quotes.\nI think some example using this can be in the tutorial (with multiple tables, as one can be better handled by \"-\").\n. <(cat ...) is too trivial. Good example would be a conversion to CSV-like format for q's consumption.\nAnother example is adding headers to a file that does not normally have them:\n$ q -H -O \"select * from \"<(echo 'A B'; cat a)\" left join \"<(echo 'C D'; cat b)\" on B == C\"\nA B C D\n1 2 2 222\n3 4 4 444\n5 6\n. I think Warning: column count is one - did you provide the correct delimiter? should not be shown if user explicitly specifies -c1.\n. Another approach: decouple table name and file name, makeing the former only a default for the latter.\nExample syntax:\nq -O 'SELECT * FROM a LEFT JOIN b ON a.qqq = b.c1'  -d, -H a=a.csv    -d: b=/etc/passwd\nBefore the query there are input parameters for implicit (non-overridden) files.\n. I think the main advantage of q is ability to use it as a one-liner, without any \"environment\". I don't want new features (this issue, the caching) to be API-only and command-line interface to \"stagnate\". q should support complex things, but it's primary goal should be doing simple things really simply.\nFor-programmer q analogue can be a bit like F#'s type providers, bringing information about rows in a CSV file into tab completion in Ipython or an IDE. Such library can be a backbone for q, not a \"mode\" of q.\n. Do you like my idea of the interface for this? (with a=a.csv and such, see above).\nIt is:\n1. More or less intuitive;\n2. Backwards-compatible;\n3. Allows untangling the possibly complex filename (with spaces, backslashes, newlines, non-printable characters and so on) from the query line.\n. >  I didn't want select to be case sensitive, which would have been the case if the command itself would have been called \"SELECT\".\nWhen I see select, I first think about the select(2) syscall. When I see SELECT, I immediately think about SQL.\n. > What do you think about the popularity of such a feature?\nMaybe the presence of such feature can be taken for granted, but absence seen as an disappointing moment... I don't know.\n\nOne of the reasons adding modification commands is stalled is because this requires much smarter parsing of the queries, for which I haven't found a good solution yet.\n\nMuch smarter? Means handling UPDATE ([^ ]*) and DELETE FROM ([^ ]*) and INTO ([^ ]*) in addition to FROM ([^ ]*)?\nWhy parsing of the queries for updating can be smarter than for selecting?\n. Maybe such script SELECT (along with UPDATE and friends) can be a part of q's distribution. It can also concatenate its arguments for the more seamless feel.\nProbably it should also heuristically auto-detect delimiters and presence of headers, to avoid \"out of character\" -d: -H. The \"q\" script should retain usual, more strict behaviour.\n. Bump.\n\"How to calculate square root in sqlite\" talks about some \"extension functions\". Are them applicable to Q?",
    "PetrusZ": "I finger out it's data format that caused the problem, I opened the original data with hex, I found \"efbbbf\u201c in the front of file:\nhex\n  1 0000000: efbb bf22 6964 222c 226c 6576 656c 222c  ...\"id\",\"level\",\n  2 0000010: 226c 696d 6974 6275 7967 6f6c 6422 2c22  \"limitbuygold\",\"\n  3 0000020: 6c69 6d69 7462 7579 676f 6c64 626f 7822  limitbuygoldbox\"\n  4 0000030: 2c22 6c69 6d69 7462 7579 6578 7063 6172  ,\"limitbuyexpcar\n  5 0000040: 6422 2c22 6c69 6d69 7462 7579 6170 222c  d\",\"limitbuyap\",\n  6 0000050: 226c 696d 6974 6275 7970 7670 222c 226c  \"limitbuypvp\",\"l\nThis is  UTF-8 BOM, you can see here: http://en.wikipedia.org/wiki/Byte_order_mark\nIt seem a windows feature. I'm using debian 5.0.2 and ubuntu 14.04.\n. eh ... I have problem:\nbash\n $ q -H -O -d , 'select * from ./dailytasks.csv where typeid = 1' -A    \nCannot decode data. Try to change the encoding by setting it using the -e parameter. Error:'ascii' codec can't encode character u'\\ufeff' in position 0: ordinal not in range(128)\nI have tried -e, but it didn't work too. I' ll lcheck this later on my laptop.\n. Thank you so much! Using -e utf-8-sig works fine.\n. Thank you again, I'll run and test it.\n. :+1:\n. ",
    "oderwat": "File is sent! TY for your time!\n. Yeah.. just checked in an hex editor. I would never had guessed that they use BOM in that file. Actually I think it sucks and may even explain other problems I had in the past when giving those files to other people. Guess it is time to change adminers export code in my installation.\n. ",
    "LeDom22": "Yes, I will email it to you\n. Hello Harel,\nHere is a sample file. It is semicolon-separated and encoded in cp1252\nI have not tried yet but I guess that locales also impact date fields. If\nthey are parsed as strings, comparisons will fail.\nKeep up with this great work!\n- Dom\nOn Sun, Dec 14, 2014 at 11:08 PM, Harel Ben-Attia notifications@github.com\nwrote:\n\nAh... That's an interesting one :)\nCan you provide an example file - 10-20 lines will be enough, so I can\nanalyze what is needed?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/harelba/q/issues/80#issuecomment-66932268.\n. \n",
    "haozh": "hi\nI meet a similar problem. \nMy column contains string like \"033586\".\nq -A shows it is \"int\", so I get result \"33586\", \"0\" is missing. Is there a workaround?\nIf q support data-type row in csv/txt file or runtime parameter to specify data type, that will be a great help !\nThanks\n. Hi Harel\nThank you very much !\nPlease send windows version to 2951424955@qq.com\nHao\n. ",
    "leafonsword": "@harelba \nGolang?\n. @bitti \nThx~\n. @harelba \nI think following solutions are OK:\n1.nl file.text | q \u201cselect * from - where c1 > 3\u201d\n2.q \"select * from file.txt limit 3,999999\"\n. @harelba \nThx!\n. Hope support Python3 soon~. ",
    "Hunter-Github": "Thanks a lot for clarifying your ideas. The easiest way to resolve this issue might be amending the man page to direct an inquisitive reader at SQLite3's own complexity guarantees (or lack thereof).\n. ",
    "kmehkeri": "It works fine now. Thanks for the quick fix and a great tool!\n. ",
    "iambibhas": "Ah. That works. Thanks @harelba.\n. ",
    "ralhei": "```\nHi Harel,\nthanks for your reply. See my comments below.\nOn 14.02.15 10:10, Harel Ben-Attia wrote:\nHi @ralhei sorry for the late reply,\n  Thanks a lot for your input and your efforts. \n  In regard to installing using pip install - While it's another\n    easy way to install q, it collides with my plans regarding q and\n    python. I plan to provide q as a python module so q's\n    capabilities will also be exposed as an API (e.g. import\n    qtextasdata ...), \nMaybe I'm getting this wrong, but if you plan to provide q as a\npython module it makes even more sense to have a python installer.\nOtherwise, how would non-python-specialists know where to put the\nmodule so that it is found by the python interpreter???\nand I treat the command-line variation as \"non python\n    specific\", meaning that the fact that q is written in python is\n    incidental.\n  Using pip install for installing the command line variation,\n    although useful, contradicts it. For example, I'm not sure how\n    \"uninstalls\" are supposed to be performed by pip for q. Another\n    point is that in all other installation forms, q is installed on\n    /usr/bin/q and using pip it seems to be installed in\n    /usr/local/bin/q.\nMaybe it helps to think of installing q via pip as being equivalent\nto install it from the sources which you provide as zip or tar\nfiles. There the user is also free to install things either in\n/usr/bin/, /usr/local/bin or whereever. Also, no automatic uninstall\nis provided there either. \nFrom a OS point of view it is rather irrelevant whether binaries are\nfound in /usr/bin or /usr/local/bin, as the desired effect is that\nthe user can execute the tool from the command line without worrying\nabout its location.\nThere is a 'pip uninstall' feature, however, this indeed does not\nremove the q file from in the bin directory. Whether this is a flaw\nin my setup file, or a problem with pip, is something that I cannot\nanswer in the moment.\nHaving the setup.py file available is - even if you personally\nprefer a different way of installing - maybe just another convenient\nways for other users.\nFor example, in my company, people do not have root access to their\nmachines. So for them it is extremely helpful to have a way to\ninstall q in a virtual environment. It is a one line command. The\nsolutions with rpm/dep install won't work, the other ones requires\nsome additional possibly error prone steps, like\n- download the tar.gz file\n- unpack it in some locations\n- create a virtual env\n- find out where the bin directory of the virtual env is (e.g.\n~/.virtualenv//bin)\n- copy the q file from the untarred package into\n~/.virtualenv//bin\n- cleanup and remove the untarred package\nSo, you see, compared to pip install -e\n  git+https://github.com/harelba/q.git#egg=Package this is a\nlot of more work. \nThat's why I got inspired to contribute the setup.py file.\nBut, of course, it's your package and your decision, and I'm\ngrateful that q exists, it is a really nice tool.\nAll the best,\nRalph\nWhen I make the python module public, I will definitely\n    consider using what you're suggesting here so the q command line\n    can be installed alongside with it.\n  In regard to the license, you're right that it's not exposed\n    properly. The code does say which license is it, but it's not\n    exposed in the README/site. The license is GPLv3, similar to\n    many command line tools. \n  Hope you understand the reasoning behind my decision,\n    Harel\n  \u2014\n    Reply to this email directly or view\n      it on GitHub.\n.\nHi Halen,\nthanks for your fast answer.\nOn 14.02.15 11:33, Harel Ben-Attia wrote:\nHi Ralph,\n  I see your point about the virtualenv installation. \n  Also, in regard to my plan of providing a python module - The\n    plan is definitely to provide a pip install method of installing\n    q, but my plan is to change the project structure so it becomes\n    a \"standard python module\" and not just add setup.py. However,\n    adding a setup.py, perhaps in a subfolder so it doesn't clutter\n    the root project folder, can provide the \"pip instlal git://\"\n    capability without hurting almost anything. \nI've been following discussions of adding new stuff as a standard\npython module, very often it is a multi-year process with very\nstrict and high demands on the new package. E.g. one must ensure\nthat the API will stable for very long time, and often this is a\nvery hard thing to guarantee. It might also be rather an obstacle to\nfurther evolve a package. I think there are a few nice blog postings\naround which describe this issue.\nBut thanks for considering adding the setup.py file. Whether one can\nput that in a subdir is beyond my knowledge at the moment, I'm not\nsure what the search path for pip is within a package. Probably\nrequires some reading in the docs.\nThanks a again for your considerations,\nciao ciao\nRalph\nAlso, I'm hardly an expert on python packaging, but i believe\n    that providing an uninstall is probably just adding some stuff\n    to setup.py. I'll take a look at that.\n  I'll think about it some more and decide what to do.\n  Thanks for the input\n    Harel\n  \u2014\n    Reply to this email directly or view\n      it on GitHub.\n```\n. Nice approach - thanks for your hint!\n. ",
    "turtlemonvh": "@ralhei \nIf you want pip uninstall to work you can change setup.py from\nscripts=['bin/q'],\nTo\nentry_points={\n    \"console_scripts\": [\n        \"q=bin.q:run_standalone\"\n    ]\n}\nSee the \"Automatic script creation\" docs for setuptools for more info.\nI agree that a setup.py file would be helpful.\n. ",
    "maxharlow": "Yes that works -- thank you\n. ",
    "llagerlof": "+1. Nice idea. This q tool is already a must-have, but this feature would be awesome!\n. Understood, thanks.\nBut then the -d parameter should be always mandatory. Maybe q could raise an error or warning if the user not specify it.\n. Ok. I think I can live with the always-present -d option. \nAnd I will wait this automatic delimiter detection.\n@harelba, you can close this issue, if you wish.  Thanks.\n. ",
    "mahiki": "this is an interesting idea, since it warns about a suspected header row if you don't use -H.\nI'd like to add to this issue with an overlapping concern:\nI want to refer to columns with c1, c2, ..., cN while using -H to disregard the header row\nThe reason is that I often receive a file with spaces in the header row column names, for example this tab-delimited file:\nhead -n2 datafile.txt\nweekend_day MarketPlace Id  XSAN    Product Group Description   pg_rollup\n30-SEP-16   1   A00DVOV gl_towels   Hardlines\n30-SEP-16   1   A00KI83 gl_boss Hardlines\nmy problem\nI tried the backtick enclosure from from q issue #108, but I got the following:\nq -Ht \"select xsan from datafile.txt where `Product Group Description`='gl_boss'  limit 5\"\nzsh: command not found: Product\nquery error: near \"=\": syntax error\nTable for file: datafile.txt\n  `weekend_day` - text\n  `MarketPlace Id` - int\n  `XSAN` - text\n  `Product Group Description` - text\n  `pg_rollup` - text\nso the shell is evaluating the backtick statement as a command\nproposed solution\nThis would be easy to manage if I could just refer to the columns by c1, c2, ..., cN, however this will include the header row in the data. Therefore being able to indicate there is a header row, but refer to columns by column number would alleviate problems with parsing the column names for whatever reason. \ntroubleshooting info\nIt works fine with the following:\nq -Ht \"select xsan from datafile.txt limit 3\" \nXSAN\nA00DVOV\nA00DVIY\nA00KI83\nusing the -A flag:\n``\nq -HAt \"select xsan from datafile.txt limit 3\"\nTable for file: datafile.txtweekend_day- textMarketPlace Id- intXSAN- textProduct Group Description- textpg_rollup` - text. Thanks @bitti !\nI was using double quotes because of the readme examples, is there any reason those are \"  instead of '?. Adding to this issue, one common case is where the fields are actually not very large, but there are single quote or double quotes contained in the field. note to self: always clean data before processing!\nsed s:\\\"::g data.txt > tmp ; sed s:\\'::g tmp > data.txt. this is a pretty big missing functionality. something like this makes it easier to generate histograms, for instance. I've been using q regularly, such a perfect use-case. But now I want to do more, and my workflow is pushing toward copying file to local postgres db.\n\nIf q can't run CTE, and I have to migrate workflow to postgres local db, there is not much sense in continuing to use q because I would already have invested  effort in managing a postgres workflow for local csv files.. yes you are right that a subquery satisfies CTE functionality, and I don't need recursive CTE for generic use-cases in my work. I guess the note about sqlite getting CTE functionality gave me hope.\nI'm aware of the subquery limitation and the workaround, but based on your reminder I took another look at #102. It works great, although I have to lookup the quoting syntax pretty much every time.\nOn balance, this is easier than the steps of creating a schema in postgres, and copying the file in.\nStill -- if its possible to gain the new sqlite CTE functionality, it would be excellent.. ",
    "dkvasnicka": "Hi and thanks for the prompt response,\nIn this case I was writing the decimal fields without any computation but other whole-number fields were involved in some computation - so coercing all stuff into string isn't an option.\nFor now I've solved this using a custom script so this bug isn't a blocker for me.\n. ",
    "mrsarm": "@harelba I having a similar issue, a columns with code numbers, some of the numbers start with leading '0', but when i make a query, the command detect that the column is a number, and outputs the number removing the leading 0, what it's incorrect, becuase the code must have all the digits. How i can force 'q' to read all the columns or some of them as strings?. Thanks @bitti , it worked!. ",
    "ajmazurie": "I am revisiting this issue. I have GB to TB-sized CSV files I'd like to slice and dice using q, and would like to save the intermediate sqlite data object as a file for further processing. Best, Aur\u00e9lien. ",
    "ShinNoNoir": "In my case, it's because sometimes one of the fields can be very large. (I'm reading CSV files generated via the Amazon Mechanical Turk platform.)\nAt the very least, it'd be nice if the field size limit could be configured through a command line flag.\n. @harelba: I'm not in an immediate need of a modified version, but thanks for the offer. Also, I'll see if I can produce some sample data. (I'll have to go over it and anonymize some fields.)\n. ",
    "ckuenne": "I would like to bump this issue. It would be great if you could add this variable as a user configurable parameter.\nCarsten. ",
    "guludo": "Any update on this? :-) I'd love such a feature.. ",
    "kwarunek": "only python 2.x > http://harelba.github.io/q/requirements.html\n. ",
    "jungle-boogie": "Hello,\nUsing the single file, I'm able to use the cool script. I'd rather you focus on improving what you know rather than checking incompatibilities. ;)\nThanks!\n. Hi @harelba,\nThank you for correcting!\n. :+1: \n. I hope we don't have to wait much longer for q to work with python3.. Does sqlite3 depend on q having support for CTE? If so, maybe q was made prior to CTE's in sqlite3.\n. ",
    "joshua-wu": "That solution can not solve the problem like this:\n\"select avg(a.cost) as 19_cost, avg(b.cost) as 0_cost, avg(a.qcost) as 19_qcost, avg(b.qcost) as 0_qcost from \n(select c37 as url, avg(c3) as cost, avg(c10) as qcost from 19.csv group by url) a \njoin \n(select c37 as url, avg(c3) as cost, avg(c10) as qcost from 0.csv group by url) b on a.url == b.url where a.cost > b.cost\"\nAny Help?\n. ",
    "alexthehurst": "I believe this is fixed in my PR: https://github.com/harelba/q/pull/110\n. ",
    "garethrees": "Sorry, being an idiot. Didn't realise the default delimiter was space. Also passing -H and using the non -H header name.\n. Ah thats really helpful. Cheers! :beers: \n. ",
    "zaxebo1": "additionally, probably these will link will be also of some use to you : \n1) http://python-future.org/compatible_idioms.html   ( <<== Very very important)\nhttp://python-future.org/\n2) https://docs.python.org/2/library/2to3.html\n3) http://pythonhosted.org/six/\n4) http://python-modernize.readthedocs.org/en/latest/\nhttps://github.com/mitsuhiko/python-modernize\n. @NDevox:    +1\n. ",
    "NDevox": "What are the main roadblocks leading to this? I'm interested in helping if possible.\n. You have tests covering that right?\n. @harelba It has been a while for both of us ;-).\nHonestly, I ended up not using this lib although I found the idea really intriguing and can see myself possibly using it in the future (if it were python3). So I'm still interested in what is happening although I'm surprised this ticket popped back up.\nPackaging in python is notoriously bad. Something like go would be a better option if you really wanted easy x-platform packaging. But we're in python and a new language port is a hell of a task. But I don't understand why packaging is the issue for porting to python3?\nPyinstaller should run fine in 3.x.\nSo I'd argue the main issue shouldn't be distribution but just porting the code. Once ported it should be able to be distributed as before with maybe some minimal changes.\nWith that said, I've not run pyinstaller before, I do not know the niches. Do you have any documentation for running it?\nSo now a question: The code itself. I took a quick gander. Everything is in a single file, with no extension, simply called q, correct? Why? Is there a specific design choice for putting everything in a single file like that? I understand the want for a single executable but having all the code in a single file during development sounds like a coding nightmare to me personally.\nI may (strong emphasis on the may here) have a go at making everything compatible with 3 just as an experiment, but having a look at the codebase, and the emphasis on distribution right now is making me wonder what else needs to be taken into account first.\nFinally @leafonsword and @jungle-boogie if you have time feel free to try and port it yourself.\nAs I said i have no current vested interest in this project, just a casual interest. the codebase isn't that big so a port should be relatively simple to do.\nSorry for the lengthy reply, this project is clearly proving popular so I think it would be a shame for it to be stuck as 2.7 only. But I want to understand as much as possible before putting time in.. Tests are in a single file as well :stuck_out_tongue: \nI was under the impression that pyinstaller had the capabilities to merge everything into a single binary. Is this a specific problem for linux/mac?\nI've had an initial stab at this, trying to figure out how much effort it would take. My opinion is that, despite the codebase not being that large, the way it is structured is going to make it a lot harder.\nHaving a lot of tests is good. But as you say most of these are run through the binary - so act as integration tests. This means that the tests, when failing, do not give much valuable feedback - I don't know what is broken if they fail, just that there was an error, somewhere, in the codebase.\nI'm looking at this on my IDE and the actual obvious incompatible code is not a lot. The print statements need to be changed (realistically you should be using something better like logging in my opinion). Long no longer exists - but considering the use of SQLite you probably could just use int for everything anyway. Exceptions don't support Exception, e and instead need to be Exception as e. A couple import catches for moved modules. Dictionaries should just use .items. and use range instead of xrange.\nMost of these are really small. The biggest issue will likely be unicoode -> str as all strings are unicode and can trip people up.\nAll of these are easily made cross compatible.\nI don't personally think distribution would be an issue despite the worries on it. If the code can be written in a way that it supports 2.7 and 3.x at the same time the exact same distribution method can be used.\nThe issue I have is without being able to see where the tests break I cannot easily see the smaller hidden parts which need changing. I doubt there are many but as a non-user of this lib and not being knowledgeable of this codebase I don't necessarily think it would be worth my time when someone who knows the code could do it much faster.\nHonestly I like the idea of porting this project but it's more work than anticipated due to my lack of knowledge of the codebase.\nQ really needs three things in my opinion:\n\nUnit tests which interact directly with the code so you can see what goes wrong where and how.\nRefactoring to clean up the code - there are lots of unused variables, non PEP8 code, bits that feel unwieldy. More to the point ideally you'd split this up across multiple files (and combine them in the distribution process). It's ok to focus on the client experience but in reality the dev experience is still important. There are solutions which deal with both sides.\nPorting as discussed above.. \n",
    "serima": "@harelba Thank you! \n. Note: the diff with whitespace ignored.\nhttps://github.com/harelba/q/pull/129/files?w=1\n. I installed q via homebrew.\n% brew info q\nq: stable 1.5.0, HEAD\nTreat text as a database\nhttps://github.com/harelba/q\n/usr/local/Cellar/q/1.5.0 (4 files, 69.6K) *\n  Built from source on 2016-11-16 at 11:33:52\nFrom: https://github.com/Homebrew/homebrew-core/blob/master/Formula/q.rb\nReproduced it.\n% cat test1.csv\nheading1,heading2\nvalue1,value2\n% cat test2.csv\nheading1,heading2\nvalue3,value4\n% q -HOb -d, \"select a.heading1, b.heading2 from ./test1.csv a join ./test2.csv b on a.heading1 = b.heading1\"\nTraceback (most recent call last):\n  File \"/usr/local/bin/q\", line 1639, in <module>\n    run_standalone()\n  File \"/usr/local/bin/q\", line 1628, in run_standalone\n    q_output_printer.print_output(STDOUT,sys.stderr,q_output)\n  File \"/usr/local/bin/q\", line 1319, in print_output\n    self._print_output(f_out,f_err,results)\n  File \"/usr/local/bin/q\", line 1361, in _print_output\n    fmt_str = \"%%-%ss\" % max_lengths[i]\nIndexError: list index out of range\nzsh: exit 1     q -HOb -d,\n% q --version\nq version 1.5.0\nCopyright (C) 2012-2014 Harel Ben-Attia (harelba@gmail.com, @harelba on twitter)\nhttp://harelba.github.io/q/\nThen I cloned this repo and tried same thing.\n```\n% git show --summary\ncommit cfe2d047810cc54821bc3832e2076d373c643d0a\nAuthor: Harel Ben-Attia harelba@gmail.com\nDate:   Mon Jan 30 01:06:10 2017 +0200\nUpdate README.markdown\n\n% git br\n* master\n% ./bin/q -HOb -d, \"select a.heading1, b.heading2 from ./test1.csv a join ./test2.csv b on a.heading1 = b.heading1\"\nheading1,heading2\n% ./bin/q --version\nq version 1.6.0notreleasedyet\nCopyright (C) 2012-2014 Harel Ben-Attia (harelba@gmail.com, @harelba on twitter)\nhttp://harelba.github.io/q/\n```\nThis bug seems to be fixed in Ver 1.6.0. this commit? https://github.com/harelba/q/commit/a053b815312c0d8eadf6f42b47ddd6edbe1e4fb8. @harelba I also want to know when is the next release for brew packages.. I understand what you want to say, but in SQL, SELECT and FROM are always paired.\nIf q is designed based on sql, FROM - seems to be essential.. @sweentown Thank you for your advice. Certainly, if we do not refer to the table, FROM is optional.. @aghasemi Please see also #102. It may helps you.. @jitbit Just try to enclose the column name in double quotes like q \"select \\\"user-id\\\" from test.csv\". ",
    "jitbit": "Because I'm on windows ;)\n. @harelba  I don't want comments inside queries. I want to IGNORE SOME LINES IN THE FILE.\n. Oh, yeah, right, probably you are right, \"WHERE\" will work, but usually the \"column\" structure inside a commented line is badly formatted...\nPS. One of the reasons its a great idea is that typically log files (which I'm sure is 70% use case with your app) have some sort of commenting. Anyways, thanks for an awesome product.\n. Ok thanks!. ",
    "tphummel": "@harelba good to know. thanks for the great project!\n. ",
    "ryanmjacobs": "Hi @harelba, thanks for the quick reply.\nI just checked q --version and it says that I am indeed on v1.5.0.\nHowever, I just cloned the master branch and ran bin/q from there \u2014 that worked fine.\n. ",
    "ernestohs": "I have the same problem, in my case I don't have a header, the file size is ~9.1 MB and I can't find the \"issue\" on my data; I did the same as @ryanmjacobs and the version 1.6.0 works for me.\n. ",
    "rsalmei": "It's the -HO that crashes it...\nIf you run just q -H \"select * from file1 where age = 26\" it doesn't crash indeed, but q -HO \"select * from file1 where age = 26\" does.\nVersion 1.5.0 installed by homebrew. Could you fix it please?\n. Just do:\n```\necho 'asd zxc' > x.txt\necho '1 2' >> x.txt\nq -HO -b \"select * from x.txt where asd=-1\"\n```\nAnd you get:\nTraceback (most recent call last):\n  File \"/usr/local/bin/q\", line 1639, in <module>\n    run_standalone()\n  File \"/usr/local/bin/q\", line 1628, in run_standalone\n    q_output_printer.print_output(STDOUT,sys.stderr,q_output)\n  File \"/usr/local/bin/q\", line 1319, in print_output\n    self._print_output(f_out,f_err,results)\n  File \"/usr/local/bin/q\", line 1361, in _print_output\n    fmt_str = \"%%-%ss\" % max_lengths[i]\nIndexError: list index out of range\n. ",
    "marcobra": "I agree i spent about an hour to get a pipe delimited file working was the -t param was the issue \nit kill the -d delimiter param with no warning or doc about it:\nq -d \"|\" -H \"select cnum,canno,de_nome,de_recapiti,impo_2016 from ./myarchive2016.psv where de_nome like '%scu%' limit 3\"\nworks\n. ",
    "mmascolino": "Thanks for the response.  I'd be happy to test something once you have something ready.  Just let me know.\n. ",
    "nduc": "@harelba I've rebuild and tried running it but I'm getting an error\n\u03bb q -H -d , \"select * from *.csv\"\nTraceback (most recent call last):\n  File \"\", line 53, in \nTypeError: limit must be an integer\nq returned -1\nAm I missing a library?\n. I am using Python 2.7.11 on Windows 10\n\u03bb python --version\nPython 2.7.11\n. Harel, please, you have saved me many hours already.  It's the least I can do.\nI am on Windows 10 64 bit Home edition.\nBy rebuild, I mean I followed the instruction in Create Windows Setup Instruction in the dist folder.\nI've replace sys.maxsize with 131072 and I got an error\n\u03bb q -H -d , \"select * from *.csv\"\nquery error: table temp_table_10001 has no column named c1\nPython 2.7.11 (v2.7.11:6d1b6a68f775, Dec  5 2015, 20:40:30) [MSC v.1500 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sys\nprint sys.maxsize\n9223372036854775807\n\n\n\nAnd there's the problem.  I am using Python 2.7.11 64-bit!\nI tried it with 32-bit version\n\u03bb python\nPython 2.7.11 (v2.7.11:6d1b6a68f775, Dec  5 2015, 20:32:19) [MSC v.1500 32 bit (Intel)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sys\nprint sys.maxsize\n2147483647\n\n\n\nbut I'm still getting error when I build\n\u03bb python c:\\q-build-environment\\PyInstaller-3.1.1\\pyinstaller.py -F --distpath=win_output --workpath=win_build c:\\q\n\\bin\\q\nTraceback (most recent call last):\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\pyinstaller.py\", line 14, in \n    from PyInstaller.main import run\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\PyInstaller__main__.py\", line 21, in \n    import PyInstaller.building.build_main\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\PyInstaller\\building\\build_main.py\", line 32, in \n    from ..depend import bindepend\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\PyInstaller\\depend\\bindepend.py\", line 42, in \n    from ..utils.win32.winmanifest import RT_MANIFEST\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\PyInstaller\\utils\\win32\\winmanifest.py\", line 97, in \n    from PyInstaller.utils.win32 import winresource\n  File \"c:\\q-build-environment\\PyInstaller-3.1.1\\PyInstaller\\utils\\win32\\winresource.py\", line 20, in \n    import pywintypes\nImportError: No module named pywintypes\nI'm sorry if I'm wasting your time.  I really appreciate your efforts!\n. I started from scratch and made sure all my installs were 32 bit and I get a different error now.\n\u03bb q -H -d , \"select * from *.csv\"\nquery error: table temp_table_10001 has no column named c1\n. ",
    "rweisleder": "Joins work as they should, so this issue can be closed.\nThanks\n. ",
    "anthezium": "q version 1.5.0\nCopyright (C) 2012-2014 Harel Ben-Attia (harelba@gmail.com, @harelba on twitter)\nhttp://harelba.github.io/q/\n. Thanks for getting back on this!\nThe query works in sqlite3, so if you decide not to add CTEs (at least in the near term), you could just change this sentence on the site:\n\"All sqlite3 SQL constructs are supported, including joins across files (use an alias for each table).\"\nAnd add stuff to the \"Limitations\" section.\n. ",
    "eddified": "Thanks!\n. ",
    "svenXY": "Great that it's fixed, I can confirm it.\nSo - when do you plan to release 1.6.0 for Arch? There is another bug that you fixed as well for me since then...\nThanks,\nSven. I put a comment into the AUR page for q.\nThanks for fixing it anyway.\nSven. Cool. Thanks for fixing it.\n. ",
    "chenholmes": "--as-text helps me,thank you.\n. ",
    "m3ng9i": "Another way is to use -f parameter, example: \n```\n$ cat example.txt\n24161010220097026928    603.43\n42422402656113053824    327.95\n64951386600336086457    647.55\n36495639647086452073    119.20\n47026818054162399542    748.17\n$ q -d $'\\t' -f 1=\"%d\" \"select * from example.txt where c2>500\"\n24161010220097028096    603.43\n64951386600336089088    647.55\n47026818054162399232    748.17\n```\nBut when I replace %d with %s, the result becomes:\n2.41610102201e+19       603.43\n6.49513866003e+19       647.55\n4.70268180542e+19       748.17\nI writed some test code in python:\nprint(\"%d\" % 24161010220097026928)\nprint(\"%s\" % 24161010220097026928)\nBoth of the lines output 24161010220097026928, but why it's different in q?. ",
    "hazemkmammu": "Thank you very much. ",
    "cclauss": "Is this already fixed in master?. Once #184 is merged, we can rebase this PR and remove allow_failures.. The Travis CI docs say:\n\nAllowed failures are items in your build matrix that are allowed to fail without causing the entire build to fail.  This lets you add in experimental and preparatory builds to test against versions or configurations that you are not ready to officially support.\n\nThis sounds like our case with Python 3 so let\u2019s put these tests in place with allow_failures enabled (on both master and python3 branch) while the compatibility with Python 3 is perfected.  Outwardly Travis builds will continue to pass even though Python 3 compatibility is not yet achieved or guaranteed.. My advise is to accept this PR first.\nIt will not break anything and it will give you a good idea of how deep the rabbit hole is.. With a codebase that supports both, there can be experimentation to ensure that the port is complete and that the Python 3 packaging works as well as the Python 2 version.. My advise is don\u2019t do it on a separate branch.  The big bang approach is rarely successful on larger codebases.  The goal is a single codebase that supports both Python 2 and Python 3 so step 1 is to add Python 3 to your continuous integration in allow_failures mode.  Step 2 is to add this PR because it should not break compatibility with Python 2.  Use the CI and your own testing to make sure that is the case.  Step 3 is to make small changes to the encoding logic and make sure that each change does not break compatibility with Python 2.  In the end, you will have a codebase that you can flip back and forth between Python 2 and Python 3 to ensure that all functionality is equivalent.  Incrementalism is the key \u2014 Small changes with testing on Python 2 after each change.  Only after you are 100% satisfied that all functionality is equivalent on Python 2 and Python 3 should consider dropping support for legacy Python.. If Facebook can make a codebase that works in both Python 2 and Python 3 than you can too...  https://talkpython.fm/episodes/show/185/creating-a-python-3-culture-at-facebook. Done better elsewhere.. Can you please try to resolve conflicts?. Closed in favor of #195. travis-ci/travis-ci#9069 Python >= 3.7 on Travis needs to run on dist: xenial see https://github.com/jupyter/jupyter/pull/385/files. https://github.com/harelba/q/blob/master/test/test-suite#L1999 file() was removed in Python 3 (there are other issues too).. Do you have any performance benchmarks for comparing Py2 and Py3?. This approach does not follow https://docs.python.org/3/howto/pyporting.html#use-feature-detection-instead-of-version-detection  Also unicode(x, encoding) will fail in Python 3 and you have an instance of that in this file. . list comprehension is easier to understand.. six.u() and six.b() and not widely used these daze with u\"string\" and b\"string\" being supported on all supported versions of Python.. If x is really large range(x) on Python 2 has memory overhead but if x is small who cares.  In general I use from six import xrange instead.. You should lint your tests too.. What about c[0].decode('uft-8') in both Py2 and Py3?. six.cStringIO() might be faster on Python2 but I doubt it matters here.. b'\\xef\\xbb\\xbf' works.. You don't like the ternary style?  BOM = self.f.read(3) if six.PY2 else self.f.buffer.read(3). ",
    "unode": "This is 2 years old, quite a few conflicts and changes since.\nBack then py2.6+ was still a reasonable target, now Py3+ is a better choice. \nFeel free to cherry pick whatever still applies.. ",
    "sweentown": "@serima I wasn't aware that FROM was required in SQL.  MySQL grammar, for example, specifies that the FROM clause is optional:\nhttps://dev.mysql.com/doc/refman/5.5/en/select.html\nSELECT\n    ....\n    select_expr [, select_expr ...]\n    [FROM table_references <-- the square brackets here indicate that the from clause is optional\n   ...\n@harelba I frequently use q with simple queries from piped data, but I have not tried to do a join on stdin.  I tried to do a simple example and I can't even get join on stdin to work:\n$ <data q 'select * from - join -'\nquery error: ambiguous column name: main.temp_table_10001.c1\n$ <data q 'select main.temp_table_10001.c1 from - join -'\nquery error: ambiguous column name: main.temp_table_10001.c1\nI'd be genuinely surprised if a complex join against stdin is more common than simple queries.  But as it is your project, it is your decision :). ",
    "cscetbon": "@harelba that'd be great !. ",
    "barrycforever": "A little disappointing, but the workaround gave the right result in the small csv and the original csv too.  Thanks!. Thanks for the hint on previewing the interpretation of data.  I should have realized that nothing comes for free. :)\nIs there any way to provide a hint of the datatype (or force the interpretation of a datatype)?  Thanks!. Okay, thanks.. ",
    "contentfree": "Getting the same error with 24 columns. Works for me, with this file, with 23 columns but not 24. . I don't run into the issue when I use the right command-line options. In my case, -H -d, to specify that it's CSV and has a header row. ",
    "nilpix": "Thanks for your feedback!. ",
    "patatetom": "I agree !. shell completion is a really good thing for terminal users, but it is broken by q when you open quotes for writing sql query (we must completely write the /path/filename) : I suggest you to force \"from -\" for keeping shell completion...\nbash\ncommand | q \"sql statement from -\"\ncat tubular | q \"sql statement from -\"\nq \"sql statement from -\" < tubular\notherwise, very good job and thanks for q :-). effectivly, the shell auto-completion works (with bash / sh), but only on a new line...\n:-( q 'select * from tub<tab>\n:-) q 'select *\nfrom tub<tab> tubular'\nI was even considering finally deleting the from - to have an implicit reading from the standard input (write less)...\neg. from\nbash\nq 'select * from - where c1=1 order by c2' < tubular\nto\nbash\nq 'select * where c1=1 order by c2' < tubular\nbut I didn't see coming the problem \"join\" (I don't use it) : so, sorry for the wrong good request ;-). some precisions :\n```bash\n$ cat -t myfile.tsv\n1^IC:\\Windows\\file.1\n2^IC:\\Windows\\file.2\n$ q -v\nq version 1.6.3\nCopyright (C) 2012-2017 Harel Ben-Attia (harelba@gmail.com, @harelba on twitter)\nhttp://harelba.github.io/q/\n$ q -t 'SELECT * FROM myfile.tsv' | cat -t\n1^IC:Windowsfile.1\n2^IC:Windowsfile.2\n$ sqlite3 <<<.show\n        echo: off\n         eqp: off\n     explain: auto\n     headers: off\n        mode: list\n   nullvalue: \"\"\n      output: stdout\ncolseparator: \"|\"\nrowseparator: \"\\n\"\n       stats: off\n       width: \n    filename: :memory:\n```. ",
    "hwine": "@patatetom that important enough to warrant it's own issue, as it really isn't related to this one.. > Regarding python2 vs python - I'm not familiar with the standard - Is \"python2\" executable supposed to exist in all python2 installations?\nActually, TIL there is a standard: PEP 397, which means this is even supported on Windows \\o/  The relevant parts are in the Python Version Qualifiers section (about 2/3 of the way through - there are no internal id's).\nI actually filed this not knowing of the standard, just seeing it used everywhere (linux, macOS) I am. :smiley:\n. ",
    "fxcoudert": "To quote PEP 394:\n\nso python should be used in the shebang line only for scripts that are source compatible with both Python 2 and 3\n\nand \n\nPython 2 only scripts should either be updated to be source compatible with Python 3 or else to use python2 in the shebang line\n\nPlease merge this, it will help in Homebrew distribution: https://github.com/Homebrew/homebrew-core/pull/32952. @harelba yes, correct. My advice would be:\n- switch shebang line to python2 right away (and cut a new release for users)\n- maybe later,  you  can work on making it Python 3 compatible\nBut given #1 is literally a one-character change, there is not much reason to delay . ",
    "njlr": "Looking through the code I could only find regexp and sha1 https://github.com/harelba/q/blob/6b06a7cba3426ae0c77dc6507b6d9da1fa4e11d6/bin/q#L158. ",
    "stonedbovines": "I can reproduce the same result as @patatetom on both linux (q 1.6.3) and windows (q 1.5.0).\nI can get the backslashes by escaping them (double backslash) in myfile.tsv, when they then show up as single backslashes in the result.. If you do actually need ranges then you should refer to a relevant SQL guide, giving something like:\nq \"select * from ./random.log where c1 between '2017-07-20' and '2017-07-23'\"\nor\nq -d \";\" \"select * from ./random.log where c1 between '2017-07-20 08:00:00,000' and '2017-07-23 17:51:23,456'\"\n. ",
    "runflowcode": "I realized my last comment was missing the proper formatting, here's the updated version:\n```\n$ cat random.log\n2017-07-20 15:02:13,512;random logger;CRITICAL;Optimization Event,newvalue: 758, oldvalue: 56, user: Larry, origin: desktop\n2017-07-20 15:02:13,512;random logger;INFO;Configuration Event,newvalue: 154, oldvalue: 51, user: Eric, origin: mobile\n2017-07-20 15:02:13,512;random logger;WARNING;Control Event,newvalue: 712, oldvalue: 435, user: Saurbh, origin: onlinei\n2017-07-20 15:02:13,512;random logger;ERROR;Optimization Event,newvalue: 541, oldvalue: 679, user: Ariel, origin: desktop\n2017-07-20 15:02:13,512;random logger;CRITICAL;Status Event,newvalue: 532, oldvalue: 623, user: Badr, origin: online\n$q -t -d \";\" \"SELECT * From ./random.log where c1 >= '2017-07-20' between '2017-07-20'\"\n```\n. ",
    "hardys": "Merging this would be good - I was initially confused about the licence for this project since it's not mentioned anywhere on the github.io docs, and the repo has the LICENCE file in a non standard location in docs/ atm.. ",
    "coddingtonbear": "Incidentally -- if you were looking for reasonable things people might want to use the above functionality for: there was recently an excellent article on Hacker News that touched on the neat things you can do with sqlite's user-defined functions: http://tech.marksblogg.com/sqlite3-tutorial-and-guide.html.  In their example they were extracting the hostname from stored values that are full URLs.. ",
    "AkilaRamachandran": "I tried with a 300 MB file, even a simple query with where condition takes 7 to 8 minutes to execute. surprised how Log parser manages to return the same result in 3 seconds.. ",
    "simonw": "I got this working on my machine, here's how (for anyone else who wants to try this):\ncd /tmp\ngit clone https://github.com/harelba/q\ncd q\ngit checkout save-db-to-disk\nbin/q 'select count(*) from demo.csv' -S /tmp/q-demo.db\n\nThen I ran sqlite3 /tmp/q-demo.db \".schema\" to verify the database.. This worked too:\nps -ef | ./q -e latin1 -H \"select UID, COUNT(*) cnt FROM - GROUP BY UID ORDER BY cnt DESC limit 5\" -S /tmp/ps.db\n\nIt created a database with a single table with the name - - which works fine but does feel a tiny bit odd. Not sure if it's worth fixing though - you could call that table stdin instead perhaps? Probably not worth the trouble.. ",
    "gmondo": "Thank you bitti for the response. The problem I exposed is simplified: my csv has tens of fields and I would select only a few. Considering that this usage of an header file seems doable by q manual I think that cat and pipe could make less readable a script calling q. . ",
    "Oblomov": "FWIW, that's sadly not infrequent for Python applications.. ",
    "rolfen": "By the way, the documentation says that spaces should be supported:\n\nPlease note that column names that include spaces need to be used in the query with back-ticks, as per the sqlite standard.\n\nThey are not, on latest master.\n. ",
    "vyskubov": "As a workaround I'm using Sed for Windows to remove the last line with the warning as follows:\nq \"SELECT 1\" > result.tmp\nsed \"$d\" result.tmp 1> result.csv. ",
    "Suleman-Elahi": "Thanks...!!!. ",
    "avilella": "Resolved with q version 1.7.x and:\necho -e \"col\\n032907365\\n\" | q --as-text -O -H -d ',' \"SELECT col  FROM - \" 2>/dev/null\ncol\n032907365. ",
    "psychemedia": "I thought it was a py2/3 issue. (I have Python 3.6.4 :: Anaconda custom (64-bit) leading from my default path; I dumped the default Mac python a long time ago... Which makes me wonder if other things elsewhere are broken if I try to run them!). \"Dumped\" as in I avoid it and try not to default to it.\nOn Macs, I think there is an explicit python2 at /usr/local/bin/python2?\n. ",
    "Berkmann18": "I'm running into the same issue.\n```bash\n$ ./q \"SELECT c6,COUNT(1) FROM ../examples/exampledatafile GROUP BY c6\"\nFile \"./q\", line 139\n    except ImportError, e:\n                      ^\nSyntaxError: invalid syntax\n$ ../bin/./q \"SELECT c6,COUNT(1) FROM exampledatafile GROUP BY c6\"\nFile \"../bin/./q\", line 139\n    except ImportError, e:\n                      ^\nSyntaxError: invalid syntax\n```\nEDIT: this issue doesn't occur with the AUR version.. @harelba It is indeed.. Ah awesome, thank you.. ",
    "likeshumidity": "As a temp fix, if python2 is in your path, can just change /bin/q first line from:\n```\n!/usr/bin/env python\nto:\n!/usr/bin/env python2\n```\n. ",
    "petski": "Poor mans way to do this:\ncurl -s https://api.github.com/repos/harelba/q/releases/latest \\\n| grep \"browser_download_url.*deb\" \\\n| cut -d '\"' -f 4 \\\n| wget -qi -\n(Based on https://gist.github.com/steinwaywhw/a4cd19cda655b8249d908261a62687f8). Hehe, I do, but jq doesn't seem to be installed on most default systems though. In my defense, I told its a poor man's way of doing it. OP will get the drill.\nP.s. found out Debian and Ubuntu have q in their package archives as \"python-q-text-as-data\". ",
    "captlid": "\nstill getting the same problem. The file is tab delimited. I have sqlite on my computer at home the queries work fine on the same imported file. \n. and apparently the delimiter too .\nq -H -d, \"select ....\"\nor \nq -H -t \"select ....\"\nIt works now. Without the delimiter specified it says no columns.. ",
    "lavvy": "I am not yet too conversant with it though but I know that APKBUILD file of\nalpine is almost equivalent to PKGBUILD file of arch, only with a very\nlittle syntax difference.\nOn Fri, Oct 19, 2018, 3:54 AM Harel Ben-Attia notifications@github.com\nwrote:\n\ni'd be glad to create a package for this, but i'm unfamiliar with the\nentire flow of releasing such packages. If you could elaborate on this,\nthat would be great.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/harelba/q/issues/179#issuecomment-431277230, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGsC4xLOmuQORu5-bYDENOSPjX-G5Bidks5umYUcgaJpZM4XdYaD\n.\n. \n",
    "peylight": "@bitti \nThat error is about OS and python.\nBut it happened if there are too many input files. Like a wildcard for ex:\nq \"SElECT COUNT(*) FROM ./path/to/myfiles_* WHERE c1 == 'sample'\"\nI think q try to open all files in once and then the error will be happened. @harelba \nBy cat and then pipe into q it's working fine.\n$ ulimit -a\nopen files                      (-n) 1024\nSo if you make more than 1024 files, it's will be happen.\nI can't run another command when the error happening. it's very fast!. Yea, I can give you the exact files and command.\nDownload files\nCommand: q \"SElECT COUNT(*) FROM q-test/* WHERE c1 == 'sample'\"\nOutput: IOError(24, 'Too many open files'). ",
    "rseemann": "Hi @harelba, thanks for the reply!\nI had a similar problem in another project, and I fixed it with utf-8. I'll close this thread since I believe it might be the same problem and reopen if I can't solve it then, does that make sense?. Curiously enough I was trying to run the command from within a Python script and it works\noutput = subprocess.check_output(['bash','-c', bashCommand])\nbut if I try to run the exact same command from the command-line it gives me the ascii error. I no expert in bash, so it might be something obvious here that I might be missing.. ",
    "mbrukman": "@bitti \u2014 the image appears to be also used on http://harelba.github.io/q/ but that needs to be updated as well, but I wasn't sure how to do that, so I left it alone.\nIf @harelba is OK with this as well, I'm happy to delete the image in the same commit.. @bitti \u2014 good catch! I didn't even realize that was text on the main site and a screenshot here; I could have saved myself some time and copy-pasted from the site instead of re-creating the commands manually. \ud83d\ude04 \nRemoved the image from the PR in the latest updated commit.. @mcanlas \u2013 yes, this PR is OK for merging, at least from my perspective.. @bitti, @mcanlas, @harelba \u2013 any thoughts/updates on this PR?. ",
    "mcanlas": "Is this PR okay for merging?. "
}
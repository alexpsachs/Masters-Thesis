{
    "nvie": "Fixed by creating a PyPI package from it. It can be installed globally, or by installing it in every project that you want to use it in (and it can self-update\u2014which is a nice side-benefit).\n. Fixed.\n. This will conflict with -v. Therefore, all logging that is written with -v should be written to stderr, or -v and --raw should be mutually exclusive.\n. I'm not planning on any more features to the 0.x branch, and this won't apply for our future work. So please spare yourself the effort.\n. Fixed in f3982435704696705b762e072d6609b4342e407d.\n. Yeah, there's a weird ordering issue with PyPI. They are aware of it, and working on a fix.\n. That's awesome!\n. Hi @brutasse, I think your ideas are great, and I'd love to work with you to get this nicely implemented. Before implementing, however, would it be a good idea to get our minds synced? I nice way would be to describe the API in a cram test, explicitly defining behaviour for these commands.\nI've made a beginning to it. (I've dubbed the command pip-compile, as pip-bundle would be too confusing, indeed. Nevertheless, that's just a working title and we can change it easily.)\nThis Gist-based interface description can be changed quite easily and we can implement when we reach a stable state. Feel free to fork and modify the example I started to match your idea first\u2014we'll work from there.\n. This is great!\n\n@nvie awesome. At first I got confused by the cram tests since they failed on my machine: python3 is my default python. They still fail at the moment, pyflakes shows some issues with undefined names in some places.\n\nYeah, I should fix these issues\u2026\n\nI updated the gist. I like the pip-compile name much more than pip-bundle. I like the use of a .in extension for top-level requirements, too :)\n\nCool, I think we can work with those, then.\n\nI don't think the tool should touch .in files but rather show what updates are available and then the user choses to update his .in requirements and run pip-compile again to generate the .txt files. pip-compile would sort of replace pip-dump, I guess.\n\nYep, I agree with this. I think it's good to have a flag to upgrade automatically (in case there is no conflict). This flag will also help us write the test cases.\n\nAlso I think it should be encouraged to pin stuff in requirements.in. Maybe even not support not-pinned requirements? Not sure about this one, in your first example it's probably not important to pin the nose dep in dev-requirements.in but since requirements.in controls what goes into your production environment it should only contain pinned packages.\n\nI think the default invocation should refuse them, indeed. But with an optional -f flag, you should be able to use the non-pinned version, too, I think.\nRethinking Things\nLet's first introduce some new terminology, to better articulate the problem:\n\n\nTerm\nMeaning\n\n\nSource Spec\nAll of the *requirements.in files together\n\n\nCompiled Spec\nAll of the *requirements.txt, and .pipignore together\n\n\nRecorded State\nSource Spec and Compiled Spec, as kept under version control\n\n\nEnvironment\nThe Python virtual environment, specifically the list of installed packages in there\n\n\nThe point of pip-tools is to keep the Environment and the Recorded State in sync all the time, while supporting checking for updates.\nThe current toolset pip-review and pip-dump are designed to work with the reality that the Environment and *requirements.txt files are leading\u2014they are in essence managed manually.  pip-review keeps the packages in the environment up-to-date, and pip-dump records the env state in version control.\nHowever, our new approach flips this reality up-side-down and requires the tool to be in control all the time and should generate both the Environment and the Compiled Spec files.  This is a pretty significant difference.\nThe result is a bit of a mess and responsibilities are a bit unclear, so let's say we ditch the current tools and start over with new, differently named tools, to avoid any confusion.\n```\n                              YE OLDE WAY\n                   (review)               (dump)\n      environment  ------->  environment  ----->  spec.txt\n\n\n\n                          THE FUTURE\u2122\n\n                (compile)            (sync)\n       spec.in  -------->  spec.txt  ----->  environment\n\n                      (compile --outdated)\n       spec.{in,txt}  ------------------->  spec.{in,txt}\n\n```\nLet's say we assume the tools to be in charge and are responsible for periodically generating both the *.txt spec files and the actual virtual environment.  As these tools bluntly \"overwrite\", this makes it a bad developer practice to manually pip install any packages, or to manually modify requirements.txt.\nEffectively, it means that manually adding new lines to requirements.txt, or pip-installing new packages to the Environment will result in loss of them eventually, once someone runs pip-compile or pip-sync respectively.\nWe need a way to \"sync\" the Recorded State to the Environment.  I'm thinking of a new command named pip-sync, which not only installs packages, but also uninstalls, in order to reflect the environment exactly as based on the specs.  The net result would be identical to creating a new environment and running pip install -r <spec> for all Compiled Spec files.  Example:\n```\n$ pip freeze\nabc==1\nbar==1\nfoo==1\n$ cat dev-requirements.txt\nbar==1\n$ cat requirements.txt\nfoo==2\nqux==1\n$ pip-sync   # sync the environment (does not only install, does also uninstall)\n$ pip freeze\nbar==1  # unchanged\nfoo==2  # updated\nqux==1  # newly installed\nuninstalled abc==1\n```\nThis would also make for a great deployment command for services like Heroku (instead of pip install -r requirements.txt), to retire packages that aren't needed anymore.\nI'll stop now, to make my thoughts digestable.  Are we on the same page?\n. The main reason for naming the tools pip-something was a bit of wishful thinking where these commands could potentially fit nicely into pip itself as a subcommand. So you could read pip-compile as pip compile. The other reason for not picking a tool w/ subcommands is for simplicity. They're \"just two scripts\", no actual Python packages are installed with pip-tools currently.\nI'm -0 on naming \"tool\" anything other than \"pip-\".\nI'm with you on the necessity for an actual \"outdated\" command, although I don't particularly like it being a non-verb, but that's a personal itch. (I can rant away at Chef's knife kitchen command for hours :))\nI concur with being careful when syncing, but worst case you'd have to reinvoke it with the correct params, so not too much harm done there. If this bumps into resistance too much, we could have it warn about/confirm uninstalls by default and allow -f to force it. Also, a --no-uninstall flag would be good thing to have, I think.\nLastly, I personally prefer not having to specify -r in front of every spec file in the command invocations. I know it's in line with pip itself, but I consider that design choice a bit unfortunate, as it breaks the possibility of using shell wildcards in the tool invocation, like:\n$ tool sync specs/*.txt\nMore thoughts?\n. > One issue with that is you can't have common utilities shared between commands (_check_output for instance). Are you opposed to pip-tools adding something to the python path for sharing between scripts?\nI am not, and I think we will eventually end up there. I currently lived with the duplication, because I just wanted to avoid that (I wanted to keep the tools lean).\n\nThe gist is up-to-date, I kept pip-outdated for now but this could be called pip-review instead.\n\nI was thinking the same.\n\nI'll update my branch to add the cram tests, support .in files, add the pip-sync script. And maybe update pip-review to implement the pip-outdated behavior.\n\nCool. Please share your branch by opening a pull request so we can work on it together if you're ready.\n. OK, I've updated the gist once more (to be a bit more descriptive about what's going on and what's important in each step).\n@brutasse, could you take a look at the pip-compile-specifics.txt file I've added? What do you think? Am I overcomplicating things here, or is this useful? I was thinking only pip-outdated should reach out to PyPI, or pip-compile at least should only do that if the currently pinned secondary versions don't match criteria (so it could reach out to PyPI to find versions that do match).\nOr do you think pip-compile should always reach out to PyPI, find the latest versions that still match all criteria, and record those? It would definitely simplify things for us (as we don't have to consider requirements.txt in pip-compile at all), but it would lead to (secondary) package updates on every compile, and compiles would always be lengthy.\n. > I'd say people shouldn't touch requirements.txt manually while using pip-tools. For that use case I'd put the raven and simplejson requirements in the .in file, [...]. It'd be much simpler than trying to make pip-compile aware of stuff manually changed in compiled files\u2026\nI agree that this would allow for a much simpler implementation of pip-compile. It would also, however, lead to a situation where pip-review is used to search for / upgrade any top-level dependency versions, whereas pip-compile is used to search for / upgrade secondary dependencies. Don't you think that's a bit weird from a UX perspective?\nMy thinking was: always try to compile specs with as much of the same pinned versions that you currently are using (where possible) and only (ask to) upgrade them when there's no other way. Or when an explicit pip-review takes place.\nI'd also really like a fast and deterministic pip-compile. In essence, the behaviour that I would like is that when you run pip-compile immediately after another pip-compile, without changing the Source Specs in between, the second invocation should under no circumstances change anything to the Compiled Specs.\nAny idea how Bundler's implementation relates to this?\n. OK, some bad news for me from the front. I think I'll have to let go of my pipe dream where we can figure out the dependency calculation without downloading any packages upfront. To get to the actual dependencies a package has, setup.py must be executed and dependencies are calculated runtime (depending on OS or Python version).\nThis is an ugly fact of life we have to deal with :(\nOn the other hand, this makes things clear: the only safe remaining way of calculating the actual dependency tree seems to be to \"just install\" the top-level packages:\n- Create a new, temporary, virtualenv\n- Install the Source Specs (top-level deps)\n- Run pip-freeze -l: that's our (flattened) resolved dependency tree\n- Cleanup\u2014remove the temp env\nI guess this is what @brutasse suggested in the first place, so sorry if I'm a bit late to the party of understanding the hairy parts of how this works under the hood :)\n. Awesome stuff. I've made some good progress in collecting and normalising specs. Will hopefully be push-ready by the end of the afternoon.\n. Hey @brutasse, please check out my work on the Spec normalization/conflict detection. I've kept your functions in there, although the main() function does not call all of them anymore. We need them later, but I wanted to get the bare normalization/conflict detection logic in first.\nI've tested this with the following inputs:\n```\n$ cat requirements.in\nraven==1.9.3\nbegin whitespace\nend whitespace :)\nsentry==5.0.13\n$ cat dev-requirements.in\nnose>=1.2.0,<1.4.0\nThe following line is completely obsolete, because line 1 is much more narrow\nnose>1.1.8,<=1.5.0\nThe following line does narrow down the spec from line 1\nnose>=1.1.8,<1.3.0\nUncomment the following line if you want to render all previous nose specs obsolete\nnose==1.2.1\nUncomment the following line too if you want to test a conflict :)\nnose==1.2.2\nThe following foo specs will result in foo==1.5.0\nfoo>=1.5.0\nfoo<=1.5.0\n```\nI've also added some TODO notes for further implementation, so be my guest if you want to take some of them and improve. Hope you like this.\n. Hey @brutasse, I just quickly pushed out a commit that moved all data structures to a module, so we can share this among the other tools, whenever necessary. Feel free to move more supporting code to the module, to keep the scripts lightweight and easily readable.\n. Btw, if you want to connect in real time, I'm online on irc.freenode.net under #pip-tools.\n. @brutasse, I've plain committed your Gist to the project. I think the trick is to not add the pinned versions to the spec set as we're still building the set, but instead postpone that to the very last moment possible.\nThe trick is not to get lost in confusion here. There will be lots of hairy cases, even ones we can't come up with now. So I suggest to stop working on the algorithm right now and first come up with a test framework that makes it easy to express and test some assumptions / weird edge cases.\nInstead of the piptools.{cache,pypi} modules, we actually need piptools.package_manager, that we can use to instantiate the backend that provides access to packages, and package info. Its interface should consist only of method calls supported by the PyPI backend, like:\n``` python\nclass PackageManager(object):\n    def find_best_match(self, spec):\n        ...\ndef get_dependencies(self, name, version):\n    ...\n\n```\nActual package contents don't matter that much to us, so the interface should hide PyPI / URL / cache path details.\nNot only does this make the code more readable, it also allows us to stub out the whole backend with a fake one, for speeding up our test cases. We can use this to bypass the PyPI downloads/caching, speeding up our tests.\nImagine the following to express a fake dep tree. In essence, it's a DSL for creating a mini-PyPI on the fly and use that as a test stub:\npython\n{\n  'foo-0.1': ['bar'],\n  'bar-1.2': ['qux', 'simplejson'],\n  'qux-0.1': ['simplejson<2.6'],\n  'simplejson-2.4.3': [],\n  'simplejson-2.6.0': [],\n}\nI foresee a test case, looking like this:\n``` python\nclass MyTest(unittest.TestCase):\n    def test_lookup(self):\n        pkgmgr = StubbedPackageManager(contents_from_dict_above)\n        name, version = pkgmgr.find_best_match('bar>=1.0')\n        assert name == 'bar'\n        assert version == '1.2'\n    deps = pkgmgr.get_dependencies(name, version)\n    assert 'qux' in deps\n    assert 'simplejson' in deps\n\n```\nThat's really all there is to it. I'll create this PackageManager structure now, so we can move the existing PyPI / package cache code in there first. Then, we can express our needs in test cases and go on with the actual problem.\n. Hey @brutasse, I've improved our test cases today, and also tinkered a bit with the idea for dependency resolving. Eventually, I've come up with the following body of code that could eventually be moved into a module. (I've kept it in the test case while still tinkering with it.)\nCurrently, this is a \"while true\" loop with an ugly break after 4 rounds, but the core of my findings is this.\nIt should:\n- add top-level dependencies\n- do a breadth-first (!) recursion to add all secondary dependencies. In other words, after the first round, we have 1st and 2nd-level deps in the spec set\n- normalize and check for conflicts\u2014if there are any, there's an impossible situation and we raise an exception\n- now, we have a new (partial) spec set and can reconsult the package manager to find best matches\n- we add these next-level dependencies and recurse\nIt still does not solve all of our problems yet, but I think we can actually pull this off by adding some smart backtracking logic, but that'll be the next step.\nTo illustrate the above with your counterexample:\n``` python\ncontent = {\n    'foo-0.1': ['bar'],\n    'bar-1.2': ['qux', 'simplejson'],\n    'qux-0.1': ['simplejson<2.6'],\n'simplejson-2.4.0': [],\n'simplejson-2.6.2': [],\n\n}\n```\nThe top-level dep here is \"foo\". Applying the logic described above, this yields the minimalized spec set for the total:\npython\n['foo', 'qux', 'bar', 'simplejson<2.6']\nWhich can be easily pinned down by resolving find_best_match() again on the result. Which is exactly what we want! Running the test will render the following output:\nAfter round #1:\n  - foo\nAfter round #2:\n  - foo\n  - bar (from foo==0.1)\nAfter round #3:\n  - qux (from bar==1.2)\n  - foo\n  - bar (from foo==0.1)\n  - bar (from foo==0.1)\n  - simplejson (from bar==1.2)\nAfter round #4:\n  - qux (from bar==1.2)\n  - qux (from bar==1.2)\n  - foo\n  - bar (from foo==0.1)\n  - bar (from foo==0.1)\n  - bar (from foo==0.1)\n  - simplejson (from bar==1.2)\n  - simplejson<2.6 (from qux==0.1)\n  - simplejson (from bar==1.2)\nAfter round #final:\n  - qux (from inferred)\n  - foo (from inferred)\n  - bar (from inferred)\n  - simplejson<2.6 (from inferred)\n:sparkles::beer::sparkles:\nPS: The number of calls to find_best_match() will increase heavily by this algorithm, but fortunately this function can be memoized for performance reasons.\n. I've done some work to detect duplicate Specs in a SpecSet: all Spec instances now have their qualifier list stored as a frozenset instead, making them immutable, hashable and comparable. I've also reduced the whole SpecSource hierarchy into simple \"just-strings\" sources. Together, this makes adding duplicate specs to SpecSet impossible, leading to more unified output:\nAfter round #1:\n  - foo\nAfter round #2:\n  - bar (from foo==0.1)\n  - foo\nAfter round #3:\n  - bar (from foo==0.1)\n  - foo\n  - qux (from bar==1.2)\n  - simplejson (from bar==1.2)\nAfter round #4:\n  - bar (from foo==0.1)\n  - foo\n  - qux (from bar==1.2)\n  - simplejson (from bar==1.2)\n  - simplejson<2.6 (from qux==0.1)\nAfter round #final:\n  - bar (from <inferred>)\n  - foo (from <inferred>)\n  - qux (from <inferred>)\n  - simplejson<2.6 (from <inferred>)\nAlso, the SpecSet iterator interface now always returns the specs in a sorted fashion, making it easier to write test cases.\n. Hey @brutasse, I've added an example of a package dependency structure that our current approach can't handle, plus a few thoughts I had for resolving this programmatically. I'm interested in your thoughts, too.\n. Hey @brutasse, parenting is great, but time-consuming :)\nFinding time to work/review stuff for my open source projects is challenging to say the least currently, as we're figuring out a stable rhythm with our kids, work and sleep. Nevertheless, I'll try to review your patch. Feel free to work on more features\u2014I could certainly use the help currently!\nThanksalot,\nVincent\n. Thanks so much for this work, @brutasse, and sorry for not responding to this sooner! I love the work you've put into this. I like the logger and the raising of ConflictError over assertions. I've fixed a few of the remaining inconsistencies, and a few broken unit tests.\nRunning the cram test suite seems to have become pretty slow. Might this be due to a change you've made? Or just network stuff? It's still running here (~10 min now).\nThanks for your work!\n. Thanks for the patches! I'm all for searching case-insensitive. In an ideal world, packages take care of using the correct case, but that's not how packages work in reality. The sentry example above would fail and there would not be a way of using pip-tools in these real situations.\nI don't think we should change the casing in the compiled output wherever possible, I think. In case of conflict, just pick one\u2014the first occurrence maybe? It's arbitrary anyway.\nAlternatively, we could add a -i flag to have this behaviour, but I'm generally not a big fan of adding flags like this.\nDo you agree?\n. That's even better. Is there a way we can already detect the canonical casing currently, or do we need to add another, explicit, HTTP request for this?\n. Fixing would be even better\u2014I like :)\n. pip freeze reports the following when I add that URL to requirements.txt:\n```\nSkipping line because it's not clear what it would install: http://gevent.googlecode.com/files/gevent-1.0b4.tar.gz\n  (add #egg=PackageName to the URL to avoid this warning)\ngreenlet==0.4.0\npip-tools==0.2\nThe following requirements were added by pip --freeze:\ngevent==1.0b4\n```\nSo it's pip freeze removing the URL, not pip-dump.\n. Thanks!\n. Today, I've released pip-tools==0.3.4, which features unofficial python3 support. The reason why it's unofficial is that the tests aren't passing, but the reason for that isn't pip-tools related, but related to the packages we use in the tests (which are actually downloaded from PyPI), and that aren't Python3 compatible themselves. An example is Flask. We should rewrite our tests to use other packages, or stub out PyPI in our test cases (probably better, but more complicated).\nSo go ahead and start using pip-tools for your Python3 projects and let me know if there's any issues with it!\n. Yeah. I'm a bit puzzled on this one. As it's 2012, and would love to push adoption of packaging/distutils2, I think using scripts is more future-proof than entry_points is. On the other hand, operating systems like Windows require scripts to be installed with a .exe extension, which is exactly the purpose of entry_points.\nTruly, I don't know what's wise and would love a packaging guru to step into this discussion.\n. Nope. Let's move on to pip==1.3.x.\n. Yeah, I'd be glad to accept that patch!\n. LOL!\n. The thing is that pip tools depend on pip freeze output. I've tried this concrete example myself, and from the pip freeze output, this package is indistinguishable from normal PyPI packages. There's no way around this other than to use -e (editable) packages instead, which pip-tools support, too.\nYou should install it using:\nconsole\n$ pip install -e bzr+http://path/to/bzr/url#egg=PyChart-dev\n(The #egg= part is important.)\nAfter that, pip freeze should be aware of this package's source, and pip-tools can use that to update smartly.\n. If I install using that command, and then run pip-dump, this gets inserted in requirements.txt:\n-e bzr+http://download.gna.org/pychart/bzr-archive/@7#egg=PyChart-1.39-py2.7-dev_r7\nWhich looks good to me. What's the problem?\n. I'm no bzr expert (never used it), but I simply followed the instructions as suggested in that output: \"you may be able to use bzr upgrade\". After upgrading that checkout, it worked.\n. Sounds like a great idea.\n. Thanks. I've released this in 0.2.1.\n. If you want to prepare a patch/pull request for this, I will accept it. Otherwise, I'm closing this issue.\n. Not going to support this for the 0.x branch. Might be useful do it in our future branch, though. It makes sense in the .in files.\n. Sounds like something comparable to #7. It's the advertised version on PyPI that has the ordering wrong. pip-tools only obeys PyPI as it does the ordering, not pip-tools.\n. Yeah, I'd be fine with it, too. We should explicitly assume PEP386 compliance, then, though. But that's also a good thing to do anyway.\n. I'd be happy to accept a patch that implements this. The patch should add a\n\"pip_tools\" package, which contains a NormalizedVersion, much like the\n\"future\" branch already has (it can be copied from that branch literally).\nThen, the implementation is easy.\n. I'd accept a patch where all three patterns are directed, no problem. No pattern is better than the other, IMO.\n(.*)requirements.txt\nrequirements[_-](.*).txt\nrequirements/(.*).txt\n. Cool. Could you update the cram tests with a test case for this?\n. Thank you!\n. That's interesting. I didn't notice the recent work on pypa/pip, but it looks like we can ditch pip-review soon, too. Good stuff.\n. Will accept a patch for this, if provided. Not entirely sure what hairiness I need to deal with here. (Would it require pip-review to parse that file and use those options to control pip? There are probably some classes in pip's library that provide support for this, but I'm unsure.)\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. Thanks!\n. Thanks for this, @svetlyak40wt.\n. Thanks for this one, too.\n. I'm not planning on improving pip-review much anymore, since pip grew the pip list --outdated command in version 1.3.1 recently. I'm not sure how pip list deals with git-based URLs though.\n. This is just output from the underlying system call to pip freeze -lr requirements.txt. I found that hiding that line explicitly could lead to situations where lines are removed from the requirements.txt files when those packages don't exist. Essentially, these messages concern lines that are removed from the file (and thus lost).\nIn your Django case, the correct name Django gets put in there from the virtualenv, but the lowercased django isn't found and thus removed from the file. Hence the warning.\n. Again, this is output from the underlying commands. I don't consider this a bug in pip-tools, although I acknowledge this might be confusing. The behaviour is correct though, and re-running pip-dump doesn't show the output after the first run.\n. This went into 0.3.3.\n. Agreed. Feel free to submit a patch if you want.\n. Yes.\n. Thanks a lot!\n. Thanks!\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. You should add a #egg=pkgname suffix to each -e line in your file, otherwise pip cannot know the package name.\n. Thanks!\n. I'll accept a PR for this if you (or anyone) provides one.\n. I'll still accept a PR for this if you want to provide one. If so, please make sure we don't break anything for people not on the latest version of pip, so please add a conditional checking for a high-enough pip version.\n. pip-review is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Could you run it again and see what pip freeze -lr /tmp/tmpguafnJ returns\nwhen you run it manually?\nOn 10 June 2013 23:27, Peter Farrell notifications@github.com wrote:\n\nI have my requirements in ./requirements directory with base.txt and\ndevelopment.txt (using `-r base.txt) at the top.\nUsing pip-dump in my virtualenv -- I get the following trace:\n(verde)peter@Blizzardme:~/PycharmProjects/Verde$ pip-dump\nRequirement file contains argparse, but that package is not installed\nException:\nTraceback (most recent call last):\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/basecommand.py\", line 139, in main\n    status = self.run(options, args)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/commands/freeze.py\", line 99, in run\n    line_req = InstallRequirement.from_line(line)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/req.py\", line 118, in from_line\n    return cls(req, comes_from, url=url)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/req.py\", line 43, in init\n    req = pkg_resources.Requirement.parse(req)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2510, in parse\n    reqs = list(parse_requirements(s))\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2436, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2414, in scan_list\n    \"Expected ',' or end-of-list in\",line,\"at\",line[p:]\nValueError: (\"Expected ',' or end-of-list in\", 'wsgiref==0.1.2-r base.txt-r base.txt', 'at', ' base.txt-r base.txt')\nStoring complete log in /home/peter/.pip/pip.log\nTraceback (most recent call last):\n  File \"/home/peter/.virtualenvs/verde/bin/pip-dump\", line 159, in \n    main()\n  File \"/home/peter/.virtualenvs/verde/bin/pip-dump\", line 155, in main\n    dump_requirements(args.files)\n  File \"/home/peter/.virtualenvs/verde/bin/pip-dump\", line 118, in dump_requirements\n    _, new = pip_info(tmpfile)\n  File \"/home/peter/.virtualenvs/verde/bin/pip-dump\", line 91, in pip_info\n    raw = check_output(cmd)\n  File \"/usr/lib/python2.7/subprocess.py\", line 575, in check_output\n    raise CalledProcessError(retcode, cmd, output=output)\nsubprocess.CalledProcessError: Command 'pip freeze -lr /tmp/tmpguafnJ' returned non-zero exit status 2\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/issues/46\n.\n. This is a pip exception, has nothing to do with pip-tools. Looks to me like you got a syntax error in your requirements file. There seems to be an entry called\n\n`wsgiref==0.1.2-r base.txt-r base.txt`\nin your file.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. I don't know why you would want this feature, but I guess it's no harm. The main reason for sorting is that it plays very well with version control systems.\n. I must admit I did not keep track of everything that's been happening in setuptools-land lately. @brutasse, you got a clue why the egg_info command is gone, or what it was replaced with?\n. Nice.\n. Thanks!\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. This invalid version spec is coming from a dependency of django-oscar:\nphonenumbers<=6.0.0a,>=5.9.2 (from requirements.in:1 ~> django-oscar==0.7.2)\nThe thing is, we're using NormalizedVersion from the distlib library. Here's the relevant snippet from that library. According to this line, this version in particular is rejected explicitly (as release levels need a sequence number, according to the spec). In other words, 6.0.0a1 would be fine for the version number.\nThis puts pip-tools in an odd position where on the one hand we want to support any version used in the wild, but on the other hand, this makes determining version numbers ambiguous. Removing this ambiguity is one of the main reasons why PEP426 exists in the first place, and having projects use standardized versions is a good thing.\nSo in this case, there are three options:\n1. The phonenumbers project should not be using the 6.0.0a version, but use a standardized version instead;\n2. The django-oscar project should declare the dependency a little bit differently (e.g. remove the a, or make it 6.0.0a1);\n3. pip-tools should support these ambiguous packages.\nObviously, I'm in favor of 1 or 2, but it means this will be blocking you before those patches are accepted and released. I might be willing to codify this consideration in the project, so we can throw an exception that explains what's going on whenever pip-tools comes across a non-standard version number.\n. Awesome, thanks!\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. Yes, that is not supported. This is a duplicate of this comment by @brutasse on #39.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. Thank you, sir.\n. Awesome work, thanks @svetlyak40wt !\n. I suppose if  pip supports it like that, then we should, too.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. I'll accept a pull request that converts the bin/* files to proper setup.py-based scripts. Feel free to contribute.\n. I'm rewriting the entire pip-tools suite (almost from scratch). I definitely want to support Windows, but I will need some help from others that actually use Windows to report and fix bugs.\nThe master version of pip-tools won't see any updates anymore\u2014all my efforts are focused on the rewrite.\n. Agreed, passing down options makes sense.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. That would technically belong in the pip repo, but I'm pretty sure they won't add it, as it would bleed guilty knowledge about pip-tools into it.\n. That's actually pretty nice. You should propose it to the pip project as a feature request, perhaps. All pip-tools commands could then become dash-less, too :)\n. The way git works is that if a subcommand isn't found, it tries to look for an executable script on your PATH named git-$SUBCOMMAND, which is a really simple solution. Simply creating a git-foo script somewhere on your path and making it executable is enough. You can use a shebang line to implement it in any language of choice.\nIt's a really simple technique and would fit pip just as well, I think. But... I'm not sure if the pip maintainers would consider this a useful addition to the tool.\n. I'd rather fix setup.py to support installing these scripts executably for Windows users directly, instead of adding instructions for this hack in the README. I'm certain setup.py supports this.\n. @zed Yes, agreed. This is the way to go, thanks. If anyone feels like providing this patch, I'm happy to pull.\n. You might want to start using $ pip list --outdated instead of pip-review.\n. The use of pip-review is pretty much obsoleted by pip's support of listing outdated packages, so you might want to consider using pip list --outdated instead.\n. If you want to prepare a PR for this, I'm willing to accept it. I'm closing this issue for now.\n. Seems like we should be able to simply ignore these entries? Not sure when they occur in pip freeze output, but happy to accept a patch that fixes this.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. Fixed in 6b72ea5702db9263815720d551432a37888204ea :)\n. The future of the pip-review command is a bit uncertain, as pip now \noffers this functionality too, via pip list --outdated. That said, I \nwould definitely like to have this feature in the mean time, so feel free \nto provide a patch and I'm happy to pull.\nOn 18 December 2013 18:18, Chris Wood notifications@github.com wrote:\n\nIt would be useful to be able to restrict what pip-review will check, to \nsave time and focus on particular modules.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/issues/68\n.\n. I don't continue work on the current version of pip-tools, only the future branch.\n. Thanks!\n. Thanks, I didn't even notice!\n. This was fixed a while ago.\n. This has been fixed since a while by always lower-casing the generated output.\n. Yes, I'm hoping to allocate some time somewhere soonish to implement pip-sync on the future branch, merge that into master and then release pip-tools 1.0.\n. Apparently, I had hidden it on PyPI, so my bad. It's fixed now :)\nThanks.\n\nOn 22 February 2014 09:34, Bruno Reni\u00e9 notifications@github.com wrote:\n\n@nvie https://github.com/nvie do you know what's going on?\nhttps://pypi.python.org/simple/pip-tools/ works fine but\nhttps://pypi.python.org/pypi/pip-tools returns a 404\n\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/issues/77#issuecomment-35797844\n.\n. I'm closing old issues. If this issue still applies and requires my attention, I ask you kindly to re-open it or file a new issue. Thanks.\n. As we've now shifted all focus on finishing pip-tools' future branch (pip-compile + pip-sync) and want to replace the master branch with it once we're ready to release it as 1.0, pip-review and pip-dump will have no use anymore. pip-review can be replaced by pip list --outdated. The only thing missing will be the interactive mode.\n\nThis means I'm closing all tickets related to pip-review and pip-dump now, including this one.\n. Awesome work, thanks @svetlyak40wt !\n. Awesome work, thanks @svetlyak40wt !\n. Thanks, @beeftornado !\n. This whole project is pretty much a big experiment, and pip has changed a lot over the last few months, so supporting many versions of pip is pretty hard for me. So yes, ideally you'd upgrade to pip 1.5+. You should probably do that for better reasons than pip-tools, anyway :)\n. Thanks!\n. This does not seem to be an issue on the latest future branch, so I'm closing this issue (and the associated pull request).\n. Fixed in e8e44714847553081b28dc51722790042b03b986.\n. [1] means the program exited with an exit code of 1, which is unexpected. I fixed the Python based tests last night after this issue brought it to my attention. I did not get a chance to look at these test failures.\n. Nope, I also get the exact same result over here...\nOn 6 May 2014 15:57, Daniel Hahler notifications@github.com wrote:\n\nI did not get a chance to look at these test failures.\nBut you also get them? Or is it something specific to my setup?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/issues/89#issuecomment-42304764\n.\n. All tests that could be fixed are fixed now (on 22a0a259b92bc623eee1b4005a492aff69a4dd1d), including Python 3.4 support. The only failing tests that remain are now the tests for the yet-unimplemented features and one feature that was recently broken. (I'm looking into this further.)\n. Thanks!\n. This has been fixed on the latest future branch already, so closing.\n. It was related to some unicode/string confusion which has been addressed in my efforts to make it Python 3 compatible. As a side effect, the reported behaviour seems to have been disappeared. I could reproduce the issue before, but not after those fixes, so I believe this is authentically fixed, yes.\n. TBH, I think I want to redesign a few things at the heart of pip-tools. The need for downloading was once very pragmatic: we needed to check which dependencies a certain package version has declared, and there was no way of simply looking that up. It would be awesome if PyPI could offer this API, but it does not, at least not yet. Downloading these packages and persisting them saved us a re-download of them for future runs. But the only thing we really need from the packages is which dependencies there are; not their contents. Since we now have the dep_cache, we could download the package, look at it, record those deps in the dep_cache, throw away the package cache. So I think we can remove the entire package caching entirely.\n. This is now fixed in 6410332128114e592144d8e2cd30c391851d7dca.\n. If you add a plain pyyaml in your requirements.in file, it'll be allowed to pick any version that matches, but should still try to pick the highest version. If this keeps giving trouble, I'd advise you to add pyyaml>=3.10 in your requirements.in file. The IrrationalVersionError is thrown by a distutils2 library (we simply distribute it to avoid creating a dependency on distutils2), so is out of our control. I personally think this is a good thing, and projects should start to adopt a somewhat standardized versioning scheme\u2014this definitely helps, so I don't feel like working around it in pip-tools.\n\nLet me know if you still experience any issues.\n. Try running it with the --include-sources flag :)\n. I'd rather not let workarounds for peripheral tooling like this (even though it's arguably pretty common) bleed into the pip-tools project. The problem is that I cannot possibly know all the odd uses of pip out there (I'm personally not an apt user). However, this sounds like something that could be scripted easily by yourself, by filtering pip-review's output. Then each tool does one thing, and does it well.\n. I have no idea. I suppose this should be done by iterating over the directory contents of the virtual env in some way.\n. Well, that solution assumes a /usr/local/lib dir, not a virtual env, and assumes eggs. Both are not generic enough. I think you can mimic this logic in your own scripts if it applies to your environment.\n. Wouldn't you be able to use\n$ pip list --outdated --local\n?\n. And if you add that --allow-external option? :)\n. Aren't those warnings, while the rest of the listing of $ pip list --outdated --local continues? In that case, you might want to try running it as $ pip list --outdated --local 2>/dev/null instead?\n. Actually, it's not a workaround. pip-review will be deprecated once pip-tools makes it to 1.0 (which is hopefully soonish). The work on the future branch will replace pip-review + pip-dump with pip-compile and pip-sync instead. pip-review will be gone and pip list --outdated will be the proper tool to use from then onwards.\n. What Python version are you on? What command did you run that triggered this?\n. What OS?\n. I found this similar report that seems to imply that this has to do with a recent Ubuntu upgrade. Can you confirm that's the same on your end? If so, that's probably a bug with pip itself, not this package.\n. \"the pip tool\" is \"pip\"\n\"pip-tools\" is a separate project, not associated with pip or PyPA (the organization)\n. I'm not sure if I know how to help with this.\n. Awesome :)\n. We lowercase deliberately, because there are other subtleties that break if we don't. To be clear: this is a warning only, right?\n. I agree we could do this, but it requires more bookkeeping. Pull requests will be accepted for this feature, if provided. I'm closing this issue.\n. Thx!\n. Thanks\n. Python 3 support is coming, I'm working actively on it at the moment.\n. Thanks, sorry I did not merge earlier :)\n. +1 to this idea.\n. Thanks!\n. pip-dump is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. The name of the package is piptools, without the dash.\n. pip-tools (with a dash) is the name of the project, and the key under which it is registered on PyPI. piptools (without a dash) is the package name. And yes, it is a normal Python package. pip-tools could have shipped with more than one package, but it does not, like most Python packages. It is not named with a dash, because Python prohibits the use of dashes in package names.\nCompare this, for example, with a project called Flask-SQLAlchemy. It's a project name, not a package name. The package name that becomes available after installation is flask.ext.sqlalchemy, not Flask-SQLAlchemy.\nHere's what I get after installing pip-tools:\n``` pycon\n\n\n\nfrom pkgutil import get_loader\nget_loader('piptools').get_filename()\n'/Users/nvie/Projects/pip-tools/piptools/init.py'\n``\n. Sorry, I don't think I can help, because I don't understand what it is you're trying to do.\n. As [said before](https://github.com/nvie/pip-tools/issues/109#issuecomment-54085116) these packages should not be reported bypip freezein the first place. I don't think checking for the install path is the best way to solve this.\n. I believe this should be addressed, if at all, inpipitself, not inpip-tools. Imagine an option flag topip freeze, like--local, only listing packages from the \"current\" virtualenv, not systemwide packages. If this apparently does not work on Debian systems, I think you should file a bug forpip freeze. I don't feel like working around this issue frompip-tools.\n..pipignoreis now gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Thanks, fixed!\n.pip-review` is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Hmm, this is the strangest thing I've ever seen. It actually looks like a conflict to me. Can setuptools even deal with this? Normally, the version specs are AND'ed together, not OR'ed. So in this case that would mean:\n\n\n\ndjango-mptt==0.5.2 AND django-mptt==0.6 AND django-mptt==0.6.1\nwhich... will never be possible. This is exactly what pip-compile is failing on for you.\n. Oh wow, thanks for that link. This shines a new light on how we've been resolving dependencies. We might need to review some of the internals to fix this.\n. Thanks!\n. Can you provide me with the Python version and a dump of your pip freeze? Thanks.\n. I'm happy to accept a patch that fixes this problem. I don't have access to a Windows machine myself to test on, though.\n. This flow is supported since pip-tools 1.6. It's even the default behaviour now! If you have a requirements.in and requirements.txt file, and you add a new dependency to the .in file and re-run pip-compile, it will only add the new dependency to the .txt file. Similarly, if you remove a dependency, it will remove it (and any nested dependencies that are not used by other packages).\nThe only way to enforce existing packages to be upgraded these days is to use pip-compile --upgrade explicitly. Hope that helps!\n. pip-dump is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Yeah, the history of this project is that it started out as a collection of shell scripts, then evolved to more Python. It was never built in Python or Windows support from the start. That said, feel free to change these bits (preferably one commit at a time, to make reviewing the changes easier).\n. pip-review only exists in < 1.0 versions of pip-tools, and will not be continued development.\n. Thanks!\n. Thanks, yes, that'd be appreciated!\n. Thanks for the hard work, @michaeljones!\n. You should check out the future branch on this repo, which is all about solving this very problem.\n. Additionally, you may check out this blog post: http://nvie.com/posts/better-package-management/\n. I've released the current master branch as 0.3.6 now. Thanks for the work!\n. pip-dump is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Thanks :)\n. I'm happy to merge this in if someone wants to make a pull request for this.\n. Hi @b4stien, that's great. I'm wondering what \"it\" refers to? And what do you mean by vendorized/declared-as-dependency exactly?\n. Hi @b4stien, thanks for that! This weekend I have been trying out a few things that could be changed about the current pip-tools implementation. A lot of the requirement parsing, spec resolving, etc. has been NIH'ed by me when I first wrote this tool and it turns out that most of this logic is also accessible in a reusable fashion inside pip \u2014\u00a0at least it is nowadays.\nOne of the current problems with the pip-compile code is that I think it has become way too complicated, and I basically want to rewrite it now that I'm more familiar with pip/packaging/setuptools' code bases, and with the experience of writing it the first time, of course.\nMy goal is to reuse pip's internals everywhere unless that proves to be impossible. So far, it seems that only the dependency resolver itself is not available in pip, but I think that's pip-tools' core value anyway. All the rest is hacking around or reimplementing concepts that are solved problems in pip.\nThis means I'm going to rewrite pip-tools' internals quite rigorously. This will mean that I'll also fully adopt the helper classes from packaging, which means we'll have the prerelease issues covered, too.\nMy plan is to drop support for all pip versions and only support >= 6.1. I hope that by doing this, it will lower maintenance on pip-tools, keep the code much more straight-forward, and we can support all the packaging edge cases a bit better (i.e. no more hacks to support --find-links, etc.)\n. It's not necessary anymore. Re-running pip-compile will locally modify your .txt file, which has the same effect pip-review had.\n. I've removed the need of using BytesIO, it broke for me on Python 3. It threw an error and left an empty (so invalid) ZIP file in the cache, corrupting things, and requiring a manual removal of the cache. By not specifying the encoding here upon write, and opening the file in write mode with the same implicit defaults as pip's internals opened the file with in the first place (I had to read the pip source), avoids the different behaviour in both Python versions. Let me know if this is still causing issues for you.\n. I don't think pip-tools broke anything. You should have read the original error message and addressed it, whatever it was about. Blindly using sudo can cause all kinds of damage and you should avoid it unless there's absolutely no other way and you actually need root access.\nI'm sorry to say I have no clue what happened in this particular case, and I'm unable to help you :(\n. Good to hear you have recovered. I would recommend not using the system Python version ever and instead use a virtualenv that is owned by a non-root user, and never use sudo to install a Python package.\n. Thanks!\n. I've redesigned everything, making all tests useless (see #153). We should add them back in, but it requires a little bit of thought and organization, so not just yet!\n. Hey Daniel. Yeah, sorry for this to be a little sudden. It was indeed rather spontaneous. I was looking at these incoming pull requests from last week, and felt like they were all hacks that were necessary due to some design choices made in the past that weren't the best way forward. (This is not to say the authors were putting hacks together! Instead, the core wasn't simple enough to support many use cases in the first place.)\nThen I just thought \"well, screw this, I'll try to rewrite it from scratch right now\".\nI feel that sometimes an issue with spending free time on open source projects is that some work simply needs to be done that requires a burst of attention, rather than a few hours here and there.\n. I've included this in the redesign #153, too!\n. This is also fixed by the rewrite #153. Thanks for the work nonetheless!\n. This use case is exactly what the pip-compile script on the future branch is all about. I'm getting closer to a 1.0 release, which will remove pip-dump and replace it with pip-compile. I'm not going to patch pip-dump (aka the master branch, aka any 0.* release) anymore in favor of the new tool suite.\n. Whoops, this targets the wrong branch. Closing and will reopen for future.\n. Hi @lseek, thank for this. I'll have a look into what it takes to support this. I've personally never used them, but pip-compile should definitely support these use cases too. It may have a bit of an impact on the internals, but I'll have a look and see if I can add this.\n. Do you have an example of a package that uses extras, so I can look into this?\n. I'm going to work on this as far as time permits me. I've added a message that this is not yet supported in the meantime.\n. Agreed, this is unanticipated behaviour. The pip-tools==0.3.6 output is plain wrong. The exact semantics here are unclear to me. I do consider it a bug if this behaviour differs from pip. To avoid stepping into the pitfall again of supporting all edge cases, I'm happy to block the particular use case of adding direct links, whether they are \"normal\" links or VCS urls\u2014in essence, it requires downloading the package and installing it.\nIf we can support this by using pip internals, great. Until then, I'll go for a patch that detects this use case (InstallRequirement has a link, but is not editable?), and emits an error that this is not yet supported.\n. I've added a message that this is not supported, so it fails explicitly, instead of doing the wrong thing. The error message now suggests using the -e option.\n. Thanks for finding this bug. It's now fixed.\n. This was fixed in ef0d1808609557dfbe04f98f2bd8668866a00512.\n. Closing this since pip-review is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Sorry 'bout that \u2014 reopened.\n. Thanks a lot!\n. This is now merged into master, so I'll manually close this PR that targets the future branch.\n. Sorry 'bout that \u2014\u00a0reopened.\n. Hey @blueyed, I've tried understanding this PR multiple times now, but I think there's just too much going on. If you want, you can try submitting a smaller PR against the latest master, and I'm happy to review again. Try breaking things up a little more, so I can review smaller patches at a time. Thanks!\n. I'm not sure what XDG is, but I don't think it should bleed into pip-tools.\n. My gut feeling says this is an XDG specific thing, that does not apply to all users/envs. Personally, I had never heard of this standard. Windows users may require a different standard. Mac OSX another one. If pip indeed uses this, that's great. Perhaps there's a way of using the pip internals to get this cache dir? Alternatively, we could support a command line flag to set it, though that may not be ideal.\n. I'll add it myself\n. This is addressed in pip-tools==1.0.5 now. Thanks!\n. This is addressed in pip-tools==1.0.5. Thanks!\n. Sorry 'bout that \u2014 reopened.\n. Merged into master, so manually closing this now. Thanks!\n. Since speed is the main goal here, I don't think we should add this to pip-tools. I want to keep it light, and try to keep support for edge cases minimal. This issue was opened Jun 2015, I assume that was still in the pip 6 days. Pip itself has seen massive speed-ups during that time, and pip-compile has gotten way faster as a direct result of that, so I'm not sure if this is still a valid argument. I'm closing this issue now.\n. pip-review is gone with the release of 1.0: http://nvie.com/posts/pip-tools-10-released/\n. Closing this. Dupe of #325.\n. Thanks for the patches, @dstufft! :)\n. Thanks for catching this one! Fixed in e964f3f92b96d58a6666198a57221e231620981b.\n. This is great\u00a0\u2014\u00a0thanks, @hugopeixoto! I got one error trying this out locally. I had an environment where I had installed Jinja2==2.6 manually, then ran pip-sync with the following input file:\n```\n\nThis file is autogenerated by pip-compile\nMake changes in flask.in, then run this to update:\n\npip-compile flask.in\n\nflask==0.10.1\nitsdangerous==0.24        # via flask\njinja2==2.7.3             # via flask\nmarkupsafe==0.23          # via jinja2\nwerkzeug==0.10.4          # via flask\n```\nThen I got the following error:\n```\n\npip-sync --dry-run examples/flask.txt\nTraceback (most recent call last):\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.9/bin/pip-sync\", line 9, in \n    load_entry_point('pip-tools==1.0dev1', 'console_scripts', 'pip-sync')()\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.9/lib/python2.7/site-packages/click/core.py\", line 664, in call\n    return self.main(args, kwargs)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.9/lib/python2.7/site-packages/click/core.py\", line 644, in main\n    rv = self.invoke(ctx)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.9/lib/python2.7/site-packages/click/core.py\", line 837, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.9/lib/python2.7/site-packages/click/core.py\", line 464, in invoke\n    return callback(args, **kwargs)\n  File \"/Users/nvie/Projects/pip-tools/piptools/scripts/sync.py\", line 27, in cli\n    to_be_installed, to_be_uninstalled = sync.diff(requirements, installed)\n  File \"/Users/nvie/Projects/pip-tools/piptools/sync.py\", line 25, in diff\n    elif module.version in requirements[key].specifier:\nTypeError: argument of type 'SpecifierSet' is not iterable\n``\n. This is AWESOME. Thanks, will merge immediately :)\n.pip-review` is gone in 1.0: http://nvie.com/posts/pip-tools-10-released/\n. I agree pip-sync should be able to handle a list of file inputs. When inconsistencies are detected, we can do any of these:\n1. Behaviour is undefined when inconsistencies are found\n2. Refuse to sync until inconsistencies are resolved (and report the inconsistency)\n\nI'm for option 2 here, possibly with a --force option to ignore conflicts and falling back on 1?\n. I do agree with @Koed00 the issue that pip-sync will uninstall pip-tools itself (or its dependencies) is a more immediate issue that should be tackled first.\n. Thanks, I fixed this one in pip-tools==1.1.1, see 74d175770e76c64141ec37935373ec09a068bb94.\n. This is fixed in pip-tools==1.0.3.\n. I don't think it's useful. If there's a specific flow that depends on the tool, let's see if we can figure out an alternative.\nEither you could run pip-compile (and check the git diff) or run pip-compile --dry-run (and diff it to your current requirements.txt yourself), or run pip list --outdated (to check your venv for outdated packages).\nI do agree that the guided interactive prompting would be a nice to have, but I don't think we need to bring back pip-review for it. Perhaps we could think about a pip-compile --interactive instead, but I'd need to think about more details here.\n. Unless I'm missing something, I think you can all use pip list --outdated instead, which has gotten much better lately, and is also maintained. Today, it basically provides the same features, and has replaced the need for pip-review entirely.\n. I can see the convenience in that, but it's not enough to keep around a separate project for and maintain it even if pip already offers most of this. Since the only added value is the convenience of prompting for upgrades, I suggest you script around pip list for that. A rudimentary simple way of doing this in a shell script that would basically bring back pip-review is this:\nsh\npip list --outdated | while read pkg; do\n    echo $pkg\n    echo 'Update now? [yn]'\n    read answer\n    if [ \"$answer\" = \"y\" ]; then\n        pip install -U $(echo \"$pkg\" | cut -d' ' -f 1)\n    fi\ndone\n. @bnice5000: for the best rationale behind the tool over pip install -U, see this blog post I wrote up a while ago: http://nvie.com/posts/better-package-management/\n. That's great, thanks!\n. Great, thanks!\n. This exception is a Windows-specific exception related to the atomic-renaming of files. Apparently, the file handle is still open and thus the file cannot be renamed on the Windows platform. We could fix it here, but this particular code is vendored from the excellent boltons library, so it should really be addressed there.\nA minimal example that should fail for you is:\nconsole\n$ pip install boltons\npython\nfrom boltons.fileutils import AtomicSaver\nwith AtomicSaver('foo.txt') as f:\n    f.write('whatever')\nThat should give the same failure for you probably. If so, please do report this to the boltons library, and when it's addressed, I'll vendor back in that latest version.\n. @MrLeeh Thanks! I've updated the code to match the latest version from boltons. Can you confirm it works for you if you try out the latest master? If so, I'll release this to PyPI.\n. Awesome! It's now released at 1.1.3 on PyPI.\n. This should be fixed in pip-tools==1.1.0. Thanks for reporting!\n. Can you try 1.1.2? :)\n. This is awesome \u2014\u00a0I went in and did a bit of polishing/renaming and added some docstrings for clarity. Thanks, @hugopeixoto !\n. Released this as pip-tools==1.1.0 :)\n. Thanks! I've merged this in with the changes as made by @davidovich in #208. Thank you both!\n. This has been released as part of pip-tools==1.1.5 just now. Thanks for contributing!\n. Correct, this is a missing feature. pip-sync should support it. Any volunteers for implementing this?\n. Thanks!\n. Thanks @mahmoud, I did upgrade this back when you reported in August, but I forgot to thank you for it and close this ticket :)\nBtw, do you have any idea if this should be fixed in AtomicSaver? I think it'd be a nice little corner case to fix generically: #268 \n. Yeah, it's super annoying. I've just pushed a fix for this to PyPI (pip-tools==1.1.4). Sorry about the delay here.\n. Thanks!\n. This has been released as part of pip-tools==1.1.5 just now. Thanks for contributing!\n. Thanks, @paulkernfeld! Your analysis is spot-on, and I really look forward to properly support these. If you can address your point 3 (i.e. change the format of the dependency cache), then I'm happy to merge this in!\n. This is great work\u2014thanks a lot, @paulkernfeld! Sorry I didn't find the time to review this a bit earlier. I had one case locally that behaved a bit strange. If I create a test file with the following contents, it still compiles all the extras, where I expected just the main ipython package:\nipython#[notebook]\nNote: there's no space between the 'n' and the '#'. I'd assume this would be considered a comment, but it wasn't. If you add a space, it acts how I would expect (the #[notebook] is considered a comment).\nOtherwise, this looks fine, and I'm happy to merge it in.\n. OK, I don't care in that case. If this is a bug in pip, it'll probably be fixed. If it's not, then we're just pip-compliant. No big deal.\nThanks for the work, I think it's awesome :)\n. This should be fixed as a side-effect of the pip-tools==1.4.1 release I just pushed to PyPI. Let me know if this is still a problem!\n. I've added support for this in 9501b18e37ccbce1cb69aa2cf3cd89953c3a2ece. Thanks, @AlexRiina \n. Thanks!\n. This has been released as part of pip-tools==1.1.5 just now. Thanks for contributing!\n. :+1: \n. This has been released as part of pip-tools==1.1.5 just now. Thanks for contributing!\n. I'd rather not remove this directive, since it may have effects elsewhere (i.e. where we print out log lines for packages with non-ascii names). Instead, wouldn't this simply fix the problem, too? https://github.com/nvie/pip-tools/compare/nvie:d2dde84...nvie:5e7fcb3 Let me know if this works for you.\n. Thanks @Chronial, this is fixed in 3a439045c71851151c51953b79f5994d3a12d9fe now.\n. Thanks!\n. This is now released as part of pip-tools==1.1.6. Thanks for the patch!\n. I just merged #222, which addresses the same issue! Thanks!\n. Closing this. Dupe of #325.\n. I don't know what is causing this. Apparently python setup.py egg_info fails when executed on this package. Can you run that manually and see if there's perhaps more output? Any help here is appreciated!\n. I'm making #343 the main issue for tracking this.\n. Oh, this could actually be a powerful feature. Who curates that DB? How is it kept up to date?\n. I like the idea, but perhaps this should not belong in pip-tools\u2014it'll be hard to support / keep up to date. That said, I would totally like to have a tool/script I could run that just checks the contents of requirements.txt against this DB and report which packages are recommended to be updated. Then use pip-tools to record/perform the update.\n. Thanks!\n. Thanks, I've modified the messaging slightly and released this as 1.3.0!\n. I'd add an --output-file (-o), defaulting to the autogenerated file name. This will be a little more generic. Happy to merge if someone can provide a patch.\n. Seems that #265 may already support this. Will review shortly. \n. Thanks!\n. pip-compile \"just\" creates a file, the OS/Python is responsible for applying the correct umask, not pip-tools.\n. I believe this should be fixed in the boltons library instead. I want to keep this code vendored in, not branch off from it. Feel free to open an issue on https://github.com/mahmoud/boltons/issues, though. If it gets fixed there, I'll just include it here again. Thanks!\n. I'm sorry, I checked the wrong branch for some reason, assuming AtomicSaver has not seen any changes. The new version looks great to me! I have just included it. Thanks for this awesome library and your quick response, @mahmoud :)\n. This should now be fixed with pip-tools==1.4.2. Let me know if you still experience this issue, @chaoflow!\n. Closing this, see #268. (This should be fixed in boltons instead.)\n. This has been addressed in 1.4.1. It was a weird bug, but fixed in 8f64896e11a1d34a27b4f4c5d12bed04336a7c75. Thanks for finding/reporting!\n. Thanks!\n. There's no reason preventing it per se, it's just not been implemented. The main thing that's tricky is to figure out how to distill the dependencies from it, and what name the package ends up with locally when you install it. You cannot derive these properties from the URL itself, you'll need to download the package, unpack it, inspect it. So there's definitely some work involved. If you feel brave, I invite you to try and add it to pip-tools, it'd be a cool feature :)\n. I dont understand this feature yet. What is the use case? Is it that if you put django>=1.8,<1.9 in .in, then compile, that this same spec (django>=1.8,<1.9) would remain (\"be copied\") to the output .txt file (rather than getting 1.8.7 pinned or so)?\n. I don't think pip-tools is the project for this (it's quite the opposite of the philosophy). If your use case requires it, why not simply have a requirements.txt and no requirements.in at all?\n. If you did not want to upgrade to Django 1.9 yet, you can put django<1.9 in your .in file, and it would never compile to a version that's 1.9 or higher. Still, it would put you on the latest version in the available range. If you don't want that, you can even pin the version in your .in file, i.e. django==1.8.7, and it would never go to Django 1.8.8 or higher.\nI'm not sure what use case I'm missing.\n. I have briefly experimented with the idea of allowing a \"directive\" syntax in the *.in files, e.g.\ndjango<1.8,>=1.7    # pip-tools:verbatim\nOr something similar. I think this is too much of an edge case to support generically in a tool like this. I think you can solve this problem yourself easily by scripting around pip-tools here. I.e. where you invoke pip-compile now, instead do the following:\n$ (pip-compile --output -; echo \"django>1.8,<=1.9\") > requirements.txt\n. This is certainly a valid use case, but I'd simply run pip-compile twice in a row. What's the added value of adding an extra option for this?\n. Yeah, the cache is on-disk, so you should get the perf benefit, even though it takes two runs. This just keeps the already large amount of CLI options a bit more managable.\n. Correct, we're not doing a great job here yet. PR #265 already brought - handling for input arguments, but we definitely need to do the same for outputs. I'd like to follow the Unix convention of using '-' as the \"filename\" to specify stdout, i.e.:\n$ pip-compile --output-file -\nShould read requirements.in, and write its output to stdout.\n. The idea behaviour here:\n- Don't name option --footer. It's not necessarily a footer. I just happened to have grouped them there for convenience.\n- The real noise is caused by version changes in these comment lines. I'm fine removing the version spec for these lines from the output. Basically, it'd just say that \"setuptools\" is ignored.\nThis way, we'd solve the problem of those lines not remaining stable over time and causing noise (which I agree is a good idea), but we don't introduce an extra flag.\nWhat do you think, @jonafato? Could you change this PR to reflect that?\n. Agreed, this is annoying. I don't think we can guarantee desired behaviour here under all circumstances. In a way, pip-tools has to work with the output of pip, which on a single machine, is quite platform-specific. However, I would support a command line flag like --ignore appnope or something like that if that would practically solve this issue. If you want to make a PR for this, I'm happy to pull it.\n. How would you detect those without residing to parsing the package's metadata yourself? Right now, I'm basically letting pip do all the heavy lifting by simply asking it what the dependencies are. Not sure if we can detect from there is certain deps are platform specific.\n. Monkey patching sys.platform is likely going to pose issues around command invocation on Windows (pip-tools uses subprocess, which likely uses sys.platform to dispatch OS specific behaviour). As far as the other options we have, I'm unsure about. I agree it'd be nice to automatically exclude/ignore the platform-specific packages, and to add a command line flag to not do that by default. In that case, it may be good to still write the requirement into the requirements.txt file, but to comment it out. (So the user won't be surprised the package is not in there.)\nIf you have time to prepare a PR for this, I'm happy to pull.\n. At the very least, I'll need to have some motivation for this. \n. We have now concluded that appnope itself is not an issue (https://github.com/nvie/pip-tools/issues/285#issuecomment-169847863), so I'm closing this one.\n. Closing, duplicate of #343.\n. One of the edge cases is if you have existing dependencies, i.e. suppose you start out with an *.in file like:\nwerkzeug\nAnd at any point in time that happens to be compiled (so in *.txt):\nwerkzeug==0.6  # an old version\nThen, you add flask to your *.in file:\nflask\nwerkzeug\nIf we then compile this using --no-upgrade, this would only update Flask, right? But now it happens to be the case that the latest version of Flask (which this compiles to) wants the latest version of werkzeug, i.e. it has a dependency of werkzeug>=0.11. Ignoring werkzeug's required upgrade here will lead to a broken *.txt file.\nSo either we should fix this (by also updating werkzeug (and its outdated dependencies, recursively) too in this case), or simply letting this command error in such cases, and advise the user to run it without the --no-upgrade flag after all. Obviously the former approach is preferred :)\n. Perhaps a clearer example / test case.\nSuppose you are already using jinja2 and werkzeug, but not Flask yet.\n$ cat requirements.in\njinja2\nwerkzeug\nAnd suppose your werkzeug is on a (very) old version, but you use a recent (but not most recent Jinja2 version):\n```\n$ cat requirements.txt\n\nThis file is autogenerated by pip-compile\nMake changes in foo.in, then run this to update:\n\npip-compile foo.in\n\njinja2==2.7.3          # NOTE: reasonably recent, 2.8 is newer\nmarkupsafe==0.23       # via jinja2\nwerkzeug==0.6          # NOTE: pretty old version of werkzeug\n```\nNext, you decide to also use Flask for your project. So you add \"flask\" to the *.in file, and then run pip-compile --no-upgrade. Since the latest version of Flask requires werkzeug>=0.7 (we don't have it, but it still fits our specs) and jinja2>=2.4 (we already satisfy that). So what should happen is:\n```\n$ cat requirements.txt\n\nThis file is autogenerated by pip-compile\nMake changes in foo.in, then run this to update:\n\npip-compile foo.in\n\nflask==0.10.1\njinja2==2.7.3          # UNTOUCHED\nmarkupsafe==0.23       # UNTOUCHED\nwerkzeug==0.11.3       # bumped to latest version matching our specs\n...\n```\nThe point being that werkzeug should still be updated, even though we specified --no-upgrade. But Jinja2 isn't. If we had werkzeug<0.10 in our original *.in file, this upgrade should fail since it could not satisfy the new requirement with the existing requirements. Simply ignoring werkzeug in this process because it's already in our output file misses these cases.\n. Interesting, @ejames! I do agree this is a sound approach. Strangely, I did not pick up from the diff that this is how it really worked. This may have been a bit more clear if the code reflected how you just explained it.\nCould we therefore change the implementation so that we instead instantiate a Repository subclass that does just that proxying? I.e. first check the requirements.txt, then the real Repository? If we then just pass that Repository instance along, no other parts of the resolver are affected, right? There would have to be no special casing anywhere else, nor passing around of extra kwargs to several pieces. I think it'd make the code simpler (and thus simpler to read/understand).\nI think the test case is sound. If we can keep that green, we're good.\n. Thanks!\n. Can you double-check if this is not the same issue as this? It seems you're using a global pip as well.\n. What's in our requirements.in and requirements.txt files, and what do these say?\n$ which pip\n$ pip --version\n$ pip list\n. What's your version of pip-sync?\n$ sh -c \"$(dirname $(which pip-sync))/pip list | grep pip-tools\"\nAnd, what does this show you?\n$ pip-sync --dry-run\n. Do you have any idea what virtualenv those packages are coming from? From your earlier output\n(venv)?  pip  which pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n(venv)?  pip  pip --version\npip 7.1.2 from /Users/zackhsi/Desktop/pip/venv/lib/python2.7/site-packages (python 2.7)\n(venv)?  pip  pip list\npip (7.1.2)\nsetuptools (18.2)\nwheel (0.24.0)\nIt's apparently not that one.\n. At least we can conclude that appnope is not a special package, or is causing any issues. It's just that pip-sync ends up with the wrong virtualenv.\n. Is that the same list as\n$ sh -c \"$(dirname $(which pip-sync))/pip list\nwould produce? I.e. that particular env?\n. Could, by any change, your $PATH variable contain a .?\n. What does echo $PATH say?\n. The problem seems to be that pip-sync, installed globally, invokes pip and this leads to a different pip being invoked than the one that gets invoked when you run it locally in a shell that has the virtualenv activated.\nHere's what happens:\n- You invoke pip-sync\n- Your shell looks at the PATH to find the binary to run\n- It checks your virtualenv first (activating it has put it on the very front of the PATH)\n- It does not find it there (since it's not installed in the virtualenv)\n- It continues with looking at PATH, and reaches the path /usr/local/bin\n- There it finds pip-sync and runs it\n- Then, pip-sync invokes pip, and this process starts again:\n- The subprocess will look at the PATH to find what binary to run for this \"pip\" thing\n- It SHOULD check your virtualenv first\nSo either, the PATH is different, or . is on the PATH (leading to the global virtualenv being found), or something else I cannot figure out from reasoning about this.\nIf a . is on the PATH, that's generally a thing you should avoid (many weird or dangerous things can happen in Unix from it).\nOtherwise, you can also try something else: to (also) install pip-tools inside the virtualenv you want to manage. This is harmless to do, but may also cause your issue to disappear. (Still, there's a bug with this, since the global installation should also work.)\n. What's \n$ which -a pip\n$ which -a pip-sync\nsay?\n. What happens when you pip install pip-tools in the desired venv, and then try things again?\n. Oh wow, I have no idea about this. But this last example pinpoints the problem exactly. Does this only apply to the pip-tools binaries? Or is the behaviour the same for all the other binaries in that venv (if any exist)?\nPurely looking at the difference between these two which commands, I'd even be inclined to conclude this is an issue in your shell, rather than pip-tools, pip, or even python\u2026?\n. Yes, exactly. I cannot explain this, what shell are you using?\n. > Of course, I'm invoking the global pip-sync. Should installing pip-tools inside a virtualenv be a requirement of using pip-sync to manage that virtualenv? If so, here's another interesting result:\nSince 1.6.1, you can invoke the pip tools in two ways:\n- Install it globally or locally, however you prefer, and just run pip-sync from inside an activated virtaulenv (pip-sync will install into that env, no matter if it's installed globally or locally)\n- Install it locally, don't use an activated virtualenv, but specify the full path to pip-sync, e.g. /path/to/my/local/env/bin/pip-sync whatever. That will also install packages into that local env.\nHave you tried doing the above on the latest version (1.6.1)?\n. Thanks! Yes, @alexwforsythe please do make a new issue to cleanup the thread a little and have a focused discussion.\n. Thanks!\n. I think this is the only related change between those minor versions: https://github.com/nvie/pip-tools/pull/283/files. Any idea what may be causing this? \n. Is this still an issue after upgrading to 1.6.1?\n. Yes, this has been deliberately changed. The previous behaviour was considered a bug by many. The pip-sync behaviour right now is that it will simply invoke pip, leading to whichever pip happens to be on the PATH first. You can inspect for yourself which version that is by running:\n$ which pip\nThat will likely point to /usr/bin/pip in your case.\n. Glad that fixed it!\n. Oh wow, that's definitely not wanted. I'd even say it's exactly what pip-tools as a project tries to protect against in the first place! Hmm. The --upgrade fix was a fix for a different problem. This poses an interesting challenge\u2026\n. I'm indeed going to revert 1.4.3 to be equal to 1.4.2 first thing as soon as I'm behind a computer (which should be within an hour).\n. This should now be fixed in 1.4.4. Thanks for catching this one! I'll see if I can work out a fix in 1.4.5 for the original problem without introducing this behaviour :relaxed: \n. I like the idea of echoing the command flags to the output file, but I don't feel like adding an --index/--no-index flag just for this. IMHO, this does not have to be configurable. Ideally, if any non-default index is used, I'm fine just always echoing that in the output's header. If it's just using PyPI (the default), then don't emit it. What do you think @daradib? Could you change this PR to reflect that?\n. I'm sorry, I think I mis-read your change (I thought this only reflected the emitted comment line in the header, I missed the actual emission of the --index-url line itself). Yeah, your argument makes sense now. I'll pull this in\u2014thanks, @daradib!\n. Thanks! Much better this way!\n. Can't merge this now (I'm on mobile). Will merge when I'm on a desktop machine. \n. Thanks!\n. Oops, thanks!\n. pip-sync invokes pip, which will resolve to whatever which pip is on your system. In this case, since the env is not activated, this will likely be /usr/local/bin/pip (global installation), not your env. Before 1.4.1, the behaviour was to use the local pip, but this led to more confusion as people who would install pip-tools system-wide would see their system wide packages installed/uninstalled, which is more harmful than this. I guess we can't have the cake and eat it too :)\n. Closing this now. If this was not the case, feel free to reopen it.\n. All I can say here is that I'm very torn on the subject. Your argument is perfectly valid, and I can see why it confuses you. On the other hand, the argument against inadvertently removing system-wide installed packages is also valid: if users did not install pip-tools in their local envs, either by accident or because they assumed they could use a globally installed pip-sync. So I don't know a good solution here that works for everyone. That's why I'm erring on the safe side with this.\n. I think this is a good approach indeed! Do you feel like implementing a PR for this, @chaoflow?\n. Thanks, good idea!\n. Thanks!\n. Thanks!\n. @ejames I'm so happy with this PR! The way the implementation ended up is very elegant, and it works like a charm. I definitely want to make this a major release to call out some attention to this feature. However, I don't really like the command line flag's name (and the class name) yet.\nThat said, I'm thinking about making this behaviour the default, and having it emit a console message to stderr to indicate people can do a \"full upgrade\" using a --upgrade option. How about that?\n. I'll merge this in now as-is and I'll add another patch to make this the default option before releasing this as 1.6. Thanks, this is an awesome improvement to pip-tools I'm sure everyone will love :)\n. Done in 72ac64c...3a2de92.\n. Yeah, the packaging tools sport a lot of flags and options, making for many edge cases. I've never heard about --system-site-packages until now. Can those packages be detected from the output of pip list? If not, this is going to be tough to support. (I'm also not aiming to support all use cases, only the ones that will not turn the pip-tools source into a big hairball.)\n(Btw, I just released pip==1.5 with pip8 support, but this problem is unaffected by it.)\n. This looks really interesting. Is there currently a way of generating those lines in some way? Or does one currently really have to copy the hash from the download URL and manually paste it into the requirements.txt?\n. \"Fixed\" in 1.4.5. For now, you'll have to use pip 7. I'll look into what the upgrade to 8.0 meant for the internals, and try to fix things so pip-tools works with 8.0+ later.\n. I've just released pip-tools==1.5, adding support for pip>=8. Have fun :)\n. I agree with @blueyed here. You typically also check in the requirements.txt into version control, which should reflect the last time the file changed anyway.\n. Closing this in favor of using file modification dates and/or VCS systems for tracking these dates. Thanks for the work/initiative though!\n. What happens if you manually run this command?\n. Can you specify the contents of your .in file here?\n. Agreed. This is as intended. I do agree the messaging could improve. Closing this in favor of #220.\n. I'm not sure if I follow. Can you explain what you expect and what the actual result is?\n. There recently (since pip 8) popped up this bug: #313, sounds like they're the same. I'm closing this now, since this bug has been addressed and is part of the pip-tools 1.6 release of earlier this morning. Let me know if that fixes it for you. If not, reopen this issue so we can investigate.\n. Thanks!\n. Thanks, @scottbelden !\n. Can you give an example .in file that does not have the problem and one that does? Thanks!\n. Thanks!\n. Sounds like a bug in pip to me:\nconsole\n$ pip install decorator==4.0.8\nCollecting decorator==4.0.8\n  Using cached decorator-4.0.8-py2.py3-none-any.whl\nInstalling collected packages: decorator\n  Found existing installation: decorator 4.0.7\n    Uninstalling decorator-4.0.7:\n      Successfully uninstalled decorator-4.0.7\nSuccessfully installed decorator-4.0.7\nThis happens even without the use of pip-tools. Perhaps report over at pypa/pip?\n. No problem :+1: \n. The script pip-review does not exist with pip-tools 1.6 anymore. It's been removed since 1.x. It is replaced by pip-compile and pip-sync since then. Are you perhaps using a 0.x version of pip-tools?\n. Yeah, pip-review and pip-dump where more directly acting on the installed env, which is less reliable, and therefore more likely to give different results on different machines. The pip-compile and pip-sync scripts have been created as a better alternative to the problem of pinning your dependencies, by making (\"compiling\") the list which is a declaration that the env should obey to, and pip-sync makes it so by installing/uninstalling all packages so the env matches the prescription.\nI do realize taking away pip-review has caused some pain with people that liked it, but I believe it's for the better. Onwards and upwards :)\n. This code is vendored in from the excellent boltons library. See https://github.com/mahmoud/boltons/blob/master/boltons/fileutils.py#L236 for the original code. If this contains a bug, please open an issue against that library. If the fix does get included there, I'll re-vendor it in here. Thanks!\n. The requirement of <= is weird, and non-standard. Mathematically speaking, 1.9 is included in that range, but 1.9.1 is not. This is almost never the intention. So they likely either mean to say < 1.9 (if only 1.8.x should be supported) or < 2 (if 1.9.x must be supported). Pip-tools just follows the spec. The place to fix this is inside this package itself, not in pip-tools or pip.\n. Rather than coding up an entire command for this, couldn't you just run pip freeze > requirements.in? This tool is about as complex as I can/want to maintain for now. I'd like to keep pip-tools as simple and focused as possible, as pip contains a lot of inherent complexity.\n. To run the tests, run:\n$ tox\n. Thanks!\n. If someone can make a PR for a --exclude <pkg> flag, I'd be happy to merge the PR. Please make sure to allow multiple occurrances of this flag, i.e. --exclude foo --exclude bar ...\n. Thanks!\n. TIL about format control. Ideally we could just pass through these flags in a way that would not require pip-tools to specifically add support for it, but I don't know a better way for now.\n. Thanks!\n. This is already fixed in #371, closing this now. Thanks!\n. I've generalized the logic a little bit, but thanks for initiating this!\n. This is indeed the expected behaviour. I've just updated the README file to reflect this a little better. Thanks!\n. > Generally it seems to me that pip-tools should find another way around to figure dependencies out. Because this discovery means that every package that pip-tools processes, needs to have its build dependencies satisfied.\nYeah, that would be ideal. This is notoriously hard with Python packaging though. As the thread you linked points out, Python packages don't carry static metadata, so they typically need to execute setup.py to acquire that information. If you have ideas for a better solution, I'm obviously interested!\n. Thanks for the PR, @rfleschenberg. This has however already been fixed in the latest master, so this branch does not merge cleanly anymore. I'll close this PR now.\n. The error message definitely needs to be improved, this is a low-level error that should be handled. That said, these requirements conflict and pip-tools will not be able to handle this.\n. I don't think picking either version is a good choice. pip-tools was born from the need to make these conflicts explicit in the first place. In a way, detecting these conflicts is it's job. The conflict itself should be addressed, pip-tools should not ignore it.\n. LOL, thanks!\n. Weird, I cannot replicate this locally on a Mac. I was using the exact same commands you were using here. Must be some platform-specific issue.\n. Thanks for the report. Are you on the latest version of pip-tools with this? The reason I'm asking is that I'm pretty sure this has been addressed in an earlier version, so this would be a regression.\nThe code that's failing is vendored into pip-tools from boltons, a pretty sweet library maintained by Mahmoud Hashemi. \nI believe this was the issue back then, which was fixed: https://github.com/mahmoud/boltons/issues/45. Assuming you're on the latest version of pip-tools, perhaps we can see if @mahmoud has any idea what could be causing this?\n. Ugh! So this was fixed, and I remembered this, but I did not vendor this back in? That's pretty stupid of me :) I'll have a look later, probably tomorrow. Sorry about that :)\n. This is now fixed and released as 1.6.3, see https://pypi.python.org/pypi/pip-tools. Thanks for reporting @floqqi and thanks for fixing @mahmoud!\n. Hmm. @mahmoud any idea what could be happening here? Are you running this for a user that's allowed to overwrite requirements.txt? Any error when you run this?\n. Does it fail silently, or do you see a stack trace? Please share if so.\n. If you remove the *.bak file, does it keep getting back? If so, that's a bug. If not, can we close this?\n. Sweet. Once it's released, I'll roll a new version of pip-tools right after.\n. I just pushed out pip-tools 1.6.4 that includes this fix, see https://pypi.python.org/pypi/pip-tools. Thanks for the contributions!\n. Wow, that's pretty bad. Thanks for reporting, this obviously needs to be fixed.\n. This is a bug. Thanks for reporting!\n. I don't understand this one. I tried running your test case on master and got a test.txt including django and django-queryset, no matter if I had django-autocomplete-light installed or not (sounds like the correct behaviour to me). When running this on your branch however, those 2 extra dependencies disappear for me when django-autocomplete-light is installed.\nSo it seems that the expected behaviour you describe is already the case on master and is not present on your branch.\nCan you clarify?\n. pip made some changes to its internal data structures in 8.1.2 it seems. I'm having a look to see if there's a quick fix for this or if it's more fundamental.\n. Hmm, yeah these changes affect pip-tools at the core. We need to carefully inspect how these data structures were refactored and how we need to follow those new patterns. For now, I'll address this by issuing a warning that 8.1.2 is not supported in pip-tools 1.6.*, since I lack the time to properly fix this now.\nI'll change the codebase to match 8.1.2, but it won't be a backward compatible fix, so I'll roll a new minor release for it when it's done.\nThis will be the situation:\n- pip-tools < 1.7 will only support pip < 8.1.2\n- pip-tools >= 1.7 will only support pip >= 8.1.2\n. pip-tools==1.6.5 is now available which errors with a more helpful message than a random stack trace with internals.\n. Should be fixed in pip-tools==1.7, which was released yesterday!\n. Closing since it's superseded by #403.\n. @zeebonk is correct here. This question has occasionally popped up here, perhaps I should put it in a FAQ of some kind. The purpose of pip-tools is to expose these issues with dependencies. If you don't use pip, you'll get an arbitrary version installed, which leads to undefined behaviour in production if you're unaware of which version you're getting. In this case, package A explicitly declares to be incompatible with any version of Django other than 1.4. Package B ditto for 1.8.7. There is no version of Django that can ever match these version requirements. Picking either version will just lead to trouble with the other package. At the core, this problem needs to be solved by reaching out to the package A and B maintainers and asking them to remove the explicit pinning of dependencies, which is almost always a bad idea. What is typically really meant by ==1.4 is >=1.4 (to express forward-compatibility) or <1.5 (to express backward compatibility.\n. Not sure how to tackle this. If you have a suggestion, please submit a PR.\n. Great catch, and great report, too! This behaviour can definitely be explained by the current implementation. After a few rounds of dependency resolution, some package version (in this case ampq 2.x) may indeed include some extra packages (in this case vine). Then after some more rounds, it may turn out ampq 2.x cannot satisfy the dependencies, but 1.x can, so it \"drops down\". This action should then also remove vine from the dependencies again of course, but this is not checked in the current implementation.\nThe proper way to implement this is probably to trace all dependencies back to all the instances that referred to it. So that, as soon as we remove ampq==2.0.2 from the graph, the vine package becomes unreachable and gets dropped, too.\nThe explanation for why it works the second time is that it's not run with the --upgrade option, which would re-hit PyPI to get the latest versions. If you omit an explicit --upgrade, the normal behaviour is to only add/remove lines but not touch versions (unless we have to). The way it achieves this is by pretending that the existing requirements.txt is the \"first PyPI\" server it checks. By that time, it concludes that there's only ampq==1.4.9, and as such the vine dependency is unused and thus removed. Two ways to replicate the initial behaviour would be to either: (1) edit the requirements.txt manually to point ampq to 2.0.2 and then re-run pip-compile. This will bump the version down again, but you'll note the vine dep is not removed; or (2) call pip-compile --upgrade.\nSo yes, this is a bug, and I welcome a PR for it for anyone that wants to give it a try.\n. Thanks for the contribution!\n. Sorry about the long delay here. I've been incredibly involved lately and having a hard time keeping up with issues/PRs on my open source projects at the moment.\nRe the ticket: I don't understand what the pin in that example line is doing:\ngit+https://github.com/simon-weber/gmusicapi.git@develop#egg=gmusicapi==10.0.2rc1\nWhen I change the 10.0.2rc1 to 0.1 here locally and recompile, I get the exact same output, same dependencies. I can even change it to foobar and it still works. So it doesn't seem to do anything?\nHere's what I had in mind for the feature.\nUnfortunately Pip is making all of this hard since there are 3 different concepts at play here:\n- Name vs URL, so my_package vs https://example.org/my_package.tar.gz.\n- \"Just\" URLs vs VCS-based URLs, which are treated specially. So https://example.org/my_package.tar.gz vs -e git+git://github.com/foo/bar.git#egg=bar.\n- Pinning to a specific symbolic reference for VCS URLs: So -e git+git://github.com/nvie/pip-tools.git#egg=pip-tools vs -e git+git://github.com/nvie/pip-tools.git@example-branch#egg=pip-tools.\nIn this latter case, the symbolic name example-branch points to a specific commit at a specific time. When you call pip-compile on an input file that has this branch name in the URL, it should \"compile\" this specification to the following URL:\n-e git+git://github.com/nvie/pip-tools.git@1db806f2f92375bd37b5e5c2b4fe4481e43deb3c#egg=pip-tools\nIn other words, it \"resolves\" the branch to the commit it points to at compile time, and pins it to that. When you invoke the same command later, the commit may be different. Normal upgrade semantics should be at play here. If the branch has advanced, but requirements.txt still exists, don't upgrade it, unless --upgrade is passed along, in which case we should re-resolve.\nVCS URLs should not be dealing with version numbers. Their branch names as given after the @ symbol in the URL should be considered to be the \"spec\", very much akin to foo, or foo<2.0, or foo>=1.4. The \"actual commit\" they point should go into the output file and is very much the same as a \"pinned version\", i.e. foo==1.7.9.\nDoes that make sense?\n. tl;dr\nInput:\n-e git+git://github.com/nvie/pip-tools.git@example-branch#egg=pip-tools\nOutput:\n-e git+git://github.com/nvie/pip-tools.git@1db806f2f92375bd37b5e5c2b4fe4481e43deb3c#egg=pip-tools\n. Wow, @jmbowman! This is fantastic! Thanks for making this effort. I like how you even managed to keep the stuff backward-compatible! \ud83c\udfa9 \nI just tried it out myself and discovered a few more places that had to be touched, but those were easy fixes. You can check them out in 90349b55e66431037199597bbeed9c659325cd7d.\nThis is so very appreciated, thanks so much! \u2764\ufe0f \n. Can you verify that this is not a Python version issue? We've seen this in the past. Seems like you're using Python 3 for running pip-compile. In the second snippet, is the python command you're running also Python 3?\n. Can you remove the short option (-u) and rename this option (both internally and externally) to --allow-unsafe? Otherwise, looks good to me. Thanks!\n. Thanks!\n. I might roll a new version some evening this week.\n. Thanks, this feature is awesome, and sorry I have not merged this before, @coagulant !\n. This is such a lovely feature!  Thanks for your efforts here, @dstufft\u2014even though this has been submitted months ago.  While testing this locally, I found one issue with it (under Python 2 at least):\n$ pip-compile examples/hypothesis.in --generate-hashes\nTraceback (most recent call last):\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/bin/pip-compile\", line 11, in <module>\n    load_entry_point('pip-tools', 'console_scripts', 'pip-compile')()\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/nvie/Projects/pip-tools/piptools/scripts/compile.py\", line 187, in cli\n    hashes = resolver.resolve_hashes(results)\n  File \"/Users/nvie/Projects/pip-tools/piptools/resolver.py\", line 80, in resolve_hashes\n    return {ireq: self.repository.get_hashes(ireq) for ireq in ireqs}\n  File \"/Users/nvie/Projects/pip-tools/piptools/resolver.py\", line 80, in <dictcomp>\n    return {ireq: self.repository.get_hashes(ireq) for ireq in ireqs}\n  File \"/Users/nvie/Projects/pip-tools/piptools/repositories/local.py\", line 71, in get_hashes\n    return self.repository.get_hashes(ireq)\n  File \"/Users/nvie/Projects/pip-tools/piptools/repositories/pypi.py\", line 159, in get_hashes\n    for candidate in matching_candidates\n  File \"/Users/nvie/Projects/pip-tools/piptools/repositories/pypi.py\", line 159, in <setcomp>\n    for candidate in matching_candidates\n  File \"/Users/nvie/Projects/pip-tools/piptools/repositories/pypi.py\", line 166, in _get_file_hash\n    download_dir=tmpdir, only_download=True, session=self.session\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/pip/download.py\", line 822, in unpack_url\n    hashes=hashes\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/pip/download.py\", line 663, in unpack_http_url\n    unpack_file(from_path, location, content_type, link)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/pip/utils/__init__.py\", line 605, in unpack_file\n    untar_file(filename, location)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/lib/python2.7/site-packages/pip/utils/__init__.py\", line 551, in untar_file\n    path = os.path.join(location, fn)\n  File \"/Users/nvie/Projects/pip-tools/.direnv/python-2.7.12/bin/../lib/python2.7/posixpath.py\", line 73, in join\n    path += '/' + b\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 47: ordinal not in range(128)\nThis is triggered by this line in the pip source code that, while unpacking the tarball, tries to join a byte string with a unicode string.  This works most of the time, until there's a file in the tarball with problematic bytes.  In this case, this fails for these values for me:\nfn = 'tests/staticfiles_tests/apps/test/static/test/\\xe2\\x8a\\x97.txt'\nlocation = u'/var/folders/87/p3dsfch97wjg7j89wtrgm0vh0000gn/T/tmpSfauribuild'\nI'm not going to let this PR be blocked by it.  Once this bug is addressed inside pip, pip-tools will also work with it.  Thanks a lot!\n. The error I reported here earlier is already being tracked in pip here: https://github.com/pypa/pip/issues/1140, and it seems like https://github.com/pypa/pip/pull/1473 has a fix.\n. You should probably just include those packages in your requirements.in. Conda will likely install these for you automatically in the env it creates (but I'm not sure, I don't know it). If you run pip-sync, it checks which packages are in the env and which are defined in requirements.txt, and synchronizes them. pip cannot see that these packages were installed by Conda, so pip-sync can't either.\nWhat's in requirements.txt and what is the output of pip freeze before you run the pip-sync step?\n. Well, pip-compile is a dependency management tool. If you have 2 or more version requirements that are incompatible with each other, the only valid behaviour for pip-compile is to fail. This isn't super user friendly, but at least it's the correct thing to do. After all, it's the entire purpose of pip-tools in the first place to resolve these things and tell you about incompatibilities that will otherwise cause runtime problems.\nI've thought about building a flag into pip-compile for this, but it leads to a lot of follow-up questions, like how do you communicate the error, etc? I thought about adding a flag to \"simply ignore these\", or \"pick an arbitrary version and pin that\", but then how do you treat secondary dependencies? It opens cans of worms everywhere.\nThe true fix for this is to reach out to the package maintainers and ask them to fix their dependency declarations. If you need a practical workaround for this, try manually editing the compiled requirements.txt until this problem is resolved.\n. Agreed, this might be a bit troublesome. I have not had a change to review #388 fully yet, but that might be the way to go. You could --exclude the troublesome package, and either manually pin it after compilation succeeded, or leave it to chance by not adding it at all (which will be the equivalent in practice anyway \u2014\u00a0it's undefined which version you'll get in production).\n. Hi @jamescooke, I just saw your post, it looks great!  Once suggestion I could make here is that you can include the shared .in file (so not the .txt file!) from within your .in files.  That way, pip-compile has just a tiny little bit more information to compile the per-env output files.\n. To answer the original question, you can use this for your requirements-dev.in:\n-r requirements.in\ndjango-debug-toolbar\nAnd then use this to compile it:\npip-compile requirements-dev.in\nAnd then it all just works\u2122.\n. Thanks for the extensive explanation, @jamescooke!  You're absolutely correct.  This is why I normally also advise to always recompile both (all?) .in files at once, never one and not the other, so the pinned versions remain in sync.  But yeah, I agree that this takes discipline and if you forget about it, the tooling won't protect you against these diverging pins indeed, your example illustrates that perfectly.  Not sure how we can build support for this workflow into pip-compile natively.\nThanks for shining a little light on this subject.\n. Yeah, you can use pip's syntax like this for that:\ndjango~=1.10.0   # read as \"1.10.x\"\nWhich technically translates to:\ndjango>=1.10.0,<1.11\nIf you would use:\ndjango~=1.10   # read as \"1.x\"\nThen it would translate to:\ndjango>=1.0,<2.0\n. Yep. And then you re-run pip-compile --upgrade requirements.in as often as you like.\n. > So the .txt version would have to be pinned to the minor versions only.\nSorry, that's not the goal of pip-compile and is not supported. Either you'll have to update the .txt, or pip install directly from the .in file in your CI env.\n. This would be great. If anyone wants to contribute this, I'd encourage that.\n. Thanks!\n. This should be fixed by #410 and be supported from pip-tools 1.8 onwards, which will be released shortly.\n. This is now fixed via #410 and will be part of pip-tools 1.8.  Thanks for the fantastic report, @coriolinus!\n. I love this idea, thanks for your contribution, @nnja!\nThe way pip-compile's default invocation vs the upgrade logic works is quite clever.  Basically, what happens is that if an output file exists (and the --upgrade flag is not given), it will add the current pins from the output file into the list of input specs, and then \"just\" trigger the resolver.\nIn that light, using this new -P <pkg> flag really means \"don't perform the above-mentioned process for this package <pkg>\".  Or, when a pin is given: \"ditto, but also pin it to <pin> while you're at it\".\nSo really all you need to do here is to filter the list of \"existing pins\" and therefore discarding those explicit pins in favor of the stuff provided on the command line.  I've gone ahead and implemented that in d0dd26a79fc66758af134e9687ed2a9a8cbb1d29, which as you can see does not require any special handling for this feature at all.  (It's perhaps best to look at the net-diff against master for the best view of the total change required for this.)\nThanks a lot!\n. Thanks for this great patch, @acdimalev!\n. I'd love to leave that to the README of IPython instead of pip-tools.  pip is the exception here, since pip-tools itself needs it. Thanks!\n. Not sure if this makes this more readable immediately.  Consider this snippet of output I got with this feature:\n```\nraven==5.31.0 \\\n    --hash=sha256:9f06a22083b3ff25a8203b1bf5907df4eb8f74ad8575d61a4a023bde456c5cff \\\n    --hash=sha256:d5fb3f54af217209110f1d931798f46627a5d9e84768bddaa224f27b82ab3d7d\nvia rq, segment-queue\nredis==2.10.5 \\\n    --hash=sha256:5dfbae6acfc54edf0a7a415b99e0b21c0a3c27a7f787b292eea727b1facc5533 \\\n    --hash=sha256:97156b37d7cda4e7d8658be1148c983984e1a975090ba458cc7e244025191dbd\n```\nIn this case, it's not super clear that the \"via\" line belongs to the line above it or below it.  What if we take away that ambiguity by doing this instead:\nraven==5.31.0 \\\n    --hash=sha256:9f06a22083b3ff25a8203b1bf5907df4eb8f74ad8575d61a4a023bde456c5cff \\\n    --hash=sha256:d5fb3f54af217209110f1d931798f46627a5d9e84768bddaa224f27b82ab3d7d\nredis==2.10.5 \\\n    --hash=sha256:5dfbae6acfc54edf0a7a415b99e0b21c0a3c27a7f787b292eea727b1facc5533 \\\n    --hash=sha256:97156b37d7cda4e7d8658be1148c983984e1a975090ba458cc7e244025191dbd \\\n    # via rq, segment-queue\nThat way, it's always clear which items it belongs to because it follows the indentation.\n. Thanks!. I'd like to add that pip-tools is one of pipenv's dependencies, even though they've vendored it in and patched it \u2014 see https://github.com/pypa/pipenv/tree/master/pipenv/patched/piptools. The resolver engine inside pipenv is powered by pip-tools. I guess therefore that improvements to pip-tools could still end up trickling into pipenv.. Thanks for pinging me about this @rsyring \u2014 I've just updated the recommendation in that post! Cheers!. > @nvie any update on this?\nSorry, I'm not actively involved in this project anymore. I had too many projects going on, and had to let some go. That's why I transferred this project to the jazzband org, so others can keep maintaining it.. The Travis build failed due to an unknown variable it here.\n. I would use .info() here. You'd want this normal output to go to stdout (this is not a bug, or a warning, or an error). You are running to tool to specifically get this output. The \"helpful\" message that \"No update information is found\" is validly an warning, and should go to stderr.\n. You need to call item.lower() instead of returning the method itself. I'm not currently behind a computer to patch it myself.\n. Intuitively I would expect the output to be:\n```\n$ cat develop.txt\n-r base.txt\npytz== (glob)\nsix== (glob)\ntimes==* (glob)\n$ cat base.txt\npython-dateutil==* (glob)\n```\nHave you considered that approach? I'm not saying one is better than the other, but I'm trying to compare the two and see what would make most sense.\n. OK, that makes sense. I'll pull after you've updated the typo. Thanks, lovely change.\n. Is this a perf optimization? I really like to avoid adding perf logic in the middle of the processing, as it complicates the logic and makes it hard to follow along what's happening. Let's solve it first, and optimize for performance later. If this is not strictly necessary for the functionality, please remove it from this PR.\n. I do not understand this.\n. Here, too. Please avoid caching things in the middle of the core logic. This will quickly explode the complexity. Untangling this stuff was my main driver behind the rewrite in the first place.\n. This also removes the not constraint.editable condition \u2014 was this intentional? After trying it out, whether or not -e is added to VCS URLs seems to make no difference, but I don't know if it should.\n. Is there perhaps a convenient property on a Link object in the pip project that would expose this is-this-a-vcs-url check (constraint.link.scheme not in vcs.all_schemes) publicly without having to rely on implementation details like fetching that vcs object? Ideally I don't want to know about this at all.\n. I don't understand the need for this logic change.\n. Do other type of requirements even exist?\n. I really want to avoid reimplementing things that should be in pip. If this is currently not in pip, my judgment is we simply cannot use it. Please remove this hack and wait for this to be pulled into pip. I'm happy to bump the requirement to the then-latest pip version \u2014\u00a0no problem.\n. The proper way of adding this would be to contribute this convenience method on Link to pip itself, have that PR pulled, wait for the pip release, and then bump the requirement to the latest pip. (See also the comment below about not relying too much on pip's implementation details.)\n. Are you saying the tests directory is not excluded correctly?\n. Ohhh, I think I know what's wrong: the argument to exclude= should be a list. By passing in a string, it'll probably exclude the dirs 't', 'e', 's', 't', 's' ;)\n. Addressed in pip-tools==1.0.3.\n. I see you changed the data type under the version key to a dict here, which makes sense. To not further complicate the logic in the FakeRepository helper, let's just use this convention everywhere in this JSON file, so make all the other lists dicts (with only a '' key), too.\n. We should probably wrap this yield in a try: ... finally: block.\n. py.test offers a super nice way of creating temp dirs: you can simply have it inject the fixture tmpdir to this function, and you can use it by saying tmp_dir_path = str(tmpdir). It'll be cleaned up automatically by pytest.\n. See https://pytest.org/latest/tmpdir.html\n. This looks great!\n. I agree UTC is unambiguous, but I'm not sure how useful it is. I think I'd prefer the local date for the user that last generated it. This is for human consumption after all.\n. I like the idea for this flag. Rather than putting in an explicit comment here and repeat some of the code, I'd be fine simply setting the unsafe_packages set to the empty set (on line 62) if this flag is active. This will not make setuptools stand out from the rest (but that's the intention if this flag is used anyway).\n. I do think using private pip APIs can be a bit problematic, but I can see the usecase here though. Still, relying on these private APIs means at any time in the future pip-tools can break when pip decides to change its internals. We'll see what happens, I guess :)\nThanks for the PR!\n. I'm having a hard time understanding this condition. Why is the is_pinned_requirement() in there?\n. Seems like this x.link and not x.link.is_artifact is a recurring pattern. Can you make a helper function for that? Example: is_vcs_link(ireq)\n. Then you can also make another helper method is_editable_vcs_link(ireq), which also does the .editable check. I think having those functions will make the rest of the changes below more readable.\n. ",
    "devxpy": "Can it be installed globally?\nBecause it seems to want to remove my global packages when I run pip-sync inside my virtualenv!. Is there a possibility of having seup.py support for this?\nfor example, one might do -\n...\n    extras_require={\n            'dev': ['ipython', 'pip-tools', 'twine']\n    },\n    ...\nWhich would generate the following dev-requirements.txt -\nipython           7.2.0      \n<my pkg>          0.0.1      <path>\npip-tools         3.1.0      \ntwine             1.12.1     \n...\n(-e . is ofc implied, since it's setup.py we're reading from)\nIt could be sensible to even have this such that pip-compile will generate separate *-requirements.txt file for every extras_require field. Can we please have some sort of progress bar? Quite frustrating to just see it sit there... ",
    "lsmagalhaes": "Hello,\nI use pip-tools everyday on my work, and I want this feature. Do you mind if I implement it?\n. ",
    "r1chardj0n3s": "Fixed :-)\n. ",
    "almet": "I can see different interesting points here:\nFirst, you're talking about what we do want to pin and what we don't want to. I think it really depends what's the use of this file will be. For instance, when deploying, I want to have all my dependencies pinned to a version that I know is working. This is what pip freeze > requirements.txt does.\nThe thing you're proposing with top-level dependencies sounds weird to me, because we already have this: that's what in the setup.py of your project, isn't it?\nThe second thing is dependency management and conflict detection. If the version specifiers are used in the setup.py, then it's perfectly possible to know if there is a conflict somewhere or not. Packaging / Distutils2 provide a tool to solve a dependency tree, see http://docs.python.org/dev/library/packaging.depgraph.html\n. typo: requirement?\n. ",
    "brutasse": "@ametaireau the context here is not necessarily a python library: this could be used on, say, a Django site which doesn't have a setup.py.\nI fully agree that all deps should be pinnded when deploying, what I'm suggesting is a tool that translates top-level deps (=stuff you actually need, pinned as well) to the full list of requirements, the result of pip freeze. So you maintain only the top-level deps and the tool resolves the rest for you. Both the top-level requirements and the full list are maintained under version control.\nThe idea is that when you use or update raven, you don't want to worry about which version of simplejson you can use.\n. I have started a branch at https://github.com/brutasse/pip-tools/compare/master...features;bundle\nFor now it just resolves top-level requirements into a full, pinned list. It doesn't install, it just generates a usable requirements.txt.\nThere is no check for dependency conflicts yet.\nThe name (pip-bundle) probably isn't a good idea given that pip already has a bundle command. The command also needs to be split into subcommands for the equivalent of bundle install, bundle outdated and bundle update (not sure we want this one, it's more or less the equivalent of bundle outdated, updating the top-level requirement and re-running bundle install. I'm not a big fan of a command that would alter the top requirements file).\n. @nvie awesome. At first I got confused by the cram tests since they failed on my machine: python3 is my default python. They still fail at the moment, pyflakes shows some issues with undefined names in some places.\nI updated the gist. I like the pip-compile name much more than pip-bundle.\nI like the use of a .in extension for top-level requirements, too :)\nI don't think the tool should touch .in files but rather show what updates are available and then the user choses to update his .in requirements and run pip-compile again to generate the .txt files. pip-compile would sort of replace pip-dump, I guess.\nAlso I think it should be encouraged to pin stuff in requirements.in. Maybe even not support not-pinned requirements? Not sure about this one, in your first example it's probably not important to pin the nose dep in dev-requirements.in but since requirements.in controls what goes into your production environment it should only contain pinned packages.\n. Sure, we're on the same page!\nOne thing that strikes me as odd is the way commands are named, pip-<something>, I think a main command with subcommands feels more natural. It's what many command-line tools do, see pip, git, apt-get\u2026 Was there a particular reason to choose this naming scheme?\nThe advantage of this approach is that adding new subcommands is easier. compile --outdated doesn't do the same thing as compile so they could be split into 2 subcommands.\nExample with a generic name (tool):\nGenerate .txt from .in:\ntool compile [-r spec.in]\nShow available packages:\ntool outdated [-r spec.in]\nApply updates:\ntool update|upgrade [-r spec.in] [package[==version specifier]]\nSync the environment:\ntool sync [-r spec.txt] [-r other-spec.txt]\nBut then we need to replace tool with an appropriate name for \"something that manages requirements and syncs environments\". I like ruby's choice of \"bundle\", there also are a bunch of synonyms. Maybe this is premature bikeshedding though :)\n+1 for uninstalling although it needs to be done carefully: tool sync installs everything from *-requirements.txt but tool sync -r dev-requirements.txt would uninstall stuff from requirements.txt, right? It may be a good idea to promote using tool sync -r requirements.txt [-r prod-requirements.txt] for production and tool sync for development, which grabs all the *-requirements.txt.\n. Ok, thanks for the explanation about the commands \"just being scripts\". One issue with that is you can't have common utilities shared between commands (_check_output for instance). Are you opposed to pip-tools adding something to the python path for sharing between scripts?\nI'm fine with not having the -r options.\nThe gist is up-to-date, I kept pip-outdated for now but this could be called pip-review instead.\nI'll update my branch to add the cram tests, support .in files, add the pip-sync script. And maybe update pip-review to implement the pip-outdated behavior.\n. On Tue, Oct 2, 2012 at 9:01 PM, Vincent Driessen\nnotifications@github.com wrote:\n\nOK, I've updated the gist once more (to be a bit more descriptive about\nwhat's going on and what's important in each step).\n\nVery nice :)\n\n@brutasse, could you take a look at the pip-compile-specifics.txt file\nI've added? What do you think? Am I overcomplicating things here, or is this\nuseful? I was thinking only pip-outdated should reach out to PyPI, or\npip-compile at least should only do that if the currently pinned secondary\nversions don't match criteria (so it could reach out to PyPI to find\nversions that do match).\n\nI'd say people shouldn't touch requirements.txt manually while using\npip-tools. For that use case I'd put the raven and simplejson\nrequirements in the .in file, and the compiled file would\nincidentally have the same content. It'd be much simpler than trying\nto make pip-compile aware of stuff manually changed in compiled files\u2026\n\nOr do you think pip-compile should always reach out to PyPI, find the\nlatest versions that still match all criteria, and record those? It would\ndefinitely simplify things for us (as we don't have to consider\nrequirements.txt in pip-compile at all), but it would lead to (secondary)\npackage updates on every compile, and compiles would always be lengthy.\n\nBy default I think compile should rebuild the whole thing. But there\nare ways to optimize, using a local cache for instance. for\nraven==1.9.3, pip-compile can look at the cached version of raven so\nit'd be fetched only the first time. For secondary requirements it's\ntrickier to be lazy with PyPI. And of course for packages which are\nreferenced but not hosted on PyPI (e.g. redis) there is no way to\nguess what the path to pip's cached version is without asking PyPI.\nOn the other hand pip-review / pip-compile aren't the kind of tasks\nyou do all the time, and with my current implementation resolving\nsentry==5.0.13 to a full list of its 25 requirements takes 90\nseconds. If sentry had pinned requirements itself, it'd be much faster\n:)\n. Indeed, packages need to be downloaded. Mostly, I guess, because of the dynamic nature of setup.py and the fact people change install_requires depending on the python version or OS. The implementation I added yesterday doesn't create a temporary environment, it just runs setup.py egg_info to get the requirements:\nhttps://github.com/nvie/pip-tools/blob/future/bin/pip-compile#L100-119\nAn issue with \"just installing\" the source spec in a temporary environment is that pip isn't aware of dependency conflicts. If I put in a requirements file:\nsentry==5.0.14\nDjango==1.3\nThere is clearly a conflict here because sentry requires Django>=1.4.1,<=1.5. But pip install runs fine and Django 1.3 is installed, probably because the last requirement wins. This is especially annoying when you have a pinned version of Django on the top of your requirements file and something else in the tree requires 'Django': the pinned version won't be taken into account and you'll just get the latest version the first time you run pip install\u2026\n. @nvie awesome :) I'll look at integrating that with the package parsing code later tonight.\n. @nvie I have something ready for pip-compile here: https://gist.github.com/cfffc3537604cc311767. I haven't committed it because I'm not sure it's the best approach. What I currently do is:\n- If a top-level spec is pinned, resolve it\n- If it is not, find the latest version and resolve spec==latest_version\n2nd-level specs are also resolved to the latest versions available. Supposing we have package X that requires \"simplejson<2.6\" and package Y that requires \"simplejson\", the algorithm will add \"simplejson==2.4.0\" and *simplejson==2.6.2\" to the SpecSet when resolving X and Y respectively, leading to a conflict.\nThis is sub-optimal but that particular problem shouldn't happen often and I can't think of an easy way to solve this\u2026 Let me know what you think.\nAnd finally once the specset is resolved for all spec files, we need to dump things to the appropriate compiled files. This probably requires fixing SpecSet not to lose the source information (I saw your note) and adapt it to make it possible to get dependency information from the data structure directly.\n. @nvie nice! It's definitely a better approach than adding 2nd-level deps already pinned.\nI couldn't spare some time tonight, and probably won't tomorrow either. Feel free to move stuff around, it shouldn't break anything on my side before sunday :)\n. @nvie I just pushed a PackageManager that works with PyPI. I left a couple of TODOs in the code, there's a decision to make about how aggressive we want to be with package caching. Let me know what you think :)\n. Hey @nvie, how's parenting? :)\nI just completed the implementation of pip-compile and pushed a bunch of changes in the code and tests:\nhttps://github.com/nvie/pip-tools/compare/a9f3b910e6...bb38a98148\nI believe pip-compile works pretty great now. What's missing:\n- pip-review\n- pip-sync\n- python3\nDo you have time to review my changes as I push them? I plan to keep working on the missing features in the next couple of days to finally reach a releasable state.\n. Um, not sure what's going on with the cram tests. They were running fine last time I checked. Pip-compile seems slower than usual, maybe there are issues with pypi\u2026\n. Hey @nvie, funny that you mention the project as I was playing with it again yesterday night. I just pushed a small fix, I think this is looking pretty good already. It's fast again (was probably a PyPI issue), here's what happened when I resolved sentry in a requirements.in:\namqp==1.0.11\nanyjson==0.3.3\nBeautifulSoup==3.2.1\nbilliard==2.7.3.27\ncelery==3.0.18\ncssutils==0.9.10\nDjango==1.4.5\ndjango==1.5.1\ndjango-celery==3.0.17\ndjango-crispy-forms==1.2.3\ndjango-indexer==0.3.0\ndjango-paging==0.2.4\ndjango-picklefield==0.3.0\ndjango-social-auth==0.7.22\ndjango-social-auth-trello==1.0.3\ndjango-static-compiler==0.3.1\ndjango-templatetag-sugar==0.1\ngunicorn==0.17.2\nhttpagentparser==1.2.2\nhttplib2==0.8\nkombu==2.5.10\nlogan==0.5.5\nnydus==0.10.5\noauth2==1.5.211\nPygments==1.6\npynliner==0.4.0\npython-dateutil==1.5\npython-openid==2.2.5\npytz==2013b\nraven==3.3.3\nredis==2.7.2\nsentry==5.4.5\nsetproctitle==1.1.7\nsimplejson==3.1.3\nsix==1.3.0\nSouth==0.7.6\nEverything is correct, except the duplicate Django / django requirement. Did we decide something about PyPI's case insensitivity? Often private indices are case-sensitive so IMO packages themselves should take care of using the correct letter case.\nI think what's left is compiling multiple .in files into multiple .txt files.\n. Oh actually the handling of multiple .in files is done :)\n. I think in case of conflict we really need to pick the correct one: it's not completely arbitrary, there is a canonical casing. PyPI and its mirrors are case-insensitive but people who run private mirrors don't necessarily do it with the same software.\nWith PyPI it's transparent: https://pypi.python.org/simple/webob/ actually redirects to https://pypi.python.org/simple/WebOb/. With other software it might not be the case. I ran into issues in the past when package could simply not be installed from a custom index where simple/WebOb/ returns a 200 but simple/webob/ a 404\u2026\nI submitted a pull request to django-social-auth to fix the sentry issue.\n. Probably! Since we download the packages we should be able to infer it from the filename directly and even fix incorrect names.\n. The next version of pip-tools will use pip's internal APIs for querying PyPI, and pip already support python3. As for subprocess, the API is pretty much the same between py2 and py3. Issues will mostly be related to unicode / bytestrings so I'm not sure it'd be worth adding the sh dependency. I guess we'll see when the actual work happens :)\n. packaging / distutils2 don't support entry_points, only scripts. I'd be interested to have @nvie 's opinion on this but entry_points aren't the most future-proof solution.\nWhat's the issue in non-unix systems? PATH management?\n. In this case pip-tools could add support for pip options in requirements file. With this requirements file:\n--find-links http://download.gna.org/pychart/\nPyChart==1.39\nPyChart installs just fine, so pip-dump (or rather its successor) could save the pip options instead of dropping them.\n. PyPI is correct in this case, the last pelican version is 3.0.1 and @yegle has a more recent version since it's a development build.\nSo the question is, should pip-review only test for greater versions available on PyPI? I'd be fine with it\u2026\n. +1, ideally the whole pip syntax (--find-links, --index-url etc) should be supported. It's just a matter of parsing them and forwarding to pip's PackageFinder.\nCareful with pip-sync as a bash script as it probably won't work on Windows :)\n. I couldn't reproduce this with any python 2 or 3 and setutools/distribute combination. Did you install pip-tools globally? I'm guessing you're on Mac OS, this may be a path issue with a package manager or something.\nCan you try to locate the pip-review executable and see if its path is in your $PATH?\n. With the currently released version of pip-tools, I'd recommend using pip list --outdated instead of pip-review and forgetting about old pip versions. IMO pip-tools shouldn't bother supporting too old versions of pip as it's very easy to upgrade pip inside of a virtualenv.\nIn the future branch, pip-review could directly use pip's internals.\n. @nvie the changelog mostly mentions bugfixes and the egg_info command definitely works for me with the latest setuptools (1.1.1). There are no mention of failed egg_info runs in pip-compile output (from the future branch).\n. The issue is that the future branch isn't completely ready. pip-compile works fine (although apparently not with most recent pip versions) but pip-sync is missing.\nAn option would be to ship the future branch with only the pip-compile script as it's already very useful. pip-review from master can already be replaced with pip list --outdated (although there's no interactive mode). And we can always work on pip-sync for a future version.\nAny toughts on this @nvie?\n. I don't get it. How is pip even able to find 1.7a2? I don't see it on https://pypi.python.org/simple/Django/\n. @nvie do you know what's going on?\nhttps://pypi.python.org/simple/pip-tools/ works fine but https://pypi.python.org/pypi/pip-tools returns a 404\n. Thanks @nvie :)\n@freder does pip-review work properly for you now?\n. Thank you! Closing :)\n. ",
    "bionikspoon": "@nvie \nOn the topic of 2.6, is there any particular reason pip-tools is not supporting it, or at the very least testing against it?\nI'm wondering if there's anything fundamentally breaking that 2.6 can't reasonably do?  If not, I'd love to make this work.  I like this tool, but trying to use it in my build pipeline is putting me in an awkward position when I have to deal with py26.\nCurrently the error points to a dict comprehension, which is unsupported in Py26. \nI found a few\n- https://github.com/nvie/pip-tools/blame/master/piptools/sync.py#L60\n- https://github.com/nvie/pip-tools/blame/master/piptools/sync.py#L94\n. ",
    "yegle": "Will you consider add some more dependency so py3k support would be easier?\ne.g. use requests1 for HTTP requests, and use sh2 for capturing subprocess output?\n. ",
    "shakaran": "Any ETA for support python3? With upcoming Django 1.5 supporting Python 3 is important moving quickly to support more python 3 packages in servers.\n. +1 @nvie Thanks for solve this issue. I will test with some production servers and report again if I see issues\n. I am having the same problem with development packages. For Trac is reporting a more updated version 1.0, but it is wrong because I am using development version 1.0.1dev-r11409. Same for bzr (2.5.1) vs 2.6b2 or others.\n\n# pip-review --interactive\nWarning: cannot find svn location for flup==1.0.3.dev-20110405\nWarning: cannot find svn location for django-inlinetrans==0.4.12bdev-r70\nWarning: cannot find svn location for Trac==1.0.1dev-r11409\nPIL==1.1.6 is available (you have 1.1.7)\nUpgrade now? [Y]es, [N]o, [A]ll, [Q]uit y\nDownloading/unpacking PIL==1.1.6\n  Could not find a version that satisfies the requirement PIL==1.1.6 (from versions: )\nNo distributions matching the version for PIL==1.1.6\nStoring complete log in /root/.pip/pip.log\nTrac==1.0 is available (you have 1.0.1dev-r11409)\nUpgrade now? [Y]es, [N]o, [A]ll, [Q]uit n\nbzr==2.5.1 is available (you have 2.6b2)\nUpgrade now? [Y]es, [N]o, [A]ll, [Q]uit n\nNo update information found for django-compressor\nNo update information found for django-evolution\nNo update information found for filebrowser-safe\nNo update information found for grappelli-safe\nNo update information found for iotop\nmimeparse==0.1.4 is available (you have 0.1.3)\nUpgrade now? [Y]es, [N]o, [A]ll, [Q]uit y\nDownloading/unpacking mimeparse==0.1.4\n  Could not find a version that satisfies the requirement mimeparse==0.1.4 (from versions: )\nNo distributions matching the version for mimeparse==0.1.4\nStoring complete log in /root/.pip/pip.log\nNo update information found for template-utils\n\nThis issue makes that I cannot trust deeply in pip-review and I always have to run with --interactive\n. Thanks! Upgrading to setuptools 1.1.1 apparently fix this issue\n. @vphilippon I would like that you reopen, after update again all the packages, I get the following now:\n```\npip-sync\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-sync\", line 11, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 53, in cli\n    requirements = sync.merge(requirements, ignore_conflicts=force)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/sync.py\", line 73, in merge\n    key = ireq.link or ireq.req.key\nAttributeError: 'Requirement' object has no attribute 'key'\n```. Not sure which is the proper way to update, since it seems revert back to 1.4.2:\n[~]# pip install --upgrade pip\nRequirement already up-to-date: pip in /usr/local/lib/python2.7/site-packages\n[~]# pip install pip-tools\nRequirement already satisfied: pip-tools in /usr/local/lib/python2.7/site-packages\nRequirement already satisfied: first in /usr/local/lib/python2.7/site-packages (from pip-tools)\nRequirement already satisfied: click>=6 in /usr/local/lib/python2.7/site-packages (from pip-tools)\nRequirement already satisfied: six in /usr/local/lib/python2.7/site-packages (from pip-tools)\n[~]# pip install -U  pip-tools\nCollecting pip-tools\n  Downloading pip_tools-1.10.1-py2.py3-none-any.whl\nCollecting setuptools (from pip-tools)\n  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 868kB/s \nRequirement already up-to-date: first in /usr/local/lib/python2.7/site-packages (from pip-tools)\nCollecting click>=6 (from pip-tools)\n  Downloading click-6.7-py2.py3-none-any.whl (71kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 5.0MB/s \nCollecting six (from pip-tools)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nInstalling collected packages: setuptools, click, six, pip-tools\n  Found existing installation: setuptools 28.8.0\n    Uninstalling setuptools-28.8.0:\n      Successfully uninstalled setuptools-28.8.0\n  Found existing installation: click 6.2\n    Uninstalling click-6.2:\n      Successfully uninstalled click-6.2\n  Found existing installation: six 1.10.0\n    Uninstalling six-1.10.0:\n      Successfully uninstalled six-1.10.0\n  Found existing installation: pip-tools 1.4.2\n    Uninstalling pip-tools-1.4.2:\n      Successfully uninstalled pip-tools-1.4.2\nSuccessfully installed click-6.7 pip-tools-1.10.1 setuptools-36.5.0 six-1.11.0\n[~]# pip-sync \nUninstalling cycler-0.10.0:\n  Successfully uninstalled cycler-0.10.0\nUninstalling pycrypto-2.6.1:\n  Successfully uninstalled pycrypto-2.6.1\nNot uninstalling wsgiref at /usr/local/lib/python2.7, as it is in the standard library.\nCollecting click==6.2\n  Using cached click-6.2-py2.py3-none-any.whl\nCollecting pip-tools==1.4.2\n  Using cached pip_tools-1.4.2-py2.py3-none-any.whl\nCollecting six==1.10.0\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: first in /usr/local/lib/python2.7/site-packages (from pip-tools==1.4.2)\nInstalling collected packages: click, six, pip-tools\n  Found existing installation: click 6.7\n    Uninstalling click-6.7:\n      Successfully uninstalled click-6.7\n  Found existing installation: six 1.11.0\n    Uninstalling six-1.11.0:\n      Successfully uninstalled six-1.11.0\n  Found existing installation: pip-tools 1.10.1\n    Uninstalling pip-tools-1.10.1:\n      Successfully uninstalled pip-tools-1.10.1\nSuccessfully installed click-6.2 pip-tools-1.4.2 six-1.10.0\n[~]# pip-sync\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-sync\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 53, in cli\n    requirements = sync.merge(requirements, ignore_conflicts=force)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/sync.py\", line 73, in merge\n    key = ireq.link or ireq.req.key\nAttributeError: 'Requirement' object has no attribute 'key'\n. I only have a requirements.txt file. After remove pip-tools 1.4.2 from the requirements file and update pip-tools to latest, when I run pip-sync, it seems to work perfectly now without traces.. ",
    "Fandekasp": "Would be nice to have python 3 support yes, pip-tools became essential in all my python2 projects, and I really miss it now that I work on a python3 project :)\n. well installing with the bzr url was what I tried first, actually. But somehow, pip-dump does the same thing, replacing -e .. by Pychart==1.39\nHere is the exact command:\npip install -e bzr+http://download.gna.org/pychart/bzr-archive#egg=pychart\nIt's true that for other projects, mercurial or git urls work fine. wonder if the problem comes from bazaar\n. Here is what happen for me: http://sprunge.us/ILfK?sh\nI realize now that the problem doesn't come from pip-dump, so I'm closing that ticket.\nThanks for your help !\n. hey you're right xD I was stupid :) It seems like we have to go and bzr upgrade everytime with pip install some bazaar packages, but afterward pip-dump works fine. sweet !\n. Looking forward to testng out the next version :)\n. ",
    "nigma": "The main issue, or difference, is that Windows lacks support for shebang lines and uses file extensions to associate .py files with Python interpreter.\nThis means that in order to execute a file defined in the scripts it must have an extension (.py) that is associated with the proper Python interpreter.\nAs long as the PATH management is not a problem anymore (virtualenv does the job), the file association is still problematic when there are multiple interpreters installed on the system (on a typical Windows box I have numerous Python versions in both 32 and 64bit arch and a handful of virtualenvs)\nFortunately there's a light at the end of the tunnel. The file-to-interpreter association has been addressed very recently in Python 3.3 by bundling a Python Launcher for Windows.\nThe Launcher acts as a middleman and simply uses the shebang line to pick a Python interpreter that will be used for executing a particular .py script (just like on Linux). This works fine, but requires a couple things:\n- Python 3.3 has to be installed and the .py[ocw] extensions associated with the launcher (I had to do it manually).\n- When executing a script from the virtualenv, the script's shebang line must be absolute path and point to the virtualenv's interpreter. This seems to be solved by easy_install/pip already as all the files installed with scrips get a correct shebang prepended.\n- A full script name (filename including extension) must be typed in the command line (can be altered by changing env properties) as opposed to invoking .exe scripts by just base name.\nOn the other hand the solution that SetupTools provides with entry_points is that it simply creates .exe executable wrappers that bind directly to the Python interpreter used during install.\nI'm really glad that this stuff is being addressed in Python 3.3, but at the moment I'm not less puzzled by the current state of packaging.\nAfter this investigation the most convenient way (well, at least for the end user) of distributing scripts still seems the entry_points approach, but now I think I'm going to lean toward bundling additional .py script files and advertising use of Python 3.3 or pylauncher for the sake of making things easier in the future.\n. ",
    "tarekziade": "well since you already use setuptools in your project I guess it's ok to use console entry points, which are indeed a tad better for win32 support right now\nIt won't be hard to jump into another thing later.\n. ",
    "antonagestam": "This is biting me right now, I have  -e git+https://github.com/antonagestam/django-quicky.git@patch-1 in requirements.in. The branch has updated but when I run pip-compile that is not reflected in requirements.txt. Is there a workaround to clear the cache?\n. @svetlyak40wt Thanks for the workaround, localshop looks cool.\n@nvie Would a PR that adds a --clear-cache flag and clears the entire cache be accepted?\n. @svetlyak40wt So I think the problem might have occurred because I installed pip-tools with a VCS URL, how meta ;)\n. @nvie @svetlyak40wt Not sure if this should close the --no-cache issue though?\n. Cool beans!\n. I was thinking kind of like hub are doing it: https://github.com/github/hub#aliasing\n. Not sure, but I think that solution wouldn't require any implementations in pip. I think, in this case, git is aliased to hub and then hub just calls git for all commands that are \"git native\". I'll have to look into the implementation but I think it should be possible. \npip compile would be nice, right? ;)\n. Cool, thanks for the response! :+1:\n. Should there be a from __future__ import unicode_literals. Is 3.4 supported btw?\n. @nvie Very cool, thanks!\n. Quick workaround for people looking for a solution is to install pip 1.5.6:\n$ pip install pip==1.5.6\n. ",
    "svetlyak40wt": "I struggling with this issue too. Right now just deleting ~/.pip-tools/cache manually.\nFor editable requirements it is better to update cached version of the repository than to ignore cache et all.\nBy the way, I'm thinking that it is not a big problem and consider to remove editable dependencies from requirements.in completely. It is much more reliable to build a wheel packages and to put them to a localshop and then use in requirements.txt as usual.\n. Wooo,  I found that this issue already fixed and merged in pull-request #84 \n. #84 solves root of the problem. You shouldn't want to wipe cache now :)\n. Hi @nvie, why do you want to copy NormalizedVersion into the pip_tools package instead of adding dependency from verlib package?\n. Hi guys! I have a patch!\n. I support this issue. I went even father and keep all my requirements in a separate directory like this:\nrequirements/base.txt\nrequirements/development.txt\nrequirements/production.txt\n. This is because previous version of pip review tested versions for equality, but now it tries to compare them. The problem is that pytz's author does not uses semantic versioning. I hope, in near future all packages at pypi will be forced to use proper versioning.\n. Do you know a python library other than verlib, which is able to compare not only semantic versions but also a weird \"numbers\" like in pytz's.\n. No it tries to do it's best, normalizing version number, but it is not always possible.\nAs I mentioned before, previous pip-review version did not compared version numbers at all and this was wrong behavior. I fixed it adding verlib ask dependency. Probably, for version numbers which can't be normalized, we should try lexicographical string comparison.\n. @nvie i fixed the patch.\n. By the way, I've tested this feature for a while, and seems it works as expected.\n. Well, that is because you have to add these repositories, as \"editable\",\nlike that:\n-e git+\nhttps://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-master\nOn Thu, Oct 3, 2013 at 9:03 PM, jerzyk notifications@github.com wrote:\n\nI've installed your version, got error:\nRequirement file contains git+https://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-master, but that package is not installed\nRequirement file contains git+https://github.com/jerzyk/django-user-accounts.git@11d8970c3e10a4624b3d1f274b4eea1b8dd0c982#egg=django_user_accounts-master, but that package is not installed\nRequirement file contains git+https://github.com/masci/django-notification.git@py3_django15_support#egg=notification, but that package is not installed\nException:\nTraceback (most recent call last):\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/basecommand.py\", line 134, in main\n    status = self.run(options, args)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/commands/freeze.py\", line 89, in run\n    line_req = InstallRequirement.from_editable(line, default_vcs=options.default_vcs)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/req.py\", line 87, in from_editable\n    res = cls(name, comes_from, source_dir=source_dir, editable=True, url=url, prereleases=True)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/req.py\", line 44, in init\n    req = pkg_resources.Requirement.parse(req)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2700, in parse\n    reqs = list(parse_requirements(s))\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2638, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2606, in scan_list\n    raise ValueError(\"Expected \"+item_name+\" in\",line,\"at\",line[p:])\nValueError: ('Expected version spec in', 'pip_tools-origin/work-with-vcs', 'at', '/work-with-vcs')\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/pull/56#issuecomment-25638631\n.\n\n\nAlexander Artemenko (a.k.a. Svetlyak 40wt)\nBlog: http://dev.svetlyak.ru\nPhotos: http://svetlyak.ru\nJabber: svetlyak.40wt@gmail.com\n. Then, please paste a complete requirements.in file for further\ninvestigation.\nOn Thu, Oct 3, 2013 at 9:35 PM, jerzyk notifications@github.com wrote:\n\nthey are specified exactly like this - in the correct form (-e\ngit+https://...@...#egg=xxx)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/pull/56#issuecomment-25641279\n.\n\n\nAlexander Artemenko (a.k.a. Svetlyak 40wt)\nBlog: http://dev.svetlyak.ru\nPhotos: http://svetlyak.ru\nJabber: svetlyak.40wt@gmail.com\n. It is strange. Why do you need all these versions and hashes in this file?\nIdea of pip-compile it that you only need you root dependencies preferably\nwithout versions, then you run pip-compile and it produces final\nrequirements.txt with expanded and pinned dependencies.\nOn Thu, Oct 3, 2013 at 9:55 PM, jerzyk notifications@github.com wrote:\n\nhere it is:\ndjango-appconf==0.6\ndjango-authority==0.7\ndjango-braces==1.2.2\ndjango-constance==0.6\ndjango-extensions==1.2.2\ndjango-filer==0.9.5\ndjango-floppyforms==1.1\ndjango-forms-bootstrap==2.0.3.post2\ndjango-haystack==2.1.0\ndjango-jsonfield==0.9.10\ndjango-model-utils==1.5.0\ndjango-mptt==0.6.0\ndjango-notification==1.1.1\ndjango-page-cms==1.5.2\ndjango-pagination==1.0.7\ndjango-picklefield==0.3.0\ndjango-polymorphic==0.5.3\ndjango-postman==3.0.1\ndjango-suit==0.2.5\nDjango==1.5.4\ndocutils==0.11\neasy-thumbnails==1.4\neventlog==0.6.7\nhtml5lib==0.95\nipython==1.1.0\nmetron==1.2\nPillow==2.1.0\npytz==2013b\nPyYAML==3.10\nraven==3.5.0\nreadline==6.2.4.1\nsix==1.4.1\nSouth==0.8.2\nUnidecode==0.04.14\nWhoosh==2.5.4\n-e git+https://github.com/masci/django-notification.git@py3_django15_support#egg=notification\ndjango-ajax-selects\n-e https://github.com/masci/django-notification.git@py3_django15_support#egg=notificationdjango-ajax-selects-e git+https://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-master\ndjango-mailer==0.1.0\n-e https://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-masterdjango-mailer==0.1.0-e git+https://github.com/jerzyk/django-user-accounts.git@11d8970c3e10a4624b3d1f274b4eea1b8dd0c982#egg=django_user_accounts-master\n-e https://github.com/jerzyk/django-user-accounts.git@11d8970c3e10a4624b3d1f274b4eea1b8dd0c982#egg=django_user_accounts-master-e git+https://github.com/svetlyak40wt/pip-tools.git@work-with-vcs#egg=pip_tools\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/pull/56#issuecomment-25642849\n.\n\n\nAlexander Artemenko (a.k.a. Svetlyak 40wt)\nBlog: http://dev.svetlyak.ru\nPhotos: http://svetlyak.ru\nJabber: svetlyak.40wt@gmail.com\n. @jerzyk, you are talking about pip freeze, but this particular patch is about pip-compile from pip-tools package. They are completely unrelated.\nI've checked you requirement.txt in my environment, doing: pip install -r requrements.txt and pip freeze, and it worked without errors.\n. Anyway, @jerzyk , there is no pip-dump in future or work-with-vcs branches. Unless you provide a complete step by step scenario (starting from virtualenv blah; source blah/bin/activate, I will not be able to help.\n. I've made a rebase relative to the current future branch, and fixed cram tests, because they were incompatible with pip>1.5.\n. Well, after further investigation, I found, that pip (1.4.1), by default, sets allow_all_insecure and allow_all_external on PackageFinder during pip install. Although such simple patch fixes the issue:\ndiff\ndiff --git a/piptools/package_manager.py b/piptools/package_manager.py\nindex 7b3125a..56da428 100644\n--- a/piptools/package_manager.py\n+++ b/piptools/package_manager.py\n@@ -260,6 +260,8 @@ class PackageManager(BasePackageManager):\n                     index_urls=['https://pypi.python.org/simple/'],\n                     use_mirrors=True,\n                     mirrors=[],\n+                    allow_all_external=True,\n+                    allow_all_insecure=True,\n                 )\n                 link = finder.find_requirement(requirement, False)\n                 self._link_cache[specline] = link\nI think it is not best idea, to hardcode these parameters. It is better to add ability to pass any unknown command line option down to pip's package manager.\n. Christian, take a look at future branch of this repository. It provides a command pip-compile, which updates you requirements.txt. Having this file in the VCS, in it's turn, allows to make a diff and discover new packages' versions. \nSent from my iPhone\n\nOn 27 \u043d\u043e\u044f\u0431. 2013 \u0433., at 1:54, Ionel Cristian M\u0103rie\u0219 notifications@github.com wrote:\n\u2014\nReply to this email directly or view it on GitHub.\n. I belive, @coagulant already solved this issue: https://github.com/coagulant/pip-tools/commit/2b1f0bc0763fefc81152d730e7aa7080c2ad4258. But seems, he forgot to make a pull request.\n. What is wrong with installation right from the git?\n. Use separate virtualenvs for python2 and python3\n\nSent from my iPhone\n\nOn 24 \u0438\u044e\u043d\u044f 2015 \u0433., at 18:13, Sam Mason notifications@github.com wrote:\nIs there a nice way to choose between reviewing package states for python 2/3? I currently hack the source to switch between calling out to pip2 and pip3.\nMaybe allowing multiple versions to coexist like pip does? I'd then be able to run pip3-review and pip2-review and have it see the \"correct\" packages.\n\u2014\nReply to this email directly or view it on GitHub.\n. @smason Sam, virtualenv is must have for python devlopment. I'm wincing remembering days when virtualenv didn't exist.\n. Right. I'll fix it.\n. I also, thought about this second approach, but it is more complex to implement and have no additional benefits, because results are compiled and not for humans anyway.\n. Fixed  a typo, squashed, pushed.\n. \n",
    "zakdances": "I'm experiencing this issue too:\ngevent==0.13.8 is available (you have 1.0rc2)\n. @brutasse You're right. I had messed up enviromental variables.\nI added the line source ~/.bashrc to my .bash_profile and now it works.\n. ",
    "mktums": "Unfortunately, I don't know what it is\u2026 But I'll try to do it :)\n. I don't know how to handle behavior when requirements.txt isn't exists the right way. Any suggestions?\n. Since cram has this WONTFIX issue I've added optional argument\u2026 \n. Duplicating pip show functionality.\n. I've forgot to fix your typos here, and can't update PR since I've dropped my fork :(\nSending PR #27\n. ",
    "stefanfoulis": "+1\n. @blueyed normal editables seem to work now.\nI'm curious: is this a hard feature to implement?\n. In pip-tools==1.1.4 the error message is now:\npip-compile does not support URLs as packages, unless they are editable. Perhaps add -e option?\n. @blueyed I tested with #169 \nWhen I put https://example.com/my-package-0.1.tar.gz in requirements.in:\npip-compile requirements.in\npip-compile does not support artifact URLs as packages (but you can use VCS URLs). Constraint: https://example.com/my-package-0.1.tar.gz (from -r requirements.in (line 1)).\nWhen I put -e https://example.com/my-package-0.1.tar.gz in requirements.in:\npip-compile requirements.in\nTraceback (most recent call last):\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/bin/pip-compile\", line 9, in <module>\n    load_entry_point('pip-tools==1.0dev1', 'console_scripts', 'pip-compile')()\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/click/core.py\", line 700, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/click/core.py\", line 680, in main\n    rv = self.invoke(ctx)\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/click/core.py\", line 873, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/click/core.py\", line 508, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/stefanfoulis/tmp/pip-tools/pip-tools-169/piptools/scripts/compile.py\", line 76, in cli\n    for line in parse_requirements(src_file, finder=repository.finder, session=repository.session):\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/pip/req/req_file.py\", line 89, in parse_requirements\n    for req in req_iter:\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/pip/req/req_file.py\", line 147, in process_line\n    wheel_cache=wheel_cache\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/pip/req/req_install.py\", line 118, in from_editable\n    editable_req, default_vcs)\n  File \"/Users/stefanfoulis/.virtualenvs/tmp-pip-tools-169/lib/python2.7/site-packages/pip/req/req_install.py\", line 1113, in parse_editable\n    editable_req\npip.exceptions.InstallationError: https://example.com/my-package-0.1.tar.gz should either be a path to a local project or a VCS url beginning with svn+, git+, hg+, or bzr+\n. Does this PR also handle installing from a source .tar.gz (from an url)?\n. thanks for the answer @Chive :-)\n. ok, I see this can't match.\n. ",
    "beeftornado": "https://github.com/nvie/pip-tools/pull/82 might solve this issue\n. Same error. pip@1.5.4 python@2.7\n. https://github.com/nvie/pip-tools/pull/81 fixes it. You can install that version directly using\nshell\npip install git+https://github.com/svetlyak40wt/pip-tools.git@pip-1.5-and-wheels-support -U\nOr using my personal one with extra-index-url support\nshell\npip install git+https://github.com/beeftornado/pip-tools.git@beeftornado -U\n. ",
    "jikamens": "I don't think there's anything wrong with pytz's versioning. It is unambiguous, and as such verlib or whatever else pip-tools is using to compare version numbers should be able to compare two different pytz versions. The failure here is not in pypi versioning rules, but rather in pip-tools regressing and no longer working properly in a case that used to work properly.\nPlease note that the version numbers used by pytz are tied to the external source for time-zone data on which each pytz module is based, so it would be inappropriate for pytz to be forced to use a different versioning scheme just to conform to the whims of the pypi maintainers.\napt and rpm can handle version numbers like pytz's with no trouble. python software that compares version numbers should be able to handle them as well.\n. I do not know if such a library exists. One might surmise that since verlib is currently at version 0.1, it is a rather early version and perhaps not all necessary functionality has been implemented. Perhaps if the issue is that verlib doesn't know how to handle these version numbers, a bug report could be filed with the authors of verlib.\n. ",
    "dreadatour": "+1 for having problems with pip-tools 0.3.1 and pytz version compare. Agree with Jonathan.\nBy the way, Travis build is broken: https://travis-ci.org/nvie/pip-tools\n. As Jonathan Kamens said:\n\nOne might surmise that since verlib is currently at version 0.1, it is a rather\nearly version and perhaps not all necessary functionality has been implemented.\n\nGood news everyone: verlib is not supported anymore: https://bitbucket.org/tarek/distutilsversion/issues\n\nThis standalone distutils.version repository is not under development anymore,\nbut maybe your request could be adapted for distutils2/packaging.\n. Vincent, you're welcome!\n. Gosh, this is wrong fix! Did you even try it?\n\nHere is correct one: https://github.com/dreadatour/pip-tools/commit/b36b43123719a22b86d37b453b87fbae085dec30\n. ",
    "johnmarkschofield": "Will do so.\nMy plan is to output debug and info to stdout, and warning, error, and critical to stderr.\nYou OK with that?\n. ",
    "simonkern": "I strongly suggest you start using virtualenvs. In addition, virtualenvwrapper might be an interesing choice for you as well.\n. ",
    "schwa": "Why? For the stuff I work on installing into ~ is fine. Virtualenv (inc. wrapper) is a chore and a pain if you don't need to keep your environments isolated. I don't need isolation.\npip install supports --user, pip-review should too. If it doesn't it's not really working well with pip.\n. Alright there's a pull-request coming - see https://github.com/schwa/pip-tools/tree/feature/user_flag\nHowever this requires a version of pip that supports --user in the freeze subcommand. That's coming too (need to fix broken unit tests). See https://github.com/pypa/pip/pull/803\n. Fundamentally changing the tools provided by a project isn't so much a deprecation as a total project re-purpose.\nIt might have been better to leave this repo (and pypi entry) as-is (and marked as \"no longer developed\") and create a new repo for pip-sync.\nIt would behoove someone to fork this repo and rollback the depreciation.\n. ",
    "maestrofjp": "Sure,  this is what I get:\n(verde)peter@Blizzardme:~/PycharmProjects/Verde$ pip freeze -lr /tmp/tmpQFFt3_\nRequirement file contains argparse, but that package is not installed\ncoverage==3.6\nDjango==1.5.1\ndjango-admin-bootstrapped==0.4.1\ndjango-braces==1.0.0\ndjango-cache-panel==0.1\ndjango-classy-tags==0.4\ndjango-cms==2.4.2\ndjango-debug-toolbar==0.9.4\ndjango-discover-runner==0.4\ndjango-filer==0.9.4\ndjango-model-utils==1.4.0\ndjango-mptt==0.5.2\ndjango-polymorphic==0.5\ndjango-reversion==1.7\ndjango-secure==1.0\ndjango-sekizai==0.7\neasy-thumbnails==1.2\nhtml5lib==1.0b1\nmock==1.0.1\nMySQL-python==1.2.4\nPIL==1.1.7\npython-dateutil==2.1\npytz==2013b\nselenium==2.33.0\nsix==1.3.0\nSouth==0.8.1\nException:\nTraceback (most recent call last):\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/basecommand.py\", line 139, in main\n    status = self.run(options, args)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/commands/freeze.py\", line 99, in run\n    line_req = InstallRequirement.from_line(line)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/req.py\", line 118, in from_line\n    return cls(req, comes_from, url=url)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg/pip/req.py\", line 43, in __init__\n    req = pkg_resources.Requirement.parse(req)\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2510, in parse\n    reqs = list(parse_requirements(s))\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2436, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/home/peter/.virtualenvs/verde/local/lib/python2.7/site-packages/setuptools-0.6c11-py2.7.egg/pkg_resources.py\", line 2414, in scan_list\n    \"Expected ',' or end-of-list in\",line,\"at\",line[p:]\nValueError: (\"Expected ',' or end-of-list in\", 'wsgiref==0.1.2-r base.txt-r base.txt', 'at', ' base.txt-r base.txt')\n. There doesn't seem to be an exception at all on the same files when using straight up Pip. Pip has no problem with this:\npip install -r requirements/development.txt\nThe only time I get an exception is when using pip-dump.  Using PIP 1.3.1 on Python 2.7.\nAfter monkeying around, I've found it only happens when using the -r command with pip freeze.  If I add a line return at the end of base.txt it stops erroring.  So it has to do with the way that pip concatenates the files together -- it munges the last line of base.txt (wsgiref) with the first like of development.txt (-r base.txt).\nOff to file a ticket with Pip.\n. I'm going to reopen this ticket.  I found the bug in pip-dump.  The temp file it writes out doesn't add a new line at the end.  When I cat the temp file, it looks like this:\n(verde)peter@Blizzardme:~/PycharmProjects/Verde$ cat /tmp/tmpbo1pS2\ncoverage==3.6\nDjango==1.5.1\ndjango-admin-bootstrapped==0.4.1\ndjango-braces==1.0.0\ndjango-cache-panel==0.1\ndjango-classy-tags==0.4\ndjango-cms==2.4.2\ndjango-debug-toolbar==0.9.4\ndjango-discover-runner==0.4\ndjango-filer==0.9.4\ndjango-model-utils==1.4.0\ndjango-mptt==0.5.2\ndjango-polymorphic==0.5\ndjango-reversion==1.7\ndjango-secure==1.0\ndjango-sekizai==0.7\neasy-thumbnails==1.2\nhtml5lib==1.0b1\nmock==1.0.1\nMySQL-python==1.2.4\nPIL==1.1.7\npip-tools==0.3.4\npython-dateutil==2.1\npytz==2013b\n-r base.txt\nselenium==2.33.0\nsix==1.3.0\nSouth==0.8.1\nSouth==0.8.1-r base.txt## The following requirements were added by pip --freeze:\nWhat I don't understand is that South==0.8.1 is repeated twice.  It only occurs once in my requirements files.\n. ",
    "ysugiura": "I encountered the same problem. The workaround I found was adding LF at the last line in my case.\n. ",
    "coagulant": "I've tried compiling django-oscar, now it outputs UnsupportedVersionError:\n(pip-tools)\u279c  pip-tools git:(future) \u2717 pip-compile\nTraceback (most recent call last):\n  File \"/Users/coagulant/.virtualenvs/pip-tools/bin/pip-compile\", line 184, in <module>\n    main()\n  File \"/Users/coagulant/.virtualenvs/pip-tools/bin/pip-compile\", line 175, in main\n    compile_specs(src_files, include_sources=args.include_sources, dry_run=args.dry_run)\n  File \"/Users/coagulant/.virtualenvs/pip-tools/bin/pip-compile\", line 122, in compile_specs\n    pinned_spec_set = resolver.resolve()\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/resolver.py\", line 44, in resolve\n    if not self.resolve_one_round():\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/resolver.py\", line 28, in resolve_one_round\n    new_deps = self.find_new_dependencies()\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/resolver.py\", line 91, in find_new_dependencies\n    all_deps = self.find_all_dependencies()\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/resolver.py\", line 72, in find_all_dependencies\n    for spec in spec_set.normalize():\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/datastructures.py\", line 503, in normalize\n    new_spec_set.add_spec(self.normalize_specs_for_name(name))\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/datastructures.py\", line 446, in normalize_specs_for_name\n    if ops['<='](less_than, greater_than):\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/datastructures.py\", line 19, in _normalized\n    nv1 = NormalizedVersion(v1)\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/version.py\", line 42, in __init__\n    self._parts = parts = self.parse(s)\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/version.py\", line 268, in parse\n    result = _normalized_key(s)\n  File \"/Users/coagulant/.virtualenvs/pip-tools/lib/python2.7/site-packages/piptools/version.py\", line 203, in _pep426_key\n    raise UnsupportedVersionError('Not a valid version: %s' % s)\npiptools.version.UnsupportedVersionError: Not a valid version: 6.0.0a\n. Oops, wrong branch :flushed:\n. I have both django-model-utils and latest Django beta in my requirements.in:\ndjango-model-utils==1.5.0\n-e git+https://github.com/django/django.git@1.6b4#egg=Django\nAfter compilation I get unusable requirements.txt, because Django is mentioned twice (it literally does not work):\n-e git+https://github.com/django/django.git@1.6b4#egg=Django\ndjango-model-utils==1.5.0\ndjango==1.5.4\nThat is because django-model-utils depends to django>=1.4, so compilation adds 3rd line, which spoils the show.\nI guess it would be a right thing to make pip compile able to compare VCS lines with others specs properly and leave only latest one (being 1.6b4 in this case), much like how it works with normal PyPI packages.\n. @blueyed Not sure, but it's a bug anyway.\n. There is one already in pip\n$ pip list --outdated\ndjango-reversion (Current: 1.6.6 Latest: 1.8.0)\ncoverage (Current: 3.5.2 Latest: 3.7)\n...\n. @svetlyak40wt dependency_links is not the same as find-links. Anyway pip no longer respects dependency links by default since 1.5, so I don't think it's worth merging as is.\n. @nvie any thoughts how we should improve this code to get PR merged?\n. ",
    "maikhoepfel": "@nvie, thank you for the explanation. @mbertheau brought this to my attention in https://github.com/tangentlabs/django-oscar/issues/1524. I just merged an updated version spec for phonenumbers (we just bumped the min requirement), so the immediate issue should be resolved.\nI'll see about notifying phonenumbers about this as well. \nI'm guessing this issue can be closed.\n. ",
    "jerzyk": "+1 \n. I've installed your version, got error:\nRequirement file contains git+https://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-master, but that package is not installed\nRequirement file contains git+https://github.com/jerzyk/django-user-accounts.git@11d8970c3e10a4624b3d1f274b4eea1b8dd0c982#egg=django_user_accounts-master, but that package is not installed\nRequirement file contains git+https://github.com/masci/django-notification.git@py3_django15_support#egg=notification, but that package is not installed\nException:\nTraceback (most recent call last):\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/basecommand.py\", line 134, in main\n    status = self.run(options, args)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/commands/freeze.py\", line 89, in run\n    line_req = InstallRequirement.from_editable(line, default_vcs=options.default_vcs)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/req.py\", line 87, in from_editable\n    res = cls(name, comes_from, source_dir=source_dir, editable=True, url=url, prereleases=True)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pip/req.py\", line 44, in __init__\n    req = pkg_resources.Requirement.parse(req)\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2700, in parse\n    reqs = list(parse_requirements(s))\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2638, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/Users/jerzyk/Projects/Envs/ekonektor/lib/python2.7/site-packages/pkg_resources.py\", line 2606, in scan_list\n    raise ValueError(\"Expected \"+item_name+\" in\",line,\"at\",line[p:])\nValueError: ('Expected version spec in', 'pip_tools-origin/work-with-vcs', 'at', '/work-with-vcs')\n. they are specified exactly like this - in the correct form (-e git+https://...@...#egg=xxx)\n. here it is:\ndjango-appconf==0.6\ndjango-authority==0.7\ndjango-braces==1.2.2\ndjango-constance==0.6\ndjango-extensions==1.2.2\ndjango-filer==0.9.5\ndjango-floppyforms==1.1\ndjango-forms-bootstrap==2.0.3.post2\ndjango-haystack==2.1.0\ndjango-jsonfield==0.9.10\ndjango-model-utils==1.5.0\ndjango-mptt==0.6.0\ndjango-notification==1.1.1\ndjango-page-cms==1.5.2\ndjango-pagination==1.0.7\ndjango-picklefield==0.3.0\ndjango-polymorphic==0.5.3\ndjango-postman==3.0.1\ndjango-suit==0.2.5\nDjango==1.5.4\ndocutils==0.11\neasy-thumbnails==1.4\neventlog==0.6.7\nhtml5lib==0.95\nipython==1.1.0\nmetron==1.2\nPillow==2.1.0\npytz==2013b\nPyYAML==3.10\nraven==3.5.0\nreadline==6.2.4.1\nsix==1.4.1\nSouth==0.8.2\nUnidecode==0.04.14\nWhoosh==2.5.4\n-e git+https://github.com/masci/django-notification.git@py3_django15_support#egg=notification\ndjango-ajax-selects\n-e git+https://github.com/jerzyk/django-lineage.git@8aaf9d7ae4dc1d1a2dcc8b4d4da880c5cf401b4d#egg=django_lineage-master\ndjango-mailer==0.1.0\n-e git+https://github.com/jerzyk/django-user-accounts.git@11d8970c3e10a4624b3d1f274b4eea1b8dd0c982#egg=django_user_accounts-master\n-e git+https://github.com/svetlyak40wt/pip-tools.git@work-with-vcs#egg=pip_tools\n. first of all all version numbers are added automatically by the pip-dump\n(but version numbers are good for the projects that are being updated from time to time)\nclue is not to discuss what is in the requirements.txt, but why pip-tools with your patches are crashing, don't you think? ;)\n. sorry, but I'm talking about pip-dump, of course it may interact in the background with pip freeze, but the issue is how those two programs work together and pip-dump installed from your repo reports those errors, where original one does not - so I suppose this is related to your changes\n. Shoudn't you have \"-e \" in front and \"@\" as version separation?\n. add an option to check for vulnerabilities... https://pyup.io/safety/. ",
    "blueyed": "@coagulant \nI just wondered if the egg name is relevant here.. django != Django?\n. For reference: #93 is about a similar issue for -e.\n. Note: this appears to have happened in the future branch already: 981a062d\n. @patcon \nThanks for the hint (missed the Github notification about/for it).\nPundler looks (also) interesting.\n. Yes, it appears to be fixed in Python 2, but not for Python 3: #137\n. > Is 3.4 supported btw?\nGiven the error above it does not seem so.. ;)\nIt should probably get added to tox.ini / Travis and any bugs need to be fixed.\nI am not sure about unicode_literals. Maybe that something is this directions would also fix issue #87 (UnicodeEncodeError when writing wheel / cache files).\n. The python tests appear to be fixed (I had to update six, six.PY2 was not available in my old copy (1.3.0).\nThe cram tests fail now as follows. What does [1] mean here?\n```\n===> Running cram tests\n!\n--- pip-compile-allows-inheritance.t\n+++ pip-compile-allows-inheritance.t.err\n@@ -23,9 +23,8 @@\n as well.\n$ pip-compile develop.in >/dev/null 2>&1\n+  [1]\n$ cat develop.txt\n-  python-dateutil== (glob)\n-  pytz== (glob)\n-  six== (glob)\n-  times== (glob)\n+  cat: develop.txt: No such file or directory\n+  [1]\n!\n```\n. > I did not get a chance to look at these test failures.\nBut you also get them? Or is it something specific to my setup?\n. @nvie \nAre you sure?\nAre you mixing it up with https://github.com/nvie/pip-tools/commit/972405eb8ffb52fc88a6f0e976f51f6de518eb71 maybe?\n. Great.\nI've just also tried to reproduce #87, but could not.\n. OTOH, it was not only meant to be a fix for #87, but to add support for using pip's download cache.\nGiven the drawback regarding the archives being unzipped in the cache, is there a better approach?\n. @nvie \nCan you provide some feedback regarding my last comment?\nWouldn't it be good to use pip's download cache? (which certainly has changed / will change with newer pip versions)\n. This happens for normal editables, like -e /home/user/src/django-master, too.\n. @stefanfoulis \nCan you try if https://github.com/nvie/pip-tools/pull/169 fixes this one, too?\n. @ashwinvis \nIt does not work with -f https://github.com/lock8/django-rest-framework/archive/d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6.tar.gz#egg=djangorestframework for me: no error and no output..?!\npip-compile 3.1.0, pip 18.1.. AFAIK -f refers to --find-links, i.e. it allows to use a different index, doesn't it?. @ashwinvis \nWhat is your output with -v?\nGiven your example I only get:\n```\nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\nFinding the best candidates:\nFinding secondary dependencies:\nResult of round 1: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\n```. I would be interested in a workaround for this.\nI have tried adding pyyaml==3.11 as an explicit dependency (wasn't defined before, but comes via django-supervisor==0.3.2 ~> watchdog==0.7.1), which did not help. (But later this line was the one that triggered this bug).\nwatchdog defines it at https://github.com/gorakhargosh/watchdog/blob/master/setup.py#L78\nIt seems like pyyaml is a new dependency somehow, or some change triggered it to be pulled in.\nAt least it's not in pip freeze of an older virtualenv and this pip-compile error just popped up today.\n3.09 appears to be the version used by pyyaml: http://pyyaml.org/download/pyyaml/PyYAML-3.09.tar.gz\nThe error went not away after cleaning ~/.pip-tools, but only after I removed the explicit pyyaml line from the requirements.in file (which I only have added as a try to work around this).\nThe current state is that the error went away somehow again, and I still have pyyaml==3.11 in the compiled requirement files.\n. @nvie \nAgreed.\nBut please note that I had no pyyaml dependency defined manually initially and when debugging it, found out that it was pyyaml that caused trouble.\nIt would be nice if pip-tools/pip-compile could catch this exception and provide a more detailed error itself (e.g. mentioning the package name and where it comes from).\n. This can be triggered using this simple requirements.in file:\npyyaml==3.11\nwatchdog\nIt does not happen with just pyyaml or pyyaml>=3.11.\nThis is without the fix/change committed to watchdog (https://github.com/gorakhargosh/watchdog/issues/234), which has not been released yet.\n(The traceback is the same, with the line numbers differing a bit.)\nI have triggered this again, because I've tried to include the compiled base.txt requirements list with dev.in etc.\n. I have just tried NormalizedVersion from pip's vendorized distlib, and it accepts 3.09.!\n. > To be clear: this is a warning only, right?\nNot even that, just a note. And pip probably has to follow some redirect first probably.\nWhile the lowercased name could be used internally, the compiled file might still contain the original name, couldn't it?\n. Which pip version are you using?\n. That would be https://github.com/nvie/pip-tools/issues/124 then.\n. I guess we should vendorize/import package_to_requirement then?\nWhere should it go?\n. I'd recommend to try https://github.com/nvie/pip-tools/pull/126 instead.\n@nvie \nReviewing (and hopefully merging) it would be appreciated!\n. Should have been against the \"future\" branch: see #126.\n. Fixed via https://github.com/nvie/pip-tools/pull/126.\n. The rewrite mentioned above happened in/via #153.\n. This is fixed by now.\n@zacstewart \nDo you mean you get the error with just pip or pip-compile?\nAnyway pip install gnureadline works for me - maybe it was fixed in pip also?\n. This is fixed by now.\n. @nvie \nIs the import good like this?  Shouldn't it be moved to the top?\n. Fixing this for Python 3 might have broken it for Python 2.\nSee https://github.com/nvie/pip-tools/issues/147.\nThe Python 3 issue might have been this: https://github.com/nvie/pip-tools/issues/137 ?\n. I think we should get the test suite back on track (#146), which would help fixing this and prevent any regression of it.\n. This is provided via --annotate now with #153 (but the previous -i is gone).\n. I can recommend pyenv.\n. @nvie \nGreat news! (although you could have announced it before maybe, or was it spontaneous? :))\nAnyway, why don't you think that the current cram test suite could be used until there is something better?\nAt the very least this would show up any regressions/changes caused by the rewrite, too.\n. Thanks for your reply, I totally agree.\nHowever, I still think that the existing cram tests would be still useful (when fixed / adjusted) for now.\n. Fixed via #153.\n. A workaround is to use -e with them, but then they are not getting pinned to the current commit, like it used to be:\n-e git+https://github.com/nvie/pip-tools.git@future#egg=pip-tools\n. The previous format was:\ngit+git://github.com/bfirsh/django-ordered-model@9895bb4b31742274c1906065b2f339344f1f7fb8#egg=django-ordered-model\nI am using Git links in cases where I have a fork of a project with some additional changes or if a package (update) has not been released yet.\nI don't know if this can be done with pip functions.\nThe workaround is using -e, which is OK for me at the moment - and should not get blocked / cause an error.\n. I've created a followup issue for pinning Vcs dependencies: #161.\nThe current behaviour of not pinning those anymore (to a specific commit) is bad and defeats the purpose of pip-compile (for Vcs packages)..\n. Closing this a dupe of #104.\n. This is also included in #169.\n. @nvie \nWas this closed intentionally?\n. While this change is probably good anyway, I'd really like to cache the editables also for a certain period, e.g. 5 minutes.\nWhen building the requirements for my project, I call pip-compile for multiple requirements.in files (dev, testing, production), and they should all use the cached result from the first run.\n. The test failures are unrelated: see #167 for fixes.\n. This is included in https://github.com/nvie/pip-tools/pull/169 also.\n. It looks like this could be done through from pip.vcs import get_src_requirement:\ndef get_src_requirement(self, dist, location, find_tags=False):\n    \"\"\"\n    Return a string representing the requirement needed to\n    redownload the files currently present in location, something\n    like:\n      {repository_url}@{revision}#egg={project_name}-{version_identifier}\n    If find_tags is True, try to find a tag matching the revision\n    \"\"\"\n. I have this working now!\nPR will follow later.\n. That's not really related and for Github only, isn't it?!\nbtw: the PR I was referring to be made has been closed/rejected in the meantime: https://github.com/nvie/pip-tools/pull/169\n. @tysonclugg \nWithout looking at it again that would have been https://github.com/jazzband/pip-tools/pull/169.. Cool!\nWould be probably good to have this with pip-compile from the future branch, too.\n. Sadly that's probably not fit so good anymore, after the rewrite that has taken place recently.\nYou might be interested in #169 - reviewing it would be appreciated.\n. With the case at hand, it would be nice if pip-compile would handle this better.\npip-tools could maybe fallback to using the most recent version and issue a warning?!\n(Maybe a command line switch could be required for this?)\nCurrently it aborts and fails to compile the requirements altogether.\nThe issue is that I am installining django-factory_boy from Git (because of required fixes), and there it requires the upcoming/unreleased version of another package (factory_boy).  This is (hopefully) rather uncommon (and has been reported in https://github.com/rbarrois/django-factory_boy/issues/3), but pip-compile could be more helpful in handling it.\nEdit: installing both packages from Git fixes / works around this actually.\n. This would be especially useful in case there are multiple packages involved.\nI was just seeing this:\n\nCould not find a version that matches djangorestframework<=3.1.2,==3.1.3,>=2.3.8,>=3.0.4\n\nIt would be helpful to see directly where the <=3.1.2 is coming from.\n. @nvie \nWithout this being merged the Travis build results for new PRs are not that useful (because they will all fail).\nTherefore, please merge / review this.\n. @nvie \nPlease re-open?!\n. I've squashed / cleaned up the commits.  (the old tree is available at https://github.com/blueyed/pip-tools/tree/pinned-vcs.bak)\nPlease let me know if the topics should be split again, but it's mainly only the fix-tests PR (#167) that's not required to be in here.\n. @nvie \nI think this should be re-opened, too.\n. I've spent a lot of time on this when creating the PR and have tried to comment as good as possible on @nvie's remarks.\nI suggest trying this PR, possibly rebased on / merged with master, and report back if it works for you.\n. @mkleehammer \nHave you tried it?  (you can use it through my branch)\nAlso feedback on the review from @nvie and my answers might help to push this forward.\n. For reference: there appear to be conflicts with master (this PR is still against future).\nI've not looked to closely at them, but would be happy to do so after feedback from @nvie.\n. I'd argue that this is a much requested feature, has some additional tests, and does not break existing ones.  Also it is separated in several commits, which can be reviewed separately (instead of looking at the whole diff).\nI see your point though that you should not merge something that's not clear, but I won't pick this up again for rewriting, at least in the next days.\nFeel free to re-implement this feature in a different way, maybe using this PR's total diff and/or commits as hints.\nFor my projects it's good to use it as is, and as a result I might rebase this in the future etc, but won't rewrite/restructure it - also because I cannot see how it should be done (apart from the one place where you could use the pip library by now safely, and which was not the case in June).\n. > Should someone create a bug for this feature?\nI'd say so, please go ahead.\n. @nvie \nPlease re-consider - it's a standard and makes it easier for users.\nThere would also be a lib for Python, but it's rather easy and therefore I have not added it as a dep.\n. @grigi \nGood find.  I agree that we should use this instead.\n@nvie \nShould I provide a new/updated PR, or will you do it yourself?\n. @nvie \nWas this (and others) closed because of (temporarily?) deleting the future branch?!\n. Thank you!\n. @nvie \nThis should probably get re-opened, too.\n. @nvie \nSpeed is not the main issue here, but clarity: if you bump your requirements, it's much clearer if it only gets changed in base.txt, and not also in prod.txt, base.txt etc.\nI am currently using the following, which somehow works around it, but dependencies will be also added to dev.txt, although they are in base.txt already.\nLuckily this is only a problem if the versions differ (e.g. because dev.txt was updated, but base.txt was not): \n\nDouble requirement given: pytest==2.9.2 (from -r requirements/dev.txt (line 28)) (already in pytest==2.9.1 (from -r requirements/travis.txt (line 11)), name='pytest')\n\nThis Makefile snippet replaces some custom \"Depends on\" comment with \"-r\", after running pip-compile.\nmake\n$(PIP_REQUIREMENTS_DIR)/%.txt: $(PIP_REQUIREMENTS_DIR)/%.in\n    pip-compile --no-header --output-file \"$@.tmp\" \"$<\" >/tmp/pip-compile.out.tmp || { \\\n      ret=$$?; echo \"pip-compile failed:\" >&2; cat /tmp/pip-compile.out.tmp >&2; \\\n      $(RM) \"$@.tmp\" /tmp/pip-compile.out.tmp; \\\n      exit $$ret; }\n    @sed -n '1,10 s/# Depends on/-r/; s/\\.in/.txt/p' \"$<\" > \"$@\"\n    @cat \"$@.tmp\" >> \"$@\"\n    @$(RM) \"$@.tmp\" /tmp/pip-compile.out.tmp\nAnd improved speed is a nice side effect of this, too - of course.\nSo, please consider re-opening this issue.\n. Should work in https://github.com/nvie/pip-tools/pull/169 IIRC.\n. pip-sync != pip-compile.\nApart from that, https://github.com/nvie/pip-tools/pull/169 has been closed without being merged (at least for now).\n. @GaretJax \nInteresting!\nPlease consider creating PRs against this repo, too.\nIt seems to address issues that are often asked for (editables).  See also https://github.com/nvie/pip-tools/pull/169.\n. While I can see that having a timestamp might be useful in general, I cannot see why it helps in your use case.\nIt sounds like you want to force a changed .txt if the .in was changed?  Why isn't it enough to only commit the .in then, without the unchanged .txt?\nbtw: you can regenerate .txt files for .in files in make with this:\n$(PIP_REQUIREMENTS_DIR)/%.txt: $(PIP_REQUIREMENTS_DIR)/%.in\n    pip-compile $< > $@\n. > no way to verify that someone regenerated the file without regenerating the file yourself\nBut the file being changed does not mean that it's correct?!\nFor that you would need to regenerate it anyway, and that might include some updates then? (there's a new flag regarding that, but still).\nYou might want to add a check to your pipeline that does the real check, but possibly in a subtask on Travis/CI instead of in the regular developer's test suite.\nGiven caches etc (and not too many VCS requirements), it should be pretty fast.  With Travis for example you should cache ~/.cache, which pip uses by default.\n. What I understood from the original issue was that 1.9 should be considered to be equal to 1.9.23, and that you would use 1.9.0 to refer to that.  But then \"1.9.0\" might not exist, but only 1.9.\n\"semver magic\"?!\n. Hmm, sounds like you might have mixed up the tests?\nThere does not appear to be a relevant change between master and this branch apart from that, but I'll rebase it just to be sure.\n. Just tried it again (on master), and still can confirm it, using Python 3.5.2.\n```\n% mktmpenv\nUsing base prefix '/usr'\nNew python executable in /home/daniel/.pyenv/versions/tmp-system-UtVk4B/bin/python3\nAlso creating executable in /home/daniel/.pyenv/versions/tmp-system-UtVk4B/bin/python\nInstalling setuptools, pip, wheel...done.\nIgnoring indexes: https://pypi.python.org/simple\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /home/daniel/.pyenv/versions/tmp-system-UtVk4B/lib/python3.5/site-packages\nRequirement already satisfied (use --upgrade to upgrade): pip in /home/daniel/.pyenv/versions/tmp-system-UtVk4B/lib/python3.5/site-packages\nEntering a new shell session with virtualenv 'tmp-system-UtVk4B' (system).\nIt will be removed when exiting (ctrl+d or 'exit').\n% pip install -e .\nObtaining file:///home/daniel/Vcs/pip-tools\nCollecting click>=6 (from pip-tools==1.7.1.dev0)\nCollecting first (from pip-tools==1.7.1.dev0)\n  Using cached first-2.0.1-py2.py3-none-any.whl\nCollecting six (from pip-tools==1.7.1.dev0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nInstalling collected packages: click, first, six, pip-tools\n  Running setup.py develop for pip-tools\nSuccessfully installed click-6.6 first-2.0.1 pip-tools six-1.10.0\n% cat test.in\ndjango-autocomplete-light[gfk]\n% rm ~/.cache/pip-tools/depcache*\n% pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\ndjango-querysetsequence==0.6  # via django-autocomplete-light\ndjango==1.9.7             # via django-querysetsequence\n% pip install django-autocomplete-light\nCollecting django-autocomplete-light\nInstalling collected packages: django-autocomplete-light\nSuccessfully installed django-autocomplete-light-3.1.6\n\u03bb m@o\u2717\u276f\u276f pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\ndjango-querysetsequence==0.6  # via django-autocomplete-light\ndjango==1.9.7             # via django-querysetsequence\n% rm ~/.cache/pip-tools/depcache*\n% pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\n```\nThen on my branch (rebased on master):\n```\n% gco -\nSwitched to branch 'pypi-ignore_installed'\nYour branch is up-to-date with 'blueyed/pypi-ignore_installed'.\n% grbiom\nSuccessfully rebased and updated refs/heads/pypi-ignore_installed.\nIt will still fail first, because of the existing cache:\n% pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\n% rm ~/.cache/pip-tools/depcache*\n% pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\ndjango-querysetsequence==0.6  # via django-autocomplete-light\ndjango==1.9.7             # via django-querysetsequence\n% pip uninstall django-autocomplete-light\nUninstalling django-autocomplete-light-3.1.6:\n  \u2026\nProceed (y/n)? y\n  Successfully uninstalled django-autocomplete-light-3.1.6\n% rm ~/.cache/pip-tools/depcache*\n% pip-compile test.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\ndjango-autocomplete-light[gfk]==3.1.6\ndjango-querysetsequence==0.6  # via django-autocomplete-light\ndjango==1.9.7             # via django-querysetsequence\n```\nIf I remember correctly this might be related to django-autocomplete-light using \"GFK\" instead of \"gfk\" in the extras_require.  But the main difference is that two separate code paths are used in the beginning.\n. btw: it behaves differently when installing django-autocomplete-light using --editable: then it works on master, too.  Probably because it's not picked up in that case?!\n. Replacing \"GFK\" with \"gfk\" in \u2026/pyenv/tmp-system-UtVk4B/lib/python3.5/site-packages/django_autocomplete_light-3.1.6.dist-info/METADATA and \u2026/pyenv/tmp-system-UtVk4B/lib/python3.5/site-packages/django_autocomplete_light-3.1.6.dist-info/metadata.json fixes it on master, too.\n. @vphilippon \nYes, it is old already and I do not remember the details, only that I digged into this deeply likely back then for the fix that is being ignored for so long.\nIt is likely because of the case of the extras dep getting lost somewhere, and IIRC it is inside of pip itself.\nI think it picked up the already installed dep in one part of the code, but not another - and ignoring it altogether (or rather where this patch applies to) fixed it then.. @piotr-dobrogost \nHere we are.. ;)\n(As for stats / how likely inclusion will be: https://github.com/nvie/pip-tools/pulls?q=is%3Apr+author%3Ablueyed+is%3Aclosed). The problem is that pip exports the source into a separate directory without the .git dir:\nrequirements.in:\n-e git+https://github.com/pytest-dev/pytest-django@master#egg=pytest_django\nOutput from pip-compile with printing the cmd in pip using the following patch:\ndiff\ndiff --git i/pip/vcs/__init__.py w/pip/vcs/__init__.py\nindex e0da09d..45876e1 100644\n--- i/pip/vcs/__init__.py\n+++ w/pip/vcs/__init__.py\n@@ -315,6 +315,7 @@ def run_command(self, cmd, show_stdout=True, cwd=None,\n         command name, and checks that the VCS is available\n         \"\"\"\n         cmd = [self.name] + cmd\n+        print(cmd)\n         try:\n             return call_subprocess(cmd, show_stdout, cwd,\n                                    on_returncode, command_level,\nThe call from pip-tools to pip is dependencies = reqset._prepare_file(self.finder, ireq).\n['git', 'clone', '-q', 'https://github.com/pytest-dev/pytest-django', '/tmp/pip-cxmdcbog-export']\npip-clone: https://github.com/pytest-dev/pytest-django\n - Add classifier for Python 3.5 in setup.py (#366) (3 weeks ago, 175de01, user Hahler)\n['git', 'show-ref']\n['git', 'rev-parse', 'HEAD']\n['git', 'checkout-index', '-a', '-f', '--prefix', '/tmp/tmprsvzkgtzsource/pytest-django/']\nTraceback (most recent call last):\n  File \"\u2026/pyenv/project/bin/pip-compile\", line 9, in <module>\n    load_entry_point('pip-tools', 'console_scripts', 'pip-compile')()\n  File \"\u2026/pyenv/project/lib/python3.5/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"\u2026/pyenv/project/lib/python3.5/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"\u2026/pyenv/project/lib/python3.5/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\u2026/pyenv/project/lib/python3.5/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"\u2026/Vcs/pip-tools/piptools/scripts/compile.py\", line 163, in cli\n    results = resolver.resolve()\n  File \"\u2026/Vcs/pip-tools/piptools/resolver.py\", line 103, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"\u2026/Vcs/pip-tools/piptools/resolver.py\", line 185, in _resolve_one_round\n    for best_match in best_matches\n  File \"\u2026/Vcs/pip-tools/piptools/resolver.py\", line 186, in <genexpr>\n    for dep in self._iter_dependencies(best_match))\n  File \"\u2026/Vcs/pip-tools/piptools/resolver.py\", line 243, in _iter_dependencies\n    for dependency in self.repository.get_dependencies(ireq):\n  File \"\u2026/Vcs/pip-tools/piptools/repositories/pypi.py\", line 132, in get_dependencies\n    dependencies = reqset._prepare_file(self.finder, ireq)\n  File \"\u2026/Vcs/pip/pip/req/req_set.py\", line 491, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"\u2026/Vcs/pip/pip/req/req_set.py\", line 127, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"\u2026/Vcs/pip/pip/req/req_install.py\", line 428, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"\u2026/Vcs/pip/pip/utils/__init__.py\", line 718, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmprsvzkgtzsource/pytest-django/\n. @dfee \nHave you tried https://github.com/jazzband/pip-tools/pull/385?\nNeeds to be rebased, but might be easy.. @dfee \nSorry about that.\nBy now the project moved to jazzband so I could fix the label at least.\nAs for the PR, you might want to revisit/update it maybe?\n(I've put too much time/effort into pip-tools already, which has mostly not been accepted or ignored when it wasn't a jazzband project, so I came passive in that regard). @dfee \nThe milestone seems to be 1.10.1 now: https://github.com/jazzband/pip-tools/milestone/2. @vphilippon \nSorry, but it's not very likely that I'll come back to this in the near future.. @dfee \nFeel free to pick it up!. pytest-django?. Thank you for picking this up, @dfee!\nAnd happy birthday! :birthday: . I personally do not like pipenv, and prefer to use pip-tools therefore.. @atugushev \nOh, I forgot that I am turning git+ URLs into https already (note the second snippet, which does not use git+ anymore).\nSo I think the issue still applies, except for that you have to use non-git URLs in the first place.. @atugushev \nIt needs to be editable, i.e. add -e in front.. @atugushev \nOh, sorry.. editable is not necessary.. this should work (as a result):\nhttps://github.com/lock8/django-rest-framework/archive/d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6.tar.gz#egg=djangorestframework --hash=sha256:8a25e5ea1727e83bb55c3459b1116161ebf67314696672227a053265564c6af9\nI am using this to transform/compile requirements:\nmake\n$(PIP_REQUIREMENTS_ALL):: $(PIP_REQUIREMENTS_DIR)/%.txt: $(PIP_REQUIREMENTS_DIR)/%.in\n    @pip-compile --no-header --generate-hashes $(PIP_COMPILE_ARGS) --output-file \"$@.tmp\" \"$<\" >\"$@.out\" || { \\\n      ret=$$?; echo \"pip-compile failed:\" >&2; cat \"$@.out\" >&2; \\\n      $(RM) \"$@.tmp\" \"$@.out\"; \\\n      exit $$ret; }\n    @sed -n '1,10 s/# Depends on/-r/; s/\\.in/.txt/p' \"$<\" > \"$@\"\n    @# Keep and transform '-e git+' as-is (includes the hash).\n    @sed -n -e '/-e git+/ {s~^-e git+\\(http.*\\)@\\([^#]\\+\\)\\(#.*\\)\\?~\\1/archive/\\2.tar.gz\\3~; s~\\.git/archive~/archive~; p}' \"$<\" >> \"$@\"\n    @# Remove any editables (not supported with hashes).\n    @sed -e '/^-e /d' \"$@.tmp\" >> \"$@\"\n    @$(RM) \"$@.tmp\" \"$@.out\"\nThe relevant part is sed -n -e '/-e git+/ {s~^-e git+\\(http.*\\)@\\([^#]\\+\\)\\(#.*\\)\\?~\\1/archive/\\2.tar.gz\\3~; s~\\.git/archive~/archive~; p}' to transform -e git+ (for pip-compile to work) into https://\u2026 in the result.. Hmm, also no failed request in https://travis-ci.org/jazzband/pip-tools/requests.. :confused: . Should also get .coveragerc fixed from #711 .\nI suggest to integrate the reporting to coveralls into #711 also - it is OK to use both IMHO.. Yes, why not?. Looks better now already: https://codecov.io/gh/jazzband/pip-tools/tree/codecov/piptools\nI suggest to also include tests, since it will show if tests are not being executed for unexpected reasons etc.. Yeah, sounds good to me.. Restarted Travis builds - there appears to have been a temporary failure with downloads for py34/py35/pypy/pypy3.. Great.  Will let you / somebody else merge this.. Looks good: https://travis-ci.org/jazzband/pip-tools/builds/493877860 - canceled the build.. > Basically it's the old partially fixed issue #369 with one corner case not being fixed.\nDo you have a suggested fix?  (sorry, have not investigated, but it sounds like you know the corner case from debugging already). This will likely cause problems with codecov timing out though - if there are non already (given the huge number of builds for linux already).\nWe're having trouble (likely due to flags and number of builds) in pytest (and other projects) since months.\nSo in general it would be better to only run selective envs with coverage, by using a coverage factor in tox (see pytest's tox.ini).. @atugushev \nWhen looking at PRs via codecov comments or the UI in general it might time out on their backend (\"connection reset\").\nSee https://github.com/pytest-dev/pytest/pull/4846#issuecomment-467949404.. Here https://codecov.io/gh/jazzband/pip-tools/pull/745?src=pr&el=h1 is not timing out, although there are a lot of builds.\nSource code size might also be a factor - I don't know.\n. So I suggest adding flags (\"windows\" / \"linux\" (or rather $TRAVIS_OS_NAME)) here after all.. For reference: extra coverage due to this: https://codecov.io/gh/jazzband/pip-tools/compare/c45862bd3e99df71fc020f828395fc7f6766be7c...fc69f3a2eab8d54c1a9850dcb2fdd9ec3d6930c5/changes\nBut it could be achieved with a single (or few) jobs only probably also.. Needs to be rebased.\nThen hopefully the code offsets/diff with https://codecov.io/gh/jazzband/pip-tools/pull/747/changes are correct also.\nIt looks like there was still some branch missing at least - but still an improvement, of course!. :+1: \nI've created https://github.com/jazzband/pip-tools/pull/748 to handle CI better in this case.. Manually canceled Travis.\nThere were two builds for it (the first one being auto-canceled).. Same time on AppVeyor compared to master.\nBut slower on Travis: 2h06 vs. 1h49 - might not be relevant though.\n. > @blueyed\n\ni've just noticed that since https://travis-ci.org/jazzband/pip-tools/jobs/483526623 the tests are not run for pip==19.0.\n\nhttps://github.com/jazzband/pip-tools/pull/753. Travis jobs are down to 45 (from 58) with this: https://travis-ci.org/jazzband/pip-tools/builds/500616125.. Canceled AppVeyor, to save the environment and because it is not necessary to check.\nBut hopefully Codecov complains about dropped coverage.. It is meant to reduce the number of jobs.\nBut it is also true that pypy is slower with coverage (so much that pytest does not run it with coverage in the first place, but here it is still okish).. TODO: squash-merge. Hmm codecov status/comment would be good.\nhttps://codecov.io/github/jazzband/pip-tools/commit/17ffcc448c6c5485407b768a7caa3c08099b02a8 says:\n\nReports are queued for processing... Please review report with caution, it may change.\n\nPushed notifications manually on https://codecov.io/gh/jazzband/pip-tools/commit/17ffcc448c6c5485407b768a7caa3c08099b02a8/build (\"Notifications are pending CI completion. Waiting for GitHub's status webhook to queue notifications. Push notifications now.\").. IIRC it is a special case in RequirementSet() - without this, the source dir would not be created.\n. It is waiting in https://github.com/pypa/pip/pull/2913.\n. I wasn't sure.\n. Yes, that would be Link.is_artifact (see desc for https://github.com/blueyed/pip-tools/commit/b6c4a0b12fcc5f5359dcd72b82d7dd3f54a5e20f).\n. IIRC it comes from https://github.com/blueyed/pip-tools/commit/82aec97a10d5af438eff86df1f3921bbf83ff029. It provides both a performance optimization, but is required actually, because otherwise pip would complain about an existing build dir.\n. I think here the same reason from above applies.\n. This PR  is also about allowing editable requirements again, so yes, it was intentional.\n. Should I change this?\nI would rather leave it as-is for now, but then fix this later.\nBut I could also do a fixup-commit for this PR.\n. It used best_match = ireq for both ireq.editable and is_pinned_requirement(ireq).  This hunk adds ireq.link to it, which includes everything to be downloaded.\nI cannot remember if that is correct/required really from the top of my head now, sorry.\nMaybe it should also use Link.is_artifact?!\n. What about just os.makedirs(cache_dir, exist_ok=True)?\n. Might have some drawbacks, e.g. with regard to older Python versions though.\nIt was just that mkdir -p came into my mind when seeing this.\nAfter all, your current code is OK and readable!\n. @cjerdonek \n\nshouldn't the log be going to stderr like most logging does?\n\nAgreed.  Do you want to create an issue for it?\nBut then this log line is still a bit too verbose (move it to debug? / add a prefix like \"writing line\"?).\n. It does not use the logging module, but click: https://github.com/nvie/pip-tools/blob/d521d766113e69ea0a10017bb968d3b3bb242e5f/piptools/logging.py#L21-L22.\n. It should upload to coveralls after running (test) jobs.\nWith this it will have a separate (new) job that would run tests, and upload coverage for this only.. Yes, but not for qa ones etc.\nCoveralls allows for multiple/parallel uploads itself.\nIf codecov would be used it could have different flags per job.. Just piptools (withinclude) might work better (also for report, but uses run as default IIRC).\nIt uses the module then.. Weird.  It says for me:\n\nCoverage.py warning: --include is ignored because --source is set (include-ignored)\n\nAnd it seems include with run can be removed.  Seems to be fine with report.. I assume there would be some way to detect this list automatically, but it would likely require to import it then?. Awesome!. src_files == ('setup.py',) ?. Couldn't this be left/adjusted?. Not really.\nJust wondered first why it was removed - but that was not due to it not being there then anymore.. You're welcome!  Thanks for working on pip-tools! :). This should be asserted/checked in a test.\nOr are there existing, unchanged ones already for it?. I think this should be more specific, and could probably also be asserted from a test that runs --help again (less overhead).. Re \"more specific\": it should match other parts of the line printing this, so that it would not match e.g. a copyright line etc.. ;). Not sure if this is guruanteed to exist always.  It should fallback to just \"pip\" maybe if it does not exist.. Also, what about using [sys.executable, \"-m\", \"pip\"] ?. This could be used in general, also with $VIRTUAL_ENV.. suggestion\n    - codecov --flags windows --name %TOXENV%. suggestion\n    - \"SET PATH=%PYTHON%\\\\Scripts;%PATH%\"\nShouldn't that be enough?  (but just a guess). Yes, in general - but flags might trigger timeouts with codecov.io, so I would rather only use name here for now.\nIt does not post comments anyway, does it?  (where the flags would be visible). Ok.. Yeah, of course: https://ci.appveyor.com/project/jazzband/pip-tools/builds/22717305\nFixed.. Just a nitpick: the asserts could be dedented.. Just --cov should be enough - then it uses the config (which as source=\".\" though).  I think those should be in line.. Not covered?. Done.  Sorry for missing it in the first place... suggestion\n    # Ensure the build_isolation option in PyPIRepository has the expected value.. I'd prefer to assert/compare the whole list, but maybe only still the single option.  This would include the number of calls then also - currently it might just assert nothing silently.. 1. MockPyPIRepository.call_args_list might be empty\npython\nassert [call[0][2] for call in MockPyPIRepository.call_args_list] == [expected, \u2026]. btw: you could use [ci skip] for the commit with the suggested changes - since it should be squashed later anyway, and would save some hours of CI.. Yes, or the above, where you expect it to be a non-empty list.. > Or we just could assert it called once (before iteration over call_args_list) to ensure the call_args_list is not empty:\n\npython\nMockPyPIRepository.assert_called_once()\n\nYes.  Although I would prefer a single assertion using a list comprehension instead.. Yes, one assertion is better in my opinion, too.\nassert [call[0][2] for call in MockPyPIRepository.call_args_list] == [expected] is just that, no? :)\nThanks for adjusting!. It is used for matrix expansion, in the codecov name, and in https://github.com/blueyed/pip-tools/blob/3add05df690b4fd3350e1deb48909055897f45b1/tests/test_cli_compile.py#L19-L24.. ",
    "laike9m": "change all pip-review/pip-dump files to pip-review.py/pip-dump.py and it will work fine.\n. I agree with you, it's better to modify setup.py.Frankly I just wrote what I did into README...\n. ",
    "joao-carloto": "I've recently installed pip-tools in windows 8.1 using PIP and I still had the same problem. The pip-review and pip-dump files had no extension and could not use them without renaming. Tested with Python 2.7.8 and 3.4.2 with the same results.\n. ",
    "itavero": "What's the status of this issue? Still seems to be broken on this end.\n. ",
    "jgonggrijp": "pip-review should work under Windows as of version 0.5. Otherwise, please submit a ticket to my fork.\n. Was done in https://github.com/jgonggrijp/pip-review/commit/a08ee67c7e5c670267f90f7b992ede2107a9cde8.. Was fixed in jgonggrijp#16. First release after pip-review 0.5 will include this fix.. For the new standalone pip-review, this (i.e. the unavailability of the script under Windows) was fixed in version 0.5. If this is still an issue with the current pip-compile and pip-sync, I suggest having a look at https://github.com/jgonggrijp/pip-review/commit/a08ee67c7e5c670267f90f7b992ede2107a9cde8 for a clean way to tackle this. See also https://github.com/jgonggrijp/pip-review/issues/12.\nAs for the unrelated issue reported by @jmozmoz, this is a duplicate of https://github.com/nvie/pip-tools/issues/182, https://github.com/jgonggrijp/pip-review/issues/14 and https://github.com/jgonggrijp/pip-review/issues/17. It was fixed in https://github.com/jgonggrijp/pip-review/pull/16. I plan to publish a new version to PyPI that includes the fix soon.. @b4stien pip-review is now living on as a separate package, see here. If you think your pull request is still relevant, please consider submitting it to my fork (develop branch).\n. @wil93 pip-review can do something that neither pip list -o nor pip-compile can do: it can upgrade all outdated packages for you, interactively or fully automatically.\nNow, using pip list -o and then manually installing the updates that you want may be not much more effort than using pip-review --interactive. pip-review --auto, on the other hand, is much more convenient, especially because it can also be used in a scripting context. For me, pip-review --auto was sufficient reason to create a new repo and a new package for pip-review.\nHowever, if people still think that the pip ecosystem doesn't need pip-review after reading this, I'd like to hear their reasons. I don't want to keep something in the air while people think it isn't needed (mind you, the package was downloaded about 1100 times since I uploaded it one month ago).\n. @wil93 What you describe is possible. However, I don't think it is less effort than sticking with Vincent's existing code for pip-review and accepting @b4stien's pull request (which is pretty much ready if I'm right). Personally, I don't really care how the functionality is provided, as long as it is there, and this is not something I want to spend a lot of time on.\nI don't mind if somebody else takes up the challenge and implements what you describe. If it works well, I don't even mind if they call it pip-review 2.0 or something like that (though @nvie should probably have a say in that as well). If something better exists I'll be happy to drop the current pip-review package, no feelings hurt. But I want the functionality to exist and as long as I have to do it, I'll take the least effort route.\n. @Jc2k No, those commands are not equivalent.\npip-compile && pip-sync pins the currently installed versions of your dependencies (presuming you defined your top-level dependencies in requirements.in) and uninstalls all non-dependency packages except for pip-tools and pip-review. So if you have SQLAlchemy installed but it is not a dependency of your project, it will be gone after this command.\npip-review --auto && pip freeze >requirements.txt updates all of your packages and then pins all of them in your requirements.txt. So in this case, SQLAlchemy (if installed) would be updated and pinned regardless of whether it is a dependency or not.\nWhen I said pip-review --auto is useful in a scripting context, I meant to say it is currently the most convenient way to update all packages in your virtualenv without human intervention. Suppose you have a virtualenv in which you want to keep all packages up-to-date all the time; you could for example have that by writing a crontab that runs pip-review --auto periodically.\nAs for ideal workflows, I think pip-compile, pip-sync and pip-review augment each other nicely. Suppose your project is in a state where everything works harmoniously. You pin your requirements using pip-compile and now you can reproduce your well-balanced set of packages everywhere using pip-sync. A few weeks or months later, packages have been updated, some containing important security fixes. You use pip-review to get things up-to-date and make sure that everything still works. If it doesn't and you think the update is not worth the trouble of fixing things, you can roll back easily using pip-sync. If it does work, or if you decide to fix things until everything does work, you can pin the new set of requirements using pip-compile. Rinse and repeat.\n. @wil93 After re-reading I noticed that you said\n\n(while, for non-interactive upgrade, there is pip-sync)\n\nI should point out that this is a different kind of upgrading than pip-review --auto. The latter always upgrades every installed package to the latest version available. pip-sync, on the other hand, will upgrade a package if it is required and the installed version is older than the version in the requirements.txt, but otherwise it will downgrade, uninstall or ignore a package (even though a newer version might be available). It may also install a new package if it is in your requirements.txt but not in your environment (pip-review --auto will never do that). Even when pip-sync does upgrade a package, it will upgrade it to the version in your requirements.txt, which might be older than the latest available version. pip-sync makes your environment reflect your requirements.txt 100% no matter how outdated it is. These commands really serve very different purposes.\n. @b4stien In the meanwhile I received a pull request by @rvause which solves this issue. pip-review is now at version 0.4. @nvie I think this ticket can be closed.\n. pip-review is now a separate package on PyPI.. I'm glad this was tackled, it greatly improves interoperability with pip-review because it separates updating from pinning.\n. @eli-collins pip-review can now be installed separately from PyPI. Please resubmit your issue to the pip-review issue tracker if you feel it is still relevant.. Reference for random wanderers: pip-review lives on as a separate package.\nThis issue was recently fixed on the pip-review repository: #14.. I went ahead and created a standalone package for pip-review: https://pypi.python.org/pypi/pip-review\nPlease don't be shy and submit as many pull requests as you can manage to my fork: https://github.com/jgonggrijp/pip-review\nThe initial release is called version 0.3.7, but the only difference with 0.3.6 is that I removed pip-dump (which is obviously obsolete now that we have pip-compile and pip-sync) and changed the name and maintainer of the package.\n. You are probably looking for pip-compile --upgrade.. My understanding is that pip-compile --upgrade does not actually upgrade anything. It just pins the latest available versions of your main requirements as well as the latest compatible versions of their dependencies in your requirements.txt, instead of the versions that are currently installed. So you end up with a requirements.txt that, if installed using pip-sync or pip install -r, will cause your main requirements and their dependencies to be updated.\nFWIW, I believe that you are not supposed to manually pin your requirements in the requirements.in. You should just list them (without versions) and leave the pinning to pip-compile. The requirements.txt should be the single source of truth about what versions of what packages you rely on.\nAs far as I know, there is no tool that lists only the outdated primary requirements and lets you update them interactively. I think the following workflow comes nearest: run pip list to see the installed versions and pip-compile -Un to see the latest versions. The latter command states explicitly whether a package is a primary requirement or an indirect dependency, so you can use that to mentally filter the information. Selectively upgrade primary requirements using pip install -U. Finish by running pip-compile (without -U and without -n) to pin the new state of your virtual environment. Optionally, run pip-sync after this to remove packages that you don't need anymore.\nPersonally, I tend to use a lazier workflow: simply upgrade everything using pip-review --auto (from the separate pip-review package). Most of the time, everything still works fine after the update and you can directly pin the new state using pip-compile. Some obsolete packages might remain in your environment but pip-compile won't list them, so you don't need to be afraid that you'll install \"garbage\" when later using pip-sync. If something breaks, fix the problems before pinning, or when there are too many problems, revert to the pre-update state using pip-sync and then only install the most urgent updates.. Yes, I think the workflow with piprot that you describe is pretty much against the philosophy behind pip-tools and pip-review. Partly because requirements.in is not at all supposed to look like a pinned requirements.txt, and partly because the point of pip-tools is that you start with a working environment and then pin it in order to ensure that the exact same environment can be reproduced elsewhere in full working order (see pin your packages and better package management). You should first check that a new version works and then pin it, not the other way around.\nFrankly, I don't see much place for piprot in a workflow based on pip-tools. I can only think of using it as a diagnostic; if piprot reports that your requirements.txt is 300+ days outdated, that might be a reason to consider putting some effort into updating. However, I think requires.io is a more flexible, powerful and convenient tool for such diagnostics.\nThe reason that you describe, for starting to pin versions in the requirements.in and using piprot, might be outdated. ;-) In the past, pip-compile always implied --update, so that might explain why you were seeing unwanted changes. Nowadays, pip-compile only pins installed versions by default. So you might be able to do away with piprot entirely.. To whom it concerns: pip-review reached version 1.0 (PyPI). It has become a thin wrapper around pip list --outdated that still enables you to update your packages automatically or interactively.\n@nvie pip-review has been ostensibly under BSD license since before I adopted it. I figured it might be time to make it official (see https://github.com/jgonggrijp/pip-review/issues/56). Do you agree if I add the 3-clause BSD license text to the pip-review repository with the following header?\nCopyright 2012-2015 Vincent Driessen\nCopyright 2015-2017 Julian Gonggrijp\n\n. Welcome, and thanks for accepting!\n. For completeness, pip-review now lives on as a separate package. Just pip install pip-review if you want the old program back. The repo is under my account.\n. @kengruven The intended usage is to not pin the versions in requirements.in. You are supposed to let pip-compile pin the versions for you. If you pass the --upgrade flag, it pins the latest version rather than the currently installed one.\nThe added convenience of pip-review is that you can try (review) the upgrades without pinning the new versions in advance. However, strictly speaking you can live with just pip-compile and pip-sync.\n. ",
    "MaikuMori": "In hub case, what Anton said is correct, they just use alias. Git\nsubcommand is nice solution too, but a bit harder to manage if you want to\nprovide more than one command.\nOn Thu, Jul 24, 2014 at 11:00 AM, Vincent Driessen <notifications@github.com\n\nwrote:\nThe way git works is that if a subcommand isn't found, it tries to look\nfor an executable script on your PATH named git-$SUBCOMMAND, which is a\nreally simple solution. Simply creating a git-foo script somewhere on\nyour path and making it executable is enough. You can use a shebang line to\nimplement it in any language of choice.\nIt's a really simple technique and would fit pip just as well, I think.\nBut... I'm not sure if the pip maintainers would consider this a useful\naddition to the tool.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/nvie/pip-tools/issues/60#issuecomment-49978167.\n. \n",
    "zed": "There is setuptools' entry_points argument -- something like:\nsetup(\n      # other arguments here...\n      entry_points = {\n        'console_scripts': [\n            'pip-review = pip_tools.review:main',\n            'pip-dump = pip_tools.dump:main',\n        ],\n    },\n    packages = find_packages()\n    # `scripts` argument should be dropped\n)\nIt automatically generates scripts for you with the correct extension, and on Windows it will even create an .exe file so that users don\u2019t have to change their PATHEXT settings.\nIt requires moving the scripts into pip_tools package. Running from a checkout could be preserved for backward compatibility e.g., in  bin/pip-review:\n```\n!/usr/bin/env python\nbin_parent_dir =  # e.g., see twisted bin/_preamble.py\nimport sys\nsys.path.insert(0, bin_parent_dir)\nfrom pip_tools.review import main\nmain()\n```\nOr current bin/ scripts could be just dropped and python -mpip_tools.review or python -mpip_tools.dump could be used instead to run from a checkout directory without installing the package.\nGenerating scripts is one step closer to Windows support: #58 \n. The removal breaks Upgrading all packages with pip use-case.\n. ",
    "TWAC": "pip-review does not seem to expect CRLF, this fixes it for me:\n``` diff\n--- a/bin/pip-review\n+++ b/bin/pip-review\n@@ -140,7 +140,7 @@ def get_installed_pkgs(local=False):\n output = check_output(command).decode('utf-8')\n\n\nfor line in output.split('\\n'):\nfor line in output.splitlines():\n         if not line or line.startswith('##'):\n             continue\n```\n. \n",
    "philippeowagner": "+1 from my side for this idea. I'd love to see this integrated into pip-tools (pip-review)!\n. ",
    "graingert": "@coagulant but those are for packages that are currently installed. I want it to look at the requirements.txt/requirements.in file and pypi without actually installing any packages.. Is this was @sontek was getting at in #65?\n. instead of ignoring -markerlib what about also ignoring anything without a valid package name:\n^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$. @vphilippon as defined: https://github.com/pypa/warehouse/blob/2b54d1e6a004ced695745d1f38a4304d81d6f8f7/warehouse/packaging/models.py#L82. @vphilippon https://github.com/jazzband/pip-tools/issues/618 done. Now I can't install any packages\n. FYI you can use\npython\nimport hashin\nhashin.get_package_hashes('pip-tools', version='1.8.1')\nhttps://github.com/peterbe/hashin/pull/31. @jonafato I think it would be better to continue to use PackageFinder directly, and upgrade it in pip\nSee https://github.com/jazzband/pip-tools/pull/487 and https://github.com/pypa/pip/pull/4423. Might be better to use packaging https://packaging.pypa.io/en/latest/requirements/ instead, which is safe to depend on.. @jayfk is the code to generate safety-db free software? If this were available I can see this being integrated.. @jayfk if you made the code that generates safe-db available under a free software licence I can see this as something that would be very easy to add. sort of fixes https://github.com/jazzband/pip-tools/issues/475. @jdufresne neato, @davidovich can we get a release soonish?. Also as it happens, I've got a bunch of these directories nested. I'd like to update all the f'requriements{category}.txt' files in one shot.. Yes. Means you don't have to download the package to get the hash. @suutari it would be even better if you pulled the SHA256 hashes from PyPI directly.. Fixes https://github.com/jazzband/pip-tools/issues/544. @suutari have a look at https://github.com/jazzband/pip-tools/issues/543 too.. Let's continue to only use free (freedom) network services, and chastise\nthose that use promote or create non-free network services.\nOn 16 Oct 2017 21:26, \"Ryan P Kilby\" notifications@github.com wrote:\n\nThe suggestion to integrate pyup.io has been discussed and denied in #486\nhttps://github.com/jazzband/pip-tools/issues/486. My recommendation\nwould be to create an independent tool that checks requirements files for\noutdated dependencies, and then use that as part of your workflow. This\ntool could be used with or without pip-tools. eg,\nUsers of pip-tools would have a workflow of:\nwrite requirements.in > check w/ safety > pip-compile\nThose who don't use pip-tools would then just do:\nwrite requirements.txt > check w/ safety\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/579#issuecomment-337028840,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAZQTKle8iduOiIumJ5k8BE2Qhd-k0Lfks5ss7vygaJpZM4P6eZ1\n.\n. pip-compile --upgrade doesn't unpin deps pinned in requirements.in. @vphilippon . @vphilippon it's a plain old ubuntu python3 -m venv. does this include the new hash of the new wheel in the case that someone uploads a .tar.gz, then later a wheel?. I often hit this scenario. Because I regularly find packages without wheels and I go and poke the maintainer to go release them. https://github.com/jazzband/pip-tools/issues/521 was closed by https://github.com/jazzband/pip-tools/pull/641 but it doesn't actually help my use-case, where I regularly regenerate my requirements.txt . @nvie any update on this?\n@estan can you resolve the conflicts please?. probably to ignore all the blacklisted packages from PyPI. it's bad to depend on setuptools: https://github.com/pypa/setuptools/blob/master/CHANGES.rst#v3400. \n",
    "edoshor": "Happens with pip-review as well.\nTraceback (most recent call last):\n  File \"/home/edos/projects/accounts/venv/bin/pip-review\", line 269, in <module>\n    main()\n  File \"/home/edos/projects/accounts/venv/bin/pip-review\", line 229, in main\n    installed = list(get_installed_pkgs(local=args.local))\n  File \"/home/edos/projects/accounts/venv/bin/pip-review\", line 147, in get_installed_pkgs\n    name, version = line.split('==')\nValueError: need more than 1 value to unpack\npip freeze gives:\n```\n-f /home/edos/.pip/repo/\n...\n``\n. It happens to me when I usefind-links` in my global pip.conf\n. ",
    "simonluijk": "I am getting the same error in pip-review when pip freeze outputs -f file:///home/simon/.wheelhouse.\nSimply ignoring any line starting with '-f' solved the issue. I don't know about side effects though.\n. ",
    "nealmcb": "Why was this closed?  It still seems like it would be good to have a \"dry-run\" option!\n(Also note, it looks like the master branch is once again current....)\nCan you re-open this?  Thanks!\n. ",
    "shazow": "+1 to this, would be swell. :)\n. ",
    "josegonzalez": "I'd prefer using pypi rather than git installations. One issue is that certain pip versions are pure fail at installing from git (recently fixed actually, but broken since setuptools 1.2.1).\n. Which versions of pip do not currently work? (also pip is a POS, but that's a separate issue)\n. ",
    "datakid": "I think I probably ran an update in the very small window before the\nDjangonauts realised they had made a mistake - close and ignore.\nOn 10 February 2014 12:25, Bruno Reni\u00e9 notifications@github.com wrote:\n\nI don't get it. How is pip even able to find 1.7a2? I don't see it on\nhttps://pypi.python.org/simple/Django/\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/nvie/pip-tools/issues/76#issuecomment-34594543\n.\n\n\nFrom this perspective it is natural that anarchism be marked by\nspontaneity, differentiation, and experimentation that it be marked by an\nexpressed affinity with chaos, if chaos is understood to be what lies\noutside or beyond the dominant game or system. Because of the resistance to\ndefinition and categorisation, the anarchist principle has been variously\ninterpreted as, rather than an articulated position, \u201ca moral attitude, an\nemotional climate, or even a mood\u201d. This mood hangs in dramatic tension\nbetween utopian hope or dystopian nihilism...\nhttp://zuihitsu.org/godspeed-you-black-emperor-and-the-politics-of-chaos\n. I like it and use it regularly. \nSorry. I liked it, and used it regularly.\n. Note that the script above didn't work for me. \nThis did:\n``` bash\n!/bin/bash\nfor pkg in $( pip list --outdated | cut -d' ' -f 1 )\ndo\n    echo $pkg\n    echo \"update now? [yn]:\"\n    read answer\n    if [ \"$answer\" == \"y\" ]; then\n        pip install -U $pkg\n    fi\ndone\n```\n. ",
    "freder": "yes, it works now.\npip-review -v:\n[...]\nChecking for updates of pip-tools\n[...]\n. ",
    "lvh": "My coworker, @manisht, has hit this issue on a PR. At first, I thought that would just not be a valid thing for pip freeze to do by default, so it's a buggy config issue. find-links looks like it really only makes sense to supply by default for install, not for all commands that happen to have a --find-links flag. OTOH the contract for requirements files is pretty much \"a bunch of stuff you can pass to pip install\"...\nI guess pip-review shouldn't die on it though. Maybe catch it and warn?\n. ",
    "keimlink": "If you use pip to install Wheels then the recommended way to configure pip is to use --find-links to tell pip where to look for local available Wheels.\nIMO pip-review should simply ignore any option it does not understand. A package name must not start with a dash, so no package could be ignored by accident.\n. Awesome! :+1: Thanks a lot @khwilson and @nvie! \n. A workaround is to set CUSTOM_COMPILE_COMMAND to the desired value:\nconsole\nCUSTOM_COMPILE_COMMAND=\"pip-compile --output-file requirements.txt setup.py\" pip-compile. ",
    "Midnighter": "If anyone still hits this problem, an easy solution is to change your pip.conf slightly. Instead of having find-links in the global section, move it to the relevant subsections. Your config might then look like this:\n[global]\nuse-wheel = true\nwheel-dir = /your/home/.pip/wheelhouse\n[install]\nfind-links = /your/home/.pip/wheelhouse\n[wheel]\nfind-links = /your/home/.pip/wheelhouse\n. ",
    "ryanscott": "+1 for Python 3.4 support. I really want to use this for a new project we're doing in 3.4.  \nI'm still learning python, so I'm not totally sure of how to even hack this to work. does anyone have a fix in a fork somewhere, or even a quick and dirty solution to get it working temporarily?\n. ",
    "davidfraser": "I have attached my extremely awkward wrapper of piptools/scripts/compile.py.\nThis will find a comment with # fork: original_spec url\nIt will then run pip-compile with original_spec, and then substitute the url into the resulting output\nThis works for us but was a horrible workaround :)\npip_compile_with_forks.txt\n. ",
    "davidovich": "Is this still something relevant in the latest release (1.8.1 as I write this) and/or workflow ?. Closing old issue. This issue references obsolete/deleted code. Please reopen if you feel this is still an issue.. This issue was opened at a time where there was a package_manager.py file in pip-tools. I will now close. If you think that this is still a problem, feel free to reopen. . pip-dump is no longer present in latest releases. If you feel that this needs to be addressed, please feel free to reopen.. This issue is really old, will close. If you feel that the issue is still present in the latest version, don't hesitate to reopen.. This now works as expected:\necho \"GitPython==0.3.2.rc1\" > requirements.in\npip-compile\nNote that --pre can also be used to allow pre-releases.\nReopen if you feel that this is still a problem.. Closing as old, and not using cram anymore.. This was implemented in PR #476 to fix a new bug introduced in 1.8.1. Closing.. #208 is an attempt to fix this in a more general way (by supporting pip.conf files). Please feel free to comment.\nAlthough both approaches might be needed (and combined) if command line support is wanted.\n. Fixed in #283\nClosing.. closed by #460. Thank you!\n. As of 1.4 this is fixed and verified.\nsee https://github.com/nvie/pip-tools/commit/3a439045c71851151c51953b79f5994d3a12d9fe\n@nvie this issue can be closed.\n. Unfortunately, this breaks python3 (others have had this issue, as mentioned in the bug ticket reference #209):\n...\n  File \"f:\\git\\pip-tools\\piptools\\scripts\\compile.py\", line 124, in cli\n    results = resolver.resolve()\n  File \"f:\\git\\pip-tools\\piptools\\resolver.py\", line 68, in resolve\n    os.environ['PIP_EXISTS_ACTION'] = b'i'  # ignore existing packages\n  File \"F:\\Python35-32\\lib\\os.py\", line 686, in __setitem__\n    value = self.encodevalue(value)\n  File \"F:\\Python35-32\\lib\\os.py\", line 740, in check_str\n    raise TypeError(\"str expected, not %s\" % type(value).__name__)\nTypeError: str expected, not bytes\nAs an alternative, I may be able to set the PIP_EXISTS_ACTION option through the new pip options support introduced in #208, which defers the setting to pip, which does not use unicode_literals. But to me this seems overkill.\n. Is this still happening ?. Thank you, I will now close, don't hesitate to open if it happens again!. Closing as it has been implemented.. click is already a sub dependency of pip-tools. I suppose this is ok for now. Closing.. @rpkilby I don't quite get what you mean. The issue states a repro step where click is mentioned in requirements.in, and a run of pip-compile and pip-sync is run. I have observed locally that click is indeed installed. \nThis was an issue regarding pip-tools, but you are saying that you do not have pip-tools installed.\nI am trying to tidy up the project somewhat, so I may have closed prematurely. But I still have very little to run on for this over one year old issue. . No problem!. Is this still happening with newer pip-tools?. Pip (which does the heavy lifting for pip-tools) should already take care of installing sub dependencies first: see pip installation order.\nIs this issue still relevant with a more modern pip ?\n. Thank you. Closing, by assuming that the issue is resolved by the pip sub-dependency management change. . Closed by #468 . @nvie The fix was implemented upstream and is available in 16.1.0.\nThanks!\n. closing, please reopen if it is still an issue.. Closing as duplicate to #204, let's continue there.. I think this is safe to close because of #418 being merged.. @nvie This PR addresses problems with pip-tools not respecting values like trusted-hosts in the pip.{conf,ini}.\n. @nvie I really agree with your private implementation usage comment, but short of maintaining a duplicated implementation, I thought this was the least problematic of the two.\nThat said, as it is pip doing the heavy lifting, we must comply to it's configuration pre-conditions as much as possible... \nThanks for the fast merge.\n. I had tried to make a PR on this very issue, and @nvie kindly pointed me to report upstream, which I did. Is there an automatic vendoring procedure that would have brought that in pip-tools ?\n. Closing as non reproducible. Reopen if you think this is closed by error.. Closing as implemented by #283. Reopen if you feel that recent pip-tools does not support this.. @GMLudo is this still a problem ? It may be the same issue as #369, which is about a bug when setup_requires is used to install local eggs.. @IlyaSemenov This is fixed in the upcoming 1.8.1 (currently as an rc3) \nI cannot reproduce with 1.8.1rc3. \nCan you verify that it also fixes your issue? Thanks.. Thanks, closing then. . This should be supported now if you use a recent version of pip. In fact, it is in pip 8.0 which is the lowest version supported by pip-tools. See: https://pip.pypa.io/en/stable/news/. I think this is a duplicate of #466, let's continue there.. Can you verify if this is still the case in 1.8.1 ? I think I may have fixed your issue in #450. Thanks.. Will close in favor of #427 which fixes the duplication problem related to normalization. Thanks for having a stab at this!. >  That a -r in an *.in file is translated to mean \"use existing versions in that file (or compiled version thereof), and write everything else to output\".\nI think this would bring value, and keep existing processing. The only change would be in the file collection phase before invoking the resolver. I believe this is not too hard to implement, I am open to a PR for this functionality (with relevant tests).\n. This seems to be a duplicate of #376. L'ets close this and continue in #376. This is a use case implemented in PR #213, but has not gained any traction. It is a bit outdated, but the actualisation work shouldn't be hard.. closed by #485. I am also not certain if we need to mandate (or absolutely recommend) the use of setup.py.\nrequirements.in are sometimes an additionnal set of requirements related to the environment, but not necessarily needed by the package build as described by setup.py. It's not one or the other. @dstufft describes it well: if you need to install a webserver, you will not put that into the setup.py, as setup.py should describe what is needed by the package (be it a library or an application). Put in other words, I see the install_requires param as describing all the imports that will be made directly by running/using the package.\nI see this PR more helpful for libraries which typically do not use the requirements.in. It would help in reproducibility in build systems, but then again, this has drawbacks: it would also mask some errors that users of the library will have when using it unpinned (using the pinned requirements.txt for builds will isolate the more general open version usage, which can make subtle variations in sub dependencies on each install). Id rather catch a sub dependency breaking this library than let it be discovered by the user.. I think this has waited enough, and I am in favor of merging. Let's put this in the 1.9 release (there are two or three bug fixes I would like to land before incorporating features.\n(in retrospect, I was never against this ;-)) I think we have done enough bikeshedding.\nCaveat: The only thing missing now is tests. We must strive to augment coverage as this project has shared maintainership. . Thanks to all. This work was merged in #418, closing.. This should be fixed with newer versions of setuptools (Update setuptools to 28.7.1 and higher). Closing, feel free to reopen if still present.. Thanks!. Closing as per https://github.com/jazzband/pip-tools/pull/485. I have tried to reproduce, without error. Here is my setup:\nWindows 7\npip==9.0.1\npip-tools==1.8.0\n. @mitbal You might try to recreate your virtualenv, as it has possibly captured an older version of setuptools which fails to parse the environment markers.. Will go without tests right now, because we need to make a 1.8.1 release, and manual tests are conclusive.. Thanks ! for this @jonafato . fixes #376 . Fixed in #457 . Thanks for the report!. pip will honour the flags output by pip-compile, including --trusted-host. This PR is not needed anymore since pip 8.0 can read the flag.\nSee: https://pip.pypa.io/en/stable/news/. @barrywhart Yes, you are right, pip-compile has a cache, and naming seems to be dependent on this cache...\nUsing the --rebuild flag outputs a case-consistent (lowercased and normalized) set of requirements. I believe a cache flush should fix these problems and future ones because the latest versions of pip do a better package name normalizing.\npip-compile --rebuild. @jdufresne while there is still some mystery about name transformations throughout the download/caching of packages, I think that lower casing is the least surprising. \nThe alternative is to have package names change case on odd compilation runs, which is more annoying. \nThe problem exists in the tooling around pip, and I believe there is no general API to normalize a package name. Note that this is not exclusive to pip-tools, the pipenv project has had a lot or problems with exactly this situation. . @barrywhart this issue is resolved by #452, and merged in 1.8.1rc already. Have you had time to explore the case preserving scenario ?\nI think stabilizing compilation output brings more value right now, and we could revisit later?. @IlyaSemenov I appreciate the thoroughness of the analyse of my answer, but the problem is a compound one and is hard to pin down right now, given that I do not have all the answers concerning interactions in the pip-tools, pip and setuptools code bases. All these can introduce naming variations.\n\nI think that lower casing is the least surprising \nHow so? The least surprising is to have the original, package maintainer supplied capitalization. Any mangling whatsoever is more surprising.\n\nFrom the compile viewpoint, it is less surprising that two runs produce a stable output. I agree that it can be surprising that the case is not preserved, and that the ideal case is to maintain it.  \n\nNo, this is a false dilemma. The real alternative is to keep package names always in the original, package maintainer supplied capitalization.\n\nAgreed. The current observable implementation is not exact, and results in varying case in the output. Comments on this issue seem to agree that lowercasing is an acceptable solution which is why #452 was merged in.\n\nI believe it's a good compromise between minimizing efforts and maximizing result.\n\nI also think so.. @jdufresne I monitored issues surrounding proper_case (https://github.com/kennethreitz/pipenv/search?q=proper_case&type=Issues&utf8=%E2%9C%93) in the pipenv project, and that enforced my current thought that it is not a simple fix.\n@all Are we ok with the lowercasing?. @mlfz Clearing the cache is already possible: \npip-compile --rebuild\nor \npip-compile -r. @jdufresne I am really not convinced that this needs to happen at output time. I think the analysis should take place at resllolve time. What is your take on this?\n. @jdufresne could you add a test to show that appdirs, packaging, etc is not added to the compiled requirements due to being unsafe?. Yes that is what I had in mind.\nI will try to merge tomorrow, or next Monday.. Thanks @jdufresne !. @AndreasBackx There is a kind of check at startup, but for the moment it is a lower bound at pip 8.0. Unfortunately, pip does not provide a programmatic API, but pip-tools depends on it to do it's work, so it could theoretically break at a future pip version. \nsee assert_compatible_pip_version() in utils.py (https://github.com/jazzband/pip-tools/blob/master/piptools/utils.py#L27). Closing. We will make sure to support latest pip.. @jonafato I beleive your guess is correct, setuptools is removed in piptools/writer.py, which is too late to determine dependencies brought in by the unsafe setuptools.\nNow that setuptools has external dependencies, we would need to bring the analysis early on in the resolving stage, and populate a kind of safety_ignore set. Other legitimate constraints could also contribute to remove an element from the safety_ignore, if they are needed by top-level or unrelated packages' sub-dependencies.. @nvie Hi, you must be very solicited, but I would like to add my voice to this request. Knowing what is the way forward will influence how and if we use pip-tools in the future. \nWe use pip-tools as a dependency validator to ensure user serviced venv creation is kosher. It is a central part of our validation process.\nAt my knowlegde, pip-tools is the only standalone dependency validator available right now (this is excluding conda in which you have to buy-in). Until pip grows one (which is a long standing pip issue), I beleive pip-tools remains a viable and very useful solution to sane python package management.\nThat being said, I would be very sad to see it go out of maintenance. I would also like to thank you for starting and steering the project.. I have reached out and had a very warm contact with @nvie by mail and he has transferred ownership to jazzband, where we it is possible to share maintainership of the project. https://jazzband.co/ is a really nice initiative. \n@jezdez beat me to the point! \nI am really eager to see how this turns out for a so popular project!. Seems this is working out, closing.. Please note that #378 was NOT the feature that added the regression. I believe this bug was present since the introduction of groupby().\nSorry for the wrong deduction.. Fixes #384. The project has moved to the jazzband organization and it seems that past builds have not followed from nvie. \nPushing another commit (or amending the last commit) should trigger a build in the new organization. . https://travis-ci.org/jazzband/pip-tools/builds/199895230\nNote however that the \"This branch has no conflicts with the base branch\" message is flawed here. Rebasing on master should help.. Thanks @klaplong !. pip-sync works in tandem with pip-compile which creates a pinned version of your dependencies and their sub-dependencies.\nYou need to run pip-compile first on a requirements.in file. In your example, you would have ipython in your requirements.in, then run pip-compile.\npip-sync is used to update an existing virtualenv to match what would have appeared in a previous pip-compile invocation.\nClosing. Do not hesitate to reopen if you still need info.. Thanks!. @jmbowman Hi, could you have a look at this PR, I believe you were the original author which introduced the conditional. \nThis happens on 1.8.1rc1, but is probably present in earlier versions.\nThe problem is that the req.specs and req.specifier are sometimes not equivalent. Here is a snapshot of a RequirementSummary() construction:\nself.key: u'sqlalchemy'\nstr(req.specifier): '!=0.9.5,<2.0.0,==1.0.8,>=0.7.8,>=1.0.0'\nsrt(req.specs): \"[(u'!=', u'0.9.5'), (u'>=', u'0.7.8')]\"\nClearly, these are not equivalent, and produce unstable resolving.\nWhat do you think? I would really like to ship a 1.8.1 ASAP.. Also tested manually on pip 8.0, 8.1.1 and 9.0.1. You made me doubt this for a moment. But the req does have a specifier and a specs, it is a pkg_resources.Requirement.parse(req) object: https://github.com/pypa/pip/blob/8.0.0/pip/_vendor/pkg_resources/init.py#L2990-L2993\nWhen I remove the conditional, the resolver stabilises much faster, and does not break on my example requirement topology (which is admittedly a local set, but is bound to happen again in the wild).\nI have setup a local testing rig with the 1.8.1rc1 version. This minimal setup exhibits the breakage:\nUpdate: I have committed a test to encode this test in tests/test_cli.py\ncd pip-tools\nmkdir -p localtests/fake_package\ncd localtests\nsave the following setup.py to fake_package/setup.py\n```\nfrom setuptools import setup\nsetup(\n    name='fake_with_deps',\n    version=0.1,\n    install_requires=[\n        \"python-dateutil>=2.4.2,<2.5\",\n        \"colorama<0.4.0,>=0.3.7\",\n        \"cornice<1.1,>=1.0.0\",\n        \"enum34<1.1,>=1.0.4\",\n        \"functools32<3.3,>=3.2.3\",\n        \"futures<3.1,>=3.0.3\",\n        \"ipaddress<1.1,>=1.0.16\",\n        \"jsonschema<3.0,>=2.4.0\",\n        \"psutil<3.2,>=3.1.1\",\n        \"pyramid<1.6,>=1.5.7\",\n        \"pyzmq<14.8,>=14.7.0\",\n        \"SQLAlchemy==1.0.8\",\n        \"python-memcached==1.57\",\n        \"xmltodict<=0.11,>=0.4.6\",\n        ],\n)\nand invoke compile\nrm -f fake_package/fake_with_deps-0.1-py2-none-any.whl && pip wheel --no-deps -w fake_package/ ./fake_package/ && pip-compile --rebuild -v -n -f fake_package/\n```\nNote that this PR makes the resolver converge very fast on this example (5 rounds). . @jmbowman Thanks for the info, I'll be removing the duplicated condition.. @barrywhart can you squash your commits ?\nI think we can add this to the next feature release (1.9).. I think this will be good for the 1.9 release.. Thanks @barrywhart !. Thanks @AndreLouisCaron !. We can focus on 1.8.1 and 1.9 for now, maybe open another ticket for more general ideas and directions we would like to take.\n1.8.1 should bring a bit more stability in the resolver. I do not want to add other non bug fixing to this release. I think the rc3 will probably materialize next week if I don't hear anything more preventing it's release.\n1.9 should bring small changes and features, there are a lot of good PRs, but some lack tests, and because of the new collaborative maintainership, I think it is a deal breaker for those. I am not sure what PRs are more important than others, I will survey the number of involvement and issue/PR participation and probably merge the best ones (with tests :-)).\nAnybody wanting to be involved should open a request with jazzband, that will let them add Labels and close old tickets. There are really old issues that don't even apply anymore.\nI am on vacation for the next 8 days, so I will not be able to participate that much.. @underyx I'm all for a 1.9 release, but I do not have alot of metrics for finalizing. 1.9 has a lot of new features and I wouldn't want to break anybody. I will create a rc1 for you to test. \nIn the meantime, does installing on master fix your use cases ?. @myw Thanks for your suggestion, but I still feel this as being a 1.9, and as you say, the major feature is to support setup.py install_requires as a requirements.in. \nNo new code entered without tests, so this brings confidence a bit up. I will be making a 1.9rc1 today (if time permits), which should also marinate a little while.. 1.9 is out, will close this. Thanks everyone!. I'm afraid this differs too much from pep440 and adds a parallel system (albeit semver) that needs to be maintained. There is some notation available to attain the same functionality. Please see https://www.python.org/dev/peps/pep-0440/#version-specifiers.. Does this happen in 1.8.0 ?. Should be fixed by #476. . I'm not sure that we are ready to take on (support and maintain) unreleased python versions, so if you keep only the latest release we could gladly merge.. @SylvainDe Since we depend on pip internals, I would greatly value a variant of your PR that could setup running travis on pip master. That way, we could know in advance that pip-tools is about to fail (upon next pip release). What do you think ?. Ok, I will merge if we test only 3.6.\nThanks!. Thanks!. Thanks for this @majuscule . Yes I think test_data is a good compromise. . Thanks @suutari. @vphilippon yes, you are right, appending .txt to a filename missing extension is the simple solution.. Thanks @derek-miller !. I have no problem with the fix, it's just that it works very well without the memoization... Would you mind taking #485 for a spin and see if your use case is broken ?. Thanks @derek-miller !. I believe this is a duplicate of #466, which is fixed by upcoming merge of #476.. Can you check if this is still happening on 1.8.2 ? Thanks.. I like the relatively simple approach of caching the dependency, and it fixes in an unobtrusive manner the dependencies of an editable. \nCan you put up a test case so we can catch this in the future (preferably in a commit of the failing situation and then the commit fix)?\nFor the test case, you could probably use -e on the fake_package in tests/fixtures.. @mattlong For your info, I would like to make a 1.8.2 release to fix this regression. The only thing missing is tests. :-). @mattlong outstanding! thank you!\nfixes #466. Thanks I have reproduced. It only happens on second compile run, when an output requirement already exists.. This should be fixed when #461 is merged. Pip also went this way because windows is very keen on holding file handles more than we would deem necessary.. Closing, will be fixed by the upcoming 1.9.0. In the meantime, you can try installing the master branch and use the python -m piptools compile invocation. Do not hesitate to reopen if need be.. I believe this is a duplicate from #466. Let's continue there.. @majuscule can you rebase on master ? I added Windows testing, and I suspect that the tempfile.NamedTemporaryFile() usage will break (because of the lack of delete=False parameter). On Windows, the file is deleted at context manager exit, breaking subsequent processing.. Good, thanks !\n@majuscule In fact, if you have a little time, I want one bugfix for the 1.8.2 release, before 1.9, but @mattlong has not responded to add tests on https://github.com/jazzband/pip-tools/pull/476. Are you up for it ? :-). Thanks @tysonclugg and @majuscule . This issue should be fixed in the (unreleased) 1.9.0. Can you check that your use case is covered on the master branch? Thanks.\nsee #477. OK, this was introduced by 5bf5b97db36f7ade80cbce40b4de2175886f7100\nWill fix and add a test.. Thanks @derek-miller !. I am not sure that we want to add coupling to another library. I believe not every package will adhere to the safety-db, hence making this feature only applicable on a subset of packages.\nWhy not use the two tools in sequence and weave them by the shell ?\nI don't think pip-tools should favor any external commercial service. \nOthers may have differing ideas, but I am -1 on this.. Because the db is updated by hand (as I understand, the package maintainer must opt-in, hence adhere), it cannot be perfect, hence giving an --upgrade-insecure option the possibility to lie for packages not part of the db.\nIf package A is insecure, but not part of the db, pip-compile with the option flag will do nothing on that package, but will suggest to the user that their packages are ok.\nIn other words, there is too much chance of giving a false security assessment. It cannot be generalized reliably.\nLeaving the responsibility to the user to use safety for his curated packages is perfectly fine, but I don't think this should be baked in pip-tools as a general option.\n. Closing as not general. . I'm not certain that the default would be to include test_requires and setup_requires.\nsetup_requires list dependencies that is needed at install time only (and is managed solely by setuptools)\ntest_requires are not related to the runtime of the application.. Thanks for your contribution. Can you add some explicit tests for the new path ?. Thanks @craigds. I can see a use fo reqwire, but I am really not sure pip-tools should become an advertiser for it. Thanks for trying :-). can you try upgrading setuptools in your venv and see if that helps ?. I believe the sub-dependencies are pinned incorrectly for this package.\nIf I run:\necho google-cloud | pip-compile -v -U -o req.txt - | grep google-cloud-logging\nWe can see the culprit in the output:\ngoogle-cloud-error-reporting==0.24.1 requires gapic-google-cloud-error-reporting-v1beta1<0.16dev,>=0.15.0, google-cloud-core<0.25dev,>=0.24.0, google-cloud-logging<2.0dev,>=1.0.0\ngoogle-cloud-error-reporting==0.24.1 requires google-cloud-logging<2.0dev,>=1.0.0. This constraint cannot be satisfied, so there is currently no solution.\nA workaround would be to pin arificially the dependency that requires google-cloud-logging>=1.0.0:\necho -e 'google-cloud\\ngoogle-cloud-error-reporting<=0.24' | pip-compile -v -U -o req.txt -. Closing as not a defect in pip-tools (but it is probably a defect of google-cloud package versioning). Pip does not do constraint validation, so it is possible to install conflicting packages. These conflicts may or may not allow correct runtime behaviour, but pip-tools will fail because the constraints are the only contract that it can follow. It is telling you that you have an invalid configuration (according to the authors of the sum of sub packages).. This PR is missing some tests.. I though about this some more, and tests here are hard to write, and would not give us more confidence that we indeed need pkg_resources (provided by setuptools). Having setuptools as a install_requires will almost always be ensured by the environment. Let's wave the tests for this.\nThanks for the contribution!. I believe the initial implementation was correct in removing the unsafe packages at runtime. This PR introduces a similar kind of logic in the Output writer. I am on @jdufresne side here, and I am not sure there is a regression. Can you write a test that demonstrates the failure?. The spirit of --allow-unsafe is to control if unsafe sub-dependencies are output to the requirements.txt, not to filter a user input which is the user desired state of wanted dependencies. requirements.in state what the user wants. The --allow-unsafe flag is to remove unwanted side effects of sub-dependencies which are unsafe. \nEven unsafe here is a bit hard to define. \nBut if the said unsafe requirements are in the input which YOU control in the requirements.in, pip-tools cannot decide to remove them for you.\nI used this setup to convince myself there was no bug here. The following creates a fake package with only setuptools as sub-dependency.\nmktmpenv\npip install pip-tools\nmkdir pkg && cd pkg\ncat <<EOF > setup.py\nfrom setuptools import setup\nsetup(install_requires=['setuptools'])\nEOF\ncd ..\necho \"-e pkg\" | pip-compile -n -o r.txt -\nThe result:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file r.t -\n\n-e file:///Users/david/.virtualenvs/tmp-ef044366e6edf64/pkg\nDry-run, so nothing updated.\n```\nWith --allow-unsafe:\n```\ndavid$ echo \"-e pkg\" | pip-compile --allow-unsafe -n -o r.t -\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file r.t -\n\n-e file:///Users/david/.virtualenvs/tmp-ef044366e6edf64/pkg\nappdirs==1.4.3            # via setuptools\npackaging==16.8           # via setuptools\npyparsing==2.2.0          # via packaging\nsix==1.10.0               # via packaging, setuptools\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools==35.0.2\n```\n. @dschaller You are the only one saying the feedback is not critical. I feel the overall gist of this is that you want the change in, without regards to style or existing code. \nI have made comments and tried to understand the nature of what this PR trying to fix something that I am still not convinced is broken, but they seem to be ignored.\nThe response I got from the example code was that I didn't read a previous PR. I understand what that PR brought to the table, but I believe we are now in a better position than at that time.\nThe problem is that if you put something in the input, it is not for pip-tools to decide to remove that wanted state. If you want to remove pip from the output, don't put it in the input as a wanted requirement in the first place. I sill fail to understand your use case.\nI can perfectly see that I would like to put pip in the requirements.txt AND not wanting to have unsafe dependencies end up in the compiled requirements. This PR would break that.\nFor example, with the same setup as above, imaging I would like to have pip, so with this PR, I would need to add --allow-unsafe, but at the same time, I would not want appdirs, packaging, etc:\n```\ndavid$ echo -e \"-e pkg\\npip\" | pip-compile --allow-unsafe -n -o r.t -\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file r.t -\n\n-e file:///Users/david/.virtualenvs/tmp-ef044366e6edf64/pkg\nappdirs==1.4.3            # via setuptools\npackaging==16.8           # via setuptools\npyparsing==2.2.0          # via packaging\nsix==1.10.0               # via packaging, setuptools\nThe following packages are considered to be unsafe in a requirements file:\npip==9.0.1\nsetuptools==35.0.2\n```\n. This makes sense, and is easy to test. Look at the tests/test_cli.py here. You just need to add a test that uses the quiet flag and evaluate if there is any output.. @ned2 this is by design since version 1.6.\nAs you say in the issue, if you want to explicitly upgrade, use the -U option. This is not a workaround.. I don't see the value this PR adds. Adding a automatic checker would bring value for future contributions, but aesthetic changes like these are counter productive, and produce unnecessary conflicts for every pending PR.. @dschaller I really appreciate the style of this new attempt, but I am still very not convinced that pip-tools should go down this route (even if a past PR made it in, that does not mean that was a wanted direction).\nAside from a better implementation (IMHO), the main question resides from the last PR, why do you absolutely need to put a known unsafe in your requirements.in and not want it in the compiled requirements? Why just not write it in the first place ?. Thanks @jdufresne. If you unpin and use only top level dependencies in your requirements.in, do you still see this error ?\notherwise, to debug this, you can pipe the output of a pip-compile -v into grep to see which package wants the conflicting versions:\n```\npip-compile -v | grep -i pysaml2. I think this is a source to target error. This seems to be for the pip-review repo, which originated from pip-tools. Closing.. Thanks @jonafato. Good call @vphilippon :-)\nLe mar. 19 sept. 2017 \u00e0 18:59, Kenneth Reitz notifications@github.com a\n\u00e9crit :\n\nthanks for the help, @vphilippon https://github.com/vphilippon!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/562#issuecomment-330604806,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABX6cvnrbAOnHReveNl9vyg5uU1h_1tBks5sj_MHgaJpZM4PbjwT\n.\n. @tuukkamustonen What prevents you from outputing frozen requirements per environment and use that as an input to the pip install (with the tox matrix reference) (did not test) ?:\n\ncommands = pip install -r requirements_{env}.txt. I wholeheartedly agree with you @vphilippon. This is a great idea,\nconsidering that pip 10 will probably offer an alternative, and that the\nexternal interfaces to pypi will probably stay compatible with pip9, I\nthink absorbing pip will let pip-tools continue to be relevant by itself\nfor the phase out period.\nLe mer. 22 nov. 2017 \u00e0 18:18, Vincent Philippon notifications@github.com\na \u00e9crit :\n\nAllright, so the best short-medium term idea I have right now would be to\nvendor pip 9.0.1 as a whole and use it for the whole dependency resolution\npart.\n(Bonus point: I'm thinking about vendoring pretty much any dependency to\nstop having issues with pip-sync risking breaking itself.)\nI highly doubt anyone will have the time to redesign all of this to work\nwith pip 10, and I'm not even touching the subject of retro-compatibility.\nAlso the risks of breaking stuff are...\"fairly high\" (cough it's going\nto be a firetrucking mess cough)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/580#issuecomment-346498696,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABX6ckXOkFv_8VlYBUSAGLMuRRY-eDxTks5s5KvggaJpZM4QA6-4\n.\n. > As a user, I would be happy if pip-tools had a strict dependency on pip 9.\n\nAs a typical setup, pip is only installed once in the system environment (aside from virtualenvs of course, but talking from a boostapping view). When the user will have upgraded to pip10, it will break pip-tools for that user. If we vendor pip and that user gets the new pip-tools version, it will isolate him from any other change. \nRight now, pip-tools has many workarounds and special cases to support varying pip versions, vendoring will help eliminate those. \n. LGTM. Wow! at last!\nLGTM. LGTM . Much better ;-) LGTM. Thanks for the issue. We will welcome any PR that can fix this problem that you are having, in the pure open-source fashion :-). @vphilippon\n\nIf they deploy on multiple environments from a single repo, then the requirements.txt shouldn't be commited to source control.\n\nI'm not sure you should cristallise in doc not commiting requirements.txt to source control. Not doing so opens the door to non deterministic builds (because a compile phase is needed to re-create).\nIf you want to talk about this, I would instead mention that the files on different platforms should have different annotation (a platform suffix) corresponding to the source environment. That way you keep deterministic behavior for the different platforms.  . pip-tools is just a resolver (plus the pip-sync companion). It all depends on your needs, as pip-tools can be seen as the plumbing behind what pipenv offers you. If you just need to manage the result of a resolver, then bare pip-tools might be ok. We use it this way to provide a reproductible deployment of dependencies. If you want all the developper tooling and project oriented virtualenv management (and more), then pipenv would be a better fit.  \nAlso, pipenv has an army of developers and is being actively developed. Pip-tools is slower in its development.. Yes, this is exactly why we do it by leaving the file around in pip-compile.. This test is made to explicitly validate that case is NOT changed. (test is named test_format_requirement_annotation_case_sensitive). Are we sure this is ok ?. Yes that makes sense. \nGoing forward, I think adding and maintaining tests is really important, particularly in this new \"distributed\" maintainership model (jazzband) that pip-tools is using. I know it is more work, but it is better to spend time locally than distribute the debugging effort on the internet :-). \nMaybe this should be included in the 1.8.1 release I am preparing.  A release candidate 1 is out already. . To avoid confusion, the --no flag should provide symmetry to the positive version: --no-emit-trusted-host.\n. I don't see a problem for this right now, but I am not the authority on pip-tools code either. . Thanks for your contribution!\nAs it is, this has the possibility of changing the index-url order, which is a property that needs to be maintained.\nAs a user, I might use index urls to explicitly set desired index usage ( A -> B -> C ).\nSee http://stackoverflow.com/a/39835527/28275 for a clean reference implementation that keeps order.\n. This would need delete=False, especially on Windows.. I believe this is not related to the issue you are trying to fix. Please open another PR for the package extras related modification, and remove this commit.. The merged PR #468, which does a similar fix for index-urls unicity, uses the OrderedDict idiom to get unique entries, could you reuse that?. pkg-resources does not seem to be a pypi package, I believe this should be removed from the PR because it is also irrelevant to the change.. I think that there are Windows idiosyncrasies that would be taken care of if you used pathname2url (like in the original pip code you pointed at: \nurllib_parse.urljoin('file:', urllib_request.pathname2url(path))\nthis should also help the failing Windows build.. perfect. I'm not certain that cwd is a correct base here. I could call pip-compile outside of the directory which contains the requirement file, and that would leave us with a non installable compiled requirements.txt. Am I mistaken in thinking that the base should always be relative to the directory of the input file ?. I don't understand this refactoring of correct code.\nFor what it's worth, pytest now doesn't discriminate between a yield and an ordinary fixture. @fixture is only needed.. As this is managed by click, this would not be necessary (testing the long option). . Keep the named arguments here.. reverse the if logic here, use the positive first.. Why so much mocking ? I think it is best to use the real implementation as much as possible in order to test the implementation and not the mocks.. Is the 'marker' here a real package? deduced from the USAFE_PACKAGES dependencies? or is it a test marker, something that you make up for your test ?. Because it makes the contract clear and not hidden in the function body.. I would try to take a more neutral tone, as this seems to be accusing. How about: \nThere are incompatible versions in solved versions of dependencies\n. We know which pypi repository was tried, I would add it here. If it is an internal server, this will help diagnose errors vs public pypi. Small nit:\nIs the $(pypi) repo reachable?. With this change the no_index flag is inhibited. Is this wanted?. This is not quite the same intent. I see that you are reconstructing the parameter invocation based upon the index_urls, but that exposes too much of the implementation detail. I don't quite grasp why you are doing this.. I believe this will be stable now that we use vendored pip. same: I believe this will be stable now that we use vendored pip. ok\n. ok. I agree. Otherwise, LGTM.. ",
    "ashwinvis": "This issue should be closed. Specifying the path to tarball as following works\nini\n-f  https://example.com/my-package-0.1.tar.gz. Just end the link with .tar.gz. I managed to get it working with:\n-f https://software.ecmwf.int/wiki/download/attachments/56664858/ecmwf-api-client-python.tgz. @blueyed You are right. It ignores the line. I had several packages in the requirements.in, and when I saw no errors I assumed it installed ecmwf-api-client. It did not :-/. ",
    "cjerdonek": "As a simpler case, have you tried a zip / tar.gz with a local file while using the file scheme (and with no flags)? For example:file:///<abs_path>/0.6.1.1.zip#egg=<PackageName>. Sounds good, thanks!\n. Thanks! Do you know what release this functionality was or will be added\nin, and if there was an associated PR fixing it?\nOn Wed, May 2, 2018 at 4:52 AM Tyson Clugg notifications@github.com wrote:\n\nThis now works if you specify the output file as -.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/275#issuecomment-385952077,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAVt7twEQ3PbygdREWGt8b_Ryui3smR_ks5tuZ3wgaJpZM4G48Ws\n.\n. FYI, I just checked, and I don't see any evidence in the code this functionality was ever added. I also tried running with -o - using version 2.0.1, and it created a file with name -. So it looks like this issue shouldn't be closed.. Note that it's possible this is due in part to a similar bug / behavior in pip itself. If I run the following with no internet:\n\n$ pip install --retries 0 MyPackage > temp.txt\nthen pip's stderr output looks like this--\nCould not find a version that satisfies the requirement MyPackage (from versions: )\nNo matching distribution found for MyPackage\nWithout the retries argument, you will at least see connection errors prior to this. As with this report, it would be better if pip's final error message says something about the inability to connect.\n. FYI, I filed the analogous issue for pip here: https://github.com/pypa/pip/issues/3642\nIt's possible that if pip fixes that issue the right way, then this bug will also go away (at least for the versions of pip where that issue is fixed).\n. @nvie Out of curiosity, have you considered articulating to the pip folks what needs to be part of their public API so that pip-tools can have forward compatibility?\n. @kiorky Do you have a sense for whether the information needed by pip-tools and salt can sensibly be made available through pip's stable command-line API to prevent issues like this in the future? Or through some other stable interface?\n. Worked for me. Thanks! \ud83c\udf89 \n. Issue #275 is also related.\n. Issue #353 is related in that fixing this issue could very well wind up fixing that one. But, for example, it's possible that not all of pip-compile --verbose's output uses the LogContext class (which is what this issue covers). We'd want to check that separately as part of issue #353, and maybe even also have a different type of test case (e.g. an end-to-end test using the CLI).. Also, this seems to be where pip-tools' vendored pip would show this extra info: https://github.com/jazzband/pip-tools/blob/3d9e0ecffd2fffbf99ef638300996a61825cdd03/piptools/_vendored/pip/utils/init.py#L696-L707. Just to chime in, I'm a happy long-time pip-tools user. Probably because of pip-tools, I've never felt a need to try pipenv. pip + pip-tools does what I need.. By the way, @nvie, as a pip maintainer and fan and user of pip-tools, I'd love to know if there is any low-hanging fruit in terms of possible changes to pip that could help make pip-tools more robust in its reliance on pip.. Instead of removing logging, shouldn't the log be going to stderr like most logging does? If it goes to stderr, it wouldn't be interfering with the stdout issue.\n. Sure, I can make an issue. (I would need to confirm first that log does indeed go to stdout.)\n. Done here: https://github.com/nvie/pip-tools/issues/362\n. ",
    "szepeviktor": "I was looking for a tool to send myself a pip update report weekly.\nDo you think pip-review is a good choice?\nHave you seen yolk's commands?\n. I'd like to exclude python packages installed by apt because upgrading by pip is dangerous: apt may downgrade a fresh version upon apt-get upgrade\n. Oh! Thank you.\nCould you get it merged here?\n. @nvie Would you please merge this?\n. \"pip-review2\" told me\n```\npip-review2\nEverything up-to-date\n```\n. Can pip-tools display the package directories to have them filtered by a shell script?\n. If you take a look at @farukuzun 's commit, you can see how it is done in python.\nI think pip-tools cannot display packages dirs to parse it with a shell script.\n. That is way it would be nice to merge.\n. Could you give me an example line of bash how to filter out Debian (dist-packages) packages?\n. It lists e.g. fail2ban wich is a debian package:\nCould not find any downloads that satisfy the requirement fail2ban\nSome externally hosted files were ignored (use --allow-external fail2ban to allow).\n. Even then\n. Yes this way it would work.\nI think is much like a work-around.\nSo I will use that fork above.\n. Thank you for your replies.\n. Sorry! I meant for detecting Debian OS packages (dist-packages dir)\n. *correction again\nDebian wheezy 64 bit + python 2.7.3\nthere is no problem on squeeze 32 bit + python 2.6.6\n. The command:\npip install pip-tools\n. In the bug report you mentioned, they say it is a pip-tools bug. Is it?\n. Also on a stock wheezy python-pip version 1.1-3 there is no problem.\n. These Debian packages are mostly backported - I'd like to run the newest pip on wheezy\npython-colorama 0.2.4-1.1\npython-distlib 0.1.8-1~bpo70+1\npython-html5lib 0.95-1\npython-pkg-resources 0.6.24-1\npython-requests 2.0.0-1~bpo70+1\npython-setuptools (>= 0.6c1) 0.6.24-1\npython-six 1.6.1-1~bpo70+1\n. This is only a bug with pip version 1.5.4 backported from jessie\n. Oh! Excuse me for reporting.\npip cause this error while installing pip-tools.\n. After downgrading pip by apt and removing all dependecies (python-* packages)\npip install -U pip\ninstalled pip 1.5.5 :)\n. Just for comparision:\n```\nroot@szerver4:~# dpkg -l|grep python-\nii  python-all                         2.7.3-4+deb7u1                     all          package depending on all supported Python runtime versions\nii  python-all-dbg                     2.7.3-4+deb7u1                     all          package depending on all supported Python debugging packages\nii  python-all-dev                     2.7.3-4+deb7u1                     all          package depending on all supported Python development packages\nii  python-apt                         0.8.8.2                            amd64        Python interface to libapt-pkg\nii  python-apt-common                  0.8.8.2                            all          Python interface to libapt-pkg (locales)\nii  python-central                     0.6.17                             all          register and build utility for Python packages\nii  python-chardet                     2.0.1-2                            all          universal character encoding detector\nii  python-dbg                         2.7.3-4+deb7u1                     all          debug build of the Python Interpreter (version 2.7)\nii  python-debian                      0.1.21                             all          Python modules to work with Debian-related data formats\nii  python-dev                         2.7.3-4+deb7u1                     all          header files and a static library for Python (default)\nii  python-magic                       5.11-2+deb7u3                      amd64        File type determination library using \"magic\" numbers (Python bindings)\nii  python-minimal                     2.7.3-4+deb7u1                     all          minimal subset of the Python language (default version)\nii  python-pip                         1.1-3                              all          alternative Python package installer\nii  python-pkg-resources               0.6.24-1                           all          Package Discovery and Resource Access using pkg_resources\nii  python-pyrex                       0.9.8.5-2                          all          compile native-code modules for Python from Python-like syntax\nii  python-setuptools                  0.6.24-1                           all          Python Distutils Enhancements (setuptools compatibility)\nroot@szerver4:~# pip2 list --outdated\nsetuptools (Current: 3.5.1 Latest: 3.6)\nWebOb (Current: 0.9 Latest: 1.3.1)\nCould not find any downloads that satisfy the requirement antlr-python-runtime\nSome externally hosted files were ignored (use --allow-external antlr-python-runtime to allow).\nCould not find any downloads that satisfy the requirement pyrex\nSome externally hosted files were ignored (use --allow-external pyrex to allow).\nCould not find any downloads that satisfy the requirement python-apt\nSome externally hosted files were ignored (use --allow-external python-apt to allow).\nmercurial (Current: 2.2.2 Latest: 3.0)\nchardet (Current: 2.0.1 Latest: 2.2.1)\nCould not find any downloads that satisfy the requirement magic-file-extensions\nSome externally hosted files were ignored (use --allow-external magic-file-extensions to allow).\nCould not find any downloads that satisfy the requirement devscripts\nSome externally hosted files were ignored (use --allow-external devscripts to allow).\nroot@szerver4:~# pip-review\nNo update information found for Magic-file-extensions\nPyrex==0.9.4.1 is available (you have 0.9.8.5)\nWebOb==1.3.1 is available (you have 0.9)\nchardet==2.2.1 is available (you have 2.0.1)\nNo update information found for devscripts\nmercurial==3.0 is available (you have 2.2.2)\npython-apt==0.7.8 is available (you have 0.8.8.2)\npython-debian==0.1.21-nmu2 is available (you have 0.1.21)\n. I've already opened another issue about handling python packages coming from Debian apt https://github.com/nvie/pip-tools/issues/96\n. On PyPI it is with a dash:\nhttps://pypi.python.org/pypi/pip-tools/0.3.5\n. python\n\n\n\nimport pkgutil\npkg_dir = pkgutil.get_loader('piptools')\npkg_dir is None\nTrue\n``\n. Could you help me to write a PR to make pip-tools compatible withpkgutil.get_loader`?\nhttps://docs.python.org/2/library/pkgutil.html#pkgutil.get_loader\n. In pip-review-debian.py I would like to use:\n\n\n\npython\npkg_dir = pkgutil.get_loader(name)\nif pkg_dir is not None and pkg_dir.filename.startswith('/usr/local/lib/python'):\n    yield name, 'dev', 'dev', True\nto identify non Debian installed packages which can be upgraded with pip.\nIn https://github.com/nvie/pip-tools/issues/109\n. Honestly I do not know much about pkgutil but would like to submit a PR to make pip-tools compatible.\nCould it be that pip-tools is not a \"package\"?\n. Thank you for the explanation!\nSo my code in get_installed_pkgs()\npython\npkg_dir = pkgutil.get_loader(name)\nif pkg_dir is not None and pkg_dir.filename.startswith('/usr/local/lib/python'):\n    yield name, 'dev', 'dev', True\ndoes the wrong thing when interprets name (which comes from pip freeze) as package name when it is a project name?\nOn Debian squeeze\n``` python\n\n\n\nfrom pkgutil import get_loader\nprint get_loader('piptools')\nNone\nprint get_loader('pip-tools')\nNone\n``\n. I don't havepiptools` dir in /usr/local/lib/python2.6/dist-packages, only a dir called:\npip_tools-0.3.5-py2.6.egg-info\n\n\n\npip-tools was installed by pip install pip-tools\n. I'd like to run get_loader() to determine whether it is a local package or a Debian package.\nAnd __init__.py is not created. I do not know why.\n$ /usr/local/bin/pip install -v pip-tools\nDownloading/unpacking pip-tools\n  Using version 0.3.5 (newest of versions: 0.3.5, 0.3.4, 0.3.3, 0.3.2, 0.3.1, 0.3, 0.2.1, 0.2, 0.1)\n  Downloading pip-tools-0.3.5.tar.gz\n  Downloading from URL https://pypi.python.org/packages/source/p/pip-tools/pip-tools-0.3.5.tar.gz#md5=cace02ee789a54b3cdc72b0994baddcd (from https://pypi.python.org/simple/pip-tools/)\n  Running setup.py (path:/tmp/pip_build_root/pip-tools/setup.py) egg_info for package pip-tools\n    running egg_info\n    creating pip-egg-info/pip_tools.egg-info\n    writing requirements to pip-egg-info/pip_tools.egg-info/requires.txt\n    writing pip-egg-info/pip_tools.egg-info/PKG-INFO\n    writing top-level names to pip-egg-info/pip_tools.egg-info/top_level.txt\n    writing dependency_links to pip-egg-info/pip_tools.egg-info/dependency_links.txt\n    writing manifest file 'pip-egg-info/pip_tools.egg-info/SOURCES.txt'\n    warning: manifest_maker: standard file '-c' not found\n    reading manifest file 'pip-egg-info/pip_tools.egg-info/SOURCES.txt'\n    writing manifest file 'pip-egg-info/pip_tools.egg-info/SOURCES.txt'\nRequirement already satisfied (use --upgrade to upgrade): argparse in /usr/local/lib/python2.6/dist-packages (from pip-tools)\nInstalling collected packages: pip-tools\n  Running setup.py install for pip-tools\n    running install\n    running build\n    running build_scripts\n    creating build\n    creating build/scripts-2.6\n    copying and adjusting bin/pip-review -> build/scripts-2.6\n    copying and adjusting bin/pip-dump -> build/scripts-2.6\n    changing mode of build/scripts-2.6/pip-review from 644 to 755\n    changing mode of build/scripts-2.6/pip-dump from 644 to 755\n    running install_egg_info\n    running egg_info\n    writing requirements to pip_tools.egg-info/requires.txt\n    writing pip_tools.egg-info/PKG-INFO\n    writing top-level names to pip_tools.egg-info/top_level.txt\n    writing dependency_links to pip_tools.egg-info/dependency_links.txt\n    warning: manifest_maker: standard file '-c' not found\n    reading manifest file 'pip_tools.egg-info/SOURCES.txt'\n    writing manifest file 'pip_tools.egg-info/SOURCES.txt'\n    Copying pip_tools.egg-info to /usr/local/lib/python2.6/dist-packages/pip_tools-0.3.5-py2.6.egg-info\n    running install_scripts\n    copying build/scripts-2.6/pip-dump -> /usr/local/bin\n    copying build/scripts-2.6/pip-review -> /usr/local/bin\n    changing mode of /usr/local/bin/pip-dump to 755\n    changing mode of /usr/local/bin/pip-review to 755\n    writing list of installed files to '/tmp/pip-LqEUUg-record/install-record.txt'\nSuccessfully installed pip-tools\nCleaning up...\n  Removing temporary dir /tmp/pip_build_root...\n. Or could you include a pip-review-debian in your repo?\n. The patch.\npatch\n--- /usr/local/bin/pip-review   2014-08-18 14:05:35.000000000 +0200\n+++ /usr/local/bin/pip-review-debian    2014-08-18 14:15:49.720735513 +0200\n@@ -7,6 +7,7 @@\n import logging\n import sys\n import json\n+import pkgutil\n try:\n     import urllib2 as urllib_request  # Python2\n except ImportError:\n@@ -148,10 +149,14 @@\n             name = line.split('#egg=', 1)[1]\n             if name.endswith('-dev'):\n                 name = name[:-4]\n-            yield name, 'dev', 'dev', True\n+            pkg_dir = pkgutil.get_loader(name)\n+            if pkg_dir is not None and pkg_dir.filename.startswith('/usr/local/lib/python'):\n+                yield name, 'dev', 'dev', True\n         else:\n             name, version = line.split('==')\n-            yield name, parse_version(version), version, False\n+            pkg_dir = pkgutil.get_loader(name)\n+            if pkg_dir is not None and pkg_dir.filename.startswith('/usr/local/lib/python'):\n+                yield name, parse_version(version), version, False\n. To avoid this on new pip-tools releases:\nbash\npip install pip-tools --upgrade\ncp -a /usr/local/bin/pip-review /usr/local/bin/pip-review-debian\npatch /usr/local/bin/pip-review-debian ./pip-review-debian.patch\n. Oh! Thank you!\nThis patching is a \"light load\" on me considering the frequency of pip-tools updates.\n. ",
    "farukuzun": "Hello @szepeviktor,\nCheck out my commit on my fork. \n. @szepeviktor \nThey probably won't. I did it for personal usage. You can install pip-tools via my fork.\n@nvie \nIf you guys have plan to add option like --exclude-apt\nI may open a pull request with necessarily changes .\n. It's ok. I tested it. It works lovely.\n. ",
    "bardo": "Seconded. It would also enable modular dependencies in a simple way:\nbash\n$ cat requirements-{base,dev}.in | pip-compile - -o >requirements-dev.txt\n$ cat requirements-{base,production}.in | pip-compile - -o >requirements-production.txt\nGreat way to share a common versioned package list between different environments. The proper way to allow this would require quite a bit of reworking of .in files, allowing for some kind of \"include\" directive, but I think it's overkill at the moment.\n. Having a quick look at the code, the problem seems to lie in the call to pip.index.package_to_requirement(). I'll open the bug with them, sorry for the noise.\n. Just as a reference: https://github.com/pypa/pip/issues/2070\n. Fixed by #134.\n. ",
    "rsyring": "The .pipignore file is still referenced in the diagram that is part of the project readme.\n. FWIW, I led the adoption of Pipenv at my company but am now looking at different options.  We have encountered a number of bugs/obscurities in Pipenv that make the tool unreliable to use.  I like pipenv and hope it matures over time, but until then, I'm going to be looking for alternatives and this project is top of my list (along with Poetry).\nAlso, @nvie given my experience above and the many open issues on the pipenv repo, you may want to re-consider the recommendation at the top of this post (this is actually what I came to post an issue about).. @nvie that new article you linked to was very helpful.  Thank you!. ",
    "gavinwahl": "Multiple == specifiers are used in the example here: http://pythonhosted.org//setuptools/setuptools.html#declaring-dependencies. And django-filer is indeed installable.\n. With pep-440, this is the correct behavior.\n. Python 2.7.8\nargparse==1.2.1\nclick==3.3\n-e git+https://github.com/nvie/pip-tools.git@da68396da2d309404f00c8440f87295d7f0a4967#egg=pip_tools-origin/future\nsix==1.8.0\nwsgiref==0.1.2\nThis is when using the --find-links option with a local directory.\n. Is this expected to be difficult to implement? I might be able to do it with some guidance.\n. ",
    "jhermann": "While the text is not very explicit, given the exposed behaviour of setuptools, the comma is just a delimiter, not an operator (boolean or otherwise). It delimits a list of one-sided ranges, that are constructed into a sorted list of version intervals. Overlaps are eliminated, and you finally get an ordered set of allowed disjunct ranges (intervals). \"==x, ==y\" thus translates to \"[x,x] \u222a [y,y]\".\n. ",
    "estan": "I agree. I would like it if pip-compile took something like a -d option with a list of abstract dependencies to update. Then I could update say numpy selectively with pip-compile -d numpy. It should error if numpy is not declared in requirements.txt, or if upgrading just numpy is impossible without also upgrading some other requirement in requirements.txt (due to shared dependencies).\n. I was thinking, as a workaround right now, would it be possible to:\n1. Temporarily pin all dependencies you want to keep fixed in requirements.in to whatever exact versions are in requirements.txt currently.\n2. Add your new dependency to requirements.in, unpinned.\n3. Run pip-compile -U, the other requirements would be left unchanged since they are pinned, but the new requirement would be brought in since it's unpinned.\n4. Undo any changes in 1, and depending on how you want to maintain the new dep, either pin it in requirements.in or leave it as is.\nWould that work? Then pip-compile would error if the new dep could not be added because of conflicting deps. The same approach could be used if you just want to update an existing dep without touching the others.\n. I guess my above approach won't really work, since I guess it might still do unwanted updates of transient (indirect) dependencies in step 3? So some support in pip-compile for selectively updating one or more dependencies while keeping others fixed (if possible) would still be nice.\n. I would like to be able to pass -I to pip when doing pip-sync. The reason is I'm unfortunately required to use --system-site-packages on my virtualenv, to get access to the distro installed PyQt (which is not on PyPI yet, so not in requirements.txt), but I still want pip-sync to install as much as possible into the virtualenv (and not skip stuff that is already satisfied by the system-wide site-packages).\n. Just want to check, is there still no way to do this?. @ryanhiebert: Could you explain further? Can I just put \".\" in requirements.in and it'll just work? Sorry for my ignorance :)\nI still think this feature has merit, because often you want to maintain a separate test-requirements.txt with additional deps needed only for testing, and those should be generated from tests_require, so some way of specifying what args to setup to consider would be needed. Or maybe that's possible with your approach as well? \n. I think this is a duplicate of #192 , right @nvie ? Also there's #321 which is more of a general wish for passing options through to pip.. Our CI builds recently started failing because of this. Since it seems the Ubuntu bug is slow-moving, would it be OK to add @graingert's workaround to official piptools? It just adds pkg-resources to PACKAGES_TO_IGNORE in sync.py.. Just as some extra piece of info, here's the link to the thread on distutils-sig@ about the coming PIP 10:\nhttps://mail.python.org/pipermail/distutils-sig/2017-October/031642.html. I bumped into this limitation again today and decided to have a look at it. Turns out it's not so easy since the pip API that pip-sync is using to parse the requirements files (parse_requirements) does not provide the info about any encountered --no-binary flags out-of-the box, so fixing it would take some refactoring.. Let me know if you want me to combine the test cases into a single case.. Sorry been completely swamped at work lately. Will try to resolve conflicts tomorrow when at the office.. Alright, will fix tomorrow when I'm back at work.. Yes, I began that work with https://github.com/jazzband/pip-tools/pull/704 which adds support for --find-links to pip-compile. Similar PRs could be made for other flags.. ",
    "MinchinWeb": "I started a branch to fix this:  https://github.com/MinchinWeb/pip-tools/tree/windows-scirpts\nThis allows the commands to be run on a Windows machine, and should allow them to run on other OS's as well.\nHowever, following commands are called from the command line and are often not available on Windows machines:\n- touch - pip_dump.py line 92\n- pip - pip_dump.py line 93, pip_review.py line 137, 211\n- cat - pip_dump.py line 123\n- sort - pip_dump.py line 123\n- rm - pip_dump.py line 127\n. Ok, I'll keep hacking at it, bit by bit...\n. Thanks for all your work on this! I can confirm that they installed for me on Windows without issues. I'll have to read up on them and work to add them into my workflow.. Has this been fixed in v1.8.1? My basic use of pip-tools seems to suggest it has.... ",
    "jmozmoz": "If I call the pip-review script directly the first character is missing:\nc:\\opt\\Python34>python Scripts\\pip-review\n)artopy==0.12.0rc1 is available (you have 0.11.x\n)ython==0.22 is available (you have 0.20.2\n(the packages are cartopy and cython)\n. ",
    "nikolay": "@blueyed That error was with version 1.4.1 install with Python 3.4. When I did pip install -U pip, it got upgraded to v6.0.6 and the error now is:\n$ pip-compile requirements.in\nTraceback (most recent call last):\n  File \"~/pyenv/versions/test/bin/pip-compile\", line 9, in <module>\n    load_entry_point('pip-tools==1.0', 'console_scripts', 'pip-compile')()\n  File \"~/pyenv/versions/test/lib/python3.4/site-packages/pkg_resources.py\", line 353, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"~/pyenv/versions/test/lib/python3.4/site-packages/pkg_resources.py\", line 2302, in load_entry_point\n    return ep.load()\n  File \"~/pyenv/versions/test/lib/python3.4/site-packages/pkg_resources.py\", line 2029, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"~/pyenv/versions/test/lib/python3.4/site-packages/piptools/scripts/compile.py\", line 13, in <module>\n    from piptools.package_manager import PackageManager\n  File \"~/pyenv/versions/test/lib/python3.4/site-packages/piptools/package_manager.py\", line 22, in <module>\n    from pip.index import Link, PackageFinder, package_to_requirement\nImportError: cannot import name 'package_to_requirement'\n. Any update?\n. ",
    "digital-carver": "Just adding another instance of this issue: \n...\nlouis==1.3 is available (you have 2.3.0)\n...\noneconf==0.0.1.dev0 is available (you have 0.2.8.1)\nPAM==0.1.4 is available (you have 0.4.2)\nPIL==1.1.6 is available (you have 1.1.7)\n...\nThis keeps me from being able to use the --auto flag at all, and having to go through everything interactively to make sure downgrades don't happen. \n. ",
    "mbertheau": "I'll be using blueyed/fixes-for-pip now and if I don't report any problems here, then the branch works fine for me and it's a +1 to merge :)\n. ",
    "michaeljones": "Fixed in #131. All good.\n. Ah, I removed argparse because it is installed on my system and I cannot get my python/virtualenv setup to ignore my system modules so the tests working working for me. I'll update my new tests to work on travis.\nThe others were failing anyway though I will see if they can be trivially updated.\n. I have done my best. Feel free to ignore the last two commits as they aren't strictly related but they do attempt to fix the other test failures.\n. No trouble. Thanks for writing the tool, I've found it very useful!\n. This has affected me too as I'm trying to install packages which are submodules of my project. It seems the issues arises here: https://github.com/pypa/pip/blob/develop/pip/req/req_install.py#L1131\nI suspect it is hard to avoid as pip-tools uses the pip library and the pip library has no reason to keep a relative path around. It wants to deal with absolute paths.\nWhich means that it is unlikely that we'd get the pip library changed to help us so which suggests that we'd have to get the output from the pip library and go back and make any paths relative again which is an option but not incredibly desirable as it might be hard to know exactly when to do it and when not to.\nPersonally, I'm going to try switching from using submodules and relative paths to having git+git:// paths in my requirements.in file but at the same time it seems very reasonable to try to fix this as it makes little sense to have absolute paths in a requirements.txt file when there is little chance that the paths in different development environments or the production environment is going to be the same. That is certainly what affects me. Of course, the requirements.txt should also be in git so it should be possible to pip-compile and then edit the requirements.txt before committing it.\n. ",
    "Surgo": ":(\n. ",
    "mlew": "Same thing is happening for me\n. This appears to be fixed on master, we just need a new version to be cut and released to PyPI.\n. ",
    "maniqui": "^^ To me too.\n. hi @nvie \nI've updated to latest release (v0.3.6).\nThe first time I ran pip-dump after upgrade, it emptied both of my requirements files (requirements/common.txt & requirements/dev.txt). \nI was confused.\nSo, just after that (and without resetting my requirements files to a good state), I ran pip-dump for a second time. Now, my requirements/common.txt file got populated with all the requirements, but the requirements/dev.txt file remained empty.\nThen, after that, I fixed the files manually, by deleting dev dependencies from common.txt and adding them back to dev.txt. \nSo far, so good, and it was the time for a final test: I've re-run pip-dump, to see if it would mangle my files again, or if it would do nothing at all (which was expected, as both my common.txt and dev.txt were now manually fixed and up-to-date). Thankfully, pip-dump did nothing, probably because there was nothing to do, as the requirement files were up-to-date.\nOf course, there is still initial the issue of pip-dump emptying my requirement files on first run. I was able to reproduce it several times, by reverting the contents back.\nSo, my guess (and maybe it's something you could try to do to reproduce my issue) is that pip-dump might be failing (by emptying the requirements files) when they are outdated.\nThe easy test would be to have an outdated requirements file and run pip-dump.\nLet me know if you need more info.\n. ",
    "carlkibler": "I can confirm the behavior. I think pip-review should be enhanced to mimic current pip behavior, which is to by default only consider release versions. This means alpha, beta, and rc would not be recommended. This is overriden in pip with the \"--pre\" flag.\nI may take a shot at adding this, if no one else a pull request out yet. It sounds like fun.\n. ",
    "b4stien": "Same behaviour here. From a user's perspective it's quite a biggie.\n. After a quick look: it will not be straightforward.\nThe fact that a release is a pre-release is not specified on PyPI (see for instance https://pypi.python.org/pypi/SQLAlchemy/1.0.0b5/json). I'm not sure how pip is able to handle them. Parsing the version and inferring from there maybe ?\n. I'm currently working on a PR for this.\nIt will use https://pypi.python.org/pypi/packaging, do you prefer it vendorized (like pip does) or declared as a dependency?\n. By \"it\" I meant \"my PR\", sorry.\nVendorizing a dependency is the process of copying/pasting its source inside yours. The alternative is to \"declare\" this dependency for pip/setuptools to grab it at install time (that's what I did in my PR).\n. It seems @nvie intended to rewrite (or has rewritten, see the future branch) a huge part of the project, but it hasn't made its way to PyPI yet.\nI ended up using my own fork for this specific matter.\n. @jgonggrijp it is still relevant, I'll try to submit a PR there this week.\nBTW: This issue should probably be closed if pip-review doesn't live here anymore.\n. Wow, didn't know about pip list! Thank you so much!\nIt even includes a --pre flag and only show stable versions by default.\n. @jgonggrijp no problem, glad it's resolved! Gonna leave a comment there.\n. Tests look pretty obscure to me (especially when it comes to dependencies, argparse seems to be manually installed in every test...).\nI guess packaging (the new dependency) should be added somewhere there but no clear idea where.\n. ",
    "wil93": "Hi, when will this change make it to pypi? The current version of pip-tools does not include it.\n. I'd like to ask, though: if pip-compile does not suggest to upgrade to beta versions (because the massive-rewrite branch was merged), then why can't pip-review just use pip-compile as a dependency? (wherever pip-review now lives).\nAlso: from what I undestand, pip-review is now no longer needed (you can just use pip-compile), but I can see how it could still be useful in some cases.\nLastly: is there an official reason from pip-tools' author as of why pip-review was dropped?\n. Nevermind, I found the thread where the author explains the reason: #185.\nI also realized that I can just do pip list --outdated and it's exactly the same as running pip-review. So my question is now: why are we trying to keep pip-review running?\n. @jgonggrijp Fair enough. I can see how --interactive can be a very good reason to keep it (while, for non-interactive upgrade, there is pip-sync). However, consider this. You could:\n1. Convert the pip-tools fork named \"pip-review\" to a standard github repository named \"pip-review\" (I'm not sure if this is hard to do, maybe you should contact github), or just delete it and create it again.\n2. Drop every line of code that belonged to pip-tools.\n3. Rewrite pip-review as a very simple wrapper to pip list (I guess it should be at most 30 lines of code?).\nThe pip-review package would then still exist, with these advantages:\n- The alpha/beta/rc issue would be automatically gone, without intervention, because it would just use pip list under the hood.\n- It would be simpler to maintain, for the same reason.\nActually, you could even make it a shell script (but then you would lose the possibility of uploading the package to pypi, so I'd stick with python).\n\nmind you, the package was downloaded about 1100 times since I uploaded it one month ago\n\nI'm not sure how many are aware of pip list --outdated.\n. ",
    "chhantyal": "Also pip-review --auto actually installs beta/rc releases. If this is not going to be fixed soon, would be great if we could update docs. \n. ",
    "Jc2k": "I don't use pip-tools yet but have been following it on GitHub since i heard of it with a view to use pip-sync. This thread has reminded me to get round to using it now the future branch is merged.\nMy ideal workflow would be to update requirements.txt (aided by pip list -o or pip-compile) and then run pip-sync. I  would want to review the changelogs of the projects I'm updating. So i'd want to update my requirements.txt then git diff before i commit. But then other devs and the prod/stage deployments would use pip-sync on my refreshed requirements.txt.\nAre these commands roughly equivalent (given a suitably unpinned requirments.txt?:\npip-compile && pip-sync\npip-review --auto && pip freeze > requirements.txt\nCan I tell either of these stuff like 'don't upgrade Django to the next major release'?\n@jgonggrijp you said you use pip-review --auto is a scripted way. What is that?\n(Btw we shouldn't get hung up on the download statistics - they are heavily skewed by indexers and mirrors. A fresh release can get 1,600 downloads just from bots).\n. ",
    "zacstewart": "I'm also having this problem while trying to pip install gnureadline. I'm using Python 3.4.2 and pip 6.0.8.\n. ",
    "renta": "My issue was because of old packages versions in Ubuntu, I think. Pip was installed with apt. So I've fixed it with:\nsudo apt-get purge python-pip # remove an old \"apt\" version\neasy_install -U pip # install it with python setuptools\nhash -r # current user can use \"pip\" command\nNow my pip is OK.\nWorth reading:\nhttp://stackoverflow.com/questions/27341064/how-do-i-fix-importerror-cannot-import-name-incompleteread\nhttps://askubuntu.com/questions/561377/pip-wont-run-throws-errors-instead\nhttps://bugs.launchpad.net/ubuntu/+source/python-pip/+bug/1306991\nhttp://stackoverflow.com/questions/16237490/i-screwed-up-the-system-version-of-python-pip-on-ubuntu-12-10\n. @nvie , @blueyed I use virtualenvwrapper for my python projects. I wonder, what startegy of updating packages would be the best:\n- mass-updating all the packages in pip with something like \"pip-review --auto\" like I do in apt, for example.\nOr\n- manuaily updating package-by-package to check if there is a confclict after the update. Because  after this crash I suppose that pip could not guarantee the safe update.\n. ",
    "jamezpolley": "I had this error, and fixed it by upgrading pip. \nMy system pip was  pip 1.5.6 and had this error; once I upgraded I got 6.1.1 and the error was gone.\n. ",
    "rossgzc": "Thanks for your reply @jamezpolley! That worked :)\n. ",
    "kevgathuku": "Okay. Thanks for taking the time to reply.\nLet me try it out.\n. ",
    "jonathanchu": "\n. ",
    "klinkin": "\n. ",
    "piotr-dobrogost": "For instance requests library uses them \u2013 https://github.com/kennethreitz/requests/blob/v2.6.0/setup.py#L75\n. It looks like currently there's even no bug for this. I guess it might be difficult for people to find this thread as this is closed PR now. Should someone create a bug for this feature?\n. Duplicate of issue #185 \n. It seems the first way of invoking pip-sync described by @nvie does exactly what you describe in your point no 3.\n. I don't know what method pip-tools uses to find out what's installed but in case it uses pip you may be interested to know that argparse (and a few others libs) are special cased in pip. See https://github.com/pypa/pip/issues/1570 and https://github.com/pypa/pip/issues/3132\n. @kengruven \nSee this pip issue \u2013 Allow pip list --outdated to take a requirements file\n. @nvie \n\nWould you consider merging the behavior in this PR as a stop-gap so vcs urls are at least functional?\n\nWould you? :)\n\nIn this latter case, the symbolic name example-branch points to a specific commit at a specific time. \n\nWhat if there's a tag name not a branch name given after '@' in the vcs url? Do you also want to treat it like a moving target and pin it to the commit hash?\n. Could this be reopend and possible merged/reviewed?. ",
    "hale": "I ran into this when including ipython in requirements.in.\nSo, that's an example of a package that uses extras.  \n(The install docs say to install like ipython[all])\n. ",
    "paulkernfeld": "I added a pull request, #202, that adds support for extras in pip-compile. Please let me know what I need to do to get this merged in!\n. Pull request #202 is merged now, so this issue can be closed.\n. @coriolinus can you open a new issue for this, please? In the new issue, can you describe what you are expecting to happen, compared with what is actually happening? Are you seeing an error, for example? If you tag me on the new ticket I can take a look.\n. Hey @mattrobenolt, it works in some situations without my patch, but it will fail in other cases. I think that it will fail in any of these cases:\n1. If my project requires package A, and package A requires package B with with extras, pip-compile will not put the extras for package B into requirements.txt.\n2. pip-compile doesn't know how to merge extras correctly. For example, it knows that it should merge ipython>=2 and ipython<3 into ipython>=2,<3, but it doesn't know that ipython[notebook] and ipython[nbconvert] should be merged into ipython[nbconvert,notebook].\n3. The pip-compile dependencies cache is keyed on only name and version (not on extras), so the cache doesn't know that ipython[notebook] and ipython have different dependencies. I actually just realized that my pull request does not fix this problem, so I need to update it.\n. All right, @nvie, let me know what you think of this update! Sorry it's so long; I can split it into smaller commits if that's helpful.\n. @habnabit you're right, thanks for finding and fixing that! That is what I get for making changes without testing them :(\n. I have added some tests for the cache, and incorporated @habnabit's fix. Right now I think Travis is broken, but the build passes on my computer.\n. Hey @nvie, is there anything I can do to make it easier to get this patch in?\n. Oh, very interesting point about the hash symbol. This is something that I know nothing about. Basically, I'm calling pip.req.InstallRequirement.from_line, and it ignores the hash symbol before the extras. We can definitely work around this, e.g. by only sending it the text before the hash symbol, but are you sure that's what we want to do?\nI sort of think that this is the intended behavior, because when I run pip install -r requirements.txt, and requirements.txt has the one line, ipython#[notebook], pip installs the notebook extra! Is this perhaps a bug in pip's parsing code? Or... a feature?\n. Oh boy. Actually, it looks like pip 6 considers ipython#[notebook] to mean ipython, whereas pip 7 considers it to mean ipython[notebook].\n. Yay! Thanks @nvie!\n. I don't have access to the machine that I used when I originally reported this issue, but when I run tox on my new machine I no longer see this error.. Nice!\n. Thanks for the tip! I implemented this.\n. You're right. Good point.\n. ",
    "coriolinus": "I'm encountering this right now.\nSample from requirements.in:\npip-tools               # needed for pip-compile and pip-sync\nhypothesis[django]      # testing framework\nSample from requirements.txt:\nhypothesis==3.6.0\npip-tools==1.7.0\n. @paulkernfeld No problem; I created the issue last night but forgot to mention you then. Either way, it's up now: #408 .\n. Same issue:\n```shell\n$ pip-compile --version && cat requirements.in && pip-compile --dry-run\npip-compile, version 1.8.0\npip-tools               # needed for pip-compile and pip-sync\nDjango                  # fundamental web framework\ndjangorestframework     # REST framework\ndjango-storages         # Used for modeling S3 objects within Django models\npsycopg2                # Used for interacting with Postgres DBs\nboto                    # direct interface to AWS, used by Django\nboto3                   # modern direct interface to AWS, used by our code\nhypothesis[django]      # testing framework\nbcrypt                  # allows django to read bcrypt-encrypted passwords\ncroniter                # get datetime of next run of a cron job\ndjango-choices          # Enum-like objects for multiple choice fields\ndjango-extra-fields     # for Base64ImageField\ndjango-push-notifications # Push notifications helper\ndjango-rest-swagger     # Alternate documentation generator for DRF\ndjango-ses              # email manager which routes emails over AWS SES\ngeopy                   # geographic functions for i.e. distance between two lat, long points\ngooglemaps              # geo functions with a more detailed API for neighborhoods etc\nmailchimp3              # client library for Mailchimp API v3.0\nPillow                  # image processing library so ImageFields work\nPyYAML                  # parse YAML files\nresponses               # test helper to mock out HTTP within the Requests library\nsocial-auth-core        # AKA python-social-auth. Goal here is to move away from RSA, which\nsocial-auth-app-django  # is a thin wrapper over this functionality anyway.\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nbcrypt==3.1.2\nboto3==1.4.4\nboto==2.45.0\nbotocore==1.5.12          # via boto3, s3transfer\ncffi==1.9.1               # via bcrypt\nclick==6.7                # via pip-tools\ncookies==2.2.1            # via responses\ncoreapi==2.2.4            # via django-rest-swagger, openapi-codec\ncoreschema==0.0.4         # via coreapi\ncroniter==0.3.15\ndefusedxml==0.5.0         # via python3-openid, social-auth-core\ndjango-choices==1.5.0\ndjango-extra-fields==0.9\ndjango-push-notifications==1.4.1\ndjango-rest-swagger==2.1.1\ndjango-ses==0.8.1\ndjango-storages==1.5.2\ndjango==1.10.5\ndjangorestframework==3.5.4\ndocutils==0.13.1          # via botocore\nfirst==2.0.1              # via pip-tools\ngeopy==1.11.0\ngooglemaps==2.4.5\nhypothesis[django]==3.6.1\nitypes==1.1.0             # via coreapi\njinja2==2.9.5             # via coreschema\njmespath==0.9.1           # via boto3, botocore\nmailchimp3==2.0.7\nmarkupsafe==0.23          # via jinja2\noauthlib==2.0.1           # via requests-oauthlib, social-auth-core\nolefile==0.44             # via pillow\nopenapi-codec==1.3.1      # via django-rest-swagger\npillow==4.0.0\npip-tools==1.8.0\npsycopg2==2.6.2\npycparser==2.17           # via cffi\npyjwt==1.4.2              # via social-auth-core\npython-dateutil==2.6.0    # via botocore, croniter\npython-dateutil==2.6.0    # via botocore, croniter\npython3-openid==3.0.10    # via social-auth-core\npytz==2016.10             # via hypothesis\npyyaml==3.12\nrequests-oauthlib==0.8.0  # via social-auth-core\nrequests==2.11.1          # via coreapi, googlemaps, mailchimp3, requests-oauthlib, responses, social-auth-core\nresponses==0.5.1\ns3transfer==0.1.10        # via boto3\nsimplejson==3.10.0        # via django-rest-swagger\nsix==1.10.0               # via bcrypt, pip-tools, python-dateutil, responses, social-auth-app-django, social-auth-core\nsocial-auth-app-django==1.1.0\nsocial-auth-core==1.2.0\nuritemplate==3.0.0        # via coreapi\n``\nNote thatpython-dateutil` appears twice.. ",
    "cancan101": "Take a look at: https://developer.github.com/changes/2016-02-24-commit-reference-sha-api/\n. I would love to see this feature merged.\n. Further --process-dependency-links does not get passed through from the in file to the txt file.\n. pip 8 allows specifying --process-dependency-links in the requirements file which then means that pip will use the dependency-links section in setup.py to satisfy requirements. pip-tools does not consider the dependency-links section even tho I have set  --process-dependency-links in my requirements.in file.\n. Closing as duplicate of: https://github.com/nvie/pip-tools/issues/379. See: https://github.com/nvie/pip-tools/issues/390#issuecomment-258356585. ",
    "georgexsh": "+1 for VCS pinning \n. with pip 8.1.2 under python 2.7.9 breaks too.\n. ",
    "tysonclugg": "@blueyed You mentioned you have this working, can you submit the PR in order to move this issue along?. Bonus points for allowing pip-compile --environment '{\"python_version\": \"2.7\", \"sys.platform\": \"darwin\"}' --output-file requirements-2.7-darwin.txt requirements.in as a kind of \"cross-compilation\" support.  But seriously, don't let this request get in the way of getting something good in now - perfect comes later.\n. I'm not sure of the use case for this over compiling from a master requirements.in specified like this:\n-r requirements/foo.in\n-r requirements/bar.in\nGiven the lack of activity for nearly 3 years, and the suggested alternative, I'm closing this for now.. Duplicate of #165.. This now works if you specify the output file as -.. Apologies, reopened!. Closing due to lack of activity and also since the use case seems to be handled by running pip directly as per the most recent comment.. @firebirdberlin This looks like a great start, but sadly it's been a long time and this PR is now stale. We'd love to see a fresh pull request if you've got some time!. @dirn I re-read the post.  Nowhere does it say that setup.py should not be used with applications.  The closest statement I could find was this:\n\nYou\u2019ve read this far and maybe you\u2019ve said, ok I know that setup.py is designed for redistributable things and that requirements.txt is designed for non-redistributable things but I already have something that reads a requirements.txt and fills out my install_requires=[...] so why should I care?\n\nIt's difficult to know the authors intent, but the statement does not preclude the use of setup.py by applications.  I could even argue that the implied link given by the statement \"I already have something that reads a requirements.txt and fills out my install_requires=[...]\" suggests that applications should have a setup.py.  But that would be reading too much into what was written - perhaps @dstufft can weigh in and tell us what was intended?. Having abstract requirements in setup.py is encouraged, and having concrete requirements in requirements.txt is also encouraged.  This patch maintains use of requirements.in by default over reading from setup.py, the only point of contention (at this early stage) seems to be the inclusion of 2 words and an associated hyperlink in the README file.\nI'll admit that this point of contention has been a bug-bear of mine since about 2008.  I've never liked how Django messes with sys.path and seems to have an aversion to Python packaging.  I think I found why Django does this, it has to do with one of the original Django authors hating on Python packaging back in 2007.  Being an influential person, that attitude is still with us across the majority of the Python community nearly 10 years later.  And since the internet is an echo chamber, that attitude appears countless times by many individuals without much thought, such as has been demonstrated today.\nFrankly, the \"my project doesn't need a setup.py\" drivel has to stop.  Writing a basic setup.py is easy, I've not heard a good reason to avoid doing so.\nSome lines from the Zen of Python are relevant here:\nSpecial cases aren't special enough to break the rules.\nThere should be one-- and preferably only one --obvious way to do it.\nNow is better than never.\nApplications are not special enough that we should advise that they omit having a setup.py.\nThe canonical way to define dependencies in Python is via the install_requires argument to setup() in setup.py.\nI'm taking a stand on this issue now - the Python packaging ecosystem has come a long way in the last 10 years, it's time to re-evaluate the status quo.  Stop referencing any material written on the matter published more than a few months ago - it's likely based on ancient hearsay.  The article presented by @dirn as a counter to my comment in the README was written in July 2013 - it's ancient.. > To track the abstract requirements, I could use a setup.py, but seeing as this is neither a library nor an application (really there's almost no executable Python code to found anywhere), introducing Python code feels out of place. This is why I use a requirements.in here.\nI agree, this seems like a valid reason not to use setup.py - you describe a situation where you're not distributing any Python modules.\nI think I draw the line at the point where you expect others to import your modules, or you import your own modules.  This is the point where namespaces come into effect.  Simple scripts such as this serve as a prime example of where a namespsace isn't required, and hence there is no need for setup.py.\nI don't consider public vs private distribution of packages to be relevant here, not all packages are uploaded to PyPi.  Having installable requirements such as -e git+https://github.com/nvie/pip-tools.git@master#egg=pip-tools (or a private repo URL) is useful, setup.py makes this work.\nWould changing those 2 words from \"you should\" to \n\"you should if you are distributing a python module\" help to settle this?  Where would you draw the line and suggest people should use setup.py?. Clearing up my previous comment, I feel the difference between a python script and a python module is that one isn't expected to be imported, the other is.\nThe whole point of setup.py is to define a distribution, which is a collection of one or more namespaced packages.  Once defined, tools such as wheel can build a package which can be distributed in a separate step (eg: using twine).  The other things defined in setup.py are things like declaring which versions of Python are supported, how to run tests (with python setup.py test), what the version is, etc.  These are all useful in their own right - defining your distribution (including its requirements) is useful, even if a package is not going to be distributed to the general public.  As stated before, if you're not using namespacing, you don't need setup.py, and requirements.in serves as a useful proxy for install_requires.. I've pushed a new commit, with a significantly toned down version of the offending line in the README.  Regarding not using setup.py, it now says \"it's easy to write one\" instead of \"you should\", which removes the authoritative tone entirely.\nIs it time for @nvie to review?  I'm happy to squash the commits in this or a separate branch if that's what is preferred.. @Groxx Given you commented after the second commit was pushed, I'm a little confused if you're +1 or -1 given that the recommendation has been removed.  Can you please clarify?  It looks like you object to the PR as it currently stands.... @Groxx How do you feel the documentation prioritises setup.py?  Can you suggest what I can change to resolve that?\nSpecifically, are you suggesting I swap the order of the two sub-sections within the Example usage for pip-compile section of the README?. @anthrotype: It seems the conversation about this PR devolved into philosophical discussion on the merits of setup.py vs requirements.in, and when either is appropriate.\nSome concern was raised about my proposed changes to the documentation describing the new behaviour, and I made changes in accordance with the concerns raised.  However, the detractors against the doc changes (@Groxx, @dirn, @davidovich) have not responded positively to the updated documentation.  They haven't responded firmly against either (unless @Groxx eventually makes a response to my question about further changes to the docs), so I'd have to assume the updated docs are probably OK.  If not, the detractors better speak up - we can assume consent through silence given they've had over 2 months to respond.\n@rpkilby asked a fundamental question:\n\nSorry if I'm missing it, but what's the actual use case for this PR?\n\nI've decided to leave the answer to this question to people such as yourself.  The reality is that this PR isn't likely to be accepted unless @nvie agrees with the changes, and Vincent hasn't made comment yet.  Perhaps you can describe why this PR would be useful for yourself, it would clarify that there is indeed a use case for the proposed changes.. @myw What you've put forward makes sense to me, so I'm +1 on adding options to support the behaviour you're requesting.  But certainly not as the default behaviour.. @myw If you're still interested, I suggest you create a new PR (based on current master) with options to support the use case you have described.. How about using GNU make with the following Makefile:\n```\nrequirements%.txt: requirements%.in\n    pip-compile -o \"$@\" \"$<\"\n.PHONY: requirements\nrequirements: $(wildcard requirements*.txt)\n```\nThen you run make requirements, job done.. @merwok I guess my point was that your workflow may be better supported by existing build tools, rather than expecting each tool to be a swiss army knife supporting all possible use cases.  I'm -1 on this since you could indeed write a shell script to handle your use case.\n@vphilippon - what are your thoughts?. I vote for un-vendoring pip, along with using stricter version requirements in our install_requires.\nThe release of pip-tools that un-vendors pip should depend on pip>=8.0,<10.1.  Patch releases of pip should be allowed.  When a new major/minor version of pip is released, nothing will break.  People can submit issues/pull requests to enable new versions of pip when they become available.. @techalchemy You're right that CalVer will complicate things, especially given the new pip deprecation policy.  But I'm not aware of a better alternative, aside from helping the pypa team to make the necessary pip internal APIs public if at all possible.\nThe pip release cadence docs state that master should always be in a releasable state.  We should include a pipmaster factor in our tox.ini (and .travis.yml) in order to catch issues before a breaking version of pip is released.. @vphilippon You're right that having an install_requires line for pip is a bad idea.  Having the pipmaster factor in CI should be enough, without the install_requires changes I had proposed.. @vphilippon Would you like me to pick this up and add the pipmaster CI factor?. @techalchemy We're not using appveyor.yml, and I don't have access to change settings on the pip-tools Appveyor project.  Either I create an appveyor.yml, somebody gives me access, or someone else adds the pipmaster factor there.  Which will it be?. @techalchemy I'm trying to understand why we should use an environment variable to determine the pip version rather than inspecting the pip.__version__ attribute in the test suite.\nCan you please enlighten me?. @techalchemy May I change the decorator to wrap unittest.expectedFailure instead of unittest.skipIf, so that we can see an unexpected pass/fail when the environment doesn't match reality, rather than skipping the test?\n```diff\ndiff --git a/tests/test_cli.py b/tests/test_cli.py\nindex dfea0cf..6d6c820 100644\n--- a/tests/test_cli.py\n+++ b/tests/test_cli.py\n@@ -16,10 +16,11 @@ from pip import version as pip_version\nPIP_VERSION = parse_version(os.environ.get('PIP', pip_version))\n-skip_unless_pip9 = pytest.mark.skipif(\n-    PIP_VERSION < parse_version('9'),\n-    reason='needs pip 9 or greater'\n-)\n+def expected_failure_unless_pip9(test_method):\n+    if PIP_VERSION < parse_version('9'):\n+        return unittest.expectedFailure(test_method)\n+    else:\n+        return test_method\n@pytest.yield_fixture\n@@ -359,7 +360,7 @@ def test_generate_hashes_with_editable():\n     assert expected in out.output\n-@skip_unless_pip9\n+@expected_failure_unless_pip9\n def test_filter_pip_markes():\n     \"\"\"\n     Check that pip-compile works with pip environment markers (PEP496)\n```. @techalchemy I don't see the point of shunning unittest in favour of pytest in this case, since it's no more verbose and functionality is identical.. @techalchemy My mistake, carry on!. Small aside: I understand the desire to squash changes down to the minimal required, but I'm a little dismayed that contributions are incorrectly attributed to the person who performed the squash.  Obviously this is more of a social issue than a technical one.\nIt bummed me out when I finally had a change accepted into @django, only to have a core member gain attribution (in terms of git stats) for the work I did.  Not so much the case here, but you get the gist of what I'm saying.\nDon't worry about undoing the squash in this particular instance - just be mindful of the social implications when squashing and/or rebasing commits.\nI wonder if git could be improved to maintain attribution for squashed commits?. > You should consider the reason for your offer here\u2014 are you here to help out the project, or are you merely interest in attribution?\nI already said not to worry about undoing the squash - I'm here to help.  My offer to help isn't dependent upon attribution.  I'm sorry I mentioned it, I'm ready to move on.. @techalchemy I've just become aware that GitHub recently added support for \u201cCo-authored-by\u201d trailers on commit messages.  This answers my question regarding if git could be improved to maintain attribution for squashed commits.\nCould you please amend the commit message(s) to include the relevant \"Co-authored-by\" trailer lines to include myself and the others who have contributed in various forms (ie: code, ideas, etc) to this PR?. This is weird - CI is passing, but tox -e py27 fails on my Ubuntu 16.04.4 LTS machine:\n```\nGLOB sdist-make: /home/tclugg/projects/pip-tools/setup.py\npy27 inst-nodeps: /home/tclugg/projects/pip-tools/.tox/dist/pip-tools-2.0.3.dev2+gaf34bcc.zip\npy27 installed: attrs==17.4.0,click==6.7,coverage==4.5.1,first==2.0.1,funcsigs==1.0.2,mock==2.0.0,more-itertools==4.1.0,pbr==4.0.2,pip-tools==2.0.3.dev2+gf050dc7.d20180502,pkg-resources==0.0.0,pluggy==0.6.0,py==1.5.3,pytest==3.5.1,six==1.10.0,-e git+git@github.com:tysonclugg/pip-tools.git@af34bcc94de44e8269d0899b0e3cb646cd136a41#egg=small_fake_with_deps&subdirectory=tests/test_data/small_fake_package\npy27 runtests: PYTHONHASHSEED='488695276'\npy27 runtests: commands[0] | pip --version\npip 10.0.1 from /home/tclugg/projects/pip-tools/.tox/py27/local/lib/python2.7/site-packages/pip (python 2.7)\npy27 runtests: commands[1] | coverage run -m pytest --strict --doctest-modules tests/ piptools/\n=========================================================== test session starts ============================================================\nplatform linux2 -- Python 2.7.12, pytest-3.5.1, py-1.5.3, pluggy-0.6.0\nrootdir: /home/tclugg/projects/pip-tools, inifile: setup.cfg\ncollected 99 items                                                                                                                         \ntests/test_cache.py ....                                                                                                             [  4%]\ntests/test_cli.py ....................                                                                                               [ 24%]\ntests/test_fake_index.py ......                                                                                                      [ 30%]\ntests/test_minimal_upgrade.py .                                                                                                      [ 31%]\ntests/test_repositories.py ...                                                                                                       [ 34%]\ntests/test_repository_local.py ..                                                                                                    [ 36%]\ntests/test_repository_pypi.py ...                                                                                                    [ 39%]\ntests/test_resolver.py .................                                                                                             [ 56%]\ntests/test_sync.py .......................                                                                                           [ 79%]\ntests/test_top_level_editable.py .                                                                                                   [ 80%]\ntests/test_utils.py ......                                                                                                           [ 86%]\ntests/test_writer.py .......                                                                                                         [ 93%]\npiptools/io.py ....                                                                                                                  [ 97%]\npiptools/utils.py ..                                                                                                                 [100%]\n======================================================== 99 passed in 25.79 seconds ========================================================\npy27 runtests: commands[2] | /home/tclugg/projects/pip-tools/tests/test_data/check_module_not_importable.py small_fake_module\nFAIL: \nERROR: InvocationError: '/home/tclugg/projects/pip-tools/tests/test_data/check_module_not_importable.py small_fake_module'\n___________ summary ____________\nERROR:   py27: commands failed\n``. @vphilippon I suspect that pip from the virtualenv is responsible for this.  However, my understanding is that your on vendoring pip 9.0.3 should prevent this from happening - care to take a look?. @dirn I read the post you reference about an hour ago while looking for a suitable URL for the link in question.  I don't think it says we should or should not usesetup.py, I think the point was more that you shouldn't do stupid stuff likeinstall_requires=list(open('requirements.txt'))`.  Did I miss something?. ",
    "taion": "@vphilippon Would you consider a version of #169 updated again for the current master? Or at least the part the resolves the commit for editable VCS URLs?. :+1:\nI was unpleasantly surprised when I ran pip-sync with a global pip-tools install.\n. If you're going to vendor pip, then there's no actual downside to calling into _internal in your vendored pip, since those imports won't change under you anyway.\nInstead of vendoring pip==9.0.1, then, I think it might be better to vendor pip==10.0.0 (when it comes out), and just update the imports to pip._internal (as appropriate for the vendored version).\nAt a glance, it didn't look like the internal organization changed meaningfully, but I guess we can verify that empirically.. Fair enough! I guess either way, it moves \"upgrading to Pip 10\" to something that can be handled via a PR, in case anybody needs the changes.. Yeah, TensorFlow's packaging is a little weird. What this ends up looking like is that we logically want to specify something in requirements.in like:\ntensorflow-gpu==1.3.0; 'linux' in sys_platform\ntensorflow==1.3.0; 'linux' not in sys_platform\nBut pip-compile then fails on OS X, because there's no tensorflow-gpu==1.3.0 there.. There is no tensorflow-gpu==1.3.0 at all for OS X on PyPI, so something weird happens. The second line works, though (at least with Pipenv).\nFrom this and other issues on Pipenv, this isn't really addressable without some even more invasive hackery, so this is probably a CANTFIX.\nI might poke around at this a bit on my own but it's not immediately obvious that there's a solution here that isn't ridiculously gnarly.. Hah, not a problem. Here's what happens:\n```\n$ cat requirements.in\ntensorflow-gpu==1.3.0; 'linux' in sys_platform\n$ pip-compile --version\npip-compile, version 1.10.2\n$ pip-compile --rebuild --verbose\nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  tensorflow-gpu==1.3.0\nFinding the best candidates:\n  found candidate tensorflow-gpu==1.3.0 (constraint was ==1.3.0)\nFinding secondary dependencies:\n  tensorflow-gpu==1.3.0 not in cache, need to check index\nCould not find a version that satisfies the requirement tensorflow-gpu==1.3.0 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0)\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 722, in call\n    return self.main(args, kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 102, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 199, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 285, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/repositories/pypi.py\", line 152, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 554, in _prepare_file\n    require_hashes\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 278, in populate_link\n    self.link = finder.find_requirement(self, upgrade)\n  File \"/usr/local/lib/python3.6/site-packages/pip/index.py\", line 514, in find_requirement\n    'No matching distribution found for %s' % req\npip.exceptions.DistributionNotFound: No matching distribution found for tensorflow-gpu==1.3.0\n```\npip-tools can deal with the package itself just fine, but it fails when it tries to grab the package to resolve dependencies.. It's the same sort of problem as https://github.com/kennethreitz/pipenv/issues/857, though the same problems there don't come up given that pip-tools itself runs in the virtualenv rather than outside of it.\nOne mitigation in this case could be that, for packages that do upload their dependencies to PyPI (are these the packages that use twine?), we just use the stated dependencies from PyPI rather than download the package to resolve it.\nThis wouldn't solve the problem in full generality, but it would fix things for e.g. tensorflow-gpu. This would also fix @mpolden's specific problem in https://github.com/kennethreitz/pipenv/issues/857, actually, since APScheduler does in fact publish its install requirements to PyPI, though again it wouldn't fix the general case.\nThough frankly Pipenv is a bit of a no-go for us anyway due to https://github.com/kennethreitz/pipenv/issues/966.. It's on the JSON payload. See info.requires_dist:\n- https://pypi.python.org/pypi/tensorflow-gpu/1.3.0/json\n- https://pypi.python.org/pypi/APScheduler/json. I'm not sure if this API really lets you distinguish between \"no dependencies\" and \"dependencies not published to PyPI\", though. Maybe not that important in practice.. Ah, I see it's not so straightforward in the code given how you hook into RequirementSet from Pip to do the lookup.. I think there's still the problem that we literally can't evaluate the transitive dependencies for a package that we can't install/download, though.\nThe bottleneck here isn't really the evaluation \u2013 it's that unless we try to read the deps from the PyPI API (instead of using pip's approach), we don't have a way to get transitive deps at all for non-installable packages.. Oh, hey, scikit-learn[alldeps]! Adding that was probably among the least favorite PRs I've ever made \ud83e\udd23 \nSo, that does work, but it's not exactly what I want. Ideally, I'd like for this package (and its exclusive dependencies) to show up in my generated requirements.txt, with the appropriate markers.\nImagine I started with:\ntensorflow-gpu; 'linux' in sys_platform\ntensorflow; 'linux' not in sys_platform\nI'd want something like:\nbleach==1.5.0             # via bleach, tensorflow-tensorboard\nenum34==1.1.6             # via enum34, tensorflow\nhtml5lib==0.9999999       # via bleach, html5lib, tensorflow-tensorboard\nmarkdown==2.6.9           # via markdown, tensorflow-tensorboard\nnumpy==1.13.3             # via numpy, tensorflow, tensorflow-tensorboard\nprotobuf==3.5.0.post1     # via protobuf, tensorflow, tensorflow-tensorboard\nsix==1.11.0               # via bleach, html5lib, protobuf, six, tensorflow, tensorflow-tensorboard\ntensorflow-gpu==1.4.0; 'linux' in sys_platform\ntensorflow-tensorboard==0.4.0rc3  # via tensorflow, tensorflow-tensorboard\ntensorflow==1.4.0; 'linux' not in sys_platform\nwerkzeug==0.12.2          # via tensorflow-tensorboard, werkzeug\nwheel==0.30.0             # via tensorflow, tensorflow-tensorboard, wheel\nFor carrying through dependencies transitively, suppose I had:\nsix\ntensorflow; 'linux' not in sys_platform\nThen I would want something like:\nbleach==1.5.0; 'linux' not in sys_platform\nenum34==1.1.6; 'linux' not in sys_platform\nhtml5lib==0.9999999; 'linux' not in sys_platform\nmarkdown==2.6.9; 'linux' not in sys_platform\nnumpy==1.13.3; 'linux' not in sys_platform\nprotobuf==3.5.0.post1; 'linux' not in sys_platform\nsix==1.11.0\ntensorflow-tensorboard==0.4.0rc3; 'linux' not in sys_platform\ntensorflow==1.4.0; 'linux' not in sys_platform\nwerkzeug==0.12.2; 'linux' not in sys_platform\nwheel==0.30.0; 'linux' not in sys_platform. Yeah, giving Python packaging limitations this does make sense.\nThis is incompatible with checking in requirements.txt, though, right?. It's tricky, though. For example, we mostly develop on OS X, but our CI runs in a Linux Docker container.. Nothing \u2013 just running pip compile on OS X gives good-enough results. Not perfect, but even when there are mismatches, it\u2019s good enough. This may well be due to specific pecularities with our setup, though (we now get tensorflow-gpu from the Docker image whenever we run on Linux, so we never need to put it in our requirements files).. To be more concrete, now that I'm at a laptop, in the one repo where this even comes up for us, we do:\ntensorflow; 'linux' not in sys_platform\nOn Linux we use a Docker image with TensorFlow pre-installed.\nIt so happens to work, but it's obviously far from perfect.\nIn practice, though, with the frequency with which people develop on OS X but deploy on Linux, it's not clear to me that the recommendation here taken literally would improve DX.\nReally this is the fault of Python packaging; it's not clear there's much that's possible here. I would soften this to \"may want to\". I would imagine the vast majority of developers these days are using flows where they develop on OS X but deploy on Linux.\nWhile it's technically true that there may be issues, it's very, very uncommon for these issues to actually arise. We have a pretty broad array of web, data engineering, data science, and machine learning projects, and this has only come up for a single project for us \u2013 and it's a project that has a sufficiently complicated deploy routine that we're definitely in a \"warranties are void\" place.\nI would instead downgrade this to something like \"what if my dependencies behave differently on different platforms?\".\nWhile in principle this can be a problem whenever people work across multiple platforms, in practice it's very seldom an actual problem.. I've seen (and contributed) to a number of those issues.\nOn OS issues, my impression and experience is that they come from a small, vocal minority. If there's an overwhelming majority of users for whom everything \"just works\", you're unlikely to hear from them. My impression is that the majority of users of pip-tools (and Pipenv) are more in the web dev branch of Python development.\nIn those cases, where you're mostly just spinning up a Django or Flask app with attendant other dependencies, the OS compatibility issues really do almost never come up. And it really is ubiquitous in that environment to develop on OS X and deploy on Linux.\nPython version mismatches are terrible, though, and... yeah, those cases should call for different \"compiled\" requirements files.. ",
    "ssalonen": "Well actually this is good news...the rewrite implements almost everything in this pull request (like --pre, --index-url and relaxed version patterns).\nHowever, those were just nice-to-have features for me. What I was really looking for is the \"PinnedPackageManager\", which basically allows the user to have a pin list which is used in the compilation phase. This is quite different from the pip tools compile behaviour which resolves against latest versions found in the pypi/repo. We are planning to use this kind of pinning functionality in our CI environment to test the package with different version sets (e.g. pkg1==1.0 & pkg2==1.2 and pkg==1.1 & pkg==1.3). \nDo you think this kind functionality would be suitable in pip-tools or should it be a separate? Actually when writing this I realized that something similar could be done using devpi perhaps...\nI'm happy to review the #169, hope that I have some time next week.\n. Had a look at the commits, I'm afraid I don't have enough in-depth understanding to go through all in detail but generally speaking seems OK to me.\n. ",
    "phongvcao": "Okay nevermind I found the answer. The /usr/bin/pip-review file itself is a Python script compatible with both Python 2 and 3. Thus, to update Python 2 modules under python2.7/site-packages, I issue the command:\nsudo python2 /usr/bin/pip-review --interactive --auto\nand to update Python 3 modules under python3.4/site-packages (Arch Linux):\nsudo python /usr/bin/pip-review --interactive --auto\n. ",
    "bh": "Any news here? I need this :-)\n. :+1: for merging this PR!\n. ",
    "mkleehammer": "Any movement on this?  I could really use this.\n. I found the same error and it has to do with the contents of my existing requirements.txt.  I found this out when I tried to reproduce it for you and found that it doesn't fail if I copy the file to a different name - the output file doesn't exist in that case.\nThe line that causes the error is:\ngit+https://github.com/mkleehammer/sourcemap#1.0.1\nThis is a module I needed that isn't done well enough to put on pypi.  I have not yet looked into making this work with pip-compile, so any insights into this would be good too.\n. ",
    "voidlily": "I'd like to see this as well.\n. ",
    "jamesob": "\nyou can try submitting a smaller PR against the latest master\n\nlol 144 line diff, 42 lines of which are tests. I'm curious what \"small\" looks like @nvie .\n@blueyed you should consider forking in a more official capacity -- but as is, we'll consider using your fork!. ",
    "grigi": "XDG_CACHE_HOME is defined by freedesktop.org: http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html\nXDG_CACHE_HOME is what a unix-login users cache-directory should be. pip itself uses it, and is generally considered a good place to put things. IOW, I can setup a server and only provide it one general cache directory with correct permissions.\nSince pip itself uses it, you can expect it to be defined wherever pip-tools are used.\n. Hmm, valid point:\nhttps://github.com/pypa/pip/blob/develop/pip/utils/appdirs.py#L13\nThat should be it.\n. I don't see how the -r is less likely to cause inconsistencies than importing many files.\nIt feels very natural that it should accept a list of requirements files. I consider it important for usability.\n. Could pip list -o work for you?\nElse, have a look at piprot?\nhttps://github.com/sesh/piprot\n. Hi @nvie \nUnfortunately pip-tools==1.1.1 doesn't resolve this.\nMinimal test case (works in both py2.7 and py3.4)(pip==7.1.0 used):\ncreate & activate empty virtualenv:\n$ echo pip-tools > requirements.in\n$ pip install pip-tools\n$ pip-compile\n```\n\nThis file is autogenerated by pip-compile\nMake changes in requirements.in, then run this to update:\n\npip-compile requirements.in\n\nclick==4.1                # via pip-tools\nfirst==2.0.1              # via pip-tools\npip-tools==1.1.1\nsix==1.9.0                # via pip-tools\n```\n$ pip-sync\nRequirement already satisfied (use --upgrade to upgrade): six==1.9.0 in ./lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): click==4.1 in ./lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): first==2.0.1 in ./lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): pip-tools==1.1.1 in ./lib/python2.7/site-packages\n. @nvie It works :+1: \nThanks for such a great tool :-)\n. ",
    "smason": "I've been putting off learning virtualenv for too long, looks like it's time to take the plunge\u2026\n. Alexander Artemenko wrote:\n\n@smason Sam, virtualenv is must have for python devlopment. I'm\nwincing remembering days when virtualenv didn't exis\n\nPackage/version dependencies haven't been much of an issue for\nme\u2014but then again, I've generally stuck with Python's standard\nlibrary\u2026\nSam\n. ",
    "lsmag": "This is also useful when I have multiple requirements including base.txt (development.txt,test.txt,production.txt). Right now when I change base.txt I must regenerate all dependant files.\n. ",
    "pmdarrow": "@nvie is there any interest in a PR for this?\n. ",
    "hugopeixoto": "Thank you for giving this a try :)\nI was testing with python 3.4 and pip 7.1.0. I see that you are using python 2.7.x.\nCan you tell me which version of pip do you have installed?\nThat's probably the cause of the problem. I didn't test for 2.x.\nIf you tell me which version of pip you're using I'll try to figure out a way to do it in a more compatible way.\n. It seems that SpecifierSet.__contains__ (which allows us to do version in specifier) was only added in pip 7.0.0. I changed it to specifier.contains(version), which was added in pip 6.0. This should fix the issue.\n. You could add a line to dev-requirements.in with -r requirements.txt and run the following:\npip-compile requirements.in\npip-compile dev-requirements.in\npip-sync dev-requirements.txt\nOne could change pip-sync to accept a list of requirements file instead of just one file, which would make it possible for you to run pip-sync *requirements.txt, but then pip-sync would have to have a mechanism of detecting inconsistencies between files.\n. Regarding the uninstallation of self:\nsync.EXCEPTIONS already includes pip-tools, which solves half of the problem.\nHere are a couple of ideas on how to handle dependencies (some of them are unfeasible, but I'm listing them anyway):\n1. Remove all dependencies from pip-tools (probably not a good idea)\n2. Vendor all the dependencies (which is something pip does)\n3. Hardcode all dependencies in sync.EXCEPTIONS (might become cumbersome to maintain)\n4. Create the dependency tree at runtime for all installed exceptions based on the current set of installed modules, and whitelist them\nBoth (2) and (4) are solutions that seem feasible, but (4) is probably the way to go.\n. @Koed00 and @grigi, could you try out the PR and see if it works for your cases?\n. You're right. It seems that when I specify nargs=-1, click ignores the default value. Either I'm doing it wrong, or it may be a bug.\nIf anyone can spot anything that I'm doing wrong there, let me know.\nMeanwhile, I added an extra check, manually checking for no arguments.\n. ",
    "Koed00": "This is a somewhat bigger problem; pip-tools is pinned in my dev-requirements.txt and not in requirements.txt. Therefore pip-sync will actually uninstall pip-tools own requirements like click and first.\n. pip-sync requirements.txt dev-requirements.txt works great. It neatly cleans up my env. This has already saved me over 10% in instance sizes btw.\npip-sync with no arguments just uninstalls everything except pip-tools and its requirements.\nMaybe it should default to looking for requirements.txt at the least.\n. ",
    "luzfcb": "Thanks :+1:\n. @nvie yes,\n. I just saw you updated the version in pypi. I'll test again and inform you in a few minutes\n. ",
    "gavinjackson": "Thanks, that's great!\n. ",
    "pquentin": "This means we no longer have a replacement for pip-review --interactive. The rationale for removing pip-review is that pip-compile + pip-sync are enough, however that means that we need to use pip-compile, something I'm not yet ready to do.\n@nvie Is there any hope to get pip-review back, eg. \"patches welcome\"? Or is it decidedly incompatible with the vision?\n. The workflow really is \"interactively update a pinned requirements.txt file\". If pip-compile --interactive could do it, I'd be fine with it.\nI think I'll end up using requirements.in at some point, so if nobody else wants interactive review, it's possibly not worth it.\n. Thank you!\n. Thanks for the pointer. However, our issue is about with pip-compile, not pip-sync, so I don't think this could be the issue?\n. It's also possible that something else changed when we switched versions. If we continue to get hit by this, I'll try to make sure the issue is reproducible.\n. Sorry @nvie I thought I had closed the issue, because I can't reproduce this anymore. I'll reopen if needed. It does work with pip-tools 1.6. (1.6.1 does not appear to exist yet.)\nThanks for the follow-up, it's much appreciated!\n. ",
    "nvenkataraman1": "I use pip-review to keep my virtual environment packages up to date and not to manage packages for a particular project. I don't really have a need to pin my packages and don't find pip-compile helpful since I'm not tied in to a specific package version / upgrade path.\nCould you bring back pip-review for use cases, like mine, where we are purely looking to upgrade all packages without pinning them? Thanks!\n. pip list --outdated doesn't upgrade packages, though, does it? We still need to upgrade them one at a time. With pip-review --interactive or auto, I can instruct the process to upgrade all packages at once.\n. ",
    "spout": "+1, was using pip-review to upgrade packages easily.\n. Script with current/latest version:\n``` bash\n!/bin/bash\nwhile read line; do\n    test -z \"${line}\" && continue;\n    echo ${line};\n    pkg=$(echo $line|cut -f 1 -d' ');\n    echo -n \"Upgrade now? [y/n]: \";\n    read answer </dev/tty;\n    test \"${answer}\" == \"y\" && pip install -U ${pkg};\ndone< <(pip list --outdated)\n```\n. ",
    "fidelleon": "I'd be more than happy having pip-review back :(\n. Before:\n$ pip-review --interactive\nYes/No/All/Quit * 12\nAfter:\n$ pip list --outdated\n[list of 12 pip outdated packages]\n$ pip install -U package1\n$ pip install -U package2\n...\n$ pip install -U package12\n. That's what I was thinking as a substitute - point.\n. ",
    "joernhees": "@nvie i guess this is mostly a problem of handling deprecation which impacts peoples' workflows... this thread is pretty hidden and so most developers who update pip-tools and used pip-review before will suddenly think \"wat?\"...\nslowly deprecating it with a warning would've been a bit nicer, but seems it's too late for that now... maybe you could include a few words about pip-review being dropped in favor of pip list --outdated or even the script? That would be quite easy for you and save many people some time i guess...\n. thanks...\n. ",
    "outime": "It'd be great to have a note in the README about pip-review being removed and the alternative.\n. ",
    "bnice5000": "I am a little confused and maybe the is not the place to voice this concern. My main use of pip-tools was to keep my cutting edge virtualenv up to date with the latest packages. The removal of pip-review has broken this workflow.\nAn additional note, how does the new system in anyway differentiate itself from the python recommended method of pip install -r requirements.txt -U. If I now have to keep another requirements.txt for my bleeding edge virtualenv, why would I use an additional third party library?\nI am really unsure as to what benefit the refactoring of this package brings over what pip already offers?\n. ",
    "vinitkumar": "In case, you want to update all at once without having to press y all the time (mostly if your project has lots of dependencies, use this:\n``` sh\n!/bin/bash\nfor pkg in $( pip list --outdated | cut -d' ' -f 1 )\ndo\n    echo $pkg\n    pip install -U $pkg\ndone\n```\n. ",
    "bbirand": "Sorry to bring this issue back up, but I did find a use case for which pip list --outdated is not great. Form what I understand, the main point of separating the requirements into the .in and the .txt file is so that we have a more maintainable file. I keep all my pinned dependencies in requirements.in. \nWhen I want to update the dependencies of my project, I only want to see the newer versions of the packages that are listed in requirements.in, not of every one installed in the environment. The assumption is that I can go over upgrading the main ones, and let pip figure out the necessary upgrades to all the packages.\nDo this make sense? Is there a way of using the tools that are in the this project to address this point?. Not quite, because I'd like to see the packages that needs to be updated, and manually update them. My understanding of pip-compile --upgrade is that it directly updates everything to the latest version, right?. @jgonggrijp Thanks for sharing your workflow. What you are saying makes a lot of sense. I think I initially had only the package names in my requirements.in. At some point, I added a single new package to my  requirements.in, ran pip-compile, and saw a whole of changes (that I didn't want to deal with then). This is when I started also pinning some of the packages in requrements.in. \nBut I think I found a solution to the above problem. I also use piprot, and run piprot requirements.in to show me the outdated files. I can then selectively decide to update packages as I see fit, by making manual changes. For instance, I'd like to immediately implement a minor update to psycopg2, while updating from django 1.9 to 1.10 is a bigger task.\nI wonder if this breaks some patterns that pip-tools is supposed to address?\n. ",
    "daverck": "pip-review is still a nice tool to upgrade python package system wide but it will stop if a package fail to install.\nI recommand pipdate instead : https://github.com/nschloe/pipdate. ",
    "stlehmann": "@nvie: You' re right. This is a bug in the boltons library. I' ll address it there and be back with any feedback later on.\n. Allright. Just made contact to the author of boltons. The issue was known and has been fixed but not yet rolled out on PyPi. The version with the fix will be 0.6.6.\nThe current version with the fix can be installed via pip by:\npip install -e git+https://github.com/mahmoud/boltons.git#egg=boltons\n. Great work. Now it compiles :+1: \n. I like the idea of Pipenv. However, after using it a lot I switched back to pip and pip-tools. Mainly because of these reasons: \n\nPipenv is very slow mainly because of the hash functionality\nThe update procedure for packages is somehow obscure. I couldn't figure out how to update specific packages in a clean way\nInconsistencies when moving to different operating systems due to differing hashes\n\nSo I think pip together with pip-tools is a great and transparent way for package management and it should definetely stay.. ",
    "tonyseek": "@nvie Could you review/merge it?\n. Thank you! :smile:\n. It seems the problem of appnope is not fixed by #460 actually. There is an issue #563 which is discussing about this.. @Groxx @davidovich Thanks for your review. The branch is up-to-date now and the revision is pushed.. agree and done. ",
    "chrisbarr": "+1 - this would be really useful for us :)\n. ",
    "Chive": "Seems like it puts packages into ~/.cache/pip-tools/\n``` bash\nroot@9edd2e5e9f0c:~# dir -R ~/.cache/pip-tools/\n/root/.cache/pip-tools/:\ndepcache-py2.7.json  pkgs  wheels\n/root/.cache/pip-tools/pkgs:\ntabulate-0.7.5.tar.gz\n/root/.cache/pip-tools/wheels:\naldryn_client-2.0-py2-none-any.whl  requests-2.7.0-py2.py3-none-any.whl\n```\nYou can reuse that during pip install:\n\nthis command works without internet connection\n\nbash\nroot@9edd2e5e9f0c:~# pip install --no-index --find-links=~/.cache/pip-tools/pkgs --find-links=~/.cache/pip-tools/wheels -r requirements.txt\nIgnoring indexes: https://pypi.python.org/simple\nCollecting aldryn-client==2.0 (from -r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): click==5.1 in /usr/local/lib/python2.7/site-packages (from -r requirements.txt (line 2))\nCollecting requests==2.7.0 (from -r requirements.txt (line 3))\nCollecting tabulate==0.7.5 (from -r requirements.txt (line 4))\nInstalling collected packages: tabulate, requests, aldryn-client\nSuccessfully installed aldryn-client-2.0 requests-2.7.0 tabulate-0.7.5\n. Heads up @stefanfoulis, this was changed in pip-tools 1.0.5 https://github.com/nvie/pip-tools/blob/master/piptools/locations.py#L8\nOn Linux, it might be ~/.cache, but on OS X (at least for me) it's /Users/<username>/Library/Caches/pip-tools\n. ",
    "ariscn": "Bump, would love to see this functionality.\nEdit: Actually, it looks like pip-sync is bugged. Given a file with a -e pin to a Git commit, pip-sync is syncing the PyPi version instead.\n. ",
    "danni": "This feature would be super useful when building with Docker also because if the commit hash doesn't change then the Python requirements cached layer won't invalidate.\n. ",
    "jdufresne": "This looks like a duplicate of #161. This looks like a duplicate of #353. --index & --no-index could continue to exist as aliases that also issue a warning when used. That way, existing code won't break. The hope is people will notice a need to change before an eventual backwards compatibility breakage.. Having investigated this further, I see this happens because I'm not passing the --upgrade option. So, maybe this is intended behavior and not a bug.\nI guess I'm confused why the output file should ever influence how pip-compile works. I find that very confusing.\nI see this area of the code.\nLooks like upgrading was changed to opt-in in commit 3a2de926a1e247f9905f1dc30ba3af5e4574c77e. So this is just a misunderstanding.. > As for the capitalization problem. Do we want to just normalize to lowercase? This is easy to implement. Or do we want to keep the original capitalization? This is a little more work, since the original capitalization needs to be stored in the cache.\nI recently noticed this change while working on master. To be honest, I was a bit surprised by this and thought there was a new bug converting \"Django\" -> \"django\". In my opinion, I think a project's specified name on PyPI should be preserved as-is instead of trying to force it to some other convention. I understand .lower() is easier than some other solution, but it seems odd to choose a name other than what a 3rd party package has specified.. When reviewing the output of pip-compile, the fewer name transforms and mangling, the easier it is to check for correctness and compare to a verifiable source, which I see as a very important feature of pip-compile. A particular capitalization is how a 3rd party package chose to register and spell their package. We should honor that.. > The problem exists in the tooling around pip\nAre you aware of any upstream issues, PRs, or commits that identify or approach this problem? If so I'd like to track the issue there as well. I was unable to find any.. I agree that while name transforms may not be ideal, lower casing is the best compromise given the limitations of upstream tools. Without the hard work to get to the root of the issue, I see no reason to have this hold back a release. Thanks for hearing me out and the informed thoughtful responses.. Sure, I can revisit this.\nIt occurred to me that this needs to happen recursively. For example, packages included by packages included by setuptools should also not be included in the requirements. Until that is fixed, this is not ready for merge. I'll take a look at that problem once resolving the merge conflict and Travis build.. The updated PR is ready for review. I have:\n\nRebased to resolve merge conflicts\nReworked logic to run at resolve time instead of output time\nAs a result, we don't need allow_unsafe at output time which allowed some removal of dead code\n\nJust a thought, is there a good use case for allow_unsafe? I was curious about removing it entirely, but I'm not sure if the backwards compatibility would be a problem for some.. @davidovich I have updated the test fixture to include appdirs and packaging as dependencies of setuptools. In test_resolver.py, these dependencies are not included in the resolved set of the the html5lib test case. I believe this demonstrates what you're asking for.\nLet me if you had something else in mind. If so, could you provide a bit of guidance as to what the test might look like? I'd be happy to write additional tests, just want to make sure I'm adding what you're looking for.. Apologies if I have introduced a regression.\n\nFixed bug where unsafe packages would get pinned in generated requirements files when --allow-unsafe was not set\n\nDo you have additional details on this? I do not use the --allow-unsafe option and I do no have pinned unsafe packages in my output. So I'm curious how you triggered this. Do you have an example, minimal requirements.in? What is the command line you're using?\nHere is the command I use:\n$ ./venv/bin/pip-compile --version\npip-compile, version 1.9.0\n$ pip-compile --upgrade --generate-hashes -o requirements.txt requirements.in\n...\n[no unsafe packages]. If I run with --allow-unsafe setuptools appears in my requirements.txt as: \n```\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools==35.0.2 \\\n    --hash=sha256:19d753940f8cdbee7548da318f8b56159aead279ef811a6efc8b8a33d89c86a4 \\\n    --hash=sha256:1e55496ca8058db68ae12ac29a985d1ee2c2483a5901f7692fb68fa2f9a250fd \\\n    # via html5lib\n```\nAre you saying this behavior is incorrect? setuptools should not have a pinned version? If that is the case, what is the expected output?. > But if I run pip-compile --no-index -o requirements.txt requirements.in pip also gets pinned when it shouldn't.\nThanks for following up. Can you provide the requirements.in file that triggers this? Or a minimal version of it?. > pip==9.0.1\nI'm having trouble understanding your use case. Can you expand on it?\nWhy do you put pip in your requirements.in if you don't want it in your requirements.txt?\nWhat is the point to putting pip in a requirements file anyway. As you use pip to install requirements, it won't be updated until after it is invoked.. WIth 4e2f928902ac30878b4e989c843d00fa07085c4c merged, should this now be closed?. I like the idea of moving away from click and towards using the stdlib for logging. In doing so, can we avoid introducing a new dependency, coloredlogs? It seems as pip-tools is a very low level tool used to build projects instead of consumed directly by projects it should strive for a minimal number of dependencies.. Thanks for the analysis and reasoning. I agree that 1 & 4 are definitely not wanted. Your considerations of the trade offs are very fair.. Maybe it would be acceptable to just not have colored logging? What are your thoughts?. > there were several comments on the last pull request that outlined why this is a valid use case. The entire point of the flag is to pin these dependencies as noted in the help output. It does not say that this is only for transitive dependencies.\nIMO, the use cases were stated but never adequately explained. They mostly boiled down to \"we do this\" without explaining the why. I think if someone that uses this feature explains why this is critical to their development workflow, it would be easier to understand how best to review these PRs in order to help everyone.. Any idea why using editable packages fail on Windows with Python 2.7 with the error: \nWindowsError(123, 'The filename, directory name, or volume label syntax is incorrect')\nI do not have a Windows box handy to easily test.. Rebased but and still receiving appveyor only issues. The error is different tho, I now see:\nException ignored in: <finalize object at 0x11a3530; dead>\nTraceback (most recent call last):\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\weakref.py\", line 519, in __call__\n    return info.func(*info.args, **(info.kwargs or {}))\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\tempfile.py\", line 698, in _cleanup\n    _shutil.rmtree(name)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 482, in rmtree\n    return _rmtree_unsafe(path, onerror)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 372, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 372, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 372, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 372, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 377, in _rmtree_unsafe\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"C:\\projects\\pip-tools\\.tox\\py34-pip8\\lib\\shutil.py\", line 375, in _rmtree_unsafe\n    os.unlink(fullname)\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\appveyor\\\\AppData\\\\Local\\\\Temp\\\\1\\\\tmp7ih6qhnesource\\\\django\\\\.git\\\\objects\\\\pack\\\\pack-3e97656eababafe662700fb821cea4092a0ee735.idx'\nLooks like appveyor is failing to clean a temporary directory. I'm unable to investigate this as I don't have a Windows box to easily test with. Any thoughts?. Updated PR.\n\nRebased on latest master\nDropped changelog entry\nSimplified commit message to one line\n\nThanks. Ready for review.. Thanks for the investigation! I really appreciate it. I've made the suggested updates:\n\nRemoved test_editable\nUse in instead of ==. Duplicate of #569 which has been handled and released.. Can someone provide an easy to follow bash script to reproduce?\n\nHere is what I'm running, but it works fine. I'm first clearing all caches. The requirements.txt contains only the platform dependencies. pip install -r requirements.txt executes successfully.\nHere is my test script:\n```bash\n!/bin/bash\nset -x\ncat requirements.in\nrm -rf ~/.cache/pip\nrm -rf ~/.cache/pip-tools\nrm -f requirements.txt\nrm -rf venv\npython3 -m venv venv\nvenv/bin/pip install --upgrade pip setuptools\nvenv/bin/pip install pip-tools\nvenv/bin/pip-compile -o requirements.txt requirements.in\nrm -rf venv\npython3 -m venv venv\nvenv/bin/pip install --upgrade pip setuptools\nvenv/bin/pip install -r requirements.txt\n```\nHere is the output running this. Commands are prefixed with a \"+\":\n```\n+ cat requirements.in\ndjango-auth-ldap\n+ rm -rf /home/jon/.cache/pip\n+ rm -rf /home/jon/.cache/pip-tools\n+ rm -f requirements.txt\n+ rm -rf venv\n+ python3 -m venv venv\n+ venv/bin/pip install --upgrade pip setuptools\nRequirement already up-to-date: pip in ./venv/lib/python3.6/site-packages\nCollecting setuptools\n  Using cached setuptools-36.5.0-py2.py3-none-any.whl\nInstalling collected packages: setuptools\n  Found existing installation: setuptools 36.2.0\n    Uninstalling setuptools-36.2.0:\n      Successfully uninstalled setuptools-36.2.0\nSuccessfully installed setuptools-36.5.0\n+ venv/bin/pip install pip-tools\nCollecting pip-tools\n  Using cached pip_tools-1.10.1-py2.py3-none-any.whl\nCollecting six (from pip-tools)\n  Using cached six-1.11.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools)\n  Using cached click-6.7-py2.py3-none-any.whl\nRequirement already satisfied: setuptools in ./venv/lib/python3.6/site-packages (from pip-tools)\nCollecting first (from pip-tools)\n  Using cached first-2.0.1-py2.py3-none-any.whl\nInstalling collected packages: six, click, first, pip-tools\nSuccessfully installed click-6.7 first-2.0.1 pip-tools-1.10.1 six-1.11.0\n+ venv/bin/pip-compile -o requirements.txt requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\ndjango-auth-ldap==1.2.15\ndjango==1.11.5            # via django-auth-ldap\npyldap==2.4.37            # via django-auth-ldap\npytz==2017.2              # via django\n+ rm -rf venv\n+ python3 -m venv venv\n+ venv/bin/pip install --upgrade pip setuptools\nRequirement already up-to-date: pip in ./venv/lib/python3.6/site-packages\nCollecting setuptools\n  Using cached setuptools-36.5.0-py2.py3-none-any.whl\nInstalling collected packages: setuptools\n  Found existing installation: setuptools 36.2.0\n    Uninstalling setuptools-36.2.0:\n      Successfully uninstalled setuptools-36.2.0\nSuccessfully installed setuptools-36.5.0\n+ venv/bin/pip install -r requirements.txt\nCollecting django-auth-ldap==1.2.15 (from -r requirements.txt (line 7))\n  Using cached django_auth_ldap-1.2.15-py3-none-any.whl\nCollecting django==1.11.5 (from -r requirements.txt (line 8))\n  Using cached Django-1.11.5-py2.py3-none-any.whl\nCollecting pyldap==2.4.37 (from -r requirements.txt (line 9))\nCollecting pytz==2017.2 (from -r requirements.txt (line 10))\n  Using cached pytz-2017.2-py2.py3-none-any.whl\nRequirement already satisfied: setuptools in ./venv/lib/python3.6/site-packages (from pyldap==2.4.37->-r requirements.txt (line 9))\nInstalling collected packages: pytz, django, pyldap, django-auth-ldap\nSuccessfully installed django-1.11.5 django-auth-ldap-1.2.15 pyldap-2.4.37 pytz-2017.2\n```. > I seems like Py2 has no issue with that and falls back to a default '<', while Py3 raises a TypeError instead (I don't know a lot about Py2 vs Py3).\nFYI, This change in Python3 comparison behavior is described here: https://docs.python.org/dev/whatsnew/3.0.html#ordering-comparisons. Worked for me! Thanks for the quick fix.. > Nevertheless, don't get me wrong, but I think this is still quite hacky, and I don't see the point. Why don't you just revert the commit 4900c7c which is causing this? Why would I ever want hashes of non-compatible wheels to my requirements.txt?\nA lot of this is described in the commit message and issue #520. But I'll summarize.\nI work on a project that has a diverse group of environments:\n\nDifferent Linux distros\nMacOS\nAll Pythons are 3.4+ but could be on a different version\n\nMy Project is Python 3 only. For security purpose, I want to generate package hashes for some additional verification that the download is correct.\nIn 1.9.0, when generating requirements.txt on one machine it might fail when running on another machine. It failed because the hashes generated were not compatible with the running OS & Python\nversion. As I saw it, I had some options:\n\nUpdate pip-tools to generate all hashes \nStop generating hashes\nUnify all developers on one platform\n\nI saw 1 as the best tradeoff at the time (and still do).\n\nIf you're trying to generate a requirements.txt which works for all platforms and Python versions, then this would make sense, but since the dependencies aren't calculated for all platforms, but only for the current one, the requirements.txt doesn't work for another platform anyway. Creating a requirements.txt which works for all platforms is requested in #563 and if that is ever implemented this hack would cause more harm than good.\n\nMaybe it is just \"luck\" of my dependencies, but I'm not seeing an issue regarding transitive packags in my project.\n\nThe whole point why I want locked requirements is confidence and having platform independent requirements.txt doesn't help there.\n\nI understand. But with hashes specific to binary platforms, this means the requirements.txt is only useful for identical environments to which it was generated. This seems limiting to me.\n. > If it had been impossible (or way too hard) to keep the feature without breaking the dependencies resolving, I would have suggested to revert of course.\n\n@suutari-ai I think a case justifying the feature has been provided. If you have a case or example that would prove this feature to be harmful to the users, outweighing the gain, please provide it.\n\nI think this is very fair. If an argument or evidence can be provided to show that this will be too difficult to maintain or is breaking previously working environments. Then I agree, reverting the feature is justified.. > Unify all developers to use one of N pre-specified platforms and generate a requirements-{platform_tag}.txt for each supported platform\nThis seems like a mess to manage, honestly.\n\nLet CI and deployment scripts use requirements.txt, but developers on a different platform use requirements.in.\n\nA problem with this is the requirements in requirements.in aren't pinned. So different development environment will be using different versions of the dependencies, causing subtle behavior differences.\nEasily pinned dependencies was the killer feature to adopt pip-tools in the first place.\n\nFor me the 'generate hashes for all platforms' feature is more harmful than useful, because I don't want hashes of another platform to my txt files.\n\nHmm, this is interesting to me. @suutari-ai I think you make a convincing argument. I may be persuaded that generating all hashes was a bad idea for a requirements.txt that will be used on a production server. I honestly don't know what is best at this point. If there is large agreement to revert the feature, I won't stand in the way.. That makes sense to me. So, IIUC, pip-sync's job would be to:\n\nRemove all extraneous packages installed to site-packages\nRerun pip install -r requirements.txt to install all missing packages\n\nYeah, I like that.. Thanks for the review. I've made suggested edits.. Thanks for investigating. I've updated the vendored files and README.rst. Tests are now green.. Rebased to latest master with the pip10 work. This PR now only affects the README.. Thanks @atugushev !. This is displaying as, ['distribute', 'pip', 'setuptools'] where previously it was displaying as pip, setuptools & distribute. Did you intend to include the full repr value of the list? Maybe do the following instead:\npython\n', '.join(sorted(UNSAFE_PACKAGES)). Just my personal preference, the commas are more readable than the repr. I'm open to other people's preference if they'd like to see something else.. Typo platoforms -> platforms (multiple docstrings). I would wrap this in a try/finally in case an exception is raised and then caught somewhere else in the stack.\npy\ntry:\n    yield\nfinally:\n     Wheel.supported = original_wheel_supported\n     Wheel.support_index_min = original_support_index_min\n     self._available_candidates_cache = original_cache. Thanks for looking at the failure. I've made this change. However, it looks like tests are still failing.. You can use NamedTemporaryFile as a content manager to ensure it is always deleted when finished:\n```py\nwith tempfile.NamedTemporaryFile('w') as tmp_req_file:\n    tmp_req_file.write('\\n'.join(req_lines))\n    tmp_req_file.flush()\ncheck_call([pip, 'install', '-r', tmp_req_file.name] + pip_flags + install_flags)\n\n``. Literals are always slightly faster thandict()`:\npy\nireq_hashes = ireq.options.get('hashes', {}). This block of code looks extremely similar to _format_requirement in writer.py. Any chance this could be factored out to some common utility function?. I'm not sure I understand why this test was removed. Can you explain?. Is that a feature that is safe to remove? What happens to users using that feature? For example, the users from #493.\nThere are a number of areas that still reference editable requirements. If we do remove this, does that require cleanup too? Here is just one example:\nhttps://github.com/jazzband/pip-tools/blob/b558fc86fa128c1a19fc4efee37df4c5031eced8/piptools/sync.py#L74-L75. Oh! In the temporary requirements.txt, the requirement will include a -e, is that right? If so, got it. Maybe an explicit test for this case could be included alongside test_format_requirement_ireq_with_hashes.. ",
    "vphilippon": "See #161. @spookylukey Could you reproduce and link you pip/setuptools versions for reference please? that would be great. Thanks!. You're right: pip-sync doesn't respect environment markers.. Fixed by #600. This is from a long time ago, but still. I recently made a comment covering an issue in pipenv.\nThat comment a some information how the dependency resolution is done.\nhttps://github.com/kennethreitz/pipenv/issues/875#issuecomment-337717817\n. Should be fixed by #568, closing.. Environment markers from requirements.in are now preserved, giving a solution to deal with environment specific dependencies. Given that and that appnope is not an unsafe dependency, I'll close this.. Given how things evolved, I will close this.. This was a while ago. I'll close for now. If a similar issue still occurs with the latest version, fell free to reopen with your requirements.in/txt.. That's a different issue, yet a known issue that should be fixed in the upcoming v1.10. Could try it with the master branch (or wait for 1.10 to come out and then try it)?\nI'll reopen for tracking purposes.. pip-tools seems to be in the requirements.txt, pinned to 1.4.2. Either it got included by mistake by a previous pip-tools version, or it's in your requirement.in.\nSo\n1 - Make sure pip-tools is not in your requirements.in*\n2 - Re-run pip-compile to freshen your requirements.txt\n3 - Run pip-sync\n\nIf you actually want to have pip-tools in your requirements.in (and know what you're doing), then keep it, but run pip-compile --upgrade-package pip-tools, then run pip-sync.. Allrighty, I'm glad you're on the road.\nClosing. Has this been since in recent releases?\n\nNote to contributors: Try to reproduce by causing a failure with a git repo on pip's side.. @ryanhiebert Could you give me your upgrade script, as well as a requirements.in to reproduce?. Alright, closing this for now. If anyone comes up with this again, we'll reopen.. FTR, I stumbled on the cause of this in the code: The editable dependencies are never added to the dependency cache (which is ok, editable packages requirements are likely to change for a same version), and the OutputWriter uses the reversed dependency cache to generate the \"via\" comments.. Likely fixed in pip-tools 1.5 and pip 8.\nFeel free to reopen if needed.. Closing this, a lot has changed. Comment if this is still an issue.. Closing as there's no activity on this for ~1 year.\nFeel free to reopen if you need.. This seems to be related to an issue with paths/links when they appeared in the output file (i.e. requirements.txt). That was fixed by PR #485, which was merged in master.\nCould you try on your side and confirm if it fixed your use case?\nThanks!. @AlexandrPy Good to hear you got this fixed.\nSo for anyone else stumbling on this: Be sure to check your existing requirements.txt for any non-editable URLs.. (Catching up on stuff)\nAlright, so I assume everyone here has pip-tools installed in the root python environment, and not in a the project virtualenv?\nIf so, that's a recipe for disaster. There's a PR coming in to update the doc and state that (better late than never...), but you should install and run pip-tools' commands in and from your project virtualenv. Cross-virtualenv stuff is a mess.\nI hope that clarifies and helps!. @asmaps This is an issue that is fixed in master (cannot make release, check #531) where pip-compile fetches python 2 dependencies, even in a python 3 environment.\nFor the record, this issue was not in 1.9.0, so you can always downpin to that version for the time being (sigh)\n@pablote could you provide your pip-compile --verbose output. We'll confirm if its the same (or similar) issue.\nNote: If you decide to try with the code in master, be sure to run pip-compile --rebuild to clear the dependency cache, which could be corrupted due to the said bug.\n. This is now fixed in 1.10.2\nMake sure to clear you dependency cache: pip-compile --rebuild. Should be fixed: pkg_resources was added to PACKAGES_TO_IGNORE.. FTR, this is still a thing right now. I reproduce the test (with sliglty different commands):\n$ echo \"django-autocomplete-light[gfk]==3.1.5\" > test.in\n$ pip-compile -n --rebuild test.in\n=> the right dependencies are listed.\n$ pip install django-autocomplete-light==3.1.5\n$ pip-compile -n --rebuild test.in\n=> Missing dependencies.\nOne thing I noted though, with the -v output when django-autocomplete-light==3.1.5 is installed\n```\n$ pip-compile -nv --rebuild test.in\nUsing indexes:\n  https://pypi.python.org/simple/\n                      ROUND 1\n\nCurrent constraints:\n  django-autocomplete-light[gfk]==3.1.5\nFinding the best candidates:\n  found candidate django-autocomplete-light[gfk]==3.1.5 (constraint was ==3.1.5)\nFinding secondary dependencies:\n  django-autocomplete-light[gfk]==3.1.5 not in cache, need to check index\nNo handlers could be found for logger \"pip.req.req_set\"\n  django-autocomplete-light[gfk]==3.1.5 requires -\n\nResult of round 1: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file test.txt test.in\n\n--trusted-host pypi.rdv.geo.ubisoft.onbe\ndjango-autocomplete-light[gfk]==3.1.5\nDry-run, so nothing updated.\n``\nNote theNo handlers could be found for logger \"pip.req.req_set\"` line.\nSetting ignore_installed=True does solve the issue, but I'd like to understand why, and what the impacts are.\n@blueyed If you have some insight on what's going on behind the scenes, I would appreciate. I'm fully aware this was a long time ago, so if you've moved on to other things, I'd understand that as well.\n. @blueyed Thanks for the info, that'll help me out. Closing, #612 is the rebased version.\nThanks for your original PR, I'm sorry it stayed there for so long.. @amadornimbis It definitely needs a rebase on master. The version update in setup.py and CHANGELOG.md should be removed too, as that'll be handled when doing the release (the CHANGELOG entry itself should be kept though).\npkg_resources seems fine to add to the list, as it usually comes from setuptools anyway.\n_markerlib is not a valid package name to install anyway (pip install _markerlib leads to and InvalidRequirement error), so I feel like it can be ignored too.\n@davidovich Could you take a look at this? It seems like this would fix some error related to pip-sync uninstalling pkg_resources.. Will be completed in #613. @graingert Hm, maybe. but that should be opened in its own issue, that'll be lost on here.. My initial thought on this was to make the change if we were to make a major release.\nNow that it's happening, while I would also prefer it to be consistent, I feel it isn't worth it to make an additional breaking change for aesthetic reasons.\nIf others feels like this is really irritating and should really be changed, speak up. I just don't want to make this change if there's no one else that want it changed.\n@davidovich @jdufresne @suutari / @suutari-ai If you would like to chime-in as common contributors. Good idea, thanks for the input!. No follow up, I'll close this, feel free to mention if it's still an issue.. This was a bug with setuptools likely having trouble parsing environment markers (python_version and extra). Like many noted, this was fixed in newer setuptools version (28.7.1 and higher).\n@nkuttler It's going to be hard to get a nicer error message in that kind of case, as this is an unexpected bug from setuptools, the kind that \"should never happen again\".\nI'll close this as it's fixed, feel free to reopen if needed.\nThanks!. The distribute version (0.6.24) seems pretty old, although I can't compare with my setup at the moment (on my phone).\nYou can try creating a virtualenv using virtualenv 15.1.0, confirm that the pip-compile works, and check the version of distribute installed. Hello, could you rebase this on master please?\nIt's been a while, but I swear I'll try to review and test this to get it in 1.10.1 (unless 1.10.0 gets stalled for long, in which case I could bundle it in)\nAlso, could you:\n- Add the relevant tests\n- Add the relevant changelog info\nThanks!. Was merged by #538.\nThanks for the contribution!. @dfee My pleasure.\nNow sadly, you'll have to install the head of master for a while until jazzband's PyPi upload is fixed, so I can release 1.10.0.. @das-g The bad news is, we're currently blocked and can't do any official release of pip-tools (see #531).\nAs soon as we're able to make new releases, I'll check to add that package to the list, unless Ubuntu manages to fix that first.. Closed by #555. This should be fixed with newer versions of setuptools (update setuptools to 28.7.1 and higher as @jasonm \n mentionned).\nClosing, feel free to reopen if it happens again.. This sounds like a sensible feature to me. PRs are welcomed!. I tried to reproduce and the error seems to be fixed.\nI tried using:\npip-tools: 1.8.2\npip: 8.1.1 and 9.0.1\nCan you check on your side to see if it's fixed? Thanks!. I've been keeping an eye on this issue and did a bit of analysis on my end too.\nWith the example given so far, we act a bit like as if we know which package changed and needs to be backtracked. If we have that information, it's not too hard (and I'd even propose and send a PR for an option to backtrack, even if it would require multiple downloads, I'd use it gladly).\nI've found that a big part of the issue is knowing which package to backtrack. If X, Y and Z all directly depend on A, which one to we backtrack first? We also need to have a mechanic to start back at our \"backtrack starting point\", because if we backtrack X for example and find nothing, we'll want to restore the constraints that we had before backtracking, before trying with Y.\nThis is getting complex really quick. Solving this is still complex, even if we had dependency informations published by PyPI. Although if there's a reasonable way to do this in terms of algorithmic complexity, I'm pretty sure we can poke the people in the PyPa community to have a reasonable way to get dependency informations. After all, pip is trying to get dependency resolving too, so they'll likely hit that point eventually.\n@dan-passaro I don't know much about Ruby. Are there \"full dependency resolving tools\" on their side? Do they have the constraint of \"one version of a package/gem/thing installed at any given time\"?. (Catching up over a year later)\nA lots has changed. Can you tell if this is still required in the latest release (1.10.2)?\nIf yes, could you give an example showing why its needed? I haven't seen any reported cases where this seemed to cause issue.. The AttributeError: 'NoneType' object has no attribute 'name' is something that was fixed in master.\nI tried to reproduce, it doesn't happen here anymore.\nInstead I get:\npiptools.exceptions.UnsupportedConstraint: pip-compile does not support URLs as packages, unless they are editable. Perhaps add -e option? (constraint was: https://files.pythonhosted.org/packages/6a/11/114c67b0e3c25c19497fde977538339530d8ffa050d6ec9349793f933faa/lockfile-0.10.2.tar.gz (from -r requirements.in (line 1)))\nSo here's the expected meaningful error.\n@Groxx pip 9.0.0 dropped the support for the autodetection of requirement names from URLs, so the #egg=some_name part is pretty mandatory (https://github.com/pypa/pip/blob/master/NEWS.rst), unless it get implemented back in pip-tools. But something tells me they dropped that support for some reason on the pip side. You can open another issue (or even submit a PR) for that if you feel it's worth discussing.\n@davidovich Like the other one poked you for, this is fixed in master.. I just noticed something while trying to reproduce:\nIf you rename dev-requirements.txt to dev-requirements.in it works.\nIf a dev-requirements.txt with some -e in it exists, it will fail.\nAfter some more testing, I found that if the output file (the .txt one) has -es in it , it will fail.\nEx:\n\ngit clone https://github.com/pyca/cryptography\ncd cryptography/\nmv dev-requirements.txt dev-requirements.in\npip-compile dev-requirements.in -> succeeds\npip-compile dev-requirements.in -> now fails, as dev-requirements.txt exists with a -e in it\npip-compile dev-requirements.in -o foobar.txt -> succeeds\npip-compile dev-requirements.in -o foobar.txt -> now fails, as foobar.txt exists with a -e in it\nEdit foobar.txt and remove the lines with -e\npip-compile dev-requirements.in -o foobar.txt -> succeeds\npip-compile dev-requirements.in -o foobar.txt -> now fails, as foobar.txt contains -e.\n\nThat makes sense, as the part failing is the one where the output file is read to get the previously existing pins. looks like there's something not handling the editables link in that part.. I tried to reproduce on master, this seems to be fixed (likely related to #484).\n@davidovich looks like this can be closed.\n. Closed by #540. duplicate of #293. @orf Sorry for the long wait.\nIf you still want to pick this up, you can go and rebase this on master.\nYou can remove the changelog entry, and put it in the description of your PR. Maintainers will take care of writting up the changelog afterwards (I realised this was an incredible source of conflicts).\nIf not, feel free to say no, I will completely understand. I'll likely take up the work to get it in myself.\nIn any case, thanks for your contribution \ud83d\udc4d . Finally covered by #524. Hi, could you:\n- Check if the issue still occurs with the latest version?\n- If it does, give the pip version and a sample requirements.in?\nThanks!. I'll close this as an error introduced in the requirements.txt somehow. If someone thinks there's something to be done here, speak up and we'll see.. Should be fixed by #568, closing.. I can't reproduce this, I tried using:\npip-tools: 1.8.2\npip: 9.0.1 and 8.1.1\nCan you confirm if it's already fixed, or give some additionnal info?\nThanks!. Done with PR #555. Fixed by #557. Should be fixed by #568, closing.. @AndreasBackx The lastest  version of pip is tested for every change.\n(https://github.com/jazzband/pip-tools/blob/master/tox.ini#L6)\nso the latest version is pretty safe.\nA nice idea though would be to add a test with the master branch of pip (but configure it as an \"allowed to fail\" configuration) to see if the upcomming pip release is going to break pip-tools, and prepare accordingly.. Nothing happened here for a good while. I'll go ahead an close.\nAlso, if anyone would actually need this, please indicate how/why that's needed.. I have absolutely no knowledge of conda, unfortunately. I don't know how it manages its \"python environments\".\nI'll need the input of someone knowledgeable here.. Dealth with by #568. (Catching up)\nThe feature looks good to me.\nThis would need a rebase on master and some tests too, and that should be good.. Giving an error is an option, but how about simply adding \".txt\" for the output filename , as it's already the used extension for the output file.\nEx:\npip-compile no_extension\nResult: no_extension.txt is created, containing the pip-compile output.\nThat way, if anyone happens to be using files without extension as input, it won't break them, while not outputing to exact same file for every filename without extension, which is the case right now.. I started to work on this.\nA good part from the job comes from the fact that when merging requirements, we gave up on all information from where they came from, so it wasn't really possible to point out which packages requirements are in conflict.\nBut I think I found a way to keep track of the \"parent\".\nYou can see my progress on this branch: vphilippon/pip-tools@084c549aba9bc3d424296f7c36bd8208fa0aa809. Quick update: I haven't had much time to continue on this.\nAlso, even if we manage to show the tracking of the dependency, in the case where the are a lot of packages, its going to be a mess. Something like:\nCould not find a version that matches djangorestframework==3.2.*,>=3.3.1\n(from line X ->pkg_a vX.X.X->pkg_b vX.X.X->[pkg_c vX.X.X, pkg_d vX.X.X]->djangorestframework==3.2.*,>=3.3.1\nand this wasn't a deep example. Not that it didn't even give the required version of djangorestframework by pkg_c vX.X.X and pkg_d vX.X.X seperately, as we can't (simply) do that, as we merged the requirements (a thing pip does not really support, which is why we where clearing the tracking information).\nDoing a grep on a verbose output is the best way for tracking a conflict right now, and will still be the only way to get the \"per-parent specifier berfore merge\" even if I ever manage to get through this.\nUnless someone brings in additional value for the tracking info I mentionned, it's likely I won't go through with this.. @jedie There's a few things you can try out:\n- Make a stricter regex (or re-grep the result) to get lines with something like django.*< to get the packages that put an upper bound on django. I really just eyeballed that regex example, you might need something better.\n- Try with another requirements.in, removing some of the \"suspects\".  That's a guessing game.\nI hope this helps you out a bit.. This is something that has been in the back of my head for a while, but I'm not sure it's that straightforward.\nBy depending on the internals of pip, and not being able to specify the required pip version, we end up depending on setuptools in the cases where the version of pip installed uses pkg_resources instead of packaging (see that function key_from_req we have in utils.py).\nI'm worried about the effect of setting a lower bound on setuptools, especially for users with older versions of pip that might not be able to update setuptools.\nThe only \"easy\" way I see out of this is when we'll drop the support of old pip versions, and we could then move to packaging and try to be as clean as possible. But as a lot of users use the version of pip distributed by their linux distribution, many of which are lagging behind.\nNow if I missed something that actually makes all of this way easier than that, correct me, I'll be pleased.. Closing this, as a simple lowerbound will not cut it.\nIf we end up vendoring pip 9.0.1 we might be able to do something like that.. Doing pip install does not include the setup_requires and test_requires, so by default, I would suggest to not include those for sure.\n\nbut if requirements.txt represents the complete state of things in a\nvirtualenv, i think it should be nailing that stuff down. \n\nHere's the catch: tests_require and setup_requires dependencies are never installed in the virtualenv. Running python setup.py test does not install the packages in the virtualenv, in only puts them in a .eggs directory. It's the same for setup_requires.\nThe more I think about this, the more I feel like we should not bring that kind of behaviour, even as an option.\nI'm definitly interested in clearer error message though!. Allowing to configure the list of packages to ignore sounds good to me. Considering the amout of issues related to that, it would at least let people fix their own setup, and then we can consider adding it to the default list if it's universal.\nI'd suggest going with a command line option, as it's how most options are given and it's easy to document.. Out of curiosity, I checked if tabular was \"pip-installable\":\n```\n$ pip install tabular\nCollecting tabular\n\n  Downloading tabular-0.1.tar.gz (1.1MB)\n    100% |################################| 1.1MB 787kB/s\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"e:\\tmp\\pip-build-gqxnbg\\tabular\\setup.py\", line 50, in \n        raise ImportError(\"distribute was not found and fallback to setuptools was not allowed\")\n    ImportError: distribute was not found and fallback to setuptools was not allowed\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in e:\\tmp\\pip-build-gqxnbg\\tabular\\\n``\nIfpip install` fails, I'd say it's not something wrong with pip-tools.\nAgain, out of curiosity, I looked at the setup.py, and it got some logic trying to import distribute_setup, which I believe requires to have a distribute_setup.py script (which isn't there) or the installed setuptools version to offer some support for distribute, which isn't the case for me (setuptools==35.0.2). I'm not a distribute pro, so I might be missing something, but as @alexwlchan suggested, that's something with the setup.py and most likely out of pip-tools hands.\nI'll go ahead and close the issue.\n@alexwlchan if you (or anyone else encountering this) find out there's actually something wrong with pip-tools regarding this, feel free to reopen the issue.\nThanks for reporting!. I looked in the code and found this comment:\nhttps://github.com/jazzband/pip-tools/blob/master/piptools/sync.py#L81\npip-sync was designed to be used with the output of pip-compile only. This means that both files should only have pinned requirements.\nIf one of the files has unpinned requirements (like invoke>=0.15.0 (from -r requirements/dev.txt (line 12))), then that file is certainly not the output of a pip-compile.\nSo here's why it's considered incompatible. pip-sync didn't take the math class, that's pip-compile thing. I hope that helps you out @mkoistinen.\nThe error message could definitely be improved though, it's quite misleading.. Doc updated by #609, closing.. You're right. Having pip installed but not setuptools is somewhat a rare case, but it's possible.\nIt'd be good to have an automated test in an environment where setuptools is not already installed before doing the pip install of pip-tools.\nIt would probably look like:\n Make a pip-tool wheel (not a zip/tar.gz), (doind that actually requires setuptools to be installed)\n Uninstall setuptools\n Do the install of pip-tools with the previously generated wheel, which should install setuptools\n Do the tests as usual.\nNot sure how complicated that would be.. I went through the discussion on whether this is a bug or not, and I think I can summarize this:\n@dschaller, you're pointing out that prior to 1.9.0, --allow-unsafe was an \"all or nothing\" option, but with the change of PR #441, we brought an unexpected change where giving an unsafe pin directly in requirements.in now stays in the output, which goes against the original idea.\n@davidovich, you're saying that in retrospective, maybe thats a good change, because it allows us to say \"Yes, I want pip to be pinned, but not the other unsafe ones (like setuptools)\".\nIMHO, I can see use cases where both behaviour would be good. And I think I have an idea to keep the best of both worlds: having a way allow specific unsafe packages, in the same way we do with --upgrade and --upgrade-package.\nGiving --allow-unsafe would let all unsafe go through, but if I set --allow-unsafe-package pip instead, then only pip is allowed.\nWe keep the previous behaviour that our users expects, while giving more control to the users.\nWhether we should do that in one or many PR is debatable (oh no... what have I done ;) ), but I feel like this is the way to go.\nWhat do you say about this?\nOn another subject, @dschaller, I haven't fully analysed the code change (sorry, I'll try to!), but the feeling I got is that we went back at filtering the output rather than filtering on the input. Am I right? If so, do you think its possible to change that? I think its a better way to handle the case, and I also got a feeling that the unsafe dependencies of unsafe requirements directly in the requirements.in are going to get through, which goes against the work in PR #441.\nAs always, thanks everyone for your time!. LGTM.\nCould you rebase this on master please?\nThanks!. Tried to reproduce, here's a minimal reproduction case:\nsetup.py:\n```\nfrom setuptools import setup\nsetup(\n    name='test_piptools',\n    version='0.0.1',\n    install_requires=\"\"\"\\\nmarkovify>=0.6,<1.0\ntweepy\n\"\"\"\n)\n```\nThen, run pip-compile, which will parse the setup.py:\npip-compile\nNote: This only happens if parsing the setup.py directly.\nIf we pipe \"-e .\" to pip-compile's stdin, which is equivalent to having a requirements.in with \"-e .\", like\necho \"-e .\" | pip-compile -o requirements.txt -\nthen it works.\nSo that's most likely in the parsing of the setup.py, as mentionned by @rpkilby.. I'm like the idea. The only thing bugging me is the effect of a pip-sync for an app also using coloredlogs.\nI did a simple test with your branch on Windows:\nrequirements.in:\ncoloredlogs<6.0\nthen\n$ pip-compile\n$ pip-sync\nand it went well, no problem with modifying coloredlogs while pip-sync used it itself.\n(If only it could go that well all the time... sigh).\nThat was a really simple test, it would probably be a good idea to do test the behaviour a bit more.\nNote, I actually managed to \"break\" pip-sync doing this:\nrequirements.in:\ncoloredlogs<1.0\nthen\n$ pip-compile\n$ pip-sync\nending up with coloredlogs==0.8 installed, which broke pip-sync.\nSo to be nice, we should set a lower bound on coloredlogs, so that running pip check will tell that something is wrong in that case.\nI'll make a more complete review after the branch is rebased with the latest changes.. Thinking about this over again, this is the same situation with any of of pip-tools dependencies (click or anything else). I did have to go out of my way a bit to break it. I think a reasonable idea here is to use coloredlogs, and put a version specifier, like we should, in install_requires. Let's try to put that lower bound as low as possible to minimize the conflict zone. I don't feel like this is lib with a high risk of conflict though.\nSorry for taking so long on this. Its a bit hard to get motivation when you know you're somewhat blocked ahead by being unable to make releases and give value to the time spent. /rant.. Given the time that has passed, I will close this.\nIf, with the increasing usage of docker, there's a higher demand to address this usage, we could come back to this.\nThanks for the report by the way, at least this is now known \ud83d\udc4d .. The situation seems to have improved to a reasonable level with #557.\nIn --verbose mode, we could use some output during the hashing though.. That was brought a while agot, by this commit:\nhttps://github.com/jazzband/pip-tools/commit/50f06cbc588cfc099105b6a0b62abff027f3ad21\nAnd I'll admit I'd really like to have @nvie input on this.\nThe only reasons I could think of would be one of those:\n1. Some packages, like pip, can be installed with a package manager on linux, and could cause some conflict, but I never tested (I'm mostly on Windows these days).\n2. Those are setup/installation packages, and most app (not all though) don't need to pin them.\n3. When doing pip freeze, some of the installed packages are not listed, unless --all is specified. Maybe the original intent was to follow that.. Coming back on this:\npip, setuptools and distribute and responsible for building/installing packages. Generally speaking, one shouldn't have to pin these as they should have little impact on an application. (I've actually met quite a few exceptions to this assumption in my daily work, but I'll go ahead and hope that I just happen to be an unlucky fellow doing some funky things.)\nFor, PACKAGES_TO_IGNORE, it seems to be to cover some edged cases with global packages. for pip, setuptools and wheel, I would say it's to avoid issues while pip-sync is running. Forpip-tools, that's just to be keep the tool installed. Forpip-review, I think its a simple exclusion for this tool that is used to track packages upgrade and might be interesting to use alongsidepip-tools`.\nI'll close this issue, as it seems there's not much else we can add other than guessing. If this cause any other issue in the future, we can spend the time to figure out the exact needs and impact.. Thanks for the PR!. I just tried with the head of master, and it seems to work. Try rebasing on master to see if it works now.. I'll try to setup a Windows+Py3 environment and investigate this.\nIf I haven't replied in 2 days, ping me.. (Catching up) (like #424)\nA bunch of thing has moved. If you're still interested and willing to, get this rebased on master.\nYou can also remove then changelog entry and just put a nice one-liner in the PR description, maintainers will take care of it in due time.. Dang, still a py3 error on appveyor. I'll try to reproduce.\nEdit: I was able to reproduce, I'll investigate.. This is not only an appveyor issue: This really happens on windows with python 3.\nIts simply ignored.\nrequirements.in:\n-e git+https://github.com/django/django.git@1.11.1#egg=django\nThen:\n```\n$ pip-compile\nException ignored in: \nTraceback (most recent call last):\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\weakref.py\", line 548, in call\n    return info.func(info.args, *(info.kwargs or {}))\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\tempfile.py\", line 797, in _cleanup\n    _shutil.rmtree(name)\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 494, in rmtree\n    return _rmtree_unsafe(path, onerror)\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 384, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 384, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 384, in _rmtree_unsafe\n    _rmtree_unsafe(fullname, onerror)\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 389, in _rmtree_unsafe\n    onerror(os.unlink, fullname, sys.exc_info())\n  File \"c:\\users\\vphilippon\\projects\\pip-tools\\venv\\lib\\shutil.py\", line 387, in _rmtree_unsafe\n    os.unlink(fullname)\nPermissionError: [WinError 5] Access is denied: 'F:\\tmp\\tmpqgb_vo8dsource\\django\\.git\\objects\\pack\\pack-b0575a33cf48a7cf219e39160c194330f3d05391.idx'\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\n-e git+https://github.com/django/django.git@1.11.1#egg=django\npytz==2017.3\n$ echo $?\n0  # The operation is still considered as a success, but the cache still contains the django directory.\n``. This is not part of your change though. You can work around this (in the context of this PR) by usinginchecks instead of==` on the output.\nYou can also remove test_editable as have some tests are already covering this (test_editable_package_vcs). Thank you for your contribution, and for your time \ud83d\udc4d . Just to double check: I noted that the example of requirement.txt you've given actually contains rPy2 2.8.6, I assume that yours contains 2.8.5, right?\nAlso, you said that you performed a pip-compile, and then a pip freeze, and the package r2py was not updated, but I'll also assume that you did pip install -r requirements.txt in between to actually update your venv with the new requirements.txt, right?\nIf those 2 things above are true, could you give me your full requirements.txt before and after trying to upgrade, as well as the verbose output of each of your pip compile command?\nI'll try to get the time to setup R and try to reproduce, but in the meantime, maybe I'll be able to help with that additional info.\n. pip-compile makes no change to the installed packages, so yes, you have to do pip install -r ......\nThe --upgrade flag is actually to upgrade the version in the resulting requirements.txt.\nNow, you might ask:\n\"Wait, you mean re-runing pip-compile without --upgrade does not update the requirements.txt? Why? Why would a runpip-compile` then?\"\nAnd thats a good question. The thing is that when re-running pip-compile, the tool will make sure that the requirements.txt still respects what's in the requirements.in. Much like if you do pip install resquests>=2.13.0, and requests 2.14.0 is already installed, it won't upgrade it to the latest (say 2.17.3). For pip install to update requests to the latest, you must use --upgrade. It's the same thing for pip-compile: For the version in the requirements.txt to be updated after it was first generated, you use --upgrade.\nI hope that makes sense. Tell me if not, that can be confusing.\nP.S. pip-tools also have a pip-sync command to make sure your venv only contains what's in your requirements.txt (meaning, it may uninstall stuff), but I haven't really used it personnally.. Thanks for the PR!. Could you give an example setup.py, requirements.txt and pip freeze output?\nI'm not sure to exactly understand what is happening on your end.. Ah! Here's a thing: pip-sync is designed to be used only with a requirements.txt generated with a pip-compile!\nAnd I can tell the requirements.txt is not: Django is not in the requirements.txt, so you end-up with what we could call an undefined behaviour.\nAlthough, I noted that the same happens even if I have only \"normal\" packages in the requirements.txt: the dependencies also get installed.\nResult: the requirements.txt is not the product of a pip-compile, it's not \"fully pinned\" (meaning, including subdependencies), and that use case is not supported, hence the weird behaviour.\nNow, that begin said, @jtiai, is this just an inconsistency you noted, or is it a behaviour you actually need?\nIf I had to guess, I think I can see what you're trying to accomplish: You want to be able to do something like pip-compile --no-deps and/or pip-sync --no-deps, am I right?\nThat's not yet possible AFAIK, but that might be a feature to add, if you actually need it.. I'd go for some documentation first, that won't hurt.\nFor the feature, I'd skip it for now. If it comes up again as an actual need, we'll take a look at it (and I'll try to remember to poke you back on your offer :) ). Heads up: Still waiting for the roadies to investigate and/or address the security issue and reactivate the PyPi uploads. I try to monitor this daily, so you'll be notified as soon as I get the green light.\nAlso, no promises, but I'll try to get some simple PRs merged and add them to the 1.10.0 release, as this release issue is taking longer than I expected. \nThanks for your patience!. @dfee None, unfortunately. I'm still waiting for a reply from the roadies about the PyPi authentication.\n@nvie By any chance, do you still have access to PyPi to make a pip-tools release, or were your rights revoked when moving to jazzband?. Small update here: It seems the original security issue is still in cause (according to a recent reply from @jezdez to another similar issue).\n@dfee I feel like it's not a small decision to take (which I can't take either), and we might have other solutions to look at right now.\nAlso, small clarification just in case: we're still able to merge PRs and push code. We just can't publish the package to PyPi.\n@jezdez As a roadie, in the current situation, is it possible for you to manually perform an upload to PyPi for us?\nI don't intend to ask for a release each week, but at least we could release the work from the past few months.. Good news: While the release-on-tag feature is still disabled, @jezdez indicated he'll be able to do a one-off release of pip-tools.\nI've marked PR #567 and PR #557 for the 1.10.0 release, as they'll fix some common issues. I suggest we get those in, and then go ahead for the 1.10.0 release.. Hi all, the last PRs for 1.10.0 are merged, I've given the go for the release, so we'll freeze the code until the release.. \ud83c\udf89 pip-tools 1.10.0 was officially released! \ud83c\udf89. @jezdez We missed a pretty awful bug, essentially breaking pip-sync on Python 3. If you find some time for releasing 1.10.1, that would be great.\nSorry about that.. @jezdez We got another important bugfix in that would require a 1.10.2 release, if you have the time.\nIf we ever meet at PyCon or somewhere, I owe you a coffee.. Great news: We should be able to get back on making pip-tools releases. I'll let you all read up here: \nhttps://github.com/jazzband/roadies/issues/64#issuecomment-345477476\nSo @davidovich and myself are currently assuming the new Lead role on pip-tools. This essentially means that we are responsible for approving any release before it reaches the main PyPi.\nI intend to release 1.10.2 soon. I'll report back to confirm that we're unblocked once I'll see the release on PyPi.. 1.10.2 is out, everything is rolling!\n\ud83c\udf70 . Odd, I have the same package (with the same version) working allright with pip-compile, on Windows.\nI have a strong feeling the error is due to pip trying to build the package (which is Windows Only) on linux, but I would expect the environment marker to prevent that.\nEdit: After thinking about it, I think pip-compile will still perform at least python setup.py egg_info to get the dependencies list, but I might be wrong, this need some investigation.\nQuestion: How have you made sure it's not due to the environment marker?. I did a bit of testing, and it confirmed my original thought: The environment markers directly in the requirements.in file are not taken into account by pip-compile. But, those in the sub-dependencies are.\nEx: (warning: do not read before your first coffee)\nWith this setup.py:\nsetup(\n    name='dummy-test',\n    version='1.0.0',\n    extras_require={\n        ':sys_platform == \"win32\"': ['pypiwin32==219'],\n    },\n)\nand this requirements.in:\ndummy-test\nWe get this requirements.txt (on windows):\ndummy-test==1.0.0\npypiwin32==219            # via dummy-test\nand on linux (or if I change the condition to sys_platform != \"win32\":\ndummy-test==1.0.0\nBut if I put pypiwin32==219; sys_platform == \"win32\" in the requirements.in instead, and remove the extras_require, I get:\ndummy-test==1.0.0\npypiwin32==219 ; sys_platform == \"win32\"\nand if I change the condition to sys_platform != \"win32\" (still on windows):\ndummy-test==1.0.0\npypiwin32==219 ; sys_platform != \"win32\"\nAlso, as you would expect now, if the top-level package has dependencies too, they will get in the requirements.txt too. In other words, in the examples above where we put the marker in the requirements.in, change pypiwin32==219 by requests, and you'll get requests' dependencies on both platform. The environment markers in the requirements.in are really not taken into consideration, and merely copy-pasted to the requirements.txt.\nResult: We'll need to standardise the behaviour here and parse/compute the markers in the input file.\n. I digged a bit more into this: pip-tools does no handling whatsoever to exclude \"unsupported requirements\". It just happens that setuptools takes care of the marker evaluation before returning the requirements, which isn't done for plain requirements.in file. we have to do that ourselves.\nI could filter out \"unsupported requirements\", but then I have to make a choice on whether to keep or remove the feature that adds the requirements with their markers in the requirements.txt.. Dealth with by #647. Thanks for the contribution!. Fixed in 1.10.1. You're right, I was under the impression that the formatting was changed, but it's actually the value passed that might be different. That would be a different test.\nAdding a test suite in tox using pip 8.1.1 seems reasonable to me, as this version (while having a more recent bugfix version) seems to still be common in the wild.\nYou can go ahead and add a test environment with pip 8.1.1.\nAlso, sorry for the delay, life happened. Thank you for your time!. @Lucas-C You also have to add the new pip 8.1.1 tox environment in .appveyor.yml, for each python version already listed. That's for the Windows tests.\nWe're almost there, pretty sure we'll be done after that.\nThanks again.. Thanks for your contribution!. This looks a lot like a the previous bug you reported on the matter: #396\nI get a failure with pip 8.1.1, but it's AttributeError: Requirement instance has no attribute 'name' instead.\nI tried with python 2.7 though, and setuptools 28.8.0.\nI tried with pip 8.1.2  and pip 9.0.1, and it works fine.\nCould you try updating pip to 8.1.2 and see if it fixes the issue?\nIf not, try updating your setuptools version in the virtualenv (not in the requirements.txt, this one is not the issue).\nI've seen a lot of issues with outdated pip and setuptools.. Thanks for the contribution!. @graingert Is this still needed/relevent with #557 merged?. Fixed by #557. I think this is not in the scope of pip-tools.  pip-tools takes care of dealing with requirements.in/requirements.txt to keep the set of concrete dependencies fresh and sane, and be able to reproduce a given virtualenv.\nI'm not ready (as an active contributor to pip-tools) to endorse and then maintain such a feature.\nThank you for your suggestion!. Thanks for the detailed example, it made the case way easier to check.\nSadly, I have some bad news: you will get the exact same behaviour using pip uninstall too.\nThe reason is that both backports.ssl_match_hostname and backports.shutil-get-terminal-size claim to install the backports package. So when uninstalling backports.shutil-get-terminal-size, pip checks in the dist.info, and then uninstall every file claimed to be installed by it.\nQuick example:\n```\n$ pip install backports.ssl-match-hostname==3.5.0.1\n$ pip install backports.shutil-get-terminal-size==1.0.0\n$ pip uninstall backports.shutil-get-terminal-size\nUninstalling backports.shutil-get-terminal-size-1.0.0:\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\description.rst\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\installer\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\metadata\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\metadata.json\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\record\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\top_level.txt\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports.shutil_get_terminal_size-1.0.0.dist-info\\wheel\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports__init__.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports__init__.pyc\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports\\shutil_get_terminal_size__init__.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports\\shutil_get_terminal_size__init__.pyc\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports\\shutil_get_terminal_size\\get_terminal_size.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports\\shutil_get_terminal_size\\get_terminal_size.pyc\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\shutil_backports__init__.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\shutil_backports__init__.pyc\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\shutil_backports\\get_terminal_size.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\shutil_backports\\get_terminal_size.pyc\nProceed (y/n)?\nNotice that these two are in the list to be uninstalled:\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports__init__.py\n  f:\\virtualenvs\\tmp-f0598575165ea714\\lib\\site-packages\\backports__init__.pyc\n```\nMaybe we could circumvent this by:\nA) force the reinstall with something like --ignore-installed in the install phase, but that would force the reinstall for everyone, when 90% of the cases don't need it.\nB) Improve pip-sync to accept and pass all \"pip flags\" to pip, which would let the user use the flags in A), letting the user control the behaviour if needed.. So in the end this is all about different ways to define packages and how pip deals with them, pip-tools has no control over this it seems.\nI'll close this here. Thanks for the talk \ud83d\ude04 !. I tried to reproduce with python 3.6.2, and I can't. functools32 is not collected.\nrequirements.in:\nipython[notebook]\nrequirements.txt:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nbleach==2.0.0             # via nbconvert\ndecorator==4.1.2          # via ipython, traitlets\nentrypoints==0.2.3        # via nbconvert\nhtml5lib==0.999999999     # via bleach\nipykernel==4.6.1          # via ipywidgets, notebook\nipython-genutils==0.2.0   # via nbformat, notebook, traitlets\nipython[notebook]==6.1.0\nipywidgets==7.0.0         # via ipython\njedi==0.10.2              # via ipython\njinja2==2.9.6             # via nbconvert, notebook\njsonschema==2.6.0         # via nbformat\njupyter-client==5.1.0     # via ipykernel, notebook\njupyter-core==4.3.0       # via jupyter-client, nbconvert, nbformat, notebook\nmarkupsafe==1.0           # via jinja2\nmistune==0.7.4            # via nbconvert\nnbconvert==5.2.1          # via notebook\nnbformat==4.4.0           # via ipywidgets, nbconvert, notebook\nnotebook==5.0.0           # via ipython, widgetsnbextension\npandocfilters==1.4.2      # via nbconvert\npexpect==4.2.1            # via ipython\npickleshare==0.7.4        # via ipython\nprompt-toolkit==1.0.15    # via ipython\nptyprocess==0.5.2         # via pexpect, terminado\npygments==2.2.0           # via ipython, nbconvert\npython-dateutil==2.6.1    # via jupyter-client\npyzmq==16.0.2             # via jupyter-client\nsimplegeneric==0.8.1      # via ipython\nsix==1.10.0               # via bleach, html5lib, prompt-toolkit, python-dateutil, traitlets\nterminado==0.6            # via notebook\ntestpath==0.3.1           # via nbconvert\ntornado==4.5.1            # via ipykernel, notebook, terminado\ntraitlets==4.3.2          # via ipykernel, ipython, ipywidgets, jupyter-client, jupyter-core, nbconvert, nbformat, notebook\nwcwidth==0.1.7            # via prompt-toolkit\nwebencodings==0.5.1       # via html5lib\nwidgetsnbextension==3.0.1  # via ipywidgets\n```\nThe only difference I can think of is that I used a Dokcer container (not macOS), and our setuptool version could be different.\nFrom the top of my head, I'd say:\n- Try doing pip install jsonschema and confirm it works.\n- Check/update your setuptools version\n- Triple-check that you're really using pip-tools 1.9.0.\nIf all else fails, send the verbose (-v) output of your pip-compile.. As I mentioned in #554, make sure to delete your requirement.txt and try to pip-compile with --rebuild to clear your cache. Tell me if it works out.. Something is not right. My pip-compile on a python 3.6 system will give me ipython 6.1.0, while you get 5.4.1. And the version 5.4.1 is the one offering a py2 wheel, while 6.1.0 offers a py3 wheel.\nSomehow, your system act as if is was running in python 2.7. That goes beyond requirements parsing: AFAIK, pip-tools takes no part in the checking the wheel's python version compatibility: that's all handled by pip.\nIn the same venv, could you give me:\n- Give me your setuptools version (you can also try updating it)\n- Try doing pip install ipython[notebook] and give me the output, as well as a pip freeze --all output.. I'll need a bit more information to help you out here. Does your private index mirror those packages? What is your actual pip configuration?\nThat'll help me out looking in the right direction, and tell you if it's possible, and how.. This is extremely similar to #552. I've tried with a docker container, to no effect.\nBut, I then tried with python 2.7, and functools32 did not appear in the requirements.txt. But it did get installed with pip install, which really confused me.\nSo I tried another pip-compile, deleting the requirements.txt and clearing the cache with --rebuild, and then functools32 got listed (with python 2.7).\nI then retried with --rebuild on python 3.5 (in a docker container), and it did not get listed.\nSo I'm not sure how/why, but I got feeling your pip-tools cache has old or bad stuff. Please try again with --rebuild, and tell me if it fixes the issue.\nIf it does work, I'll ask right away: do you use both python 2.7 and python 3.X? I'd like to figure out what is wrong with the pip-tools cache.. Most likely the wheels cache yes. I'll close this issue, if it occurs again, ping.\nI'll pray this one was only a cache issue.. Indeed, thanks for the reminder.. @suutari @suutari-ai Could you rebase on master, I'll be ready to get this in right away. Thanks!. Thanks for the PR!. Also, try with the --rebuild option to clear your pip-tools cache. If you happen to use both py2 and py3 on your system, you might get hit by the py2 dependencies being cached for django-auth-ldap.\nIn would actually make sense, as pip-tools caches the dependencies based on the package name and version, and django-auth-ldap provides 2 different wheel for py2 and py3, which each provides a different dependencies list.\n@MHLut Please check with what @suutari-ai said, and then try clearing your cache, and tell us about the result. If you could save the verbose output of you different attempts (both failed and successful), that would be really helpful for us to pinpoint the exact cause for the future.\nIt's the 3rd time in a short while we get reports of this. I'd really like to know the exact cause to do something about it.. Can't reproduce either, using Python 3.6.2 in a Docker container.\nFor some reason I can't figure out, pip seems to download the Python 2 django-auth-ldap wheel.\nThis starts looking a lot like #552.\nCould you try simply running pip install django-auth-ldap==1.2.10 to confirm it works?. @MHLut I noticed your pip install ended up using a cached py3 wheel. Could you try doing the same with the --no-cache and --ignore-installed flags? I want to be 100% sure that pip won't go and grab the py2 wheel.\nIf pip-compile --rebuild with no .txt file, in the same venv as the succeeding pip install --no-cache --ignore-installed, grabs py2 dependencies, then when have some major issue.\n. pyldap comes directly from your base.in file (pyldap==2.4.28). It does not appear as required by django_auth_ldap in the computation. So pip-compile really only detects the Py2 dependency here (unless it reappears if you remove it from your base.in)\nI used a Fedora 26 Docker image and updated to Python 3.6.2 to try and reproduce this, and I can't.\n~Only difference, I removed the mysqlclient==1.3.10 line as it failed (probably don't have the dependencies to build it).~ (Edit: installed the dependencies and included it, still works)\nI just realised that when using pip install, I told you to use --no-cache, which is nice, but didn't actually clear pip's cache (or at least, I'm not a 100% sure it did). You could try doing that.\nIt's likely to be around ~/.cache/pip.. @suutari-ai I looked at the PR you linked and your analysis makes a lot of sense. But here's a thing: 1.10.0rc1 and 1.10.0rc2 are not really out yet: they were attempts for a release that failed as the Pypi upload was (and still is) disabled for all of Jazzband's projects.\nSo, unless someone is pointing to master or something, this shouldn't be out in the wild.\nMaybe the issue is with a something related to hashes or platforms, but included in 1.9.0? IIRC, there was quite a bit of moving around these parts when releasing 1.9.0. Still, I think you've put the finger on something : pip-tools seems to be overriding some of pip's behaviour, which breaks my \"we're not messing up with pip\" assumption. I'm now sad.. And I can reproduce too with python 2.7 on Windows. We need to check if the feature of generating hashes on all platform can be fixed an kept, or removed.\n@jdufresne maybe you can give some input here.\n. @jdufresne I took you script and adjusted it for python2, and I reproduced (in a Docker container: python:2.7) :\n(Edit: First execution did not clear cache, fixed it)\n```\n!/bin/bash\nset -x\ncat requirements.in\nrm -rf ~/.cache/.pip\nrm -rf ~/.cache/.pip-tools\nrm -f requirements.txt\nrm -rf venv\npython2 -m virtualenv venv\nvenv/bin/pip install --upgrade pip setuptools\nvenv/bin/pip install pip-tools\nvenv/bin/pip-compile -o requirements.txt requirements.in\nrm -rf venv\npython2 -m virtualenv venv\nvenv/bin/pip install --upgrade pip setuptools\nvenv/bin/pip install -r requirements.txt\n```\nOutput\n```\n+ cat requirements.in\ndjango-auth-ldap\n+ rm -rf /root/.cache/.pip\n+ rm -rf /root/.cache/.pip-tools\n+ rm -f requirements.txt\n+ rm -rf venv\n+ python2 -m virtualenv venv\nNew python executable in /venv/bin/python2\nAlso creating executable in /venv/bin/python\nInstalling setuptools, pip, wheel...done.\n+ venv/bin/pip install --upgrade pip setuptools\nRequirement already up-to-date: pip in /venv/lib/python2.7/site-packages\nRequirement already up-to-date: setuptools in /venv/lib/python2.7/site-packages\n+ venv/bin/pip install pip-tools\nCollecting pip-tools\n  Using cached pip_tools-1.10.1-py2.py3-none-any.whl\nCollecting six (from pip-tools)\n  Using cached six-1.11.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools)\n  Using cached click-6.7-py2.py3-none-any.whl\nCollecting first (from pip-tools)\n  Using cached first-2.0.1-py2.py3-none-any.whl\nRequirement already satisfied: setuptools in /venv/lib/python2.7/site-packages (from pip-tools)\nInstalling collected packages: six, click, first, pip-tools\nSuccessfully installed click-6.7 first-2.0.1 pip-tools-1.10.1 six-1.11.0\n+ venv/bin/pip-compile -o requirements.txt requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\ndjango-auth-ldap==1.2.15\ndjango==1.11.5            # via django-auth-ldap\npyldap==2.4.37            # via django-auth-ldap\npytz==2017.2              # via django\n+ rm -rf venv\n+ python2 -m virtualenv venv\nNew python executable in /venv/bin/python2\nAlso creating executable in /venv/bin/python\nInstalling setuptools, pip, wheel...done.\n+ venv/bin/pip install --upgrade pip setuptools\nRequirement already up-to-date: pip in /venv/lib/python2.7/site-packages\nRequirement already up-to-date: setuptools in /venv/lib/python2.7/site-packages\n+ venv/bin/pip install -r requirements.txt\nCollecting django-auth-ldap==1.2.15 (from -r requirements.txt (line 7))\n  Using cached django_auth_ldap-1.2.15-py2-none-any.whl\nCollecting django==1.11.5 (from -r requirements.txt (line 8))\n  Using cached Django-1.11.5-py2.py3-none-any.whl\nCollecting pyldap==2.4.37 (from -r requirements.txt (line 9))\n  Using cached pyldap-2.4.37.tar.gz\nCollecting pytz==2017.2 (from -r requirements.txt (line 10))\n  Using cached pytz-2017.2-py2.py3-none-any.whl\nCollecting python-ldap>=2.0 (from django-auth-ldap==1.2.15->-r requirements.txt (line 7))\n  Using cached python-ldap-2.4.44.tar.gz\nRequirement already satisfied: setuptools in /venv/lib/python2.7/site-packages (from pyldap==2.4.37->-r requirements.txt (line 9))\nBuilding wheels for collected packages: pyldap, python-ldap\n  Running setup.py bdist_wheel for pyldap ... error\n  Complete output from command /venv/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-8oDn1L/pyldap/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/tmpDpPPeopip-wheel- --python-tag cp27:\n  defines: HAVE_SASL HAVE_TLS HAVE_LIBLDAP_R\n  extra_compile_args:\n  extra_objects:\n  include_dirs: /usr/include /usr/include/sasl /usr/local/include /usr/local/include/sasl\n  library_dirs: /usr/lib /usr/lib64 /usr/local/lib /usr/local/lib64\n  libs: ldap_r\n  running bdist_wheel\n  running build\n  running build_py\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  creating build\n  creating build/lib.linux-x86_64-2.7\n  copying Lib/ldapurl.py -> build/lib.linux-x86_64-2.7\n  copying Lib/ldif.py -> build/lib.linux-x86_64-2.7\n  copying Lib/dsml.py -> build/lib.linux-x86_64-2.7\n  copying Lib/slapdtest.py -> build/lib.linux-x86_64-2.7\n  creating build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/init.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/async.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/compat.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/init.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/deref.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/libldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/openldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/ppolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/psearch.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/pwdpolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/readentry.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/sessiontrack.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/simple.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/sss.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/cidict.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/dn.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/extop/init.py -> build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/extop/dds.py -> build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/filter.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/functions.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/ldapobject.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/logger.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/modlist.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/resiter.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/sasl.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/init.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/models.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/subentry.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/tokenizer.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/syncrepl.py -> build/lib.linux-x86_64-2.7/ldap\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  running egg_info\n  writing requirements to Lib/pyldap.egg-info/requires.txt\n  writing Lib/pyldap.egg-info/PKG-INFO\n  writing top-level names to Lib/pyldap.egg-info/top_level.txt\n  writing dependency_links to Lib/pyldap.egg-info/dependency_links.txt\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  reading manifest file 'Lib/pyldap.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  warning: no files found matching 'Makefile'\n  warning: no files found matching 'Modules/LICENSE'\n  writing manifest file 'Lib/pyldap.egg-info/SOURCES.txt'\n  running build_ext\n  building '_ldap' extension\n  creating build/temp.linux-x86_64-2.7\n  creating build/temp.linux-x86_64-2.7/Modules\n  gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DHAVE_SASL -DHAVE_TLS -DHAVE_LIBLDAP_R -DHAVE_LIBLDAP_R -DLDAPMODULE_VERSION=2.4.37 -IModules -I/usr/include -I/usr/include/sasl -I/usr/local/include -I/usr/local/include/sasl -I/usr/local/include/python2.7 -c Modules/LDAPObject.c -o build/temp.linux-x86_64-2.7/Modules/LDAPObject.o\n  In file included from Modules/LDAPObject.c:9:0:\n  Modules/errors.h:8:18: fatal error: lber.h: No such file or directory\n   #include \"lber.h\"\n                    ^\n  compilation terminated.\n  error: command 'gcc' failed with exit status 1\n\nFailed building wheel for pyldap\n  Running setup.py clean for pyldap\n  Running setup.py bdist_wheel for python-ldap ... error\n  Complete output from command /venv/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-8oDn1L/python-ldap/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/tmpkWNqOVpip-wheel- --python-tag cp27:\n  defines: HAVE_SASL HAVE_TLS HAVE_LIBLDAP_R\n  extra_compile_args:\n  extra_objects:\n  include_dirs: /usr/include /usr/include/sasl /usr/local/include /usr/local/include/sasl\n  library_dirs: /usr/lib /usr/lib64 /usr/local/lib /usr/local/lib64\n  libs: ldap_r\n  running bdist_wheel\n  running build\n  running build_py\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  creating build\n  creating build/lib.linux-x86_64-2.7\n  copying Lib/ldapurl.py -> build/lib.linux-x86_64-2.7\n  copying Lib/ldif.py -> build/lib.linux-x86_64-2.7\n  copying Lib/dsml.py -> build/lib.linux-x86_64-2.7\n  copying Lib/slapdtest.py -> build/lib.linux-x86_64-2.7\n  creating build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/init.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/async.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/init.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/deref.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/libldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/openldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/ppolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/psearch.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/pwdpolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/readentry.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/sessiontrack.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/simple.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/controls/sss.py -> build/lib.linux-x86_64-2.7/ldap/controls\n  copying Lib/ldap/cidict.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/dn.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/extop/init.py -> build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/extop/dds.py -> build/lib.linux-x86_64-2.7/ldap/extop\n  copying Lib/ldap/filter.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/functions.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/ldapobject.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/logger.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/modlist.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/resiter.py -> build/lib.linux-x86_64-2.7/ldap\n  copying Lib/ldap/sasl.py -> build/lib.linux-x86_64-2.7/ldap\n  creating build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/init.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/models.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/subentry.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/schema/tokenizer.py -> build/lib.linux-x86_64-2.7/ldap/schema\n  copying Lib/ldap/syncrepl.py -> build/lib.linux-x86_64-2.7/ldap\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  running egg_info\n  writing requirements to Lib/python_ldap.egg-info/requires.txt\n  writing Lib/python_ldap.egg-info/PKG-INFO\n  writing top-level names to Lib/python_ldap.egg-info/top_level.txt\n  writing dependency_links to Lib/python_ldap.egg-info/dependency_links.txt\n  file Lib/ldap.py (for module ldap) not found\n  file Lib/ldap/controls.py (for module ldap.controls) not found\n  file Lib/ldap/extop.py (for module ldap.extop) not found\n  file Lib/ldap/schema.py (for module ldap.schema) not found\n  reading manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'\n  reading manifest template 'MANIFEST.in'\n  warning: no files found matching 'Makefile'\n  warning: no files found matching 'Modules/LICENSE'\n  writing manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'\n  running build_ext\n  building '_ldap' extension\n  creating build/temp.linux-x86_64-2.7\n  creating build/temp.linux-x86_64-2.7/Modules\n  gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DHAVE_SASL -DHAVE_TLS -DHAVE_LIBLDAP_R -DHAVE_LIBLDAP_R -DLDAPMODULE_VERSION=2.4.44 -IModules -I/usr/include -I/usr/include/sasl -I/usr/local/include -I/usr/local/include/sasl -I/usr/local/include/python2.7 -c Modules/LDAPObject.c -o build/temp.linux-x86_64-2.7/Modules/LDAPObject.o\n  In file included from Modules/LDAPObject.c:9:0:\n  Modules/errors.h:8:18: fatal error: lber.h: No such file or directory\n   #include \"lber.h\"\n                    ^\n  compilation terminated.\n  error: command 'gcc' failed with exit status 1\n\nFailed building wheel for python-ldap\n  Running setup.py clean for python-ldap\nFailed to build pyldap python-ldap\nInstalling collected packages: pytz, django, python-ldap, django-auth-ldap, pyldap\n  Running setup.py install for python-ldap ... error\n    Complete output from command /venv/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-8oDn1L/python-ldap/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-CsVUiX-record/install-record.txt --single-version-externally-managed --compile --install-headers /venv/include/site/python2.7/python-ldap:\n    defines: HAVE_SASL HAVE_TLS HAVE_LIBLDAP_R\n    extra_compile_args:\n    extra_objects:\n    include_dirs: /usr/include /usr/include/sasl /usr/local/include /usr/local/include/sasl\n    library_dirs: /usr/lib /usr/lib64 /usr/local/lib /usr/local/lib64\n    libs: ldap_r\n    running install\n    running build\n    running build_py\n    file Lib/ldap.py (for module ldap) not found\n    file Lib/ldap/controls.py (for module ldap.controls) not found\n    file Lib/ldap/extop.py (for module ldap.extop) not found\n    file Lib/ldap/schema.py (for module ldap.schema) not found\n    creating build\n    creating build/lib.linux-x86_64-2.7\n    copying Lib/ldapurl.py -> build/lib.linux-x86_64-2.7\n    copying Lib/ldif.py -> build/lib.linux-x86_64-2.7\n    copying Lib/dsml.py -> build/lib.linux-x86_64-2.7\n    copying Lib/slapdtest.py -> build/lib.linux-x86_64-2.7\n    creating build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/init.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/async.py -> build/lib.linux-x86_64-2.7/ldap\n    creating build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/init.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/deref.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/libldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/openldap.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/ppolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/psearch.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/pwdpolicy.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/readentry.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/sessiontrack.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/simple.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/controls/sss.py -> build/lib.linux-x86_64-2.7/ldap/controls\n    copying Lib/ldap/cidict.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/dn.py -> build/lib.linux-x86_64-2.7/ldap\n    creating build/lib.linux-x86_64-2.7/ldap/extop\n    copying Lib/ldap/extop/init.py -> build/lib.linux-x86_64-2.7/ldap/extop\n    copying Lib/ldap/extop/dds.py -> build/lib.linux-x86_64-2.7/ldap/extop\n    copying Lib/ldap/filter.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/functions.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/ldapobject.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/logger.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/modlist.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/resiter.py -> build/lib.linux-x86_64-2.7/ldap\n    copying Lib/ldap/sasl.py -> build/lib.linux-x86_64-2.7/ldap\n    creating build/lib.linux-x86_64-2.7/ldap/schema\n    copying Lib/ldap/schema/init.py -> build/lib.linux-x86_64-2.7/ldap/schema\n    copying Lib/ldap/schema/models.py -> build/lib.linux-x86_64-2.7/ldap/schema\n    copying Lib/ldap/schema/subentry.py -> build/lib.linux-x86_64-2.7/ldap/schema\n    copying Lib/ldap/schema/tokenizer.py -> build/lib.linux-x86_64-2.7/ldap/schema\n    copying Lib/ldap/syncrepl.py -> build/lib.linux-x86_64-2.7/ldap\n    file Lib/ldap.py (for module ldap) not found\n    file Lib/ldap/controls.py (for module ldap.controls) not found\n    file Lib/ldap/extop.py (for module ldap.extop) not found\n    file Lib/ldap/schema.py (for module ldap.schema) not found\n    running egg_info\n    writing requirements to Lib/python_ldap.egg-info/requires.txt\n    writing Lib/python_ldap.egg-info/PKG-INFO\n    writing top-level names to Lib/python_ldap.egg-info/top_level.txt\n    writing dependency_links to Lib/python_ldap.egg-info/dependency_links.txt\n    file Lib/ldap.py (for module ldap) not found\n    file Lib/ldap/controls.py (for module ldap.controls) not found\n    file Lib/ldap/extop.py (for module ldap.extop) not found\n    file Lib/ldap/schema.py (for module ldap.schema) not found\n    reading manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'\n    reading manifest template 'MANIFEST.in'\n    warning: no files found matching 'Makefile'\n    warning: no files found matching 'Modules/LICENSE'\n    writing manifest file 'Lib/python_ldap.egg-info/SOURCES.txt'\n    running build_ext\n    building '_ldap' extension\n    creating build/temp.linux-x86_64-2.7\n    creating build/temp.linux-x86_64-2.7/Modules\n    gcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DHAVE_SASL -DHAVE_TLS -DHAVE_LIBLDAP_R -DHAVE_LIBLDAP_R -DLDAPMODULE_VERSION=2.4.44 -IModules -I/usr/include -I/usr/include/sasl -I/usr/local/include -I/usr/local/include/sasl -I/usr/local/include/python2.7 -c Modules/LDAPObject.c -o build/temp.linux-x86_64-2.7/Modules/LDAPObject.o\n    In file included from Modules/LDAPObject.c:9:0:\n    Modules/errors.h:8:18: fatal error: lber.h: No such file or directory\n     #include \"lber.h\"\n                      ^\n    compilation terminated.\n    error: command 'gcc' failed with exit status 1\n----------------------------------------\n\nCommand \"/venv/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-8oDn1L/python-ldap/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-CsVUiX-record/install-record.txt --single-version-externally-managed --compile --install-headers /venv/include/site/python2.7/python-ldap\" failed with error code 1 in /tmp/pip-build-8oDn1L/python-ldap/\n```\npyldap was included as a dependency on Python 2, while it should not (the error on the pip install is with trying to build pyldap)\nIt's hard to make 100% reproducible case, as I'm not sure of the pattern used to select the wheel when multiple are detected as ok.. @suutari-ai In my case, the best_match still point to Python2 wheel, and I get pyldap.\nBut what you explained happens almost exactly in get_dependencies, within this call:\nself._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\nI'll try and use a context manager to keep the monkeypatch of \"Wheel\" only in the context of get_hashes. I think that should do it.. @azaghal Seems like the same issue yes, assuming you're on Linux.. I've got something working that fixes this while keeping the feature from #520.\nIt'll be a fix, not a piece of art.\nI should be submitting a PR soon-ish after cleaning my stuff up.. @azaghal We're currently unable to make releases, as the PyPi upload has been deactivated for all project part of Jazzband at the moment (see https://github.com/jazzband/pip-tools/issues/531 and https://github.com/jazzband/roadies/issues/64). That's in the hands of the admin/roadies right now.\nAs soon as we're able to make new releases, it will be my pleasure to make one (believe me, I'm waiting for it too).. This means you have a dependency conflict in your requirements. It's not a bug.\nYou specified django-prices>=0.4.11,<0.5a0.\nOne of the other package (or one of its dependencies) requires django-prices>=0.6.1, which conflicts with the <0.5a0 you specified.\nYou can:\nA) remove your <0.5a0 requirement if it's not needed.\nB) Look at the verbose output of pip-compile -v and find which packages (and version) requires django-prices>=0.6.1. You can use grep or a similar search tool to help you out.. I currently see no real value for this in pip-tools.\nDo you have a concrete example as to why you would need to merge multiples requirements.in like that?. Alright, allow me to rephrase that: Why do you have multiple requirements.in file with duplicates that need merging? What is your workflow?\nMy current stance on this is \"that's not in the scope of pip-tools\", but I wanted to take the time to consider your situation, in case I missed something that would make this part of \"pip-tools' job\" before saying yes or no.. Thank you for the detailed explanation, that did clarify the need a lot.\nThis is a pretty specific need, and I don't think this should be done and maintained in pip-tools. I understand that pip-tools has a part of the logic that could be used to do this, but the proper feature doesn't fit in pip-tools.\nI think this is a job for a separate tool, or even a feature request in pip itself, like supporting/merging duplicate requirements from the input files. In any of these cases, feel free to ping me to get some help or input.\nThank you for your time and using pip-tools.  . Hi @twig,\nThis is actually a normal behaviour: pip-compile's job is to find candidates that respects all constraints.\nIn this specific case, you required requests==2.18.4, and vero requires specifically requests==2.7.0.\nYou effectively end up with a version constraints conflict, and that's what pip-compile is telling in the end.\nBy installing requests==2.18.4, it would go against the requirements of vero. If you were to do a straight pip install -r requirements.in followed by a pip check, it would tell you that the requirements of vero are not met, having requests==2.18.4 while requests==2.7.0 is required.\nI hope that makes sense.\nWe likely need an explanation of this in the README, you're not the first one to ask.. You might be interested to know that vero v2.0.1 has unpinned requests to avoid unnecessary dependencies conflicts.\nRelated issue: waveaccounting/vero-python/issues/26\nNow, about that override option, let me present it this way: by overriding the version of requests installed, you would deliberately go and install a version that goes against the expectations of some of your other dependencies that explicitly stated it required requests==2.7.0 exactly.\nThere are cases where it could be ok, but in most cases, it comes down to having packages defining the right requirements constraints for their use cases instead. A lib strictly pinning some of its dependencies should have a really good reason for doing so.. First @jcerjak, thanks for the PoC. If I get this right, the idea in your PoC is that when getting a conflict, with --allow-conflicts, we try again with the original requirement from the requirements.in file as an override, and if that requirement can be met, we warn the user and go on.\nThe idea makes some sense as it's similar to the way pip goes through requirements by default: handling only the top-level or first met dependency requirement. If I was to agree on adding this option (oops, spoilers!), that would likely be the way I would suggest to do it.\nBut, here are my concerns. First, the technical concerns:\n- Make sure that this not corrupt the dependency cache in some way that registers that requests==2.18.4 is a valid dependency for vero==2.0.0.\n- Make sure that re-running pip-compile without --allow-conflicts and an existing requirements.txt will not keep requests==2.18.4 and will raise a conflict error again.\n- Check what happens with pip-sync with a file like that. In this case, I'm pretty sure that the pip install vero==2.0.0 done by pip-sync will uninstall requests==2.18.4 and install requests==2.7.0, as it processes the lines one-by-one.\nBeyond that, my main concern is this: This option allows users to easily create a requirements.txt file that is incoherent, and break one of the promise of pip-tools. Why use pip-tools then? Just use pip install+pip freeze > requirements.txt. Maybe use pip check to spot some incompatibility and fix them as needed. As I've mentioned, the suggested behavior is like pip: If someone doesn't need the guarantee of coherency and reproducibility from pip-tools, why use pip-tools? IMO, this guaranteed coherency is one of pip-tools's strength.\npipenv has taken a similar approach, the --skip-lock option, which falls back to installing as pip would, essentially skipping the pip-compile part. I believe this is sane: do not create a Lockfile (or requirements.txt) that is incoherent.\nI believe those dependencies conflicts should be addressed at the lib level, rather than making it easier to ignore the problems. I believe that lib maintainers should be encouraged to be thoughtful on their dependencies constraints, and adopt good practice that are future-proof (ex: leave the pinning and downpining to the app level, give leeway). If a lib is slow-moving, then I would highly suggest that the maintainers avoid adding tight constraints that will be a pain to users. For the abandoned lib, that's a shame, and that falls in the list of consequences of un-maintained libs.\nSo, in my opinion, this would hurt more than it would help down the line. I do not see this as an harmless option that will help some users, I see this as opposed to what pip-tools tries to achieve.\nNow, this is my take on this. This is an open-source and Jazzband project. Anyone who disagrees with  my vision is strongly encouraged to say so. If this is a widely desired option, then OK, I'm not here to impose my vision on everyone. I'm acting as Lead, not Boss.. To isolate the issue, could you try performing a pip-compile -v of that requirements.txt (preferably named requirements.in to avoid overwriting it with your test)?\nThat should give us some more information, and also figure out where the issue is coming from.\nI've been using -e requirements daily with pip-tools, with no issue like this, so there's some digging to do.\nAs a sidenote, I should mention that pip-tools have been locked out of any releases for over 3 months (see jazzband/roadies/issues/64), and we have no idea of how long the situation will persist. This could be an issue for pipenv if this persists, @kennethreitz.. That would point out there's a difference between how the pip-compile script and pipenv interact with the Resolver. From the top of my head and a really quick glance at pipenv's code, I'd bet the difference is with how the editable requirements object is defines/created/handled on pipenv's side before passing it to the Resolver.\nIf I compare:\nhttps://github.com/kennethreitz/pipenv/blob/975b8911cc2341db95392dee582f9fe502f8f925/pipenv/utils.py#L105\nand\nhttps://github.com/jazzband/pip-tools/blob/2439a7f586f336c19dae513efbdf3e4dba2c9698/piptools/scripts/compile.py#L172\npipenv uses pip.req.InstallRequirement.from_editable directly,\npip-tools uses pip.req.parse_requirements instead.\nI don't have the time to go through pip's code right now, but IIRC, it's not as straightforward as parse_requirements calling InstallRequirement.from_editable for lines starting with -e.. Carrying over the environment marker would't be easy, because that's handled directly by pip and we simply get the resulting dependencies after their evaluation.\nEven if we'd manage to do this in a maintainable way, we'd have to compute the dependencies for each condition branching from those markers for the output to be exact.\nSo unfortunately, the generated requirements.txt are not guaranteed to be \"environment independent\", and IMHO, trying to achieve this would be a really hard task that would require a lot of testing.. #460 was really only about keeping explicit environment markers from the requirements.in, not being environment-agnostic (you can see the discussion of PR, suutari-ai covers it well). It's only a feature to deal with simple well known cases with no subdependencies where it is safe to simply keep the environment markers.\nAs an example, you could add enum34; python_version < '3.4' to your requirements.in, as you know it will be one of your dependencies in your example, and it's risk-free as enum34 has no dependencies, so the resulting requirements.txt should not vary. This gives you an environment-agnostic requirements.txt for this simple case, which could not be done before that PR.. pip-compile will act like pip: If no constraints are given, it will fetch the latest version of a package.\n\nwhy didn't it pick the 1.x instead of trying to go to 2 since that is incompat?\n\nBecause pip-compile does not try to \"backtrack\" to find a matching version.\nIt's a shame, but it's not a trivial thing to do. See #397 for a bit more info.\nIn your specific case, I'd say downpinning django-celery-email <2.0 in your requirements.in would be your solution.\nHope this helps!. There's an issue on that: #473. It does gets asked a lot, and I agree it would really help.\nI had started some work on this, but had no time to work on it for a while.\nThe general idea would probably be to populate the InstallRequirements.comes_from property and to display it on failure, although it gets messy. grep is indeed your best friend right now.\nI'll admit I'm waiting for #531 to be resolved before putting more energy in new features right now.\nContributions are still welcomed though!. Closing, keeping #473. Checked out your branch, tested, it works.\nThis also fixes #433 (I can reproduce everytime using the --rebuild option).. I noticed your branch isn't based on master. I'll rebase your branch, add the changelog entry and submit a PR, that'll save you the trouble.\nThanks for the nice work!. \"Merged\" by #568 \nThanks again @suutari-ai . The error occurs only in Python 3+.\nI do not think this is related to #566. The error occurs in sync.py, the traceback is pretty explicit here.\nInstallRequirement does not define <, so it fails.\nI seems like Py2 has no issue with that and falls back to a default '<', while Py3 raises a TypeError instead (I don't know a lot about Py2 vs Py3).\nIt's pretty bad the tests did not catch that. I'll try to fix that ASAP.\n. @jdufresne This works on Docker for me, can you confirm this works for you?\n(pip install git+https://github.com/vphilippon/pip-tools@fix-pip-sync-py3#egg=pip-tools). Requesting review from @suutari-ai and @jdufresne \n(As a side note, any reason why I wasn't able to add you as \"official reviewers\"?). @suutari-ai  Thanks for the review and detailed explanation of your concern. As @jdufresne mentionned, while a compiled requirements.txt is not guaranteed to be cross-platform compatible, it can be in some cases, like his.\n\nWhy would I ever want hashes of non-compatible wheels to my requirements.txt?\n\nIn the case where a requirements.txt is cross-platform (i.e. there are no dependencies difference between platforms), but the packages still provides different wheels per platform (i.e. these packages requires compilation), having all hashes would keep the happy case of having a requirements.txt that is cross-platform compatible.\nCurrently, I see no harm in having --generate-hashes including all hashes.\nI agree with you that reusing a requirements.txt on multiple platforms is something that should be done really carefully, and on a case-by-case basis after making sure there are no dependencies difference. That's the case of @jdufresne. Following the \"we are all adult here\" mindset of Python, I wouldn't feel justified to remove this feature as I was able to make it work and it provides some value to some users.\nIf it had been impossible (or way too hard) to keep the feature without breaking the dependencies resolving, I would have suggested to revert of course.\n@suutari-ai I think a case justifying the feature to be kept has been provided. If you have a case or example that would prove this feature to be harmful to the users, outweighing the gain, please provide it (this is exactly why I requested your review, I definitely wanted a second opinion on this).\n. @suutari-ai @jdufresne Thanks you both for your input on this.\nI'll go ahead and merge the fix.\n. Out of curiosity, could you try this and see if the problem occurs:\necho \"-e .\" | pip-compile -o requirements.txt - (don't forget the - at the end, it's required to read from stdin)\nIn short, this will do a pip-compile of -e . instead of parsing the setup.py directly.\nIf it works, then the issue is likely with the way we handle the setup.py parsing, which is my guess.\nIt would also possibly give you a workaround until we fix it.. Allright, thanks!\nI was hopping to get a 1.10.2 release (including a pretty important bugfix), so I was keeping master stable without merging additional PR for now, but its unfortunately out of my hands. It might take a little while before we merge this, but it should go in without any issue later on.\nAnyway, even if I was to merge it right away, we still can't make any release.. @simlun We should be good to get stuff moving again.\nYou can rebase on master.. Thanks for the comeback.\nFor the other PRs, I intended to go through them, check what each did, and cross-check them to check if one of them covered a corner case another one didn't and making sure we got the best fix/solution at the end.\nI'll get on re-reviewing this as soon as I got time to. Also, if you want to go ahead and make that cross-check right yourself, that will certainly help out.. Edit: tl;rd, this comment on \"pinning VCS\" still applies: https://github.com/jazzband/pip-tools/pull/372#issuecomment-250093591\nI did some more tests with the PR and noted something: The URLs in the resulting requirements.txt are not \"pinned\" in any way. This means that if someone puts git+https://github.com/jazzband/pip-tools.git@master#egg=pip-tools in the requirements.in, it stays as-is in the requirements.txt. master is not converted to the SHA of the current HEAD of master. We can't have that in the requirements.txt: When the HEAD of master will change, the requirements.txt will become invalid, as if we had any unpinned dependencies in it.\nThe behavior of pip is that if you do \npip install git+https://github.com/jazzband/pip-tools.git@master#egg=pip-tools\nthen pip freeze will give something like\nclick==6.7\nfirst==2.0.1\npip-tools==1.11.1.dev3+ga2446fa\nsix==1.11.0\nwhich gives the \"version\" of pip-tools on master at that moment. But pip looses the \"source\" of the dependency in that case, which is not ok in our case. That means pip-tools might need its own logic to determine the SHA (and implement a logic for the various supported VCS, git is not the only one out there). We might be able to reuse the logic that pip has that pins the SHA when installing with -e.\n(This suddenly seems to explain why only the editable mode was ever supported in pip-tools with VCS URLs) (Edit: Yup, after re-reading through the comments of #372, that's it)\nIt looks like there's a fair amount of stuff that needs to be dealt with for this feature.. I fixed the exact same issue (with the same package) in pipenv by updating its patched-and-vendored version of pip-tools to use the latest fix in pip-tools master branch. I'm pretty sure it should be fixed.\nWhen trying with the master branch, be sure to clear you cache too.. Closing, tell us if the issue still occurs on master.. (Nevermind, reopening to maybe avoid duplicated issues, as I have no idea of when an official release will be made.). This is now fixed in 1.10.2\nMake sure to clear you dependency cache: pip-compile --rebuild. I don't have access to an OSX machine, but I'm unable to reproduce this on Linux.\nOne other difference I spotted is that you're using pyenv. I don't know if it could cause any issue here.\nOther than that, you can try using the --rebuild flag and/or deleting the existing requirements.txt if one already exists.\nIf you want to go further, if you can log or inspect the ireq object in piptools/utils.py, line 121, knowing which requirements is in cause would help.. Thanks @jdufresne, small steps forward!. Thanks @jdufresne !. Without installing the dependencies, right now you can:\n- run pip-compile --upgrade\n- Diff your requirements.txt using your source control (or do a diff of a previously made copy of your requirements.txt)\nThis is a sensible way to do it IMO that does not require additional work or feature in pip-tools.\nYou can stay fresh and keep an eye out for risky package upgrades.\nIf anyone thinks that pip-tools really needs a new command for this, come have a talk!\nClosing, thank you for using pip-tools \ud83d\udc4d . Just a quick update: I did a quick test, installing the head of pip's master branch, and pip-compile does not work straight out of the box with pip 10 as of now (that was to be expected).\nThere will be work to do.. > Let me know how I can help.\nSend a PR to make pip-tools work with pip 8.0.1 to 10* \ud83d\ude04. (Yeah, I know, that's a big one)\nOtherwise, we now have your name if we need a contact, that's a good start, thanks!\n* That's an ideal case, but I'm aware we might have to drop some past version support. I'd rather not drop pip 9 support right away though, and with the major refactor, that might a big part of the issue. sigh. @pradyunsg Its allright, I just couldn't let the opportunity pass and not ask :).\nActually, if I'm not mistaken, you're working on bringning dependency resolution to pip, right? Is that something that's aimed for pip 10? That would change the game a lot here.\nAlso, I've been looking for a \"pip 10 change resume\" to figure out what changed and how things are working now. I you have some resources to point me to, I'd take it.. Allright, so the best short-medium term idea I have right now would be to vendor pip 9.0.1 as a whole and use it for the whole dependency resolution part.\n(Bonus point: I'm thinking about vendoring pretty much any dependency to stop having issues with pip-sync risking breaking itself.)\nI highly doubt anyone will have the time to redesign all of this to work with pip 10, and I'm not even touching the subject of retro-compatibility. Also the risks of breaking stuff are...\"fairly high\" (cough it's going to be a firetrucking mess cough). @davidovich \n\nThis is a great idea, considering that pip 10 will probably offer an alternative [...]\n\nJust to prevent any bad surprise, as @pradyunsg mentionned, pip 10.0.0 will likely not provide dependency resolution. This adds even more importance to have pip-tools work even if the user has pip 10 installed (which you already agreed on, I'm simply pointing out the additional need for support, confirming that being blocked to pip 9 would be a pain for many.)\n@pradyunsg \n\n[Tons of nice pip 10 stuff] won't be available to users of pip-tools.\n\nWe can vendor pip 9.0.1 for the dependency resolution part, and still rely on pip 10 (or whatever pip the user has installed) for the pip-sync part (IIRC, nothing too funky happens in there, TBC), and let users benefit from most of pip 10.\nFor PEP 518, I'm not sure that would go affect dependency resolution, but I might be wrong.\nFor better PEP 508 support, that's a bummer, but it might be easier for us to actually patch pip 9.0.1 for that part (those are words that I may regret later).\n. @taion The issue is that pip 10 also did breaking changes for us beyond the restructuration in pip._internal. I did some testing locally, using pip 10 and adding _internal in every import. That took care of the import issues, but the interface and role of the classes changed (namely RequirementSet, a core piece currently needed by pip-tools).\nOf course, if we vendor pip, it will make it easier to eventually change pip-tools code in order to use pip 10 without having to worry with compatibility break with pip 8-9.\nI would be happy if it could be done right away, but realistically, I highly doubt it will.\nOnce again, I would be happy to be proven wrong!. Thanks for the reports. That's a known issue, fixed on master (try it out).\nWe're eagerly waiting for the ability to make a 1.10.2 bugfix release (see https://github.com/jazzband/pip-tools/issues/531, pypi upload deactivated for Jazzband projects).\nSorry for the wait, it's in the hands of Jazzband's admin/roadies at the moment.. Thanks for the report.\nCould you try clearing your cache with pip-compile --rebuild?\nIf it doesn't work, give me the --verbose output.. \ud83d\udc4d \nAlso: This is now fixed in 1.10.2\nMake sure to clear you dependency cache: pip-compile --rebuild. Considering the pip-sync command, it needs to be installed in the same virtualenv that you want to sync, which is each project virtualenv, because pip-sync will simply perform packages install/uninstall in the current active virtual environment.\nThat means option 4.\nI don't know much about pipsi, but if tools installed with pipsi are available when in another virtualenv (like a given project's virtualenv), then it could work (or cause horrible unforeseen side-effect). If you want to give us you're feedback on how the pip-sync command work in these condition, it could be good to know, and document if needed.\nAt the moment, I think that option 4 is the only fully valid option that will work with pip-tools as a whole (not only pip-compile). That should indeed be clarified in the doc.. @bhrutledge It might not seem like it, but that's a pretty loaded question right there. It points back to the broader question \"how do I deal with dev requirements in Python\", and there's not a single viewpoint on this in the community, AFAIK.\nAlthough, I'm not an expert on all the fancy options available in setup.py using setuptools, but I would be very careful about including pip-tools somewhere in a setup.py. I would deal with pip-tools the same way you deal with pip. If there was a \"dev_requires\" section in setup.py, it could be the right spot, although it would need some testing to make sure everything goes smoothely.\nPersonally, I keep my dev packages in a separate dev_requirements.in/.txt. But in the end, it comes down to using a way that fits your need, as long as you're aware that this is part of your \"python environment tooling\", like pip is.. Hey @costypetrisor, thanks for putting your time into this.\nI've looked at the pip call your trying to avoid, and I think that if you could simply have download_dir=None in those case, it would fix it.\nSo, the condition you've put to detect that we have an \"editable requirement locally available\" could be put in pypi.py, with the condition for VSC sources.\nIf you can give it a try, we could get this to be a maintainable patch, and merged.. @costypetrisor Could you rebase this on master? _compat/contextlib2 was removed.. @bhrutledge Sorry for the delay, trying to catch up on everything.\nWhat you wrote seems pretty good to me.\nNo need for changelog/tests on this.\nFor the build, that's fixed in master now.\nYou'll have to rebase on master, and we converted the readme to .rst meanwhile, so it essentially means you'll have to rewrite your thing, sorry about that.\nPing me as soon as your done, it should get right in.\nThanks!. > It also means that if a package does not exist on the current OS (eg for tensorflow-gpu on MacOS`), then compile fails.\nFor the record, it's the responsibility of the package requiring tensorflow-gpu>=1.2 to specify its only a linux/windows dependency if it doesn't exist on MacOS (assuming it supports MacOS itself), and pip-compile would respect that (except in 1.10.0 and 1.10.1, where its broken. Its fixed on master and should be part of 1.10.2, when a release will be possible).\nAbout having the ability to compile for a specific environment, its interesting, but really hard to do well. That likely means having to trick pip to believe its running in a given environment. And then we have the case of sdist packages (.zip, .tar.gz, etc) that need to be built, and could definitely be unbuildable on the current OS (as in, running the setup.py could be impossible on the current OS).\nIn other word, I wouldn't expect this to be done soon. Contributions are always welcomed, but I would point toward supporting the upcomming pip 10 first :smile:.. You should be able to do that currently (or something alike, environment markers are allowed in requirements.in. I'm not familiar with OS X: what's its sys_platform value?. But, if pip-compile respects the environment marker here (first line), then it shouldn't try to install that tensorflow-gpu==1.3.0 package on OS X.\npip-tools is supposed to respect the environment markers explicitely given in the requirements.in, so this really strikes me as odd.\nWould you give me the pip-compile --rebuild --verbose output of that?\n(Am I \"fighting\" to keep an issue open? I think I need to consult a professional....). My bad, the environment markers are simply copied to the resulting requirements.txt. It looks like it will still do the lookup and fail here. I have a hunch of how this could be fixed, maybe it wouldn't be so hard (famous last words) in our case. Although, don't hold your breath.\nI would need to check if PyPi actually provide an API to get those dependencies, but I doubt it.. Note to self: stop making \"guesses\" past 1:00 AM.\nThank you for the info, its good to know, maybe we can make something out of this.. Proper environment marker handling from the requirements.in was added in 2.0.0, and I'm currently documenting the \"official\" stance of pip-tools regarding cross-environment usage.\nIn short, pip-compile must be executed for each environment. We have the same issues described in this article about pypi regarding the execution of setup.py. We cannot safely and consistently know the dependencies required for a linux installation while on a windows installation, as an example.\nSo in the current state of things, it's a dead end. If someday there's a deterministic way to know the dependencies of any package without ever having to execute possibly environment-dependent code, then it'll be doable.. That's an issue fixed by PR #571. pip-tools is fetching a Python 2 dependency (subprocess32) even if its running in a Python 3 environment. It fixed on master, but not released yet (see #531, we can't make new releases at the moment).\nTry it out on master, an confirm if it works.\nIf it doesn't work, run pip-compile --rebuild to clear the dependency cache that could be corrupted.\n(P.S. Thanks for the well done report, it makes spotting those kind of things so much faster!). This is now fixed in 1.10.2\nMake sure to clear you dependency cache: pip-compile --rebuild. Thank you for sharing this \ud83d\udc4d \nAs you suspected, this is too hacky to get in pip-tools in the current state.\nI'll close this, as no work is planned to be done from here.. Hi @pgrzesik, sorry for the delay, this one fell into a crack on my side.\nRebasing on master should fix the build.. Thanks @pgrzesik !. This is a known issue fixed in master, and will be part of 1.10.2, when a release will be available (see #531 as of why a bugfix is not out yet).. Hi @therefromhere, thanks for looking into this.\nSo, you're saying this is just really a vendoring of contextlib2 (this was done before my time here I think)? If so, yeah, let's go ahead with a condition dependency for py2.7, using those sweet environment markers.. You simply missed a part of the exception message:\n\nCould not detect requirement name, please specify one with #egg=\n\nSo, at the end of your url, add #egg=django.. Well, first, my bad, I was confused by the last part of the url. The #egg= part should be the name of the package, so in this case: #egg=django-geoposition. <- (edited)\nAnd by taking a closer look, I realized you're not using a VCS url, you're directly using a zip/tar archive which isn't yet supported.\nSo you workaround at the moment would be to point so a branch/commit of your repo instead.\nPoke me again if you need any clarifications :).. I've looked at the linked issue, and I see why you want to do this. In your case, I might have a better alternative for your.\nThe best alternative I can give you is that you could download the archive and use the --find-links option, if you're going to use that precise archive for a while.\nIf I've looked at this properly, you would put django-geoposition>=3.0.1 in you requirements.in, and the do pip-compile --find-links /path/to/dir-containing-the-archive.\nThat's the best workaround I can think of right now, until pip-tools supports tar/zip archive urls directly.\nI hope this helps.\n. Hi @bittner, thanks for the report and suggestions.\nI would suggest to go with A). Mostly because:\n\nPypandoc uses pandoc, so it needs an available installation of pandoc. \n\nand there are no prebuilt wheels for Linux with pandoc, so that could be an issue with the BMs for the releases. Let's just keep the release process as simple as possible, especially in the Jazzband context :p. (But thank you for the info, I might use that in my own projects) \nThanks again, you can submit a PR, that would be really appreciated!. https://pypi.python.org/pypi/pip-tools/1.10.2rc1\nAllright, so this is a lot more readable than it used to be, hurray!\nBut the rendering seems to fail. I'll reopen for anyone who wants too look into this.. https://pypi.python.org/pypi/pip-tools/1.10.2rc2\nEh, still not working. I guess that's what RCs are for.\nI won't block 1.10.2 for this though, but it bugs me nonetheless. . Todo: try this out: https://github.com/pypa/warehouse/issues/2600#issuecomment-346156317. Thanks a lot @bittner.\nIf anyone can deal with #592, that would be great. Otherwise, I'll try to get on it once I can.\nI'll also come back for a proper review. At the first glance it seems nice, I'll have to take a closer look at the matrix split, as well as the rendered result.. @bittner I think your rebase overwrote the changes from PR #589, (sorry about that mess \ud83d\ude1e ).\nYou'll have to put those back in the README.rst.\nI think that if you click the \"allow edit to maintainers\", I could do it for you, as I made the choice to merge the other PR first. Tell me if you want me to take care of it.. It seems pypy is not available on the windows builder. You can just go ahead and remove it for now, I'll merge later tonight once it get green.\n(I sure would like Jazzband to give its members access to kill build jobs right now).. Thank you @bittner \ud83d\udc4d . Thanks you for the heads-up. Hi @lukeschlather, I see your use case, but I think it's not reasonably feasible.\nThe main issue is that within a version range for a package, its dependencies can change. This means that you cannot pin those dependencies. It ends up breaking the whole idea of the reproductible environment.\nMy suggestion would be to keep those in a seperate requirements file and use that file as is.. Hm, I see. I'll admit I'm not sure I see the value in this. If you want a newer version of your top-level packages installed, I would rather run pip-compile --upgrade-package <package-to-update> in order to keep the transitive dependencies unchanged.\nI feel like this option could do more harm than good to most users.\nStill, I'm open and ready to discuss about your usage, either to find another solution for you, or change my vision on this.. Closing, feel free to comment back, I'm still open to discussion \ud83d\udc4d \nThank you for using pip-tools!. @davidovich Thanks for the quick review!. Thought: Is it because the deploy step is only executed on the main repo, and every py2.7 job is waiting for the other jobs to finish before doing the deploy step itself, causing a deadlock?\nThe solution would be to select one of the specific job (say, py2.7 PIP=9) to wait and take care of the deploy. That's worth trying I guess.. \ud83e\udd47 . Thanks for the input, I'll go and read on that.. I think this should be good now. All iterations are there.. Thanks for the PR \ud83d\udc4d . I also think that pipdeptree is worth mentioning, as is can also help out with \"debugging\" the dependency tree.\nIt could be added in a \"Other useful tools\" section.\n. I would suggest to go with a pip-tools.cfg file or something alike, rather than tox.ini. I feel like options in tox.ini should be about testing and such.\nNice suggestion \ud83d\udc4d . I haven't looked much at the setup.cfg file. If it's meant to be extensible for other tools like that, then that sounds more reasonable to me.. Oddly, python setup.py --restructuredtext --strict would fail with a parse error before my change. But now I'm testing this on the master branch (without this change) from home, and it passes.\nI'll look deeper into this.\n. Ha! I think I found it: ..code-block:: <bash|ini> requires to have pygments installed, which isn't installed on the PyPi server. We have to fall back to the default .. code-block.\nThat falls in that category of issues:\n\nUsage of non-builtin lexers (e.g. bibtex) will pass locally but not be recognized/parsable on PyPI. So I tested with readme_renderer, and it did not find any error in the README.rst in master (which fails), so it's not enough.\n\nLooking at the readme_renderer ReadMe, it's said that the tool is for upload toward Warehouse, while we're still using the legacy-pypi (unless I've missed an update). Is it still worth adding another flow for the future here? Should we wait? I'm not sure, lets address this separately.\nHere's what I'll do: I'll merge this and make another RC to check if it fixes this.\nIf not, we'll assemble again! superhero pose\nThanks for your help everyone. Please keep coming back :). @jezdez Thanks, that's good to know. Then again, I'm left wondering why the rendering failed if this same official tool is used on the PyPi server.. So I just tried out doing the same as legacy-pypi with render_readme.rst.render, and if I uninstall pygments, it returns None, which would fallback to render_readme.txt.render, which matches pretty well what we see on the page.\nAs a bonus, I've also checked on Warehouse: No rendering either.\nhttps://pypi.org/project/pip-tools/1.10.2rc1/\nI'm not sure why pygments wouldn't be installed on legacy-pypi or Warehouse. I'll try to find where to report issues/questions about the live instances. That is if this PR fixes the rendering.\nEdit: Nevermind, Pygments works for other projects. That's not the issue.. /cc @bittner @jezdez . Credits to @bittner for tackling this in the first place \ud83e\udd47 . Might be a false alarm, my bad.\nI was thinking of some issues with pipenv using pip-tools, but it might be related to something else.\nLet's test this though just to be sure.. Alright, test done, we are safe here!\n(I would have been kind of surprised the issue was never raised before otherwise.). Thank you, this is really appreciated!. @jdufresne Thanks for the report, and good catch!\nI took a quick look at the code: pip-sync does a pip install pkg_a pkg_b ..., not a pip install -r requirements.txt, which is the cause of the issue.\nI feel like this could be changed, which could simplify pip-sync and align it more with pip. That would deserve a look.. This is pretty odd, there shouldn't be dupplicate.\nOne thing I noted though is that I can't find any PyPi distribution named jaeger-client-python, only jaeger-client. Plus, the branch you gave me provides a distribution named jaeger-client too (https://github.com/jaegertracing/jaeger-client-python/blob/unicode/setup.py#L19). \nBy the way, are you sure that your other libs depend on jaeger-client-python, and not jaeger-client? Because, as I said, jaeger-client-python doesn't exist.\nSo, first, the editable requirement should end with egg=jaeger-client.\nThis could be the cause of the error, although I'm not quite sure why.\nIf this issue still occurs after changing the egg name, I'll need:\n- the pip-compile --verbose output\n- a requirements.in file to be able to reproduce (which includes other packages that depends on jaeger-client.), if you can provide one.. Thanks for the code improvement \ud83d\udc4d . I suggest that we avoid naming such a file requirements.txt, because:\n- It will likely be overwritten and committed by people doing pip-tools testing (i.e. me :) )\n- It could be confusing and give a counter-example of what pip-compile is expected to generate.\nI suggest:\n- Keep the .gitignore unchanged (excluding requirements.in/txt)\n- Name the new file dev_requirements.txt\n- Put the dev dependencies in it:\n-e .\nmock\npytest\ntox. @jdufresne Thanks again for the contribution, sorry about the long pending.. Hi @davidjlloyd, thanks for the details.\nI did a small test and can point out this: you will get the proper result if you use a requirements.in file like this:\nrequirements.in:\n-e .\nHaving pip parse the requirements.in file, and then process the setup.py, works fine.\nThe \"direct\" setup.py parse done by pip-compile is incomplete and would need improvement.\nI would generally advise to use requirements files, its the battle-tested flow. Hindsight being 20/20, I might have voted against parsing the setup.py directly in retrospective. \nContributions are welcomed of course \ud83d\ude04 . Hi @jeremyarr \nTo use a private pypi, it works that same as pip: configure in pip.ini/pip.conf or using --index-url/--extra-index-url.\nFor the dependency_link issue, could you give me an example with an existing package?. Hello @nicain,\nThis flags already exists. Is there any extra messages still going through?. Hi @jasonm, yes PRs are always welcomed!\nI had the idea of making a more generic solution, passing all unknown flags to pip so we don't have to redefine them on pip-sync, but it was getting mixed with src_files and it was getting messy, so I've set that aside for now. We can add requested options directly on pip-sync as it's been done before for now.\nFYI, pip environment variables can also be used to set options not directly defined on pip-sync. \nEx: PIP_EXISTS_ACTION=w. Hi @graingert, thanks for the report.\nI'm almost certain this isn't an issue with pip-tools, but rather with one of the upstream library, likely urllib3.\nBut something's off: I thought that pip was vendoring its libs, so I'm surprised to not see the path to pip vendored lib there. Another thing that caught my eye is the share/python-wheels/urllib3-1.19.1-py2.py3-none-any.whl/..., I'm not used to seeing lib used outside of site-packages, even less directly from wheels.\nI have a pretty strong gut feeling that something special is going on with this setup. You probably have a better idea of what's going on than I do.. Well I learned something today: virtualenv vs venv!\nI really need to get up-to-date on python 3 (shakes fist at my own legacy projects).\nBack on track: I found a similar issue, regarding pip rather that pip-tools:\nhttps://stackoverflow.com/questions/37495375/python-pip-install-throws-typeerror-unsupported-operand-types-for-retry#comment62486615_37495375\nSkipping the part pointing out to a network issue (you got that figured out already), others are pointing to the surprising part where pip isn't using its vendored libs. In that case, venv is also used.\nThen, I searched a bit on pip Github issue tracker:\n- https://github.com/pypa/pip/issues/3943#issuecomment-245306197\n- https://github.com/pypa/pip/issues/4779#issuecomment-335945617\nThat seem to be related to the patched pip version distributed by Ubuntu.\nIn the end, this is not on pip-tools side*.\nI hope you have a few pointers nonetheless!\n* (That is, until we make the move to vendor pip to avoid the major breaks from pip 10. I won't have the luxury of throwing back the ball then.). For correctness for anyone reaching this: The cause is that boto3 actually has a dependency on botocore>=1.8.29 in this case, effectively conflicting with the explicit requirement of botocore<1.8.0 given in the requirements.in here.\nIt isn't related to nested editable dependency (boto3 has none).. Hello @tuukkamustonen, sorry for the delay.\nYou are right, this is due to how we handle the markers in the requirement file. pip-compile currently has a very partial support for environment markers, mostly for happy cases. \nThis is a known issue; pip-compile currently doesn't interpret environment markers in a requirement file itself. It simply brings them over to the output file. As pip-compile ends up processing mypy's setup.py on python 2.7, you get this result.\nThe reason it \"worked\" under v1.10.0 and v1.10.1 was actually due to a bug where the python 3 wheels would be used even on python 2 to get the dependencies (effectively meaning you were getting the python 3 variant of the dependencies, which may or may not be different). As a result, the .tar.gz was never used, the setup.py not executed, and you weren't getting any issue (beside getting the dependencies list of a python 3 wheel while on python 2, of course).\nThis is something I would like to see fixed in the future of course, but I'm bit short on time.\nContributions are always welcomed \ud83d\ude04 . Hi @onyb, thanks for your offer!\nI can point you to https://github.com/jazzband/pip-tools/blob/master/piptools/scripts/compile.py#L187 and https://github.com/jazzband/pip-tools/blob/master/piptools/resolver.py#L74\nAlthough, the solution might be be to do something similar to https://github.com/jazzband/pip-tools/blob/master/piptools/sync.py#L114 (use .match_markers) to filter packages based on the system.\nBut we have to be carefull with to not break the generate_hash logic that generates hashes for all platforms.\nThat being said, filtering dependencies based on the current platform could also cause some issue with the work done in https://github.com/jazzband/pip-tools/pull/460.\nIf we mix the 2, what I forsee is that we will get in a state where we compute dependencies only for the dependencies of the current system/environment, while introducing potential \"unresolved\" packages in the final file, essentially breaking the reproductibility promise.\nIf we want to support the workflow where you have a single requirements.in file with markers, with which you perform a pip-compile on each target environment (python version + OS), then we would have to revert https://github.com/jazzband/pip-tools/pull/460, as it appears simply incompatible to me.\n@barrywhart @suutari-ai @davidovich Any input here?. @onyb i did the same check as Barry, and couldn't reproduce either. What's the setuptools version in the virtualenv? Older version did not interpret markers in the install_requires. Otherwise, check if functools32 might be coming from another package which doesn't have the environnement marker.\n--\n\n[...] it's trying to do something that isn't really well supported by the underlying tools. That is, there's no universal, cross-Python version/environment way to get dependency information for a package.\n\n@barrywhart You summarized the whole pain of trying to deal with cross-environment dependency resolution. Thank you \ud83d\udc4d \nRegarding your functools32 example, I tried to pip-compile under python3, and it fails due to being python2 only. That is because the environment marker doesn't prevent the attempt of dependency resolution of functools32.\nBut, if I perform the pip-compile on python2, then it will succeed, and I would have a requirements.txt \n with the environment marker on functools32.\nIn other words, the pip-compile has to be done in an environment where all packages are installable to succeed.\nMy honest feeling is that pip-tools is never going to be able to generate a true cross-environment requirements.txt. The main reason is the non-deterministic aspect of setup.py, as described here: https://dustingram.com/articles/2018/03/05/why-pypi-doesnt-know-dependencies\nI think that a clear statement regarding this should be made: That the vision of pip-tools is to generate a requirements.txt that is complete and reproducible for the environment on which it was generated (with the possibility of an exception of the generate-hashes, for the happy case where the dependencies are the same across environments). This means that a requirements.in should be pip-compile'd on each target environment individually. With that said, we can improve pip-tools by having it parse the environment markers in the requirements.in, as pip, would and include/exclude the top-level dependencies from the dependency resolution. That would effectively exclude it from the final requirements.txt too.\nSadly, this wouldn't cover the case where a lib forgets to put environment markers on a package, as given by @barrywhart . The \"real\" answer is to have the offending package fix its dependencies, as it would with any version conflicts. The only other way around that would be to keep the environment markers from requirements.in in the requirements.txt as we do now, and in this particular instance the python2 version of the requirements.txt would work on python3, if there's no other variation. Maybe this could be added as an option instead to avoid unneeded markers for users that follow the suggested flow.\nTo be a 100% honest, I would prefer this feature (copying environment markers) to be removed. I hate to take away any contribution to the Open-Source (heck, we need more!), While it appeared to be a first step toward a better solution at first, I fear that keeping this feature would give us a lot of headache and a lot more trouble with this change now. I had to deal with a similar behavior when offering help and support with Pipenv, and I'm absolutely not willing to offer the same help and support to users here; It was really rough and confusing for everyone. I also doubt pip-tools has enough other knowledgeable contributors to take care of that user support.\nHere is a are some examples of issues users would likely face and request support for (or worse, not be aware of at all):\n- Users would use the option to keep the markers, and then be surprised to see th package disappear from the requirements.txt when pip-compileing  on another environment.\n- Users would use the option to keep the markers and put a marker on a package with dependencies of its own that are possibly not required on the current environment, be lead to believe that they have a \"happy case\" where the requirements.txt is cross-environment, and then proceed to install on another environment, resulting in a non-deterministic installation with packages not listed in the requirements.txt, breaking one of pip-tools base promises.\n- Users would use the option to keep the markers and put a marker on a package with dependencies of its own that are possibly required only on the current environment, be lead to believe that they have a \"happy case\" where the requirements.txt is cross-environment, and then proceed to install on another environment, resulting in installing packages that are meant for another environment.\n@barrywhart If you have a maintainable idea to keep that feature to un-break some cases as you mentioned, I would be happy to hear it.\nOf course, such a change would be done as part of a Major release.\n(That was quite a wall of text. There was a lot to point out. I really wish this was easier to deal with.). @barrywhart That was a general question. And of course, I intend to better document and put upfront the idea that pip-tools aims to take care of a single environment.\nI'll let the idea of keeping the optional marker-copy behavior running in my head for a while, see how it comes out.. Todo: Document the expectation that pip-compile should be executed for each target environment.\n@barrywhart For your first idea, that wouldn't play too well with the --no-header option. Also, I'm not too keen on adding additional things like that: I'd like the requirements.in/requirements.txt files to stay \"pure pip requirements file\". I like the idea of trying to warn/prevent a user trying to run a file on the wrong environment, but I think this kind of \"advanced\" stuff would be better suited with pipenv.\nFor your second idea, that sounds like a good tool idea. I wonder how well it could be done simply with tox without any additional work here (except maybe documented examples), or how well it would stand as a standalone tool made to deal with other cross-environment tasks. I have a hunch that we'd be better off with a distinct tool, but I might be biased as a kind-of-busy-maintainer.. Hi @chaoflow, thanks for the report.\nI was able to reproduce the issue by doing simply this:\n1. python3.6 -m venv venv\n2. ./venv/bin/pip install -e git+https://github.com/pallets/click@55682f6f5348f5220a557f89c3a796321a52aebf#egg=click\n3. $ python\n    >>> from click import *\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n    AttributeError: 'module' object has no attribute 'FloatRangeecho'\nI'm not sure what's going on. FloatRangeecho isn't even a type defined in click. Something seems odd. Doing from X import * is generally discouraged, but is still valid.\nThis can either be raised over to the click devs, or assumed to be a quirk of this dev, unreleased version of click.\npip-tools could be adjusted to only import the necessary click modules to avoid this specific issue.. Hi @john-bodley, \npip-tools will not be changed to attempt to generate a requirements.txt files for other Python versions than the one of the current environment.\nThe reasons are pretty much covered in my recent comment here https://github.com/jazzband/pip-tools/issues/635#issuecomment-374291159 as well as with this article: https://dustingram.com/articles/2018/03/05/why-pypi-doesnt-know-dependencies\nIn short, the dependency resolution results depends on way to many variable to try and fake them to obtain a reliable result.\nWhat should be done is to execute a pip-compile on each target environment (ex: in each distinct virtualenv). With that in mind, pip-tools should be improved to interpret the environment markers in the requirements.in to generate a valid requirements.txt for the environnement in which it is executed. That would make the requirements.in reusable as an input file to pip-compile in each environment. \nThat's the plan so far regarding the multi-environment support.\n. Thanks for that first contribution @justicz, nice work!. A pip-compile with an existing requirements.txt would not have updated the versions (pins) unless required to, or explicitly told to do so with --upgrade or --upgrade-package. I think this PR actually aligns --generate-hashes with that: No changes unless required or explicitly told to do so.\nNow we might want to add and option analog to  --upgrade/--upgrade-package regarding hashes. Maybe something like  --regenerate-hashes. Manual edits to the compiled requirements.txt can lead to a lot of trouble, and I usually advise against it (warranty void if touched \ud83d\ude1b ) .\n(btw @graingert thanks for the observation, and for poking maintainers for wheels too \ud83d\ude04 ). @justicz Your point on increased security sounds right to me, but don't quote me on this, I don't consider myself enough of a security guy to argue that it's better or worse in terms of security.\nWhat I can say though is that I prefer this new behavior regarding how and when the requirements.txt and its content gets to be modified by a pip-compile.. Hi @jbergknoff-10e, thanks for the PR!\nFirst-off, I'd like to point out that using pip install -r requirements.txt with a requirements.txt generated from a pip-compile would likely be sufficient in a container IMO, as I you're always starting from fresh, so you shouldn't have to uninstall any pre-existing python packages, right?\nNonetheless, this is still a good feature to have on pip-sync.\nFew things though:\n- We should name the flag --user to mirror pip interface (I know that this isn't consistent on all options, we'll get that done over time).\n- We should pass the --user flag to the install command so that the packages are installed in the configured user local directory.\nFor the tests, given the nature of the change, you can skip that. Sometime, \"perfect\" is the enemy of \"better\".. Excellent, thanks for the tests and detailed report.. Thanks for the PR!. Allright, agreed. We'll track this one for the case of the first generation.. I didn't change the test matrix to stop testing on multiple pip version right away. I want to make sure that it still works with earlier pip versions (as I took away some conditional code). If it passes, I could clear the matrix to make CI faster.. @pradyunsg FYI. There seems to be some issue when running through coverage, I'll get digging (likely tomorrow).. - [x] Don't forget to Vendor pip 9.0.3 now that its out, while at it.. For the record: I installed pip 10 (commit https://github.com/pypa/pip/tree/b20235d7c9e6a7ece969567ad0a21b55aea96624), and both pip-compile and pip-sync work fine so far. I went as far as to uninstall pip to test pip-compile, and it keeps working.\nLook like we'll survive through this!. @JoergRittinger Thanks for the detailed example.\nYour original idea seems to be the best one in the end.\nPlus, it will keep the environment markers in the requirements.txt when the marker evaluates as true, which should cover some easy cross-environment cases, as long as the pip-compile is done on the python version that includes the additional packages (FYI @barrywhart for your usage).. You're right, that's what happens with late night releases. Thanks for pointing it out\nEdit: Added to changelog and releases notes. That was definitely an important one.. Hi @cyrilleverrier, thanks for taking the time to report.\nWe've got that covered in https://github.com/jazzband/pip-tools/issues/580 and the solution is current merged in master, you can test it out if you want. It'll be released in pip-tools 2.0.0 soon\u2122\ufe0f. I've got a few changes I'd like to get in first, but I don't intend to delay it too much.\n(Keeping open for now, as I expect others to hit this.)\n. I'm getting on it right away. Thanks for the notification. Done:\nhttps://pypi.org/project/pip-tools/2.0.0/. FYI 2.0.1 coming up shortly: There was missing package_data in there, which will make pip-compile fail with an error like:\nIOError: Could not find a suitable TLS CA certificate bundle, invalid path: <path_to_virtualenv>\\lib\\site-packages\\piptools\\_vendored\\pip\\_vendor\\certifi\\cacert.pem. pip-tools 2.0.1 is out, everything should be working even with pip 10.0.0.. Hi @slykar, that's true, -P only specifies which packages should be upgraded even if valid existing pins are found in the requirements.txt. pytest==3.0.5 would actually be an invalid input right now, but the way pip-compile parses it, it accepts it and only keeps the name of the package. If you want to upgrade to a specific version, you can pin it in the requirements.in.\nIf you'd like to see -P/--upgrade-package take the version into account, I'm open to see a PR for that!. Uh! That example is not going to work indeed, that slipped through, sorry about that. I'm pretty sure pip-compile never had this behavior either.\nSo we should either fix the README, or make this work, because that actually seem like a neat feature.. @tuukkamustonen @barrywhart @cancan101 @taion @JoergRittinger\nFYI as you've been the ones discussing mostly discussing the cross-environment topic.\nDo tell me if something isn't clear or if you have any questions!. @taion Good point about source control. That depends on the user's case I guess. If they deploy an app/service through CI/CD on a single defined Python environment (ex: Only Py3.6 on linux), then committing requirements.txt to source control makes sense. If they deploy on multiple environments from a single repo, then the requirements.txt shouldn't be commited to source control.\nI'll figure a way to point that out w/o having to write a dissertation on CI/CD.. @taion Argh, yeah I see. If you don't mind me asking: what are you currently doing to deal with that?. (Pushed but I still need to take care of the source control recommendations). @davidovich @tuukkamustonen @taion Updated, with a distinct section regarding commiting.. Basing myself on this pip PR: https://github.com/pypa/pip/pull/5215\nWe have to make a similar change to our vendored pip code too.\nChanging this line seems to be sufficient: https://github.com/jazzband/pip-tools/blob/5c26e9fde8b560085cc89d0a2dd7c6709f1b68cf/piptools/_vendored/pip/models/index.py#L16\nbut we should make the change in the doc and pip._vendor libs too. Also, put a note in piptools/_vendored/README.rst.. Hi @jdufresne,\nLooking at things, we're going to unvendor pip in PR #657 , which will have pip-tools use the default PyPI URL matching what pip gives us.\nI'll keep this PR open until the pip unvendoring is confirmed and merged.\nAs always, thanks for contributing, it's much appreciated!. I cannot reproduce with pip-tools v2.0.1.\npip-tools v2+ has vendored pip 9.0.3 internally to keep compatibility with pip 10.0.0 when installed.\nDid you update pip-tools to v2.0.1?. Hi @cjerdonek, thanks for the report\nWith what you linked, we might only have to set pip's logger level set to INFO to get that missing information. We could try to add it by default and if it's too verbose, only set it for --verbose.. Hi @darrenleeweber,\nIf you do pip list, that should give you pip-tools' version. I'm ready to bet it's <2.0.0, and you just have to update to 2.0.1 for pip 10 support \ud83d\ude03 .. I'll go ahead and close the issue, comment back if upgrading to latest pip-tools (currently 2.0.2) did not solve the issue.. Thanks for putting this together @techalchemy !\nAs @suutari said, if we're un-vendoring, we'll want to put back the pip \"dimension\" in the test matrix.\nHere's the commit that remove it: https://github.com/jazzband/pip-tools/commit/61163e043fbedb6c3fefb406399321e7b2221ec6\nIdeally, we'd keep pip 8 to 10 compatibility, but if this gets messy, we can look to drop pip 8. I'll try to find some usage stats on pip 8 first though.\nEdit:\nHere's some pip download stats, over the last 30 days:\n| version  | download_count |\n| -------- | -------------- |\n| 9.0.3    |      8,023,066 |\n| 6.1.1    |      4,364,196 |\n| 9.0.1    |      3,030,700 |\n| 10.0.1   |      2,826,028 |\n| 10.0.0   |      2,594,810 |\n| 10.0.0b2 |      1,247,348 |\n| 10.0.0b1 |        337,727 |\n| 7.1.2    |        325,324 |\n| 8.1.1    |        163,670 |\n| 8.1.2    |         49,028 |\nThe data doesn't take into account people with pip 8 installed and not downloading it, but it gives an overview.. @techalchemy\nOk first off, once again, thanks a million time for your work.\nSecond, sorry for the delays in reply, its hard to get the time to give a whole proper check/reply at this.\nBy unvendoring, we fully match the pip the end-user chooses to use, default PyPI URL and everything else. I agree with @suutari and you on this.\nBut by unvendoring, we also put ourselves (and all pip-tools users) back at risk of breakage on any pip update. The stuff we use was explicitly refactored in a package named _internal for crying out loud.\nI hate everything about this situation.\nThe least-horrible thing I can think of is to go ahead and unvendor pip, and if we get to regret this, just vendor it back.\nThoughts?. Thanks for everyone input.\nLets go with un-vendoring then, I think it makes sense.\n@tysonclugg's suggestion to put pip in the install_requires worries me for Windows user though. Doing pip install pip-tools will cause issue as pip will to uninstall/modify itself if it doesn't match, leaving ugly errors like:\nException:\nTraceback (most recent call last):\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\commands\\install.py\", line 342, in run\n    'Successfully downloaded %s', downloaded\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\req\\req_set.py\", line 795, in install\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\req\\req_install.py\", line 767, in commit_uninstall\n    response = ask_path_exists(\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\req\\req_uninstall.py\", line 142, in commit\n    rmtree(self.save_dir)\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 49, in wrapped_f\n    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 212, in call\n    raise attempt.get()\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 247, in get\n    six.reraise(self.value[0], self.value[1], self.value[2])\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\_vendor\\retrying.py\", line 200, in call\n    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\utils\\__init__.py\", line 102, in rmtree\n    onerror=rmtree_errorhandler)\n  File \"c:\\python27\\Lib\\shutil.py\", line 247, in rmtree\n    rmtree(fullname, ignore_errors, onerror)\n  File \"c:\\python27\\Lib\\shutil.py\", line 247, in rmtree\n    rmtree(fullname, ignore_errors, onerror)\n  File \"c:\\python27\\Lib\\shutil.py\", line 247, in rmtree\n    rmtree(fullname, ignore_errors, onerror)\n  File \"c:\\python27\\Lib\\shutil.py\", line 252, in rmtree\n    onerror(os.remove, fullname, sys.exc_info())\n  File \"f:\\virtualenvs\\tmp-abfbe3982928ec9c\\lib\\site-packages\\pip\\utils\\__init__.py\", line 114, in rmtree_errorhandler\n    func(path)\nWindowsError: [Error 5] Access is denied: 'f:\\\\tmp\\\\pip-o9ongj-uninstall\\\\virtualenvs\\\\tmp-abfbe3982928ec9c\\\\scripts\\\\pip.exe'\nwhich will end up as recurring Github a few minutes later.\nThis would force Windows users to use python -m pip install pip-tools, and that's going to break stuff for a lot of users and automated CI system right now. As a dev working on Windows... Please don't \ud83d\ude04 .\nWe can add a check early in pip-compile/pip-sync to check pip version and act on that. The check will occurs on pip-tools's use rather than install, but I think it's the best we can do right now. IIRC there was a check like that prior to vendoring pip.. Thanks everyone!. @Miserlou In you case, can you check the version of rpy2 used? The latest version rpy2 2.9.3 doesn't support python 2.7:\n$ pip install rpy2\n<snip> \n    Complete output from command python setup.py egg_info:\n    rpy2 is no longer supporting Python < 3. Consider using an older rpy2 release when using an older Python release.\n@kurtwheeler There's a bug in pip-tools swallowing most of the setuptools failure output reported by pip, which makes debugging a bit harder.\nCan you give a go to pip install rpy2 and just to make sure that it actually works? I don't have R installed on my machine, which also cause this error.\nIf that pip install works fine, what might help us for now will be to hack pip-tools a bit: check https://github.com/jazzband/pip-tools/issues/654#issuecomment-382822832 , changing if (logger.getEffectiveLevel ... to if(True) to get more log details will help figuring out what's wrong in building the package.. @tysonclugg Thanks for that, that gives something good to work on.\nI don't have the time right away to investigate, but I can tell you that pip-sync will use the installed (virtualenv) pip for the pip install/pip uninstall commands rather than the vendored pip.\nSo yes, the pip from the virtualenv can have an impact with pip-sync.. Hi @jfly, thanks for taking the time to report here too! \nI'm pretty sure you've put the finger right on it. We'll have to improve that lowercasing logic a bit.. Hi @vlovich and @MichaelAquilina , thanks for the report.\nI was able to reproduce. on pip-tools 2.0.2 . I expect this to be fixed in our next 3.0.0 release, as we've undone the pip vendoring mechanic that likely caused this. I'll try to get the release out soon.\nIn the meantime, you can test by installing from the HEAD of master.\nCheers!. This should be fixed in version 3.1.0. Tell us if it's still happening.\nCheers!. @suutari Thanks for the pointers!\nNow that I think about it, I think I did notice some odd \"via comment\" differences between pip9 and pip10 yesterday, but only once. I'll have to re-test, making sure to clear the cache this time.. Seems like it was addressed indeed, thanks, I should be able to move on with this. . IIRC, pip under python3 on liunx will be named pip3, which is why /usr/local/bin/pip. Easiest solution right now would be to add a symlink. I haven't used python on linux for a long while, so I can't confirm right now.\nI would generally suggest to run pip-compile/pip-sync from inside a virtualenv, which removes some of these differences.. Thanks for the report @suutari, and thanks for the info @rpkilby .\nAs you might have noted, I don't have much time to give to pip-tools right now. If someone wants to submit the PR, I'll do my best to join in on the review, as well as take care of the release in due time.. Hello @kevindawson,\nI gave a try with the example setup.py, plus an install_requires. I do get the same warning, but the pip-compile still seems to work. can you check if the requirements.txt was properly generated?. @kevindawson \n\nwhy do I have to fill out a install_requires section\nshould your app not be doing that for me\n\npip-tools will not change or update your install_requires section, that is not its purpose.\nThe goal of pip-tools is to take a set of requirements (defined in a requirements.in file or in your setup.py file in the install_requires), perform a deep dependency resolution, then output a requirements.txt (the \"standard\" pip requirements file) which will contain a set of fully pinned requirements and versions that match those requirements. That file can then be used for reproducible installation of the python environment (typically, in a virtualenv).\nI suggest that you read the following doc/articles:\n- https://packaging.python.org/discussions/install-requires-vs-requirements/\n- https://caremad.io/posts/2013/07/setup-vs-requirement/\n- https://nvie.com/posts/better-package-management/\nAs for the specific error you're getting now, my_projects was declared as a dependency of you current package, pip-tools is looking for is on pypi, and cannot find it. install_requires should contain the abstract dependencies of your project (i.e. what libs your project needs to work).. I'm glad if that helped you out, have a nice day!. I believe pip-tools still has a legitimate reason to be maintained as a dependency management solution for users using pip. Moving to pipenv is not something that can be done overnight in a large enterprise context with pipelines relying heavily on pip/pip-tools (i.e. my exact context). As long as pip is relevant in the python packaging ecosystem, I think that pip-tools will be relevant too.\nAlso, as @nvie said, pipenv currently relies on pip-tools' resolver. We've had back-and-forth contribution and discussion with pipenv core contributors such as @techalchemy too. Work done in pip-tools still benefits to pipenv right now.\nAs for mentioning pipenv in the readme and describing what's pip-tools role now, yeah that sounds like a good idea. . Hello @jscissr, thanks for the PR, and sorry for the delay.\nUnfortunately, I'm not in favor of this change: keeping \\r\\n endings on Windows is still useful and relevant. A simple example: reading the requirements.txt with notepad, which doesn't handle \\n endings properly.\nAs for your specific use case with git, there's a way to configure it to take care of the line endings conversion for you. You can check https://help.github.com/articles/dealing-with-line-endings/ or https://git-scm.com/book/be/v2/Customizing-Git-Git-Configuration#_formatting_and_whitespace.. Can you give it a try with pip>=10 and pip-tools==3.0.0rc1? In that pre-release version, we unvendored pip, so if all it needed was a fix from pip 10, it should work now.. Allright, good to hear. I aim to release 3.0.0 officially next Monday (2018-09-24).. @ztane pip-tools 3.0.0 is officially out. Cheers!. Thanks @browniebroke !. > Should there be a command-line option to bypass the cache?\nThere's the --rebuild option that would address that here.\n\nWould hashes have helped here?\n\nThe hashes aren't used as part of the cache key, so it wouldn't have helped in this case. But maybe that's exactly something we'd want to change. There's your case here, but also the case where sdist and wheel distributions for a same package end-up giving different dependencies (thanks arbitrary code in setup.py), or if someone reuploads/overwritte a package on pypi, changing the dependencies. That's worth giving this some thought.\n\nSo I've determined what caused the cache to get the wrong value. In my system site-packages, I have an untagged post release of 3.5.0, so when I run pip compile from my main Python environment, I get dependencies resolved as part of that environment (and cached):\n\nYou mean that pip-compile will use the package installed in the current environment as part of the \"source\" it can check? That rings a bell (I feel it was something you reported before @suutari, no?). Can I ask how you installed that untagged post release of 3.5.0 of flake8 exactly (i.e. with pip install -e, or non-editable with a file uri, or something else)?\nThis is worth some investigation, but I think we should test that again with pip-tools 3.0.0rc1 (or 3.0.0, I should get it officially out soon :tm:), which unvendors pip.. Hello @elcolie,\nCould you give the output of pip-compile graph.in -o graph.txt --verbose please?\nThanks!. As @suutari-ai mentionned, pip-tools behaved as expected, taking into account that it doesn't perform backtracking, and the user has to investigate the conflict and take action. Having a better resolver would be awesome, but is project out of reach at the moment.\nThe solution here is to investigate the verbose output, as @suutari-ai did, as spot there's a conflict on prompt-toolkit, where \n- ipython==7.0.1 requires prompt-toolkit<2.1.0,>=2.0.0\n- jupyter-console==5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0\nAnd from there, check if an earlier version of ipython supports prompt-toolkit<2.0.0, or if an earlier version of jupyter-console supports prompt-toolkit>=2.0.0 (usually, you have more chances of previous versions supporting older versions, so checking to downgrade ipython is the most likely to yield a valid solution). \nAnother option after that is to check if contributions can be made to make jupyter-console next version compatible with prompt-toolkit>=2.0.0 to move forward, but I'm going past finding a valid set of constrains here \ud83d\ude04 \nI'll go ahead and close the issue. Cheers!. Hello @vlovich,\nI'm not able to reproduce the error on Windows, and I don't have an OSX machine to test.\nI tested with both pip-tools v2.0.0 and v3.0.0, and if any of the package fail to install, I receive a non-0 return code.\nCan you give a sample requirements.txt?. Thanks @eseifert, that helped, We might be able to fix this today.. Will be released in the next hours unless something terrible happens.. @techalchemy If you'd like to review, please do!. Hello @mashayev \nI investigated the verbose output, I found this:\n- docker-compose==1.17.1 requires requests!=2.11.0,<2.12,>=2.6.1\n- docker==2.7.0 (from docker-compose constraint: docker<3.0,>=2.5.1): requests!=2.18.0,>=2.14.2\nRight here, we can see there's a conflict. pip check can also tell us:\n$ pip install docker-compose==1.17.1\nCollecting docker-compose==1.17.1\n<snip...>\n$ pip check\ndocker 2.7.0 has requirement requests!=2.18.0,>=2.14.2, but you have requests 2.11.1.\nI checked, docker==2.6.1 yields a valid result, but pip-tools doesn't do the backtracking itself. So adding docker==2.6.1 to the requirements.txt will fix this.\nSide note: docker-compose and docker constraints on requests are pretty tight and tend to yield conflicts like this pretty often.\nClosing as the solution was to investigate with --verbose to find a valid version.. Thanks @richafrank for the contribution, it's really appreciated :). Hello @thisfred, thanks for the feedback.\nI see how this can be a pain point for you, but I would be against changing the default behavior here. Having the information that the requirements.txt was generated using some specific index by default is something that makes sense in most cases (and fits in the idea that the generated requirements.txt is a set of concrete dependencies.\nI'm sorry this lead to a leaked token though. Using --no-index seems to be your best option here.\nI'll go ahead and close the issue. If there are strong arguments in favor of changing this, feel free to reply.\nCheers!. If a package has declared a dependency on pillow, there's sadly no way to indicate to use pillow-simd instead.. Hello @cayla, thanks for the detailed report!\nI spotted one odd thing: In each case, only one of  the 2 Django constraint is kept.\nIn one case, it keeps Django>=1.8, and in the other, it keeps Django<1.10.\n...\nAnd as I wrote those line I realised: django-haystack did a small mistake, the requirement should be:\ninstall_requires = [\n    'Django>=1.8,<1.10'\n]\non a single line for the Django package.\nThe only fix for this is for them to go back and release a bugfix version with the requirements fixed.\nOn a side note, I'm not entirely sure why pip-tools is picking a different one in certain conditions. Not sure if that's in pip-tools or pip realm.... Thanks for the report @blueyed \nThat sounds like a good idea, it's a user given parameter, we should keep it in the output.. I think the best point to start would be this: https://github.com/jazzband/pip-tools/blob/446e56d27005dd7d04f0f8c384113ca2f58d88fb/piptools/repositories/pypi.py#L188\nThat's the function called to obtain the dependencies of a package.\nThis relies on the internals of pip, so beware, it can get messy.\nGood luck!. Hello @impredicative,\nI'll be making a release to pypi soon, worst case this evening (EST). Quite a busy day today.\nOur (my) current approach is to try to make releases a soon as possible once something gets in, which alleviates this \"limbo\" situation. Although, if you have an idea to track fixed-but-unreleased issues with Github system, I'd be sincerely happy to hear it (maybe as a separate issue for visibility on the discussion).\nCheers!. Had a small pipeline issue last night (#716), I'll make a 3.3.1 release ASAP. Done: https://pypi.org/project/pip-tools/3.3.1/\nCheers!. I'll be making a 3.3.1 release (with the pipeline fixed) during the day).. Done: https://pypi.org/project/pip-tools/3.3.1/\nThanks @atugushev for the prompt help & fix!. Thanks for the quick fix @atugushev, I'll be making a 3.3.1 release soon (I'd rather do that than deleting/re-creating a tag). pip-tools v3.4.0 released. Hi @eliasbrange, thanks for the report.\nI'm unfamiliar with PEP517 (/todo Read that thing!). I would naively wish for pip to handle the mechanic.\nOne thing to note, is that this specific install seem to work with pip==18.1.\nAlso, there's an issue on the subject in the pendulum project, about issues with pip>=19.0.1:\nhttps://github.com/sdispater/pendulum/issues/335\nAdditionally, there seem to be some issues related to pip 19.0.1 and poetry compatibility (I don't have quite the time to dig into these at the moment):\nhttps://github.com/sdispater/poetry/issues\nSo there's the possibility that this is one of the upstreams issue, or that there's something more to do in pip-tools now to initialise some PEP517 mechanic (hunch from the pip._vendor.pep517.wrappers.BackendUnavailable exception raised, as seen when using --verbose)\nAny help to untangle this would be greatly appreciated!. I went and read https://pip.pypa.io/en/stable/reference/pip/#pep-517-and-518-support to catch up a bit on the subject of build isolation and what it implies.\nIt seems pip enables build isolation since its first inclusion (https://github.com/pypa/pip/commit/c08d4cc806897de687785c723ac7b767c1bc2711)\nSo it seems reasonable to enable it by default in pip-tools too.\nAs for the \"breaking change\" nature of this change, I don't mind as it's meant to adapt to the \"new reality and expectation\" of the python packaging ecosystem.\nThe Grand Question: Do you think that:\nA) It's actually a pip-tools bug we're only catching now, that having isolation disabled most likely caused more pain to users since pip 10.0.0 release, and that enabling it will cause little surprise/impact to users? If so, include in our next feature/bugfix.\nB) Enabling isolation by default may have multiple impacts that will require careful attention for a substantial part of the userbase, or could bring odd behavior in pip-tools itself, and thus should be announced strongly? If so, time for a 4.0.0 (which is ok, it's just numbers! Let's take advantage of Semantic Versionning to raise user awareness on this, as we can).\n@atugushev @eliasbrange Voice your opinion, you likely have more info on the latest feature of pip than I do right now.\nAlso pingning @techalchemy and @pradyunsg for input on the impact/risk, if you have time \ud83d\ude04 .. @atugushev Thanks for that PR, I'm always interested in some cleanup/maintanabillity improvements.\n\n\nUse click's file option with atomic mode instead of ExitStack+AtomicSaver.\n\n\nCool, let's get that in!\n\n\nNew feature: add support for stdout as output file, for example pip-compile -o- [...]\n\n\nI'd suggest to take this one out of the current MR for now. The feature is pretty much unusable with the issue you've raised, so I wouldn't merge/release that yet.\nI'm super happy about seeing work done to get this feature though, so you're more than welcome to open a second MR once this is merged to get the discussion going.\nI'll get on the review proper soon:tm: too.. pip-tools v3.4.0 released. @kaeptmblaubaer1000  I'm preparing a release right now. Taking the CI time into account, in the next few hours.. pip-tools v3.4.0 released. I would pin this to the current latest commit of pytest-django master branch.\nThe pytest-django package could stop using setuptools_scm.\nAlso, if they break the master branch for a while, I'd rather not have our tests fail on that.. Add a check that pytest is also in the output, as it's one of pytest-django's install_requires.\nWe want to make sure that we still collect the subdependencies.. Good catch\nWhile at it, as primary_packages was generated using key_from_req, we should use that here too for consistency.\nkey_from_req(ireq.req) in primary_packages. Well I learned something today, I though the exiting part would always be called, even with an exception, without the try. Just tried it out and you're right, will fix this, thanks.. Is there a reason to exclude pypi.python.org here? \nIf its needed to exclude pypi-hosted packages, the we need to exclude the configured pypi url too for users using their local devpi instances and such.. We still have a limitation on constraints: We don't support file link (as checked in is_url_requirement), so we still need to make a check to warn the user, both here and in pip-sync.. This will go in a 1.11.0 section on top.. I would like to see pypy added in appveyor (for windows) too, in order to keep the tests the same.. This was an example to use pip-compile straight with a setup.py file, without a requirements.in, so the example should be kept as before.. @merwok If I'm following your use correctly, you would have something like:\nrequirements.in:\nmyapp\nrequests[security]\nand then test-requirements.in:\npytest\nflake8\nand you would want separate requirements.txt and test-requirements.txt, which are the pip-compile output of each?\nIf so, then you might have duplicate dependencies in each, which would force you to do pip install of each separately, but that breaks the guarantee that the dependencies of each will be met (your test-requirements.txt and requirements.txt requests requirement may differ, as an example. Note that I took requests, even though pytest and flake8 won't require it, but I think you'll get the idea.).. @suutari-ai Thank you for the input. I like where you're going with this. I will have to revisit this idea after my coffee, but it seems to make sense to me right now.\n@simlun Did you have a specific use case for the way you implemented this, or where you simply (and kindly!) taking care of an existing open issue?\nLet's make sure we're all on the same page \ud83d\ude04 . I could not find a case where the result would be None. Di you find one?\nAlso, I'm not sure I'd like to silently skip hashes, except for clear, defined cases, so I would rather see it fail here. Otherwise, that might bring up a false sense of security, which is really not nice. . Calling get_best_match here means that the lookup on PyPi will still be done, and an error will occur if the packages isn't found. One of the main use case I see of --exclude would be to deal with the lack of proper environment marker handling we have right now, but it would't work in this case.\nIs there a way to get the same logic without doing the lookup? Alternatively, catching and ignoring the NoCandidateFound error could be enough for now.. By doing this, the dependencies becomes strictly pinned version to the best current candidate, which will make the resolving fail if there's a need of changing candidate.\nTry the following requirements.in:\ndocker-compose\nrequests\nIt will fail with something like Could not find a version that matches requests==2.11.1,==2.18.4, while it should succeed, if you try it on master.. You're right, I'll change that. Updated, I rephrased a bit. There we go. I might be missing something, but ireq.link will only have a value when using files uri or vcs urls, right?\nIf that's the case, why check for the presence of index urls and discard?. Ok, I'll have to do some more tests on this, as ireq.link seemed to be always None when I was installing packages from PyPi. I probably did something wrong with my tests.\nIf the Pypi url can be present like you said, then the logic here cannot simply exclude pypi.python.org, because it's not the only possible PyPi server. We have pypi.org, as well as any private pypi server defined by the user.. It thought so too, but unfortunately there's another condition unrelated to the pip version.\nWhen parsing the installed packages (in pip-sync), the returned object by pip.get_installed_distributions is a DistributionEggInfo or something alike, which comes from pkg_resources, and those object do not have a .name attribute.\nThe if hasattr(req, 'key') condition masked that case, making us believe it was only a pip difference.. See comment above: same applies.. Thanks for tackling this one!\nUnfortunately, that implementation breaks the current behavior of keeping the markers in the requirements.txt which allows to make simple cases compatible cross-environment. See https://github.com/jazzband/pip-tools/issues/635#issuecomment-374291159 and the replies afterward.\nWhat could be done would be to make the filtering inside the Resolver, maybe in check_constraints or somewhere that applies only to the top-level constraints, so that it only excludes them from the dependency resolving part, which is were the issue currently comes from.\nWhile I was originally discussing the removal of the markers in the requirements.txt in the comment I linked, I'm now inclined to keep them, because:\n- It keeps the simple cases of (ex: functools32 and enum34) working cross-environment\n- It keeps a form of information in the requirements.txt that will indicate that there was originally a marker, and that will help with offering support if someone puts a maker on a package with dependencies and can't figure out why some sub-dependencies aren't included in the requirements.txt.. Agreed, I'll move that down. The error is caused by adding a trailling / here, in conjuction with this: https://github.com/jazzband/pip-tools/blob/fe8c30e14201abf46f5639a79b65f56398b6ff9f/piptools/writer.py#L58\nThe result is that the index-url is added to the output as if it wasn't the default one. Removing the trailling / here will restore the behavior.. I guess the configured index on the BMs is still the old one, or it's in the vendored pip code.. > While it's technically true that there may be issues, it's very, very uncommon for these issues to actually arise.\nMy experience contributing and offering support with pip-tools and pipenv, which reached a wider part of Python users, was quite the opposite. OS mismatch issues did happen often enough to be a pain, and then there were the Python version based mismatch which are surprisingly common. If you feel like it, take a dive in pip-tools and pipenv past issues: its and adventure \ud83d\ude01 .\nI expect that the number of libs that will only support py3 will increase over time as we're getting closer to 2020, which I also expect to increase the number of cases of difference in requirements between Python versions as the Py2-Py3 transition continues.\nI'm not confortable switching this as a \"what-if\" warning. I think that the statement a few lines below stated the possibility of reusing the same requirement.txt across environments, with the pitfalls. Though, I can add information about what's expected to be more or less risky  (Py2/Py3 vs OS X/Linux vs package tightly bound to the OS, etc.) to avoid being too alarming. . sorted_candidates is unused, that's what makes flake8 fail.. That statement isn't true in all cases: if the user did provide --pre, then the pre-release versions where actually \"tried\".\nIncompatible requirements can still occur with --pre (ex: mypackage<1.2.3a1,>=1.2.3a2), so we still want to give the proper message.\nFrom the top of my head, I'd say we could pass the PackageFinder instance (self.finder) through NoCandidateFound.__init__ instead of index_urls, and use the info from the finder instance to make the proper message (getting the index URLs and check allow_all_prereleases).. The default index URL changes with the pip version, so I would make this match the one provided by pip itself if we stop vendoring pip.. We have a duplicate PIP=master here. Could we have a comment pointing which section is pre-pip 10 and post-pip 10 please, that will help future contributors (and the future \"us\"!). I think this is a good addition, thank you.\nIf it's possible to do the same on Appveyor, that would be great. Can you add PIP=8.1.1, 9.0.1 and 9.0.3 to the py3.7 matrix please.\nLet's keep appveyor/travis matrix as close as possible.. This would be a regression of pip-tools support of process_dependency_links with older pip version.\nI'd rather have this argument conditionally on pip<19.\nSomething like:\n```\nfinder_kwargs = {\n    \"find_links\": pip_options.find_links,\n    \"index_urls\": index_urls,\n    \"trusted_hosts\": pip_options.trusted_hosts,\n    \"allow_all_prereleases\": pip_options.pre,\n    \"session\": self.session,\n}\nif pkg_resources.parse_version(pip.version) < pkg_resources.parse_version('19.0'):\n    finder_kwargs[\"process_dependency_links\"] = pip_options.process_dependency_links\nself.finder = PackageFinder(**finder_kwargs )\n```\nOr something cleaner using _compat if you think of something.. Good idea, I added an explicit mention to Semantic Versioning. ",
    "mahmoud": "Agreed! The older version could have done a better job, and I think the current design does a better job than I had originally hoped. I replied over on the other thread; I was unable to reproduce that issue, so I'm hoping it's just a matter of environment or permissions getting copied over from an existing file.\n. The umask behavior has changed quite a bit since the early version pip-tools first included, so make sure you have the new version of pip-tools with the new version of boltons.fileutils.\nI worked with Brian Warner (of buildbot fame) to get an intuitive and secure behavior for the new atomic file save. The only way I can see a file created with those permissions are:\n1. The atomic_save call explicitly passed those permissions in the file_perms argument.\n2. There was an existing requirements.txt file with those permissions. boltons' AtomicSaver will copy the permissions of an existing file, which is usually the desired behavior.\nIn all other cases, the umask is there by default (as @nvie mentioned), and should be respected. The docs talk about this a bit, but this method is pretty short and readable, describing the whole semantics in ~25 lines.\nHope this helps! If not, totally feel free to file an issue on boltons itself. :)\n. Lemme know if there's anything I can do to help! I've only gotten compliments on the new AtomicFileSaver Windows support. Proud to say I'm pretty sure it's the most complete pure-Python solution in the FOSS world! :)\n. Hey! I'm back. I'll pull up Windows and see if I can reproduce this. What version of Windows are we on? (I've been testing on 7.)\n. Just tried out 1.6.3 on my Windows 7 machine. The behavior is as follows:\nWhen the file does not already exist, everything is created fine. When the file does exist, I see it replaced fine as well. This could be confusion related to the backup file part of the Windows ReplaceFile API. I was under the impression that it would remove it if the swap was successful, but it seems to be a higher-level, non-temporary backup, similar to emacs's xyz.txt~-style backups.\nSo, new version of boltons tonight, with the .bak file removed. I'll leave a comment here as soon as it's ready.\n. All ready to go! .bak files should be a thing of the past. :)\n. ",
    "ryansydnor": "closing as this is a duplicate of https://github.com/nvie/pip-tools/issues/198\n. ",
    "frgtn": ":+1: I think it's a good idea.\n. ",
    "mattrobenolt": "fwiw, I just commented out the check, and it appeared to work...\nI was just about to submit a patch for:\n``` diff\ndiff --git a/piptools/resolver.py b/piptools/resolver.py\nindex 8892175..811edb2 100644\n--- a/piptools/resolver.py\n+++ b/piptools/resolver.py\n@@ -98,10 +98,6 @@ class Resolver(object):\n                 msg = ('pip-compile does not support URLs as packages, unless they are editable. '\n                        'Perhaps add -e option?')\n                 raise UnsupportedConstraint(msg, constraint)\n-            elif constraint.extras:\n-                msg = ('pip-compile does not yet support packages with extras. '\n-                       'Support for this is in the works, though.')\n-                raise UnsupportedConstraint(msg, constraint)\n def _group_constraints(self, constraints):\n     \"\"\"\n\n```\nthen saw this. Not entirely sure why it worked for me without your patch.\n. Ah, I tested against a very simple case which worked fine. :)\n. ",
    "habnabit": "Found a bug in this PR, which I haven't written tests for but at least resolved: habnabit/pip-tools@973e62391656e20a26c724f9eca67f52caf26a43\n. ",
    "shimon": "@jaccarmac have you tried simply installing pip-tools within the virtualenv? This works for me.\n. FWIW, we've worked around this gap by having a separate dev-linux-requirements.in file, and handling different bundles of requirements with make. It would still be nice to have this case handled by pip-tools, though (simply passing through to the generated requirements.txt file).\n. ",
    "jaccarmac": "That works.\nHowever, the documentation seems to indicate that pip-sync respects boundaries and that both pip-compile and pip-sync are global-type commands as opposed to the kind you install in every virtualenv. The picture near the top of the README is misleading as well.\n. ",
    "houjunchen": "I got this problem too.\nI thought pip-tools will be used in every project so I installed it globally. And then pip-sync remove all my global packages and install project packages in both global and virtual environments. Maybe this should be highlighted in README.\n. ",
    "cdwilson": "I ran into the same issue today.  Why are relative paths are expanded?\n. ",
    "breerly": "Looks like https://github.com/nvie/pip-tools/issues/325 is related\n. Nevermind, looks like pip-tools should be installed in the virtualenv\n. ",
    "jredwards": "I've been planning to move all of the requirements handling across all of our repos to using pip-tools and just realized it had this issue, which may completely shelve that plan. This is a pretty significant problem.. Found this thread searching for the same thing. Might be worth mentioning this in the readme.. @cmc333333 I'm also seeing this error in 1.8.0 but not in 1.7.0, with what seems like a perfectly reasonable list of packages.\nusing pip==9.0.1. @davidovich \nMy use case appears to be broken entirely in the master branch (i.e. it does not work even if the output file does not exist). \nI ran the same command on the same .in file. \nHere is the version, according to pip-compile --version:\npip-compile, version 1.8.3.dev23-ng62d85ea\nHere is my output:\npython\n\u21d2  pip-compile -v requirements.in -o requirements.txt\nUsing indexes:\n  https://pypi.python.org/simple\nTraceback (most recent call last):\n  File \"/Users/<username>/.virtualenvs/<virtualenv>/bin/pip-compile\", line 9, in <module>\n    load_entry_point('pip-tools==1.8.3.dev23-ng62d85ea', 'console_scripts', 'pip-compile')()\n  File \"/Users/<username>/.virtualenvs/<virtualenv>/lib/python2.7/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/<username>/.virtualenvs/<virtualenv>/lib/python2.7/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/Users/<username>/.virtualenvs/<virtualenv>/lib/python2.7/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/<username>/.virtualenvs/<virtualenv>/lib/python2.7/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/<username>/workspace/Local/cloned/pip-tools/piptools/scripts/compile.py\", line 183, in cli\n    primary_packages = {key_from_req(ireq.req) for ireq in constraints if not ireq.constraint}\n  File \"/Users/<username>/workspace/Local/cloned/pip-tools/piptools/scripts/compile.py\", line 183, in <setcomp>\n    primary_packages = {key_from_req(ireq.req) for ireq in constraints if not ireq.constraint}\n  File \"/Users/<username>/workspace/Local/cloned/pip-tools/piptools/utils.py\", line 44, in key_from_req\n    key = req.name\nAttributeError: 'NoneType' object has no attribute 'name'\n. ...revising this comment. It looks like this was broken in 1.8.0 the same way it is broken in 1.8.2.\nIt was broken in a slightly different way in 1.8.1.\nIt worked in 1.7.0. ",
    "orf": "So the core of this comes down to this function here, inside pip. I don't think it would be easy to fix in pip itself, so I will see if I can manipulate the result while outputting it to the file by making the result relative.. Fixed up the tests, I used the github editor but didn't indent properly :-1: \n. Hi, is there any problem or comment on this MR? Some hashes are better than no hashes, and if you need to add a requirement from a git repo it's a shame to loose all other hashes for non-git requirements.. Didn't mean to close. I've added tests, and it's working well.. It's also common to have a requirements file in a directory like project/requirements/{debug,prod}.in, containing only the requirements. In this case the editable requirements are not in a sub-directory of their referencing requirements and so the current implementation wouldn't work.\nI guess to cover this case we could just say 'it resolves to a relative path if it is in a sub-directory of the requirements file' and be done with it (providing we can fix the issue you described). People who need this fix can then move the files as needed to make it work?. ",
    "ztane": "the relative paths should actually use file:foo/bar so that the #egg=spam fragment could be appended. Unfortunately this breaks in pip-compile, as pip-compile tries reading foo/bar/setup.py after setting foo/bar as the current directory!. as for why #egg=spam would be needed, is for pip dependency resolver to find out what abstract requirement is provided by the said file link! Otherwise it is going to attempt to download spam from Warehouse!. @rollcat almost works... except that we merged 2 repos so it is a slug and hardly speaks.. Waiting for this...!!!. I don't know why I hadn't seen pip-tools before; now that I've seen it today, I am going to use it instead of pipenv. I don't like how the pipenv handles for example virtual environments at all - the defaults are off and I am not interested in configuring tools to be sane with environment variables. That said, naturally pip-tools should grow towards the pipfile + lock approach.it just doesn't work for me\n@androiddrew I think Pipfile is saner than that... I too would prefer setup.py - but unfortunately with the current apps we have we are using lots of 3rd party libraries with badly bounded requirements and we cannot patch all of them - especially the older released versions that we depend on.. and I need to provide the distribution name as without it the resolver doesn't seem to find out that the spam abstract dependency is provided by ham/bacon, and it doesn't seem to be possible to give the name of the distribution to a relative path without the file: scheme.. Ooh shiny @vphilippon it seems to work. Awesome!. ",
    "rollcat": "If your parrot is a git submodule, this works well: -e git+path/to/dead/parrot#egg=parrot.\n. ",
    "ssanderson": "I took another stab at trying to fix this in https://github.com/jazzband/pip-tools/pull/702.. ",
    "tillahoffmann": "I've been using the following post-processing step to get around the problem: make_paths_relative.py < requirements.txt (script below)\n```python\n!/usr/bin/env python\nimport os\nimport re\nimport sys\npattern = re.compile(r\"\\s-e\\s+file://(?P.?)\\n\")\ninputs = sys.stdin.readlines()\nfor line in inputs:\n    match = pattern.match(line)\n    if match:\n        path = match.group(\"path\")\n        path = os.path.relpath(path)\n        line = f\"-e {path}\\n\"\n    sys.stdout.write(line)\n```. ",
    "spookylukey": "I'm seeing this too. If it can find it on PyPI, then it will install from there instead of from the VCS repo, and then exit successfully. This is a serious bug, because you think everything is installed as specified, but in fact a different version gets installed.\n. This is not really fixed.\nIf you have requirements from a VCS URL, and do pip-sync, then it will uninstall and reinstall them every time you do pip-sync. e.g. I see something like:\nUninstalling ometria-0.1.5:\n  Successfully uninstalled ometria-0.1.5\nCollecting ometria from git+https://github.com/spookylukey/ometria.git@a636fa529ac98d7420494f200b299df728135451#egg=ometria\n  Cloning https://github.com/spookylukey/ometria.git (to a636fa529ac98d7420494f200b299df728135451) to /tmp/pip-build-QRVTtO/ometria\n  Could not find a tag or branch 'a636fa529ac98d7420494f200b299df728135451', assuming commit.\nThis means a lot of unnecessary network traffic.\n. @benzkji I don't see that behaviour with most recent pip (7.1.2). I see instead this:\nObtaining ometria from git+https://github.com/spookylukey/ometria.git@a636fa529ac98d7420494f200b299df728135451#egg=ometria (from -r requirements.txt (line 46))\n  Updating /home/luke/.virtualenvs/wolfandbadger/src/ometria clone (to a636fa529ac98d7420494f200b299df728135451)\n  Could not find a tag or branch 'a636fa529ac98d7420494f200b299df728135451', assuming commit.\nIn other words, pip just updates the repo and runs the setup.py again, while pip-tools uninstalls completely and then clones the repo etc.\n. This appears to be fixed for me, perhaps due to a setuptools/pip change or something? At any rate, after the first pip-sync run, subsequent pip-sync runs to completion successfully without any internet connection.. @vphilippon \nIt looks like I'm using:\nsetuptools 39.0.1\npip 9.0.3\npip-tools 1.11.0\nI have git+git and git+https requirements lines, and these work fine. I've found that Mercurial repos (hg+https scheme) do not have the same behaviour - they crash if I run pip-sync without internet connection.. ",
    "benzkji": "same here. If I cannot use some of my open source packages under development, I must go back to pip install -r. In fact, how can it fail? is it not doing exactly the same as pip install -r when installing a single package?\n. so for now, I'm going with pip-compile, and then pip install -r.\n. @spookylukey I see the same behaviour when doing a pip install -r requirements.txt, so this might be an issue with pip itself?\n. @spookylukey true. should have had a closer look.\n. ",
    "benje": "also came across this issue, thanks for the tip @benzkji \n. :+1: thank you\n. ",
    "AlexRiina": "There may be other cases where this doesn't work, but the following passed the tests and worked for me with an editable dependency:\n```\n--- a/piptools/sync.py\n+++ b/piptools/sync.py\n@@ -108,7 +108,7 @@ def diff(compiled_requirements, installed_dists):\n for key, requirement in requirements_lut.items():\n     if key not in satisfied:\n\n\nto_install.add(str(requirement.req))\nto_install.add(str(requirement.link or requirement.req))\n```\n. \n",
    "nirvana-msu": "I have exactly the same problem. It seems impossible to specify a VCS package pointing to a particular tag/commit without it being reinstalled each time by pip-sync.\nI traced the problem down to the way it compares installed packages vs requirements. In the case of installed packages, the key is basically installed_dists = pip.get_installed_distributions(skip=[]); for dist in installed_dists:  key = key_from_req(dist), which just returns a package name. Whereas for the requirements, the key is {r.link or key_from_req(r.req): r for r in compiled_requirements}, i.e. instead of using key_from_req it uses r.link which is the full VCS URL.  Obviously full VCS URL would never match a package name either way you try to specify it.\nIs there any workaround for this?. I may be missing something obvious, but it's still not working for me.  I have this following line in compiled requirements.txt:\nhashpumpy==1.2; sys_platform==\"linux\"\nThis line comes from the same line in requirements.in (environment marker is simply copied).\nIf I try to run pip install -r requirements.txt on Windows, it correctly ignores the package. However if I try to run pip-sync requirements.txt, it tries to install the package (which fails because these binaries are not meant to be compiled under windows - only on linux).. hmm, it didn't help. To give you some background, I used to work with python 2.7 in a conda environment (and pip-tools correctly resolved functools32 dependency). Now I'm trying to upgrade to 3.5.3 and face this issue.\nI tried with --rebuild (and deleting the .txt file) but it did not change anything.  functools32 still got listed.\nAt that point I still had functools32 package installed in my conda environment because I just upgraded python from 2.7 to 3.5.3 with conda (so the package just persisted from the previous state of my conda environment).\nI then uninstalled it manually via pip (pip uninstall functools32) and tried with --rebuild again. This time it failed, apparently while trying to install functools32 (error listed below). Now I seem stuck as I always get this error no matter what I try to do...\n(<envname>) C:\\<path>>pip-compile --rebuild requirements.in\nTraceback (most recent call last):\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\runpy\n.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\runpy\n.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\<username>\\AppData\\Local\\Continuum\\Anaconda2\\envs\\<envname>\\Scripts\\p\nip-compile.exe\\__main__.py\", line 9, in <module>\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\click\\core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\click\\core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\click\\core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\click\\core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\piptools\\scripts\\compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\piptools\\resolver.py\", line 107, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\piptools\\resolver.py\", line 195, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\piptools\\resolver.py\", line 274, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\piptools\\repositories\\pypi.py\", line 145, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\pip\\req\\req_set.py\", line 634, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\pip\\req\\req_set.py\", line 129, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\pip\\req\\req_install.py\", line 439, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"c:\\users\\<username>\\appdata\\local\\continuum\\anaconda2\\envs\\<envname>\\lib\\site-\npackages\\pip\\utils\\__init__.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with\n error code 1 in C:\\Users\\<username>\\AppData\\Local\\Temp\\tmpbcec3wc5build\\functools\n32\\\n. Hm, I got this resolved now, though I'm not exactly certain what the root cause was. I think it had something to do with pip/wheels caching incompatible version on matplotlib distribution.\nHere's what I did. I tried debugging the following script:\n```python\nfrom pip.req import InstallRequirement\nfrom piptools.scripts.compile import get_pip_command\nfrom piptools.repositories.pypi import PyPIRepository\npip_command = get_pip_command()\npip_options, _ = pip_command.parse_args([])\nsession = pip_command._build_session(pip_options)\nrepository = PyPIRepository(pip_options, session)\nireq = InstallRequirement.from_line('matplotlib==2.0.2')\n```\nI then placed a breakpoint on this line, and changed self.ignore_installed to True, and continued. From that point onwards everything worked fine.. I am not exactly sure which cache I ended up clearing with this.. maybe wheels?\n. ",
    "Herst": "@nirvana-msu Following your explanation it should never work if I understand it correctly but in my case I have four VCS entries in my requirements file and it works for three of them.. ",
    "ssbarnea": "This missing feature makes the entire pip-tools kinda useless for me. pynotify is clearly a good example of library that is needed on specific platforms.\n. ",
    "jbg": "FWIW, I\u2019ve just observed that pip-sync ignores the environment markers in requirements.txt and tries to install packages that are marked for a different environment.\n. We\u2019d like to work around this issue by listing ipython in requirements.in like this:\nipython; sys_platform == 'darwin'\n\u2026 since we only use it in development and we all use Macs, and then having pip-compile carry the environment marker through to requirements.txt for ipython and any packages that are below it and nothing else in the dependency tree.\nI guess that would depend on #206 \n. ",
    "merwok": "I checked today if pip-compile supported environment markers; the desired behaviour for me would be to get pinned versions with the environment markers copied in the output file.  That way, I can still have just one requirement file, but cpython vs pypy could install the appropriate database driver (for CI tests).\nThat said, for now I have different files for local dev, testing, CI tools, and deployment, but I haven\u2019t had a need yet for requirements specific to one of the deployment environments (e.g. debug tools on dev vs monitoring tools on staging and prod), so maybe later I\u2019ll go down the road of splitting requirements more and handling the complexity in my pip-sync commands.. pip-compile does support transforming setup.py to requirements.txt, and setup.py supports environment markers:\npython\ninstall_requires = [\n    \"requests\",\n    \"importlib; python_version == '2.6',\n]\nbut I do not know if pip-compile setup.py will preserve the markers.. I have independent requirements files for runtime (requires), test deps, CI (i.e. tox), deployment deps.  I need a four-line shell script wrapper to compile them all to their respective target. . You are right!  #532 is that other feature.. \u201cThe main thing that's tricky is to figure out how to distill the dependencies from it\u201d\nThe same issue exists with editable requirements (#466).\n\u201cand what name the package ends up with locally when you install it\u201d\nThis could be solved with the same syntax as editable reqs: add #egg=requirement-name. A nice interface for this could be the command-line end marker: pip-sync requirements.txt -- --pip-option-1 --pip-option-2=no. Debian and derivatives have split pkg-resources from setuptools for a long time.  I ran into this issue in my first ten minutes of trying out pip-tools.. Work-around: pip uninstall setuptools && pip install -U setuptools. Having appdirs in the compiled requirement files but not setuptools has been causing issues for me on CircleCI in the past weeks.. > in that scenario, it's very helpful to have one pinned-down requirements.txt file from which you install all dependencies\nAre there downsides to installing all dependencies from two requirement files?  In a previous project using docker, we would install only the runtime dependencies in the app container, and the test dependencies only in the test container that was built FROM the app container.. +1!  I find it\u2019s very easy to have requirements split into runtime / test / deployment / CI files, and (sometimes) cpython vs pypy.  I\u2019ve been using a makefile to generate files but it\u2019s not terribly user-friendly.. Well makefiles are somewhat awful to write and don\u2019t work on all OSes.\nI used to have them then switched to small shell scripts.. FTR I have found https://pypi.org/project/pip-compile-multi/. It seems like these packages should be made proper namespace packages.. This could be an inspiration: https://github.com/peterdemin/pip-compile-multi (hi @peterdemin). As a user, I would be happy if pip-tools had a strict dependency on pip 9.. > As a typical setup, pip is only installed once in the system environment (aside from virtualenvs of course, \nI\u2019m never not using virtualenvs, so I didn\u2019t consider that.  Vendoring is always a defeat to me, but do what you must!. It\u2019s not meant to (the section names are made to match setup.py commands), but some tools look there too.\nA difference is that a tox.ini section like pytest needs to be tool:pytest in setup.cfg.. rst2html is part of docutils; rst2html5 looks like an independent project.\nThe setup.py check command provided by distutils can check the validity of long_description if docutils is installed: https://github.com/caravancoop/configstore/blob/master/tox.ini#L52. Ticket #638 is the same issue with a more generic title.. To make things explicit, you could add here \u00abexcept for setuptools, pip and pip-tools itself.\u00bb. I was hoping for a different behaviour: for each .in file found in the directory (or directories), use {basename}.txt as output file.  The idea is to have different files for runtime requirements (direct dependencies), test deps (additional things for CI, not installed during deployment), extra deployment deps (not installed locally or duing CI).  At the moment I have a wrapper script that iterates over in files to create separate txt files.. @vphilippon Yes that\u2019s right.  I don\u2019t have issues with duplicate dependencies; typically I have Pyramid or Django and various libs in the runtime requirements, test deps that work with Pyramid or Django but don\u2019t depend on them (see https://github.com/nedbat/django_coverage_plugin/issues/44 for example), CI dependencies which are an other disjoint set (tox, coverage and pip-tools).  To run tests I call pip-sync runtime.txt tests.txt (see https://github.com/tox-dev/tox/issues/149#issuecomment-290263985 ), and the only problem I\u2019ve had was with six, which I solved by re-compiling the files so that the latest version gets picked up in both files.  So far I haven\u2019t used test deps that need requests, which could easily cause a version conflict since third-party API clients will probably depend on a specific version range of requests.\nThis has been working very well on many projects in the last year!  Depending on the task, different sets are installed: ci.txt to bootstrap, runtime.txt + tests.txt inside tox testenvs, runtime.txt + deploy.txt for deployment.  I was looking forward to this ticket (and #604 :) to be able to do pip-compile deps/ and remove my wrapper script that loops other source files.\nIf it is agreed that this is about taking many sources and creating a unique frozen requirements file, then I\u2019ll probably open another ticket to ask for an option to make pip-compile accept multiple source files (or a directory) and compile each one to a frozen version with matching filename.  Then the two use cases would be satisfied :). ",
    "barrywhart": "Related need: Support environment markers for indirect dependencies.\nExample: I have an application that uses jsonschema. In Python 2.7, jsonschema requires functools32. In Python 3.3, it does not.\nIn my requirements.in file, I'd like a way to specify an environment marker for functools32 without implying that this is an application dependency.\nProposed syntax -- leading question mark for modifying indirect dependencies:\n? functools32; python_version=='2.7'. Observation: pip-compile uses pip.req.parse_requirements to parse the requirements.in file, which does parse the environment markers, e.g.\n```\n\n/home/vagrant/.pyenv/versions/2.7.10/envs/venv-2.7.10/lib/python2.7/site-packages/piptools/scripts/compile.py(183)cli()\n(Pdb) constraints[6].markers\n\n```\n\nThis seems like good news because it's already doing part of what we want.  I'll see if I can figure out a way to include these markers in the output requirements.txt file.. Good point. BTW, I am creating a PR for the original issue (not including my addition).. The PR is up now. The existing and new tests are passing for me locally, but the Travis build is failing. For some reason, I am unable to view the Travis build log. Would appreciate any help getting unstuck here.. PR is passing Jenkins now.. Replaced PR https://github.com/jazzband/pip-tools/pull/459 with equivalent, squashed PR https://github.com/jazzband/pip-tools/pull/460. This ticket should be updated. It says that \"pip-tools\" does not support environment markers. Now that pip-compile supports them, the ticket only applies to pip-sync. Halfway there!. Are you asking about the pip-compile tool in general, or environment markers specifically? If the first, then the answer is no. The tool takes a requirement.in file as input. You may find it useful to create a one-line requirements.in file, listing the name of your library, then run that through pip-compile.\nIn general, I would say pip-compile, requirements.in, and requirements.txt are intended for use by Python applications, and setup.py is intended for use by Python libraries. This is not a hard rule, but there are some occasional quirks if you use them in a way they were not intended.. Yeah, we've gone through something similar. I created setup.py for our apps, and now wish I hadn't. Could you leave setup.py as it is, then create a requirements.in/requirements.txt with your dependencies, then do pip install -r requirements.txt instead? I think that's the ideal workflow for most applications.. If I have a requirements.in, I would try and avoid putting requirements in setup.py. It's not \"wrong\" or anything, but as you say, it seems backwards and may not be buying you much. AFAIK, requirements in setup.py don't buy you much unless it's a library where applications that use it need to know what to install.. There is now a PR for #206: https://github.com/jazzband/pip-tools/pull/459. I wonder, does pip-compile maintain a persistent cache of library dependencies? Just speculating as to the possible cause of this issue (which I've seen as well). I've noticed pip-compile seems faster on subsequent runs, which supports the idea of there being some sort of cache. And lower-casing of names is a common way to allow case insensitive lookup from a cache or other key-value data structure.\nI've occasionally seen a different issue (possibly related), where libraries appear in requirements.txt without the usual \"via\" comment, even though they do not appear in requirements.in. E.g. vine, which is required by newer versions of celery, but not older ones. This happened when I was experimenting with different versions of celery (3.1.25 vs 4.0.2), so I could see how a cache-related bug could cause this behavior.. I would prefer that case be preserved, and would be willing to spend some time to try and find a solution which does so. Would it make sense to hold off on merging this while we explore potential alternative?. I haven't had time yet to look at preserving case. I agree, this is definitely a useful improvement as-is. Let's go ahead and include it.. Any idea why the build is failing? I can't access the Travis log.. This looks like a good fix for the capitalization behavior. In the original issue (https://github.com/jazzband/pip-tools/issues/431), I had raised some questions about some possible cache-related weirdness. Should I create another GitHub issue for that? Either way (new issue or no), I will need to go back to try and reproduce the issue.. For some reason, I can't view the Travis build log. The tests are passing locally for me. Can someone tell me what the Travis build error was?. Thanks, I'm seeing build results now. Fixed a test that was failing on Python 3.x and fixed a style checker complaint about lines that are too long.. Closing this PR. I had already done a \"git pull\" without \"--rebase\", which made it hard to squash commits. Here is a new PR with the commits squashed: https://github.com/jazzband/pip-tools/pull/460.. I think your question contains a misconception: When pip-compile \"compiles\" a requirements.txt file, the Python version and OS used to execute pip-compile has no impact on the output. The environment markers are simply passed through. Thus, the requirements.txt file will simply contain a line like:\nfoobar==1.2.9; python_version < \"3.0\"\nThe environment marker is not interpreted until package installation time. Thus the requirement.txt file is potentially compatible with any version of Python.\nThis feature has proven very useful to me so far, and several others have requested it, too. The target use case is when there is a library that simply doesn't work on certain interpreters or platforms. Two examples:\n functools32 is a 2.7 backport of Python 3's built-in functools library\n PyPy does not support gevent, thus any packages such as locustio which require it do not work.\nHence, an application might contain the following lines in requirements.in:\nfunctools32 ; python_version=='2.7'\nlocustio==0.7.5 ; platform_python_implementation != 'PyPy'. Interesting, thanks for pointing out how the output differs. I hadn't noticed that.\nIn spite of the behavior you point out, this PR still implements the feature in a useful way. Your example is similar to how I am using this feature in my own work.\nIf I read the output correctly, the only difference between the two outputs is this package for Python 2.7:\nbackports.shutil-get-terminal-size==1.0.0\nTo deal with this, simply add the following line to requirements.in:\nbackports.shutil-get-terminal-size ; python_version < \"3.0\"\nWith this addition, pip-tools creates a requirements.txt file that works on both Python 2 and Python 3.\nI do appreciate the comments and suggestions here, but I don't think they make a case against merging this PR. The comments fall into two categories:\n Let's produce a different requirements.txt for each environment. While this might be easier, it does not address the original request (https://github.com/jazzband/pip-tools/issues/206), which happens to be what I need as well. Some build environments (notably TravisCI) automatically install requirements.txt; they don't expect there to be a different file per environment.\n Let's implement something more complex. These arguments seem to come from an abstract perspective, i.e. problems that could occur, not from real-world problems like those discussed in the original issue.\nPlease, let's review this PR in the context of the original issue from 18 months ago, which several people have weighed in on and agreed is useful. Once this feature is in pip-compile, if it needs additional work to handle new requirements that arise, we can revisit it then. Note this feature has no impact on the behavior of pip-compile unless someone explicitly adds environment markers to requirements.in. So this feature solves real-world problems that have affected several people, and it's harmless if you don't want or need to use it. What's not to like?\n. I'm not familiar with generate-hashes. From your description, it sounds like that option has issues or limitations already.\nI think I would prefer someone document the limitations of generate-hashes in a separate PR. Trying to have one option automatically disable another seems tricky and confusing.\n. Would anybody else like to weigh in on the --generate-hashes proposal above? I'm open to it but have no strong opinion yes or no.. Ok, I have addressed the two code review comments.. I was the author of the other change (#460). And that's true, it was a tactical improvement not intended to automatically address all possible issues. I think that other change may still help with this issue if you can identify and add the \"problem\" requirements to your requirements.in file along with an appropriate environment marker.\nIn essence, you would be working around the lack of a complete solution by manually doing what the tool does not do -- marking the problematic transitive dependencies. I'm curious to know if this helps in your situation. As others have mentioned, the potential true solution is more complex, and I don't believe anyone has worked out a detailed design for how that might work and whether it'd be useful to people, i.e. would the cure end up being worse than the disease. :-). I'm thinking you only do this for the problematic ones. No problems, no work to do. :-)\nIMHO, developers should work on Vagrant or Docker so the environments are similar. That's just me speaking as a developer; I'm a one-time contributor to pip-tools so I can't speak for the project.\nOpen source can be a challenge. My employer doesn't mind me contributing, but they don't encourage me to contribute in areas beyond those useful to my own work.\nWhenever this topic (complicated dependencies) comes up, there are lots of suggestions, but it never quite seems like anybody really has to have it. And with the complexities involved, it could easily be several days or even a few weeks of work to design, code, and test. Realistically, I don't think it's going to happen until it does become a hard requirement for someone.. I have had similar issue with functools, where it was being included as an indirect dependency of an application that needed to support both Python 2.7 and 3.x. In that case at least, there is a straightforward workaround or solution. Include the following line in requirements.in:\nfunctools32 ; python_version=='2.7'\nThe environment marker is passed through to requirements.txt even though the library requiring it doesn't include the marker, e.g.\nfunctools32==3.2.3.post2 ; python_version == \"2.7\"\nBut your case is interesting, because jsonschema is actually specifying the environment marker. Are you saying it generates a requirements.txt file with functools32 but without the environment marker? If so, that sounds like a bug in #460, and perhaps a different issue than the original poster on this issue.\npip-compile is a very useful tool, but it also seems subject to occasional breakage because it's trying to do something that isn't really well supported by the underlying tools. That is, there's no universal, cross-Python version/environment way to get dependency information for a package.\n. FWIW, I can't reproduce the jsonschema issue. I created a Python 3.6.3 virtualenv and ran pip-compile using the following requirements.in:\njsonschema==2.6.0\nAnd it generated the following requirements.txt:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\njsonschema==2.6.0\n```. Perhaps we could keep the environment marker copying, but make it optional (i.e. the default is to not propagate them).\nI see your point about people getting confused using it, but TBH, I wonder how many people who use pip-compile understand that it's intended to target only the environment where it was built. I suspect most people just try it out and only look at the details when something surprises them.\n@vphilippon, can you clarify your question here:\n\n@barrywhart If you have a maintainable idea to keep that feature to un-break some cases as you mentioned, I would be happy to hear it.\n\nAre you referring to something I said on this ticket, or is it a general question?. Idea: Just as pip-compile includes the original command in the requirements.txt file, e.g.:\n```\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --no-index --output-file requirements.txt requirements.in\n\ncachetools==2.0.1         # via google-auth\ncertifi==2018.1.18        # via requests\n...\n```\nIt could also include details on the environment. pip-compile should fail if the user runs it on a different environment (i.e. if the requirements.txt file already exists and the runtime environment does not match the environment listed in requirements.txt).. Idea: Provide a tool similar in spirit to tox which generates requirements.txt files for a specified set of environments (i.e. Python interpreter versions, CPython or PyPy). Having an automated process for creating a file per environment makes cross-environment files seem less important.. I was seeing different behavior in Python 2.7 vs 3.5. Python 2.7 preserved quotes in the original format, while Python 3.5 was converting to double quotes. I assume this is a difference in the underlying pip library which parses the lines (including environment markers) from requirements.in.. I will revisit this. A better solution might be to use double quotes in the environment marker -- I suspect these will remain double quotes in both 2.7 and 3.5. That would eliminate the need for the non-obvious replace() operation.. I have cleaned this up -- the test now uses double quotes, which work consistently on Python 2.7 and 3.5.. Will remove.. Probably copy-paste. Will review and fix or remove.. ",
    "adamchainz": "@barrywhart your feature is quite different to the original issue of outputting the markers. I think you should make a new issue for it and avoid spamming everyone here subscribed to the original issue.. PR: #459 . FWIW pip itself successfully disables the cache and emits a warning if it has problems writing: https://github.com/pypa/pip/blob/7954523ce3174ee1677f1fdc6bcca8efb2b5e95a/pip/download.py#L260\n. From 2.7:\n```\n=================================== FAILURES ===================================\n___ testformat_requirement_environment_marker ____\nfrom_line = >\nwriter = \n    def test_format_requirement_environment_marker(from_line, writer):\n        \"Primary packages should not get annotated.\"\n        ireq = from_line(\"test ; python_version=='2.7' and platform_python_implementation == 'CPython'\")\n        reverse_dependencies = set()\n\n  assert (writer._format_requirement(ireq,\n                                       reverse_dependencies,\n                                       primary_packages=['test'],\n                                       marker=ireq.markers) ==\n            \"test ; python_version=='2.7' and platform_python_implementation == 'CPython'\")\n\nE       assert 'test ; pytho... == \"CPython\"' == \"test ; python... == 'CPython'\"\nE         - test ; python_version == \"2.7\" and platform_python_implementation == \"CPython\"\nE         ?                      -  ^^   ^                                       ^       ^\nE         + test ; python_version=='2.7' and platform_python_implementation == 'CPython'\nE         ?                        ^   ^                                       ^       ^\ntests/test_writer.py:65: AssertionError\n===================== 1 failed, 52 passed in 1.02 seconds ======================\n\n```. 3.5 errored and seemed to produce no logs, if you push a new commit it'll restart the tests and it might succeed. > If I read the output correctly, the only difference between the two outputs is this package for Python 2.7\nActually there's also enum34, pathlib2, and scandir on Python 2. It would have been useful if diff was used in the example.\nHowever I still agree with you @barrywhart , I think this feature can be merged as-is and improved down the line. It's not a huge burden to copy some packages with environment markers into requirements.in to avoid having two files for two systems.\n. I think you're using a python 2 installation of pip-tools, try reinstalling it with Python 3 and recompiling. I don't get why you're modifying the result before asserting on it, why not just assert on the original value of result?. As per your previous comment, I would add a comment to the source here why you're transforming the quotes. You could also make it obvious with a if sys.version_info... for the affected Python version. ",
    "lbolla": "Is this supposed to work if, instead of requirements.in I use setup.py to declare dependencies?\nBecause, it does not seem to work for me using setup.py.. I meant in general. My use case is that my application currently uses requirements.in to declare dependencies: all used to be good, but now we are trying to pull the app to do some integration tests and doing pip install my-app is not enough, because dependencies are not declared in setup.py.\nI guess these kind of use-cases will become less frequent, with Docker and docker-compose kind of workflows.. @merwok  it does not, from my experience.\n@barrywhart the problem with that approach is that setup.py and requirements.in end up declaring the same dependencies, duplicating effort. I guess I would workaround by parsing requirements.in to figure out what dependencies to declare in setup.py, but it feels backwards to me.. ",
    "rpkilby": "@merwok - that sounds like a different feature. This issue was requesting that a directory of *.in files be compiled into a single target requirements file, not one-to-one respective targets for each input.. @davidovich - not sure if this should be closed. I don't use pip-tools in my target environments and just do a pip install -r requirements.txt on a clean virtualenv. Since pip-tools isn't installed, neither would click. . sorry for the noise - I misread the issue. I thought the issue was that pip-compile was not including click in its output because of the self-dependency issue. The issue seems to have instead been w/ pip-sync.\nIn my case, I use pip-tools locally to create the version locked requirements.txt, however I just use regular pip (not pip-sync) to install the requirements to the target environment, which is a fresh virtualenv.. I've only read parts of the conversation and skimmed over the rest (there are several words). Sorry if I'm missing it, but what's the actual use case for this PR? When would you need both an install_requires section in setup.py and a requirements.txt? What's the end user's usage supposed to look like?\nsh\n$ pip install some-package\n$ pip install -r path/to/site-packages/some-package/requirements.txt   # ???. tl;dr - I do not understand what problem this PR solves. In glossing over the thread, I have not seen an actual use case being discussed. @tysonclugg - can you provide an example of why this is beneficial to you?\n\n\nI've decided to leave the answer to this question to people such as yourself.\n\nI'm confused by this stance. Why would you propose changes without giving a reason as to why the changes are beneficial? Providing this reason seems like the best thing that can be done to advance interest in the PR. \n\nIt seems the conversation about this PR devolved into philosophical discussion on the merits of setup.py vs requirements.in, and when either is appropriate.\n\nThe debate isn't just philosophical - it's about the practical use case of this PR. As I stated in my comment, I'm confused as to what this PR is trying to accomplish. Having a setup.py module allows a package to be installable. Dependencies are managed through the install_requires attribute. Why would you distribute a separate requirements.txt that duplicates dependency management? Is it expected that users of your package run both:\nsh\n$ pip install some-package\n$ pip install -r path/to/site-packages/some-package/requirements.txt   # ???\n\nPerhaps you can describe why this PR would be useful for yourself, it would clarify that there is indeed a use case for the proposed changes.\n\nNot sure if you're directing this at me, but I plainly stated that I don't understand what problem this PR solves. How would I be able to describe how this PR is useful to me? @tysonclugg - again, can you explain why this PR is useful to yourself? . If you change the url to jazzband from nvie, it should show the build.. @graingert - From the safety docs:\n\nBy default it uses the open Python vulnerability database Safety DB, but can be upgraded to use pyup.io's Safety API using the --key option.\n\nAnd from safety-db:\n\nThe data is made available by pyup.io and synced with this repository once per month.\n\nwhich reads to me as... the open database is updated monthly from pyup.io, which contains the most current information (not the other way around). \nEither way, I'd be hesitant to add a dependency on a service that is not part of python's core infrastructure, especially one that seems to have a bus factor of 1. :confused:. It would make more sense to me that safety/pyup provides a tool for modifying requirements files generally. \nUsers of pip-tools would have a workflow of:\nwrite requirements.in > modify w/ safety > pip-compile\nThose who don't use pip-tools would then just do:\nwrite requirements.txt > modify w/ safety. ~~Not sure if it's relevant, but the trailing \\ should be unnecessary in install_requires=\"\"\"\\.~~ nvm, I'm a dummy. Forgot that the backslash removes the newline. Out of curiosity, is there a reason why you're using a multiline string here? Generally, the expected usage is a list of strings.\nMy guess is that the code is doing a for-each over the string (as it might expect to loop over a list), and failing once it tries to evaluate '>'.. I'd assume this is related to the changes in #418/#481. > Going to merge this by EOD since, as the contributing doc states, it's just documentation updates but I would love feedback if anyone has some.\nI would wait, since this touches a large number of files. It's erring on the side of more than trivial. \nEither way, I'm generally -1 on these kinds of changes unless there's style enforcement in the build. Without enforcement, the changes are kind of meaningless since nothing prevents future PR's from breaking the style guidelines. For consideration:\n\nThere is a pep257 plugin for flake8.\nI'd gather more feedback from other people, but I'm \ud83d\udc4e  on changing string quotes in code since it  negatively affects git-blame. That said, it would be more acceptable if there was enforcement of quote consistency. Maybe this plugin? . > the value is consistency which is the entire point of PEP standards. Maybe give this page a read. :D\n\nI'm sure @davidovich understands the benefits of style consistency \ud83d\ude09. The implication is that the PR does not have a net positive value on the codebase. ie, even though style consistency is a plus, is it worth mucking up git-blame and creating conflicts for existing PRs?\n@davidovich - would simplifying this to just doc strings/pep 257 make sense? It would be a lot less disruptive. . It looks like you're trying to pip-sync your system python, which I would strongly discourage. It's likely that there are python packages installed that Alpine depends on, and pip-sync would remove those if they were not present in the requirements file. \nIt's generally recommended that users install packages within a virtual environment. When operating inside an active virtualenv, there should be python/pip executables on your path that pip-sync can call. Steps should look like...\n\nInstall python3 on a clean alpine install\nChange to working directory (cd /path/to/my/project)\nCreate a virtual environment in a venv or similiarly named directory (python3 -m venv ./venv)\nActivate the virtual environment (source ./venv/bin/activate)\nInstall pip-tools\nCopy a requirements.txt file\nRun pip-sync or python3 -m piptools sync. It might also be worth providing the pip freeze output for both before and after calling pip-sync.. Just my two cents, but I'm -1 on this in its current form. This issue assumes that the .in file paths directly relate to the output requirements .txt file paths, which isn't always going to be the case. eg, the output requirements.txt may be located in the package's root directory, with the .in files in some subdirectory.\n\nInstead I'd rather see a pip-recompile (or a --recompile flag) that does the following:\n- walks a directory tree to find pip-compiled requirements .txt files\n- parse the requirements files for the original pip-compile statement that's embedded at the top\n- execute those pip-compile statements.\n\nBasically, we don't need to assume what the desired output is, this information is already present in the output requirements files.. The suggestion to integrate pyup.io has been discussed and denied in #486. My recommendation would be to create an independent tool that checks requirements files for outdated dependencies, and then use that as part of your workflow. This tool could be used with or without pip-tools. eg, \nUsers of pip-tools would have a workflow of:\nwrite requirements.in > modify w/ safety > pip-compile\nThose who don't use pip-tools would then just do:\nwrite requirements.txt > modify w/ safety\nOr a similar workflow. My point is that I don't think it's necessary or beneficial to actually integrate pyup.io into pip-tools. Hi @hyperknot. I can't recreate this myself. Tested this with the latest version of pip & pip-tools, and with python 2.7.13 and 3.6.0.\n- Create/activate fresh virtual environment\n- pip install pip-tools\n- echo webob > requirements.in\n- pip-compile requirements.in\n- cat requirements.txt\n#\n    # This file is autogenerated by pip-compile\n    # To update, run:\n    #\n    #    pip-compile --output-file requirements.txt requirements.in\n    #\n    webob==1.7.4\nHave you tried recreating this in a fresh virtual environment?. While not ideal, would ~a better solution~ an acceptable alternative be to vendor the most recent version of pip? This would require a bit of upkeep, but would avoid the cost of maintaining compatibility across three major versions of pip.. @strichter - your case might be related to #625.. For anyone else surprised by the pip version, note that pip 18 is now using a calendar based versioning and release scheme. . Maybe use sorted instead of list, since sets are unordered. idk if the output is consistent or not.. maybe ', '.join(sorted(UNSAFE_PACKAGES))? Predictable ordering and better presentation than a simple list repr.. ",
    "SanketDG": "Haha, this was almost three years back. I apologize for my naivety back then. I have since then realized you can just use the suggested alternative. :). ",
    "ibobak": "This is how I fixed this problem with jupyter kernel http://ihorbobak.com/index.php/2017/12/13/pyspark-kernel-for-jupyter-in-python-2-7-windows/  but I do believe yours can be fixed the same way. ",
    "Chronial": "I think the correct fix for this issue is to do os.environ[str('PIP_EXISTS_ACTION')] = str('i').\n. Here's an example:\nGood requirements.in\nchardet\nBad requirements.in\n--extra-index-url https://pypi.python.org\nchardet\nRun pip-compile twice.\nExpected requirements.txt\n--extra-index-url https://pypi.python.org\nchardet==2.3.0\nActual requirements.txt\n--extra-index-url https://pypi.python.org\n--extra-index-url https://pypi.python.org\nchardet==2.3.0\n. ",
    "hlawrenz": "Excellent. Thanks for pip-tools, it's been very helpful!\n. ",
    "swethaHub": "+1 This would be quite useful. . ",
    "jsm": "Having the same issue\nCan we get this merged in?\n. https://github.com/nvie/pip-tools/pull/230\n. ",
    "jonafato": "This seems like correct behavior and just a coincidence that the versions match. I don't encounter similarly incorrect behavior when installing other packages.\nBased on the source (from the tar gzip on lockfile's PyPI page), lockfile is using pbr to manage dependencies. This feature of pbr reads requirements from requirements.txt files to populate setup's install_requires attribute. lockfile provides a requirements.txt file specifying pbr>=0.6,!=0.7,<1.0 as a requirement. It seems that pbr==0.11.0 satisfies this version constraint and only coincidentally matches the version of lockfile being installed.\n. @rleonhardt I'm just a user, not the author, but happy to help!\n. @nvie Thanks for the feedback. You're right that this pull request added a lot more than it needed to address the core issue I was having. Because the changes are so different, I'm closing this PR in favor of a new one created at #294.\n. @ejames This build seems to have failed because of the error fixed in #295. If you rebase against master, things should be back to normal.\n. @nvie: Rebased against master.\n. This seems like correct behavior. Your requirements.in specifies Django==1.8.2, but django-transaction-barrier==0.3 requires Django>=1.4.0,<1.8.0. No version could satisfy both ==1.8.2 and <1.8.0, so the compilation can't proceed.\n. @graingert so is the suggestion to use hashin.get_package_hashes inside of pip-tools instead of the current implementation? If so, and if the jazzband maintainers are interested, I'm happy to put together a PR with that change.. This appears to be a duplicate of #293.. It seems like this is the result of setuptools using setup.py for dependencies instead of vendoring them as of this commit. My naive guess (without reading through the logic for this exclusion) is that the full dependency graph, including all of the setuptools dependencies, is computed and then the setuptools dependency is removed rather than ignoring it when it's first seen.. I'm unable to reproduce this. Is it still an issue?. It looks like #540 has been merged. Is this issue now solved?. ",
    "rleonhardt": "Oh, I didn't see the old <1.0 requirement, pbr==0.11.0 really is the last version before 1.0!\nSorry for bothering you, thank you for this great tool, keep up the good work!\n. Thanks for the \"fix\", it's working ;-)\npip-compile currently does not work with pip version 8.0+ yet (8.0.0 found),\nMaybe you want to remove the last 2 characters of the format string? ,\nI hope you can adapt to the new pip internals soon.\n. ",
    "quinox": "I agree.\nIn the meantime you can get the same information by running it as:\n pip-compile --verbose | grep ' requires .* django-cms'\n. ",
    "mgedmin": "If I comment out linkchecker, the error goes away.\n. ",
    "kylios": "Any progress on this issue? I'm seeing the same error caused by the package scandir == 1.4. ",
    "last-partizan": "https://github.com/benhoyt/scandir/issues/72\nhere is related issue for scandir, probably this will be fixed with next scandir update. ",
    "mlazowik": "@kylios #433 . Related: #224 #433 #434 . ",
    "GaretJax": "Sorry, PR against wrong repository. :)\n. ",
    "31z4": "@nvie I actually don't know. I've just found it here, on github. I wonder if there are better vulnerability databases containing information about Python packages. I also found https://github.com/evilchili/pypi-cve-scanner\n. @dstufft would be great if you find some time to finish and merge this!\n. ",
    "rm--": "What luck. Thanks for the fast reaction! New version works as expected.\n. ",
    "chaoflow": "sorry for the mess. pip is also not taking umask into account for its .cache/pip files - might be intentionally - and I mistook it for pip-tools.\n. OS (touch) and python (open('foo', 'w')) are doing their job.\npip-tools uses boltons AtomicSaver, which creates a temporary file and then renames it. After that it should apply the correct umask.\n. @nvi Thank you for the update and release. The issue is fixed.\n. This feels confusing to me: pip-tools and pip belong closely together and should resemble each other's command line experience. pip seems not to support being installed globally while working locally, the --user option being an exception. Please correct me if wrong, I only briefly scanned pip install --help. If pip does not support this mode of operation, I think pip-tools should neither.\nInstead pip-tools should always use the pip right next to it, instead of consulting $PATH. If it is installed globally already, it is cheap to install it also locally and by doing so you would have two pip-sync, one always operating globally and one locally.\npip (and easy_install, and python and all other python tools I experienced so far) behave identical, independent of whether the virtualenv is activated or not.\nI do not have the power to reopen here, but feel it should be reopened ;)\n. I think I found a solution:\nactivated virtualenv: pip-sync has to alter the activated virtualenv, independent of where it is installed. $PATH is consulted to find pip (#203).\ndeactivated virtualenv: pip-sync has to alter the virtualenv it is installed to (Current issue).\nUpdated: Currently, if no virtualenv is activated it consults $PATH. That I consider a bug that could be fixed by adjusting behaviour depending on whether we are in an activated virtualenv or not. ($VIRTUAL_ENV could be used to detect an activated virtualenv)\n. @nvie I'll look into it.\nAn even simpler solution could be, if pip-sync is called as pip-sync it will consult PATH, as it was also found via PATH, if it is called with an absolute or relative path ./foo/bar/bin/pip-sync it will look for pip right next to it.\nWhat do you think?\n. That would also mean, that within an activated virtualenv one would alter globally if calling /usr/bin/pip-sync. Is that desirable behaviour or not?\n. Sorry, I'm aiming to remember not to mention an issue until I know the commit works and the commit message is correct.\n. new title: pip-tools fails to install in virtualenv with --system-site-packages and pip==8.0.0\n% virtualenv -p python2.7 --system-site-packages .\n% source bin/activate\n% pip install --upgrade 'pip==8.0.0'\n% pip install --upgrade 'pip-tools==1.4.4'\nCollecting pip-tools==1.4.4\n  Downloading pip_tools-1.4.4-py2.py3-none-any.whl\nCollecting six (from pip-tools==1.4.4)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools==1.4.4)\n  Downloading click-6.2-py2.py3-none-any.whl (70kB)\nCollecting first (from pip-tools==1.4.4)\n  Downloading first-2.0.1-py2.py3-none-any.whl\nInstalling collected packages: six, click, first, pip-tools\n  Found existing installation: six 1.5.2\nDetected a distutils installed project ('six') which we cannot uninstall. The metadata provided by distutils does not contain a list of files which have been installed, so pip does not know which files to uninstall.\n. Without --system-site-packages it installs fine.\n. With pip-8.0.2 and pip-tools-1.6 this error is gone\n. It seems this was an issue with decorator-4.0.8, fixed with decorator-4.0.9. Sorry for the noise.\n. thx a lot!. @jaraco You seem to cat a different folder than the one you are removing: cat ~/Library/Caches/pip-tools/... vs rm -rf ~/Library/Caches/pip-compile/*. ",
    "skolsuper": "+1, I have to run it twice every time since upgrading to 1.3 (I was on 1.1.6 before).\n. ",
    "rfleschenberg": "What is the reasoning behind pip-tools not supporting URLs for non-editable installs? Is it possible to introduce a switch to override this behaviour?\nFor me, It would be great if pip-tools supported non-editable installs from Github repositories. I'd be happy to help implementing this, if you agree.\n. Thanks for the quick reply! I naively thought we could delegate most of this work to pip :) I will see if I can come up with a solution.\n. Sounds like a great idea to me. I never got around to come up with a real solution. This would solve my use-cases and should be reasonably easy to implement.. #405 is related, I think?. Just a small note: in many of these situations, you can just run pip directly on the .txt file generated by pip-compile, because you are operating on a fresh environment anyway.\n. ",
    "miketwo": "I wonder if an easier short-term solution would be to allow it to pass-through certain lines to the final requirements.txt file.  We have some packages that we build and host in-house, and while I would love to get to the point where we have an internal PyPi-like server, right now it's just a zip file at a particular URL. It would be nice to be able to tell pip-compile \"Do your normal thing, but just pass-through these lines\". \nMaybe a --pass-through-urls option?. It looks like there would have to be a corresponding --ignore-urls or similar on the pip-sync side, cause it can't parse them for dependencies (for the same reasons). . ",
    "nim65s": "Here is a PoC for this. Do you think I can continue and make a PR\u00a0with this kind of stuff ?\n```python\nfrom io import BytesIO\nfrom zipfile import ZipFile\nimport mock\nimport setuptools\ntry: # python 3\n    from urllib.request import urlopen\nexcept: # python 2\n    from urllib import urlopen\nurl = 'https://github.com/django/django/archive/master.zip'\nzipfile = ZipFile(BytesIO(urlopen(url).read()))\nfor f in zipfile.namelist():\n    if f == 'setup.py' or f.endswith('/setup.py'):\n        setupfile = f\n        break\nwith mock.patch.object(setuptools, 'setup') as mock_setup:\n    exec(zipfile.open(setupfile).read(), {})\n_, kwargs = mock_setup.call_args\nassert 'pytz' in kwargs['install_requires']\n``. Hum, no. We should usepip download.. Thanks ! :D\n. Hi, \nI just made a quick test to see if with this PR, we could handle links likehttps://github.com/django/django/archive/master.zip#egg=django`:\nbash\n$ echo 'https://github.com/django/django/archive/master.zip#egg=django' > requirements.in\n$ pip install -U 'git+https://github.com/edx/pip-tools.git@remove-git-url-constraint#egg=pip-tools'\n$ pip-compile\nTraceback (most recent call last):\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/bin/pip-compile\", line 11, in <module>\n    load_entry_point('pip-tools==1.7.1.dev0', 'console_scripts', 'pip-compile')()\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 163, in cli\n    results = resolver.resolve()\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/piptools/resolver.py\", line 103, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/piptools/resolver.py\", line 199, in _resolve_one_round\n    constraints = sorted(self.constraints, key=_dep_key)\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/piptools/resolver.py\", line 74, in constraints\n    self.their_constraints)))\n  File \"/home/nim/.virtualenvs/tempenv-765024555e06/lib/python3.6/site-packages/piptools/resolver.py\", line 175, in _group_constraints\n    if first(link_ireq.specifier._specs).version not in ireq.specifier:\nAttributeError: 'NoneType' object has no attribute 'version'. Hi,\nThanks for the review :)\nI tried to address your comments, but after that I noticed that there are other PR working on a similar goal, thanks to the label. What should we do about that ?. If I understand correctly,  #405 should handle urls like https://github.com/django/django/archive/master.zip#egg=django but it doesn't (see https://github.com/jazzband/pip-tools/pull/405#issuecomment-350464378), and #372 supports only vcs urls.\nI checked #372 with django's zip, and it puts django==2.0 in requirements.txt, no matter which branch was in requirements.in.\nTherefore, I think that this PR handles more cases, and correctly, but I should add two tests instead of one: \ntest_url_package & test_url_package_vcs, in order to check the same things as in test_editable_package & test_editable_package_vcs. I see, you're right.\nI don't see any easy solution right now :/\n. ireq.link can also give the link to download a requirement. For example, for six==1.10.0, it can be https://pypi.python.org/packages/c8/0a/b6723e1bc4c516cb687841499455a8505b44607ab535be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d59a81a315dc8. And if I remove or 'pypi.python.org' in ireq.link.url, I get some tests failures  because of that:\nE           AssertionError: assert 'six==1.10.0' in '#\\n# This file is autogenerated by pip-compile\\n# To update, run:\\n#\\n#    pip-compile --output-file requirements.txt...5be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d59a81a315dc8\\nDry-run, so nothing updated.\\n'\nE            +  where '#\\n# This file is autogenerated by pip-compile\\n# To update, run:\\n#\\n#    pip-compile --output-file requirements.txt...5be01091c0f24f079/six-1.10.0-py2.py3-none-any.whl#md5=3ab558cf5d4f7a72611d59a81a315dc8\\nDry-run, so nothing updated.\\n' = <Result okay>.output. Aren't those private pypi servers in index_urls ? This case is  adressed two lines later. ",
    "AndreasBackx": "Any update on this, could this perhaps be merged?. ",
    "ChiKenNeg": "Is there any news with this? seems like pip-compile-multi isn't covering URL as its based upon pip-compile... ",
    "toejough": "I'm feeling this lack, as well.  I was trying to follow a good compile/sync flow, but either they fail if I specify a patch branch for one of my dependencies, or they uninstall my dependency and reinstall the broken pypi version.. ",
    "amitsaha": "@nvie Thanks for taking a look. Yes, you are absolutely right. I am keen to hear what you think? I know I need to fix some existing tests.\n. Well, if I just had one requirement which needed to have a range of dependencies preserved, but others would be just fine to use the latest available, I would still have the use for requirements.in.\nAnyway, thanks for taking a look.\n. The reason we have a range in our requirements and why pinning it to the latest won't work for us is:\nOn every commit to master (which is for every commit), a package is built and is pushed to our own PyPi. Hence, we want the latest package between min_version  and max_version, but do not want to pin it to a certain version min_version.XXYYZZ, because then we will have to compile the requirements.in again else we will be picking up the non-latest package. \n. Sure, no worries. I felt that since we let pinned dependencies as they are, a switch for range pinned would make sense.\n. ",
    "scottbelden": "It turns out that I put this in before I realized that there was in fact a cache. The main reason was for the caching, but I now see that that's probably not necessary.\n. I think I'm having a similar issue with flake8. When I upgraded to setuptools 25.1.0 it works, but here's the output of the run with setuptools 20.0:\n$ pip list\nclick (6.6)\nfirst (2.0.1)\npip (8.1.2)\npip-tools (1.7.0)\nsetuptools (20.0)\nsix (1.10.0)\nwheel (0.29.0)\n```\n$ pip-compile -r -v requirements.in\n                      ROUND 1\n\nCurrent constraints:\n  flake8\nFinding the best candidates:\n  found candidate flake8==3.0.2 (constraint was )\nFinding secondary dependencies:\n  flake8==3.0.2 not in cache, need to check index\n  flake8==3.0.2             requires configparser; python_version < \"3.2\", enum34; python_version < \"3.4\", mccabe<0.6.0,>=0.5.0, pycodestyle<2.1.0,>=2.0.0, pyflakes!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0,>=0.8.1\nNew dependencies found in this round:\n  adding ['configparser', '', '[]']\n  adding ['enum34', '', '[]']\n  adding ['mccabe', '<0.6.0,>=0.5.0', '[]']\n  adding ['pycodestyle', '<2.1.0,>=2.0.0', '[]']\n  adding ['pyflakes', '!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0,>=0.8.1', '[]']\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  configparser\n  enum34\n  flake8\n  mccabe<0.6.0,>=0.5.0\n  pycodestyle<2.1.0,>=2.0.0\n  pyflakes!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0,>=0.8.1\nFinding the best candidates:\n  found candidate configparser==3.5.0 (constraint was )\n  found candidate enum34==1.1.6 (constraint was )\n  found candidate flake8==3.0.2 (constraint was )\n  found candidate mccabe==0.5.1 (constraint was >=0.5.0,<0.6.0)\n  found candidate pycodestyle==2.0.0 (constraint was >=2.0.0,<2.1.0)\n  found candidate pyflakes==1.2.3 (constraint was >=0.8.1,!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0)\nFinding secondary dependencies:\n  mccabe==0.5.1 not in cache, need to check index\n  mccabe==0.5.1             requires -\n  configparser==3.5.0 not in cache, need to check index\n  configparser==3.5.0       requires -\n  enum34==1.1.6 not in cache, need to check index\n  enum34==1.1.6             requires -\n  pycodestyle==2.0.0 not in cache, need to check index\n  pycodestyle==2.0.0        requires -\n  flake8==3.0.2             requires configparser; python_version < \"3.2\", enum34; python_version < \"3.4\", mccabe<0.6.0,>=0.5.0, pycodestyle<2.1.0,>=2.0.0, pyflakes!=1.2.0,!=1.2.1,!=1.2.2,<1.3.0,>=0.8.1\n  pyflakes==1.2.3 not in cache, need to check index\n  pyflakes==1.2.3           requires -\n\nResult of round 2: stable, done\nTraceback (most recent call last):\n  File \"/Users/scott/.virtualenvs/piptools/bin/pip-compile\", line 11, in \n    sys.exit(cli())\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(args, **kwargs)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 195, in cli\n    reverse_dependencies = resolver.reverse_dependencies(results)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/resolver.py\", line 267, in reverse_dependencies\n    return self.dependency_cache.reverse_dependencies(non_editable)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/cache.py\", line 139, in reverse_dependencies\n    return self._reverse_dependencies(ireqs_as_cache_values)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/cache.py\", line 163, in _reverse_dependencies\n    for name, version_and_extras in cache_keys\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/utils.py\", line 191, in lookup_table\n    for value in values:\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/piptools/cache.py\", line 164, in \n    for dep_name in self.cache[name][version_and_extras])\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/pkg_resources/init.py\", line 3047, in parse\n    req, = parse_requirements(s)\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/pkg_resources/init.py\", line 2991, in parse_requirements\n    \"version spec\")\n  File \"/Users/scott/.virtualenvs/piptools/lib/python2.7/site-packages/pkg_resources/init.py\", line 2956, in scan_list\n    raise RequirementParseError(msg, line, \"at\", line[p:])\npkg_resources.RequirementParseError: Expected version spec in configparser; python_version < \"3.2\" at ; python_version < \"3.2\"\n```\n. I didn't realize exist_ok existed. I was just mimicking these ones https://github.com/nvie/pip-tools/blob/master/piptools/repositories/pypi.py#L125-L128\nI'll change it to your suggestion.\n. Ah, yes, that's why I didn't see it, it's a Python 3 option that isn't in Python 2.\n. ",
    "khwilson": "ipython's reliance on appnope is annoying for this toolchain. If it's any help, we get around this with a bit of sed in our tox.ini, which solves the problem \"well enough\" for our purposes:\n```\n[testenv:pip-compile]\ndeps=\n    pip-tools==1.2.0\nwhitelist_externals=\n    sed\n    rm\n; iPython 4 has a darwin only platform dependency on appnope. pip-compile is not platform aware and\n; outputs this unconditionally. We use sed to patch in the dependency until pip-compile is\n; platform aware.\ncommands=\n    pip-compile requirements.in -v\n    sed -i.bak -e's/^appnope[^[:space:]]*/&;sys_platform==\"darwin\"/' requirements.txt\n    rm requirements.txt.bak\n``\n. tbh, I'm really not sure you _should_ be trying to get around it because who knows how deep the rabbit hole goes. But maybe an--ignore-platform-specificflag (or perhaps something with a better name)? Maybe pip-compile would go ahead and resolve the dependencies for the local platform and then comment them out in the resulting requirements file, or would just leave them tagged with;sys_platform=sys.platform? I think either of those options would be better than an outright--ignore`.\nThoughts?\n. The appnope dependency arises from an extras_require[':sys_platform == \"darwin\"] = ['appnope'] type situation. So there are choices you could make, e.g., monkey patching sys.platform or, perhaps better, changing the PEP-0425 tags passed to pip. (I haven't looked at the pip source in a while, but I'm pretty sure there's an option for this.)\nAs for labeling them in requirements.txt, in the worst case scenario, you could do dependency resolution twice, once with the above scheme and once in the regular way and take a set difference. Though there might be a simpler way to do this depending on how pip keeps track internally of its markers.\n. Thanks for taking the time to make suggestions! I really appreciate it.\nThe problem is that if you have a requirements.in change that doesn't affect your requirements.txt, then there is no way to verify that someone regenerated the file without regenerating the file yourself.\nAn example of this occurring happens when, say, you are using boto and you really cared about a feature from 2.8, so your requirements.in says boto>=2.8,<3. Then you realize that Amazon changed the way they handle credentials files, and they incorporated this into boto at 2.29, so you change your requirements.in to boto>=2.29,<3. However, as there is boto==2.38.0 available, nothing will change in your requirements.txt.\nNow I would like to check that if requirements.in changed, the user did, indeed, regenerate their requirements.txt. Most of the time that's easy (just check with git --diff --name-only HEAD^ and make sure that either both files are present or neither file is present), but in the above scenario, your test becomes a bit more complicated. In particular, you have to regenerate the requirements.txt file and diff.\nThere are two major issues with this strategy: First, pip-compile takes a while, so your unit tests job can slow down briefly. Not the worst, but it garners complaints.\nSecond, and more nefariously, you get things like #277 cropping up, where your CI infrastructure is on a different platform (e.g., standard Amazon Ubuntu AMI) than your development boxes (e.g., standard Mac OS X).\nAnyway, that's the whole motivating story. Perhaps there's an easier solution that I'm not seeing?\nAnd local time makes sense, but our problem is that we have devs in a couple time zones, so it becomes a bit of a game either way. Perhaps default to local time but have date take an optional argument for a UTC offset? So you can always stipulate --date 0 if you want UTC?\n. ",
    "achauve": "Yes sorry I created an issue with some context: see #277 \nAppnope is required by ipython only on OSX, so when you develop on OSX and then deploy on linux, using pip-tools generated requirements.txt breaks deployment.\n. @nvie What do you think?\n. ",
    "ryanhiebert": "Oops. Looks like I missed an existing issue about this: #204\n. FWIW, this issue came up for me about the same time that I added a -e requirement to a github repository (starting with git+git://).\n. I'm still seeing this message each time I run my upgrade script.. I may have been mis-remembering what I was seeing. I can't confirm the bug myself ATM.. FWIW, I would expect it to keep the -e ., because the foobar package still needs to be installed. For my own uses, it still needs to be installed because I have the package in a src directory (so that it's not importable without installation), and because it installs a script. It just needs to remain a relative path so that it doesn't break when installed in a different repository, machine, etc.\n. My requirements.in just has a reference to ., and then reads my setup.py, so it is possible even now.\n. I have a line in my requirements.inthat is just -e ..\n. @majuscule : You're correct. I believe there's another related issue to -e with relative paths that's open but I don't remember which one. My current workaround is this script:\n```sh\n!/bin/sh\nEnsure that pip-tools is installed and upgraded\npip install --upgrade pip-tools | grep 'Successfully installed pip-tools'\nRemove current requirements.txt, because pip-compile\ndoesn't seem to work properly with it present.\nrm requirements.txt > /dev/null 2>&1\npip-compile --output-file requirements.txt requirements.in > /dev/null\npip-compile likes to swap out editable relative paths for\nabsolute paths. That's not gonna work on production.\nsed -i '' \"s|-e file://pwd|-e .|\" requirements.txt\npip-compile is adding duplicate lines for --extra-index-url\nsed -i '' '$!N; /^(.*)\\n\\1$/!P; D' requirements.txt\nShow the diff\ngit diff requirements.txt\n```\nIt does a couple things differently because of things I've had to work around, but it works pretty well for me, apart from regularly switching the case of the initial \"D\" in my Django requirement.. #421 / #416 too.. I'm also in favor of this approach. No comment on the patch in particular, but there are definitely valid use-cases for having a setup.py file for an application where you need to pin down the dependencies to ship. In my case I also need to have a requirements.in for other reasons (declaring git dependencies, and alternate package repositories), but it's perfectly fine for a simple app to not need the requirements.in, except to make pip-tools happy, and this would be a nice default behavior.. ",
    "ejames": "Grovelling into the structure of the InstallRequirement object to ask if existing_dependency.req.specs[0][1] in ireq.req feels naughty, but isn't fundamentally any different than the operations performed in utils.as_tuple to get the version.  It seems like a style question - opinions?\nThe description of #166 suggests that there are a lot of edge cases, but I frankly couldn't think of any; if the pinned version in the existing file fits the constraint we're examining, use that version instead of going to the repo.  Is my solution brilliantly simple, or stupidly naive?  I haven't spent enough time in the guts of Python dependency management to have examples of pathological requirements files, so all I can say is that this PR works for the requirements.txt we're actually using at my company.\n. That case is already handled.  I've updated the test case in my PR to match your example:\npython\n        # Add Flask to an existing requirements.in, using --no-upgrade\n        (['flask', 'jinja2', 'werkzeug'],\n         [\n            # Add flask and upgrade werkzeug from incompatible 0.6\n            'flask==0.10.1',\n            'itsdangerous==0.24',\n            'werkzeug==0.10.4',\n            # Other requirements are unchanged from the original requirements.txt\n            'jinja2==2.7.3',\n            'markupsafe==0.23'\n            ],\n         False,  # no prereleases\n         True,  # --no-upgrade flag set\n         [\n            # The requirements.txt from a previous round\n            'jinja2==2.7.3',\n            'markupsafe==0.23',\n            'werkzeug==0.6']\n         )\nThe test passes.  (CI is failing on a different test that's also broken in master.)\nEssentially the algorithm is to act as if requirements.txt is a package repository, and always check the \"listing\" there first before going to the real package repository on PyPi.  The existing pin is used if and only if it satisfies the requirement we were about to resolve.\nIn the case of the requirement werkzeug>=0.7, the existing pin werkzeug==0.6 doesn't satisfy the specifier, so we go to PyPi and get the latest version.\nIn the case of the requirement jinja2>=2.4, the existing pin jinja2==2.7.3 specifies a version which satisfies >=2.4, so we keep jinja2==2.7.3 instead of going to PyPi to pick up a later version of jinja2.\nIf a package is subtracted from the requirements.in - for example, if you delete the requirement for flask after running the above example - then when compile is run again with --no-upgrade, we never try to resolve a requirement for flask, and therefore the existing pin for flask==0.10.1 is never examined, and flask drops out of the resulting requirements.txt.  The werkzeug==0.10.4 pin would remain because the requirements.in would still contain werkzeug.\n. Thanks for help on the PR, @jonafato, @nvie.\nRewritten as #301.\n. It occurs to me that this is also a good way to get started with pip-compile by capturing a good environment state.\nBy running these commands on a known good environment:\n$ mv requirements.txt requirements.in\n$ pip freeze > requirements.txt\n$ pip-compile requirements.in --minimal-upgrade\nYou now have a requirements.in that captures all of the working knowledge from your old requirements.txt, and a requirements.txt that captures a proven list of pinned versions that form a good environment.\n. I might have been in the process of coming down with the flu while rewriting the PR this morning.  Let me take some aspirin and read the diff again.\n. @palfrey - That fixes the issue.\n. MinimalUpgradeRepository is an awkward name, since it describes the \"why\" of the class instead of the \"what\", and the \"minimal\" part is a function of the way other code uses the class, and not of the class itself.  But I'm not sure what a better name would be; I settled on that name because it at least makes it clear that the class exists solely for the purpose of supporting the similarly-named flag.  Feel free to rename it.\nI would be in favor of making the behavior default with an --upgrade flag to get new versions - especially since --upgrade is a flag that works on Pip itself.  In practice I almost always want to avoid upgrading pinned requirements.\n. ",
    "jgoettsch": "I filed a related issue, #288, where it is installing into the global packages directory instead of my virtualenv. My issue only exists with 1.4.1 or newer. If you revert to 1.4.0 or earlier does it resolve your problem?\n. So that requires the virtualenv be activated first:\n$ source /usr/local/envs/foo/bin/activate\nThanks. Please close.\n. ",
    "zackhsi": "Still an issue\n(venv)\u279c  pip  which pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n(venv)\u279c  pip  pip --version\npip 7.1.2 from /Users/zackhsi/Desktop/pip/venv/lib/python2.7/site-packages (python 2.7)\n(venv)\u279c  pip  pip-sync\nCannot uninstall requirement appnope, not installed\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-sync\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 68, in cli\n    pip_flags=pip_flags))\n  File \"/usr/local/lib/python2.7/site-packages/piptools/sync.py\", line 137, in sync\n    check_call(['pip', 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n  File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py\", line 540, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['pip', 'uninstall', '-y', 'appnope', 'aws-shell', 'awscli', 'boto3', 'botocore', 'colorama', 'configobj', 'decorator', 'docutils', 'flake8', 'futures', 'gevent', 'gnureadline', 'greenlet', 'httplib2', 'ipython', 'ipython-genutils', 'isort', 'jmespath', 'lxml', 'mccabe', 'oauth2', 'path.py', 'pep8', 'pexpect', 'pickleshare', 'prompt-toolkit', 'ptyprocess', 'pyasn1', 'pyflakes', 'pygments', 'python-dateutil', 'rsa', 'simplegeneric', 'speedtest-cli', 'tinydb', 'traitlets', 'venmo', 'virtualenv', 'wcwidth']' returned non-zero exit status 1\n(venv)\u279c  pip  which pip-sync\n. ``` sh\n(venv)\u279c  pip  which pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n(venv)\u279c  pip  pip --version\npip 7.1.2 from /Users/zackhsi/Desktop/pip/venv/lib/python2.7/site-packages (python 2.7)\n(venv)\u279c  pip  pip list\npip (7.1.2)\nsetuptools (18.2)\nwheel (0.24.0)\n(venv)\u279c  pip  cat requirements.in\nrequests\n(venv)\u279c  pip  cat requirements.txt\n\nThis file is autogenerated by pip-compile\nMake changes in requirements.in, then run this to update:\n\npip-compile requirements.in\n\nrequests==2.9.1\n. sh\n(venv)\u279c  pip  sh -c \"$(dirname $(which pip-sync))/pip list | grep pip-tools\"\npip-tools (1.4.2)\n(venv)\u279c  pip  pip-sync --dry-run\nWould uninstall:\n  configobj\n  gnureadline\n  ipython-genutils\n  boto3\n  python-dateutil\n  gevent\n  prompt-toolkit\n  jmespath\n  botocore\n  speedtest-cli\n  virtualenv\n  lxml\n  pexpect\n  pep8\n  tinydb\n  ptyprocess\n  path.py\n  docutils\n  traitlets\n  isort\n  venmo\n  flake8\n  appnope\n  rsa\n  aws-shell\n  colorama\n  httplib2\n  ipython\n  pickleshare\n  decorator\n  oauth2\n  pyflakes\n  simplegeneric\n  futures\n  wcwidth\n  mccabe\n  pygments\n  greenlet\n  pyasn1\n  awscli\n```\n. They're globally installed packages\n. Yes, same\nsh\n(venv)\u279c  pip  sh -c \"$(dirname $(which pip-sync))/pip list\"\nappnope (0.1.0)\naws-shell (0.1.0)\nawscli (1.9.15)\nboto3 (1.2.3)\nbotocore (1.3.15)\nclick (6.2)\ncolorama (0.3.3)\nconfigobj (5.0.6)\ndecorator (4.0.6)\ndocutils (0.12)\nfirst (2.0.1)\nflake8 (2.5.1)\nfutures (3.0.3)\ngevent (1.1b5)\ngnureadline (6.3.3)\ngreenlet (0.4.9)\nhttplib2 (0.9.2)\nipython (4.0.1)\nipython-genutils (0.1.0)\nisort (4.2.2)\njmespath (0.9.0)\nlxml (3.5.0)\nmccabe (0.3.1)\noauth2 (1.9.0.post1)\npath.py (8.1.2)\npep8 (1.5.7)\npexpect (4.0.1)\npickleshare (0.5)\npip (7.1.2)\npip-tools (1.4.2)\nprompt-toolkit (0.52)\nptyprocess (0.5)\npyasn1 (0.1.9)\npyflakes (1.0.0)\nPygments (2.0.2)\npython-dateutil (2.4.2)\nrequests (2.9.1)\nrsa (3.2.3)\nsetuptools (18.0.1)\nsimplegeneric (0.8.1)\nsix (1.10.0)\nspeedtest-cli (0.3.4)\ntinydb (3.1.0)\ntraitlets (4.0.0)\nvenmo (0.2.2, /Users/zackhsi/homespace/venmo)\nvirtualenv (13.1.2)\nwcwidth (0.1.5)\nsh\n(venv)\u279c  pip  which pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  which pip-sync\n(venv)\u279c  pip  which pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n(venv)\u279c  pip  deactivate\n\u279c  pip  which pip\n/usr/local/bin/pip\n. There are periods, but not sole period...\nsh\n/Users/zackhsi/Desktop/pip/venv/bin:/Users/zackhsi/workspace/devbox/.bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/vmware/appcatalyst/bin:/Users/zackhsi/workspace/devbox/.bin:/usr/local/sbin:/Users/zackhsi/bin:/Users/zackhsi/.fzf/bin:/Users/zackhsi/bin\n. sh\n(venv)\u279c  pip  which -a pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n/usr/local/bin/pip\n(venv)\u279c  pip  which -a pip-sync\n/usr/local/bin/pip-sync\n. sh\n(venv)\u279c  pip  pip list\npip (7.1.2)\nsetuptools (18.2)\nwheel (0.24.0)\n(venv)\u279c  pip  which pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  pip install pip-tools\nCollecting pip-tools\n  Using cached pip_tools-1.4.3-py2.py3-none-any.whl\nCollecting six (from pip-tools)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools)\n  Using cached click-6.2-py2.py3-none-any.whl\nCollecting first (from pip-tools)\n  Using cached first-2.0.1-py2.py3-none-any.whl\nInstalling collected packages: six, click, first, pip-tools\nSuccessfully installed click-6.2 first-2.0.1 pip-tools-1.4.3 six-1.10.0\n(venv)\u279c  pip  pip list\nclick (6.2)\nfirst (2.0.1)\npip (7.1.2)\npip-tools (1.4.3)\nsetuptools (18.2)\nsix (1.10.0)\nwheel (0.24.0)\n(venv)\u279c  pip  which pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  pip-sync\nCannot uninstall requirement appnope, not installed\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-sync\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 68, in cli\n    pip_flags=pip_flags))\n  File \"/usr/local/lib/python2.7/site-packages/piptools/sync.py\", line 137, in sync\n    check_call(['pip', 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n  File \"/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py\", line 540, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['pip', 'uninstall', '-y', 'appnope', 'aws-shell', 'awscli', 'boto3', 'botocore', 'colorama', 'configobj', 'decorator', 'docutils', 'flake8', 'futures', 'gevent', 'gnureadline', 'greenlet', 'httplib2', 'ipython', 'ipython-genutils', 'isort', 'jmespath', 'lxml', 'mccabe', 'oauth2', 'path.py', 'pep8', 'pexpect', 'pickleshare', 'prompt-toolkit', 'ptyprocess', 'pyasn1', 'pyflakes', 'pygments', 'python-dateutil', 'rsa', 'simplegeneric', 'speedtest-cli', 'tinydb', 'traitlets', 'venmo', 'virtualenv', 'wcwidth']' returned non-zero exit status 1\n. So, this looks like a problem:\nsh\n(venv)\u279c  pip  which -a pip-sync\n/Users/zackhsi/Desktop/pip/venv/bin/pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  which pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  echo $PATH\n/Users/zackhsi/Desktop/pip/venv/bin:/Users/zackhsi/workspace/devbox/.bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/vmware/appcatalyst/bin:/Users/zackhsi/workspace/devbox/.bin:/usr/local/sbin:/Users/zackhsi/bin:/Users/zackhsi/.fzf/bin:/Users/zackhsi/bin\nWhy does it not use the local venv?\n. I'm so confused. It should pick the first occurrence in $PATH, not the last...\nIt works correctly for pip:\nsh\n(venv)\u279c  pip  which pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n(venv)\u279c  pip  which -a pip\n/Users/zackhsi/Desktop/pip/venv/bin/pip\n/usr/local/bin/pip\n. zsh\n(sorry for delay)\n. sh\n(venv) \u279c  Desktop  type -a pip\npip is /Users/zackhsi/Desktop/venv/bin/pip\npip is /usr/local/bin/pip\nhash -r is interesting.\n. Yup\n(venv) \u279c  Desktop  type -a pip-sync\npip-sync is /usr/local/bin/pip-sync\n(venv) \u279c  Desktop  type -a pip\npip is /Users/zackhsi/Desktop/venv/bin/pip\npip is /usr/local/bin/pip\n. ",
    "alexwforsythe": "@zackhsi: what's the result of type -a pip when you're in the virtualenv?\nOn a related note, pip-sync doesn't seem to recognize virtualenvs for me in bash or zsh:\n``` bash\nalex@spicylegato:~$ pip-compile requirements.in \n\nThis file is autogenerated by pip-compile\nMake changes in requirements.in, then run this to update:\n\npip-compile requirements.in\n\nrequests==2.9.1\nalex@spicylegato:~$ mkvirtualenv test\nNew python executable in /home/alex/.virtualenvs/test/bin/python\nInstalling setuptools, pip, wheel...done.\nvirtualenvwrapper.user_scripts creating /home/alex/.virtualenvs/test/bin/predeactivate\nvirtualenvwrapper.user_scripts creating /home/alex/.virtualenvs/test/bin/postdeactivate\nvirtualenvwrapper.user_scripts creating /home/alex/.virtualenvs/test/bin/preactivate\nvirtualenvwrapper.user_scripts creating /home/alex/.virtualenvs/test/bin/postactivate\nvirtualenvwrapper.user_scripts creating /home/alex/.virtualenvs/test/bin/get_env_details\n(test) alex@spicylegato:~$ pip list\npip (8.0.2)\nsetuptools (20.0)\nwheel (0.29.0)\n(test) alex@spicylegato:~$ pip-sync -n requirements.txt \nWould uninstall:\n  cryptography\n  httpie\n  virtualenvwrapper\n  pygments\n  ipython\n  virtualenv\n  argparse\n  pbr\n  virtualenv-clone\n  wsgiref\n  stevedore\n  pyasn1\n```\nOf course, I'm invoking the global pip-sync. Should installing pip-tools inside a virtualenv be a requirement of using pip-sync to manage that virtualenv? If so, here's another interesting result:\nbash\n(test) alex@spicylegato:~$ pip install pip-tools\nCollecting pip-tools\n  Using cached pip_tools-1.6-py2.py3-none-any.whl\nCollecting six (from pip-tools)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools)\n  Using cached click-6.2-py2.py3-none-any.whl\nCollecting first (from pip-tools)\n  Using cached first-2.0.1-py2.py3-none-any.whl\nInstalling collected packages: six, click, first, pip-tools\nSuccessfully installed click-6.2 first-2.0.1 pip-tools-1.6 six-1.10.0\n(test) alex@spicylegato:~$ type -a pip-sync\npip-sync is /home/alex/.virtualenvs/test/bin/pip-sync\npip-sync is /usr/local/bin/pip-sync\n(test) alex@spicylegato:~$ pip-sync -n requirements.txt \nWould uninstall:\n  cryptography\n  httpie\n  virtualenvwrapper\n  pygments\n  ipython\n  virtualenv\n  argparse\n  pbr\n  virtualenv-clone\n  wsgiref\n  stevedore\n  pyasn1\n(test) alex@spicylegato:~$ hash -r\n(test) alex@spicylegato:~$ pip-sync -n requirements.txt \nWould install:\n  requests==2.9.1\nAs you can see, I had to reload my shell's PATH hash table in order to use the new installation of pip-tools.\n. And what's the output of type -a pip-sync and type pip-sync in the virtualenv? If it's still using the one in /usr/local/bin, I'm guessing it's shell-related. I'm using zsh with prezto, but I haven't been able to recreate your issue.\n. Weird. In your earlier comment, which -a was showing both versions of pip-sync:\n\nSo, this looks like a problem:\n(venv)\u279c  pip  which -a pip-sync\n/Users/zackhsi/Desktop/pip/venv/bin/pip-sync\n/usr/local/bin/pip-sync\n(venv)\u279c  pip  which pip-sync\n/usr/local/bin/pip-sync\n\nDid the virtualenv change?\n. Ah, that makes sense. It took me awhile to realize that a global pip-sync isn't supposed to work on local/virtual environments. The scripts work as you say on 1.6.1. Running hash -r is still necessary to update the shell, but I don't think that's a legitimate issue.\nI was thinking some combination of the following would save other users the confusion:\n1. Include virtualenv-specific instructions in the README\n2. Show a warning when invoking a global pip-sync while a virtualenv is active\n3. Update pip-sync to fetch installed distributions of a virtualenv if one is active (this would eliminate the need to install pip-tools inside virtualenvs altogether)\nIf you're interested, I was able to get number 3 working.\n. It would seem so, but I'm not getting that behavior on Ubuntu:\n``` bash\nalex@spicylegato:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 15.10\nRelease:    15.10\nCodename:   wily\nalex@spicylegato:~$ pip-sync --version\npip-sync, version 1.6\nalex@spicylegato:~$ pip-compile requirements.in \n\nThis file is autogenerated by pip-compile\nMake changes in requirements.in, then run this to update:\n\npip-compile requirements.in\n\nrequests==2.9.1\nalex@spicylegato:~$ mkvirtualenv test\nNew python executable in /home/alex/.virtualenvs/test/bin/python\nInstalling setuptools, pip, wheel...done.\n(test) alex@spicylegato:~$ pip list\npip (8.0.2)\nsetuptools (20.1.1)\nwheel (0.29.0)\n(test) alex@spicylegato:~$ hash -r\n(test) alex@spicylegato:~$ type -a pip-sync \npip-sync is /usr/local/bin/pip-sync\n(test) alex@spicylegato:~$ pip-sync -n requirements.txt \nWould uninstall:\n  cryptography\n  pymysql\n  virtualenvwrapper\n  pygments\n  httpie\n  virtualenv\n  argparse\n  pbr\n  virtualenv-clone\n  wsgiref\n  stevedore\n  pyasn1\n```\nI believe it's because the shebang in the entry point script (/usr/local/bin/pip-sync) that setuptools generates for me doesn't use env to locate the python executable:\n``` bash\n!/usr/bin/python\n```\nAs opposed to:\n``` bash\n!/usr/bin/env python\n```\nI found a way around this, though. I can make a new issue if this is getting too far off.\n. How did you cause the error? Do you get it when calling pip-sync inside of a virtualenv?\n. ",
    "jbbarth": "Confirmed here too, we had Django upgraded to 1.9.x while it was actually locked to 1.8.3 in our requirements.in file, and we were upgrading a specific package that didn't lock the dependency properly. Maybe it has something to do with case, as in said package, the requirement was on django, not Django. Not sure.\nI'd suggest you prune pip-tools 1.4.3 if you cannot fix it soon because it breaks one of the fundamental behaviors that make pip-tools so awesome (and I mean it in a big way, you made our lives far easier, thanks!! :-))\nGood luck, happy to help if I can, let me know.\n. Great, thanks a lot!\n. Confirmed here. It was not the case in pip-tools 1.4.4, but I see this now with pip-tools 1.6. In my case the \"--extra-index\" option sits in ~/.pip/pip.conf for what is worth, so not specific to the requirements.in parsing.\nFor what is worth everything still works normally despite of this bug, so not a blocker.\n. ",
    "syphar": "thanks @nvie \n. ",
    "daradib": "@nvie I understand the hesitation to add another configuration option. However, some users have a different PyPI server configured as index-url in ~/.pip/pip.conf, for example, a local mirror or cache of the PyPI server or a private, password-protected PyPI server, in which case I think the credentials get stored in requirements.txt. So there are some use cases in my opinion. What do you think?\n. Thanks that was quick!\n. ",
    "koleror": "Hi, I add the issue myself running pip-compile with git urls indeed. \nThe format of the github urls is: -e git+ssh://git@github.com/<user>/<repo>.git@<tag>#egg=<egg>, if it can help?\nAlso, I get No handlers could be found for logger \"pip.req.req_install\". ",
    "amanagr": "Did anyone figure this out? @cancan101 ??. ",
    "suutari-ai": "This probably isn't directly usable in pip-tools, but if it's any help, here's how I implemented this feature to Prequ: https://github.com/suutari/prequ/commit/634ef5c5daae3eb1911da2b482a766667d8d6364. Yeah, I agree with @jdufresne. Introducing the new options and deprecating the old ones would be the nicest first step.. I think commit afb3db6e9b05ba618e776baf7a9b5322e3a30e65 from PR #441 fixes this. Though it breaks --no-annotate, but there is a fix for it in commit abf25d8f1754dd3717519e339d0e727cfcc10633.. Is this really something that pip-compile should do? Shouldn't the compiled requirements.txt target a single environment rather than having conditional dependencies for several different environments? (Environment = Python version, platform (linux/win/..), etc. which can be conditional with the markers)\n\nE.g. if I have a conditional requirement foobar; python_version < \"3.0\" in my requirements.in and I compile that with Python 2, should it still generate the foobar==ver dependency to the txt file (with the marker)?\nWhat if some package foo has conditional dependencies like baz2; python_version < \"3.0\" and baz3; python_version >= \"3.0\", should pip-compile be able to to add both packages with their pinned versions to the generated txt?\n\nWhat I'm thinking here is that, if the requirements.txt should be compiled in a way that it supports more than one environment, then the compilation should be able to collect all possible environment variations from the source dependencies and pin the versions and use markers accordingly. This will get very complicated. I thought pip-compile is about locking down the specific versions and specific packages and conditional entries in the generated file doesn't feel like it's in line with that.. > I think your question contains a misconception: When pip-compile \"compiles\" a requirements.txt file, the Python version and OS used to execute pip-compile has no impact on the output.\nDo you mean that your PR changes this somehow?\nAt least currently it makes a big difference. Here's an example:\n```\n$ . venv-py2/bin/activate\n(venv-py2) $ pip-compile --version; python --version\npip-compile, version 1.8.0\nPython 2.7.12\n(venv-py2) $ cat requirements.in \nipython\n(venv-py2) $ pip-compile requirements.in -o requirements-py2.txt\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements-py2.txt requirements.in\n\nappdirs==1.4.3            # via setuptools\nbackports.shutil-get-terminal-size==1.0.0  # via ipython\ndecorator==4.0.11         # via ipython, traitlets\nenum34==1.1.6             # via traitlets\nipython-genutils==0.2.0   # via traitlets\nipython==5.3.0\npackaging==16.8           # via setuptools\npathlib2==2.2.1           # via ipython, pickleshare\npexpect==4.2.1            # via ipython\npickleshare==0.7.4        # via ipython\nprompt-toolkit==1.0.13    # via ipython\nptyprocess==0.5.1         # via pexpect\npygments==2.2.0           # via ipython\npyparsing==2.2.0          # via packaging\nscandir==1.5              # via pathlib2\nsimplegeneric==0.8.1      # via ipython\nsix==1.10.0               # via packaging, pathlib2, prompt-toolkit, setuptools, traitlets\ntraitlets==4.3.2          # via ipython\nwcwidth==0.1.7            # via prompt-toolkit\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools                # via ipython\n(venv-py2) $ deactivate\n$ . venv-py3/bin/activate\n(venv-py3) $ pip-compile --version; python --version\npip-compile, version 1.8.0\nPython 3.5.2\n(venv-py3) $ pip-compile requirements.in -o requirements-py3.txt\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements-py3.txt requirements.in\n\nappdirs==1.4.3            # via setuptools\ndecorator==4.0.11         # via ipython, traitlets\nipython-genutils==0.2.0   # via traitlets\nipython==5.3.0\npackaging==16.8           # via setuptools\npexpect==4.2.1            # via ipython\npickleshare==0.7.4        # via ipython\nprompt-toolkit==1.0.13    # via ipython\nptyprocess==0.5.1         # via pexpect\npygments==2.2.0           # via ipython\npyparsing==2.2.0          # via packaging\nsimplegeneric==0.8.1      # via ipython\nsix==1.10.0               # via packaging, prompt-toolkit, setuptools, traitlets\ntraitlets==4.3.2          # via ipython\nwcwidth==0.1.7            # via prompt-toolkit\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools                # via ipython\n```\nMy point is: If you want the same requirements.txt to work for different environments, then pip-compile should collect all the possible environment markers that every package in the dependency tree has and take those into account. These env markers should then be combined and the generated file should have those env markers added.  I don't know how easy that would be, but I bet generating a different requirements.txt files for each environment is much easier.. > Note this feature has no impact on the behavior of pip-compile unless someone explicitly adds environment markers to requirements.in. So this feature solves real-world problems that have affected several people, and it's harmless if you don't want or need to use it. What's not to like?\nI think it should be at least possible to toggle between preserving the environment markers from the in-file or to resolve them. Suppose you're using the \"different file for each env\" flow which I suggested and have some env markers in the in-file.  Then you'd expect each output file to have different contents (e.g. requirements-py2.txt only having the py2 deps and requirement-py3.txt having py3 deps): each file having only the concrete dependencies for that env.\n. How does this interact with generate-hashes option btw.?\nI've previously had some problems when I used a requirements.txt file with the generated hashes and tried to install those requirements with different Python version. The txt file was for Python 3.4 and I installed it with Python 3.5, so it wasn't even a major version difference. Having the same txt file used for more than one env should then probably have hashes of each env too or otherwise the install will fail to hash mismatch.\nYeah... this is also another story and this feature will still be useful when not using hashes, but should the --generate-hashes option be disabled while using this feature? (Since these two don't combine well.). I wasn't suggesting anything fancy for this. Maybe something like this could work:\n```python\nin scripts/compile.py\n@click.option(...)\n@click.option('--preserve-env-markers', ...)\ndef cli(verbose, ..., generate_hashes, ..., preserve_env_markers, ...):\n    ...\n    if preserve_env_markers and generate_hashes:\n        raise raise click.BadParameter(\n            \"--preserve-env-markes is not compatible with --generate-hashes\")\n``\n. This should probably be closed now that #557 is merged. Right?. IMHO Pulling the hashesh from Warehouse is a different issue. This fixes two existing bugs. That would be a new feature, so I'd rather create a new PR of it.. Can't reproduce this (Ubuntu 16.04, Python 3.5.2, pip-tools 1.9.0). Are you sure that you compiled the requirement file with Python 3? Try removing the txt file and runningpip-compile` again in Python 3 venv.. I'm also wondering if there could still be some part of Python 2 tool chain installed, e.g. pip or virtualenv. Another thing that could affect the results is if there is some environment variable affecting pip configuration or a pip configuration file present.\nCan you copy-paste the output of these commands for us?\n```shell\nactivate the problematic virtualenv first\nls -l $VIRTUAL_ENV/bin\nls -l /usr/bin/{python,pip,virtualenv}*\nhead -n1 $VIRTUAL_ENV/bin/pip\nhead -n1 /usr/bin/pip\nls -l ~/.config/pip/\nls -l ~/.pip/\nls -l /etc/pip.conf\nenv | grep PIP_\n```\nAlso it seems that if a new virtualenv is created with a -v option, then there is very good debugging info in the output. So it would also be very helpful, if you could create a new virtualenv with the -v (and every other option that you used previously) and copy-paste its output too.. I just fixed a very similar problem with Prequ. See Prequ PR #12. It was caused by commit 4900c7c, which isn't part of pip-tools 1.9.0 though. (First pip-tools version which has it is 1.10.0.rc1.) Are you sure you're using pip-tools 1.9.0?. OK. I don't really know anything in the 1.9.0 code base that would cause similar behaviour and that's why I was guessing that this could be a newer code. Or maybe the cache was built with newer code? (If a broken pip-tools is used to compile incorrect dependencies, they will be cached and won't be recalculated until the cache is purged.)\n\nStill, I think you've put the finger on something : pip-tools seems to be overriding some of pip's behaviour, which breaks my \"we're not messing up with pip\" assumption. I'm now sad.\n\nThat's one of the reasons why I maintain my own fork of pip-tools, Prequ. (Another reason is that there's no new releases.). I think the non-deterministic part of this is the following line in Resolver.find_best_match:\npython\n        candidates_by_version = lookup_table(all_candidates, key=lambda c: c.version, unique=True)\nThis builds a lookup table of the candidates by version. An unordered set of InstallationCandidate objects is assigned for each version. The py2 wheel, py3 wheel and tar.gz packages are in this set. The best candidate is then selected from this set using finder._candidate_sort_key as sorter key, but since this is monkey-patched to always return the same value it effectively selects a random candidate with the best version.\nYou can try to set environment variable PYTHONHASHSEED to an integer value to create deterministic results. (I.e. try with export PYTHONHASHSEED=1, export PYTHONHASHSEED=2, ...)\n. It's not actually only the environment markers that will affect the dependencies. Also different distribution packages for the same requirement might have different dependencies.\nAn example of this is provided in issue #558: django-auth-ldap==1.2.15 has three different packages:\n * django_auth_ldap-1.2.15-py2-none-any.whl requiring django and python-ldap>=2.0;\n * django_auth_ldap-1.2.15-py3-none-any.wh requiring django and pyldap; and\n * django-auth-ldap-1.2.15.tar.gz requiring django and either python-ldap>=2.0 or pyldap depending on which python version is used to run the setup.py.\nIn ideal world everyone would use environment markers, but in reality there are many distributions in PyPI which have different dependencies for the different packages of a release.. Thanks @vphilippon. Good work!\nNevertheless, don't get me wrong, but I think this is still quite hacky, and I don't see the point. Why don't you just revert the commit 4900c7c which is causing this? Why would I ever want hashes of non-compatible wheels to my requirements.txt?\nIf you're trying to generate a requirements.txt which works for all platforms and Python versions, then this would make sense, but since the dependencies aren't calculated for all platforms, but only for the current one, the requirements.txt doesn't work for another platform anyway. Creating a requirements.txt which works for all platforms is requested in #563 and if that is ever implemented this hack would cause more harm than good.\nAs I said in PR #460, my opinion is that requirements.txt should be locking a single set of the requirements which is tested to work. The point of locking the versions is to get reproducible environment and if the platform or Python version is different, then it's no more reproducible: Those packages might act very differently on another platform and, as seen here, even the required packages might be different.\nThe whole point why I want locked requirements is confidence and having platform independent requirements.txt doesn't help there.\nIf I want to install a project for another platform (which is untested), then I should be using the requirements.in to generate a new lock file (requirements-{new_platform}.txt) and then install from that. If it works, the new lock file can be committed as a sign of \"this is also a supported and tested platform\".. > As a side note, any reason why I wasn't able to add you as \"official reviewers\"?\nBecause I wasn't member of @jazzband yet. I joined now. Didn't know it's that easy to join.. @jdufresne:\n\n\nUpdate pip-tools to generate all hashes\nStop generating hashes\nUnify all developers on one platform\n\n\nYou're missing couple options:\n\nUnify all developers to use one of N pre-specified platforms and generate a requirements-{platform_tag}.txt for each supported platform\nLet CI and deployment scripts use requirements.txt, but developers on a different platform use requirements.in.\n\nIMHO this would make sense, since having only the hashes of the main platform in the txt file should help ensuring that CI is testing the same stuff as is installed to production.\nBut anyway. I'm not against merging this. Even though it's not something that I want, it's still a feature that someone finds useful. And merging this should make pip-compile work better and would only make it easier to add an option to disable the 'generate hashes for all platforms'.. @vphilippon:\n\nCurrently, I see no harm in having --generate-hashes including all hashes.\nI agree with you that reusing a requirements.txt on multiple platforms is something that should be done really carefully, and on a case-by-case basis after making sure there are no dependencies difference.\n\nFor me the 'generate hashes for all platforms' feature is more harmful than useful, because I don't want hashes of another platform to my txt files. It's also encouraging users to reuse the same requirements.txt on different platforms, which IMHO is not a good idea.\n\nThere could be dependencies which are missing from the txt file. Happens when the generating platform didn't have those dependencies, but the using platform has. (E.g. enum34 on python 2). Then those dependencies are not pinned and things might break when they get upgraded silently.\nThere could be extra dependencies in the txt file. Happens when the generating platform has some dependencies which the using platform doesn't.\nIt's also possible that some version of a dependency is not available for all platforms which would affect which version is pinned.\n\nBut the feature is there already. Fixing it is wise. :+1: . \n\n\nUnify all developers to use one of N pre-specified platforms and generate a requirements-{platform_tag}.txt for each supported platform.\n\nThis seems like a mess to manage, honestly.\n\nYeah, this certainly needs some tooling and a new feature so that it would be possible to generate a txt for another platform.\n\n\nLet CI and deployment scripts use requirements.txt, but developers on a different platform use requirements.in.\n\nA problem with this is the requirements in requirements.in aren't pinned. So different development environment will be using different versions of the dependencies, causing subtle behavior differences.\n\nDifferent platforms will also cause behavior differences. Only way to get exactly same behavior is to remove that factor too.\nAlso, if it's not working with a certain version of a requirement, you should either (a) make it work with it by changing your code or (b) limit out the non-working versions in requirements.in (using specifiers like <ver or ~=). The locked requirements.txt should be easily upgradeable anyway, so the project should usually just work after regenerating the requirements.txt.\n\n\nFor me the 'generate hashes for all platforms' feature is more harmful than useful, because I don't want hashes of another platform to my txt files.\n\nHmm, this is interesting to me. @suutari-ai I think you make a convincing argument. I may be persuaded that generating all hashes was a bad idea for a requirements.txt that will be used on a production server. I honestly don't know what is best at this point. If there is large agreement to revert the feature, I won't stand in the way.\n\nMaybe it should be optional at least? (Which would be another PR, of course.)\nFor Prequ, I just reverted it (https://github.com/suutari/prequ/pull/12), since my plan there is to implement the 'generate a txt for another platform' feature, but it seems that pip-tools is going to another direction. That's OK too, since I'll probably just use my own tool anyway. :smile:\nIf the feature is not reverted, then this fix should be merged at least.. This sounds like the same issue as fixed in PR #571. Can you try if it works with master?. How about a [pip-tools] section in setup.cfg? Most Python tools can read their config from setup.cfg  (e.g. pytest, flake8, isort, etc.) and IMHO it's nicer to have fewer files in the project root.. There's also my earlier PR #464 which addresses this same issue, I think.. Thanks @jdufresne!. Can you run the pip-compile command with -v flag added and post the output here?. I think you found two distinct bugs with this:\n\nIn Pip. It shouldn't allow you to install ipython==7.0.1 with prompt-toolkit==1.0.15, since ipython's requirements specify prompt-toolkit<2.1.0,>=2.0.0.\n\nIn Pip-tools. The pip-tools resolver should be able to find a set of packages that matches your requirements input specifications in the graph.in, since there is one.\n\n\nTo prove that there is an existing resolution to the given graph.in, try adding ipython<7 line to your graph.in and rerun pip-compile. At least on my machine, that resolves to a set of packages without any version conflicts present.\n\n\nImplementing a resolver that could find the resolution automatically is not trivial though. There are some resolver implementations which do that kind of stuff, like libsolv, but that isn't available as a pure Python package AFAIK.. This can be solved, if you define which of the files is the \"master\".\nI have solved this in Prequ by making the main requirements.txt as the master which is always compiled first. When compiling the other requirements files, Prequ will add\n-c requirements.txt\nline to the input lines so that the compiled requirements-*.txt won't have any conflicting dependencies. Here's the relevant code: https://github.com/suutari/prequ/blob/v1.3.0/prequ/configuration.py#L182\nThough, there might still be conflicting dependencies in requirements-a.txt and requirements-b.txt, if those are not in requirements.txt, but that's not very common case (and it's not yet handled by Prequ).. argparse is not stdlib package on Python <=2.6 (or 3.0 or 3.1). See https://pypi.org/project/argparse/. Ah, true. So argparse actually is provided by the stdlib on all supported Python versions.. ",
    "richafrank": "@vphilippon I'd love to see this enhancement. If I submit a PR using the design from above, would that be welcome, i.e. filtering out editable dependencies at persistence time? Another idea is to maintain a separate in-memory-only cache for editables.. @vphilippon Closable from #694 ?. Thanks @vphilippon! I updated the test wheel.. :+1:. I believe this stems from pip-compile's use of distutils to run setup.py, which was broken in python versions up through 3.5.. Great - thanks @atugushev !. Sounds reasonable to me. I had been trying not to assert much beyond the part of the output that changes, but that's not so bad.. Certainly can do that. \nRight now allow_unsafe gets here from being passed to each call of OutputWriter.write. Is there any big benefit from the current model? I could see re-using a single instance across write calls where this value varies, if it's slow to construct the instance. It looks like both the constructor and write take a lot of arguments, and I'm not sure how they are meant to be split; however, I only see one caller of write, which is right after instantiation, so maybe there isn't an intentional divide...\nAh, the other caller is in the tests, so I'll need either a new \"unsafe writer\" fixture, or I can update the tests to expect the fixture gives them a class (or partial), instead of an instance. Or maybe there's a third, better way? Preferences there?. Ah, thanks for the history. Sounds good - will do.. ",
    "palfrey": "Tried using this and got\nTraceback (most recent call last):\n  File \"/Users/palfrey/src/potboiler/python/ENV/bin/pip-compile\", line 9, in <module>\n    load_entry_point('pip-tools==1.4.5.dev0', 'console_scripts', 'pip-compile')()\n  File \"/Users/palfrey/src/potboiler/python/ENV/lib/python3.5/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/palfrey/src/potboiler/python/ENV/lib/python3.5/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/Users/palfrey/src/potboiler/python/ENV/lib/python3.5/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/palfrey/src/potboiler/python/ENV/lib/python3.5/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/palfrey/src/potboiler/python/ENV/lib/python3.5/site-packages/piptools/scripts/compile.py\", line 132, in cli\n    for index_url in repository.finder.index_urls:\nAttributeError: 'MinimalUpgradeRepository' object has no attribute 'finder'\nwhich looks to be a thing the PyPIRepository has (https://github.com/ejames/pip-tools/blob/feature-no_upgrade_proxy_repository/piptools/repositories/pypi.py#L41)\n. Previous issue no longer occurs, and mostly works, although I did have this one odd diff\n-# setuptools==19.2          # via logilab-common, voluptuous\n+# setuptools                # via logilab-common, voluptuous\n. I'd also like to see this in. Any thoughts on getting it merged? Is it worth setting up a separate PR with just the sync.py changes?. ",
    "pwmarcz": "@nvie As a user I agree, holding back upgrades as a default would be friendlier: if I'm adding a new package, I don't want to be bothered about upgrades until I ask for them explicitly.\nVery much looking forward to using this!\n. ",
    "jezdez": "Yay! \\o/\n. @nvie The hash created by the hash checking mode is not identical to the MD5 hash generated and offered by PyPI download URLs. \"Manually pasting\" is basically the workflow that is expected for the hash checking feature, since it's basically part of the manual vetting process. You should use the hash subcommand as @ubernostrum indicated for that. I'm personally not convinced the hashin package is useful in that regard since it kind of defeats the purpose of the whole feature. Here's what I'd do right now (without pip-tools):\n- pip download <package-name>\n- vet the file that got downloaded that it's what you'd expect\n- pip hash <file that got downloaded>\n- copy the hash to requirements file\nThe features that would be useful of pip-tools to have is:\n- don't delete the --hash .. lines in the requirements \n- offer optional command line option e.g. --auto-hash to the pip-compile tool so it can be used like hashin\n. @pjenvey Doesn't sound like a bad idea to me, what do you think @dstufft?\n. @nvie Huzzah!\n. Good news everyone! @nvie has moved the pip-tools project into the Jazzband GitHub organization. That means that pip-tools is now a lot easier to contribute to.\nTo be able to review PRs and merge code, go to jazzband.co and login there to join. You're then given access to this repo and others. Please make sure you've read the Jazzband Code Of Conduct and contribution guidelines before you dive in though.\nSome more info can be found here. And of course in the documentation.\n. @vphilippon 1.10.1 is out.. @vphilippon I recommend adding a tox test env that installs readme_renderer which is the official renderer of readmes on PyPI. That way you'll know that it'll work before you release. Here's an example.. @vphilippon Just a FYI: it's also used in the \"legacy-pypi\" project: https://github.com/pypa/pypi-legacy/blob/f45232ddf7c4c501d0b2f9a6e88f136973199718/webui.py#L2001-L2008. @vphilippon Wow, what a rabbit hole filled with yaks. Good work! \ud83d\udc4f . Hi all, a Jazzband roadie here.\n@tysonclugg has asked me to weigh in here with regard to a missing attribution of contribution for the comment above and for a commit @tysonclugg has allegedly contributed as well (in his own words: \"to add the pipmaster factor to tox.ini and travis.yml\"). I can't verify the latter as it no longer is part of the pull request or the set of commits that landed in master due to the way the commits of this pull request were squashed.\nIt's a particular unfortunate combination of circumstances that requires reminding everyone of the existence of the Jazzband code of conduct which governs the conduct in Jazzband projects. One aspect of it is the ethical and professional manner in which we must collaborate, including respecting the right for attribution of contributions by fellow members.\nReading through the comments of this PR I see there were multiple opportunities to acknowledge @tysonclugg's contribution (e.g. keep the commit intact, adding a \u201cCo-authored-by\u201d line to the commit message, updating an AUTHORS file, leaving a code comment etc). As none of these steps were taken and multiple requests for attribution (e.g. https://github.com/jazzband/pip-tools/pull/657#issuecomment-387286401) have been ignored this is a violation of the Jazzband Code of Conduct.\nAdditionally I've found the response to the requests for attribution lacking the appropriate respect towards @tysonclugg as it specifically boasted to take great care to attributing contributions (and failing to do so) and called @tysonclugg's motivation to contribute into question at the same time. I want to be clear that this is not acceptable behavior towards fellow Jazzband members as it does not cater to a welcoming community.\nHere are the actions to follow:\n\n\n@techalchemy Please add an attribution for @tysonclugg's contribution somewhere along the lines of other contributions to this project.\n\n\n@techalchemy Consider offering an apology to @tysonclugg for the misconduct in professional and ethical aspects of this collaboration.\n\n\n@tysonclugg Please accept my apology for the experience you've had and my thanks for working on resolving it by reaching out to me.\n\n\nThis pull request will be locked as it's not an appropriate avenue for feedback. Please write to roadies@jazzband.co instead.\n. Update on the actions taken:\n\n\n@vphilippon agreed to add an attribution of @tysonclugg's contribution to the CHANGELOG upon the next release.\n\n\n@techalchemy has been freed from the action to add an attribution of @tysonclugg's contribution as stated in the previous comment.\n\n\nI consider the matter resolved now.. This method needs to be added to the tests.conftest.FakeRepository as well or else the tests fail\n. Do you want to recommend a particular version scheme?. suggestion\n  deploy a pip-tools release in the **Jazzband private package index** upon success.. suggestion\n  deploy it to the public PyPI if all is correct.. suggestion\n- Once the release to the public PyPI is confirmed, close the milestone.. ",
    "ubernostrum": "I've been using hashin for semi-automated hash generation, but pip hash will at least get you the hash to stdout.\n. ",
    "edmorley": "\nI'm personally not convinced the hashin package is useful in that regard since it kind of defeats the purpose of the whole feature.\n\nI agree that skipping the 'vet the package' step does increase the risk, however I'm not convinced that it's by a huge amount. \nFor me, the major advantage of the new pip 8 hashing feature is that it protects against:\n- An active MITM attacker (they'd need to MITM the connection to PyPI from every one of: my local dev environment where the hashes were generated, Travis and the stage/prod deployment -- otherwise there would be a hash mismatch against a subset of them)\n- Accidental modification of an existing package (eg uploading a newer version over an older one, which actually happened in one of our dependencies last year, and thankfully was caught by peep)\n- Malicious modification of an existing package version (eg PyPI compromise, or package owner PyPI credential leak)\nBy not vetting a new package (or a new version release of an existing package), there is a risk that either:\na) the package author is trying to sneak malicious code into the package\nb) someone other than the author created a fake new version release (either via PyPI compromise or package owner credential leak)\nHowever for (a), I struggle to believe that most people would review the code thoroughly enough to spot intentionally obfuscated malicious code in a large package (even if diffing against a known good version, the changes could be hidden in a refactor; a backdoor can be implemented in very few lines).\nAnd for (b), you'd have to manually update to the new version of the package prior to the package owner realising that someone had just created a new release on their behalf. (Side note: perhaps PyPI should email all the package owners every time a new release is created? Edit: Filed pypa/warehouse#997)\n. ",
    "aaugustin": "I agree with @edmorley. That would be the most convenient way to take advantage of this feature.\nIt improves security in the same way HSTS does. If you're constantly victim of a MITM, you're still in trouble. If you're generally accessing the Internet without anything interfering, you'll notice if something does, whether voluntarily or not.\n. Works for me as well, thanks @jmbowman and @nvie!\n. ",
    "pjenvey": "This is now supported as of #383 so this can be closed, although it could also use some documentation. In this mode can we consider putting the \"via..\" comment on the line previous to the dep name for the sake of readability?\n. > In this mode can we consider putting the \"via..\" comment on the line previous to the dep name for the sake of readability?\nhttps://github.com/nvie/pip-tools/pull/413\n. Sounds good, patch updated to your suggestion. str(marker)'s redundant here, format's {} does it for you. I don't think this comment applies?. ",
    "yetanotherportfolio": "new version of pip just came out (8.0.1) that fix itself, should now work fine (I didn't test it).\n. ",
    "peramsathyam": "@nvie : https://github.com/nvie/pip-tools/blob/f7b5f0183c6aeacf119cec172fe9ad6cbad26512/piptools/sync.py#L131\nMay be here you have to check :) \nRunning pip uninstall -y argparse result in \nCannot uninstall requirement argparse, not installed\n. ",
    "allanlewis": "I think this might be related to #328.\n. I'm seeing the same behaviour on Fedora 23:\nsh\n$ python --version\nPython 2.7.11\n$ pip --version\npip 8.1.1 from /home/foo/.virtualenvs/bar/lib/python2.7/site-packages (python 2.7)\n$ pip-sync --version\npip-sync, version 1.6.4\n$ which pip-sync\n~/.virtualenvs/bar/bin/pip-sync\n$ pip-sync -n requirements/*.txt\nWould uninstall:\n<list of everything in my global site>\nWould install:\n<list of entries in 'requirements/*.txt'>\nThat's in a new virtualenv created using mkvirtualenv with just pip-tools installed.\n. ",
    "Daenyth": "This applies to setup.py files as well - I have dependency links in my setup.py and pip-compile is saying it can't find a package.. Any update here?. ",
    "ivan-latka-exponea": "Any progress or alternative solution?. ",
    "kengruven": "I don't know, and I'm not even using Python any more.  Feel free to close this issue, if nobody can reproduce it.. Strange.  I definitely have pip-tools 1.6 here in my virtualenv, but I also have pip-compile, too.  I guess I have an old version of pip-tools, and upgrading doesn't remove it?\nI found this post but I'm a little confused.  Is the pip-review functionality simply gone?  I guess this isn't a bug, then, but I thought that pip-review was incredibly useful, and I'm sorry to see it gone.\n. I don't mean to be a pain here, but I'm curious how this works.  Is it no longer possible with pip-tools to answer the question \"Are there new versions of the libraries I'm using?\"?\nThe pip-compile/pip-sync tools are great, but pip-review seemed like the Python version of \"apt-get update && apt-get upgrade\", and that's missing now.  (Except in a separate project -- thanks, @jgonggrijp!)\nI pin all my dependencies, so I have a requirements.in which looks like:\nFoo==1.2.3\nand then I pip-compile that into requirements.txt which looks like:\nFoo==1.2.3\nBar==1.1.1  # via Foo\nand that's great, but I don't see any easy way to tell what's new upstream.  When Foo 1.2.4 (or 1.3.0, or 2.0.0) is released, how can I find out about it?\nWith pip-compile, \"--dry-run\" will print to stdout, but it prints every dependency in the list, not just a summary of the ones that have upgrades.  And \"--upgrade\" tries to \"upgrade all dependencies to their latest versions\", but only their latest versions listed in the requirements.in file, so it won't tell me when a new version is released, anyway.\nOnce you've pinned your versions in requirements.in, how do you find upgrades?  Do you regularly go through them all by hand?  What's the intended workflow here?\nthanks,\n. ",
    "hugoboos": "Same here, with version 1.6\nI have one --extra-index-url <url> line at the top of my requirements.in. On every run of pip-compile the same --extra-index-url <url> is added to requirements.txt.\n. Possible fix in #371\n. ",
    "yakky": "Confirmed here\nMaybe just extract index_urls from finder beyond this point https://github.com/nvie/pip-tools/blob/master/piptools/scripts/compile.py#L132 ?\nTested with pip >= 7\n@nvie I can work a on PR if a solution is agreed \n. ",
    "mchristopher": "@yakky @nvie any update on this? I'm running into this problem as well.\n. ",
    "anthrotype": "I have the same issue with --extra-index-url occurring multiple times, when I include a pip-compile-generated requirements.txt inside my requirements.in file, e.g. by adding -r some_requirements.txt at the top of it.\n. What'is the status of this? I would like to be able to use the install_requires in my existing setup.py to generate the requirements.txt, instead of having to have a separate requirements.in for pip-tools that duplicate those.\nThanks.. >Why would you distribute a separate requirements.txt that duplicates dependency management?\nI don't think @tysonclugg was suggesting to distribute the requirements.txt to users.\nAt least, that's not what I'd want to do. My use case is a library which has some install requirements specified in its setup.py; I also want a requirements.txt with pinned-down versions of all the install_requires (e.g. to be used by tox to run the tests, not by the users who would just pip install my library).\nI'd like to keep the requirements.txt up-to-date using pip-compile, without having to have a separate requirements.in file that is just a duplicate of what is in my setup.py. I guess I could also read the content of requirements.in into the setup.py's install_requires, but it feels like a hack.\n. Don't be sorry. Thanks to you I discovered that cool new project! ;). There is also another problem: a dependency of a requirement which has an environment marker is always added unconditionally without a marker.\nIn the example above, the python3-version of doit==0.30.3 does no longer require six==1.10.0, but the latter is still added to the generated requirements.txt (# via doit), so it's always installed even when not required.\nI think that the dependencies of install requirements that have environment markers should only be installed when the top-level requirement is. That means, the marker should propagate to all the dependencies of marked requirements (unless these are also required by some other package which has no marker?).. ",
    "pmayer": "Is there a requirements.in file in the directory you are running the command in?\n. ",
    "tkwon": "+1 on adding options like --no-cache-dir. ",
    "scythargon": "+1. ",
    "si14": "Thanks for the clarification!\n. If I understand the issue correctly, I have the same problem with git (-e git+https://...) deps.. ",
    "akx": "(Found this via the HN thread.)\nI'm unable to reproduce this with a minimal test case:\n``` shell\n$ python -v\nPython 3.5.1\n$ pip install -U pip\nSuccessfully installed pip-8.1.0\n$ pip install pip-tools\nSuccessfully installed click-6.3 first-2.0.1 pip-tools-1.6 six-1.10.0\n$ echo \"Django\" > requirements.in\n$ pip-compile requirements.in\n\nThis file is autogenerated by pip-compile\nMake changes in requirements.in, then run this to update:\n\npip-compile requirements.in\n\nDjango==1.9.4\n```\nCan you post a script/test case which does repro this?\n. @mkleehammer I think that should be @1.0.1 (if there is a tag called 1.0.1), or #egg=sourcemap==1.0.1 (to tag an arbitrary URL as referring to a certain version). Would that make it work?\n. ",
    "The-Compiler": "FWIW I see the same with -e ./scripts/dev/pylint_checkers (with that being a local path in my repo)\n. ",
    "nicktime": "Just in case someone else runs into this, I was getting this traceback with my requirements.txt having a line like\n../somepackage\nRemoving it helped.. ",
    "alevikpes": "I still have this problem. Even for the simplest case mentioned here (with a single Django package). What was the solution?. ok, found it. This happened to me, when the requirements.txt existed already. I was expecting it to be overwritten. After I removed it, I got an error: piptools.exceptions.UnsupportedConstraint: pip-compile does not support URLs as packages, unless they are editable. Perhaps add -e option? (constraint was: git+ssh://git@gitlab.local.com/back-end/custom-package.git@latest (from -r requirements.in (line 45))). It was working fine after I removed the offensive line. But I would expect these kinds of links to be handled - I do not want the package to be editable.\nFor the rest, it is a nice tool. Thanks.. ",
    "blampe": "Yeah totally agree, I have a similar use case. The real problem is that it's an absolute path instead of relative. Feel free to close this if it's a dupe of #204.\n. ",
    "fmarco": "@nvie I see and i agree, but i tought to avoid keep excluded requirements even in that phase using pip-tools functions, anyway thanks for review.\n. ",
    "cyrusd": "Ok, updated the code to reflect your suggestions, I wasn't able to get the test for the 'unsafe' option to pass (indexing issues I think) but kind of ran out of time to look into it...\n. ",
    "alekzvik": "Implemented in #377 \n. Thank you for a great project.\n@nvie, When do you plan to release this on PyPI?\n. ",
    "wedi": "I can confirm this issue, too. pip-compile 1.6.5 kept complaining about the wrong pip version after downgrading my virtual environment pip to 8.1.1\n. Just for information: the issue persists on Mac OS X in 1.7.0.\n. ",
    "Groxx": "fwiw this is true for every python binary.  e.g. having fabric installed globally means you may run fabfile.py files against the global environment instead of whatever virtualenv you're in.\nIt might still make sense to detect a virtualenv and do something smarter!  I'm just not aware of any that do so.\n. Yeah, this would be great.  We've had to add a lint step to prevent enum from sneaking in because enum34 collides with it (why the heck did they think that was a good idea?!), ideally we could --exclude enum or similar right in the .in file so it's unmissable.\n. Still a problem with 1.8.0 fwiw. It also seems to be failing on pickleshare 0.7.3.  Compiling with verbose includes output like this under \"Finding secondary dependencies\":\npickleshare==0.7.3        requires pathlib2; python_version in \"2.6 2.7 3.2 3.3\"\nand has a traceback:\nTraceback (most recent call last):\n  File \"/home/myself/irslib/env/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 194, in cli\n    reverse_dependencies = resolver.reverse_dependencies(results)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/resolver.py\", line 242, in reverse_dependencies\n    return self.dependency_cache.reverse_dependencies(non_editable)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/cache.py\", line 139, in reverse_dependencies\n    return self._reverse_dependencies(ireqs_as_cache_values)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/cache.py\", line 163, in _reverse_dependencies\n    for name, version_and_extras in cache_keys\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/utils.py\", line 181, in lookup_table\n    for value in values:\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/piptools/cache.py\", line 164, in <genexpr>\n    for dep_name in self.cache[name][version_and_extras])\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 3008, in parse\n    req, = parse_requirements(s)\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2952, in parse_requirements\n    \"version spec\")\n  File \"/home/myself/irslib/env/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2917, in scan_list\n    raise RequirementParseError(msg, line, \"at\", line[p:])\npkg_resources.RequirementParseError: Expected version spec in pathlib2; python_version in \"2.6 2.7 3.2 3.3\" at ; python_version in \"2.6 2.7 3.2 3.3\"\nIPython 5.0.0 fails similarly, probably for the same reason.  Output for IPython:\nipython==5.0.0            requires backports.shutil-get-terminal-size; python_version == \"2.7\", decorator, pathlib2; python_version == \"2.7\" or python_version == \"3.3\", pexpect; sys_platform != \"wi\nn32\", pickleshare, prompt-toolkit<2.0.0,>=1.0.3, pygments, setuptools>=18.5, simplegeneric>0.8, traitlets>=4.2\n. I can open a new issue for this, but it'd still be useful to have a --no-trusted-host option.  Local / CI / production use different pypi hosts for us (for.... reasons?  I have no idea, I just live with it), and CI and production don't have access to the local ones, so we can't allow --trusted-host in our output.  All pypi access is done via pip.conf files.\nRight now I'm just removing those lines, but it'd be great to not have to do that.. Ah, nevermind, there is one!  Awesome.  Using it now!. heh.  I just ran into this yesterday, though mine was a couple dependencies deep.  Your example is waaay smaller than mine.\nI'm not even sure what a solution would look like.  If there are multiple candidates to backtrack, what's rational behavior?  To backtrack one until it runs out of options, then try reducing the next level up by one version and retrying all bottom-most versions?  Or maybe a more breadth-first approach would yield results quicker (backtrack the bottom-most once, then try the next level up once, etc)?\n. I'll just chime in with 100% voting for -r .txt.  It's important that prod == dev == test as much as possible, and that's essentially the only way to ensure everything's a strict superset of prod.\nWe manage ours with a small script that compiles them in the right order, and (though sometimes it breaks) we do a non-upgrade compile in CI that diffs the results, to make sure nobody modified them by hand.  It has stopped several mistakes already.\nOnly real downside is that sometimes the requirements.txt pins to a version that's incompatible with something in requirements-dev.txt - in an ideal world, we'd be able to check the rules from everything at once and (sometimes) avoid that.  But it's usually pretty easy to trace down.\n\nMaybe a better end-result would be to be able to compile all .in files at once, so pip-compile can detect the dependencies between them, and have it produce multiple corresponding .txt files instead of one combined one?  I'd find that more useful in pretty much every scenario I've encountered, and it seems like the right place to do it.. Yep, exactly.  Though maybe more accurately (with fake versions):\n- django in requirements.in\n- django-debug-toolbar in requirements-dev.in\n- run pip-compile --upgrade requirements.in, succeed with django==2.0\n- run pip-compile requirements-dev.in (or with --upgrade)...\n- ... discover django==2.0 is incompatible with all versions of django-debug-toolbar...\n- ... (\u256f\u00b0\u25a1\u00b0)\u256f\ufe35 \u253b\u2501\u253b #ragequit\nIt's all made worse when the cause is in requirements-do-this-first.in and the conflict is in requirements-dev-test-localdev-really-long-req-chain.in and it takes a while to figure out why django==2.0 is being chosen in the first place.  But once people understand the process / know to use --verbose, it doesn't usually take too long.. @jamescooke yeah, it does happen.  The alternative though, if you include -r requirements.in in your requirements-dev.in, is that this is possible:\n\npip-compile --upgrade requirements.in, get django==2.0\npip-compile --upgrade requirements-dev.in, get django==1.9 and django-debug-toolbar\ndev, test, etc against 1.9, but release against 2.0.\n\nThe mismatch between dev and prod and the lack of any error or warning are largely what pip-tools helps eliminate, so to me this is an entirely unacceptable result.  I much prefer to have the second step fail, which reveals that there is a problem, rather than rely on my eyes to catch the disparity between the two.. For virtualenvs-per-python-binary, have you seen pipsi?  https://github.com/mitsuhiko/pipsi\nIt essentially does this for you.  And there's a way to specify python 2/3 when doing so.  Very much recommended, since otherwise you end up polluting the global pip install with all kinds of incompatible stuff :|\nRun-as-module is convenient in its own right tho, good idea!  And probably lots easier to use with automation.. Similar to -e requirements, you probably need to name the file - otherwise it can't be shared with other dependencies that might use it.  Does it work if you add ....tar.gz#egg=some_name to the end?\nThat said, nameless dependencies would be useful / less annoying sometimes, even if they're somewhat fundamentally crippled for a dependency manager.  Might become an option in the long run?. I'll just chime in with generally disagreeing with recommending setup.py.  If it's the blanket recommendation, for applications it somewhat suggests \"build requirements from installable-thing X, but don't install installable-thing X\".\nThat's sorta like recommending using a loose requirements.txt to build requirements.real.txt and convincing people to install .real.txt instead.  Seems prone to accidents.\nPlus, for setup.py, reading requirements.in as the install_requires is simple and flexible, while signaling \"this project uses pip-compile\".  It also matches all other requirements-*.in patterns, where using setup.py does not, unless you define complex / possibly-contradictory install extras.  \nWith .in files, it's trivial to define disjoint requirement sets, which can sometimes be useful (for e.g. generating docs, linting with mypy which may require a different version of python altogether, different pip versions via tox, etc).  They may not even be capable of executing your code, which doesn't blend well with using setup.py.. Oh.  I do like defaulting to setup.py if there's no requirements.in file - that seems quite convenient for library writers who are already using it, which is great.  There's a bit of a conflict when you later define requirements.in and it changes default behavior, but that doesn't seem too bad.  So +1 from me for that part of this request!. Changes to the code: I'm not familiar enough to have an opinion on the actual implementation.  The \"fall back to setup.py\" as a whole sounds convenient though, I like it.  Automatically does the right thing for many library projects, which is great.\nChanges to the documentation, which prioritize setup.py: yes, somewhat opposed.  It's a relatively significant high-level recommendation change, and I'm not convinced it's for an improvement.  pip-compile is mostly for baking requirements.txt, which is most-important for applications in a production environment, not libraries (whose requirements.in or .txt will never be seen by the vast majority of users).. I managed to catch the error:\n```\n[localhost] local: pip-compile --no-index --upgrade requirements.in\n\n/.../env/local/lib/python2.7/site-packages/pip/utils/init.py(496)unzip_file()\n-> print \"caught\"\n(Pdb) l\n491                     fn = split_leading_dir(name)[1]\n492                 try:\n493                     fn = os.path.join(location, fn)\n494                 except:\n495                     import pdb; pdb.set_trace()\n496  ->                 print \"caught\"\n497                     raise\n498                 dir = os.path.dirname(fn)\n499                 if fn.endswith('/') or fn.endswith('\\'):\n500                     # A directory\n501                     ensure_dir(fn)\n(Pdb) location\nu'/tmp/tmpDj6_ltbuild/scandir'\n(Pdb) fn\n'test/testdir/subdir/unicod\\xc6\\x8f.txt'\n(Pdb)\n```\n\nUnfortunately, after fixing it once with @byjott's approach, I was unable to reproduce - if you're debugging, watch out!  I can reproduce at-will now though, if anyone would like more info.. Since things have changed a bit, think you could rebase?\nI'd love to get this in, inconsistent --trusted-host lines are the one thing at the moment that prevents dev and CI environments from generating identical *.txt files, and I love having an automated sanity check to catch people who edit requirements.txt.  Not sure how to bump it for attention though (beyond updating the code / leaving a comment).. Very very glad to see this repo getting action again, thanks a bunch for diving in!  Is this an appropriate channel for general questions about all the goings on, or are you trying to keep it focused on 1.8.1 and 1.9 plans?. ",
    "Triquetra": "I am also experiencing this issue on Windows 10 with Python 3.6 in a (python -m venv) virtualenv.  pip-compile seems to work fine, but pip-sync wants to uninstall all my global site-packages.. ",
    "KeynesYouDigIt": "Im having the same problem on ubuntu 16.04 ... \npip-sync \nCannot uninstall requirement adium-theme-ubuntu, not installed. ",
    "Lucas-C": "Closing this: this has nothing to do with pip-tools, it's a setup.py / pip install limitation.\nFor those interested, I opened another issue here : https://github.com/Julian/jsonschema/issues/276\n. Note: it works fine if you specify an explicit version of setuptools:\necho setuptools==28.2.0 > requirements.txt\n. Looks ok with the latest pip-tools version. There is one remaining bug to fix that raises a AttributeError: 'Requirement' object has no attribute 'name':\nhttps://github.com/jazzband/pip-tools/pull/540. I'm totally willing to add a test, but I don't understand what you want to test here ?\nMy change is in OutputWriter._iter_lines while you asked me to add a test for OutputWriter._format_requirement which is not affected by my changes (marker=None is going to be passed as an argument).\nAlso this aims to fix a compatibility issue with pip 8.1.1 or below, so maybe the best way to test it would be to add a pip==8.1.1 testenv in https://github.com/jazzband/pip-tools/blob/master/tox.ini#L7 ?. I added a test tox env,\nbut I'm not 100% sure I got it right :). No problem: done !. Thanks for your help :). Thanks for your help.\nI'll try that tomorrow. First off, I resinstalled pip-tools: pip install pip-tools==1.9.0\nThen I did the them steps and got the following error:\n```\n$ python -V\nPython 3.4.3\n$ pip --version\npip 8.1.1\n$ pip-compile --version\npip-compile, version 1.9.0\n$ echo setuptools==18.2 > requirements.txt\n$ pip-compile requirements.txt\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.txt\n\nThe following packages are considered to be unsafe in a requirements file:\nFile \"/home/lucas_cimon/.local/share/virtualenvs/nexus_uploader/lib/python3.4/site-packages/piptools/writer.py\", line 107, in _iter_lines\n    marker=markers.get(ireq.req.name),\nAttributeError: 'Requirement' object has no attribute 'name'\n```\nThen I realized I previously applied this patch : https://github.com/jazzband/pip-tools/pull/540\nI applied it, and this time got no issue :P\nMaybe the issue was with setuptools and its version changed somehow during my tests ??\npython -c 'import setuptools; print(setuptools.version.__version__)'\n35.0.1\nAnyway, I guess once #540 is merged this ticket can be closed.. Yes, I could not reproduce the original issue, this can be close\nThanks. ",
    "omarkohl": "That's very nice :-) It works for me (at least with 'install_requires' in setup.py).\nThis ticket could probably the closed.\n. To me it is unclear when it is converted to upper and when to lower case but it is definitely not consistent and changes every few runs. This is quite annoying because the diffs in version control get a lot bigger than necessary.\nI'm fine with either lower or upper case but please consistently :-)\nAs a workaround I'm considering forcing the whole file to be lower case (including comments etc.) like this:\ntr '[:upper:]' '[:lower:]' < requirements.txt > temp.txt && mv temp.txt requirements.txt. It seems to be fairly consistent here:\n\npip-compile --upgrade -> Some package names end up in upper case (such as SQLAlchemy)\npip-compile -> Package names end up lower case (e.g. sqlalchemy)\n\nThe --rebuild option makes no difference in either case.\nUsing pip-tools 1.8.0 and pip 9.0.1. lowercase is fine with me. ",
    "majuscule": "This feature would be very useful! Regarding the workaround (-e . in requirements.in), it almost works, but then incorrectly injects a reference to the cwd in the output, i.e.: -e file:///~/src/my/project.. Thanks @ryanhiebert, I appreciate you pasting that here. Hopefully I'll have shortened some of that for us if PR https://github.com/jazzband/pip-tools/pull/468 is merged, but the fact that such workarounds are being created should be another testament to the validity of this issue and PR https://github.com/jazzband/pip-tools/pull/418.. @rpkilby in short:\n\nsetup.py hopes to be friendly and optimistic to your users, accepting patch upgrades from dependencies and trying to avoid conflicts with other packages that may be installed.\nrequirements.txt is a guaranteed working set of packages and versions.\n\nThis abstract/concrete dependency model is the use case that this PR would resolve, and is also discussed in the related issue https://github.com/jazzband/pip-tools/issues/331.\nI would very much like to use pip-tools, but this is a blocking issue for me as the suggested workaround (mentioned in that same issue) no longer functions.\n@davidovich would you reconsider merging the PR if the functionality is only enabled explicitly through a --setuptools flag or similar?. I have implemented a basic test for this feature as requested (https://github.com/jazzband/pip-tools/pull/481). Please let me know if there are any other cases that you would like covered. \n@tysonclugg please let me know if you would prefer to do this yourself, I'm just excited to see your work merged; my branch is built directly on yours.. Happy to help! If you have time, I would appreciate your thoughts on my comment to you here: https://github.com/jazzband/pip-tools/pull/418#issuecomment-285823330.. Awesome! Thanks from me also @mattlong, started working on adding the tests myself and was getting very confused.. Rebased and it does look like tests on Windows are failing. I'm afraid I don't quite follow what the error is here. In fact the only tempfile.NamedTemporaryFile() instance I see does use the delete=False parameter:\ntests/test_cache.py\n4:from tempfile import NamedTemporaryFile\n22:        cache_file = NamedTemporaryFile(mode=\"w\", delete=False)\nRegardless that doesn't seem related to this error:\n<Result InstallationError(\"Could not open requirements file: [Errno 13] Permis...ed: 'c:\\\\\\\\users\\\\\\\\appveyor\\\\\\\\appdata\\\\\\\\local\\\\\\\\temp\\\\\\\\1\\\\\\\\tmpg362ei'\",)>\nI do see that in the pip_conf fixture, it explicitly prepends a tmpdir to the written file, however my code takes place within the isolated_filesystem, and needs to be relative to the cli invocation.\nDo you have any further ideas as to what is going wrong?. I don't quite follow why that would be causing the error, but I appreciate your pointing it out. I've made the change and also removed it's use as a context manager which imo does imply it's auto-deletion. Might even be able to keep delete=True this way? I'm afraid I can't reproduce the error locally here without a windows machine to test on so will have to wait for AppVeyor.. Looks like it's all passing now. Please let me know if there's anything else I can do for this ticket or any of the others on the 1.9 milestone to get it released!. This was caused by a typo in the setup.py file that I should have caught. Not a great error message, but probably not the fault of pip-tools anyway. My bad.. whoops, thanks!. That's fine by me. I have pushed an update with the collections implementation as per your link.. ",
    "jaraco": "I don't believe this issue is fully fixed. The title mentions both install_requires and extras_require, but #418 only addressed install requires. If one wants extras, one still needs to use the hacky workaround of a requirements.in with -e .[extra1,extra2] and still ends up with the local working directory added to requirements.txt. I see #625 was opened to address that issue.. ```\ndraft $ pip-compile --output-file requirements.txt requirements.in -vv\nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  flake8\nFinding the best candidates:\n  found candidate flake8==3.5.0 (constraint was )\nFinding secondary dependencies:\n  flake8==3.5.0             requires mccabe<0.7.0,>=0.6.0, pycodestyle<2.5.0,>=2.4.0, pyflakes<1.7.0,>=1.5.0\nNew dependencies found in this round:\n  adding ['mccabe', '<0.7.0,>=0.6.0', '[]']\n  adding ['pycodestyle', '<2.5.0,>=2.4.0', '[]']\n  adding ['pyflakes', '<1.7.0,>=1.5.0', '[]']\nRemoved dependencies in this round:\nUnsafe dependencies in this round:\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  flake8\n  mccabe<0.7.0,>=0.6.0\n  pycodestyle<2.5.0,>=2.4.0\n  pyflakes<1.7.0,>=1.5.0\nFinding the best candidates:\n  found candidate flake8==3.5.0 (constraint was )\n  found candidate mccabe==0.6.1 (constraint was >=0.6.0,<0.7.0)\n  found candidate pycodestyle==2.4.0 (constraint was >=2.4.0,<2.5.0)\n  found candidate pyflakes==1.6.0 (constraint was >=1.5.0,<1.7.0)\nFinding secondary dependencies:\n  pycodestyle==2.4.0        requires -\n  flake8==3.5.0             requires mccabe<0.7.0,>=0.6.0, pycodestyle<2.5.0,>=2.4.0, pyflakes<1.7.0,>=1.5.0\n  pyflakes==1.6.0           requires -\n  mccabe==0.6.1             requires -\n\nResult of round 2: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nflake8==3.5.0\nmccabe==0.6.1             # via flake8\npycodestyle==2.4.0        # via flake8\npyflakes==1.6.0           # via flake8\n```. Other members on my team can't replicate this issue. It seems to be unique to my environment.\nI suspect there's something cached on my system.. I didn't have a ~/.cache/pip-tools, but I did find the dir:\n```\n\n\n\nimport piptools.locations as loc\nloc.CACHE_DIR\n'/Users/jaraco/Library/Caches/pip-tools'\n```\n\n\n\nAnd indeed that location has the newer deps cached:\n$ cat ~/Library/Caches/pip-tools/depcache-py3.7.json | jq '.dependencies.flake8'\n{\n  \"3.5.0\": [\n    \"mccabe<0.7.0,>=0.6.0\",\n    \"pycodestyle<2.5.0,>=2.4.0\",\n    \"pyflakes<1.7.0,>=1.5.0\"\n  ]\n}\nYet still pip-compile is getting the incompatible version:\n```\ndraft $ rm -rf ~/Library/Caches/pip-compile/*\ndraft $ python -m venv clean\ndraft $ clean/bin/pip install pip-tools\nCollecting pip-tools\n  Using cached https://files.pythonhosted.org/packages/4c/69/4f33fea6c50ff873b7905abc404d8c26aab2accf8c099d67a35aebc09871/pip_tools-2.0.2-py2.py3-none-any.whl\nCollecting six (from pip-tools)\n  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools)\n  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl\nCollecting first (from pip-tools)\n  Using cached https://files.pythonhosted.org/packages/71/12/1b24a85f543658804027de982f7e5cb80543ea33ec396c887c099c75274c/first-2.0.1-py2.py3-none-any.whl\nInstalling collected packages: six, click, first, pip-tools\nSuccessfully installed click-6.7 first-2.0.1 pip-tools-2.0.2 six-1.11.0\nYou are using pip version 10.0.1, however version 18.0 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\ndraft $ clean/bin/pip-compile --output-file requirements.txt requirements.in -vv\nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  flake8\nFinding the best candidates:\n  found candidate flake8==3.5.0 (constraint was )\nFinding secondary dependencies:\n  flake8==3.5.0             requires mccabe<0.7.0,>=0.6.0, pycodestyle<2.5.0,>=2.4.0, pyflakes<1.7.0,>=1.5.0\nNew dependencies found in this round:\n  adding ['mccabe', '<0.7.0,>=0.6.0', '[]']\n  adding ['pycodestyle', '<2.5.0,>=2.4.0', '[]']\n  adding ['pyflakes', '<1.7.0,>=1.5.0', '[]']\nRemoved dependencies in this round:\nUnsafe dependencies in this round:\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  flake8\n  mccabe<0.7.0,>=0.6.0\n  pycodestyle<2.5.0,>=2.4.0\n  pyflakes<1.7.0,>=1.5.0\nFinding the best candidates:\n  found candidate flake8==3.5.0 (constraint was )\n  found candidate mccabe==0.6.1 (constraint was >=0.6.0,<0.7.0)\n  found candidate pycodestyle==2.4.0 (constraint was >=2.4.0,<2.5.0)\n  found candidate pyflakes==1.6.0 (constraint was >=1.5.0,<1.7.0)\nFinding secondary dependencies:\n  flake8==3.5.0             requires mccabe<0.7.0,>=0.6.0, pycodestyle<2.5.0,>=2.4.0, pyflakes<1.7.0,>=1.5.0\n  mccabe==0.6.1             requires -\n  pyflakes==1.6.0           requires -\n  pycodestyle==2.4.0        requires -\n\nResult of round 2: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nflake8==3.5.0\nmccabe==0.6.1             # via flake8\npycodestyle==2.4.0        # via flake8\npyflakes==1.6.0           # via flake8\n```\nI'll see if I can break into pip-tools to learn more.. oh, duh. I still had ./requirements.txt with the wrong value also (from prior tests). After flushing the cache and removing the previous compiled file, I now get the expected results:\n```\ndraft $ clean/bin/pip-compile --output-file requirements.txt requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nflake8==3.5.0\nmccabe==0.6.1             # via flake8\npycodestyle==2.3.1        # via flake8\npyflakes==1.6.0           # via flake8\n```\n\nYou seem to cat a different folder than the one you are removing\n\nAah, good catch also. That was affecting my analysis.\nSo I've determined what caused the cache to get the wrong value. In my system site-packages, I have an untagged post release of 3.5.0, so when I run pip compile from my main Python environment, I get dependencies resolved as part of that environment (and cached):\n```\ndraft $ rm -rf ~/Library/Caches/pip-tools/* requirements.txt\ndraft $ pip-compile --output-file requirements.txt requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nflake8==3.5.0\nmccabe==0.6.1             # via flake8\npycodestyle==2.4.0        # via flake8\npyflakes==1.6.0           # via flake8\n```\nBut if I clear the cache and run the pip-compile from a clean virtualenv, I then see the correct values:\n```\ndraft $ rm -rf ~/Library/Caches/pip-tools/*\ndraft $ clean/bin/pip-compile --output-file requirements.txt requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nflake8==3.5.0\nmccabe==0.6.1             # via flake8\npycodestyle==2.3.1        # via flake8\npyflakes==1.6.0           # via flake8\n```\nThis whole issue seems to reveal a real flaw in the tool. Should it be the case that because one developer (or ci runner) in one machine once compiled and got a different set of requirements, perhaps as part of a test, that it would bleed to other projects in that machine?\nI guess part of the issue here is that flake8 itself doesn't accurately report its version between tagged versions (so there are potentially many versions of flake8-3.5.0) and makes infrequent releases (increasing likelihood someone might have installed an interim commit).\nI guess the lesson is that pip-compile is really sensitive as to the environment in which it's run... and should probably be run on a pristine machine every time to avoid unwanted artifacts leaking into the compiled result.\nWould hashes have helped here?  Should there be a command-line option to bypass the cache?. > Can I ask how you installed that untagged post release of 3.5.0 of flake8 exactly \nI don't recall, but since I don't have a local checkout of flake8, I almost certainly did pip install git+https://github.com/PyCQA/flake8 possibly but unlikely @somerev.\n\nYou mean that pip-compile will use the package installed in the current environment as part of the \"source\" it can check?\n\nYes, seems that way... Since installing from a clean virtualenv doesn't have that behavior, I think we must conclude it's the installed flake8 prerelease.. At the point where setup.py is executed, __name__ == 'builtins'.. Change setup.py to __name__ in ['__main__', 'builtins'] and __import__('setuptools').setup() works around the issue (although now that I think about it, probably only for Python 3).. To support Python 2 invocation of pip-compile, you need __name in ['__main__', 'builtins', '__builtin__'].. ",
    "esmail": "Great point. Thanks @rfleschenberg.. ",
    "jeffwidman": "FYI--latest version of ipython removed the gnureadline dependency in favor of the cross-platform prompt-toolkit.\nThe feature request is still valid in the abstract though...\n. ",
    "decentral1se": "@nvie @ariscn \nI've started working on this but have questions.\nShould this exclusion happen only for packages described in a requirements.in or should also exclude dependencies?\nIf we start to exclude dependencies, then we might break some packages. The user might not know at which level some package they want to exclude will be excluded for some other package.\n. @Groxx could you check out the branch (add-exclude-flag) of https://github.com/lwm/pip-tools/commit/f5771a3aa855887f22dad2ba4a58eef1f158f34f and give it a check if it works for you? I am afraid #388 is going to rot without review or progress.\n. @milin I am not sure (new to the code here). Try it and let's find out? Not sure what you mean by a 'backup version'? How would the interface look like?\n. Closing. The code is there if anyone needs a reference later on.\n. ",
    "milin": "@lwm Would this fix issues like this https://github.com/nvie/pip-tools/issues/366? I was thinking along with exclude,  one could also make pip-compile flexible, so if issues like that happen, we could fall back to a backup version.\n. @lwm I ended up fixing it since I needed it for a project. https://github.com/nvie/pip-tools/pull/394 as more info on the problem and the proposed solution.\n. I meant should download package A and package B. I was hoping in such a case pip-compile would ask the user which version to chose. \n. Fair enough. I guess I'd have to contact the the third party owners to update their pinned versions or atleast use >=1.4 in this case.\n. @nvie @zeebonk I am proposing the following solution for this problem. https://github.com/nvie/pip-tools/pull/394. Please take a look and let me know what you think.\n. Can confirm.\nautopep8==1.1.1\nbabel==2.3.4\nbackports-abc==0.4        # via tornado\nbackports.ssl-match-hostname==3.4.0.2\nbackports.ssl-match-hostname==3.5.0.1\nbeautifulsoup4==4.4.1\nbilliard==3.3.0.23        # via celery\nbleach==1.4.2\n. It fails during the process so the compiled file is not generated at all. Therefore manually editing the compiled file is not an option. I think if the user knows which version of a particular package pip compile should pin, we should let them have fall back versions, if in case pip-compile fails to do a resolution.\n. ",
    "youcandanch": "Replicated with latest (1.6.5), for what it's worth.\n. ",
    "triplepoint": "A near as I can tell, --trusted-host is no longer a supported feature inside a requirements.txt file.  It's still allowed as a command line parameter and a pip.conf value.\nThis seems to allude to the change, but I couldn't find direct documentation of it being changed in pip:\nhttps://github.com/pypa/pip/issues/2822\n. ",
    "LuisAlejandro": "I see this error repeating itself lately, its also reported here #279 and #228. Do you have any info on how to resolve this? @nvie @flavianh @veegee\nThanks in advance.\n. Well, for more info, if one changes this line  to show_stdout=True on pip, then you can see the output of the run_egg_info method. For example, for psycopg2:\n```\npip-compile -r -v x.txt \nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  psycopg2\nFinding the best candidates:\n  found candidate psycopg2==2.6.1 (constraint was )\nFinding secondary dependencies:\n  psycopg2==2.6.1 not in cache, need to check index\n['/usr/bin/python', '-c', \"import setuptools, tokenize;file='/tmp/tmpOeliZVbuild/psycopg2/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\", 'egg_info', '--egg-base', 'pip-egg-info']\nrunning egg_info\ncreating pip-egg-info/psycopg2.egg-info\nwriting pip-egg-info/psycopg2.egg-info/PKG-INFO\nwriting top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt\nwriting dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt\nwriting manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'\nwarning: manifest_maker: standard file '-c' not found\nError: You need to install postgresql-server-dev-X.Y for building a server-side extension or libpq-dev for building a client-side application.\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/scripts/compile.py\", line 163, in cli\n    results = resolver.resolve()\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 78, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 160, in _resolve_one_round\n    for best_match in best_matches\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 161, in \n    for dep in self._iter_dependencies(best_match))\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 230, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/repositories/local.py\", line 48, in get_dependencies\n    return self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/repositories/pypi.py\", line 135, in get_dependencies\n    dependencies = reqset._prepare_file(self.finder, ireq)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py\", line 591, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_set.py\", line 127, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req/req_install.py\", line 431, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"/usr/local/lib/python2.7/dist-packages/pip/utils/init.py\", line 718, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmpOeliZVbuild/psycopg2/\n```\nWhich seems right as i don't have the build dependencies satisfied. But is ilogical that i have to met dependencies on an information command as egg_info. I will go on with my research and report here.\n. Well, i've tracked this particular problem to this ticket psycopg/psycopg2#128, which is not a pip-tools problem.\nHowever, there are several other packages that have a bad written setup.py that causes an informative command as egg_info to fail if the build dependencies are not met or if the egg_info fails because of another unknown reason.\nGenerally it seems to me that pip-tools should find another way around to figure dependencies out. Because this discovery means that every package that pip-tools processes, needs to have its build dependencies satisfied.\n@nvie what do you think?\n. More on this pypa/pip#661\n. Yes, i mean pip-tools should die gracefully here. Although, personally i would like it to have an option to use the most recent version if there's a conflict like this, what do you think? I'm willing to make a PR with such a feature.\n$ pip --version\npip 8.1.1 from /usr/lib/python2.7/dist-packages (python 2.7)\n$ pip-compile --version\npip-compile, version 1.6.1\n. Nevermind, this is a bug related to Debian.\nhttps://bugs.debian.org/cgi-bin/bugreport.cgi?bug=814397\nAfter reinstalling pip from pypi, this is the output, which seems like a good one.\n```\n$ pip-compile -r -v text.txt \nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  reportlab==2.6,==3.1.44\nFinding the best candidates:\nCould not find a version that matches reportlab==2.6,==3.1.44\nTried: 2.3, 2.3, 2.4, 2.5, 2.6, 2.7, 3.0, 3.1.8, 3.1.44, 3.2.0, 3.3.0\n```\nThanks anyway.\n. ",
    "pablote": "This just started happening to me, is there any workaround for it? This is my requirements.in:\nassertpy\nflask\ngunicorn\njoblib\nkeras\nmatplotlib\nnumpy\npandas\nscikit-learn\nscipy\npillow\nscikit-image\ntensorflow. still not working without matplotlib. I recreated the virtualenv and now is working fine, thanks :). ",
    "asmaps": "Same here. When I comment out matplotlib it works.\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 101, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 198, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 284, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/repositories/local.py\", line 62, in get_dependencies\n    return self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/repositories/pypi.py\", line 170, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 634, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 129, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 439, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"/usr/local/lib/python3.6/site-packages/pip/utils/__init__.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmp_888bvtbbuild/subprocess32/. ",
    "kronion": "I was still seeing this issue with pip-tools 1.11.0 when trying to compile a bare-bones requirements.in containing only the word Django.\nThe issue was that the latest version of Django supports only Python 3.x, but my virtualenv was for Python 2.7. The solution was to pin Django in requirements.in: Django==1.11.\nI don't know if pip-tools can do anything to clarify the cause of the error (it already shows the same error message that you'll see if you pip install Django manually), but people should know to watch out for this.. ",
    "jimbocoder": "Also running into this one. @rrauenza did you come up with any nice workarounds when you encountered this issue?. ",
    "rrauenza": "wow -- it's been so long, I don't recall if or how I worked around this. Sorry :(. ",
    "awaisdar001": "Upgrading my pip version helped me out!!!. ",
    "beedub": "do you mean that there should be a better error msg? though it's not directly saying there's a duplicate constraint, the issue is obvious from the error\n. I'm actually unable to reproduce the stacktrace part locally using latest pip (8.1.1) and latest pip-tools. @LuisAlejandro what's versions are you using?\n. ",
    "tomoh1r": "It's not fix it.\nnot move files.\nsorry for a buggy p-r.\n. ",
    "tadams42": "Definitely platform specific, it worked on earlier Ubuntu and Python 3.4. I'll try to find time later, to dig deeper into this one\n. Tried like this (incorporating pull request #350):\npyvenv .venv\nsource .venv/bin/activate\npip install -U pip\npip install -e git://github.com/taynaud/pip-tools.git@ca3ef2e74cd2058fca903a67656e739e93531023#egg=pip-tools\necho \"flask\" >> requirements.in\npip-compile requirements.in\npip-sync requirements.txt\npip-compile now passes, pip-sync doesn't:\n```\npip-sync requirements.txt\nUninstalling pkg-resources-0.0.0:\n  Successfully uninstalled pkg-resources-0.0.0\nCollecting Jinja2==2.8\n  Using cached Jinja2-2.8-py2.py3-none-any.whl\nCollecting MarkupSafe==0.23\nCollecting Werkzeug==0.11.9\n  Using cached Werkzeug-0.11.9-py2.py3-none-any.whl\nCollecting flask==0.10.1\n  Using cached Flask-0.10.1.tar.gz\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/tmp/.venv/lib/python3.5/site-packages/setuptools/init.py\", line 11, in \n        from setuptools.extern.six.moves import filterfalse, map\n      File \"/tmp/.venv/lib/python3.5/site-packages/setuptools/extern/init.py\", line 1, in \n        from pkg_resources.extern import VendorImporter\n    ImportError: No module named 'pkg_resources'\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-i3gandlh/flask/\nTraceback (most recent call last):\n  File \"/tmp/.venv/bin/pip-sync\", line 9, in \n    load_entry_point('pip-tools', 'console_scripts', 'pip-sync')()\n  File \"/tmp/.venv/lib/python3.5/site-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/tmp/.venv/lib/python3.5/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/tmp/.venv/lib/python3.5/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/tmp/.venv/lib/python3.5/site-packages/click/core.py\", line 534, in invoke\n    return callback(args, **kwargs)\n  File \"/tmp/.venv/src/pip-tools/piptools/scripts/sync.py\", line 75, in cli\n    install_flags=install_flags))\n  File \"/tmp/.venv/src/pip-tools/piptools/sync.py\", line 156, in sync\n    check_call([pip, 'install'] + pip_flags + install_flags + sorted(to_install))\n  File \"/usr/lib/python3.5/subprocess.py\", line 581, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['pip', 'install', 'Jinja2==2.8', 'MarkupSafe==0.23', 'Werkzeug==0.11.9', 'flask==0.10.1', 'itsdangerous==0.24']' returned non-zero exit status\n```\n. After I do the following pip-sync passes:\npip uninstall setuptools \npip install -U setuptools \npip-sync requirements.txt\nWhy does first pip-sync do the following?\npip-sync requirements.txt\nUninstalling pkg-resources-0.0.0:\n  Successfully uninstalled pkg-resources-0.0.0\n. Latest try on Python 3.5.2, Ubuntu 16.04.1 LTS, pip==9.0.1, pip-tools==1.8.0\n\npip-compile passes without problems and without any workaround required\npip-sync fails:\n\n~~~\npip-sync requirements.txt\nUninstalling pkg-resources-0.0.0:\n  Successfully uninstalled pkg-resources-0.0.0\nCollecting Jinja2==2.9.4\n  Using cached Jinja2-2.9.4-py2.py3-none-any.whl\nCollecting MarkupSafe==0.23\n  Using cached MarkupSafe-0.23.tar.gz\nCould not import setuptools which is required to install from a source distribution.\nTraceback (most recent call last):\n  File \"/tmp/.venv/lib/python3.5/site-packages/pip/req/req_install.py\", line 387, in setup_py\n    import setuptools  # noqa\n  File \"/tmp/.venv/lib/python3.5/site-packages/setuptools/init.py\", line 11, in \n    from setuptools.extern.six.moves import filterfalse, map\n  File \"/tmp/.venv/lib/python3.5/site-packages/setuptools/extern/init.py\", line 1, in \n    from pkg_resources.extern import VendorImporter\nImportError: No module named 'pkg_resources'\n~~~\n@nvie, I'd say the remaining problem in this issue is equal to one described in #422 so \n\nclose one of them as duplicate\napply solution proposed by @sfriesel to close both, this one and #422 \n. moved comment to #349 \n. \n",
    "taynaud": "It is not specific to python 3.5.1.\nIn ubuntu 16.04:\n```\n[~]$ mkdir test                                                                                                            \n[~]$ cd test                                                                                                               \n[~/test]$ echo \"wheel\" > requirements.in                                                                                   \n[~/test]$ pip-compile requirements.in                                                                                      \nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in \n    sys.exit(cli())\n  File \"/usr/lib/python2.7/dist-packages/click/core.py\", line 716, in call\n    return self.main(args, kwargs)\n  File \"/usr/lib/python2.7/dist-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python2.7/dist-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/lib/python2.7/dist-packages/click/core.py\", line 534, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/scripts/compile.py\", line 163, in cli\n    results = resolver.resolve()\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 78, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 154, in _resolve_one_round\n    best_matches = set(self.get_best_match(ireq) for ireq in constraints)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 154, in \n    best_matches = set(self.get_best_match(ireq) for ireq in constraints)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/resolver.py\", line 201, in get_best_match\n    best_match = self.repository.find_best_match(ireq, prereleases=self.prereleases)\n  File \"/usr/local/lib/python2.7/dist-packages/piptools/repositories/pypi.py\", line 103, in find_best_match\n    prereleases=prereleases)\n  File \"/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/_vendor/packaging/specifiers.py\", line 753, in filter\n  File \"/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/_vendor/packaging/version.py\", line 31, in parse\n  File \"/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/_vendor/packaging/version.py\", line 200, in init\nTypeError: expected string or buffer\n```\n. ",
    "juyrjola": "I'm encountering this problem as well with Ubuntu 16.04 and Python 3.5. Apparently specifier.filter expects to have version as class pkg_resources.extern.packaging.version.Version, but it is provided with class pip._vendor.packaging.version.Version. filter then assumes the passed item is a string, and attempts to parse it.\n. Making sure we pass strings to specifier.filter fixes the problem, but I'm not sure if it's the proper fix:\n``` diff\ndiff --git a/piptools/repositories/pypi.py b/piptools/repositories/pypi.py\nindex 79c8674..0a3849c 100644\n--- a/piptools/repositories/pypi.py\n+++ b/piptools/repositories/pypi.py\n@@ -95,8 +95,8 @@ class PyPIRepository(BaseRepository):\n             return ireq  # return itself as the best match\n     all_candidates = self.find_all_candidates(ireq.name)\n\n\ncandidates_by_version = lookup_table(all_candidates, key=lambda c: c.version, unique=True)\nmatching_versions = ireq.specifier.filter((candidate.version for candidate in all_candidates),\ncandidates_by_version = lookup_table(all_candidates, key=lambda c: str(c.version), unique=True)\nmatching_versions = ireq.specifier.filter((str(candidate.version) for candidate in all_candidates),\n                                                   prereleases=prereleases) # Reuses pip's internal candidate sort key to sort\n\n```\n. \n\n",
    "mkoistinen": "I'm on Mac OS X 10.11.4 with latest command line tools (v7.3) and this exact thing happens to me as well, but only under Python 3.5. @nvie which version of the command line tools are you running?\n. lol. OK. Thanks for the explanation.. ",
    "sfriesel": "The second part @tadamic mentioned (Uninstalling pkg-resources-0.0.0) could be fixed by adding \"pkg-resources\" to PACKAGES_TO_IGNORE in sync.py because it should probably not be touched in any case. I don't know if that would also fix the original error as well (I didn't encounter it)\n. ",
    "siulkilulki": "For me the same. pip-sync uninstalls pkg-resources-0.0.0. ",
    "floqqi": "Yes, I'm using the latest version available on PyPi (on Windows 7).\n. @nvie I'm encountering some odd behavior using pip-compile now.\npip-compile.exe requirements.in --output-file=requirements.txt doesn't override requirements.txt. A file named requirements.txt.bak is now created.\n. My user is allowed to override requirements.txt. This happens even when running cmd as Administrator. No error happens, just the regular output.\n. No error is raised. Just regular output as if anything is okay:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\namqp==1.4.9               # via kombu\naniso8601==1.1.0          # via flask-restful\nanyjson==0.3.3            # via kombu\ncached-property==1.3.0\ncx-oracle==5.2.1\ndicttoxml==1.6.6\neventlet==0.18.4          # via nameko\nflask-marshmallow==0.6.2\nflask-restful==0.3.5\nflask-script==2.0.5\nflask-sqlalchemy==2.1\nflask==0.10.1\ngreenlet==0.4.9           # via eventlet\nitsdangerous==0.24        # via flask\nJinja2==2.8               # via flask\nkombu==3.0.35             # via nameko\nldap3==1.2.2\nMarkupSafe==0.23          # via jinja2\nmarshmallow-sqlalchemy==0.8.1\nmarshmallow==2.7.2\nmock==2.0.0               # via nameko\nnameko==2.3.0\npath.py==8.2.1            # via nameko\npbr==1.9.1                # via mock\npyasn1==0.1.9             # via ldap3\npymysql==0.7.2\npyodbc==3.0.10\npython-dateutil==2.5.3    # via aniso8601\npytz==2016.4              # via flask-restful\npyyaml==3.11              # via nameko\nrequests==2.9.1           # via nameko\nsix==1.10.0               # via flask-marshmallow, flask-restful, mock, nameko, python-dateutil\nsqlalchemy==1.0.12\nwebargs==1.3.2\nwerkzeug==0.11.9          # via flask, nameko\n```\nrequirements.txt.bak has this ^ content.\n. Okay, just tested it with pinning a version in my requirements.in-file. requirements.txt gets overriden, but requirements.txt.bak won't be deleted.\nThought it doesn't override something, but there was nothing to override, sorry.\n. @nvie: Now it works as expected, thank you!\n. ",
    "simon-weber": "I'm interested in taking a stab at this.\n@blueyed is it correct that your original changes don't require https://github.com/pypa/pip/pull/2913 if vcs requirements.in are of the form git+https or git+ssh? I'll be more hesitant if this would also involve pushing things through pip \ud83d\ude01 \n. Ok, both compile and sync seem solid now with our gnarly set of prod dependencies. I've also rebased onto master and fixed an unrelated linting error.\nReady for a look, @nvie.\n. (github hid the unresolved discussion about is_pinned_requirement; here's the link: https://github.com/nvie/pip-tools/pull/372#discussion_r70166343)\n. @nvie got a chance to look at this? For what it's worth, we've been running off my pr fork in prod without problems.\n. @nvie bump! \ud83d\ude01 \n. @nvie bump again! \ud83d\ude01 We're looking into contributing more, but it's upstream of this.\n. > Sorry about the long delay here.\nNo problem! Totally understand.\n\nWhen I change the 10.0.2rc1 to 0.1 here locally and recompile, I get the exact same output...\n\nOh, whoops, that was a bad example. I should have used the release tag (@10.0.2rc1) instead. I wouldn't recommend using a non-static ref for the reasons you laid out (ie, it may change and the version will then not match).\n\nI can even change it to foobar and it still works. So it doesn't seem to do anything?\n\nIt doesn't do anything at pip-tool's level, but it does for pip: since pip doesn't know the version of the package that's checked in, it will believe the version you give it via the spec. For example, try these examples in order:\n- create new venv\n- pip install git+https://github.com/kennethreitz/requests.git@v2.11.1#egg=requests==2.11.1 -> installs requests as expected\n- pip install git+https://github.com/kennethreitz/requests.git@v2.11.1#egg=requests==foobar -> reclones and reinstalls requests, even though it was already up to date with the code from that revision\n- pip install git+https://github.com/kennethreitz/requests.git@v2.11.1#egg=requests==2.11.1 -> no clone; requirement already satisfied\n- pip install git+https://github.com/kennethreitz/requests.git@foobar#egg=requests==2.11.1 -> no clone; requirement already satisfied\n- pip install git+https://github.com/kennethreitz/requests.git@v2.9.2#egg=requests -> no clone; requirement already satisfied\n- pip install git+https://example.com/foo.git@v2.11.1#egg=requests -> no clone; requirement already satisfied\nSo, pip treats the provided egg==version pair as truth of what it will find if it were to clone url@ref (which seems reasonable, if not well documented; it took some time to figure this behavior out). That's my reasoning behind requiring a pinned version: it pushes people towards a url pip will behave well with.\nWe use this format (static ref, pinned version) at Venmo pretty extensively for two different use cases:\n- internal packages installed from GitHub Enterprise\n- forks and/or contributed changes waiting on merges (eg git+https://github.com/venmo/pip-tools.git@1.7.1a1#egg=pip-tools==1.7.1a1)\n\nHere's what I had in mind for the feature.\n\nI really like this behavior. But, it does seem like a good amount of work to implement, and not something I'm likely to have time for. Would you consider merging the behavior in this PR as a stop-gap so vcs urls are at least functional? Maybe with added docs so people know to only use a static ref, and why the version is required?\n. I'm interested in this too. I've got two guesses:\n upgrading pip/etc at the same time as upgrading other packages might cause issues (though I haven't actually tested this)\n this has something to do with pip usually being globally installed by a system package manager, and is a band-aid for the conflict described in https://github.com/jazzband/pip-tools/issues/328\nAm I on the right track?. Sure thing.\n. Ah, yeah, this is something I wanted to run by you. The motivation is to restrict the input so pip-compile can take it at face value. In particular, supporting unpinned vcs links seems tough since:\n- pip doesn't seem to expose the \"true\" version of a vcs link unless it's been cloned and installed (it just seems to assume the provided version is accurate for the @ref in the link)\n- internal logic assumes that constraints are pinned later on (ie, removing _check_constraints raises a \"expected pinned or editable...\" TypeError later on)\nThoughts?\n. ",
    "nhanb": "Same issue with pip-compile:\n$ pip-compile --output-file requirements.txt requirements.in\nTraceback (most recent call last):\n  File \"/home/vagrant/virtualenvs/myapp/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/scripts/compile.py\", line 164, in cli\n    results = resolver.resolve()\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/resolver.py\", line 78, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/resolver.py\", line 147, in _resolve_one_round\n    constraints = sorted(self.constraints, key=_dep_key)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/resolver.py\", line 49, in constraints\n    self.their_constraints)))\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/resolver.py\", line 118, in _group_constraints\n    for _, ireqs in full_groupby(constraints, key=_dep_key):\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/utils.py\", line 97, in full_groupby\n    return groupby(sorted(iterable, key=key), key=key)\n  File \"/home/vagrant/virtualenvs/myapp/lib/python3.4/site-packages/piptools/resolver.py\", line 27, in _dep_key\n    return ireq.req.key\nAttributeError: 'Requirement' object has no attribute 'key'\nI'm using:\n- pip-tools 1.6.4\n- pip 8.1.2\n- python 3.4.2\n. ",
    "sivabudh": "+1\n. ",
    "voidus": "in pypa/pip#3573, it is suggested to use pypa/packaging directly, as pip is not meant to be used programmatically. Maybe that would be an option?\nhttps://github.com/pypa/pip/issues/3673#issuecomment-218539154\n. ",
    "gjcooper": "I've come across this before, and downgrading pip has fixed it for me, but just updated to the latest version of Ubuntu and am getting the same error (AttributeError: 'Requirement' object has no attribute 'key') with versions:\nPython 3.5.1+\npip-compile, version 1.6.5\npip 8.1.1\nWithin a pyvenv virtual environment. Possibly related to being on new python release.\n. ",
    "jmbowman": "@nvie Think it's safe to close this now, or giving it some time to hear feedback on the new release?\n. @timmyomahony The fix was released with pip-tools 1.7, so if you're still on 1.6 I wouldn't expect it to work.\n. Ah-ha, sounds like there's room for improvement in test coverage. \ud83d\ude03  At least the changes should be set up so that such oversights won't break any already-working code with the previous versions of pip, they'd just be bugs when trying it against the new version.\n. The condition was needed for pip versions less than 8.1.2, since versions before that got their requirement objects from pkg_resources which has a specs attribute, and ones after that get specifier from objects created by packaging; you can see the switch at https://github.com/pypa/pip/pull/3307.  I'm not sure why you're seeing both attributes at the same time; none of the code in the master branches of pip or packaging seems to set or reference a specs attribute.  Is the source code for your problem case available?  I'm a little worried that dropping the condition may break with pip between 8.0 and 8.1.1 in cases you haven't tested.. Ok, I'd been looking at when specs was removed, not when specifier was added; I hadn't realized there was a sequence of versions where both were present.  You're right in that dropping support for pip < 8, we no longer need this condition.  Note that there's a virtually identical condition in LocalRequirementsRepository that should also be removed: https://github.com/jazzband/pip-tools/blob/master/piptools/repositories/local.py#L14 .. ",
    "pdecat": "Works for me, thanks!\n. ",
    "timmyomahony": "I'm not sure if I'm doing something wrong, but I'm still getting this error with pip==8.1.2 and pip-tools==1.6 (and python==3.5):\nTraceback (most recent call last):\n  File \"/Users/user/Development/Sites/myproject/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/Users/user/Development/Sites/myproject/lib/python3.5/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/user/Development/Sites/myproject/lib/python3.5/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/Users/user/Development/Sites/myproject/lib/python3.5/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/user/Development/Sites/myproject/lib/python3.5/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/user/Development/Sites/myproject/lib/python3.5/site-packages/piptools/scripts/compile.py\", line 128, in cli\n    existing_pins[ireq.req.project_name.lower()] = ireq\nAttributeError: 'Requirement' object has no attribute 'project_name'\nDowngrading pip to 8.1.1 seems to fix it. \n. ",
    "deviousgeek": "I am having the same issue~! \n. ",
    "zeebonk": "What do you mean with \"Should download all requirements\"? You can have only one version of a dependency installed. You have two packages that pin specific different versions, there is no possible way of satisfying this. It is recommended for packages that are used by other packages to use less restrictive versions specifiers. See also: http://python-packaging-user-guide.readthedocs.io/en/latest/distributing/#install-requires\n. If A and B can both work properly with some version of Django, it is clear that the requirements pinned by A and B are too restrictive. But I can imagine that these kind of conflicts aren't that uncommon and sometimes you just don't have the influence to make maintainers of the conflicting packages change the requirements even if you can show that the change in requirements won't break the package. Therefore I think that having pip-compile ask how to resolve this (choose version manually, skip, abort), would be a nice addition.\n. ",
    "arthurio": "@davidovich I'm using 1.8.1rc3 and have the same behavior:\n1. Run pip-compile requirements.in (has --extra-index-url http://my.devpi):\nResult (as expected)\nPuts the --extra-index-url http://my.devpi at the top of the generated requirements.txt file and list all the dependencies with versions.\n\nRun pip-sync requirements.txt:\n\nExpected result\nPackage hosted in http://mydevpi gets installed\nActual result\nCould not find a version that satisfies the requirement mymodule==0.1.0\n(where mymodule is the package hosted in http://mydevpi)\n[Edit]\nAlso, @estan I think this is different from the issues you pointed to, we are not passing --extra-index-url as a parameter to pip-sync, it is part of the requirements.txt file.\nFor example, the following actually works:\npip-sync requirements.txt --extra-index-url http://my.devpi. ",
    "nefarioustim": "I am seeing the same behaviour as @arthurio in 1.9.0. My --extra-index-url is declared inside the requirements.txt and is being ignored when I run pip-sync.\nI do not think this bug is truly closed.. Just as a follow up, I can confirm that if I pass --extra-index-url as a parameter to the call to pip-sync it works as expected. It is only when --extra-index-url is included within the requirements.txt that it is overlooked.. ",
    "tekacs": "Currently having to fix this manually, everywhere - would be great to see this merged!\n. ",
    "amadornimbis": "I know this repository recently changed ownership, so I'm bumping this to see if anything needs to be changed in to get the first commit of this series merged?. ",
    "oxplot": "As a workaround, you can setup the env with no setuptools and install it after activation:\nsh\nvirtualenv --no-setuptools venv\n. venv/bin/activate\npip install setuptools\npip-sync requirements.txt. ",
    "dfee": "I have an idea \u2013\u00a0what if we don't use git checkout-index? The problem is that checkout-index effectively gets rid of the scm data, which setuptools_scm needs.\nWhy are we doing the git checkout-index step?. Perhaps this is a bug in pip itself.\nJust to aid in debugging here's a rich stack trace of how we get to the problem (starting at the last piptools call):\n5 - piptools.repositories.pypi:PyPIRepository.get_dependencies -> reqset._prepare_file\n4 - pip.req.req_set:RequirementSet._prepare_file -> req_to_install.update_editable\n3 - pip.req.req_install:InstallRequirement.update_editable -> vcs_backend.export\n2 - pip.vcs.git:Git.export -> self.run_command\n1 - pip.vcs:VersionControl.run_command -> call_subprocess\n0 - pip.utils:call_subprocess -> subprocess.Popen\nIf we go ahead and stop the execution before the last frame gets called and copy the .git directory into the tmp folder, execution completes normally. Of course, without the SCM folder, setuptools_scm fails.\nand here's the full traceback (for me):\n```\n  /private/tmp/abra/env/bin/pip-compile(11)()\n      9 if name == 'main':\n     10     sys.argv[0] = re.sub(r'(-script.pyw?|.exe)?$', '', sys.argv[0])\n---> 11     sys.exit(cli())\n/private/tmp/abra/env/lib/python3.6/site-packages/click/core.py(722)call()\n    721         \"\"\"Alias for :meth:main.\"\"\"\n--> 722         return self.main(args, *kwargs)\n    723\n/private/tmp/abra/env/lib/python3.6/site-packages/click/core.py(697)main()\n    696                 with self.make_context(prog_name, args, **extra) as ctx:\n--> 697                     rv = self.invoke(ctx)\n    698                     if not standalone_mode:\n/private/tmp/abra/env/lib/python3.6/site-packages/click/core.py(895)invoke()\n    894         if self.callback is not None:\n--> 895             return ctx.invoke(self.callback, **ctx.params)\n    896\n/private/tmp/abra/env/lib/python3.6/site-packages/click/core.py(535)invoke()\n    534             with ctx:\n--> 535                 return callback(args, *kwargs)\n    536\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/scripts/compile.py(184)cli()\n    183                             clear_caches=rebuild, allow_unsafe=allow_unsafe)\n--> 184         results = resolver.resolve(max_rounds=max_rounds)\n    185         if generate_hashes:\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/resolver.py(108)resolve()\n    107             import ipdb; ipdb.set_trace()\n--> 108             has_changed, best_matches = self._resolve_one_round()\n    109             log.debug('-' * 60)\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/resolver.py(196)_resolve_one_round()\n    195         for best_match in best_matches:\n--> 196             for dep in self._iter_dependencies(best_match):\n    197                 import ipdb; ipdb.set_trace()\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/resolver.py(265)_iter_dependencies()\n    264         if ireq.editable:\n--> 265             for dependency in self.repository.get_dependencies(ireq):\n    266                 yield dependency\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/repositories/local.py(62)get_dependencies()\n     61     def get_dependencies(self, ireq):\n---> 62         return self.repository.get_dependencies(ireq)\n     63\n/private/tmp/abra/env/lib/python3.6/site-packages/piptools/repositories/pypi.py(146)get_dependencies()\n    145             import ipdb; ipdb.set_trace()\n--> 146             self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n    147         return set(self._dependencies_cache[ireq])\n/private/tmp/abra/env/lib/python3.6/site-packages/pip/req/req_set.py(517)_prepare_file()\n    516                 req_to_install.ensure_has_source_dir(self.src_dir)\n--> 517                 req_to_install.update_editable(not self.is_download)\n    518                 abstract_dist = make_abstract_dist(req_to_install)\n/private/tmp/abra/env/lib/python3.6/site-packages/pip/req/req_install.py(589)update_editable()\n    588             else:\n--> 589                 vcs_backend.export(self.source_dir)\n    590         else:\n/private/tmp/abra/env/lib/python3.6/site-packages/pip/vcs/git.py(75)export()\n     74                 ['checkout-index', '-a', '-f', '--prefix', location],\n---> 75                 show_stdout=False, cwd=temp_dir)\n     76         finally:\n/private/tmp/abra/env/lib/python3.6/site-packages/pip/vcs/init.py(325)run_command()\n    324                                    command_desc, extra_environ,\n--> 325                                    spinner)\n    326         except OSError as e:\n\n/private/tmp/abra/env/lib/python3.6/site-packages/pip/utils/init.py(666)call_subprocess()\n    664         try:\n--> 665         proc = subprocess.Popen(\n    667             cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,\n. Thanks @blueyed. I saw the tag `pr-wanted` and figured no one had proposed any changes. This could've saved myself a few hours!. @blueyed it seems that PR is scheduled for the next patch release, no?. Ugh. It looks like https://github.com/jazzband/pip-tools/milestones/1.9.1 was deleted. Any way we can get this in https://github.com/jazzband/pip-tools/milestone/3 or https://github.com/jazzband/pip-tools/milestone/2?. Well this is a bummer!. I'm taking a stab at it. Any ideas on a package that uses setuptools_scm that is compatible across the testing environments that pip-tools uses? I.e. a package that is compatible with `envlist = py{27,34,35,36,py}`. Good idea. I've rebased your code, and I'm working to get a test up and running, I'm just unclear on where I should place the test. Should I extend something that already exists? Or, create a test_setuptools_scm.py. Maybe @vphilippon could weigh in?. Alright, check out [538](https://github.com/jazzband/pip-tools/pull/538) where I've updated the CHANGELOG and added a test.. Well, that's a hell of a happy birthday present to me! Thanks @vphilippon!. @jamescooke thanks for posting that article (though it was a while ago). I made one slight modification to it:\nRELATIVE_ROOT=..  # relative path to project's root\n%.txt: %.in\n        pip-compile --output-file $@ $<\n        sed -i '' \"s|-e file://$(realpath $(RELATIVE_ROOT))|-e $(RELATIVE_ROOT)|\" $@\n``\ni.e. this corrects the annoyance-e file:///Users/dfee/code/zebra->-e .`, making the file useful for users who don't develop / deploy from your directory.\n\nI know this isn't the really the place to discuss your Makefile, but I've grown tired of editing requirements.txt files after pip-compileing them. Other folks have too, and there doesn't seem to be a fix on the horizon.. @vphilippon any updates on this front?. Strange as it sounds, is it possible to back this repo out of the jazzband organization, as it's been going on three months now where you've been unable to push code?. @vphilippon I'd like to get this in the next release as it replaces #385. Can we get this done?. @vphilippon updated.. Sounds good.. Sounds good.. ",
    "s0undt3ch": "I'm interested in this support landing into the repo. Been bit by the lack of VCS support recently and the compiled order being wrong because of requirements from the VCS sources not being computed.. I just bumped into an issue where I need the requirements from a git+https dependency in order for the requirements file be generated with the right order. I'd love for this to land in the repo and be released.... ",
    "fuhrysteve": "Thanks @jmbowman !\n. ",
    "GMLudo": "Good catch, I was in the middle of Python 3.5 migration, I'll test that again to validate your hypothesis.\n. Sorry, I have forgotten to close this issue, the problem was gone a long time ago.\nRegards.. Gentle ping :-). Hi,\nI confirm that you fix the bug.\nBTW, I've noticed that for requirements libraries from editable eggs, you don't put the egg name as a comment like you do for a regular egg.\nAnyway, thank you a lot jazzband group for the maintenance of pip-tools, it's awesome ;-)\nHave a nice week.. ",
    "voronind": "Same issue\nrequirements.in:\ndjango-libsass == 0.7\ndjango_compressor == 2.0\npip-compile requrements.in\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file t.txt t.in\n\ndjango-appconf==1.0.2     # via django-compressor, django_compressor\ndjango-compressor==1.6    # via django-libsass\ndjango-libsass==0.7\ndjango_compressor==2.0    # via django-libsass\nlibsass==0.11.1           # via django-libsass\nrcssmin==1.0.6            # via django_compressor\nrjsmin==1.0.12            # via django_compressor\nsix==1.10.0               # via libsass\n```\ndjango-compressor and django_compressor is same package.\n. ",
    "gbataille": "exactly the same for me :(\n. ",
    "Cabalist": "Same here for Flask-Migrate:\nflask-script==2.0.5       # via flask-migrate\nflask-script==2.0.5       # via flask-migrate\n. ",
    "IlyaSemenov": "On a different set of requirements:\n```console\n$ cat requirements.in\narrow\nrq-scheduler\n$ cat requirements.txt\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\narrow==0.10.0\nclick==6.7                # via rq\ncroniter==0.3.16          # via rq-scheduler\npython-dateutil==2.6.0    # via arrow, croniter\npython-dateutil==2.6.0    # via arrow, croniter\nredis==2.10.5             # via rq\nrq-scheduler==0.7.0\nrq==0.7.1                 # via rq-scheduler\nsix==1.10.0               # via python-dateutil\n$ pip-compile --version\npip-compile, version 1.8.0\n$ pip --version\npip 9.0.1 from /Users/semenov/.pyenv/versions/3.5.2/envs/test/lib/python3.5/site-packages (python 3.5)\n``. @davidovich I confirm that the problem doesn't repeat with 1.8.1rc3 (I actually was unable to reproduce it on 1.8.0 as well \u2014 probably due to package updates, but quickly reproduced witharrow+croniter`, which however also worked correctly in 1.8.1rc3).. This repeats on macOS Sierra (case-insensitive HFS) and Ubuntu 16.04 (case-sensitive zfs), so I believe it's not filesystem-related.. I think that lower casing is the least surprising\nHow so? The least surprising is to have the original, package maintainer supplied capitalization. Any mangling whatsoever is more surprising.\nThe alternative is to have package names change case on odd compilation runs\nNo, this is a false dilemma. The real alternative is to keep package names always in the original, package maintainer supplied capitalization.\nThe problem exists in the tooling around pip\nI don't see how this could be used as an argument. The fact that the problem exists around and is not exclusive to pip-tools doesn't mean it should be ignored, it's actually the opposite. Someone will need to break the vicious circle at some point.\nThat said, I agree that lowercasing everything did fix the original problem that I reported, and I believe it's a good compromise between minimizing efforts and maximizing result. I just don't understand why false arguments are being used to put this as the most appropriate and comprehensive solution.. ",
    "borgstrom": "@nvie Is there anything else holding up 1.7.1 from landing in PyPI with the addition of --allow-unsafe?  I too have run into a situation where I want to include pip.\n. ",
    "chrismeyersfsu": "Looking forward to this feature \ud83d\udc4d \n. ",
    "DanielArndt": "Getting the same thing with notebook (which in turn would require ipython).\n. Seems this can be fixed by upgrading to a newer version of setuptools. Anything above v20.2.2 seems to work.\nI went to add this to install_requires but I guess that is considered unsafe.\nInterested to know what the correct way of \"permanently\" fixing it is.\n. ",
    "veatch": "The setuptools docs recommend using their ez_setup.py script to download and install setuptools if the required version isn't present. So maybe it's as simple as adding ez_setup.py to pip-tools and adding this to the top of setup.py:\nimport ez_setup\nez_setup.use_setuptools(version='20.2.2')\nBut I'm not confident about whether that's the right solution here.\n. ",
    "nkuttler": "I seem to have the same problem with pylint.\nValueError: ('Expected version spec in', u'backports.functools-lru-cache; python_version == \"2.7\"', 'at', u'; python_version == \"2.7\"')\nUpgrading setuptools indeed fixes this problem, but it would be good to give users a more useful error message.\n. ",
    "k0nG": "Also getting similar issue with Faker. It seems the 'extras_require' is where different python versions are specified is the problem:\nhttps://github.com/joke2k/faker/blob/v0.7.2/setup.py#L70-L80\nI've updated to pip-tools 1.8.0 and updated setuptools to 32.0.0 and that's seem to have done the trick.. ",
    "peteratticusberg": "Just posted that I had an issue here with a newer version of setuptools but then deleted it as it turns out I misundertood the OPs issue, apologies for the confusion. ",
    "bepetersn": "I'm having a similar sounding issue presently with setuptools 38.x and pip-tools 1.11, though the final exception is ValueError not RequirementParseError. The error happens after I add ipython to my requirements.in: ipython<6.0. Perhaps this version req just can't work? Using a 2.7 python.\nTraceback (most recent call last):\n  File \"/var/www/matin/app/pyenv/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 225, in cli\n    reverse_dependencies = resolver.reverse_dependencies(results)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/resolver.py\", line 297, in reverse_dependencies\n    return self.dependency_cache.reverse_dependencies(non_editable)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 139, in reverse_dependencies\n    return self._reverse_dependencies(ireqs_as_cache_values)\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 163, in _reverse_dependencies\n    for name, version_and_extras in cache_keys\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/utils.py\", line 200, in lookup_table\n    for value in values:\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 164, in <genexpr>\n    for dep_name in self.cache[name][version_and_extras])\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\", line 2582, in parse\n    reqs = list(parse_requirements(s))\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\", line 2507, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/var/www/matin/app/pyenv/local/lib/python2.7/site-packages/distribute-0.6.24-py2.7.egg/pkg_resources.py\", line 2475, in scan_list\n    raise ValueError(\"Expected \"+item_name+\" in\",line,\"at\",line[p:])\nValueError: ('Expected version spec in', u'pathlib2; python_version in \"2.6 2.7 3.2 3.3\"', 'at', u'; python_version in \"2.6 2.7 3.2 3.3\"')\n. Upgrading distribute did it for me! Thanks.. +1 this is kind of awkward. My global .pypirc or pip config or whatever it's reading from has a trusted host that I use for development on a specific project, but the host is being included in every project for which I pip-compile now. There's probably other solutions to this problem, but this seems obvious.\n. Wondering if removing --extra-index-url from requirements.txt output is possible as well.. You know I guessed wrong about what was happening. My package is named slightly differently and this isn't an issue for me. Thanks for your offer to help.. ",
    "markfink": "same here (I used an old pip version); now I have 8.1.2 and it works\n. ",
    "sbezboro": "\ud83d\udc4d \n. ",
    "EmilStenstrom": "Is there anything blocking this from being merged to master?\n. ",
    "dstufft": "Seems fine to me, I can probably add that.\n. When I wrote that post I struggled (and I still struggle) with trying to accurately describe, in my mind, the line between when you should use setup.py and when you shouldn't. It is not as strict as libraries versus applications (for instance, DevPI is an application but it is distributed on PyPI and is pip installable and uses a setup.py). In reality the difference is more like \"thing you will distribute via Python packaging channels to other people\" versus \"thing you will not\". with Pipfile and pip-compile which add a concept of differentiating a fully locked down requirements.txt/Pipfile.lock from the input to that, you end up with three distinctions:\n\nThing you will distribute via Python packaging channels.\nThing that has some semi abstract, semi concrete dependencies that you wish to distribute via non Python packaging channels.\nThing that has concrete, locked down dependencies, typically resulting from the input of the semi-abstract.\n\nA case where an application may have a setup.py, a requirments.in, and a requirements.txt is say something like Warehouse if we pretended that Warehouse was going to function as an application other people could install on their own infrastructure. In that case you'd have:\n\nsetup.py would list the abstract dependencies that Warehouse actually requires to function.\nrequirements.in / Pipfile would reference . through some construct (which would pull in itself, plus all of the abstract dependencies from setup.py as well as any additional things (like maybe gunicorn).\nrequirements.txt / Pipfile.lock would be the deployment artifact that the production instance of Warehouse would use to ensure reproducibility.\n\nI'm not sure if that all makes sense, but I don't think that every thing should have a setup.py but I don't think the line are as clear cut as libraries vs applications.. ",
    "peterdemin": "I confirm. Version 1.8.0 failed on my requirements.txt.in. But after upgrade to 1.8.1 pip-compile resolved dependencies correctly.\nNice work! Thanks!. Hi :-)\nFrom my experience, I'd like to add, that when processing directories you naturally hit a few obstacles.\n1. You need to filter input files because some files should not be compiled (for example py2 vs py3 separation).\n2. You need to configure input extension and output extension (to enable .in => .txt => .hash pipeline).\n3. You need a way to check, that pip compile was run after changes to .in files.\nIf pip-tools supported all these cases (and compatible releases, but that's not about directory input), I'd be happy to retire pip-compile-multi. Also, I think that it's fine to have pip-compile-multi as a wrapper around pip-tools and keep it separated.\nIMHO, the most complex part of pip-compile-multi is conflicts detection. I didn't review your PR, so I just want to highlight this part.. ",
    "mrmachine": "This works for me. Please merge and release so I can go back to recommending pip install pip-tools instead of pip install 'git+https://github.com/blueyed/pip-tools.git@no-download_dir-for-editable#egg=pip-tools'\n. ",
    "Sinkler": "thanks, it helped. ",
    "FlxVctr": "requirements.txt ist empty and pip freeze returns the packages installed by conda. So yeah, I'm just using pip-tools for a not intended purpose I guess. Will try to find a workflow that turns out to do what I want. Not really the job of pip-tools I guess.\n. ",
    "das-g": "I noticed this, too. I guess this is probably due to (or for the same reason as) pypa/pip#4022, which according to @RonnyPfannschmidt is caused by a bug in ubuntu. (Filed as https://bugs.launchpad.net/ubuntu/+source/python-pip/+bug/1635463). > it seems the Ubuntu bug is slow-moving\nDoes anyone know what would have to change on the Ubuntu side to fix this properly? Can we send them a patch to speed things up?\nUntil then, an official pip-tools release with the workaround merged would be much appreciated.. Should this be marked as a workaround for an alleged Ubuntu bug or would ignoring pkg-resources be a good idea even independent of that?. ",
    "felipemontoya": "I get the same thing. It looks like pip-tools breaks on this line\nIm on \npip-tools 1.7.0\npython 2.7.12\nUbuntu 14.04.05\n. ",
    "johnlinvc": "Update setuptools to 28.7.1 solved this issue for me.\n. ",
    "willhardy": "This is the standalone script I used to make this happen, in effect wrapping pip-compile. It'd be nice to just add an argument to pip-compile, but I couldn't wait and didn't want to maintain a fork, so wrote this quickly: https://gist.github.com/willhardy/8086aa8fcbd5df4986986cf4a66a440c. This should address #638.. ",
    "jedie": "Maybe i rise into the same Problem with pip-tools==2.0.1:\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.5/dist-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/piptools/scripts/compile.py\", line 243, in cli\n    allow_unsafe=allow_unsafe)\n  File \"/usr/local/lib/python3.5/dist-packages/piptools/writer.py\", line 123, in write\n    f = stack.enter_context(AtomicSaver(self.dst_file))\n  File \"/usr/lib/python3.5/contextlib.py\", line 304, in enter_context\n    result = _cm_type.__enter__(cm)\n  File \"/usr/local/lib/python3.5/dist-packages/piptools/io.py\", line 432, in __enter__\n    self.setup()\n  File \"/usr/local/lib/python3.5/dist-packages/piptools/io.py\", line 428, in setup\n    self._open_part_file()\n  File \"/usr/local/lib/python3.5/dist-packages/piptools/io.py\", line 393, in _open_part_file\n    fd = os.open(self.part_path, self.open_flags, file_perms)\nOSError: [Errno 30] Read-only file system: 'requirements.txt.part'\nI'm not sure where piptools will create the file requirements.txt.part. Is it the cwd ? This directory is write able for the current user.. > A work-around that worked for me: add a line pkg-resources==0.0.0 to requirements.in.\ndoesn't work for me.\npip-sync output:\nCollecting pkg-resources==0.0.0\n  Could not find a version that satisfies the requirement pkg-resources==0.0.0 (from versions: )\nNo matching distribution found for pkg-resources==0.0.0\n@merwok 's workaround is ugly, but worked here.. Think i have the same Problem...\nI wanted to install Django v1.11.x with django>=1.11,<1.12 and it seems that other packages doesn't support v1.11, yet... But i didn't find it via grep on a verbose output, because some many lines contains django ;)\nAny idea?. Great! It works here, thanks!. ",
    "dan-passaro": "For multiple levels of depth, I would think you can bubble it up... E.g. if you have A, and then X which depends on Y which depends on Z, and the latest version of Z needs a newer version of A, you roll back Z if allowed (e.g. if Y's dependency is something like Z>=some_version). If you find a Z that works then you're done, if that doesn't work then that version of Y is not compatible, so roll back to the next earliest version of Y and try it all over again and so on. If all versions of Y fail, then roll back to the next earliest version of X and start all over again. If you're out of earlier versions of X then it's a total failure, and probably an erroneous file/request (or maybe some old package got deleted somewhere).\n(In typing this out I think it's basically the first thing you said.)\n. I haven't done too much research and don't know how accurate this comment is, but it seems like this problem would be much easier to solve if PyPI published dependency information similar to how RubyGems does. Does Bundler not handle this scenario more gracefully?. @vphilippon yes, Ruby is like Python in that all dependencies are stored in a flat toplevel, compared to Node where each dependency gets its own tree of sub-dependencies.\nBrowsing the RubyGems site (similar to PyPI), you can see that the site itself has structured data describing the dependencies, e.g. on the page for the Capistrano gem you can see right on the page it depends on 4 gems at runtime and 4 gems for development, with version range specifiers.\nI haven't researched this claim but my hunch is that Bundler (similar to pip) can somehow query this data and use it to make choices about what versions to select. If that's true, I would expect Bundler to handle this problem better than pip-compile does currently.. ",
    "jbaum-cmcrc": "I have a different, probably much more common usage scenario which would be solved by the same feature.\n\n\nHave five packages, A, B, C, D and E, which all use pip-compile.\n\nA depends on B, C, D.\n  C and D depend on E.\nA -> B\n  A -> C -> E\n  A -> D -> E\n\n\n\nInitial situation:\n  ```\n  A/requirements.txt:\n      B==1.0\n      C==1.0\n      D==1.0\n      E==1.0\n\n\n```  \n There is a new version of E, 1.1.\n Run pip-compile on package C, integration tests pass, release C version 1.1 which depends on E==1.1\n Run pip-compile on package D, integration tests fail; no new version released, D still depends on E==1.0\n There is now a new version of package B, 1.1.\n* Run pip-compile on package A:\n  Actual result:\n  * pip-compile fails as the latest versions of packages C and D have conflicting requirements for package E.\nExpected result:\n  * pip-compile succeeds, choosing the latest version of B but versions 1.0 of all the other packages (the most up-to-date consistent configuration of versions).\n    A/requirements.txt\n        B==1.1\n        C==1.0\n        D==1.0\n        E==1.0\nImpact\nThis is particularly important when package B is frequently-changing, while the dependency graph below C and D is more complex with many opportunities for integration test failures and/or relatively slow integration tests.. Optimisation of algorithm to make it practical.\nAs written by @dan-passaro (and implied by the issue description), pip-compile would at times end up downloading and analysing large parts of the history of many packages. This would be prohibitive.\nTo avoid this, pip-compile can use the previously-compiled requirements.txt of the current package to prune the search; in most cases, none of the requirements.in files has changed, so the previous requirements.txt is a consistent, satisfactory configuration of versions \u2014 we're just checking to see if an upgrade is possible.\nThis would cut down the number of versions to be checked considerably; in most cases, there would only be a handful to check, often just one or two (the one in the previous requirements.txt and the latest version, if different).\nTo handle new pins, this pruning would be applied only to the \"try an older version of Z\" logic, not to any versions specified in any requirements. This would not handle new pins completely, but it would handle them sufficiently in practical use.\n In the scenario of multiple packages using pip-compile and uneven propagation of upgrades, this optimisation would work perfectly.\n In the scenario of multiple packages using pip-compile and a new pin in one of the dependencies, this optimisation would work OK, although if multiple packages depend on the package to be pinned, they would all have to be pinned consistently. This already needs to be done, so it's not a regression.\n* For a new pin in the package to be compiled, a manual step would be required; the user would have to specify a solution once. Once a solution was specified, pip-compile would resume working normally, upgrading from the manually-specified solution to the latest possible solution. When the pin is removed, no manual intervention would be needed and pip-compile would upgrade to the latest version.\nWhile requiring a manual step is a disadvantage, I can't see any practical way to avoid it; it simply doesn't seem to be plausible to potentially download the entire history of the dependencies in order to analyse the graph. It's also only required in limited circumstances (when adding a new pin, and then only in the package where the new pin is being added).\nFor packages not mentioned in the previous requirements.txt (or if there is no previous requirements.txt), no roll back would be attempted, effectively falling back to the current algorithm.. Having pypi publish dependency information separately would indeed help; that would eliminate the concern with the volume of downloading.\nWe would still have the problem of solving for the best solution and algorithmic complexity. I wouldn't be surprised if the problem turns out to be NP-complete (that is, the question of whether there is a solution). That doesn't rule out reasonable performance in practice, but it does make it more difficult to implement. Likely different heuristics would do well in different situations, and we would have to find ones that work well in typical use.\nMy proposed optimisation would work by restricting the number of package versions to be considered when searching in the history, then brute-forcing through the rest. That would likely be satisfactory in many common scenarios (or, at least, in our own real scenario), since there would only be a handful of versions to consider.\nThe scenario in the original post would likely be easy to solve, since there are only two top-level packages and one of them has an explicit version pin. It's therefore either obvious that it's the django-debug-toolbar that needs to be rolled back (since its dependencies cannot match the explicitly-specified ~=1.4.0) or there are only a few versions of Django ~= 1.4.0 to check.\nLikely we would be able to develop a solver which would work well in most circumstances.\n\u00a0\nA minor issue would be defining which of the possible solutions should be returned when there is more than one. In most cases, there will be one clear best solution. In the remaining cases, likely any of them can be returned; ideally the same one each time, to avoid spurious changes in the compiled requirements.txt (which would then trigger spurious CI builds and interleaved package versions which alternate upgrading different dependencies). That would require either a deterministic algorithm, or an explicit last-stage comparison with the previously-compiled requirements.txt so that the previous solution is returned if the new solution is no better.. ",
    "jamescooke": "\nrequirements-dev.in has -r requirements.txt as its first line. Is this an okay workflow?\n\nYes I totally think that's a good strategy.\n\nWondering if anyone else has used pip-tools in this fashion and has any advice?\n\nI've just published my pip-tools workflow for managing dependent requirements files: http://jamescooke.info/a-successful-pip-tools-workflow-for-managing-python-package-requirements.html\n. Hi @nvie - thanks for the kind words about the blog post :blush:.\nThe reason that I recommended including the .in files rather than the .txt files is to account for changes in the package indexes which might mean that a testing requirements file ends up with different package versions that the base file.\nAs an example, let's say that a project wants any Django that's version 1.8, so in requirements.in:\ndjango<1.9\nWhen we compile that file in October it picks 1.8.15 and makes requirements.txt:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\ndjango==1.8.15\n```\nNow, in November a new version of Django is released 1.8.16. We add or update the testing requirements (without touching the production requirements) requirements-dev.in:\n-r requirements.in\ndjango-debug-toolbar\nUsing pip-compile requirements-dev.in, we compile that to requirements-dev.txt:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements-dev.txt requirements-dev.in\n\ndjango-debug-toolbar==1.6\ndjango==1.8.16\nsqlparse==0.2.2           # via django-debug-toolbar\n```\nAs you'll see, Django has now been bumped in the dev requirements only to 1.8.16 from 1.8.15, even though the main requirements.in and requirements.txt have not changed. A good developer would spot this for sure - but I've missed something similar before on previous projects, with much resulting pain.\nIt's for this reason that I have been including the txt file instead of the in file - I've found it keeps the versions exactly the same between requirements layers.\nSo with requiremements-dev.in as:\n-r requirements.txt\ndjango-debug-toolbar\nWhen this is compiled we now get requirements-dev.txt:\n```\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements-dev.txt requirements-dev.in\n\ndjango-debug-toolbar==1.6\ndjango==1.8.15\nsqlparse==0.2.2           # via django-debug-toolbar\n```\nThis maintains the exact Django 1.8.15 version that we're looking for, regardless of the fact that the dev requirements were compiled after the new version of Django was released. When we then update the requirements.txt and recompile the dev requirements, then that version will be bumped.\nI'd be really happy to see the just works:tm: version of the kind of pinning that I'm talking about - I'm sure I'm getting the wrong end of the stick somewhere.\nOne alternative I could see is to pin the Django version in an .in file, but isn't that missing the point of the them?\nSorry for the long essay :disappointed:\n. @Groxx thanks for the django==2.0 and django-debug-toolbar example... this is the exact kind of scenario I've been concerned about. I found your example a good illustration.\n@franklinyu - The Bundler strategy might also work. Thanks for illustrating the unions of package requirements :+1:. Hi @dfee , thanks for sharing this suggestion :+1: \nI've not been able to get this working on my machine, so I won't update my article just yet. The post is on GitHub here https://github.com/jamescooke/blog/blob/master/content/1611-pip-tools-workflow.rst - feel free to open an issue / PR to discuss.. ",
    "FranklinYu": "Sorry if I missed something, but what's the issue of -ring the .txt? The workflow looks good, except that I may need to intervene when updating a single package (but I typically just update all).. > Only real downside is that sometimes the requirements.txt pins to a version that's incompatible with something in requirements-dev.txt\n@Groxx Does that happen after you upgrade (recompile) requirements.txt? So some new version in requirements.txt stops some legacy package in requirements-dev.in?\nFor (imaginary) example, django-debug-toolbar==1.6 (latest) works with django<1.9, and you bump Django to 2.0 in requirements.txt? In this case django-debug-toolbar gets out of date.. Even if it doesn't take long, it does waste time; and I believe it's one of the reason why we have this project. I came from Ruby background, where Bundler did the right thing: even if you ask it to only upgrade a single package, it re-constructs the entire dependency graph, including development ones. Similarly, when upgrading, I believe pip-compile should\n\ntake all the requirements-*.in;\nlist the union of all the packages;\nlist the union of all requirements-*.txt pins;\ntry to construct a dependency graph with all the package, satisfying all the pins;\ncome up with a list of all the new pins;\nfor each requirements-*.in, pick some packages from the final (new) pin list, and generate the respective requirements-*.txt.\n\nI'm not sure whether current Pip works with this workflow, that is, whether this workflow is a feasible solution for pip-compile.. ",
    "maxnordlund": "This bit me today and also coming from a Ruby/Bundler background I like that all dependencies is in the same lock file, but I don't want to install dev dependencies in production. However this seems incompatible with how pip currently operates. That is, one requirements.txt to rule them all, but separate common/dev dependencies.\nI had hoped that having dev.in -> dev.txt would solve this but as others have noted you get conflicts. And while I could have a -r somewhere it still would produce two lock files, which sooner or later will diverge.\nSo my question is if it would be possible to teach pip-compile to just write the dependencies for one input file, while accepting the pinned ones in another. Perhaps an example would clarify this:\n```sh\nrequirements.in\ndjango\n```\n```sh\nrequirements.txt\ndjango==1.9\n```\n```sh\ndev.in\n-r requirements.in\ndjango-debug-toolbar\n```\n```sh\ndev.txt\n-r requirements.txt\ndjango-debug-toolbar==1.6\nnote, no direct django dependency here, but still respect the 1.9 bound.\n```\nHere I've overloaded -r to point to the other file. Thoughts?. > How about considering eliminating the need for multiple files by supporting sections in requirements.in? This is how the Ruby Gemfile works, and it neatly solves the problem:\nThat would make it incompatible with vanilla pip, which isn't really an option for this set of tools IMO. For the official project supporting that idea see https://github.com/pypa/pipfile.\n\n@maxnordlund This blog post answers your question, I believe: http://jamescooke.info/a-successful-pip-tools-workflow-for-managing-python-package-requirements.html\n\nI've read that, it's linked in the first comment.\n\nIn the end I wrote a small python script to generate the two files, with dev having a -r pointing towards base.txt. Then I strip dev.txt of all common dependencies to ensure they cannot diverge. This also forces you to call pip-sync base.txt dev.txt, but that's no biggie and in my case the script actually runs that as well.\nThe sad part here is that you need another layer to get it right, either a script or make, instead of it being included in the box. The only thing I think might be good enough without changing the format too much, is the suggestion above. That a -r in an *.in file is translated to mean \"use existing versions in that file (or compiled version thereof), and write everything else to output\".. ",
    "dogweather": "How about considering eliminating the need for multiple files by supporting sections in requirements.in? This is how the Ruby Gemfile works, and it neatly solves the problem:\n```ruby\nInstall in all environments\ngem 'rails'\ngem 'mysql'\nInstall only in test\ngroup 'test' do\n  gem 'rspec'\nend\nInstall only in development\ngroup 'development' do\n  gem 'web-console'\nend\n``. @maxnordlund This blog post answers your question, I believe: http://jamescooke.info/a-successful-pip-tools-workflow-for-managing-python-package-requirements.html. Failing for me with cpython 3.5.3. I'm wondering what the commonalities are. Clearly, the current pip-tools version (1.8.2) must be working for _someone_. (?). I found I could get it to work by checking forNoneincompile.pybefore callingkey_from_req` at line 131:\n```python\n        for ireq in ireqs:\n            if ireq.req is None:\n                continue\n            key = key_from_req(ireq.req)\n```\nThis doesn't solve the root cause of why Nones make it into that list, but this seems to function fine.. ",
    "vladiibine": "Hi guys,\nGreat package.\nWanted to jump in with this observation, that for me is really important:\nI consider that having -r base.txt in a fils such as dev.in is the best workflow yet.\nOne big drawback is that this way we LOSE the comments that tell us why a dependency was installed.\nFor instance\n# base.txt\npackage0==1\npackage1==1.4   # via package0\n\nThen in dev.in\n# in dev.in\n-r base.txt\npackage2==3.3\n\nThen in the resulted dev.txt\n# in dev.txt\npackage0==1\npackage1==1.4      # !!!!!!!!!!!! the via comment will be missing here. I'd totally prefer this to remain here... :(\npackage2==3.3\n\nAnyway, that's all from me. Whoever fixes this, please take this into consideration if you can.. ",
    "anlutro": "Why can't -r base.txt statements (as long as they're .txt, not .in) just get copied as-is to the resulting .txt file?. ",
    "samkk-nuna": "I followed @jamescooke's flow and recently ended up in a state where I had to add a constraint to my base.in to help the resolver out, because of the following:\n\nAdd boto3==1.7.14 to base.in\nAdd moto==1.3.3 to test.in, which starts with -r base.txt\n\nTry compiling .txt files from these with pip-compile \u2014 base.in compiles fine, but emits a hard equality constraint python-dateutil==2.7.2 into base.txt, which then conflicts with a python-dateutil<2.7.0 constraint emitted from something in moto's dependency tree. \nI've hacked around this for now by explicitly stating python-dateutil<2.7.0 in base.in, but that feels gross. Any recommendations on better workarounds, or plans to better support things like this?. ",
    "ulope": "Indeed this makes it really difficult to use pip-sync while developing packages.\n. This would be great. We're currently running sed on the generated files to change this comment and point to the right command.. ",
    "alairock": "And that is what I put in the requirements.in right?\n. Ah. Not quite what I am looking for. Our CI runs pip install -U -r requirements.txt and we like to have patch versions auto upgrade with that. So the .txt version would have to be pinned to the minor versions only. \n. ",
    "AndreLouisCaron": "@Groxx Didn't know about pipsi.  Thanks for the suggestion, but it's not quite what I'm looking for. . ",
    "rmax": "The use case I had is that I wanted to get the results for a single package:\n$ echo jsonschema | pip-compile - -o -\nDuplicate of #359.\n. ",
    "Gastove": "This is probably a less articulate version of #408.\n. Brilliant, thanks! \n. Oh boy. Thanks for looking! I'll dig a little more on my end and report back.. Alas: I'm still getting the same result, with the same bad dep, every time: https://pastebin.com/uC1WgxT9. Ah right -- here's the pip-compile -v --rebuild output: https://pastebin.com/8dQS7Y26. Um. Well. This is all.... perfectly alarming. I will... see if I can't... dig in to this a little further on my end. Something seems to have gone completely sideways, and it's smelling increasingly like it's something subtly borked on my end. \nThanks for your time! I'll report back. . So. A pretty significant part of this seems to be some tooling we use here at my work, which has turned out to be... uh, let's say \"bad.\" Something is... deeply wrong system python. I do not understand what yet -- or, honestly, why it hasn't exploded much more dramatically much earlier. But, I seem to have tooling that is actually using python2.7 pip (and claiming it's python 3 pip), which is probably what's cramming garbage in to the cache. \nSo. If I meticulously create a new venv with absolutely for-sure only python3 and skip the pip cache for pip-tools installation, everything works great for me. \nBeyond \"oh god oh god,\" I'm not... confident yet about what is going on. I'm.... well. I very much appreciate the help digging in to this, and I apologize for any wasted time. It certainly seems like this is something deeply screwed up in my python installation. \nIn light of this, I'm going to go try and purge/re-install basically everything fresh.. Okay, I have: \n\nCompletely scrubbed global pip packages, including blowing away the cache entirely\nFresh venv install\nFresh installation of pip-tools\n\nEverything now seems groovy and consistent. \nThanks for an awesome tool and for the digging!. ",
    "cpennington": "Oops. I just noticed that it isn't preserving the github links in the output. That'll be something to fix before this lands.\n. I now see that this is a more complicated project than I had initially expected. I'll comment again when I think I have it all squared away.\n. I think this is ready to go, now.\n. This builds on top of #405 \n. ",
    "bochecha": "Hi @cpennington , are you still interested in pushing this forward? :). ",
    "nnja": "Pushed some updates - the build is now green, except for https://travis-ci.org/nvie/pip-tools/jobs/176202798 piptools/utils.py:22:1: E305 expected 2 blank lines after class or function definition, found 1 which seems unrelated to my changes, since I have not modified utils.py\n. ",
    "ewjoachim": "It seems to be the same for \"-e\" requirements.. ",
    "derek-miller": "@nvie I took a stab at this, I would appreciate it if you could take a look.. @nvie Any chance you could take a look at this?. @nvie?. Github's PR user interaction is quite terrible and marked this as Merged when I was trying to rebase, forcing me to open another PR. See #471.. Anyone have time to review this?. ",
    "dirn": "The gist is that setup.py is for libraries, requirements.txt is for applications. When installing a library\npip install pip-tools\n\nsetup.py is executed. Any requirements.txt (or Pipfile.lock since that's how I found this) would be ignored by Pip. . The two sections immediately before the quote you referenced are Python Libraries and Python Applications. The former covers setup.py and the latter covers requirements.txt. This is where I see the distinction made.\nWe have, however, digressed from my original point a bit -- probably because I mistakenly replied out of context -- but my objection was to the use of \"(you should).\" Whether or not Donald was saying that you should or shouldn't use setup.py with your applications[1], I believe his main point was that they serve distinct purposes. I use requirements.in for all of my applications (which have no setup.py) and I have no need for requirements.txt for my libraries.\nPerhaps you can explain your use case for having both setup.py and requirements.txt and why you think that's the way people should always do it. If you're installing your application with one of\n$ python setup.py install\n$ python -m pip install .\n\nthe requirements.txt file is ignored. If, on the other hand, you're installing your application with\n$ python -m pip install -r requirements.txt\n\nsetup.py serves no purpose other than to generate requirements.txt, something you could have already done with a requirements.in file.\n[1] Donald later references a sample requirements.txt file for when your library becomes your application. I'd argue that this further supports my interpretation that setup.py is for libraries and not for applications.. >Frankly, the \"my project doesn't need a setup.py\" drivel has to stop. Writing a basic setup.py is easy, I've not heard a good reason to avoid doing so.\nWhat do I gain from shipping around an executable file whose sole purpose is to serve as the basis for the contents of a static requirements.txt? For that purpose, the requirements.in meta file feels like a much saner approach.\nSpecial cases aren't special enough to break the rules.\n\nThis is the nature of this entire conversation. Is the rule \"everything needs a setup.py\" or is it \"everything that get distributed through Python packaging channels needs a setup.py and things that don't don't\"? I believe it's the latter.\nThere should be one-- and preferably only one --obvious way to do it.\n\nAgain, this calls into question what \"it\" is here. If it's \"everything,\" then setup.py would have to serve as the TOOWTDI. If, however, it's \"everything that doesn't get distributed through Python packaging channels,\" then setup.py isn't necessarily the TOOWTDI. I'd say that it isn't since the file is never used by the end system/users. setup.py encompasses a lot of functionality and information that would be ignored entirely. To me that feels like trying to make setup.py a one-size-fits-all solution.\nI try to write no more code than is necessary. setup.py feels unnecessary here.\n\nThe article presented by @dirn as a counter to my comment in the README was written in July 2013 - it's ancient.\n\nI'd argue that the way to do things \"since about 2008\" may be even less relevant, seeing as \"the Python packaging ecosystem has come a long way in the last 10 years.\"\n. Here's an example of a situation where I think setup.py is inappropriate:\nAnsible modules rely on many packages available on PyPI. When I ship around Ansible playbooks, I will include a requirements.txt that contains things like Ansible itself, pyrax, boto, docker-py, etc. These may all be abstract requirements, but everyone should run the deployment (or whatever else the playbooks do) using the same versions. requirements.txt will contain the concrete requirements. To track the abstract requirements, I could use a setup.py, but seeing as this is neither a library nor an application (really there's almost no executable Python code to found anywhere), introducing Python code feels out of place. This is why I use a requirements.in here.\nI prefer the rule\nUse requirements.in when you need a requirements.txt and setup.py when you don't.\n\nto\nUse setup.py when distributing Python code and requirements.in when you're shipping something else.\n\nThe first one feels better to me because, at the end of the day, both scenarios actually require asking \"do I need a requirements.txt?\" But the first scenario ends there. I have all the information I need to know what file to create. Depending on the answer to the question, though, the second scenario could also require asking \"am I distributing Python code or something else?\"\nTwo possible outcomes are easier for me to keep track of than three.\nNow I understand that this patch would keep requirements.in as the default and my life will continue on unaffected, but if setup.py is TOOWTDI, shouldn't that become the default? Shouldn't I, as someone doing things in a non encouraged way, have to do more work to get my solution to work?. >Would changing those 2 words from \"you should\" to \"you should if you are distributing a python module\" help to settle this?\nAt the end of the day, the decision is Vincent's to make. I just didn't agree with what I thought was an overly general recommendation, especially sincerequirements.txt is going to be ignored when you install through setup.py.. >you should\nThis is an opinion not everyone would agree with.. ",
    "woozyking": "I think the bundle format support is generally not working, for example, broken also for celery[redis]. @RazerM yes you understood it correctly.\nI've been able to reproduce this issue on both Python 2.7 and Python 3.5, macOS Sierra 10.12.1 and Ubuntu 14.04.4\nedit: all through virtualenv. ",
    "RazerM": "I can't reproduce, did I understand the issue correctly?\n```\nroot@22f7b4dea128:/# pip --version\npip 9.0.1 from /usr/local/lib/python2.7/site-packages (python 2.7)\nroot@22f7b4dea128:/# echo \"requests[security]\" > requirements.in\nroot@22f7b4dea128:/# pip-compile requirements.in\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\ncffi==1.9.1               # via cryptography\ncryptography==1.6         # via pyopenssl, requests\nenum34==1.1.6             # via cryptography\nidna==2.1                 # via cryptography, requests\nipaddress==1.0.17         # via cryptography\npyasn1==0.1.9             # via cryptography\npycparser==2.17           # via cffi\npyopenssl==16.2.0         # via requests\nrequests[security]==2.12.3\nsix==1.10.0               # via cryptography, pyopenssl\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools                # via cryptography\n```\n. @woozyking I used the docker python container. I will try to reproduce on macOS later.. ",
    "svisser": "This may be a duplicate of https://github.com/nvie/pip-tools/issues/416.. ",
    "cmc333333": "This might be a regression -- I was seeing the same until I downgraded to piptools==1.7.0\nFailing Environment:\nLinux (Alpine)\npyenv, cpython 3.6.0\npip==9.0.1\npiptools==1.8.0. While I'm generally in favor of reducing dependencies, the coloredlogs library is tailor-fit to this situation. Python's logging system isn't aware of ANSI colors (they don't make sense when outputting to a file, constructing JSON for logstash, etc.). coloredlogs allows us to add that back when writing to the console (while avoiding it for other outputs) to match the existing functionality.\nI think that leaves us with a few possibilities:\n\nContinue to write everything to stdout and skip Python's logging system. This is not a great solution as it mixes different types of messages and doesn't allow the logs to be configured.\nUse Python's logging system, but don't attempt to colorize the stdout/stderr logs. This is a regression in terms of functionality.\nUse Python's logging system and coloredlogs. This solves the problems of 1 and 2, but adds a new dependency.\nUse Python's logging system and re-implement coloredlogs in pip-tools. This is also a less-than-optimal solution as it requires new tests and maintenance for the pip-tools team. It's also trading a custom logging system for a custom coloring system, which isn't as significant a step forward.. \n",
    "rnwolf": "I have a similar issue:\n`rnwolf \ue0b0 ~ \ue0b1 workspace \ue0b1 cpptest4 \ue0b0 pip-sync                    \nUninstalling pkg-resources-0.0.0:\n  Successfully uninstalled pkg-resources-0.0.0\nrnwolf \ue0b0 ~ \ue0b1 workspace \ue0b1 cpptest4 \ue0b0 pip-compile > requirements.txt\nTraceback (most recent call last):\n  File \"/home/rnwolf/workspace/cpptest4/.direnv/python-3.5.2/bin/pip-compile\", line 7, in \n    from piptools.scripts.compile import cli\n  File \"/home/rnwolf/workspace/cpptest4/.direnv/python-3.5.2/lib/python3.5/site-packages/piptools/scripts/compile.py\", line 16, in \n    from ..repositories import LocalRequirementsRepository, PyPIRepository\n  File \"/home/rnwolf/workspace/cpptest4/.direnv/python-3.5.2/lib/python3.5/site-packages/piptools/repositories/init.py\", line 3, in \n    from .pypi import PyPIRepository\n  File \"/home/rnwolf/workspace/cpptest4/.direnv/python-3.5.2/lib/python3.5/site-packages/piptools/repositories/pypi.py\", line 14, in \n    from ..cache import CACHE_DIR\n  File \"/home/rnwolf/workspace/cpptest4/.direnv/python-3.5.2/lib/python3.5/site-packages/piptools/cache.py\", line 9, in \n    from pkg_resources import Requirement\nImportError: No module named 'pkg_resources'\n`. ",
    "noamraph": "A work-around that worked for me: add a line pkg-resources==0.0.0 to requirements.in.\n@merwok 's workaround didn't work for me.. ",
    "opokatech": "Adding pkg-resources==0.0.0 to requirements.in worked for me. Tested in following way on debian stretch:\nmkvirtualenv tmp\nmkdir tmp && cd tmp\necho pkg-resources==0.0.0 > requirements.in\npip install pip-tools\necho flask >> requirements.in\npip-compile\npip-sync\nAfter that flask has been installed and pkg-resources is still there. I tested it also by adding bpython to requirements.in (and pip-compile && pip-sync) and removing it followed by pip-compile and pip-sync. It worked without any problems, i.e. pkg-resources was preserved.. ",
    "franekp": "It not only breaks psycopg2, but also pip-tools itself!. +1 This would be very useful for packaging complete \"end products\" (e.g. web applications) with pinned dependencies. Pip documentation says that a dot in requirements.txt refers to a package from current directory. Maybe pip-compile could check whether requirements.txt is a file consisting of exactly one line with a dot and nothing else, and if it is, assume that this is an \"end product\" and write pinned dependencies to install_requires in setup.py?\nP.S. I know that install_requires should normally be used for \"abstract requirements\", but this applies only for libraries that need to co-exist with other packages.. ",
    "mitbal": "Thanks for the info, David. I myself run python 2.7.6 in Ubuntu 14.04 with the same version of pip and pip-tools.\nI wonder if this is some kind of heisenbug because my colleague who first encountered this error, suddenly, somehow, managed to successfully install the package, without doing anything. However, I still run into this error.\nI managed to install the package individually by running pip install in the terminal, but that kind of defeat the purpose of using pip-tools.\nHere's the full traceback from my terminal. The error disappears if I remove boto3 from the requirements.in file, no matter the position (up, middle, or bottom).\nTraceback (most recent call last):\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/click/core.py\", line 716, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/click/core.py\", line 696, in main\n    rv = self.invoke(ctx)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/click/core.py\", line 889, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/click/core.py\", line 534, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 223, in cli\n    reverse_dependencies = resolver.reverse_dependencies(results)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/resolver.py\", line 273, in reverse_dependencies\n    return self.dependency_cache.reverse_dependencies(non_editable)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 139, in reverse_dependencies\n    return self._reverse_dependencies(ireqs_as_cache_values)\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 163, in _reverse_dependencies\n    for name, version_and_extras in cache_keys\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/utils.py\", line 192, in lookup_table\n    for value in values:\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/piptools/cache.py\", line 164, in <genexpr>\n    for dep_name in self.cache[name][version_and_extras])\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/pkg_resources.py\", line 2625, in parse\n    reqs = list(parse_requirements(s))\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/pkg_resources.py\", line 2563, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/home/mitbal/workspace/ml-application/word_vector_service/wvsenv/local/lib/python2.7/site-packages/pkg_resources.py\", line 2541, in scan_list\n    \"Expected ',' or end-of-list in\",line,\"at\",line[p:]\nValueError: (\"Expected ',' or end-of-list in\", u'futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\"', 'at', u'; python_version == \"2.6\" or python_version == \"2.7\"'). Ah, the problem is gone and it works as intended after I updated setuptools using pip.\nThanks David!. Oops, turned out I just need to install gcc and python-dev. Problem solved.. ",
    "alexwlchan": "Wait, I just went to see whether I could find anything about what versions of pip are supported, and the README says:\nconsole\n$ pip install --upgrade pip  # pip-tools needs pip==6.1 or higher (!)\n$ pip install pip-tools\nSo I guess this is a bug?\nAh, but looking at the commit message for 95ca3fe, I see that you need to have pip >= 8 now. I\u2019ll throw together a patch.. Ah, it looks like this is already a partially solved problem: there\u2019s assert_compatible_pip_version(), but that\u2019s only called after the internal imports have been done \u2013 if you hit a version-specific problem while doing those, you never get to call that code.. There is still a possibility of error if an ImportError is thrown in utils.py, but the fix for that is more destructive, and would mean moving this function into its own file. I don\u2019t mind doing that, but I\u2019ll wait to see if reviewers think that would be necessary.. ",
    "hyperknot": "It also converts underscores to dash inconsistently.. The upper/lower case seems to depend on using -U or not in my case. . Can confirm, first run possibly-upper case, second run: always lower case.. lowercase sounds good\nOn 3 February 2017 at 17:56, Omar K. notifications@github.com wrote:\n\nlowercase is fine with me\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/nvie/pip-tools/issues/431#issuecomment-277301237, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAeKj3oTwGqRyziqbxNagtkwHq8WJGAvks5rY1wwgaJpZM4LXfP3\n.\n. I totally agree that lower case is an almost perfect solution! Let's use it!. The new lower case is perfect, but semantic_version still gets converted into semantic-version.. @klaplong as long as it's consistent there is no problem.. 10 minutes later and the same command doesn't give exception. Everything is 100% the same. Git reset same.. I also run into an error which brakes pip install. It was adding an older\nappdir package than the one supplied with virtualenv which it couldn't\ninstall.\n\nOn 2017. Feb 26., Sun at 21:41, \u00c9ric Araujo notifications@github.com\nwrote:\n\nHaving appdirs in the compiled requirement files but not setuptools has\nbeen causing issues for me on CircleCI in the past weeks.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/445#issuecomment-282585632,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeKjwud65taZgzOqj-Hc5KlNhWWPu4uks5rgeNigaJpZM4LxECN\n.\n. OK, it only happens when the requirement is pyramid.\nIt requires WebOb>=1.7.0rc2 which installs 1.8.0rc1 instead of 1.7.4.\n\nBut this mirrors the Pip behaviour, which might mean it's a bug in pip? . Actually the minimal case is as simple as: \npip install 'webob >= 1.7.0rc2'\nCollecting webob>=1.7.0rc2\n  Using cached WebOb-1.8.0rc1-py2.py3-none-any.whl\nI think it's a bug in pip.. Opened pip issue: https://github.com/pypa/pip/issues/4969\n. ",
    "klaplong": "So, the problem is sort of established. Is anybody working on a PR?. Okay. I have been digging around in the source for a while now. One thing I can conclude is that underscores changing into dashes comes from Setuptools. This is part of a normalization process and it doesn't seem to be inconsistent or causing any problems.\nAs for the capitalization problem. Do we want to just normalize to lowercase? This is easy to implement. Or do we want to keep the original capitalization? This is a little more work, since the original capitalization needs to be stored in the cache. Since all the examples in the README show lowercase, I'm going to assume that we want just lowercase.. I agree, if it were the case that pip actually cared about this. It doesn't. This is a wrapper tool for pip. Why should we care?. @hyperknot As I mentioned before:\n\nOne thing I can conclude is that underscores changing into dashes comes from Setuptools. This is part of a normalization process and it doesn't seem to be inconsistent or causing any problems.. Yes I have read a lot of code, but I could not really find where the caching actually takes place. This seems to be part of another package.. I do have a feeling we should document this case-insensitivity somewhere, although I have no idea where.. Case sensitivity is not the same as unchanged case. On the other hand, neither pip, nor pypi, nor pip-tools (as it uses pip itself) are case sensitive. Maybe it's better to either just get rid of this test, or rename it, or even write another one?. \n",
    "hgylfason": "The problem still persists.\nI'm not sure if it matters but my environment is in a virtual machine run managed by Vagrant.. - pip-tools version 1.9.0\n- pip version 9.0.1\n- python version 3.5.3\nrequirements.in\nIt doesn't seem to matter what is inside the requirements.in file. But here is a short sample that fails.\nDjango>=1.8,<1.9\ntraceback\n```\nssh://vagrant@127.0.0.1:2222/bin/python3.5 -u /usr/bin/pip-compile requirements.in\nTraceback (most recent call last):\n  File \"/usr/bin/pip-compile\", line 11, in \n    load_entry_point('pip-tools==1.9.0', 'console_scripts', 'pip-compile')()\n  File \"/usr/lib/python3.5/site-packages/click/core.py\", line 722, in call\n    return self.main(args, kwargs)\n  File \"/usr/lib/python3.5/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python3.5/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/lib/python3.5/site-packages/click/core.py\", line 535, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/lib/python3.5/site-packages/piptools/scripts/compile.py\", line 136, in cli\n    for ireq in ireqs\n  File \"/usr/lib/python3.5/site-packages/piptools/scripts/compile.py\", line 135, in \n    existing_pins = {key_from_req(ireq.req): ireq\n  File \"/usr/lib/python3.5/site-packages/pip/req/req_file.py\", line 93, in parse_requirements\n    for req in req_iter:\n  File \"/usr/lib/python3.5/site-packages/pip/req/req_file.py\", line 150, in process_line\n    cmdoptions.check_install_build_global(options, opts)\n  File \"/usr/lib/python3.5/site-packages/pip/cmdoptions.py\", line 56, in check_install_build_global\n    control = options.format_control\nAttributeError: 'Values' object has no attribute 'format_control'\nProcess finished with exit code 1\n```\nUpdate\nI think I've found what is causing the problem. I noticed after I removed the requirements.txt file then no error appeared. The offending line in requirements.txt is\nGDAL>=1.11,<1.12 --install-option=\"build_ext\" --install-option=\"-I/usr/include/gdal\"\nHope it helps.. ",
    "nazavode": "Exactly the same here. Tracked down the issue, the offending option is the --install-option=\"build_ext\", removing that seems to work.. ",
    "byjott": "I think the problem is the from __future__ import unicode_literals in piptools/_compat/tempfile.py, so this succeeds:\ntmpdir = TemporaryDirectory()\nassert isinstance(tmpdir.name, unicode)\nThis unicode directory name causes problems if trying to append a binary str via os.path.join which is not ASCII-encoded.\nI propose to either entirely remove the unicode_literals import from the above file or to explicitly use binary strings for the TemporaryDirectory.__init__ default values.. Yes, indeed, that's not enough, as apparently TemporaryDirectory is constructed with non-default arguments in a file also using unicode_literals (specifically from repositories/pypi.py:67).\nWhat finally did work for me was to edit piptools/_compat/tempfile.py and insert the following lines at the top of TemporaryDirectory.__init__:\nsuffix = suffix.encode()\nprefix = prefix.encode()\nif dir is not None:\n    dir = dir.encode()\nThis ensures the temporary directory name (self.name in this context) is a binary str, not unicode.. This looks very much like a duplicate of #433. ",
    "lefterisnik": "Hi @byjott, I tried both with no success. I get the same error . Also, pip-compile works fine when you build pip-tools under python3.\n@byjott I will try your solution and I will write back for feedback. @byjott, yes your solution works fine. thanks.. ",
    "federicobond": "I can reproduce the problem and confirm that the solution works fine.. ",
    "mhluongo": "@byjott It does look similar, but your fix for #433 isn't working (at least in our fork). @nvie if you need help maintaining, I'd be happy to put some time in.. ",
    "blr246": "I can't reproduce this anymore. Perhaps the referenced package was broken in some way?\nI'm closing this. If others following have another path to reproduce, then we can re-open or create a new issue.. ",
    "mlfz": "Thanks! My apology.. ",
    "di": "This also happens with pytest:\nNotADirectoryError: [Errno 20] Not a directory: '/var/folders/px/dlfyqxyx3mx0t_yx3dvbb9nc0000gn/T/tmp5ib4g7ldbuild/changelog/_template.rst'. Seems to be a duplicate of #438.. I think I'm experiencing this issue as well (at the time of writing, with pyramid which just released a prerelease). I tried pinning pip back to 18.0 and the issue persists, so it seems like this is coming from the pip-tools==3.0.1 release.\nI'm also having trouble reproducing this though, and the behavior I'm seeing is not that the prerelease version is getting pinned, but that a sub-dependency that has been removed in the pre-release is being removed from the compiled file.. Hmm, just noticed that @nbelakovski is using pip-tools==2.0.2 though... \u00af\\_(\u30c4)_/\u00af. ",
    "IvanAnishchuk": "@jdufresne This is an important fix for many people. Could you please fix the travis build?. @davidovich Yeah, that makes sense. Somewhere in _resolve_one_round() we could avoid adding unsafe requirements and their own requirements won't be added either.. I would like to revisit this issue, I think situation has slightly changed\nsince May and I think I could see the way for pip-compile to effectively\nreplace all other dependency management tools I tried with few simple changes.\nI could open a new issue but reusing this seems appropriate. Tell me if it's\nnot.\nLayering productions and dev requirements is something almost every project\nneeds but doesn't have. Competing tools like pipenv and poetry provide support\nfor it by default but there isn't yet a standard way to do it with pinned\nrequirements.txt files.\nI think currently-recommended way is to produce two pinned files:\nrequirements.txt and requirements-dev.txt (with the latter usually including\neverything from the former and adding some things) and there's a widely\nadopted practice to use two separate in-files e.g. requirements.in and\ndev-requirements.in with -r requirements.in in the latter one. It seems\nvaguely consistent with implementation and purpose of install_requires and\ntests_require and named extras_require in setup.py/setup.cfg (using an\nextra named dev is a way recommended for dev requirements at least\nhere)\nI'd propose to add a way to support compiling multiple sets of requirements be\nthey multiple in-files like requirements.in, requirements-test.in, and\nrequirements-tests.in or multiple sections in setup.{cfg,py} or pyproject.toml\nor some other project config format (let's not focus on particular formats for\nnow).\nIn my head, the ideal workflow should be like this:\n\n\nUser specify sets of requirements and their order somehow (two easy ways\n   are setup.cfg with install_requires, tests_require, extras_require and a\n   number of in-files spicfied in pip-compile command line, both are ordered,\n   it's important).\n\n\nSome tool iterates over those sets and compiles each set of requirements\n   into one resulting requirements-something.txt file (using in-file name or\n   requirements.txt, requirements-tests.txt, requirements-.txt pattern)\n   with added constraints for already-compiled files i.e. if we are compiling\n   req.in, req-dev.in, and req-docs.in we would actually be compiling\n   -r req.in, -c req.txt -r req-dev.in, and -c req.txt -c rec-dev.txt -r\n   rec-docs.in respectively.\n\n\nDoes it make sense to build such functionality right into pip-compile or would\nwe be better off with having it in a separate tool (I mean one I'd support\nmyself)? I think it is possible to implement it in pip-compile without breaking\ncurrent default behaviour (unless we decide that we want to break it). Like,\nperhaps, -m or --multi option that would enable generating multiple\nresulting .txt files from multiple source .in files or multiple sections in\nsetup.cfg\nUsing -c means we don't duplicate requirements in generating unless there's\na reason to and the generated sets are all compatible with each other while\nthey don't necessarily include each other. For example, for compiling docs you\ncan theoretically install only things in requirements-docs.txt without\ninstalling all your project deps. And you can always install -r requirements.txt\n-r requirements-tests.txt when your tests require project deps. Granular control\nand consistent results, ain't that good?\nI had a thought whether it makes sense to just add explicit -c lines to the\nin-files themselves and not to change any code but that coould break if the\nuser doesn't compile files in the right order (results could be very\ninconsistent). So, unless some robust automation wrapper is provided, I'd guess\nit would break pretty often in this real world. So why not to include it\nwith pip-tools?\nAre there any reasons not to do it like this? Any ideas of how to implement\nrequirements-layering better?. Ah, I can't edit pull requests to switch forks :( I guess it should be separate ticket after all. I'd also add some support for multiple sections in setup.py/setup.cfg (install_requires, tests_require, multiple extras_require) translating them (loosely) into in-files with additional constraints added for files already compiled (so that resolved versions are not conflicting). I guess the recompile approach with parsing output files should work for that as well as long as the order of compilation is preserved. Perhaps we could add some information on that into the same file header?\nI originally posted a longer comment in #492 but that was before I read this discussion. I would like to somehow achieve both better setup.py parsing and multiple-in-files compilation as parts of the same generalized workflow.\nUPD: I'm against using a makefile, I think this shouldn't depend on anything other than python and pip-tools. It could be a separate package, of course, but I think dependency layering is a common-enough practice that could benefit many pip-tools users if not most of them.\nUPD2: We probably don't even need to add any custom header. If we have explicit -c lines we can use them to establish the dependency order of the files. I guess that's it, we just need the right way to add -c lines to inter-dependent in-files (that would be automated for setup.cfg) and a recompile script to go over the txt files and recompile them (in the correct order). And if the txt files are yet present we would just make some safe enough default assumptions and allow the user to run pip-compile manually for more granular control. Easy peasy, I guess I'll just start implementing it and stop bothering you guys :). Thank you @merwok seems like exactly the tool I need. Except like many others it isn't aware about constraint files. But maybe I can modify it at least instead of implementing my own from scratch.. ",
    "emilecaron": "woops. wrong repo. sry.. ",
    "crdoconnor": "Catching the subprocess error was to stop it from spewing ugly and unnecessary stacktrace for decidedly unexceptional behavior.\nI included pkg-resources because it was a workaround that prevented the tool from being completely broken on ubuntu.\nI've since switched to pipenv though, so, moot points.. ",
    "bdeeney": "@nvie I also find pip-tools very valuable and would be willing to help maintain it.. ",
    "cjauvin": "I created a fake package as is mentioned in the steps to reproduce the problem, and carefully traced pip-compile both before and after @davidovich's patch, and I can confirm that it indeed solves a real (and subtle) issue.. I have been studying what is proposed, and it makes sense to me. Basically it boils down to:\n\nreq.specifier is better/more stable than req.specs (it's the one we want to use) \nIt was clearly introduced in pip 8.0.0, as the source shows\n\nSince pip-tools only supports pip > 8.0.0, then it makes sense to remove any reference to req.specs, and forget that it even exists!. ",
    "suutari": "This is also rebased onto master now. Any reviewers?\nThis feature will be useful with the feature implemented in PR #464.. Rebased and added some tests too.. Rebased this on to current master. Should be good for merging. Any reviewers?. In other Python projects I know fixtures are usually code that create some object instances for testing. The reason for the change of that name is that a directory named fixtures could also contain some Python code which could have tests/doctests and therefore shouldn't be ignored by pytest, but the current contents of the fixtures directory seems to be only data which can be ignored, and the setup.py files there must be ignored.\nAnyway, now that I look more carefully, it seems that there is also some other data than the \"fake PyPI\", so you're right, fake_pypi isn't good choice for the name. How about test_data then?. @davidovich How about now?. So this makes it possible to unpack distribution packages with conflicting files inside them.\nI'm just thinking that why the code unpacks the downloaded packages in the first place, since it only needs to hash the byte contents of the package. I think the code should be simplified to not do any unpacking, but rather just hash the file contents. This should fix the bug and also make it faster and smarter since unnecessary unpacking is avoided.\nI already tested this with Prequ. I'll create a PR for this to pip-tools too.. I have now created the PR for doing the hashing without unpacking as mentioned. See PR #557.\n@mete0r: I also included your test case for this. Thanks! And it seems to pass too. :). Can this be merged, please?. This is already implemented in PR #453, but it's still unmerged.\n. Does it make a difference if you specify the conditional dependency in the install_requires argument?\nHere's an example:\npython\nsetup(\n    name='mypackage',\n    install_requires=[\n        'python-cjson ; platform_python_implementation==\"CPython\"',\n        ...\n    ]). The culprit is a missing comma at the end of this line: https://github.com/pallets/click/blob/55682f6f5348f5220a557f89c3a796321a52aebf/click/init.py#L69\nThe line ends with string literal 'FloatRange' which is then concatenated with the next string literal 'echo' on line 72, because there is no comma in between.\n. I think this is a good change. IMHO being compatible with Pip 10 is better than vendoring old Pip, since vendoring always has its problems. (E.g. we would have to backport all security and bug fixes from upstream Pip, etc. The PyPI URL change being a good example.)\nIt seems that the build matrix doesn't include Pip 9 anymore though. Can you add it back with Pip 8 and 10 so we could see if all tests pass on other Pip version than 10 too?. @vphilippon I see you as the lead maintainer of pip-tools currently, so IMHO it's your call if the Pip should be vendored or not. I'd vote for not vendoring, if anyone cares.. @tysonclugg: It's .appveyor.yml here: https://github.com/jazzband/pip-tools/blob/master/.appveyor.yml. For other Pip 10 fixes see also following two commits: https://github.com/suutari/prequ/commit/deeac2cb5467e487b66c997c43675ca6f86ffcca and https://github.com/suutari/prequ/commit/3a8dd240a63c685bd4b3ee0e0cedfd7bfd4882a4. The output shows that your pip-compile has somehow cached the\nrequirements of flake8 as mccabe<0.7.0,>=0.6.0,\npycodestyle<2.5.0,>=2.4.0, pyflakes<1.7.0,>=1.5.0, but looking into\nto the released packages in PyPI the pycodestyle requirement is there\n<2.4.0.\nI think the relevant change to flake8 requirements happened in\nhttps://gitlab.com/pycqa/flake8/commit/0273ca561f0ad03adff41ce5d95a1ec31b10fe5a\nSo maybe you have tried to add flake8 as Git URL to your\nrequirements.in and then pip-tools cached the development\nrequirements? Check the contents of\n~/.cache/pip-tools/depcache-py3.7.json to verify this. If this is\nthe case, then removing that file should resolve your issue.\n. The no_index flag is passed to the get_pip_options_and_pypi_repository call which then passes it along to the finder in the constructed repository. That'll make the repository.finder.index_urls to be empty if no_index is True. It'll also consider --no-index specified in the parsed txt file.. The point here is that the call to pip.req.parse_requirements above (line 56) will update the flags in the repository.finder (which is passed to it) according to the flags specified in the txt file. Therefore the repository.finder.index_urls will contain combination of command line options and txt file options and this is then used here to construct the options given to pip install.. I don't understand what you mean by the \"implementation detail\", since there wasn't really any separation between implementation and interface here before my changes either. If implementation details should be hidden, then there should be a class like RequirementsFileParser which then could be used to parse the txt files and as a result would output the list of requirements and pip options from the parsed files. That would be nice improvement, but completely another issue.\nCan you elaborate what do you mean by exposing the implementation detail here?\n  . Why is this changed?\nIIRC changing this makes installed packages affect how the requirements are compiled, which is not a good thing.. Also PIP=18.0 should be tested. The master branch is not equal to the 18.0 version anymore, e.g. currently in master the Command class lives in pip._internal.cli.base_command, but in Pip 18.0 it's still in pip._internal.basecommand.. I think this doesn't work with Pip 18.0.. Ah, OK, great that it works. Sorry for the incorrect analysis.\nGood work! :+1: . You're right, I forgot that latest==18 (currently).. ",
    "kanemra": "No, this breaks updates if *.in file changed.  . ",
    "dschaller": "cc @nvie . cc @nvie. cc @davidovich. cc @jdufresne . @jdufresne no worries! I wanted to tag you in case I was missing a use case that isn't tested.\nThe command I'm running is pip-compile --allow-unsafe --no-index -o piptools_requirements.txt piptools_requirements.in which correctly pins pip.\n```\nThe following packages are considered to be unsafe in a requirements file:\npip==9.0.1\n```\nBut if I run pip-compile --no-index -o requirements.txt requirements.in pip also gets pinned when it shouldn't.\n```\nThe following packages are considered to be unsafe in a requirements file:\npip==9.0.1\n```\n@davidovich not trying to imply there are \"sides\" here. Just wanted to fix a bug ;) happy to fork and fix in that.. @jdufresne \npip==9.0.1. @jdufresne I do a first pass with pip pinned and then a second pass including the generated file from the first pass.\nEven if this seems odd the functionality is broken. If someone has a unsafe library in their requirements and --allow-unsafe is not set it should not be pinned in the generated requirements file.. > The spirit of --allow-unsafe is to control if unsafe sub-dependencies are output to the requirements.txt, not to filter a user input which is the user desired state of wanted dependencies. \n@davidovich this is wrong. If you look at the PR to add the flag (https://github.com/jazzband/pip-tools/pull/377) the description says,\n\nDue to some specifics I had to include some version of pip in dependencies in several projects.\nI am not sure it is needed for everyone, so I added a flag to have an option to enable that.\n\nThis is exactly what we are doing and what this pull request fixes.... Given that all the criteria in the description are satisfied, none of the feedback is critical, and most importantly this fixes a bug with intended functionality I propose that we merge this.. @davidovich happy to chat more about the changes. My apologies if that is the way my comment came off as it was not my intent. By critical I meant the inline comments left by you were mostly style ones and they seem like a personal preference.\nPlease help me understand how I can I better help you understand how this is broken.\n\nThe problem is that if you put something in the input, it is not for pip-tools to decide to remove that wanted state.\n\nPip-tools, up until the last release with the change in it, would remove unsafe dependencies from the generated requirements files even if they were part of the original requirements unless the --allow-unsafe flag was set. I guess a better question here is was this functionality unintended then?\nAs far as your example goes it seems counter-intuitive that you would want pip-tools, a tool whose entire purpose is to pin all underlying dependencies, to not pin the underlying dependencies of unsafe packages when they are allowed. Can you help me understand why you wouldn't want this?. @davidovich @vphilippon @jdufresne opened https://github.com/jazzband/pip-tools/pull/517/ to hopefully fix this bug while preserving the additions in #441. Please take a look when you get a chance. Going to merge this by EOD since, as the contributing doc states, it's just documentation updates but I would love feedback if anyone has some.. @rpkilby a style checker is a great idea. However, even if that were added these changes would be required.\n@davidovich the value is consistency which is the entire point of PEP standards. Maybe give this page a read. :D. @jdufresne @vphilippon @davidovich any chance one of you could take a look?. Updated @jdufresne @rpkilby . Ran a few things to test these changes:\nSetup:\nbash\nmktmpenv\npip install pip-tools\nmkdir pkg && cd pkg\ncat <<EOF > setup.py\nfrom setuptools import setup\nsetup(install_requires=['setuptools'])\nEOF\ncd ..\necho \"-e pkg\" | pip-compile -n -o r.txt -\nRun:\n```bash\n\necho \"-e pkg\" | pip-compile -n -o r.txt - --allow-unsafe\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file r.txt -\n\n-e file:///Users/dschaller/Desktop/testing/pkg\nappdirs==1.4.3            # via setuptools\npackaging==16.8           # via setuptools\npyparsing==2.2.0          # via packaging\nsix==1.10.0               # via packaging, setuptools\nThe following packages are considered to be unsafe in a requirements file:\nsetuptools==35.0.2\nbash\n\necho \"-e pkg\" | pip-compile -n -o r.txt -\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file r.txt -\n\n-e file:///Users/dschaller/Desktop/testing/pkg\n**Setup:**bash\nrequests==2.0.0\nsetuptools==35.0.2\npip==8.0.2\n```\nRun:\n```bash\n\npip-compile --allow-unsafe\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nappdirs==1.4.3            # via setuptools\npackaging==16.8           # via setuptools\npyparsing==2.2.0          # via packaging\nrequests==2.0.0\nsix==1.10.0               # via packaging, setuptools\nThe following packages are considered to be unsafe in a requirements file:\npip==8.0.2\nsetuptools==35.0.2\nbash\n\npip-compile\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nrequests==2.0.0\nThe following packages are considered to be unsafe in a requirements file:\npip\nsetuptools\n```\n. @davidovich @vphilippon @jdufresne please take a look at this approach. It fixes the bug and maintains existing functionality.. @davidovich there were several comments on the last pull request that outlined why this is a valid use case. The entire point of the flag is to pin these dependencies as noted in the help output. It does not say that this is only for transitive dependencies.\nAlso, if this functionality is no longer supported you should have released a major version change following semantic guidelines since this is a backward incompatible change in behavior.. Having unsafe dependencies in your requirements source allows for users to create generated requirements files that can be used in different environments where pining these packages is either wanted or not.\ne.g.\n```bash\n\necho requirements.in\nrequests==2.0.0\nsetuptools==35.0.2\npip==8.0.2\npip-compile requirements.in -o a-requirements.txt\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file a-requirements.txt requirements.in\n\nrequests==2.0.0\nThe following packages are considered to be unsafe in a requirements file:\npip\nsetuptools\n\npip-compile requirements.in -o b-requirements.txt --allow-unsafe\n\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file b-requirements.txt requirements.in\n\nappdirs==1.4.3            # via setuptools\npackaging==16.8           # via setuptools\npyparsing==2.2.0          # via packaging\nrequests==2.0.0\nsix==1.10.0               # via packaging, setuptools\nThe following packages are considered to be unsafe in a requirements file:\npip==8.0.2\nsetuptools==35.0.2\nEnvironment A:\n\npip install a-requirements.txt\nEnvironment B:\npip install b-requirements.txt\n```\n\nThis allows for a single source of truth on what requirements are needed for a project while also not forcing people to consume what are considered unsafe packages when it doesn't matter if these are pinned.\nIf you look at the comments on the original PR there are at least two other people who had use-cases that required this flag as well.. By yielding a class that I can invoke later I am allowing for keyword arguments to be passed in so that I can reuse this fixture while setting allow_unsafe.\nI'll update the fixture annotation. Unsafe packages were being pinned in requirements files even when --allow-unsafe wasn't set instead of being comments. Can you provide a reason why?. Unit tests should test functionality in isolation of other modules assuming that those modules are tested independently.. 'marker' is an  arbitrary value used for testing purposes.. For the sake of argument it makes it harder for discoverability but it makes usage much clearer because you aren't allowing for usages to rely on position.\nIn terms of discoverability of arguments there should be documentation on the function with these values.. I think it's safer to fall back to the unmodified line at the catch all.. I did consider formatting the output but figured the overhead wasn't worth it since we wouldn't be able to have it identical to the original and the list output is still consumable. Happy to update if you think it's worth it @jdufresne . ",
    "underyx": "Any chance of a 1.9.0 release soon? The merged https://github.com/jazzband/pip-tools/pull/441 would be super useful for us (and seemingly a lot of others, too.)\nThere's this issue in the latest pip release, https://github.com/pypa/pip/issues/4264, which causes pip to fail when uninstalling appdirs (a setuptools dependency.) It takes quite a bit of duct tape to keep using pip-tools like this.. @davidovich yep, I can confirm that it's fixed on master. The RC will not be necessary \u2014 I can just keep using the commit from the latest master. It'd be a bit more inconvenient however to get everyone at the company to update each of their virtualenvs to a prerelease to make sure nobody overwrites requirements.txts with versions that have appdirs in them.. ",
    "myw": "@davidovich Since you've mentioned it's a bit tricky to gather a sense of what will and won't break things when you're taking on an existing project like this, maybe the next release, currently called (1.9) should be 2.0? then people will know they have a risk of breaking when they upgrade, however low it may be, and do so a little more gingerly? it has a lot of new features already. Specifically, the one I'm interested in (pulling from setup.py) is a game-changer, in that it lets you use pip-tools in an existing project without the overhead of managing a separate requirements.in file. I wouldn't mind upgrading to 2.0 for the sake of having that feature.\nSetting the next release at 2.0 lets you make the decision around finalization more easily\u2014namely, in feature-space, not in issue-space. Once 2.0 is set, you can then start making smaller releases for the remaining features, adding them in one or two at a time, in minor version bumps, minimizing the chance of breakage. . but if requirements.txt represents the complete state of things in a\nvirtualenv, i think it should be nailing that stuff down. I agree that you\ncan make arguments either way about a reasonable set of defaults\u2014i'm happy\nto add flags for the other two so users can make their own calls.\nbut the functionality should be there, IMO\n--\nMisha\nOn Wed, Apr 12, 2017 at 2:28 PM, David Genest notifications@github.com\nwrote:\n\nI'm not certain that the default would be to include test_requires and\nsetup_requires.\nsetup_requires list dependencies that is needed at install time only (and\nis managed solely by setuptools)\ntest_requires are not related to the runtime of the application.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/pull/492#issuecomment-293666859,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAKWwlFnN7tqK5p3QsdnCGwDxeqhpKpfks5rvRfYgaJpZM4M7v-K\n.\n. My use case is inside a Docker container, not with venv on a regular system\u2014in that scenario, it's very helpful to have one pinned-down requirements.txt file from which you install all dependencies before installing the app you are deploying.\n\nA common pattern looks something like this:\nDockerfile\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . ./\nRUN pip install -e .  # or python setup.py develop\nHere, all of the requirements are installed in their pinned-down versions first, and then the second pip install doesn't actually install any additional packages (unless there are discrepancies between the install_requires field in setup.py and requirements.txt, but that's a bug).\nThe benefit of specifying setup.py as the single place for all abstract requirements  that generate the specific, pinned-down requirements in requirements.txt is that no data is duplicated (there's no need to add an additional requirements.in file), but your abstract dependencies cover all of the use cases you'd need, much like what would happen with requirements.in.\nNow in that scenario, if I want to upgrade a package, pip-compile -U foo --include_tests_require will do it for me, regardless of whether it's a test framework or an install dependency. This is, imo, the way it should be.\nI can definitely see why, given the venv behavior, enabling tests, setup, and potentially extras should not be the default. But I still think adding those options is a valid use case for a Docker environment, which is now common in app deployment, and only growing more popular.\nI won't push for adding it in if there's wide-spread consensus that even behind an option, this is the wrong direction. But I think I've outlined why it is useful, and I'll happily do the work to make this a full-fledged feature (the first commit was basically just a thought experiment).. The need to create an entire new container for testing is not really the kind of constraint you want to impose on your users. In our use case, for example, this does not work.\nMultiple requirement files can be OK, but the problem is that every pip install invocation is independent, so a downstream pip install could overwrite packages installed by the upstream one instead of resolving the of the requirements correctly down to a single, explicitly specified satisfactory version of all the packages you need, which is another big benefit of pip-tools.\nIdeally, pip-tools would basically make pip work like yarn, with a single place that defines the abstract dependencies, which programmatically get resolved and converted into specific dependencies for a system. It almost does that. This potential feature, once implemented with the right defaults and cleaned up, could help it accomplish  that goal, without resorting to additional files for duplicate specification of dependencies (setup.py vs requirements.in).. ",
    "mattlong": "+1. 1.8.1 is resulting in dependencies of -e reqs being missed. This is a pretty bad backwards-imcompatible regression.. No, it is new in 1.8.1. After some digging, I believe I've found the cause. Bear with me as I may be using incorrect terminology at times since I'm not very familiar with the internals of pip.\nFor editable reqs, the resolver always calls self.repository.get_dependencies(ireq) whenever it needs to get the dependencies of the package ireq. Under the covers, get_dependencies calls reqset._prepare_file(self.finder, ireq) which is where we jump into pip internals since reqset is an instance of pip.req.req_set import RequirementSet.\n_prepare_file sets ireq.prepared = True meaning that on subsequent calls with the same ireq object, an empty list will always be returned. Thus, we lose  the dependencies of an editable req when the same ireq obj is used to represent them on subsequent resolver rounds. The quickest fix I can think of would be to set ireq.prepared = False before calling _prepare_file to avoid its short-circuit behavior, but that feels pretty fragile. Seems like it would be cleaner and much more performant to just memoize repositories.pypi.PyPIRepository.get_dependencies.. @davidovich Great! Will try to get a test case in today!. @davidovich Added some tests! It wasn't as straightforward as I think either of us thought it might be since tests.conftest.FakeRepository fakes the very function that I would need to be testing: get_dependencies. I'm of course open to any suggestsions you might have; but it should be good to go as is.. It might be better to key this cache off some derived key of ireq rather than using ireq itself, but I was unclear what would be the optimal key to use. Open to suggestions!. Extracted this bit of code into a separate function to facilitate testing. ",
    "SylvainDe": "Hi @davidovich , to be honest, this is probably a topic you are more familiar with than I am so I can't really help you.\nHowever, I'm happy to update my change to keep only 3.6 if this may interest you. Otherwise, feel free to close the PR.. ",
    "asottile": "If you're curious, here's how a tool (that I've worked on) (which does a similar thing to pip-sync) does this. ",
    "jayfk": "One thing I forgot to add: In terms of dependencies, both tools are largely compatible. Safety pulls in requests, but we could probably make this an extra with pip-tools[safety] anyway.\n. > I believe not every package will adhere to the safety-db, hence making this feature only applicable on a subset of packages.\nI'm not entirely sure what you mean by that. There's nothing to adhere to, packages are added largely by hand. Could you elaborate?. > Because the db is updated by hand (as I understand, the package maintainer must opt-in, hence adhere), it cannot be perfect, hence giving an --upgrade-insecure option the possibility to lie for packages not part of the db.\nAh, that's the part I was missing. The package maintainer has nothing to do with that. It's a system that monitors every package on PyPi for changelogs and commit messages and flags everything that could be related to some kind of security issue. 97% are false positives, so every entry is reviewed by hand. That's what I was referring to.\nI agree with you that this kind of databases are never perfect. There will always be false positives or packages missing. If you take the CVE db for example, there's very little on Python packages in general. There are a couple of entries for Django, the Zope universe and 1-2 on requests. That's about it. I wouldn't conclude that this gives a false security assessment, tough. It should be used as something to check against, not as some kind of insurance or an ultimate source of truth.\nI've created a dataset a while ago that contains almost every requirements file (in common places) on public GitHub repos and ran some statistics on it. I don't have exact numbers in my head, but close to 50% of all requirement files had at least one dependency with a known and documented security vulnerability. This has also to do with the tools we have available. People just don't know about the security issues.\nThis is where the idea was born to make this an optional thing for a widely available tool I'm using myself :).\n. > Either way, I'd be hesitant to add a dependency on a service that is not part of python's core infrastructure, especially one that seems to have a bus factor of 1.\nHelp out :). I have a pending PR that adds a feature to warehouse (the new PyPi) allowing to deprecate a package and/or flag is as insecure: https://github.com/pypa/warehouse/pull/1462 That's something Safety could use in the long run.\nI'd be happy to work with more people on securing dependencies and to make packaging a safer place in general. It has been a pretty lonely road so far, but there's plenty room available.. ",
    "craigds": "I've added some tests \ud83d\ude04  I added mock to the tox config, since it was the easiest way to mock out the subprocess (pip) calls.\nI should point out that I've realised that pip-tools actually reinstalls dependencies of editable packages as well as the editable packages themselves.\nFor my use-case that's not a big deal, though it'd be nice to fix. Though I'm not sure where to start with it.. ",
    "e-kolpakov": "Uhm, sorry for the noise, looks like it was caused by outdated setuptools version: pip install -U setuptools solved the problem.\nMight be good to include this into troubleshooting or even actual error message thrown there.. ",
    "pmatos": "Actually eve-mongoengine is the problematic package and you just need to have that in the requirements for pip-compile to blowup.. OK, I have updated, retried and saw no more failures.. ",
    "j-walker23": "Do you have oauth2client==4.0.0? Mine also fails unless i go down to oauth2client==3.0.0. I can not find any way to force pip-compile. Is there a way?. Thanks for your help @davidovich!. ",
    "scjody": "@j-walker23 The machine I'm currently trying to run pip-compile on has oauth2client==3.0.0. Upgrading to 4.0.0 or removing oauth2client completely does not affect my ability to run pip-compile as shown above.. There's already a google-cloud issue open - see above.\nI thought that any dependencies that pip could deal with would also work with pip-compile. Sorry I was mistaken.. ",
    "ryan-lane": "@jdufresne I think the idea is that if an end-user puts pip into requirements.in, that we strip it unless --allow-unsafe is set. People sometimes put unsafe things into their .in file and we're ensuring that by default we try to do the right thing. The --allow-unsafe is there to let the end-user say \"I know what I'm doing\".\nBasically, this PR just makes piptools work as advertised, based on the arguments.. @davidovich as far as I can tell the behavior that @dschaller is putting back in was the original intended behavior. A change came in that broke the intended behavior and changed it to match what you'd prefer. A change in behavior, even if it is better in your view, is not great as it's a breaking change for every user who was relying on the old behavior.\nWe should make sure the current behavior matches the old behavior, then we should look into how we can make everyone happy. Right now there's a regression and the right thing to do is fix the regression.. ",
    "jkimbo": "@davidovich I've added a test for the sync command. Had to jump through a couple of hoops to get the command to run but not clobber my virtualenv! Anyway let me know if you had any better ideas on how to test it.. @vphilippon rebased on master. Done. ",
    "ned2": "Ah, somehow I missed that. This of course makes plenty of sense as the default behaviour. Thanks! . ",
    "offbyone": "Well, in part because setuptools allows it; if you think about it, it's not\nfar from a requirements.txt idiom.\nOn Mon, May 22, 2017 at 6:15 PM Ryan P Kilby notifications@github.com\nwrote:\n\nOut of curiosity, is there a reason why you're using a multiline string\nhere? Generally, the expected usage is a list of strings.\nMy guess is that the code is doing a for-each over the string, and failing\nonce it tries to evaluate '>'.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/jazzband/pip-tools/issues/511#issuecomment-303262598,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AALFvfmleSDoYB_9ocZMdaT71FDTqDSqks5r8jNBgaJpZM4NhJXt\n.\n--\nChris R\noffby1 on Twitter, offbyone on github\nNot to be taken literally, internally, or seriously\n. \n",
    "proinsias": "I encountered the same error \u2013\u00a0I believe it's matplotlib \u2013\u00a0see https://github.com/jheinen/matplotlib/blob/master/LICENSE/LICENSE_STIX .. I agree that a progress bar would be useful for normal operations. I would also suggest verbose output similar to the verbose output of hashin when the -v input flag is enabled. . ",
    "cooperlees": "I hit this error too. I thank @DeastinY to having the exception in the issue to make this much more Google-a-ble ... Save me manual debugging of the code :) <3\nCan confirm 1.11.0 fixed my problems.. ",
    "jmfederico": "Hi, yes, completely aware of the virtual environment workflow. That I use for pretty much everything, except for when working with Docker.\nThis is a project I'm developing in a Docker image, NOT based on the official Python image, but from a clean alpine image, and Python3 installed with apk package manager.\nI am thinking of using a virtualenv for the Docker image, but still thought I should report this.. ",
    "kaeptmblaubaer1000": "A progress bar would still be nice.. Or use console_scripts.pip entry point and execute it inside the script.. Awesome, this even fixes another issue I had (importing piptools and running it inside another script)!. When can I expect a release including this fix?. Does this also show the downloading progress of the files to hash or only hashing itself?. ",
    "atugushev": "Hello @kaeptmblaubaer1000,\nI've added a PR #743 for that feature. I hope it'll be merged.. I got the same log by these steps with asyncoro package using Docker:\nbash\n$ docker run --rm -it python:3.7 /bin/sh\n$ pip uninstall wheel\n$ pip install pip-tools\n$ echo \"asyncoro\" > requirements.txt\n$ pip-sync\n$ echo \"Exit code: $?\"\nthe output is:\n```log\nCollecting asyncoro (from -r /tmp/tmp_53lwfsx (line 1))\n  Using cached https://files.pythonhosted.org/packages/fd/79/1042b121f40be81cd634b9e42242623000b68f401397bcbc42c067296a93/asyncoro-4.5.6.tar.gz\nBuilding wheels for collected packages: asyncoro\n  Running setup.py bdist_wheel for asyncoro ... error\n  Complete output from command /usr/local/bin/python -u -c \"import setuptools, tokenize;file='/tmp/pip-install-qq5ulpmf/asyncoro/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-r77an25r --python-tag cp37:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\nerror: invalid command 'bdist_wheel'\n\nFailed building wheel for asyncoro\n  Running setup.py clean for asyncoro\nFailed to build asyncoro\nInstalling collected packages: asyncoro\n  Running setup.py install for asyncoro ... done\nSuccessfully installed asyncoro-4.5.6\nExit code: 0\n```\nAs you can see pip couldn't build wheels, but installed the package anyway via setup.py install, that's why here exit code is 0. Anyways, I can't reproduce it with non-zero exit code (even on OSX). \n@vlovich is there any chance to get full log from pip-sync?\n. Thanks for the reports!\nI've prepared a PR to fix this issue. Anyways, there is a workaround: pip-sync --user. . Hello @nbelakovski,\nThanks for the good catch. I've been digging into it and have found this PR #179. Perhaps this behavior is correct.. According to https://pip.pypa.io/en/stable/reference/pip_install/#pre-release-versions\n\nIf a Requirement specifier includes a pre-release or development version (e.g. >=0.0.dev0) then pip will allow pre-release and development versions for that requirement. This does not include the != flag.\n\nI've tested that case directly with pip install -r:\n$ pip --version\npip 19.0.1 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)\n$ echo \"coverage>=4.0a6\" > requirements.txt\n$ pip install -r requirements.txt\n...\nSuccessfully installed coverage-5.0a4\nOn the other hand:\n$ pip install coverage>=4.0a6\n...\nSuccessfully installed coverage-4.5.2\n. @nbelakovski \nYes, I suppose so. Anyways, thank you for the report. I found it interesting.. I've tried to install the editable package:\n$ cat requirements.txt\n-e git+https://github.com/lock8/django-rest-framework.git@d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6#egg=djangorestframework --hash=sha256:8a25e5ea1727e83bb55c3459b1116161ebf67314696672227a053265564c6af9\nand got this error:\n$ pip install -r requirements.txt --require-hashes\nObtaining djangorestframework from git+https://github.com/lock8/django-rest-framework.git@d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6#egg=djangorestframework (from -r reqs.txt (line 1))\nThe editable requirement djangorestframework from git+https://github.com/lock8/django-rest-framework.git@d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6#egg=djangorestframework (from -r reqs.txt (line 1)) cannot be installed when requiring hashes, because there is no single file to hash.\n@blueyed it seems there is no way to pip install editable packages with hash compare. See the issue in pip.\n. @blueyed \nok, i've tried this one:\n$ cat requirements.in\nhttps://github.com/lock8/django-rest-framework/archive/d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6.tar.gz#egg=djangorestframework\nand got this error:\n$ pip-compile requirements.in\npiptools.exceptions.UnsupportedConstraint: pip-compile does not support URLs as packages, unless they are editable. Perhaps add -e option? (constraint was: djangorestframework from https://github.com/lock8/django-rest-framework/archive/d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6.tar.gz#egg=djangorestframework (from -r requirements.in (line 1))\nJust wandering how did you compile non-editable URL package?. @blueyed \nFor -e https://... i got:\npip._internal.exceptions.InstallationError: https://github.com/lock8/django-rest-framework/archive/d9ee7d68178a6b50b55caacdb50a531b2cc0eaf6.tar.gz#egg=djangorestframework should either be a path to a local project or a VCS url beginning with svn+, git+, hg+, or bzr+. Good luck! Feel free to reopen it if you'll need any further help.. As i can see it works like a charm. \nHere's the list index_urls:\nhttps://github.com/jazzband/pip-tools/blob/master/piptools/repositories/pypi.py#L53-L55\nLet's assume user did non pass --index-url, so first element pip_options.index_url would contain a value provided by pip. Pip provides a value from pip.conf or default PyPI.simple_url.\nCould you write a test to demonstrate your assumption?. Got your point. I'm not sure it's worth it. Anyways, the description could be rephrased.. Cannot reproduce on the same environment. I got this output:\n```\nResult of round 4: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\namqp==2.4.0               # via kombu\nbilliard==3.5.0.5         # via celery\ncelery==4.2.1\nkombu==4.2.2.post1        # via celery\npytz==2018.9              # via celery\nvine==1.2.0               # via amqp\n```. @wonderbeyond \nTry to run first: pip-compile --upgrade-package kombu. It will update the kombu version to the latest in the requirements.txt. After that you'll be able to run pip-compile.\nThe problem is that in your requirements.txt you have kombu==4.2.2 (which doesn't exist anymore) and pip-compile tries to download it from this file. Here you get the exception.. Cheers!. @richafrank thanks!. > Hmm, also no failed request in https://travis-ci.org/jazzband/pip-tools/requests.. \ud83d\ude15\nIt's really strange.. Should both badges be presented in README?. Okay, i'll close this PR then and move changes for coveralls to #711.. @blueyed let's check codecov. Have no clue why did not travis run.. @blueyed on the page https://travis-ci.org/jazzband/pip-tools it says \"This is not an active repository\".. Coverage percent is not correct. There is no piptools/sync.py file in the report. See https://codecov.io/gh/jazzband/pip-tools/tree/d264d750eabf018d181c51fa266d97d9d4d6f501/piptools :(\n@blueyed a quick search show that you had dealt with the same problem in the other projects.. It seems to me codecov has a problem with duplicate file names. I've tested on my fork-repo: i've added piptools/script/utils.py and as expected piptools/utils.py has beed disappeared from reports :(. Thanks for the report! It seems --process-dependency-links is not supported since https://github.com/pypa/pip/pull/6060. @impredicative \nIt is the GitHub's behavior by default. See https://github.blog/2013-05-14-closing-issues-via-pull-requests/.. @vphilippon \nThe issue is in after_success: skip. There should be after_success: true. See for details https://github.com/travis-ci/travis-ci/issues/8337. I've prepared a PR to fix that.\n. Hello @SpotlightKid,\nThat's right. I believe the release will be uploaded as soon as closed current issue.. Glad to help!. @blueyed @suutari-ai \nGuys, i've squashed commits into one. PR is ready to merge, if you approve it.. Thank you @vphilippon!. Thanks for the report!\nDuplicates to #714. I'd rather do it in an additive way. Inspired by pip's quiet and verbose, see https://pip.pypa.io/en/stable/reference/pip/?highlight=additive#cmdoption-q.\nfeel free to use this patch\n```patch\ndiff --git a/piptools/logging.py b/piptools/logging.py\nindex 98f0528..f0bd178 100644\n--- a/piptools/logging.py\n+++ b/piptools/logging.py\n@@ -8,18 +8,19 @@ from . import click\nclass LogContext(object):\n-    def init(self, verbose=False):\n-        self.verbose = verbose\n+    def init(self, verbosity=0):\n+        self.verbosity = verbosity\n def log(self, *args, **kwargs):\n     click.secho(*args, **kwargs)\n\n def debug(self, *args, **kwargs):\n\n\nif self.verbose:\n\nif self.verbosity >= 1:\n             self.log(args, *kwargs)\ndef info(self, args, kwargs):\n-        self.log(*args, kwargs)\n+        if self.verbosity >= 0:\n+            self.log(args, **kwargs)\ndef warning(self, args, *kwargs):\n     kwargs.setdefault('fg', 'yellow')\ndiff --git a/piptools/scripts/compile.py b/piptools/scripts/compile.py\nindex cd6fe93..d24983c 100755\n--- a/piptools/scripts/compile.py\n+++ b/piptools/scripts/compile.py\n@@ -31,7 +31,8 @@ class PipCommand(Command):\n\n\n@click.command()\n @click.version_option()\n-@click.option('-v', '--verbose', is_flag=True, help=\"Show more output\")\n+@click.option('-v', '--verbose', count=True, help=\"Show more output\")\n+@click.option('-q', '--quiet', count=True, help=\"Show less output\")\n @click.option('-n', '--dry-run', is_flag=True, help=\"Only show what would happen, don't change anything\")\n @click.option('-p', '--pre', is_flag=True, default=None, help=\"Allow resolving to prereleases (default is not)\")\n @click.option('-r', '--rebuild', is_flag=True, help=\"Clear any caches upfront, rebuild from scratch\")\n@@ -65,12 +66,12 @@ class PipCommand(Command):\n @click.option('--max-rounds', default=10,\n               help=\"Maximum number of rounds before resolving the requirements aborts.\")\n @click.argument('src_files', nargs=-1, type=click.Path(exists=True, allow_dash=True))\n-def cli(verbose, dry_run, pre, rebuild, find_links, index_url, extra_index_url,\n+def cli(verbose, quiet, dry_run, pre, rebuild, find_links, index_url, extra_index_url,\n         cert, client_cert, trusted_host, header, index, emit_trusted_host, annotate,\n         upgrade, upgrade_packages, output_file, allow_unsafe, generate_hashes,\n         src_files, max_rounds):\n     \"\"\"Compiles requirements.txt from requirements.in specs.\"\"\"\n-    log.verbose = verbose\n+    log.verbosity = verbose - quiet\n if len(src_files) == 0:\n     if os.path.exists(DEFAULT_REQUIREMENTS_FILE):\n\n```. > And not be mutually exclusive?\nThat's right. Thanks @bendikro!. It raises here:\nhttps://github.com/pypa/pip/blob/master/src/pip/_vendor/pep517/_in_process.py#L36\nwith an error:\n\nNo module named 'poetry'\n\nFor some reason it wants poetry. Keep digging.. Right!\nIt doesn't go here:\nhttps://github.com/pypa/pip/blob/master/src/pip/_internal/operations/prepare.py#L119\nbecause should_isolate is False (because build_isolation is False). So it cannot build requires. I've hardcoded should_isolate=True and pip-compile went fine.. The main problem is here https://github.com/jazzband/pip-tools/blob/5029fcc4298def004a3d286aa3adfc312acb4efd/piptools/repositories/pypi.py#L164. let's investigate how pip download pendulum decides to build it in isolation mode. Ah it seems pip download builds in isolation mode by default.. > So that should probably be an optional flag to let consumers choose between installing required packages themselves, or doing isolated builds (which will take forever if there are lots of packages using different build systems).\nI guess so. Build isolation could be set on by default and an option to disable it:\n--no-build-isolation        Disable isolation when building a modern source. Gentle ping to @vphilippon.\nWhat do you think about this? Since pip builds sources in isolation by default, perhaps we should consider to do it as well. But it could be a \"breaking change\".. A) I don't think it is a bug, \"no isolation\" might be chosen for a special reason. Would be appreciated to get a comment from @techalchemy here.\nB) I totally agree. Users has to be warned if default behavior suddenly changes.\nThere is a soft way though. We could introduce --no-build-isolation and --build-isolation flags, where --build-isolation is default.. Hello @jkp,\nI've just prepared a PR #758  for the optional flag you've mentioned. I hope it'll be merged soon.. @vphilippon i think it's time to provide an optional flag to allow enabling/disabling build isolation. \"No build isolation\" still be the default behavior, but we could change it in a future releases (as a breaking change). WDYT?. Hello @miuliano, thanks for the report, i'll try to fix it ASAP.\nThank you @davidovich for the info! It should have been done the same as in compile.\n. That's because the release pyramid_jinja2==1.8 doesn't have i18n.py in a source code. See:\nhttps://github.com/Pylons/pyramid_jinja2/tree/23c6d918514cc89d70ee15c88dc2e097135fe1c9/pyramid_jinja2\nThe latest release 2.8 has that file.. No problem, happy to help.. Hello @tribbloid, thank you for the report!\nIt seems dronekit has pinned requirement future==0.15.2 in release 2.9.1, which conflicts with requirements.in. It's already fixed in master, see https://github.com/dronekit/dronekit-python/pull/873, but not released yet.\nAs a temporary workaround (before dronekit issue a release) i'd suggest to use in requirements.in the latest dronekit from git:\nfuture >= 0.16.0\n-e git+https://github.com/dronekit/dronekit-python.git@ed132c#egg=dronekit\ndronekit-sitl >= 3.2.0\npymavlink >= 2.2.8\nMAVProxy == 1.6.4\nsimplejson >= 3.10.0\nHope it helps!. Could you try pip-compile requirements.in -v?. No worries, happy to help!. Hello @ahmetb, \nSounds good to me.. Hello @vphilippon. I'm just wondering whether it is worth efforts, or \"don't touch if it works\".. @vphilippon Thank you for the feedback! Yes, that'd be better to split up the changes.. Hello @vphilippon,\nThe PR is ready to review. If you have a time take a look please.. Conflicts are resolved.. Hello @tribbloid,\nThank you for the issue report! Could you please describe what kind of an option do you request?. Since a branch src_files[0] == '-' was touched i took responsibility to test that case.. Hello @blueyed. Gentle ping. Is everything okay in the PR. Could you approve or request any changes if you'd like.. I've resolved conflicts, rebased and squashed commits. I'll merge this as soon as tests are passed.\n@blueyed, thanks for your help!. pip-tools v3.5.0 is released. Hello, @blueyed.\nI don't mind, actually. Which pip version would you suggest? Maybe the piplatest?. @blueyed \nThank you for the feedback! I've fixed the .travis.yml \ud83d\udc4d . @blueyed yes, thank you! It seems everything is okay now.. Hello @kaeptmblaubaer1000,\nThank you for the detailed report! The issue is probably here:\nhttps://github.com/jazzband/pip-tools/blob/200bc6bcc4950eb5628c1472638d66bfa7c860a2/piptools/sync.py#L140-L141\nSince pip uses python2.7 it would install packages to python2. I think pip-sync should decide to use pip2 or pip3, depending on python's version.. I've resolved conflicts, rebased and squashed commits. I'll merge this as soon as tests are passed.\n@blueyed as always, appreciate your feedback!. pip-tools v3.5.0 is released. Hello @y3g0r,\nThank you for the report! Funny to know that pip-tools can't compile itself :) \n/cc @blueyed. I've tried to investigate this issue and found that the main problem is here, see a stacktrace:\n```\n  /Users/albert/Projects/pip-tools/.venv/bin/pip-compile(11)()\n     10     sys.exit(\n---> 11         load_entry_point('pip-tools', 'console_scripts', 'pip-compile')()\n     12     )\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/click/core.py(764)call()\n    763         \"\"\"Alias for :meth:main.\"\"\"\n--> 764         return self.main(args, *kwargs)\n    765\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/click/core.py(717)main()\n    716                 with self.make_context(prog_name, args, **extra) as ctx:\n--> 717                     rv = self.invoke(ctx)\n    718                     if not standalone_mode:\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/click/core.py(956)invoke()\n    955         if self.callback is not None:\n--> 956             return ctx.invoke(self.callback, **ctx.params)\n    957\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/click/core.py(555)invoke()\n    554             with ctx:\n--> 555                 return callback(args, *kwargs)\n    556\n/Users/albert/Projects/pip-tools/piptools/scripts/compile.py(197)cli()\n    196                             clear_caches=rebuild, allow_unsafe=allow_unsafe)\n--> 197         results = resolver.resolve(max_rounds=max_rounds)\n    198         if generate_hashes:\n/Users/albert/Projects/pip-tools/piptools/resolver.py(101)resolve()\n    100             log.debug(magenta('{:^60}'.format('ROUND {}'.format(current_round))))\n--> 101             has_changed, best_matches = self._resolve_one_round()\n    102             log.debug('-' * 60)\n/Users/albert/Projects/pip-tools/piptools/resolver.py(198)_resolve_one_round()\n    197         for best_match in best_matches:\n--> 198             for dep in self._iter_dependencies(best_match):\n    199                 if self.allow_unsafe or dep.name not in UNSAFE_PACKAGES:\n/Users/albert/Projects/pip-tools/piptools/resolver.py(272)_iter_dependencies()\n    271         if ireq.editable:\n--> 272             for dependency in self.repository.get_dependencies(ireq):\n    273                 yield dependency\n/Users/albert/Projects/pip-tools/piptools/repositories/local.py(65)get_dependencies()\n     64     def get_dependencies(self, ireq):\n---> 65         return self.repository.get_dependencies(ireq)\n     66\n/Users/albert/Projects/pip-tools/piptools/repositories/pypi.py(211)get_dependencies()\n    210                 download_dir = None\n--> 211             elif ireq.link and not ireq.link.is_artifact:\n    212                 # No download_dir for VCS sources.  This also works around pip\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/pip/_internal/models/link.py(160)is_artifact()\n    159\n--> 160         if self.scheme in vcs.all_schemes: <----- !!!!!!!! here scheme is empty\n    161             return False\n\n/Users/albert/Projects/pip-tools/.venv/lib/python3.6/site-packages/pip/_internal/models/link.py(74)scheme()\n     73         # type: () -> str\n---> 74         return urllib_parse.urlsplit(self.url)[0]\n     75\n\nipdb> urllib_parse.urlsplit(self.url)\nSplitResult(scheme='', netloc='', path='git+git@github.com:jazzband/pip-tools', query='', fragment='egg=pip-tools')\nipdb> urllib_parse.urlsplit(self.url)[0]\n''\n```\nFor an ireq.link.is_artifact is True (ireq.link.scheme is empty), this block\nhttps://github.com/jazzband/pip-tools/blob/6d871f2461f2e11c24fc36a251b95c0db20775d8/piptools/repositories/pypi.py#L211-L214\nis not reachable. The form git+git@... causes the bug.. @y3g0r i've already reported a bug report to the pip project. Let's see where it goes.. Closed in favour of #765.. I've resolved conflicts. It's ready to merge now.. > Does this also show the downloading progress of the files to hash or only hashing itself?\n@kaeptmblaubaer1000 including the downloading progress.. Hello @m-aciek,\nThank you for the report! It's a good idea. I think the \"without setup.py\" section could be rephrased something like:\n\nIf you don't use setup.py (it's easy to write one), you can create a requirements.in (make sure you don't have a requirement.txt file, otherwise... etc) file to declare the Flask dependency:\n\n@vphilippon WDYT? English native speaker needed here :)\n. > We're having trouble (likely due to flags and number of builds) in pytest (and other projects) since months.\n@blueyed could you please tell me what kind of problems do you mean?. @blueyed \n\nSo in general it would be better to only run selective envs with coverage, by using a coverage factor in tox (see pytest's tox.ini).\nSee pytest-dev/pytest#4846 (comment).\n\nI see. Running selective envs with coverage sounds reasonable to me now. I'll try to test it and propose a PR soon.\n\nSo I suggest adding flags (\"windows\" / \"linux\" (or rather $TRAVIS_OS_NAME)) here after all.\n\nBuilds section looks pretty good now, see the latest codecov report. Thanks for the suggestion, Daniel!. Hello @NickG123,\nThank you for the report! It's probably related to the #721 issue. I couldn't reproduce it locally, could you change build_isolation  to True (it is False currently) in the <virtualenv>/lib/python3.7/site-packages/piptools/repositories/pypi.py file line 164 and try pip-compile again?. No worries! Thanks anyways for your contribution. I'll close this issue in favour of #721.. @blueyed i've rebased it. \nIt seems we have a regression bug from #731. See the failed build. At least we have a benefit from this PR :)\nI'll make a new PR with the fix and proper test ported from the current PR.\n. I've rebased it again. It's ready to review.. @blueyed thanks!. @blueyed \ni've just noticed that since https://travis-ci.org/jazzband/pip-tools/jobs/483526623 the tests are not run for pip==19.0.. > But slower on Travis: 2h06 vs. 1h49 - might not be relevant though.\nI'm inclined to think it's not related, because time for tox command stand at the same level \u00b135 sec.. Unfortunately, I don't clearly understand why it should be tested only on the latest PIP. Could you explain that?. @blueyed probably that's because on a pypy test are too slow (x2 time)?. Hello @inkhey,\nThanks for the report! Unfortunately i can't reproduce it locally. Could you reproduce it on a fresh debian docker image?. @blueyed thanks!. pip-tools v3.5.0 is released. Hello @jkp,\nThank you for the detailed report (especially for the repo so nicely) and good catch! There is a bug indeed with editable packages. I'll take a look shortly.. Actually, it's not a bug. This is the way how the pip works. It might be related to the installation order, since pip-tools parses requirements via pip internal api.\nFor example:\n```\n$ cat requirements.in\n-e git+https://github.com/jschneier/django-storages.git#egg=django-storages\n$ cat requirements.txt\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile -v\n\n-e git+https://github.com/jschneier/django-storages.git#egg=django-storages\ndjango==2.1.7\npytz==2018.9              # via django\n```\nLook how the order is important to pip install.  First case - django-storages will be installed:\n```\n$ cat requirements.dev.in\n-r requirements.txt\n-e git+https://github.com/jschneier/django-storages.git#egg=django-storages[boto3]\n$ pip install -r requirements.dev.in\nObtaining django-storages from git+https://github.com/jschneier/django-storages.git#egg=django-storages (from -r requirements.txt (line 7))\n  Cloning https://github.com/jschneier/django-storages.git to ./.venv/src/django-storages\nCollecting django==2.1.7 (from -r requirements.txt (line 8))\n  Using cached https://files.pythonhosted.org/packages/c7/87/fbd666c4f87591ae25b7bb374298e8629816e87193c4099d3608ef11fab9/Django-2.1.7-py3-none-any.whl\nCollecting pytz==2018.9 (from -r requirements.txt (line 9))\n  Using cached https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl\nInstalling collected packages: pytz, django, django-storages\n  Running setup.py develop for django-storages\nSuccessfully installed django-2.1.7 django-storages pytz-2018.9\n```\nSecond case - django-storages[boto3] will be installed:\n```\n$ cat requirements.dev.in\n-e git+https://github.com/jschneier/django-storages.git#egg=django-storages[boto3]\n-r requirements.txt\n$ pip install -r requirements.dev.in\nObtaining django-storages[boto3] from git+https://github.com/jschneier/django-storages.git#egg=django-storages[boto3] (from -r requirements.dev.in (line 1))\n  Updating ./.venv/src/django-storages clone\nRequirement already satisfied: django==2.1.7 in ./.venv/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (2.1.7)\nRequirement already satisfied: pytz==2018.9 in ./.venv/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2018.9)\nCollecting boto3>=1.4.4 (from django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/22/7d/d558ed7f0d916f912cfab65104375e1d7e41972c97284afe43bb6a53dd3c/boto3-1.9.114-py2.py3-none-any.whl\nCollecting jmespath<1.0.0,>=0.7.1 (from boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\nCollecting botocore<1.13.0,>=1.12.114 (from boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/63/f6/a3e18fd0b15a57efad4c2f20de4fad0a3abbdc9a60d7033af71375d0b39c/botocore-1.12.114-py2.py3-none-any.whl\nCollecting s3transfer<0.3.0,>=0.2.0 (from boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl\nCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.114->boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl\nCollecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.13.0,>=1.12.114->boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1))\n  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\nRequirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in ./.venv/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.114->boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1)) (1.24.1)\nRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.114->boto3>=1.4.4->django-storages[boto3]->-r requirements.dev.in (line 1)) (1.12.0)\nInstalling collected packages: jmespath, docutils, python-dateutil, botocore, s3transfer, boto3, django-storages\n  Found existing installation: django-storages 1.7.1\n    Uninstalling django-storages-1.7.1:\n      Successfully uninstalled django-storages-1.7.1\n  Running setup.py develop for django-storages\nSuccessfully installed boto3-1.9.114 botocore-1.12.114 django-storages docutils-0.14 jmespath-0.9.4 python-dateutil-2.8.0 s3transfer-0.2.0\n```\n. That\u2019s right!. I'll close the issue then. Feel free to reopen if you still have an issue with it.. Hello @milind-shakya-sp, \nThank you for the report! Right, the markers are in a wrong place. The order is described here:\n<requirement specifier> [; markers] [[--option]...]. Closed this in #763.. @milind-shakya-sp it looks good now. Could you please change the Changelog-friendly one-liner, because the issue number will be attached anyway.. Thanks for the contribution!. pip-tools v3.5.0 is released. @blueyed thanks!. Because we don't install packages by pip install -e ... anymore, so test for this piece of code was deleted. I would say two tests merged into one.. Agreed!. Good point! Thank you.. Agreed. I could fix piptools.utils.format_requirement to render hashes.. Please take a look at the last commit. I've fixed it.. > Oh! In the temporary requirements.txt, the requirement will include a -e, is that right?\nThat's right! \n\nIf so, got it. Maybe an explicit test for this case could be included alongside test_format_requirement_ireq_with_hashes.\n\nThere is already a test for this case test_format_requirement_editable:\nhttps://github.com/jazzband/pip-tools/blob/57ad5487221d0672f51661997be3b865afe0e19e/tests/test_utils.py#L12-L14\nBut i think there should be more lines for testing editables and pinned packages with hashes:\nhttps://github.com/jazzband/pip-tools/blob/57ad5487221d0672f51661997be3b865afe0e19e/tests/test_sync.py#L222-L234\n. @jdufresne @RDIL i've added couple tests, check it out please guys.. Maybe it would be better to injected allow_unsafe to the OutputWriter constructor instead?. I would prefer:\nassert comment('#    pip-compile --output-file dst_file src_file src_file2') in str_lines. My point is it looks a bit hacky, because the allow_unsafe injected now across two layers _iter_lines and write_header and it could go deeper. A quick research showed it was already in the constructor earlier (see #377 PR) and later was deleted by @jdufresne (see 68533537 commit). Current allow_unsafe was appeared in _iter_lines (see #517 PR).\nOf course it'll affect on tests, but we could create a writer instance and make a test for each behavior, allow_unsafe or not to allow_unsafe separately, which looks better to me.\n. Now it looks pretty good to me. Thanks!. Would be better to combine all test into one with parameterized flags.. AFAIK the pip-compile compiles only below flags to a requirements.txt:\n\n--index-url\n--extra-index-url\n--no-index\n--trusted-host\n--only-binary\n--no-binary\n\nThe rest of them would be ignored. Should the pip-compile also be fixed then?. Perfect! I think it would be great if you implement the rest of flags here in this PR, to fix the whole pipeline compile => sync. However, i'm not insisting. . Do you mean upload on each test job?. That's a nice feature in codecov, indeed. I don't have a strong opinion though about coveralls, so we could use codecov instead.. I've tried include = piptools, but got \"No data to report.\". Okay. Good point! Thanks.. I've added a backward compatibility. Your code looks good to me, so i've used it.. According to setup.py pip-tools doesn't support python 2.6/3.0/3.1 python_requires=\">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\". Should i fix it anyway?. Yes. Those packages could be found here\nhttps://github.com/pypa/pip/blob/b38e835ab2f65bea6d9e712f459b3e8e01abb0ea/src/pip/_internal/utils/compat.py#L206\nand there: https://github.com/pypa/pip/blob/0d9c05ec32bc420547d846a780507d44acd9a4a6/src/pip/_internal/commands/freeze.py#L11\nBut there would be some compatibility and circular imports issues, though, if you add stdlib_pkgs and DEV_PKGS to pip_compat.. I was wrong. Check the last commit.. There is a typo: they yes.. I like that. No need for len(src_files) == 1 then.. @blueyed i've fixed that block, see the latest commit please. Is it okay?. I think it's possible. On the other hand this header has already been tested in test_write_header. Do you think it is necessary to test once again?. Thank you for the feedback \ud83d\udc4d . I've found that there is no test for that case. I've added a test. Check it out.. Oh, that's a beautiful suggestion!. > Re \"more specific\": it should match other parts of the line printing this, so that it would not match e.g. a copyright line etc.. ;)\nGood point! Though i'm not sure how to test properly multiline string in output. See:\n-i, --index-url TEXT            Change index URL (defaults to\n                                         http://example.com)\nDo you have any suggestions?\n\nalso be asserted from a test that runs --help again (less overhead).\n\nI'd rather to make a separate test for that. In a perfect world i'd prefer one assert per one test :). What about this one? Looks a bit hacky though.\npython\nassert \"Change index URL (defaults to http://example.com)\" in \\\n    \"\".join(output.split(\" \")). Fixed!. No, it doesn't work.. Anyways, i've tested a whole string.  Check it out please.. @blueyed finally managed to fix failing test on win+py27. Reported an issue  https://github.com/pallets/click/issues/1231.. If not verbose - be verbose. Sounds kind of meaningless.. The python binary in C:\\Python36\\ folder. That's why %PYTHON% was added.. Good point. I think --flag=linux should be added also in the .travis.yml. What do you think?. Shouldn't it be outside of the environment section?. Agreed. Let's clean it up in a following PR.. In addition to that, i would also make the runner a fixture and clean up all the tests (which use the runnrt) like this one: https://github.com/jazzband/pip-tools/blob/67edc7d227507f3902090d2db8e129715279ae04/tests/test_cli_compile.py#L442-L449\nIMO the tests look more concise in that way, so let's take advantages of the magnificent pytest.. Good call!. Huh, indeed it is. I'll add a test then. Thank you for good catch.. At some point i was sure it was covered. But it's obviously not, since the falling checks :). Could you fix it the same way in the .appveyour.yml?. suggestion\n    - codecov -n %TOXENV%-windows\njust a nitpick.. Sorry didn't get your point. Could you shoot some code-snippet for the instance?. Aw, thanks!. Do you mean something like MockPyPIRepository.assert_called_once_with(..., ..., expected)?. > btw: you could use [ci skip] for the commit with the suggested changes - since it should be squashed later anyway, and would save some hours of CI.\nRight, okay.\n. Or we just could assert it called once (before iteration over call_args_list) to ensure the call_args_list is not empty:\npython\nMockPyPIRepository.assert_called_once(). In that case test_cert_option also should be fixed.. I was thinking, you know, one assertion looks better. Fixed the test.. Do we really need PIP variable?. ",
    "edesz": "Digging a bit into this:\nThe package six is bizzarely listed as a rPy2 dependency in requirements.txt. Also in requirements.txt, six is at version 1.10.0 which is the latest version.\nMaybe rPy2 cannot be updated since its apparent dependency six is already at the latest version? Could this be the cause of the problem?. Hi,\nI think you might have already resolved the problem.\nHere's the info about the questions:\n1. My pip freeze currently gives 2.8.5 - this is the version I have installed. After running pip-compile requirements.in, it produced a requirements.txt file which does list 2.8.6.\n2. Apologies since I am new to pip-tools - I was following the instructions here which indicated:\n   pip-compile requirements.in    #generates requirements.txt\n   pip-compile --upgrade          #to periodically upgrade all packages\n   I assumed the second command would have performed the upgrade. So, after pip-compile --upgrade, I    did not actually run pip install -r requirements.txt. This might be the problem - I think I followed the Updating requirements section of the README.md too literally. Is this necessary to perform the upgrade with pip install -r ....., or does pip-compile --upgrade perform this upgrade automatically?. Many thanks! Yes, all makes sense. Thanks for the details.. ",
    "jtiai": "requirements.txt:\nflake8==3.3.0\n-e hg+https://hg.repo.example/samplerepo/@1.0#egg=sample-app\npip install --no-deps -r requiremens.txt\nExpected result happens - only pinned versions without dependencies are installed.\npip-sync requirements.txt\nUnexpected result happens - flake8 is installed as pinned version without dependencies but editable version is installed with dependencies.\nsample-app/setup.py:\n```python\nfrom setuptools import setup\nsetup(\n    name='sample-app',\n    install_requires=[\n        'Django>=1.8,<1.9',\n    ],\n)\n```. I added sample repository to demonstrate the issue:\nhttps://github.com/jtiai/pipsync-issue. Oh, then this is just merely a documentation issue since nothing is indicating that pip-sync is not meant to work with any requirements.txt, but specially ones created by pip-compile.\nIf I need the feature I'll do PR.. ",
    "logston": "Maybe this error means that two different packages require two different versions of pysaml2? If so, it would be nice if this error could note that that is the case and notify me which packages have conflicting requirements.  . After unpinning, I don't see this error. Thanks. . ",
    "gersonkurz": "Yes, sorry, I messed that up.\nVon: David Genest [mailto:notifications@github.com] \nGesendet: Donnerstag, 15. Juni 2017 15:18\nAn: jazzband/pip-tools pip-tools@noreply.github.com\nCc: gersonkurz gerson.kurz@gmail.com; Author author@noreply.github.com\nBetreff: Re: [jazzband/pip-tools] Adding a --no-ssl option (#533)\nI think this is a source to target error. This seems to be for the pip-review repo, which originated from pip-tools. Closing.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/jazzband/pip-tools/pull/533#issuecomment-308726524 , or mute the thread https://github.com/notifications/unsubscribe-auth/AJJuJlrFNrkjZ_lmx1pDW4RM5OkwUOiFks5sES8SgaJpZM4N7IcK .  https://github.com/notifications/beacon/AJJuJrTVpf-m7rqlP-AAV9rwWpIVBDYrks5sES8SgaJpZM4N7IcK.gif \n. ",
    "kennell": "You are right, that was the problem. Closing this issue - thank you.. ",
    "ran-z": "Oh, I simply meant I tried removing the environment marker section and running pip-compile. I was on a Linux machine, but the error that I got was the same.\nAccording to the stack trace, it indeed looks like it tries to get the dependencies list:\nTraceback (most recent call last):\n  File \"<some-path>/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"<some-path>/local/lib/python2.7/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"<some-path>/local/lib/python2.7/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"<some-path>/local/lib/python2.7/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"<some-path>/local/lib/python2.7/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/resolver.py\", line 107, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/resolver.py\", line 195, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/resolver.py\", line 274, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/repositories/local.py\", line 62, in get_dependencies\n    return self.repository.get_dependencies(ireq)\n  File \"<some-path>/local/lib/python2.7/site-packages/piptools/repositories/pypi.py\", line 145, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"<some-path>/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 634, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"<some-path>/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 129, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"<some-path>/local/lib/python2.7/site-packages/pip/req/req_install.py\", line 439, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"<some-path>/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmp1t2Q00build/pypiwin32/\n. ",
    "JoergRittinger": "I added a pull request (https://github.com/jazzband/pip-tools/pull/647). @vphilippon can you give me feedback on this. thx. Thx @vphilippon  for you feedback. I tried an implementation which puts the package with the environment marker into the requirements.txt but excludes it from repository lookups and dependency searches. But it will get inconsistent anyway.\nIf I have the requirments.in with:\nunknown_python_3_package; python_version < '3'\nI will end up with the requirements.txt in python 2:\nunknown_python_3_package==1.0 ; python_version < '3'\nand with the requirements.txt in python 3:\nunknown_python_3_package ; python_version < '3'\nNote that I can find a version on my Repository server for python 2, but for python 3 it is not available and therefore can't be pinned.\nIf you want I can push my changes for further discussion.. Looks good to me.. ",
    "julianandrews": "Ok, a little more investigation suggests that this is actually a bug in pip itself, or possibly raven. A straight pip install still installs contextlib2.\nI'm going to close this and look into those packages.. ",
    "quantus": "I forgot to tell in the PR how this problem can be reproduced.\nSetup:\nmkvirtualenv --python=$(which python2.7) test \npip install Flask Flask-Debugtoolbar pip-tools\necho \"Flask\" > requirements.in\necho \"Flask-Debugtoolbar\" >> requirements.in\nAfter the setup:\n```\n(test) ~/Documents/tmp pip-compile requirements.in \n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\nblinker==1.4              # via flask-debugtoolbar\nclick==6.7                # via flask\nflask-debugtoolbar==0.10.1\nflask==0.12.2             # via flask-debugtoolbar\nitsdangerous==0.24        # via flask, flask-debugtoolbar\njinja2==2.9.6             # via flask\nmarkupsafe==1.0           # via jinja2\nwerkzeug==0.12.2          # via flask, flask-debugtoolbar\nVersion info:\n(test) ~/Documents/tmp pip-compile --version\npip-compile, version 1.9.0\n(test) ~/Documents/tmp pip --version\npip 9.0.1 from /Users/pekka/.virtualenvs/test/lib/python2.7/site-packages (python 2.7)\n```\nAs we can see from the output, the flask dependency was listed to come from # via flask-debugtoolbar. As the flask dependency was our primary dependency, that comment should not be there.\nWith this information I feel this PR is ready to be reviewed.. @vphilippon: I've made the change you requested.. ",
    "Mogost": "That is really good idea to get hashes from warehouse.\nIt speeds up a lot\nIt would be nice to have as an option.. ",
    "vlovich": "The following change would fix the issue when run within virtualenv.\n--- sync.py 2017-07-26 12:32:48.859134000 -0500\n+++ sync.py.new 2017-07-26 12:58:36.631153033 -0500\n@@ -133,10 +133,10 @@\n\n     if os.environ.get('VIRTUAL_ENV'):\n         # find pip via PATH\n-        pip = 'pip'\n+        pip = ['python', '-m', 'pip']\n     else:\n         # find pip in same directory as pip-sync entry-point script\n-        pip = os.path.join(os.path.dirname(os.path.abspath(sys.argv[0])), 'pip')\n+        pip = [os.path.join(os.path.dirname(os.path.abspath(sys.argv[0])), 'pip')]\n\n     if to_uninstall:\n         if dry_run:\n@@ -144,7 +144,7 @@\n             for pkg in to_uninstall:\n                 click.echo(\"  {}\".format(pkg))\n         else:\n-            check_call([pip, 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n+            check_call(pip + ['uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n\n     if to_install:\n         if install_flags is None:\n@@ -154,5 +154,5 @@\n             for pkg in to_install:\n                 click.echo(\"  {}\".format(pkg))\n         else:\n-            check_call([pip, 'install'] + pip_flags + install_flags + sorted(to_install))\n+            check_call(pip + ['install'] + pip_flags + install_flags + sorted(to_install))\n     return 0\n\nI haven't tried out fixing the absolute path usage but presumably it would be something like:\n script_dir = os.path.join(os.path.dirname(os.path.abspath(sys.argv[0]))\n python_exe = os.path.join(script_dir, 'python')\n pip_exe = os.path.join(script_dir, 'pip')\n pip = [python_exe, pip_exe]. ping.. I'm still having the issue with 2.0.2 on OSX.\n\n> pip --version\npip 10.0.1 from /Library/Python/2.7/site-packages/pip (python 2.7)\n> pip list | grep pip-tools\npip-tools     2.0.2\n\nOpening piptools/utils.py shows `from pip.req import InstallRequirement' instead of the correct version that appears on GitHub.. It wasn't a problem with the requirements.txt.  It was some issue with native packages failing to compile due to bdist_wheel issues.. Namely, inflection==0.3.1 results in:\n```\nRequirement already satisfied: setuptools in ./tools/cmdline/python-env/lib/python3.6/site-packages (from protobuf==3.6.0) (40.4.3)\nBuilding wheels for collected packages: inflection\n  Running setup.py bdist_wheel for inflection ... error\n  Complete output from command /usr/local/bin/python3 -u -c \"import setuptools, tokenize;file='/private/var/folders/xs/md937lms7k79c0fgz71blqh400_dy5/T/pip-install-4y9cs62v/inflection/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" bdist_wheel -d /private/var/folders/xs/md937lms7k79c0fgz71blqh400_dy5/T/pip-wheel-zwf3wbs7 --python-tag cp36:\n  usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n     or: -c --help [cmd1 cmd2 ...]\n     or: -c --help-commands\n     or: -c cmd --help\nerror: invalid command 'bdist_wheel'\n\nFailed building wheel for inflection\n  Running setup.py clean for inflection\nFailed to build inflection\n```\nbut the command exits with success.. ",
    "cdosborn": "I altered the setup.py of ssl_match_hostname to exclude the \"backports\" package, which works. It seems like this might be in the best interest of the package author. Do you know why they would include it?. its looks like there's 10 or so backports packages. ~~From a quick glance, backports.abc seems to handle it properly.~~\nbackports_abc isn't actually a backports namespace pkg. There are several different ways to define a namespace package. The backports packages use the 3rd approach listed in the pypa docs.\nIt looks like there is consensus that this issue should be addressed in pip:\nhttps://github.com/pypa/python-packaging-user-guide/issues/314. If you don't specifically want the pin, you can temporarily pin the requirements.in, generate the requirements.txt, and then remove the pin.. ",
    "dspechnikov": "It turned out there is no need to additional automation since requirements.in can be used for local development without pinned requirements compilation. And CI build scripts compile them on every build anyway.. ",
    "mete0r": "@suutari yeah, you are right and I considered that too, but just wanted a quick patch to have minimal impact on the status quo, i.e. depending on pip's internal API.\nFor pip >= 10.0.0 (https://github.com/pypa/pip/commit/95bcf8c5f6394298035a7332c441868f3b0169f4), the whole implementation of piptools.repository.pypi should be rewritten from scratch anyway, along with appropriate tests.. ",
    "natemara": "This would also help solve sdispater/poet#21 - which is basically an extension of #512. It would be great to see this merged.. ",
    "MHLut": "@suutari-ai The file was compiled in a Python 3 virtualenv on a machine that never even has contained an installation of Python 2. I have removed the .txt file and recompiled as per your instructions and I still get the python-ldap package in the results.\n@vphilippon I have run the command with a rebuild and verbose flag. The full results of my compilation can be read in the attached text file. Look at line 50 in the file to see the point where it's first (wrongly) asserted that there is a dependency on python-ldap.\nissue558-pip-compile-verbose.txt\n. @vphilippon Installation of this package goes well on my local venv (a new P3.6) and inside my Docker container (P3.4).\nbash\n$ pip install django-auth-ldap==1.2.10\nCollecting django-auth-ldap==1.2.10\n  Using cached django_auth_ldap-1.2.10-py3-none-any.whl\nRequirement already satisfied: pyldap in ~/.virtualenvs/python3/lib/python3.6/site-packages (from django-auth-ldap==1.2.10)\nCollecting django (from django-auth-ldap==1.2.10)\n  Downloading Django-1.11.5-py2.py3-none-any.whl (6.9MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.0MB 198kB/s \nRequirement already satisfied: setuptools in ~/.virtualenvs/python3/lib/python3.6/site-packages (from pyldap->django-auth-ldap==1.2.10)\nCollecting pytz (from django->django-auth-ldap==1.2.10)\n  Using cached pytz-2017.2-py2.py3-none-any.whl\nInstalling collected packages: pytz, django, django-auth-ldap\nSuccessfully installed django-1.11.5 django-auth-ldap-1.2.10 pytz-2017.2\nYou are correct that this is very much like #552. I did try the solution presented in that issue, but this did not solve the problem for me. For now I have manually removed the dependency line from the .txt file. Will test this setup on another workstation later, to see whether the issue occurs there as well.. @vphilippon Both on my local Python 3.6 environment and Python 3.4 Docker container I get the same result when running the installation instructions. As you can see below, the Py3 wheel is used:\nbash\n$ pip install django-auth-ldap==1.2.10 --no-cache --ignore-installed\nCollecting django-auth-ldap==1.2.10\n  Downloading django_auth_ldap-1.2.10-py3-none-any.whl\nCollecting pyldap (from django-auth-ldap==1.2.10)\n  Downloading pyldap-2.4.37.tar.gz (303kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 10.8MB/s \nCollecting django (from django-auth-ldap==1.2.10)\n  Downloading Django-1.11.5-py2.py3-none-any.whl (6.9MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.0MB 21.8MB/s \nCollecting setuptools (from pyldap->django-auth-ldap==1.2.10)\n  Downloading setuptools-36.4.0-py2.py3-none-any.whl (478kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 13.2MB/s \nCollecting pytz (from django->django-auth-ldap==1.2.10)\n  Downloading pytz-2017.2-py2.py3-none-any.whl (484kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 491kB 18.3MB/s \nInstalling collected packages: setuptools, pyldap, pytz, django, django-auth-ldap\n  Running setup.py install for pyldap ... done\nHowever, the pip-compile still creates a dependency file with the Py2 dependency added.\nNote: I've just realised I neglected to mention that the dependency list also contains the Py3 package. So both packages mentioned in line \"pyldap\" if PY3 else \"python-ldap >= 2.0\", (see first post for full snippet) are included.\nAttached to this comment is the output of $ pip-compile --output-file base.txt base.in --rebuild in a text file. This command was run on my local Py3.6 venv.\nissue558-pip-compile-output-file.txt\n. ",
    "azaghal": "Funnily enough, I am facing opposite problem after upgrading to pip-tools 1.10.x with the django-auth-ldap and Python 2. On my side pip-compile keeps pulling in pyldap insetad of python-ldap. My environment is:\n\nDistribution: Gentoo x86_64\nPython version: 2.7.12\npip version: 9.0.1\npip-tools version: 1.10.1\nrequirements.in:django-auth-ldap\n\n\n\nFor issue to be reproduced reliably I had to clear caches (via --rebuild option). As per what @suutari-ai said, after doing git bisect, the culprit is commit 4900c7c4f962264caf373c0e1d26773e31477b1b. However, when utilising Python 3 (3.4.5), I do get pyldap as requirement.. I wonder if it would be possible to somehow fake what the platform is instead? Doing a quick search (did not actually go through script in detail), there's some script that does something similar.. Another resolution difference between 1.9.0 and 1.10.x that I've seen is for package kallithea - to be more precise for one of this dependencies - mercurial. This is probably the same issue?\nWith 1.9.0, pip-compile would end-up using version 4.2.2 of the package, while with 1.10.x, it would use 4.2.3. Looking at the PyPI download page, it looks like 4.2.3 is available only for 32 and 64-bit Windows platform?\nThe issue should be reproducible with requirements.in being set to mercurial~=4.2.0 (you may need to remove .txt before each run of pip-compile).. Yep, Gentoo 64-bit.. First of all, thanks for looking into this and fixing it :) Now, what's the chance of getting a patch release out that includes this fix, since it does make it troublesome to use 1.10.x?. Ho-hum, that sounds even slightly scary. Thanks for letting me know what's up, much appreciated :). ",
    "damianarechar": "thank you @vphilippon \ud83d\ude04 . ",
    "bendikro": "I don't understand your question. Did you look at the example?\nAs written in the OP, pip does not handle multiple requirement files that have the same dependency multiple times with different version requirements.\nSo currently, it's necessary to run pip install -r requirements1.in and pip install -r requirements2.in to install successfully, as pip install -r requirements1.in -r requirements2.in will fail.. We have a common set of makefiles for different types of functionality, where each makefile has a corresponding requirement.in file. When including a certain makefile, it adds a requirement.in file to a list of pip requirement files.\nSo depending on what functionality is needed, a project may end up with a set of requirement.in files with some overlapping dependencies.\nPinning exact versions for all dependencies is only useful for the an end project, i.e. a project that is not used by other projects.\nWe have a python library used by other projects, that includes includes multiple makefiles and therefore relies on multiple requirement.in files. We do not want to pin the versions for all the dependencies of this library (as pip-tools currently does). However, we have to pin some library versions as not doing so may lead to version conflicts in end projects, depending on the order the libraries are installed. Right now we cannot use pip-tools to generate a requirement file for this python library (built from multiple input requirement files) as it pins all library versions. We therefore do this manually.\npip-tools is what we need as it does proper dependency handling to ensure there are no version conflicts. But we want to use this for a python library that should not be pinning all library versions.\nDoes this answer your questions?. > Thank you for the contribution. Could you provide a test for this feature?\nI will\n\nI think we should also consider a question: how the \"quiet\" flag should be dealt with the \"verbose\" flag?\n\nThey should be mutually exclusive, shouldn't they?. @atugushev Added tests and a class to handle Mutually exclusive options adopted from https://github.com/pallets/click/issues/257#issuecomment-403312784. > I'd rather do it in an additive way.\nAnd not be mutually exclusive?. > > And not be mutually exclusive?\n\nThat's right\n\nVery well. That enables some strange valid options like -qqq -vv which would be the same as -q.. ",
    "twig": "Wow you responded much quicker than expected.\nAnd yes, I think having that in the docs would be very helpful.\nThanks for that.\nFeels rather frustrating that there isn't an override of some sort though as bringing more libraries over to requests.in would result in even more varieties of version mismatches, especially for something as popular as the requests library.. Ahh yes, I came across that when playing with pip-tools earlier. I chose it because v2.0.0 was such a simple example to reproduce.\nHmm I agree with you on the explicit versioning for a reason, but for libraries which have been abandoned or are very slow moving (pyrax for example), ~~we don't really have a choice because using a fork of a repo doesn't seem to work as it would in the regular requirements.txt file (as I've read in #272)~~. Actually, scrap that. I just realised our old requirements.txt was using a different URL format which pip-compile doesn't like https://github.com/USERNAME/REPO/archive/COMMITHASH.zip#egg=PACKAGENAME==VERSION. Converting it to -e git://github.com/USERNAME/REPO.git@COMMITHASH#egg=PACKAGENAME==VERSION works a treat!\nIt would probably be worth noting in the docs as well I suppose.. > Why use pip-tools then? Just use pip install+pip freeze > requirements.txt\nActually, I'm mainly pip-tools to help keep reference of why libraries are in my requirements.txt in the first place.\nThe auto-generated file includes comments which are very helpful in debugging why libraries are there, something which regular pip freeze doesn't do.\nThe ability to override when needed would still be highly appreciated!. Neat! Sadly not as nice as having it in requirements.txt as pip-tools would have it\nJust an update, my initial issue was resolved by the library authors at https://github.com/waveaccounting/vero-python/issues/26\nBut it doesn't really resolve the issue in the case of inactive libraries and such.\nFrom memory, npm/yarn has a nice resolution selector when it runs into version conflicts.. I'll add my 2c here as this one is more recent activity and tests (although I prefer the simplicity of the older attempt https://github.com/jazzband/pip-tools/pull/372)\nI was digging around the pip-tools src because of the same frustrations here.\nWould it be sufficient if piptools...:\n\nchecked if the constraint.link.egg_fragment had the right information for a pinned version u'package-name==1.2.3'\ndownloaded the archive using _vendored.pip.download.unpack_url\n\n\ndef unpack_url(link, location, download_dir=None, only_download=False, session=None, hashes=None):\n\n\nlook at setup.py in the extracted folder and grab the requirements from there (I couldn't find a function for doing this)\nadd the requirements to the generated requirements.txt file\nadd the CVS/URL requirements to requirements.txt so pip would install it as it normally would\nnow pip-sync should have what it needs to check if a pinned package is installed at the right version or not\n\nDon't have much experience with python package managers so my comment may come across as naive, but feel free to correct me if I'm wrong.. ",
    "jcerjak": "Explicitly pinning versions in a library is obviously not desired. But the world is dark and full of terrors: either a library is not well maintained, or it needs to pin a dependency to a specific version due to monkey-patching, or any other exotic or not-so-exotic reason. \nThis is probably why for the above example:\n```\nrequirements.in\nvero==2.0.0\nrequests==2.18.4\n```\npip (tested with version 10.0.0) will only print a warning but still finish the installation:\n$ pip install -r requirements.in\n...\nvero 2.0.0 has requirement requests==2.7.0, but you'll have requests 2.18.4 which is incompatible.\nwhile pip-tools will raise an error:\n$ pip-compile\n...\nCould not find a version that matches requests==2.18.4,==2.7.0\nI agree that raising an error by default makes sense, I'd prefer if pip would do that as well. But IMHO there should be an option in pip-tools to only print a warning. If an application developer has pinned all the dependencies, explicitly enabled the option to not treat these warnings as errors and accepted the risk, then we should allow him/her to do so.. Here is a quick proof-of-concept how this could work.\nExample requirements.in:\nvero==2.0.0\nrequests==2.18.4\nDefault behavior (unchanged):\n$ pip-compile\nCould not find a version that matches requests==2.18.4,==2.7.0\n...\nThere are incompatible versions in the resolved dependencies.\nWith the --allow-conflicts option:\n```\n$ pip-compile --allow-conflicts\nConflicting versions found: requests==2.18.4,==2.7.0, using our constraints: requests==2.18.4\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\n--index-url https://pypi.org/simple\ncertifi==2018.4.16        # via requests\nchardet==3.0.4            # via requests\nidna==2.6                 # via requests\nrequests==2.18.4\nurllib3==1.22             # via requests\nvero==2.0.0\n```\nWould be nice if we included a comment in requirements.txt, that requests had conflicts, but this would require a bit more changes.\nThoughts?. Thanks @vphilippon for a timely and thorough response. I do agree that  ideally this should be addressed at the lib level, though sometimes you want to have an override switch to get things done. But I understand that adding this would add more complexity and maintenance burden and is also not in line with the concept of pip-tools.\nSo I'll probably go with the approach of creating a fresh virtualenv + pip freeze for now. Thanks!\n. > Actually, I'm mainly pip-tools to help keep reference of why libraries are in my requirements.txt in the first place.\nNot as nice, but you can manually check why a library was installed with pipdeptree:\n$ pip install pip-tools pipdeptree\nCheck dependencies:\n$ pipdeptree\npip-tools==2.0.1\n  - click [required: >=6, installed: 6.7]\n  - first [required: Any, installed: 2.0.1]\n  - six [required: Any, installed: 1.11.0]\npipdeptree==0.12.1\n  - pip [required: >=6.0.0, installed: 10.0.1]\nsetuptools==39.0.1\nwheel==0.31.0\nCheck which packages require click and pip:\n$ pipdeptree --reverse --packages click,pip\nclick==6.7\n  - pip-tools==2.0.1 [requires: click>=6]\npip==10.0.1\n  - pipdeptree==0.12.1 [requires: pip>=6.0.0]. ",
    "philfreo": "I renamed the above requirements.txt to requirements.in and ran pip-compile -v requirements.in and interestingly it worked without issue. All 3 requirements did what I wanted in Round 1 (where as with pipenv lock --verbose the -e requirements never showed up in Round 1's Current Constraints), and the process completed without error. What does that tell us?\n```\n$ pip-compile -v requirements.in \nUsing indexes:\n  https://pypi.python.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  Flask==0.9\n  flask-migrate==1.8.0\n  Flask-SQLAlchemy from git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (from -r requirements.in (line 2))\nFinding the best candidates:\n  found candidate flask==0.9 (constraint was ==0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\n  found candidate -e git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (constraint was )\nFinding secondary dependencies:\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\nNo handlers could be found for logger \"pip.vcs.git\"\nNew dependencies found in this round:\n  adding [u'alembic', '>=0.6', '[]']\n  adding [u'flask', '>=0.9', '[]']\n  adding [u'flask-script', '>=0.6', '[]']\n  adding [u'flask-sqlalchemy', '>=1.0', '[]']\n  adding [u'jinja2', '>=2.4', '[]']\n  adding [u'sqlalchemy', '>=0.7', '[]']\n  adding [u'werkzeug', '>=0.7', '[]']\nRemoved dependencies in this round:\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  alembic>=0.6\n  Flask==0.9,>=0.9\n  flask-migrate==1.8.0\n  Flask-Script>=0.6\n  Flask-SQLAlchemy from git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (from -r requirements.in (line 2))\n  Jinja2>=2.4\n  SQLAlchemy>=0.7\n  Werkzeug>=0.7\nFinding the best candidates:\n  found candidate alembic==0.9.5 (constraint was >=0.6)\n  found candidate flask==0.9 (constraint was ==0.9,>=0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\n  found candidate flask-script==2.0.6 (constraint was >=0.6)\n  found candidate -e git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (constraint was )\n  found candidate jinja2==2.9.6 (constraint was >=2.4)\n  found candidate sqlalchemy==1.1.14 (constraint was >=0.7)\n  found candidate werkzeug==0.12.2 (constraint was >=0.7)\nFinding secondary dependencies:\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\n  sqlalchemy==1.1.14        requires -\n  alembic==0.9.5            requires Mako, python-dateutil, python-editor>=0.3, SQLAlchemy>=0.7.6\n  jinja2==2.9.6             requires MarkupSafe>=0.23\n  werkzeug==0.12.2          requires -\n  flask-script==2.0.6       requires Flask\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\nNew dependencies found in this round:\n  adding [u'mako', '', '[]']\n  adding [u'markupsafe', '>=0.23', '[]']\n  adding [u'python-dateutil', '', '[]']\n  adding [u'python-editor', '>=0.3', '[]']\n  adding [u'sqlalchemy', '>=0.7,>=0.7.6', '[]']\nRemoved dependencies in this round:\n  removing [u'sqlalchemy', '>=0.7', '[]']\n\nResult of round 2: not stable\n                      ROUND 3\n\nCurrent constraints:\n  alembic>=0.6\n  Flask==0.9,>=0.9\n  flask-migrate==1.8.0\n  Flask-Script>=0.6\n  Flask-SQLAlchemy from git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (from -r requirements.in (line 2))\n  Jinja2>=2.4\n  Mako\n  MarkupSafe>=0.23\n  python-dateutil\n  python-editor>=0.3\n  SQLAlchemy>=0.7,>=0.7.6\n  Werkzeug>=0.7\nFinding the best candidates:\n  found candidate alembic==0.9.5 (constraint was >=0.6)\n  found candidate flask==0.9 (constraint was ==0.9,>=0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\n  found candidate flask-script==2.0.6 (constraint was >=0.6)\n  found candidate -e git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (constraint was )\n  found candidate jinja2==2.9.6 (constraint was >=2.4)\n  found candidate mako==1.0.7 (constraint was )\n  found candidate markupsafe==1.0 (constraint was >=0.23)\n  found candidate python-dateutil==2.6.1 (constraint was )\n  found candidate python-editor==1.0.3 (constraint was >=0.3)\n  found candidate sqlalchemy==1.1.14 (constraint was >=0.7,>=0.7.6)\n  found candidate werkzeug==0.12.2 (constraint was >=0.7)\nFinding secondary dependencies:\n  mako==1.0.7 not in cache, need to check index\n  mako==1.0.7               requires MarkupSafe>=0.9.2\n  jinja2==2.9.6             requires MarkupSafe>=0.23\n  werkzeug==0.12.2          requires -\n  python-editor==1.0.3      requires -\n  flask-script==2.0.6       requires Flask\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\n  alembic==0.9.5            requires Mako, python-dateutil, python-editor>=0.3, SQLAlchemy>=0.7.6\n  python-dateutil==2.6.1    requires six>=1.5\n  markupsafe==1.0           requires -\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\n  sqlalchemy==1.1.14        requires -\nNew dependencies found in this round:\n  adding [u'markupsafe', '>=0.23,>=0.9.2', '[]']\n  adding [u'six', '>=1.5', '[]']\nRemoved dependencies in this round:\n  removing [u'markupsafe', '>=0.23', '[]']\n\nResult of round 3: not stable\n                      ROUND 4\n\nCurrent constraints:\n  alembic>=0.6\n  Flask==0.9,>=0.9\n  flask-migrate==1.8.0\n  Flask-Script>=0.6\n  Flask-SQLAlchemy from git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (from -r requirements.in (line 2))\n  Jinja2>=2.4\n  Mako\n  MarkupSafe>=0.23,>=0.9.2\n  python-dateutil\n  python-editor>=0.3\n  six>=1.5\n  SQLAlchemy>=0.7,>=0.7.6\n  Werkzeug>=0.7\nFinding the best candidates:\n  found candidate alembic==0.9.5 (constraint was >=0.6)\n  found candidate flask==0.9 (constraint was ==0.9,>=0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\n  found candidate flask-script==2.0.6 (constraint was >=0.6)\n  found candidate -e git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy (constraint was )\n  found candidate jinja2==2.9.6 (constraint was >=2.4)\n  found candidate mako==1.0.7 (constraint was )\n  found candidate markupsafe==1.0 (constraint was >=0.23,>=0.9.2)\n  found candidate python-dateutil==2.6.1 (constraint was )\n  found candidate python-editor==1.0.3 (constraint was >=0.3)\n  found candidate six==1.11.0 (constraint was >=1.5)\n  found candidate sqlalchemy==1.1.14 (constraint was >=0.7,>=0.7.6)\n  found candidate werkzeug==0.12.2 (constraint was >=0.7)\nFinding secondary dependencies:\n  jinja2==2.9.6             requires MarkupSafe>=0.23\n  python-editor==1.0.3      requires -\n  flask-script==2.0.6       requires Flask\n  python-dateutil==2.6.1    requires six>=1.5\n  sqlalchemy==1.1.14        requires -\n  alembic==0.9.5            requires Mako, python-dateutil, python-editor>=0.3, SQLAlchemy>=0.7.6\n  mako==1.0.7               requires MarkupSafe>=0.9.2\n  werkzeug==0.12.2          requires -\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\n  markupsafe==1.0           requires -\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\n  six==1.11.0               requires -\n\nResult of round 4: stable, done\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\n-e git+ssh://git@github.com/closeio/flask-sqlalchemy.git@1556d1822c46019e518707af0adc034ad4363ab4#egg=Flask-SQLAlchemy  # via flask-migrate\nalembic==0.9.5            # via flask-migrate\nflask-migrate==1.8.0\nflask-script==2.0.6       # via flask-migrate\nflask==0.9                # via flask-migrate, flask-script\njinja2==2.9.6             # via flask\nmako==1.0.7               # via alembic\nmarkupsafe==1.0           # via jinja2, mako\npython-dateutil==2.6.1    # via alembic\npython-editor==1.0.3      # via alembic\nsix==1.11.0               # via python-dateutil\nsqlalchemy==1.1.14        # via alembic\nwerkzeug==0.12.2          # via flask\n``. @kennethreitz On pipenv 7.3.2 the-erequirement shown above still doesn't show up inpipenv lock --verboselike it does withpip-compile -v. The same error is shown, and because of the verbose output is still missing the-e`, I'm inclined to think this isn't just due to a Flask 0.9 issue?\nSeparately, can you share (or link me to) more details about what's wrong with Flask 0.9's setup.py? \n```\n$ pipenv lock --verbose\nLocking [dev-packages] dependencies\u2026\n                      ROUND 1\n\nCurrent constraints:\nFinding the best candidates:\nFinding secondary dependencies:\nResult of round 1: stable, done\nLocking [packages] dependencies\u2026\n                      ROUND 1\n\nCurrent constraints:\n  Flask==0.9\n  Flask-Migrate==1.8.0\nFinding the best candidates:\n  found candidate flask==0.9 (constraint was ==0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\nFinding secondary dependencies:\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\nNew dependencies found in this round:\n  adding [u'alembic', '>=0.6', '[]']\n  adding [u'flask', '>=0.9', '[]']\n  adding [u'flask-script', '>=0.6', '[]']\n  adding [u'flask-sqlalchemy', '>=1.0', '[]']\n  adding [u'jinja2', '>=2.4', '[]']\n  adding [u'werkzeug', '>=0.7', '[]']\nRemoved dependencies in this round:\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  alembic>=0.6\n  Flask==0.9,>=0.9\n  Flask-Migrate==1.8.0\n  Flask-Script>=0.6\n  Flask-SQLAlchemy>=1.0\n  Jinja2>=2.4\n  Werkzeug>=0.7\nFinding the best candidates:\n  found candidate alembic==0.9.5 (constraint was >=0.6)\n  found candidate flask==0.9 (constraint was ==0.9,>=0.9)\n  found candidate flask-migrate==1.8.0 (constraint was ==1.8.0)\n  found candidate flask-script==2.0.6 (constraint was >=0.6)\n  found candidate flask-sqlalchemy==2.2 (constraint was >=1.0)\n  found candidate jinja2==2.9.6 (constraint was >=2.4)\n  found candidate werkzeug==0.12.2 (constraint was >=0.7)\nFinding secondary dependencies:\n  jinja2==2.9.6             requires MarkupSafe>=0.23\n  flask==0.9                requires Jinja2>=2.4, Werkzeug>=0.7\n  flask-migrate==1.8.0      requires alembic>=0.6, Flask-Script>=0.6, Flask-SQLAlchemy>=1.0, Flask>=0.9\n  flask-script==2.0.6       requires Flask\n  werkzeug==0.12.2          requires -\n  flask-sqlalchemy==2.2     requires Flask>=0.10, SQLAlchemy>=0.8.0\n  alembic==0.9.5            requires Mako, python-dateutil, python-editor>=0.3, SQLAlchemy>=0.7.6\nNew dependencies found in this round:\n  adding [u'flask', '>=0.10,>=0.9', '[]']\n  adding [u'mako', '', '[]']\n  adding [u'markupsafe', '>=0.23', '[]']\n  adding [u'python-dateutil', '', '[]']\n  adding [u'python-editor', '>=0.3', '[]']\n  adding [u'sqlalchemy', '>=0.7.6,>=0.8.0', '[]']\nRemoved dependencies in this round:\n  removing [u'flask', '>=0.9', '[]']\n\nResult of round 2: not stable\n                      ROUND 3\n\nCurrent constraints:\n  alembic>=0.6\n  Flask==0.9,>=0.10,>=0.9\n  Flask-Migrate==1.8.0\n  Flask-Script>=0.6\n  Flask-SQLAlchemy>=1.0\n  Jinja2>=2.4\n  Mako\n  MarkupSafe>=0.23\n  python-dateutil\n  python-editor>=0.3\n  SQLAlchemy>=0.7.6,>=0.8.0\n  Werkzeug>=0.7\nFinding the best candidates:\n  found candidate alembic==0.9.5 (constraint was >=0.6)\nTraceback (most recent call last):\n  File \"/Users/philfreo/Library/Python/2.7/bin/pipenv\", line 11, in \n    sys.exit(cli())\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 722, in call\n    return self.main(args, kwargs)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 535, in invoke\n    return callback(args, **kwargs)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/cli.py\", line 1619, in lock\n    do_lock(verbose=verbose, clear=clear)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/cli.py\", line 944, in do_lock\n    project=project\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/utils.py\", line 160, in resolve_deps\n    resolved_tree = resolver.resolve()\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 107, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 187, in _resolve_one_round\n    best_matches = set(self.get_best_match(ireq) for ireq in constraints)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 187, in \n    best_matches = set(self.get_best_match(ireq) for ireq in constraints)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 245, in get_best_match\n    best_match = self.repository.find_best_match(ireq, prereleases=self.prereleases)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/repositories/pypi.py\", line 116, in find_best_match\n    raise NoCandidateFound(ireq, all_candidates)\npiptools.exceptions.NoCandidateFound: Could not find a version that matches Flask==0.9,>=0.10,>=0.9\nTried: 0.1, 0.2, 0.3, 0.3.1, 0.4, 0.5, 0.5.1, 0.5.2, 0.6, 0.6.1, 0.7, 0.7.1, 0.7.2, 0.8, 0.8.1, 0.9, 0.10, 0.10.1, 0.11, 0.11, 0.11.1, 0.11.1, 0.12, 0.12, 0.12.1, 0.12.1, 0.12.2, 0.12.2\n```. pipenv, version 7.3.6\nTwo new errors. This:\n```\n$ pipenv install \nRequirements.txt found, instead of Pipfile! Converting\u2026\nWarning: Your Pipfile now contains pinned versions, if your requirements.txt did. \nWe recommend updating your Pipfile to specify the \"*\" version, instead.\nNo package provided, installing all dependencies.\nPipfile.lock not found, creating\u2026\nLocking [dev-packages] dependencies\u2026\nLocking [packages] dependencies\u2026\nNo handlers could be found for logger \"pip.vcs.git\"\nUpdated Pipfile.lock!\nInstalling dependencies from Pipfile.lock\u2026\nAn error occured while installing flask-sqlalchemy==! Will try again.\n  \ud83d\udc0d   \u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752 13/13 \u2014 00:00:04\nInstalling initially\u2013failed dependencies\u2026\nCollecting flask-sqlalchemy== \u2752\u2752\u2752\u2752\u2752\u2752\u2752 0/1 \u2014 00:00:00\nCould not find a version that satisfies the requirement flask-sqlalchemy== (from -r /var/folders/gk/89t674fx4q54nlmvl9yyflnw0000gn/T/pipenv-8Pt4Ri-requirement.txt (line 1)) (from versions: 0.5, 0.6, 0.9, 0.9.1, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 1.0, 2.0, 2.1, 2.2)\nNo matching distribution found for flask-sqlalchemy== (from -r /var/folders/gk/89t674fx4q54nlmvl9yyflnw0000gn/T/pipenv-8Pt4Ri-requirement.txt (line 1))\n\u2624  \u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752\u2752 0/1 \u2014 00:00:00\n```\nAnd:\n```\n$ pipenv lock --verbose\nLocking [dev-packages] dependencies\u2026\n                      ROUND 1\n\nCurrent constraints:\nFinding the best candidates:\nFinding secondary dependencies:\nResult of round 1: stable, done\nLocking [packages] dependencies\u2026\nTraceback (most recent call last):\n  File \"/Users/philfreo/Library/Python/2.7/bin/pipenv\", line 11, in \n    sys.exit(cli())\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 722, in call\n    return self.main(args, kwargs)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/vendor/click/core.py\", line 535, in invoke\n    return callback(args, **kwargs)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/cli.py\", line 1617, in lock\n    do_lock(verbose=verbose, clear=clear)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/cli.py\", line 942, in do_lock\n    project=project\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/utils.py\", line 133, in resolve_deps\n    extra_constraints = best_matches_from(dep[len('-e '):], which=which, which_pip=which_pip, project=project)\n  File \"/Users/philfreo/Library/Python/2.7/lib/python/site-packages/pipenv/utils.py\", line 106, in best_matches_from\n    setup_py_path = os.path.abspath(os.sep.join([result, 'setup.py']))\nTypeError: sequence item 0: expected string, NoneType found\n```\nwhich looks like the same as https://github.com/kennethreitz/pipenv/issues/512#issuecomment-330670460. Closing since the pip-tools error no longer happens. However the pipenv problems remains, I opened https://github.com/kennethreitz/pipenv/issues/671. ",
    "kenneth-reitz": "I will experiment with that. . this is now fixed in pipenv. \nPlease note that Flask==0.9 has a bug in its setup.py file, and will cause an exception. Any other version works fine, though. . thanks for the help, @vphilippon!. try again with latest. ",
    "codefo": "I have the same issue, but I'm trying to install graphene-django\n$ pipenv install graphene-django==2.0.dev2017083101\n...\npiptools.exceptions.NoCandidateFound: Could not find a version that matches graphene>=2.0.dev\nAs you can see piptools cant find graphene>=2.0.dev while PyPI contains it.\nMy pipenv and python versiosn:\n$ python --version\nPython 3.6.2\n$ pipenv --version\npipenv, version 7.3.6. ",
    "tuukkamustonen": "I feel the problem. I am new to pip-tools, and I thought https://github.com/jazzband/pip-tools/pull/460 was supposed to make builds environment-agnostic. But it works on main level only then, so the implementation is like half-way there?. Manually investigating / marking the transitive dependencies with their environment markers would be too cumbersome. It's just not really a workaround, imo.\nAnyway, I have no urgent need for what this ticket is about - I just expected transitive dependencies to be also tackled, but they weren't so raised a ticket to be enlightened :).\nI tend to develop on the same OS and major version of python (e.g. 3.5.x) as what my servers are running. So locking not only the versions but also losing the environment markers is not a problem. I would assume this applies to most developers out there?\nUtility libraries/SDKs need to be compatible with multiple environments, and cannot lock versions, so there's no use for pip-tools in such projects anyway.\nOf course, locking development / test dependencies in any project can also be useful. And currently the lock files pip-tools produces are not \"safe\" to use if developers work in different OS / python versions. Is the recommended way to tackle that to develop on similar environments?. > I have no urgent need for what this ticket is about\nActually, a (pretty standard) use case where this is needed: When you support multiple python versions for a library, and run tests under tox (or jenkins), then you cannot simply have something like this in tox.ini:\nini\ncommands = \\\n    pip install -r requirements.txt\nAs requirements.txt contains the compiled dependencies against specific platform/environment, and probably doesn't work under something else (e.g. 2.7 vs 3.6).. It's just that producing/maintaining those files would be somewhat cumbersome. I would need to switch between virtualenvs and build the files for each env by hand.\nSome sort of tooling would be needed to make that sensible. There was a suggestion in https://github.com/jazzband/pip-tools/issues/635#issuecomment-374421771.\nThought, maybe a simple for loop could provide a good start, something like:\nfish\nfor ENV in 27, 34, 35, 36\n    pyenv activate project${ENV}\n    pip-tools compile -o requirements.txt requirements.in\nend. Related: pip list --outdated, pip check (seems undocumented and at early stages, see https://github.com/pypa/pip/issues/4738), pipenv check, pipenv update.\nI wonder if the implementation from pipenv could be moved into pip-tools, which pipenv uses internally anyway...(?). Oopsie, accidentally closed it :)\nThough, it really should be closed now that #647 is in. Will give 2.x a spin tomorrow!. I believe this got in into 2.0.0? It's missing from the changelog, though?. Maybe this should go to the bottom of the file instead (just before \"Note about pip\")? Let's not confuse readers by this advanced stuff until they actually know the basics?. How about an example here, just to make it clear what we are talking about:\nConsider this requirements.in:\nenum34; python_version < '3.4'\ndistro; platform_system == 'Linux'\nMaybe not needed, but shouldn't hurt to show some real \"code\".. Uh, scratch that. I was thinking about self-added environment markers now only, but this example would be just misleading. So nevermind.. ",
    "ghost": "Same here . I confirm that this problem can be reproduced by installing ansible via pip-compile. If ansible is already installed manually the problem does not happen.. ",
    "oppianmatt": "in my case the error I found is coming from ansible, putting a print just before the line shows these filenames:\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/meta\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/meta/main.yml\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/tasks\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/tasks/main.yml\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/files\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/files/foo.txt\n/tmp/tmpf9NBhMbuild/ansible test/integration/targets/unarchive/files/test-unarchive-nonascii-\u304f\u304f\u3089\u3089\u3068\u3068\u307f\u307f.tar.gz\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 107, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 195, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/usr/local/lib/python2.7/site-packages/piptools/resolver.py\", line 274, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/repositories/pypi.py\", line 145, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/usr/local/lib/python2.7/site-packages/pip/req/req_set.py\", line 620, in _prepare_file\n    session=self.session, hashes=hashes)\n  File \"/usr/local/lib/python2.7/site-packages/pip/download.py\", line 821, in unpack_url\n    hashes=hashes\n  File \"/usr/local/lib/python2.7/site-packages/pip/download.py\", line 663, in unpack_http_url\n    unpack_file(from_path, location, content_type, link)\n  File \"/usr/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 606, in unpack_file\n    untar_file(filename, location)\n  File \"/usr/local/lib/python2.7/site-packages/pip/utils/__init__.py\", line 552, in untar_file\n    path = os.path.join(location, fn)\n  File \"/usr/local/lib/python2.7/posixpath.py\", line 73, in join\n    path += '/' + b\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe3 in position 66: ordinal not in range(128)\n\nit's non ascii filenames in the package causing it to error\nunicode filenames should be valid\nit's definitely pip-tools issue since pip install -U ansible works\nseems to be similar to #433 but the fix provided there does not fix this. work around was to upgrade ansible manually and then continue using pip-tools. found it was the package django-celery-email being upgraded from 1.x to 2 requiring celery 4\nbut my django-celery-email line is unpined\nso to satisfy requirements why didn't it pick the 1.x instead of trying to go to 2 since that is incompat?. yep down pinned worked\n\ninstead of backtracking then, since it's hard\ncouldn't it display which package(s) are causing a conflict\nor even just display which packages had any version specifications when it fails to find a package\nso I could see which ones are probably the culprit\nturned on verbose and had to grep the output of a big list (lots of requirements). ",
    "jonatkinson": "We can replicate this on Amazon Linux 2016.03 running on Python 3.4.4, pip-tools 1.10.0.. ",
    "leonsmith": "Seem to be related to #566 there is also a note around the code in that section that regarding equality\n```\nNOTE: We need to compare RequirementSummary objects, since\nInstallRequirement does not define equality\n```\nEdit: Not 100% sure on the above, just reading at a surface level & seems to be the around the right area. Is there an expected release date for 2.0?\nI'm just trying to triage updating lots of projects to pin the pip version as all our builds have just started failing :( . wow awesome! thats saved me a huge headache, really appreciated!. ",
    "abravalheri": "@vphilippon, I was facing the same problem in this repository (pip-tools branch). Then I added a requirements.in file with -e . as suggested by you, and things worked fine.\nSo your guess seem to be consistent...\n(thank you very much for the information and workaround!). @vphilippon thank you very much for your comments and suggested approach (and for maintaining such a good package)!\nI have a little doubt left though: since, as you said, the support to setup.py in incomplete\n(for example I am getting a RuntimeError: 'distutils.core.setup()' was never called -- perhaps 'setup.py' is not a Distutils setup script?, when trying to run pip-compile on both example1 and example2) and adding -e . to requirements.in, as you suggested, create an absolute path in requirements.txt  (see example3) and therefore prevents sharing it between multiple developers of a same project, what would be your recommendation for being able to use the pip-tools workflow while relying on setuptools for packaging without double bookkeeping the dependencies?\nIs there an alternative workaround for this?. ",
    "simlun": "Thanks @vphilippon for the quick feedback :) The changelog is now updated and edits from maintainers is allowed.\nCheers!. Ok @vphilippon, include it whenever it's suitable - no stress :). @vphilippon I just love this project and was simply supplying a PR for an existing open issue. If I misunderstood, or someone wants to take it in another direction - feel free! :). ",
    "mhozza": "Hmm, this seems to be caching problem - seems like it reuses a cache generated from python2. \nadding -r fixes the problem, but it would be nice to have separate cache for each python version or virtal env.. Hmm, sorry, it does not fix it (it changed behaviour when I installed social-auth-core via pip - after that it put the right dependency).. ",
    "pdehaye": "that seems to have done it! weird bug.... ",
    "elcolie": "I can reproduce this on my GNU/Linux Debian Stretch\nSeems like this issue may comes from pyenv itself\nbash\n$ pyenv --version\npyenv 1.1.3-5-g7dae197\n(soken-web) sarit@sniper:~/Code/soken/soken-web$ uname -a\nLinux server 4.9.0-4-amd64 #1 SMP Debian 4.9.51-1 (2017-09-28) x86_64 GNU/Linux\n(soken-web) sarit@sniper:~/Code/soken/soken-web$ python -V\nPython 3.6.2\n(soken-web) sarit@sniper:~/Code/soken/soken-web$ pip --version\npip 9.0.1 from /home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages (python 3.6)\n(soken-web) sarit@sniper:~/Code/soken/soken-web$ pip-compile --version\npip-compile, version 1.10.1\n(soken-web) sarit@sniper:~/Code/soken/soken-web$ pip-compile \nTraceback (most recent call last):\n  File \"/home/sarit/.pyenv/versions/soken-web/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 136, in cli\n    for ireq in ireqs\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 137, in <dictcomp>\n    if is_pinned_requirement(ireq) and key_from_req(ireq.req) not in upgrade_pkgs_key}\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/piptools/utils.py\", line 121, in is_pinned_requirement\n    if len(ireq.specifier._specs) != 1:\n  File \"/home/sarit/.pyenv/versions/3.6.2/envs/soken-web/lib/python3.6/site-packages/pip/req/req_install.py\", line 287, in specifier\n    return self.req.specifier\nAttributeError: 'NoneType' object has no attribute 'specifier'. Remove requirements.txt fix this. Thank you very much.. I have put the line my requirements.in\n-e git+https://github.com/philippbosch/django-geoposition/archive/django-1.11.zip#egg=django\nError is\nbash\nremote: Not Found\nfatal: repository 'https://github.com/philippbosch/django-geoposition/archive/django-1.11.zip/' not found\nTraceback (most recent call last):\n  File \"/Users/sarit/.pyenv/versions/poink/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/piptools/resolver.py\", line 101, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/piptools/resolver.py\", line 198, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/piptools/resolver.py\", line 272, in _iter_dependencies\n    for dependency in self.repository.get_dependencies(ireq):\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/piptools/repositories/pypi.py\", line 170, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/pip/req/req_set.py\", line 516, in _prepare_file\n    req_to_install.update_editable(not self.is_download)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/pip/req/req_install.py\", line 586, in update_editable\n    vcs_backend.obtain(self.source_dir)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/pip/vcs/git.py\", line 142, in obtain\n    self.run_command(['clone', '-q', url, dest])\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/pip/vcs/__init__.py\", line 325, in run_command\n    spinner)\n  File \"/Users/sarit/.pyenv/versions/3.6.3/envs/poink/lib/python3.6/site-packages/pip/utils/__init__.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"git clone -q https://github.com/philippbosch/django-geoposition/archive/django-1.11.zip /var/folders/h5/lj3c5_050j3_yjd77zqxcn7r0000gp/T/tmp1ay6spspsource/django\" failed with error code 128 in None\npip-compile add slash at the end of the url. Any ideas?. It's mean I have to manually put line https://github.com/philippbosch/django-geoposition/archive/django-1.11.zip to the requirements.txt everytime myself isn't it?. Thank you very much.. ```bash\n$ pip-compile graph.in -o graph.txt --verbose\nUsing indexes:\n  https://pypi.org/simple\n                      ROUND 1\n\nCurrent constraints:\n  ipython-cypher\n  networkx\n  pandas\n  plotly\n  py2neo\n  python-igraph\nFinding the best candidates:\n  found candidate ipython-cypher==0.2.6 (constraint was )\n  found candidate networkx==2.2 (constraint was )\n  found candidate pandas==0.23.4 (constraint was )\n  found candidate plotly==3.3.0 (constraint was )\n  found candidate py2neo==4.1.0 (constraint was )\n  found candidate python-igraph==0.7.1.post6 (constraint was )\nFinding secondary dependencies:\n  ipython-cypher==0.2.6     requires ipython>=1.0, neo4jrestclient>=2.1.0, prettytable\n  plotly==3.3.0             requires decorator>=4.0.6, nbformat>=4.2, pytz, requests, retrying>=1.3.3, six\n  py2neo==4.1.0             requires click>=2.0, colorama, ipykernel, jupyter_client, jupyter_console, neo4j-driver<1.7,>=1.6, neotime, prompt_toolkit==1.0.15, pygments>=2.0, pytest, traitlets>=4.1.0, urllib3[secure]<1.23,>=1.21.1\n  python-igraph==0.7.1.post6 requires -\n  pandas==0.23.4            requires numpy>=1.9.0, python-dateutil>=2.5.0, pytz>=2011k\n  networkx==2.2             requires decorator>=4.3.0\nNew dependencies found in this round:\n  adding ['click', '>=2.0', '[]']\n  adding ['colorama', '', '[]']\n  adding ['decorator', '>=4.0.6,>=4.3.0', '[]']\n  adding ['ipykernel', '', '[]']\n  adding ['ipython', '>=1.0', '[]']\n  adding ['jupyter-client', '', '[]']\n  adding ['jupyter-console', '', '[]']\n  adding ['nbformat', '>=4.2', '[]']\n  adding ['neo4j-driver', '<1.7,>=1.6', '[]']\n  adding ['neo4jrestclient', '>=2.1.0', '[]']\n  adding ['neotime', '', '[]']\n  adding ['numpy', '>=1.9.0', '[]']\n  adding ['prettytable', '', '[]']\n  adding ['prompt-toolkit', '==1.0.15', '[]']\n  adding ['pygments', '>=2.0', '[]']\n  adding ['pytest', '', '[]']\n  adding ['python-dateutil', '>=2.5.0', '[]']\n  adding ['pytz', '>=2011k', '[]']\n  adding ['requests', '', '[]']\n  adding ['retrying', '>=1.3.3', '[]']\n  adding ['six', '', '[]']\n  adding ['traitlets', '>=4.1.0', '[]']\n  adding ['urllib3', '<1.23,>=1.21.1', \"['secure']\"]\nRemoved dependencies in this round:\nUnsafe dependencies in this round:\n\nResult of round 1: not stable\n                      ROUND 2\n\nCurrent constraints:\n  click>=2.0\n  colorama\n  decorator>=4.0.6,>=4.3.0\n  ipykernel\n  ipython>=1.0\n  ipython-cypher\n  jupyter_client\n  jupyter_console\n  nbformat>=4.2\n  neo4j-driver<1.7,>=1.6\n  neo4jrestclient>=2.1.0\n  neotime\n  networkx\n  numpy>=1.9.0\n  pandas\n  plotly\n  prettytable\n  prompt_toolkit==1.0.15\n  py2neo\n  pygments>=2.0\n  pytest\n  python-dateutil>=2.5.0\n  python-igraph\n  pytz>=2011k\n  requests\n  retrying>=1.3.3\n  six\n  traitlets>=4.1.0\n  urllib3[secure]<1.23,>=1.21.1\nFinding the best candidates:\n  found candidate click==7.0 (constraint was >=2.0)\n  found candidate colorama==0.3.9 (constraint was )\n  found candidate decorator==4.3.0 (constraint was >=4.0.6,>=4.3.0)\n  found candidate ipykernel==5.0.0 (constraint was )\n  found candidate ipython==7.0.1 (constraint was >=1.0)\n  found candidate ipython-cypher==0.2.6 (constraint was )\n  found candidate jupyter-client==5.2.3 (constraint was )\n  found candidate jupyter-console==5.2.0 (constraint was )\n  found candidate nbformat==4.4.0 (constraint was >=4.2)\n  found candidate neo4j-driver==1.6.2 (constraint was >=1.6,<1.7)\n  found candidate neo4jrestclient==2.1.1 (constraint was >=2.1.0)\n  found candidate neotime==1.0.0 (constraint was )\n  found candidate networkx==2.2 (constraint was )\n  found candidate numpy==1.15.2 (constraint was >=1.9.0)\n  found candidate pandas==0.23.4 (constraint was )\n  found candidate plotly==3.3.0 (constraint was )\n  found candidate prettytable==0.7.2 (constraint was )\n  found candidate prompt_toolkit==1.0.15 (constraint was ==1.0.15)\n  found candidate py2neo==4.1.0 (constraint was )\n  found candidate pygments==2.2.0 (constraint was >=2.0)\n  found candidate pytest==3.8.1 (constraint was )\n  found candidate python-dateutil==2.7.3 (constraint was >=2.5.0)\n  found candidate python-igraph==0.7.1.post6 (constraint was )\n  found candidate pytz==2018.5 (constraint was >=2011k)\n  found candidate requests==2.19.1 (constraint was )\n  found candidate retrying==1.3.3 (constraint was >=1.3.3)\n  found candidate six==1.11.0 (constraint was )\n  found candidate traitlets==4.3.2 (constraint was >=4.1.0)\n  found candidate urllib3[secure]==1.22 (constraint was >=1.21.1,<1.23)\nFinding secondary dependencies:\n  pandas==0.23.4            requires numpy>=1.9.0, python-dateutil>=2.5.0, pytz>=2011k\n  decorator==4.3.0          requires -\n  plotly==3.3.0             requires decorator>=4.0.6, nbformat>=4.2, pytz, requests, retrying>=1.3.3, six\n  six==1.11.0               requires -\n  colorama==0.3.9           requires -\n  ipython==7.0.1            requires appnope; sys_platform == \"darwin\", backcall, decorator, jedi>=0.10, pexpect; sys_platform != \"win32\", pickleshare, prompt-toolkit<2.1.0,>=2.0.0, pygments, setuptools>=18.5, simplegeneric>0.8, traitlets>=4.2\n  ipykernel==5.0.0          requires ipython>=5.0.0, jupyter-client, tornado>=4.2, traitlets>=4.1.0\n  python-igraph==0.7.1.post6 requires -\n  nbformat==4.4.0           requires ipython-genutils, jsonschema!=2.5.0,>=2.4, jupyter-core, traitlets>=4.1\n  prompt_toolkit==1.0.15    requires six>=1.9.0, wcwidth\n  jupyter-console==5.2.0    requires ipykernel, ipython, jupyter-client, prompt-toolkit<2.0.0,>=1.0.0, pygments\n  py2neo==4.1.0             requires click>=2.0, colorama, ipykernel, jupyter_client, jupyter_console, neo4j-driver<1.7,>=1.6, neotime, prompt_toolkit==1.0.15, pygments>=2.0, pytest, traitlets>=4.1.0, urllib3[secure]<1.23,>=1.21.1\n  ipython-cypher==0.2.6     requires ipython>=1.0, neo4jrestclient>=2.1.0, prettytable\n  neotime==1.0.0            requires pytz, six\n  python-dateutil==2.7.3    requires six>=1.5\n  pygments==2.2.0           requires -\n  neo4jrestclient==2.1.1    requires requests>=2.1.0\n  requests==2.19.1          requires certifi>=2017.4.17, chardet<3.1.0,>=3.0.2, idna<2.8,>=2.5, urllib3<1.24,>=1.21.1\n  pytest==3.8.1             requires atomicwrites>=1.0, attrs>=17.4.0, more-itertools>=4.0.0, pluggy>=0.7, py>=1.5.0, setuptools, six>=1.10.0\n  urllib3[secure]==1.22     requires certifi; extra == \"secure\"\n  prettytable==0.7.2        requires -\n  traitlets==4.3.2          requires decorator, ipython-genutils, six\n  click==7.0                requires -\n  networkx==2.2             requires decorator>=4.3.0\n  pytz==2018.5              requires -\n  numpy==1.15.2             requires -\n  jupyter-client==5.2.3     requires jupyter-core, python-dateutil>=2.1, pyzmq>=13, tornado>=4.1, traitlets\n  retrying==1.3.3           requires six>=1.7.0\n  neo4j-driver==1.6.2       requires neotime==1.0.0\nNew dependencies found in this round:\n  adding ['appnope', '', '[]']\n  adding ['atomicwrites', '>=1.0', '[]']\n  adding ['attrs', '>=17.4.0', '[]']\n  adding ['backcall', '', '[]']\n  adding ['certifi', '>=2017.4.17', '[]']\n  adding ['chardet', '<3.1.0,>=3.0.2', '[]']\n  adding ['idna', '<2.8,>=2.5', '[]']\n  adding ['ipython', '>=1.0,>=5.0.0', '[]']\n  adding ['ipython-genutils', '', '[]']\n  adding ['jedi', '>=0.10', '[]']\n  adding ['jsonschema', '!=2.5.0,>=2.4', '[]']\n  adding ['jupyter-core', '', '[]']\n  adding ['more-itertools', '>=4.0.0', '[]']\n  adding ['neotime', '==1.0.0', '[]']\n  adding ['pexpect', '', '[]']\n  adding ['pickleshare', '', '[]']\n  adding ['pluggy', '>=0.7', '[]']\n  adding ['prompt-toolkit', '<2.0.0,<2.1.0,==1.0.15,>=1.0.0,>=2.0.0', '[]']\n  adding ['py', '>=1.5.0', '[]']\n  adding ['python-dateutil', '>=2.1,>=2.5.0', '[]']\n  adding ['pyzmq', '>=13', '[]']\n  adding ['requests', '>=2.1.0', '[]']\n  adding ['simplegeneric', '>0.8', '[]']\n  adding ['six', '>=1.10.0,>=1.5,>=1.7.0,>=1.9.0', '[]']\n  adding ['tornado', '>=4.1,>=4.2', '[]']\n  adding ['traitlets', '>=4.1,>=4.1.0,>=4.2', '[]']\n  adding ['urllib3', '<1.23,<1.24,>=1.21.1', \"['secure']\"]\n  adding ['wcwidth', '', '[]']\nRemoved dependencies in this round:\n  removing ['ipython', '>=1.0', '[]']\n  removing ['neotime', '', '[]']\n  removing ['prompt-toolkit', '==1.0.15', '[]']\n  removing ['python-dateutil', '>=2.5.0', '[]']\n  removing ['requests', '', '[]']\n  removing ['six', '', '[]']\n  removing ['traitlets', '>=4.1.0', '[]']\n  removing ['urllib3', '<1.23,>=1.21.1', \"['secure']\"]\nUnsafe dependencies in this round:\n\nResult of round 2: not stable\n                      ROUND 3\n\nCurrent constraints:\n  appnope\n  atomicwrites>=1.0\n  attrs>=17.4.0\n  backcall\n  certifi>=2017.4.17\n  chardet<3.1.0,>=3.0.2\n  click>=2.0\n  colorama\n  decorator>=4.0.6,>=4.3.0\n  idna<2.8,>=2.5\n  ipykernel\n  ipython>=1.0,>=5.0.0\n  ipython-cypher\n  ipython-genutils\n  jedi>=0.10\n  jsonschema!=2.5.0,>=2.4\n  jupyter-client\n  jupyter_console\n  jupyter-core\n  more-itertools>=4.0.0\n  nbformat>=4.2\n  neo4j-driver<1.7,>=1.6\n  neo4jrestclient>=2.1.0\n  neotime==1.0.0\n  networkx\n  numpy>=1.9.0\n  pandas\n  pexpect\n  pickleshare\n  plotly\n  pluggy>=0.7\n  prettytable\n  prompt-toolkit<2.0.0,<2.1.0,==1.0.15,>=1.0.0,>=2.0.0\n  py>=1.5.0\n  py2neo\n  pygments>=2.0\n  pytest\n  python-dateutil>=2.1,>=2.5.0\n  python-igraph\n  pytz>=2011k\n  pyzmq>=13\n  requests>=2.1.0\n  retrying>=1.3.3\n  simplegeneric>0.8\n  six>=1.10.0,>=1.5,>=1.7.0,>=1.9.0\n  tornado>=4.1,>=4.2\n  traitlets>=4.1,>=4.1.0,>=4.2\n  urllib3[secure]<1.23,<1.24,>=1.21.1\n  wcwidth\nFinding the best candidates:\n  found candidate appnope==0.1.0 (constraint was )\n  found candidate atomicwrites==1.2.1 (constraint was >=1.0)\n  found candidate attrs==18.2.0 (constraint was >=17.4.0)\n  found candidate backcall==0.1.0 (constraint was )\n  found candidate certifi==2018.8.24 (constraint was >=2017.4.17)\n  found candidate chardet==3.0.4 (constraint was >=3.0.2,<3.1.0)\n  found candidate click==7.0 (constraint was >=2.0)\n  found candidate colorama==0.3.9 (constraint was )\n  found candidate decorator==4.3.0 (constraint was >=4.0.6,>=4.3.0)\n  found candidate idna==2.7 (constraint was >=2.5,<2.8)\n  found candidate ipykernel==5.0.0 (constraint was )\n  found candidate ipython==7.0.1 (constraint was >=1.0,>=5.0.0)\n  found candidate ipython-cypher==0.2.6 (constraint was )\n  found candidate ipython-genutils==0.2.0 (constraint was )\n  found candidate jedi==0.12.1 (constraint was >=0.10)\n  found candidate jsonschema==2.6.0 (constraint was >=2.4,!=2.5.0)\n  found candidate jupyter-client==5.2.3 (constraint was )\n  found candidate jupyter-console==5.2.0 (constraint was )\n  found candidate jupyter-core==4.4.0 (constraint was )\n  found candidate more-itertools==4.3.0 (constraint was >=4.0.0)\n  found candidate nbformat==4.4.0 (constraint was >=4.2)\n  found candidate neo4j-driver==1.6.2 (constraint was >=1.6,<1.7)\n  found candidate neo4jrestclient==2.1.1 (constraint was >=2.1.0)\n  found candidate neotime==1.0.0 (constraint was ==1.0.0)\n  found candidate networkx==2.2 (constraint was )\n  found candidate numpy==1.15.2 (constraint was >=1.9.0)\n  found candidate pandas==0.23.4 (constraint was )\n  found candidate pexpect==4.6.0 (constraint was )\n  found candidate pickleshare==0.7.5 (constraint was )\n  found candidate plotly==3.3.0 (constraint was )\n  found candidate pluggy==0.7.1 (constraint was >=0.7)\n  found candidate prettytable==0.7.2 (constraint was )\nCould not find a version that matches prompt-toolkit<2.0.0,<2.1.0,==1.0.15,>=1.0.0,>=2.0.0\nTried: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 0.9, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.25, 0.26, 0.28, 0.30, 0.31, 0.32, 0.32, 0.33, 0.33, 0.34, 0.34, 0.35, 0.35, 0.36, 0.36, 0.37, 0.37, 0.38, 0.38, 0.39, 0.39, 0.40, 0.40, 0.41, 0.41, 0.42, 0.42, 0.43, 0.43, 0.44, 0.44, 0.45, 0.45, 0.46, 0.46, 0.47, 0.47, 0.48, 0.48, 0.49, 0.49, 0.50, 0.50, 0.51, 0.51, 0.52, 0.52, 0.53, 0.53, 0.54, 0.54, 0.55, 0.55, 0.56, 0.56, 0.57, 0.57, 0.58, 0.58, 0.59, 0.59, 0.60, 0.60, 1.0.0, 1.0.0, 1.0.1, 1.0.1, 1.0.2, 1.0.2, 1.0.3, 1.0.3, 1.0.4, 1.0.4, 1.0.5, 1.0.5, 1.0.6, 1.0.6, 1.0.7, 1.0.7, 1.0.8, 1.0.8, 1.0.9, 1.0.9, 1.0.10, 1.0.10, 1.0.13, 1.0.13, 1.0.14, 1.0.14, 1.0.15, 1.0.15, 2.0.1, 2.0.1, 2.0.2, 2.0.2, 2.0.3, 2.0.3, 2.0.4, 2.0.4\nThere are incompatible versions in the resolved dependencies.\n```\nAfter I check verbose. I tried install one by one. No error raises and here is my pip freeze\nbash\n$ pip freeze\nappnope==0.1.0\natomicwrites==1.2.1\nattrs==18.2.0\nbackcall==0.1.0\ncertifi==2018.8.24\nchardet==3.0.4\nClick==7.0\ncolorama==0.3.9\ndecorator==4.3.0\nidna==2.7\nipykernel==5.0.0\nipython==7.0.1\nipython-cypher==0.2.6\nipython-genutils==0.2.0\njedi==0.12.1\njsonschema==2.6.0\njupyter-client==5.2.3\njupyter-console==5.2.0\njupyter-core==4.4.0\nmore-itertools==4.3.0\nnbformat==4.4.0\nneo4j-driver==1.6.2\nneo4jrestclient==2.1.1\nneotime==1.0.0\nnetworkx==2.2\nnumpy==1.15.2\npandas==0.23.4\nparso==0.3.1\npexpect==4.6.0\npickleshare==0.7.5\npip-tools==3.0.0\nplotly==3.3.0\npluggy==0.7.1\nprettytable==0.7.2\nprompt-toolkit==1.0.15\nptyprocess==0.6.0\npy==1.6.0\npy2neo==4.1.0\nPygments==2.2.0\npytest==3.8.1\npython-dateutil==2.7.3\npython-igraph==0.7.1.post6\npytz==2018.5\npyzmq==17.1.2\nrequests==2.19.1\nretrying==1.3.3\nsimplegeneric==0.8.1\nsix==1.11.0\ntornado==5.1.1\ntraitlets==4.3.2\nurllib3==1.22\nwcwidth==0.1.7\n. ",
    "pradyunsg": "I partially refactored the internal code for pip's resolution logic. That's still WIP so, there's that.\nLet me know how I can help. :)\n. > Send a PR to make pip-tools work with pip 8.0.1 to 10\nI am kinda short on free time right now so I think that's unlikely. :/\n\nwe now have your name if we need a contact\n\nSure! Gimme a mention and I'll respond when I can. :)\n\nI noticed that you're reaching deep into pip's internals so, whoever does this is gonna have a lot of fun. :P\n. > I just couldn't let the opportunity pass and not ask :)\nI imagine so. :P \n\nIs that something that's aimed for pip 10?\n\nIn a word, no.\nI wanted to but it turned out to be more work than I had anticipated and then I ran out of time (paid and then soon after free; thanks to college).\nI'm gonna try a different approach -- implementing the resolver independently of pip and then fitting that into pip but that's for early next year; not pip 10.\nRant hidden\n\nThere's a 30 page report that I have to submit in the morning tomorrow but I got know about it day before and everything that I did yesterday for it (after finishing 2 other assignments) was incorrect because I was given wrong instructions. So, now I have to do it all from scratch again and sleep in 10 hours. ugh.\n\n\n\nI've been looking for a \"pip 10 change resume\" to figure out what changed and how things are working now.\n\nI don't think there's one. pip doesn't document internal changes. There's PRs tagged kind - refactor by me over on pip's tracker. You'd probably want to look at them. (now that I look at them, they're a good summary)\n\nIn general, anything touching RequirementSet would have to change because RequirementSet is no longer the God Class that it was. It's still god-ly -- not as much has before though. My understanding is, that's all of pip-tools.\nBasically, things still have to move out of RequirementSet, till it basically becomes a set of requirements with some helpers to query items in the set; all operations on them would move out into other places. The current codebase is in a limbo where some stuff has been moved decoupled but some hasn't. (For example there's stuff still on RequirementSet that should really be on Resolver but well, I didn't get to it because of a lack of time mostly.)\npip 10 will have this partially; even patch versions of pip 10 might change this situation (because it's pip._internal stuff) so, you're probably gonna have to deal with that too. :-(\n. Vendoring sounds good to me. It does save pip-tools from some of the hacks it does.\nThat does come with the issue that the new pip features -- including PEP 518 support, better PEP 508 support -- and (lots of)? bug fixes won't be available to users of pip-tools.\nTo be honest, I don't see a cleaner way forward on this, other than someone putting in a lot of hours, which is not gonna be me so I'm not gonna complain about it.\n. Okay -- just checked code... \nPEP 518 would not affect dependency resolution and PEP 508 support doesn't affect dependencies, only top level requirements right now.\n\nstill rely on pip 10\n\nOh right. Cool. :)\n\nIt's just the bug fixes then, which, I guess, is cool.\n. Seems related to pypa/pip#6164.\nI'm inclined to think poetry needs to update its backend since it's not compatible with pip's PEP 517 implementation as things stand.. As for rollout of isolation to end users, @techalchemy can probably explain in better terms. :). ",
    "lawrlee": "I'm having similar problems with seaborn, which includes matplotlib as a dependency\nroot@c40c070a2024:/app# echo \"seaborn\" > test.in\nroot@c40c070a2024:/app# pip-compile test.in\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 101, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 198, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/usr/local/lib/python3.6/site-packages/piptools/resolver.py\", line 284, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/usr/local/lib/python3.6/site-packages/piptools/repositories/pypi.py\", line 170, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 634, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_set.py\", line 129, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"/usr/local/lib/python3.6/site-packages/pip/req/req_install.py\", line 439, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"/usr/local/lib/python3.6/site-packages/pip/utils/__init__.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmpif8gdc0fbuild/subprocess32/\nroot@c40c070a2024:/app# python --version\nPython 3.6.3. ",
    "hassy-6thsense": "I also have the same problem with matplotlib.\nThe problem seems to be occurring when trying to install subprocess32.\nmatplotlib depends on subprocess32 only in Python 2.7.\nHowever, here we are executing in Python 3.4, but because we are trying to get subprocess32, we have failed.\nMaybe, pip-tools always refers to package dependencies with Python 2.7.\nOn CentOS\n\nOS Type: CentOS 7.4.1708 x86_64\nPython version: 3.4.5 (python34-3.4.5-4.el7.x86_64)\npip version: pip 9.0.1 from /tmp/piptools-test/venv/lib64/python3.4/site-packages (python 3.4)\npip-tools version: pip-compile, version 1.10.1\n\nOn Ubuntu\n\nOS Type: Ubuntu 16.04.3 LTS amd64\nPython version: 3.5.1 (python3-3.5.1-3)\npip version: pip 9.0.1 from /tmp/piptools-test/venv/lib/python3.5/site-packages (python 3.5)\n\npip-tools version: pip-compile, version 1.10.1\n\n\nrequirements.in\n  matplotlib\n\n\nResults (on CentOS)\n  ```bash\n  % pip-compile --dry-run --rebuild --verbose\n  Using indexes:\n    https://pypi.python.org/simple\n                    ROUND 1\n\nCurrent constraints:\nmatplotlib\n\n\nFinding the best candidates:\n    found candidate matplotlib==2.1.0 (constraint was )\nFinding secondary dependencies:\n    matplotlib==2.1.0 not in cache, need to check index\n    matplotlib==2.1.0         requires backports.functools-lru-cache, cycler>=0.10, numpy>=1.7.1, pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1, python-dateutil>=2.0, pytz, six>=1.10, subprocess32\nNew dependencies found in this round:\n    adding ['backports.functools-lru-cache', '', '[]']\n    adding ['cycler', '>=0.10', '[]']\n    adding ['numpy', '>=1.7.1', '[]']\n    adding ['pyparsing', '!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1', '[]']\n    adding ['python-dateutil', '>=2.0', '[]']\n    adding ['pytz', '', '[]']\n    adding ['six', '>=1.10', '[]']\n    adding ['subprocess32', '', '[]']\n  Removed dependencies in this round:\n  Unsafe dependencies in this round:\n\nResult of round 1: not stable\n                        ROUND 2\n\nCurrent constraints:\n    backports.functools-lru-cache\n    cycler>=0.10\n    matplotlib\n    numpy>=1.7.1\n    pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n    python-dateutil>=2.0\n    pytz\n    six>=1.10\n    subprocess32\nFinding the best candidates:\n    found candidate backports.functools-lru-cache==1.4 (constraint was )\n    found candidate cycler==0.10.0 (constraint was >=0.10)\n    found candidate matplotlib==2.1.0 (constraint was )\n    found candidate numpy==1.13.3 (constraint was >=1.7.1)\n    found candidate pyparsing==2.2.0 (constraint was >=2.0.1,!=2.0.4,!=2.1.2,!=2.1.6)\n    found candidate python-dateutil==2.6.1 (constraint was >=2.0)\n    found candidate pytz==2017.2 (constraint was )\n    found candidate six==1.11.0 (constraint was >=1.10)\n    found candidate subprocess32==3.2.7 (constraint was )\nFinding secondary dependencies:\n    subprocess32==3.2.7 not in cache, need to check index\n  Traceback (most recent call last):\n    File \"/tmp/piptools-test/venv/bin/pip-compile\", line 11, in \n      sys.exit(cli())\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/click/core.py\", line 722, in call\n      return self.main(args, kwargs)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/click/core.py\", line 697, in main\n      rv = self.invoke(ctx)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/click/core.py\", line 895, in invoke\n      return ctx.invoke(self.callback, ctx.params)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/click/core.py\", line 535, in invoke\n      return callback(args, **kwargs)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/piptools/scripts/compile.py\", line 184, in cli\n      results = resolver.resolve(max_rounds=max_rounds)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/piptools/resolver.py\", line 101, in resolve\n      has_changed, best_matches = self._resolve_one_round()\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/piptools/resolver.py\", line 198, in _resolve_one_round\n      for dep in self._iter_dependencies(best_match):\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/piptools/resolver.py\", line 284, in _iter_dependencies\n      dependencies = self.repository.get_dependencies(ireq)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/piptools/repositories/pypi.py\", line 170, in get_dependencies\n      self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/pip/req/req_set.py\", line 634, in _prepare_file\n      abstract_dist.prep_for_dist()\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/pip/req/req_set.py\", line 129, in prep_for_dist\n      self.req_to_install.run_egg_info()\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/pip/req/req_install.py\", line 439, in run_egg_info\n      command_desc='python setup.py egg_info')\n    File \"/tmp/piptools-test/venv/lib64/python3.4/site-packages/pip/utils/init.py\", line 707, in call_subprocess\n      % (command_desc, proc.returncode, cwd))\n  pip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmp_eh5egjsbuild/subprocess32/\n  ```. ",
    "bnaul": "I tried this on master with Python 3.6.3 and am seeing the same issue:\n```\n\u279c  /tmp pip install -e git+https://github.com/jazzband/pip-tools.git#egg=pip-tools\nObtaining pip-tools from git+https://github.com/jazzband/pip-tools.git#egg=pip-tools\n  Updating ./src/pip-tools clone\nRequirement already satisfied: click>=6 in /Users/brettnaul/miniconda3/lib/python3.6/site-packages (from pip-tools)\nRequirement already satisfied: first in /Users/brettnaul/miniconda3/lib/python3.6/site-packages (from pip-tools)\nRequirement already satisfied: six in /Users/brettnaul/miniconda3/lib/python3.6/site-packages (from pip-tools)\nRequirement already satisfied: setuptools in /Users/brettnaul/miniconda3/lib/python3.6/site-packages (from pip-tools)\nInstalling collected packages: pip-tools\n  Running setup.py develop for pip-tools\nSuccessfully installed pip-tools\n\u279c  /tmp echo \"matplotlib\" > requirements.in\n\u279c  /tmp pip-compile\nTraceback (most recent call last):\n  File \"/Users/brettnaul/miniconda3/bin/pip-compile\", line 11, in \n    load_entry_point('pip-tools', 'console_scripts', 'pip-compile')()\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/click/core.py\", line 722, in call\n    return self.main(args, kwargs)\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/click/core.py\", line 535, in invoke\n    return callback(args, **kwargs)\n  File \"/private/tmp/src/pip-tools/piptools/scripts/compile.py\", line 184, in cli\n    results = resolver.resolve(max_rounds=max_rounds)\n  File \"/private/tmp/src/pip-tools/piptools/resolver.py\", line 102, in resolve\n    has_changed, best_matches = self._resolve_one_round()\n  File \"/private/tmp/src/pip-tools/piptools/resolver.py\", line 199, in _resolve_one_round\n    for dep in self._iter_dependencies(best_match):\n  File \"/private/tmp/src/pip-tools/piptools/resolver.py\", line 285, in _iter_dependencies\n    dependencies = self.repository.get_dependencies(ireq)\n  File \"/private/tmp/src/pip-tools/piptools/repositories/pypi.py\", line 152, in get_dependencies\n    self._dependencies_cache[ireq] = reqset._prepare_file(self.finder, ireq)\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 634, in _prepare_file\n    abstract_dist.prep_for_dist()\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/pip/req/req_set.py\", line 129, in prep_for_dist\n    self.req_to_install.run_egg_info()\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/pip/req/req_install.py\", line 439, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"/Users/brettnaul/miniconda3/lib/python3.6/site-packages/pip/utils/init.py\", line 707, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\npip.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /var/folders/kb/5p7n5tds71v3rsmjtjqm2w4r0000gn/T/tmppd0qfxz1build/subprocess32/\n```. Ah looks like it works with the rebuilt cache, thanks!. ",
    "bhrutledge": "I've been using pip-sync successfully with pipsi, which I think makes sense based on your description. But, I'm happy to update the docs to suggest option 4, as that seems the easiest and least surprising.\nOn that note: is there any suggestion for including pip-tools as a development requirement in setup.py or a .in file?. @vphilippon Any suggestions on what else this PR needs to be merge-ready? I haven't dug into why the Travis build failed.. @vphilippon Here's the updated PR.. ",
    "costypetrisor": "Hi @vphilippon \nThanks for looking at this :)\nI've been blindsided when I first looked at pip's code and didn't notice that the call I was trying to avoid was under a nice if controllable from the outside.\nI've updated the PR.\nThanks :)\n. @vphilippon rebase done :). hold on a second, I think I pushed the wrong thing. @vphilippon  the correct PR branch has been pushed  now :)\nThanks! :). ",
    "flaub": "Actually, there's an evaluate() method on the markers attribute of an InstallRequirement. I don't know the best place for this call to be made, but my best guess is in scripts/compile.py you could add a line like:\nconstraints = [x for x in constraints if not x.markers or x.markers.evaluate()]\nThis line could go just after collecting all the constraints for parsing requirements and just before Resolver.check_constraints(constraints)\nHere? https://github.com/jazzband/pip-tools/blob/b6a9f1fb3423dd189f050fac31ac9e47b05178e8/piptools/scripts/compile.py#L180\nAdditionally, the evaluate() method takes an environment arg, which presumably means that a command-line arg to pip-compile could be used to specify the target environment (I don't know exactly what form the environment arg takes at this time).. No, check this out. Say I have a requirements.txt like such:\ncairocffi\neditdistance\nh5py>=2.7.0\nkeras==2.0.8\npillow; platform_machine == 'armv7l'\npillow-simd; platform_machine != 'armv7l'\nrequests==2.18.4\nscikit-learn[alldeps]\nsklearn\ntensorflow-gpu==1.3.0; platform_machine != 'armv7l' and platform_system != 'Darwin'\ntheano\nNow let's say I use the following code snippet (which is pieced together from a REPL session and basically emulates what pip-compile is doing):\n```\nimport optparse\nimport pip\nfrom pip.req import parse_requirements\nfrom piptools.repositories.pypi import PyPIRepository\nfrom piptools.resolver import Resolver\nclass PipCommand(pip.basecommand.Command):\n    name = 'PipCommand'\ndef main():\n    pip_command = get_pip_command()\n    pip_args = []\n    pip_options, _ = pip_command.parse_args(pip_args)\nsession = pip_command._build_session(pip_options)\nrepository = PyPIRepository(pip_options, session)\n\nconstraints = list(\n    parse_requirements(\n        'requirements.txt',\n        finder=repository.finder,\n        session=repository.session,\n        options=pip_options))\n\nResolver.check_constraints(constraints)\nresolver = Resolver(constraints, repository)\nresults = resolver.resolve()\n\nimport pprint\npprint.pprint(results)\n\ndef get_pip_command():\n    # Use pip's parser for pip.conf management and defaults.\n    # General options (find_links, index_url, extra_index_url, trusted_host,\n    # and pre) are defered to pip.\n    pip_command = PipCommand()\n    index_opts = pip.cmdoptions.make_option_group(\n        pip.cmdoptions.index_group,\n        pip_command.parser,\n    )\n    pip_command.parser.insert_option_group(0, index_opts)\n    pip_command.parser.add_option(optparse.Option('--pre', action='store_true', default=False))\nreturn pip_command\n\nif name == 'main':\n    main()\n```\nIf you run this, you get the exact same error as reported. However, if we now filter out the constraints that don't match the specified markers in requirements.txt, the resolver is happy (and so is the respository). This is accomplished with this, just before the call to Resolver.check_constraints(constraints):\nconstraints = [x for x in constraints if not x.markers or x.markers.evaluate()]\nWe are telling pip-compile to honor the markers specified in the top-level requirements passed in. This doesn't solve any markers on transitive dependencies that might not match the platform, but that doesn't matter if the top-level ones are properly specified.. I see, sorry for the rat hole, carry on :)\n. ",
    "csilvers": "(I guess I can't attach python files, so we'll just go with the pasted version above I guess.). ",
    "pgrzesik": "Hey @vphilippon, sorry about the change of setup.py to requirements, I totally messed that up. Should be fixed now.\nRegards,\nPiotr. ",
    "haylo75": "Thanks for the context.  I checked out master and tornado 4.5.2 and its dependencies compiled just fine.  I'll close this issue since it was already addressed.. ",
    "wooyek": "Sweet :) . ",
    "bittner": "\n\nThis is due to flake8==3.5.0 (flake8==3.4.1 passes)\n\nSo, you're saying this is just really a vendoring of contextlib2\n\nThat only means that the checks are more strict now. We shouldn't look into pinning or purposely downgrading, I feel.\nWhat if we catch Exception and let errors fall through? See this SO answer for comparison.. Let me know if you want this to be fixed, and which solution you prefer. I can happily prepare a PR.. For some reason Travis wouldn't want to run only flake8 and only checkdocs in an included build matrix job. Hence I have split up the matrix build, so that every PIP version now gets its own Travis job.\nThough this takes a little bit longer to run now, probably, it enhances readability on Travis. So, I hope the additional change is fine.. The failing build is related to #592. This needs to be fixed outside this PR, probably.\nIt also needs real-world verification whether the relative URL of the illustration in the README will work on PyPI, or whether an absolute URL is needed:\nrest\n.. image:: ./img/pip-tools-overview.png\n   :alt: pip-tools overview for phase II. > If anyone can deal with #592, that would be great.\n@vphilippon If you could explain which exceptions may occur on line 110 the complaint may be easy to fix. Maybe it's not necessary to catch errors, because they indicate something serious has gone wrong.\nThe comment seems to excuse that there must be a better way, a more readable, self-explanatory implementation to cover the use case. (For example, the if clause on line 94 can probably be moved into the encapsulated function.). I've replaced the relative image URL by an absolute one that points to the GitHub jazzband account. There seems to be no way to have the image also uploaded to PyPI. So, this change is needed.. The PR should be alright now. Ready for merging.. One day someone may want to add some PyPy install commands to the AppVeyor configuration.. You're welcome!\nThe world becomes a better place when you do a good thing from time to time. :smiley: . Looks like you have to re-trigger the build. Travis has hiccups.. I would suggest to remove the [travis:after] section.\nIt is deprecated, and Travis build stages should be used instead, as explained in the warning of the related tox-travis documentation.. Keep the deployment stuff out of tox! It's meant for testing, not deployment.\nUsing just Travis should correctly get deployment going when all build jobs have completed successfully.\nNow I remember what disturbed me when I prepared my last PR... Sorry for not alerting you earlier.. Lucky me! I usually do a package.__doc__.strip() as a reflex. I never stepped into that issue. Good to know. And thanks for investigating. :+1: . I don't think code blocks are a culprit for non-rendering.. ",
    "lukeschlather": "Packages that are range-pinned in this way are designed to be tolerant to older versions of hard-pinned packages, or to have no dependencies.\nAnd so there's a bit of trouble I think for us with having a separate requirements.txt, since we really only want first-party packages to be range-pinned. We want any transitive dependencies to be hard-pinned (and pip-tools lets us do that easily, but it breaks our range-pinning assumption.)\nI wrote a patch that I'm pretty sure does exactly what we want though I can see how it might not be worth it on your end: https://github.com/lukeschlather/pip-tools/commit/7e4255901f4e18483d3bca70f217cce262d5e2df. ",
    "techalchemy": "FYI there is nothing you can do about this except maybe grab our temporarydirectory handler and its cascading permissions calls which try to assign permissions but fail with a warning instead of an error... this relates specifically to git index permission assignment on windows in nested subprocess trees.  . OK I hit most of the major items -- I have no idea if this will work with the matrix in question becuase of the way it grabs the index url -- we may need to keep this pointed at the current pypi.. Alrighty so I was wrong, but now this is mostly working except a tiny pip8 error and I would appreciate any guidance at all on what to do with that one. scratch that, it was just a new style marker test. I don't have particularly strong feelings except to say that we vendor pip over in pipenv (and have been doing so) in order to maintain compatibility with piptools specifically and that also can cause issues related to certificates, but gives us some flexibility to make changes to the dependency resolver directly.  In all likelihood we will end up vendoring pip 10 anyway so I can see an argument for that, but if you're not modifying the resolver it may not make sense.\nOverall they moved some APIs around primarily to drive some discussion about whether the internal apis really ought to be available externally, and we are in touch about that (I believe pipenv is probably the worst offender but also is part of the PyPA and works collaboratively with the pip team). Overall I don't see much risk of any sweeping API changes, but I guess you never know. Either approach is going to come with maintenance overhead. I think that's just the cost of maintainership.\nIn the end no decision is going to protect pip-tools from having to handle adjusting the codebase in the future, I'm kind of split on this one.  IMO unvendoring is easiest. Also sincere apologies I know this is nearly impossible to review properly, here is a near-complete set of the changes (sans the first commit) without the removal of pip 9:\nhttps://github.com/jazzband/pip-tools/pull/657/files/4c7fce7897e4dbbba524b59e0a52ee38e37a7aa6..3ba2956232fad316b8c07daf65c7a23d4aa96033. @suutari no problem I agree with that rebase / squash. @tysonclugg I think this approach makes the most sense for now as well except that I believe pip is switching to calver which will complicate this. . > we should include a pipmaster factor\nBest suggestion yet. I honestly think building against master to stay ahead of issues is the best bet. But I don\u2019t think there is much risk of huge sweeping changes in the near future . Sorry for the delay on this I\u2019ve been trying to manage a pipenv release. Feel free to push a pipmaster factor here to speed things up, no issues with that from my end . @tysonclugg this is because we are configuring the environment to tell the tests what we are expecting to see, i.e. in this case we expect pip 9 and above to pass the test and we indicate which pip version we expect via the environment.  What we actually get in the code may be different, so to avoid having to build sanity checks everywhere it's just easier to check the thing we are setting everything based off of.. @tysonclugg sure but it's a bit different for pytest, just a sec and I'll add it. @tysonclugg because that's the framework we're using, why would we use two frameworks when we can use one which we use for the rest of the testing? I'm not 'shunning' anything.. OK all, squashed down to 2 commits, building against pip@master, hopefully this is good to go. @tysonclugg if your intention in asking to add the pip master branch here was to get accolades by jumping onto this PR then I apologize, but we discussed the squash much earlier and it was always going to happen with or without your contributions, which will still be attributed to you even if the specific commit isn\u2019t. I am pretty careful to always include credit on the lines by the author in question, which fit handles just fine. \nBasically, I am mindful of it. I changed some things, I appreciate your willingness to help and I kept credit where possible, but this was always getting squashed. It was discussed and decided before you ever asked if you could make your change. \nYou should consider the reason for your offer here\u2014 are you here to help out the project, or are you merely interest in attribution?\nI realize those are not mutually exclusive, but this is how it works. Sometimes your stuff gets reworked. If you make it easy to collaborate with you, it\u2019s a lot more likely that projects will cooperate on bigger changes . Please stop derailing this PR over a line of testing instructions that is already attributed to you. . All set on the changes. (and we rely heavily on, and modify and push changes back to pip-tools itself). FYI I believe we accounted for this in #672 . Thanks for posting your fixes by the way, if there's anything else you changed let me know!. @suutari FYI I released a new library recently which you might be interested in to clean up some of the pip import code--\nhttps://pypi.org/project/pip-shims/ . Hi there, pipenv maintainer here :) just to chime in briefly, pip-tools is amazing and will likely always be faster than pipenv.  It occupies a space somewhere between pip and pipenv and it performs a core set of tasks in a pretty intuitive and straightforward way.  We used to patch it a lot more heavily than we do, and improvements in pip-tools always land in pipenv (and we can likely open issues on some of the things we've changed to see if it makes any sense to merge those back in upstream). \nPip-tools is quite incredible and I'm pretty sure it has been the reference implementation and the jumping off point for most of the other resolution tooling.  We have been kicking around some new libraries to handle true sat solving/backtracking but again, it will likely not compete with piptools and it still borrows heavily from many of the constructs here.  I spend about 50% of my pipenv related code time messing around in piptools for various reasons.  I would be quite devastated if y'all just ditched it !. Pipenv definitely doesn\u2019t discourage this. pipenv install -e .. Yeah this is just totally non-functional if you don't point at the new pypi, but I am not sure if that's an issue of testing in tox environments and not in proper environments.. It does.. Latest is tested, which is 18.  It doesn't particularly matter because the import shims are rewritten to handle both possibilities gracefully. Using the new import shims it tries each possibility and falls back to the next ones\n```python\n\n\n\ndef do_import(module_path, subimport=None, old_path=None):\n  2         old_path = old_path or module_path\n  3         prefixes = [\"pip.internal\", \"pip\"]\n  4         paths = [module_path, old_path]\n  5         search_order = [\"{0}.{1}\".format(p, pth) for p in prefixes for pth in paths if pth is not None]\n  6         package = subimport if subimport else None\n  7         for to_import in search_order:\n  8             if not subimport:\n  9                 to_import, , package = to_import.rpartition(\".\")\n 10             print('Import target: %s\\tpackage: %s' % (to_import, package))\ndo_import('cli.base_command', 'Command', old_path='basecommand')\nImport target: pip._internal.cli.base_command   package: Command\nImport target: pip._internal.basecommand        package: Command\nImport target: pip.cli.base_command     package: Command\nImport target: pip.basecommand  package: Command\n```. And in case you need proof:\n\n\n\n```console\n ~/g/pip-tools \ue0b0 \uf418 pip18-support \uf128  \ue0b0 vf tmp --python=python3.6\nRunning virtualenv with interpreter /home/hawk/.pyenv/shims/python3.6\nUsing base prefix '/home/hawk/.pyenv/versions/3.6.5'\nNew python executable in /home/hawk/.virtualenvs/tempenv-3369325970666/bin/python3.6\nAlso creating executable in /home/hawk/.virtualenvs/tempenv-3369325970666/bin/python\nInstalling setuptools, pip, wheel...done.\n \ue73c \u00b3 tempenv-3369325970666 \ue0b0 ~/g/pip-tools \ue0b0 \uf418 pip18-support \uf128  \ue0b0 pip install -e .\nObtaining file:///home/hawk/git/pip-tools\n[...]\nInstalling collected packages: click, first, six, pip-tools\n  Running setup.py develop for pip-tools\nSuccessfully installed click-6.7 first-2.0.1 pip-tools six-1.11.0\n \ue73c \u00b3 tempenv-3369325970666 \ue0b0 ~/g/pip-tools \ue0b0 \uf418 pip18-support \uf128  \ue0b0 pip install ptpython\nCollecting ptpython\n  Downloading https://files.pythonhosted.org/packages/d0/dd/163a698a86b9de92857f128117034bdb36f5a784d839eea3bc07e2c858ae/ptpython-0.41-py3-none-any.whl (47kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 1.1MB/s\n[...]\n\ue73c \u00b3 tempenv-3369325970666 \ue0b0 ~/g/pip-tools \ue0b0 \uf418 pip18-support \uf128  \ue0b0 ptpython\n\n\n\nfrom pip import version as pip_version\nprint(pip_version)\n18.0\nfrom piptools._compat import Command\nCommand\n\n``. FYI if you are using the code for importing from thepip_compat` file I recommend grabbing the updated version, it borrows some concepts from https://github.com/brettcannon/modutil to do imports a bit more efficiently. \n\n\n",
    "mkolodny": "Great catch seeing that there's no PyPi distribution for jaeger-client-python. That helped a lot in finding the problem:\njaeger-client-python is actually an internal company python package that depends on jaeger-client. So there was no duplicate - they're different packages with very similar names.. ",
    "davidjlloyd": "@suutari Turns out it doesn't make any difference. Setuptools Distribution class separates out any installation requirements with environment markers, and then merges them into extras_requires, so the end result is the same from the perspective of the returned Distribution object! On the plus side, this means that any solution to this issue should work for both methods of specifying conditions on requirements. I was using the older method (via extras_require) because this is supported since setuptools 18, whereas the method you suggest is only available since setuptools 36.2 and is not fully supported by pip.\nThe Distribution object provides an extras_require attribute containing the necessary information, but I'm not sure how we would want to process it. We either need to evaluate if the marker conditions are met and include/omit based on that, or we need the markers to exist in the generated requirements file. The latter is nice as it allows single requirements files to work across all environments, but then this should probably be applied to inherited dependencies too. Currently inherited dependencies are included based on marker conditions, and markers are not preserved in the generated requirements file.. ",
    "Hernrup": "We just ran head first into this one. We are not a package but rather have a requirements.in that needs to be resolved into frozen packages. In this case one of our dependencies have a extras_require section. This issue means that one dependency is going to be missing completely in the resulting requirements.txt and fail the entire install as all packages needs to be frozen in --require-hashes mode.\nWe cant see any workaround other then depending on the package specified in extras_require ourselvs. This is very inconvenient and makes upgrades a lot less flexible.\nWe would very much like to see a fix for this as the whole purpose of pip-compile seams to be to solve this kind problem.\nI might try to get a PR in for it if I get some time on my hands but in the meantime please be advised that this issue has a real and serious impact without workaround.. ",
    "moggers87": "@vphilippon Inboxen would like to move to using setup.py (and in future be installable from PyPI) and we often find that we want to use unreleased versions of packages. As an open source project, a private PyPI is out of the question.\nIt is worth noting that pip does not honour dependency-links either, unless you specify --process-dependency-links. How hard would it be to implement such an option for pip-compile?. Having a quick look at the source code, would I be wrong in thinking that --process-dependency-links could be conditionally added here?. ",
    "dizlv": "Any updates on this? We have some custom libraries, written by our own, that are stored in private git repositories. We would like to put them in setup.py, but seems it's not possible right now?. ",
    "wizpig64": "pip-sync has --quiet, pip-compile does not.. ",
    "CapedHero": "Bump. It would be a truly sweet feature.. ",
    "tonal": "For empty changes also not empty output:\n$ pip-sync -q\nEverything up-to-date. ",
    "jasonm": "Yeah, I just ran into this too:\n```bash\n$ pip-sync --version\npip-sync, version 1.11.0\n$ pip-sync --exists-action w\nError: no such option: --exists-action\n```\nI think that an implementation would look very much like the way #283 solves for #192 by adding a passthrough for --index-url and --extra-index-url.. @vphilippon @davidovich would such a PR be welcome? I'm new to contributing to pip-tools and via jazzband in general. I'm CCing you two based on the pip-tools lead discussion at https://github.com/jazzband/roadies/issues/64, but LMK if there's someone else I should ask. Thanks!. ",
    "mattgiles": "Maybe this issue does, in fact, belong on boto3. Going to open an issue there.. ",
    "onyb": "@vphilippon I had a similar issue while trying to install jsonschema==2.6.0 inside a Python 3.6 virtualenv. If you look at the install_requires section of jsonschema's setup.py file here, you'll find this:\npy\ninstall_requires=[\n    \"attrs>=17.3.0\",\n    \"pyrsistent>=0.14.0\",\n    \"six>=1.11.0\",\n    \"functools32;python_version<'3'\",\n],\npip-compile is unable to ignore the functools32 dependency under Python 3.6, and I get the following error while installing from the requirements.txt: \"This backport is for Python 2.7 only.\"\nCould you please point me to the specific sections of the codebase where I can take a look for investigating the bug, so that I can submit a PR?. ",
    "desaintmartin": "Hey, this looks interesting in adding one handy feature of pip into pip-tools. How can we go further with this? I think #674 is a good start.. ",
    "john-bodley": "Thanks @vphilippon for the explanation. . ",
    "justicz": "This change doesn\u2019t handle the case where new distributions are uploaded later for the same version. Is that going to be a problem?\nIt\u2019s definitely a good observation, but IMO that sounds like it will be a pretty uncommon issue.. OK. In that particular case, would it be acceptable for you to just delete the hashes in the generated output file for that package? Then they'll just be regenerated the usual, slow way. I would tend to think that this change provides the better UX 99% of the time.. > A pip-compile with an existing requirements.txt would not have updated the versions (pins) unless required to, or explicitly told to do so with --upgrade or --upgrade-package.\n~Good to know! My mistake, I guess the security benefit I mentioned earlier is actually already the default behavior then?~\nEdit: Oh, never mind -- I missed that your comment was talking about versions and hash pins separately. In that case, my point about increasing security by not updating hashes still stands I think.. ",
    "jbergknoff-rival": "Thanks! I've updated the CLI parameter to --user, though kept its name internally as user_only for clarity, and the --user flag is now passed through to pip install (pretty important!).\nTangent on usage in a container: what you say is largely true. However, during development, when it makes more sense to volume the vendor directory into a container than to build it into an image, it's convenient to have that vendor directory front and center (i.e. $(pwd)/vendor) rather than tucked away in the usual place, so I still prefer the pip install --user approach. Also, sometimes I work from a vanilla Python base image, but other times from an image with various CLI tools globally installed (e.g. piptools, bumpversion). In that case, having the --user separation is very useful.\nHere's a quick rundown of how I've manually tested this change:\n```\n$ docker run -it --rm -v $(pwd):$(pwd) -w $(pwd) python:3.6.4-alpine3.7 sh\napk add --no-cache git\npython setup.py install\ncd /tmp\necho requests > requirements.in\npip-compile requirements.in\n...\ncertifi==2018.1.18        # via requests\nchardet==3.0.4            # via requests\nidna==2.6                 # via requests\nrequests==2.18.4\nurllib3==1.22             # via requests\npip install bumpversion\npip freeze\nbumpversion==0.5.3\nclick==6.7\nfirst==2.0.1\npip-tools==1.11.1.dev20+g712823f\nsix==1.11.0\nPYTHONUSERBASE=/tmp/vendor pip freeze --user\nPYTHONUSERBASE=/tmp/vendor pip install --user flask\nPYTHONUSERBASE=/tmp/vendor pip freeze --user\nFlask==0.12.2\nitsdangerous==0.24\nJinja2==2.10\nMarkupSafe==1.0\nWerkzeug==0.14.1\nPYTHONUSERBASE=/tmp/vendor pip-sync --user\n...\nSuccessfully installed certifi-2018.1.18 chardet-3.0.4 idna-2.6 requests-2.18.4 urllib3-1.22\npip freeze\nbumpversion==0.5.3\nclick==6.7\nfirst==2.0.1\npip-tools==1.11.1.dev20+g712823f\nsix==1.11.0\nPYTHONUSERBASE=/tmp/vendor pip freeze --user\ncertifi==2018.1.18\nchardet==3.0.4\nidna==2.6\nrequests==2.18.4\nurllib3==1.22\n```\nSo Flask and its dependencies get installed in /tmp/vendor, and bumpversion gets installed globally. The pip-compile output has just requests and its dependencies. The result of pip-sync is that global stuff (bumpversion) is left alone, Flask and its dependencies are uninstalled, and requests and its dependencies are installed in the user base directory. To me this all seems correct.. ",
    "M0dM": "Pip tools was installed using pip2.7 in this my venv.... ",
    "cyrilleverrier": "pip/req has been moved to _internal folder in pip>=10.0.0:\nhttps://github.com/pypa/pip/tree/release/10.0.0/src/pip/_internal. Thanks @vphilippon. Looking forward to testing it.. ",
    "slykar": "@vphilippon if that's true, then I might have misunderstood the README file?\n$ pip-compile -P flask -P requests==2.0.0  # update the flask package to the latest, and requests to v2.0.0\nNote the requests package example.. For now I can only update the README file. If I find the time I would be happy to implement this.\nEdit:\n\nIf you want to upgrade to a specific version, you can pin it in the requirements.in.\n\nI think I had the same issue with this approach, but can't recall it right now.\nI'll probably try again tomorrow, as I'm in a process of updating some project dependencies step by step.\n. ",
    "kramarz": "I didn't. My bad. I am closing this.\nI was using CI script in docker image and there was pip install pip-tools command and I assumed the image didn't contain pip-tools but it did. \nI know it is a bad excuse.. ",
    "WoLpH": "This pull request is to fix: https://github.com/pypa/pipenv/issues/2046. You're right, that's a small oversight from me. Initially I was using it but dropped it in favour of an explicit loop instead of a few list comprehensions.. I actually thought it wouldn't occur because in that case it would match, but that's obviously not always the case... I'll see what I can do about it :). ",
    "Miserlou": "Confirmed with Python2.7 OSX as well.. ",
    "MichaelAquilina": "Also experiencing this issue, is there a suggested workaround in place?. ",
    "cs01": "Interesting, thanks!. ",
    "strichter": "In my case the extra deps are not added to requirements.txt.. ",
    "regisb": "I observe the same behaviour, even though the local project does not have extra dependencies. This is annoying because it forces us to manually run pip install -e . after every pip-sync.\nThis issue seems to be related to PR #507.\n. ",
    "mm-matthias": "Duplicate of #204. ",
    "oleng": "I second this. 6 years is a really long time ago, especially for a workflow tool.. ",
    "reygarcia2009": "I forgot to update this. I recreated my virtual env and started to working in it which resolved the issue.. ",
    "akaIDIOT": "Can confirm @techalchemy's fixes work for me, thanks for the effort! :+1: \nAny ideas on when this will get merged + released to PyPI?. ",
    "maciej-gol": "Adding the co-author @AmiroNasr to the conversation.. Tests are failing because of broken newest pip integration.. ",
    "kevindawson": "@vphilippon is this of any use to you\ntext\nkevin@dual:~/git/pylsusb$ pip-compile\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-compile\", line 11, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/piptools/scripts/compile.py\", line 166, in cli\n    dist = run_setup(src_file)\n  File \"/usr/lib/python3.6/distutils/core.py\", line 227, in run_setup\n    script_name)\nRuntimeError: 'distutils.core.setup()' was never called -- perhaps 'setup.py' is not a Distutils setup script?\n$ pip list |grep click\n$ pip3 list |grep click\nclick                  6.7\n$ pip list |grep first\n$ pip3 list |grep first\nfirst                  2.0.1\n$ pip list |grep six\nsix                    1.11.0     \n$ pip3 list |grep six\nsix                    1.11.0\n$ pip list |grep pip-tools\n$ pip3 list |grep pip-tools\npip-tools              2.0.2\nI normal only use python3.6, but python2.7 install as part of distro ubuntu 18.04\nmany thanks\n. @vphilippon\nwhy do I have to fill out a install_requires section\nshould your app not be doing that for me\nif I put my package name in\npython\n    install_requires=[\n        'my_project',\n    ],\nall I get:\ntext\nCould not find a version that matches my_project\nNo versions found\nWas https://pypi.python.org/simple reachable?. @vphilippon \nThanks for the reading list, until I have RTFM :))\nlater. @vphilippon \n * after some RTFM \n * watching Kenneth Reitz - Pipenv: The Future of Python Dependency Management - PyCon 2018\n * I have decided to hack around with pipenv\nregards. ",
    "codingjoe": "Wow, thank you all for the great feedback. This was a really nice discussion and it helped me to understand a couple more details.\nMy suggestion now would be to make this information also available to users users. There is a other tools section in the readme file. Maybe we can extend that a bit, add a reference to Pipenv and write one or two crisp sentences that sum up this thread. Maybe we even add a link for people like me who are interested in the full story.\nSounds good?. ",
    "androiddrew": "I guess I am a little late to the discussion, but I am going to chime in anyway. When I discovered pip-tools I dropped considering Pipenv.  Pipenv is popular, but that does not make it the right tool for my projects. Admittedly, I choose Foundation over Bootstrap, Vue over React, etc. so maybe I just don't like there being one game in town. \nI developed a distaste for the Pipfile largely due to the fact that I make my applications installable via a setup.py file. It seemed like the Pipenv project was steering people to abandon this practice. Pip-tools gives me all the capabilities I need to both define a development environment, and create an installable application.  For me it's the difference between a manual or an automatic transmission. I like my manual and maybe that makes me antiquated. \n. ",
    "eseifert": "I guess this is related to commit https://github.com/pypa/pip/commit/a5a07fe61c4861c58fded7420c929d56cde11a89, which moved pip._internal.req.req_install.InstallRequirement.from_line to pip._internal.req.constructors.install_req_from_line.. No problem. That's great news!\npip._internal.req.req_install.InstallRequirement.from_editable was also moved to pip._internal.req.constructors.install_req_from_editable by the way (see https://github.com/pypa/pip/commit/69b494aa29096838addad0735359f9406d74252a).. ",
    "helpse": "PLEASE merge it!. ",
    "sp-ricard-valverde": "Sorry, it was me the one from the previous post. Didn't notice I was logged in as one of our bot accounts \ud83d\ude1d . ",
    "peterHoburg": "I am having the same issue. \nI have tried this with fresh conda environments, old ones, simple requirements.txt or complicated ones. All of them fail. \n\nOS Type: MacOSX 10.14\nPython Version: Python 2.7.14 |Anaconda, Inc.\npip version: pip 10.0.1 from /Users/username_here/miniconda3/envs/api/lib/python2.7/site-packages/pip (python 2.7)\npip-tools version: pip-tools==3.1.0\n\nSteps to Replicate:\n\nMake a fresh conda environment with conda create --name example python=2.7.14 pip\npip install pip-tools\nnavigate to my directory with a requirements.txt in it\npip-sync\n\nExpected result:\nInstall all of the python packages in requirements.txt\nActual results:\n$ pip-sync\nCannot uninstall 'Python'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\nYou are using pip version 10.0.1, however version 18.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nTraceback (most recent call last):\n  File \"/Users/peterhoburg/miniconda3/envs/api/bin/pip-sync\", line 11, in <module>\n    sys.exit(cli())\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/click/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/click/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 74, in cli\n    install_flags=install_flags))\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/site-packages/piptools/sync.py\", line 149, in sync\n    check_call([pip, 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n  File \"/Users/peterhoburg/miniconda3/envs/api/lib/python2.7/subprocess.py\", line 186, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['/Users/peterhoburg/miniconda3/envs/api/bin/pip', 'uninstall', '-y', u'python', u'wsgiref']' returned non-zero exit status 1. ",
    "gsong": "This is reproducible in a generic Python 2.7 Docker container as well. Rolling back to pip 9.x  is a work-around:\n\n\nStart a Python 2.7 Docker container\nsh\ndocker run --rm -it python:2.7 /bin/bash\n\n\nTry to install requirements using stock pip\n```sh\n$ pip --version\npip 18.1 from /usr/local/lib/python2.7/site-packages/pip (python 2.7)\n$ cd /tmp\n$ echo \"requests\" > requirements.in\n$ pip install pip-tools\nCollecting pip-tools\n  Downloading https://files.pythonhosted.org/packages/4a/16/7d7e908a657b8b41ce665e38caaa1a4415707bdb7b06524deec9802f0b1c/pip_tools-3.2.0-py2.py3-none-any.whl (43kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.4MB/s\nCollecting click>=6 (from pip-tools)\n  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 6.9MB/s\nCollecting six (from pip-tools)\n  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\nInstalling collected packages: click, six, pip-tools\nSuccessfully installed click-7.0 pip-tools-3.2.0 six-1.12.0\n$ pip-compile\n\nThis file is autogenerated by pip-compile\nTo update, run:\n\npip-compile --output-file requirements.txt requirements.in\n\ncertifi==2018.11.29       # via requests\nchardet==3.0.4            # via requests\nidna==2.8                 # via requests\nrequests==2.21.0\nurllib3==1.24.1           # via requests\n$ pip-sync\nCannot uninstall 'Python'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip-sync\", line 11, in \n    sys.exit(cli())\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 764, in call\n    return self.main(args, kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/usr/local/lib/python2.7/site-packages/click/core.py\", line 555, in invoke\n    return callback(args, **kwargs)\n  File \"/usr/local/lib/python2.7/site-packages/piptools/scripts/sync.py\", line 74, in cli\n    install_flags=install_flags))\n  File \"/usr/local/lib/python2.7/site-packages/piptools/sync.py\", line 149, in sync\n    check_call([pip, 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n  File \"/usr/local/lib/python2.7/subprocess.py\", line 190, in check_call\n    raise CalledProcessError(retcode, cmd)\nsubprocess.CalledProcessError: Command '['/usr/local/bin/pip', 'uninstall', '-y', u'python', u'virtualenv', u'wsgiref']' returned non-zero exit status 1\n```\n\n\nRoll back to pip 9.x\n```sh\n$ pip install \"pip<10\"\nCollecting pip<10\n  Downloading https://files.pythonhosted.org/packages/ac/95/a05b56bb975efa78d3557efa36acaf9cf5d2fd0ee0062060493687432e03/pip-9.0.3-py2.py3-none-any.whl (1.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 8.5MB/s\nInstalling collected packages: pip\n  Found existing installation: pip 18.1\n    Uninstalling pip-18.1:\n      Successfully uninstalled pip-18.1\nSuccessfully installed pip-9.0.3\n$ pip-sync\nDEPRECATION: Uninstalling a distutils installed project (python) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\nUninstalling Python-2.7.15:\n  Successfully uninstalled Python-2.7.15\nUninstalling virtualenv-16.1.0:\n  Successfully uninstalled virtualenv-16.1.0\nNot uninstalling wsgiref at /usr/local/lib/python2.7, as it is in the standard library.\nYou are using pip version 9.0.3, however version 18.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nCollecting certifi==2018.11.29\n  Cache entry deserialization failed, entry ignored\n  Cache entry deserialization failed, entry ignored\n  Downloading https://files.pythonhosted.org/packages/9f/e0/accfc1b56b57e9750eba272e24c4dddeac86852c2bebd1236674d7887e8a/certifi-2018.11.29-py2.py3-none-any.whl (154kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 4.6MB/s\nCollecting chardet==3.0.4\n  Cache entry deserialization failed, entry ignored\n  Cache entry deserialization failed, entry ignored\n  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 143kB 5.3MB/s\nCollecting idna==2.8\n  Cache entry deserialization failed, entry ignored\n  Cache entry deserialization failed, entry ignored\n  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 5.0MB/s\nCollecting requests==2.21.0\n  Cache entry deserialization failed, entry ignored\n  Cache entry deserialization failed, entry ignored\n  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 10.0MB/s\nCollecting urllib3==1.24.1\n  Cache entry deserialization failed, entry ignored\n  Cache entry deserialization failed, entry ignored\n  Downloading https://files.pythonhosted.org/packages/62/00/ee1d7de624db8ba7090d1226aebefab96a2c71cd5cfa7629d6ad3f61b79e/urllib3-1.24.1-py2.py3-none-any.whl (118kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122kB 6.4MB/s\nInstalling collected packages: certifi, chardet, idna, urllib3, requests\nSuccessfully installed certifi-2018.11.29 chardet-3.0.4 idna-2.8 requests-2.21.0 urllib3-1.24.1\nYou are using pip version 9.0.3, however version 18.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n```. \n\n",
    "nbelakovski": "I'm having varying results in replicating this on different machines, but it seems that adding --rebuild and --upgrade to the pip-compile command help with reproducibility.. Looking through that PR, if I understand it correctly, I think that the behavior is correct if, using the example above, 4.0a6 was the latest version of the coverage library. If that were the case, the pin would break without --pre. But assuming 4.0 or a higher production release is available, then pip-compile should no longer look for pre-release packages.. Oh, well that seems silly. I could see someone pinning to a pre-release version wanting future pre-releases from that series, but I'd think in general one would want to avoid pre-releases since they could contain breaking code. But this would be outside the scope of pip-tools, so I guess we should close this issue?. ",
    "thisfred": "I'd be in favor of making --no-index the default behaviour. Explicitly passing in the extra index urls when needed seems less likely to cause problems in the long run.. ",
    "cayla": "For fun, here is a completely vanilla / clean Ubuntu 16.04 box exhibiting the same result of passing dependency resolution of an impossible set of requirements.  \n\nCollapsed because its long:\n\n```\n[11:46:26][cayla@bigboy.local]% vagrant init ubuntu/xenial64\nA `Vagrantfile` has been placed in this directory. You are now\nready to `vagrant up` your first virtual environment! Please read\nthe comments in the Vagrantfile as well as documentation on\n`vagrantup.com` for more information on using Vagrant.\n[~/dev/tmp]\n[11:46:28][cayla@bigboy.local]% vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing base box 'ubuntu/xenial64'...\n==> default: Matching MAC address for NAT networking...\n==> default: Checking if box 'ubuntu/xenial64' is up to date...\n==> default: Setting the name of the VM: tmp_default_1543596397362_82837\n==> default: Fixed port collision for 22 => 2222. Now on port 2203.\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n==> default: Forwarding ports...\n    default: 22 (guest) => 2203 (host) (adapter 1)\n==> default: Running 'pre-boot' VM customizations...\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2203\n    default: SSH username: vagrant\n    default: SSH auth method: private key\n    default: Warning: Connection reset. Retrying...\n    default: Warning: Remote connection disconnect. Retrying...\n    default:\n    default: Vagrant insecure key detected. Vagrant will automatically replace\n    default: this with a newly generated keypair for better security.\n    default:\n    default: Inserting generated public key within guest...\n    default: Removing insecure key from the guest if it's present...\n    default: Key inserted! Disconnecting and reconnecting using new SSH key...\n==> default: Machine booted and ready!\n==> default: Checking for guest additions in VM...\n    default: The guest additions on this VM do not match the installed version of\n    default: VirtualBox! In most cases this is fine, but in rare cases it can\n    default: prevent things such as shared folders from working properly. If you see\n    default: shared folder errors, please make sure the guest additions within the\n    default: virtual machine match the version of VirtualBox you have installed on\n    default: your host and reload your VM.\n    default:\n    default: Guest Additions Version: 5.1.38\n    default: VirtualBox Version: 5.2\n==> default: Mounting shared folders...\n    default: /vagrant => /Users/cayla/dev/tmp\n[~/dev/tmp]\n[11:47:14][cayla@bigboy.local]% vagrant ssh\nWelcome to Ubuntu 16.04.5 LTS (GNU/Linux 4.4.0-139-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  Get cloud support with Ubuntu Advantage Cloud Guest:\n    http://www.ubuntu.com/business/services/cloud\n\n0 packages can be updated.\n0 updates are security updates.\n\nNew release '18.04.1 LTS' available.\nRun 'do-release-upgrade' to upgrade to it.\n\n\nvagrant@ubuntu-xenial:~$ sudo apt-get update; sudo apt-get -y install python-pip virtualenv\nHit:1 http://archive.ubuntu.com/ubuntu xenial InRelease\nGet:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]\nGet:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]\nGet:4 http://security.ubuntu.com/ubuntu xenial-security/main Sources [138 kB]\nGet:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\nGet:6 http://archive.ubuntu.com/ubuntu xenial/main Sources [868 kB]\nGet:7 http://security.ubuntu.com/ubuntu xenial-security/restricted Sources [2,116 B]\nGet:8 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [88.3 kB]\nGet:9 http://security.ubuntu.com/ubuntu xenial-security/multiverse Sources [2,468 B]\nGet:10 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [585 kB]\nGet:11 http://archive.ubuntu.com/ubuntu xenial/restricted Sources [4,808 B]\nGet:12 http://archive.ubuntu.com/ubuntu xenial/universe Sources [7,728 kB]\nGet:13 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [402 kB]\nGet:14 http://security.ubuntu.com/ubuntu xenial-security/universe Translation-en [157 kB]\nGet:15 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3,724 B]\nGet:16 http://security.ubuntu.com/ubuntu xenial-security/multiverse Translation-en [1,844 B]\nGet:17 http://archive.ubuntu.com/ubuntu xenial/multiverse Sources [179 kB]\nGet:18 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [7,532 kB]\nGet:19 http://archive.ubuntu.com/ubuntu xenial/universe Translation-en [4,354 kB]\nGet:20 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [144 kB]\nGet:21 http://archive.ubuntu.com/ubuntu xenial/multiverse Translation-en [106 kB]\nGet:22 http://archive.ubuntu.com/ubuntu xenial-updates/main Sources [326 kB]\nGet:23 http://archive.ubuntu.com/ubuntu xenial-updates/restricted Sources [2,528 B]\nGet:24 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [235 kB]\nGet:25 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse Sources [8,740 B]\nGet:26 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [884 kB]\nGet:27 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [710 kB]\nGet:28 http://archive.ubuntu.com/ubuntu xenial-updates/universe Translation-en [290 kB]\nGet:29 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [16.6 kB]\nGet:30 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse Translation-en [8,440 B]\nGet:31 http://archive.ubuntu.com/ubuntu xenial-backports/main Sources [4,856 B]\nGet:32 http://archive.ubuntu.com/ubuntu xenial-backports/universe Sources [6,740 B]\nGet:33 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [7,288 B]\nGet:34 http://archive.ubuntu.com/ubuntu xenial-backports/main Translation-en [4,456 B]\nGet:35 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [7,804 B]\nGet:36 http://archive.ubuntu.com/ubuntu xenial-backports/universe Translation-en [4,184 B]\nFetched 25.1 MB in 4s (5,421 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following additional packages will be installed:\n  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5 libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0\n  libcilkrts5 libdpkg-perl libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1 libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython-all-dev libpython-dev libpython-stdlib libpython2.7\n  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make manpages-dev python python-all python-all-dev python-dev python-minimal\n  python-pip-whl python-pkg-resources python-setuptools python-wheel python2.7 python2.7-dev python2.7-minimal python3-virtualenv\nSuggested packages:\n  binutils-doc cpp-doc gcc-5-locales debian-keyring g++-multilib g++-5-multilib gcc-5-doc libstdc++6-5-dbg gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-5-multilib libgcc1-dbg libgomp1-dbg\n  libitm1-dbg libatomic1-dbg libasan2-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx0-dbg libquadmath0-dbg glibc-doc libstdc++-5-doc make-doc python-doc python-tk python-setuptools-doc\n  python2.7-doc binfmt-support\nThe following NEW packages will be installed:\n  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5 libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0\n  libcilkrts5 libdpkg-perl libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1 libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython-all-dev libpython-dev libpython-stdlib libpython2.7\n  libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make manpages-dev python python-all python-all-dev python-dev python-minimal\n  python-pip python-pip-whl python-pkg-resources python-setuptools python-wheel python2.7 python2.7-dev python2.7-minimal python3-virtualenv virtualenv\n0 upgraded, 59 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 73.0 MB of archives.\nAfter this operation, 209 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython2.7-minimal amd64 2.7.12-1ubuntu0~16.04.4 [339 kB]\nGet:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python2.7-minimal amd64 2.7.12-1ubuntu0~16.04.4 [1,261 kB]\nGet:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-minimal amd64 2.7.12-1~16.04 [28.1 kB]\nGet:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython2.7-stdlib amd64 2.7.12-1ubuntu0~16.04.4 [1,880 kB]\nGet:5 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python2.7 amd64 2.7.12-1ubuntu0~16.04.4 [224 kB]\nGet:6 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython-stdlib amd64 2.7.12-1~16.04 [7,768 B]\nGet:7 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python amd64 2.7.12-1~16.04 [137 kB]\nGet:8 http://archive.ubuntu.com/ubuntu xenial/main amd64 libmpc3 amd64 1.0.3-1 [39.7 kB]\nGet:9 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 binutils amd64 2.26.1-1ubuntu1~16.04.7 [2,309 kB]\nGet:10 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libc-dev-bin amd64 2.23-0ubuntu10 [68.7 kB]\nGet:11 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 linux-libc-dev amd64 4.4.0-139.165 [862 kB]\nGet:12 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libc6-dev amd64 2.23-0ubuntu10 [2,079 kB]\nGet:13 http://archive.ubuntu.com/ubuntu xenial/main amd64 libisl15 amd64 0.16.1-1 [524 kB]\nGet:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 cpp-5 amd64 5.4.0-6ubuntu1~16.04.10 [7,671 kB]\nGet:15 http://archive.ubuntu.com/ubuntu xenial/main amd64 cpp amd64 4:5.3.1-1ubuntu1 [27.7 kB]\nGet:16 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcc1-0 amd64 5.4.0-6ubuntu1~16.04.10 [38.8 kB]\nGet:17 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libgomp1 amd64 5.4.0-6ubuntu1~16.04.10 [55.1 kB]\nGet:18 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libitm1 amd64 5.4.0-6ubuntu1~16.04.10 [27.4 kB]\nGet:19 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libatomic1 amd64 5.4.0-6ubuntu1~16.04.10 [8,888 B]\nGet:20 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libasan2 amd64 5.4.0-6ubuntu1~16.04.10 [264 kB]\nGet:21 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 liblsan0 amd64 5.4.0-6ubuntu1~16.04.10 [105 kB]\nGet:22 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libtsan0 amd64 5.4.0-6ubuntu1~16.04.10 [244 kB]\nGet:23 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libubsan0 amd64 5.4.0-6ubuntu1~16.04.10 [95.3 kB]\nGet:24 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libcilkrts5 amd64 5.4.0-6ubuntu1~16.04.10 [40.1 kB]\nGet:25 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libmpx0 amd64 5.4.0-6ubuntu1~16.04.10 [9,764 B]\nGet:26 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libquadmath0 amd64 5.4.0-6ubuntu1~16.04.10 [131 kB]\nGet:27 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libgcc-5-dev amd64 5.4.0-6ubuntu1~16.04.10 [2,228 kB]\nGet:28 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 gcc-5 amd64 5.4.0-6ubuntu1~16.04.10 [8,426 kB]\nGet:29 http://archive.ubuntu.com/ubuntu xenial/main amd64 gcc amd64 4:5.3.1-1ubuntu1 [5,244 B]\nGet:30 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libstdc++-5-dev amd64 5.4.0-6ubuntu1~16.04.10 [1,426 kB]\nGet:31 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 g++-5 amd64 5.4.0-6ubuntu1~16.04.10 [8,319 kB]\nGet:32 http://archive.ubuntu.com/ubuntu xenial/main amd64 g++ amd64 4:5.3.1-1ubuntu1 [1,504 B]\nGet:33 http://archive.ubuntu.com/ubuntu xenial/main amd64 make amd64 4.1-6 [151 kB]\nGet:34 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libdpkg-perl all 1.18.4ubuntu1.5 [195 kB]\nGet:35 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 dpkg-dev all 1.18.4ubuntu1.5 [584 kB]\nGet:36 http://archive.ubuntu.com/ubuntu xenial/main amd64 build-essential amd64 12.1ubuntu2 [4,758 B]\nGet:37 http://archive.ubuntu.com/ubuntu xenial/main amd64 libfakeroot amd64 1.20.2-1ubuntu1 [25.5 kB]\nGet:38 http://archive.ubuntu.com/ubuntu xenial/main amd64 fakeroot amd64 1.20.2-1ubuntu1 [61.8 kB]\nGet:39 http://archive.ubuntu.com/ubuntu xenial/main amd64 libalgorithm-diff-perl all 1.19.03-1 [47.6 kB]\nGet:40 http://archive.ubuntu.com/ubuntu xenial/main amd64 libalgorithm-diff-xs-perl amd64 0.04-4build1 [11.0 kB]\nGet:41 http://archive.ubuntu.com/ubuntu xenial/main amd64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]\nGet:42 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1-dev amd64 2.1.0-7ubuntu0.16.04.3 [115 kB]\nGet:43 http://archive.ubuntu.com/ubuntu xenial/main amd64 libfile-fcntllock-perl amd64 0.22-3 [32.0 kB]\nGet:44 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython2.7 amd64 2.7.12-1ubuntu0~16.04.4 [1,071 kB]\nGet:45 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython2.7-dev amd64 2.7.12-1ubuntu0~16.04.4 [27.8 MB]\nGet:46 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython-dev amd64 2.7.12-1~16.04 [7,840 B]\nGet:47 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython-all-dev amd64 2.7.12-1~16.04 [1,006 B]\nGet:48 http://archive.ubuntu.com/ubuntu xenial/main amd64 manpages-dev all 4.04-2 [2,048 kB]\nGet:49 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-all amd64 2.7.12-1~16.04 [996 B]\nGet:50 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python2.7-dev amd64 2.7.12-1ubuntu0~16.04.4 [276 kB]\nGet:51 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-dev amd64 2.7.12-1~16.04 [1,186 B]\nGet:52 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python-all-dev amd64 2.7.12-1~16.04 [1,016 B]\nGet:53 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip-whl all 8.1.1-2ubuntu0.4 [1,110 kB]\nGet:54 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python-pip all 8.1.1-2ubuntu0.4 [144 kB]\nGet:55 http://archive.ubuntu.com/ubuntu xenial/main amd64 python-pkg-resources all 20.7.0-1 [108 kB]\nGet:56 http://archive.ubuntu.com/ubuntu xenial/main amd64 python-setuptools all 20.7.0-1 [169 kB]\nGet:57 http://archive.ubuntu.com/ubuntu xenial/universe amd64 python-wheel all 0.29.0-1 [48.0 kB]\nGet:58 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 python3-virtualenv all 15.0.1+ds-3ubuntu1 [43.2 kB]\nGet:59 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 virtualenv all 15.0.1+ds-3ubuntu1 [4,342 B]\nFetched 73.0 MB in 9s (8,040 kB/s)\nExtracting templates from packages: 100%\nSelecting previously unselected package libpython2.7-minimal:amd64.\n(Reading database ... 54192 files and directories currently installed.)\nPreparing to unpack .../libpython2.7-minimal_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking libpython2.7-minimal:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package python2.7-minimal.\nPreparing to unpack .../python2.7-minimal_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking python2.7-minimal (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package python-minimal.\nPreparing to unpack .../python-minimal_2.7.12-1~16.04_amd64.deb ...\nUnpacking python-minimal (2.7.12-1~16.04) ...\nSelecting previously unselected package libpython2.7-stdlib:amd64.\nPreparing to unpack .../libpython2.7-stdlib_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking libpython2.7-stdlib:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package python2.7.\nPreparing to unpack .../python2.7_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking python2.7 (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package libpython-stdlib:amd64.\nPreparing to unpack .../libpython-stdlib_2.7.12-1~16.04_amd64.deb ...\nUnpacking libpython-stdlib:amd64 (2.7.12-1~16.04) ...\nProcessing triggers for man-db (2.7.5-1) ...\nProcessing triggers for mime-support (3.59ubuntu1) ...\nSetting up libpython2.7-minimal:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSetting up python2.7-minimal (2.7.12-1ubuntu0~16.04.4) ...\nLinking and byte-compiling packages for runtime python2.7...\nSetting up python-minimal (2.7.12-1~16.04) ...\nSelecting previously unselected package python.\n(Reading database ... 54938 files and directories currently installed.)\nPreparing to unpack .../python_2.7.12-1~16.04_amd64.deb ...\nUnpacking python (2.7.12-1~16.04) ...\nSelecting previously unselected package libmpc3:amd64.\nPreparing to unpack .../libmpc3_1.0.3-1_amd64.deb ...\nUnpacking libmpc3:amd64 (1.0.3-1) ...\nSelecting previously unselected package binutils.\nPreparing to unpack .../binutils_2.26.1-1ubuntu1~16.04.7_amd64.deb ...\nUnpacking binutils (2.26.1-1ubuntu1~16.04.7) ...\nSelecting previously unselected package libc-dev-bin.\nPreparing to unpack .../libc-dev-bin_2.23-0ubuntu10_amd64.deb ...\nUnpacking libc-dev-bin (2.23-0ubuntu10) ...\nSelecting previously unselected package linux-libc-dev:amd64.\nPreparing to unpack .../linux-libc-dev_4.4.0-139.165_amd64.deb ...\nUnpacking linux-libc-dev:amd64 (4.4.0-139.165) ...\nSelecting previously unselected package libc6-dev:amd64.\nPreparing to unpack .../libc6-dev_2.23-0ubuntu10_amd64.deb ...\nUnpacking libc6-dev:amd64 (2.23-0ubuntu10) ...\nSelecting previously unselected package libisl15:amd64.\nPreparing to unpack .../libisl15_0.16.1-1_amd64.deb ...\nUnpacking libisl15:amd64 (0.16.1-1) ...\nSelecting previously unselected package cpp-5.\nPreparing to unpack .../cpp-5_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking cpp-5 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package cpp.\nPreparing to unpack .../cpp_4%3a5.3.1-1ubuntu1_amd64.deb ...\nUnpacking cpp (4:5.3.1-1ubuntu1) ...\nSelecting previously unselected package libcc1-0:amd64.\nPreparing to unpack .../libcc1-0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libcc1-0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libgomp1:amd64.\nPreparing to unpack .../libgomp1_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libgomp1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libitm1:amd64.\nPreparing to unpack .../libitm1_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libitm1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libatomic1:amd64.\nPreparing to unpack .../libatomic1_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libatomic1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libasan2:amd64.\nPreparing to unpack .../libasan2_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libasan2:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package liblsan0:amd64.\nPreparing to unpack .../liblsan0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking liblsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libtsan0:amd64.\nPreparing to unpack .../libtsan0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libtsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libubsan0:amd64.\nPreparing to unpack .../libubsan0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libubsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libcilkrts5:amd64.\nPreparing to unpack .../libcilkrts5_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libcilkrts5:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libmpx0:amd64.\nPreparing to unpack .../libmpx0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libmpx0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libquadmath0:amd64.\nPreparing to unpack .../libquadmath0_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libquadmath0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package libgcc-5-dev:amd64.\nPreparing to unpack .../libgcc-5-dev_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libgcc-5-dev:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package gcc-5.\nPreparing to unpack .../gcc-5_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking gcc-5 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package gcc.\nPreparing to unpack .../gcc_4%3a5.3.1-1ubuntu1_amd64.deb ...\nUnpacking gcc (4:5.3.1-1ubuntu1) ...\nSelecting previously unselected package libstdc++-5-dev:amd64.\nPreparing to unpack .../libstdc++-5-dev_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking libstdc++-5-dev:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package g++-5.\nPreparing to unpack .../g++-5_5.4.0-6ubuntu1~16.04.10_amd64.deb ...\nUnpacking g++-5 (5.4.0-6ubuntu1~16.04.10) ...\nSelecting previously unselected package g++.\nPreparing to unpack .../g++_4%3a5.3.1-1ubuntu1_amd64.deb ...\nUnpacking g++ (4:5.3.1-1ubuntu1) ...\nSelecting previously unselected package make.\nPreparing to unpack .../archives/make_4.1-6_amd64.deb ...\nUnpacking make (4.1-6) ...\nSelecting previously unselected package libdpkg-perl.\nPreparing to unpack .../libdpkg-perl_1.18.4ubuntu1.5_all.deb ...\nUnpacking libdpkg-perl (1.18.4ubuntu1.5) ...\nSelecting previously unselected package dpkg-dev.\nPreparing to unpack .../dpkg-dev_1.18.4ubuntu1.5_all.deb ...\nUnpacking dpkg-dev (1.18.4ubuntu1.5) ...\nSelecting previously unselected package build-essential.\nPreparing to unpack .../build-essential_12.1ubuntu2_amd64.deb ...\nUnpacking build-essential (12.1ubuntu2) ...\nSelecting previously unselected package libfakeroot:amd64.\nPreparing to unpack .../libfakeroot_1.20.2-1ubuntu1_amd64.deb ...\nUnpacking libfakeroot:amd64 (1.20.2-1ubuntu1) ...\nSelecting previously unselected package fakeroot.\nPreparing to unpack .../fakeroot_1.20.2-1ubuntu1_amd64.deb ...\nUnpacking fakeroot (1.20.2-1ubuntu1) ...\nSelecting previously unselected package libalgorithm-diff-perl.\nPreparing to unpack .../libalgorithm-diff-perl_1.19.03-1_all.deb ...\nUnpacking libalgorithm-diff-perl (1.19.03-1) ...\nSelecting previously unselected package libalgorithm-diff-xs-perl.\nPreparing to unpack .../libalgorithm-diff-xs-perl_0.04-4build1_amd64.deb ...\nUnpacking libalgorithm-diff-xs-perl (0.04-4build1) ...\nSelecting previously unselected package libalgorithm-merge-perl.\nPreparing to unpack .../libalgorithm-merge-perl_0.08-3_all.deb ...\nUnpacking libalgorithm-merge-perl (0.08-3) ...\nSelecting previously unselected package libexpat1-dev:amd64.\nPreparing to unpack .../libexpat1-dev_2.1.0-7ubuntu0.16.04.3_amd64.deb ...\nUnpacking libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.3) ...\nSelecting previously unselected package libfile-fcntllock-perl.\nPreparing to unpack .../libfile-fcntllock-perl_0.22-3_amd64.deb ...\nUnpacking libfile-fcntllock-perl (0.22-3) ...\nSelecting previously unselected package libpython2.7:amd64.\nPreparing to unpack .../libpython2.7_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking libpython2.7:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package libpython2.7-dev:amd64.\nPreparing to unpack .../libpython2.7-dev_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking libpython2.7-dev:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package libpython-dev:amd64.\nPreparing to unpack .../libpython-dev_2.7.12-1~16.04_amd64.deb ...\nUnpacking libpython-dev:amd64 (2.7.12-1~16.04) ...\nSelecting previously unselected package libpython-all-dev:amd64.\nPreparing to unpack .../libpython-all-dev_2.7.12-1~16.04_amd64.deb ...\nUnpacking libpython-all-dev:amd64 (2.7.12-1~16.04) ...\nSelecting previously unselected package manpages-dev.\nPreparing to unpack .../manpages-dev_4.04-2_all.deb ...\nUnpacking manpages-dev (4.04-2) ...\nSelecting previously unselected package python-all.\nPreparing to unpack .../python-all_2.7.12-1~16.04_amd64.deb ...\nUnpacking python-all (2.7.12-1~16.04) ...\nSelecting previously unselected package python2.7-dev.\nPreparing to unpack .../python2.7-dev_2.7.12-1ubuntu0~16.04.4_amd64.deb ...\nUnpacking python2.7-dev (2.7.12-1ubuntu0~16.04.4) ...\nSelecting previously unselected package python-dev.\nPreparing to unpack .../python-dev_2.7.12-1~16.04_amd64.deb ...\nUnpacking python-dev (2.7.12-1~16.04) ...\nSelecting previously unselected package python-all-dev.\nPreparing to unpack .../python-all-dev_2.7.12-1~16.04_amd64.deb ...\nUnpacking python-all-dev (2.7.12-1~16.04) ...\nSelecting previously unselected package python-pip-whl.\nPreparing to unpack .../python-pip-whl_8.1.1-2ubuntu0.4_all.deb ...\nUnpacking python-pip-whl (8.1.1-2ubuntu0.4) ...\nSelecting previously unselected package python-pip.\nPreparing to unpack .../python-pip_8.1.1-2ubuntu0.4_all.deb ...\nUnpacking python-pip (8.1.1-2ubuntu0.4) ...\nSelecting previously unselected package python-pkg-resources.\nPreparing to unpack .../python-pkg-resources_20.7.0-1_all.deb ...\nUnpacking python-pkg-resources (20.7.0-1) ...\nSelecting previously unselected package python-setuptools.\nPreparing to unpack .../python-setuptools_20.7.0-1_all.deb ...\nUnpacking python-setuptools (20.7.0-1) ...\nSelecting previously unselected package python-wheel.\nPreparing to unpack .../python-wheel_0.29.0-1_all.deb ...\nUnpacking python-wheel (0.29.0-1) ...\nSelecting previously unselected package python3-virtualenv.\nPreparing to unpack .../python3-virtualenv_15.0.1+ds-3ubuntu1_all.deb ...\nUnpacking python3-virtualenv (15.0.1+ds-3ubuntu1) ...\nSelecting previously unselected package virtualenv.\nPreparing to unpack .../virtualenv_15.0.1+ds-3ubuntu1_all.deb ...\nUnpacking virtualenv (15.0.1+ds-3ubuntu1) ...\nProcessing triggers for man-db (2.7.5-1) ...\nProcessing triggers for libc-bin (2.23-0ubuntu10) ...\nSetting up libpython2.7-stdlib:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSetting up python2.7 (2.7.12-1ubuntu0~16.04.4) ...\nSetting up libpython-stdlib:amd64 (2.7.12-1~16.04) ...\nSetting up python (2.7.12-1~16.04) ...\nSetting up libmpc3:amd64 (1.0.3-1) ...\nSetting up binutils (2.26.1-1ubuntu1~16.04.7) ...\nSetting up libc-dev-bin (2.23-0ubuntu10) ...\nSetting up linux-libc-dev:amd64 (4.4.0-139.165) ...\nSetting up libc6-dev:amd64 (2.23-0ubuntu10) ...\nSetting up libisl15:amd64 (0.16.1-1) ...\nSetting up cpp-5 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up cpp (4:5.3.1-1ubuntu1) ...\nSetting up libcc1-0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libgomp1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libitm1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libatomic1:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libasan2:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up liblsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libtsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libubsan0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libcilkrts5:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libmpx0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libquadmath0:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up libgcc-5-dev:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up gcc-5 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up gcc (4:5.3.1-1ubuntu1) ...\nSetting up libstdc++-5-dev:amd64 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up g++-5 (5.4.0-6ubuntu1~16.04.10) ...\nSetting up g++ (4:5.3.1-1ubuntu1) ...\nupdate-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\nSetting up make (4.1-6) ...\nSetting up libdpkg-perl (1.18.4ubuntu1.5) ...\nSetting up dpkg-dev (1.18.4ubuntu1.5) ...\nSetting up build-essential (12.1ubuntu2) ...\nSetting up libfakeroot:amd64 (1.20.2-1ubuntu1) ...\nSetting up fakeroot (1.20.2-1ubuntu1) ...\nupdate-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\nSetting up libalgorithm-diff-perl (1.19.03-1) ...\nSetting up libalgorithm-diff-xs-perl (0.04-4build1) ...\nSetting up libalgorithm-merge-perl (0.08-3) ...\nSetting up libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.3) ...\nSetting up libfile-fcntllock-perl (0.22-3) ...\nSetting up libpython2.7:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSetting up libpython2.7-dev:amd64 (2.7.12-1ubuntu0~16.04.4) ...\nSetting up libpython-dev:amd64 (2.7.12-1~16.04) ...\nSetting up libpython-all-dev:amd64 (2.7.12-1~16.04) ...\nSetting up manpages-dev (4.04-2) ...\nSetting up python-all (2.7.12-1~16.04) ...\nSetting up python2.7-dev (2.7.12-1ubuntu0~16.04.4) ...\nSetting up python-dev (2.7.12-1~16.04) ...\nSetting up python-all-dev (2.7.12-1~16.04) ...\nSetting up python-pip-whl (8.1.1-2ubuntu0.4) ...\nSetting up python-pip (8.1.1-2ubuntu0.4) ...\nSetting up python-pkg-resources (20.7.0-1) ...\nSetting up python-setuptools (20.7.0-1) ...\nSetting up python-wheel (0.29.0-1) ...\nSetting up python3-virtualenv (15.0.1+ds-3ubuntu1) ...\nSetting up virtualenv (15.0.1+ds-3ubuntu1) ...\nProcessing triggers for libc-bin (2.23-0ubuntu10) ...\nvagrant@ubuntu-xenial:~$ cat << EOF > requirements.in\n> django==1.11.16\n> django-haystack==2.5.0\n> EOF\nvagrant@ubuntu-xenial:~$\nvagrant@ubuntu-xenial:~$ ve=$(mktemp -d); virtualenv -p python2.7 $ve; source $ve/bin/activate; pip install pip-tools==3.1.0\nRunning virtualenv with interpreter /usr/bin/python2.7\nNew python executable in /tmp/tmp.jwx9JSitF9/bin/python2.7\nAlso creating executable in /tmp/tmp.jwx9JSitF9/bin/python\nInstalling setuptools, pkg_resources, pip, wheel...done.\nCollecting pip-tools==3.1.0\n  Downloading https://files.pythonhosted.org/packages/f7/58/7a3c61ff7ea45cf0f13f3c58c5261c598a1923efa3327494f70c2d532cba/pip_tools-3.1.0-py2.py3-none-any.whl (43kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.9MB/s\nCollecting six (from pip-tools==3.1.0)\n  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\nCollecting click>=6 (from pip-tools==3.1.0)\n  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 8.9MB/s\nInstalling collected packages: six, click, pip-tools\nSuccessfully installed click-7.0 pip-tools-3.1.0 six-1.11.0\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ uname -a\nLinux ubuntu-xenial 4.4.0-139-generic #165-Ubuntu SMP Wed Oct 24 10:58:50 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.5 LTS\nRelease:        16.04\nCodename:       xenial\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ which python\n/tmp/tmp.jwx9JSitF9/bin/python\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ python --version\nPython 2.7.12\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ which pip\n/tmp/tmp.jwx9JSitF9/bin/pip\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ pip --version\npip 18.1 from /tmp/tmp.jwx9JSitF9/local/lib/python2.7/site-packages/pip (python 2.7)\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ which pip-compile\n/tmp/tmp.jwx9JSitF9/bin/pip-compile\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ pip-compile --version\npip-compile, version 3.1.0\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ pip freeze\nClick==7.0\npip-tools==3.1.0\npkg-resources==0.0.0\nsix==1.11.0\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ cat requirements.in\ndjango==1.11.16\ndjango-haystack==2.5.0\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ rm -rf ~/.cache/pip-tools/\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ pip-compile -r -vv\nUsing indexes:\n  https://pypi.org/simple\n\n                          ROUND 1\nCurrent constraints:\n  django==1.11.16\n  django-haystack==2.5.0\n\nFinding the best candidates:\n  found candidate django==1.11.16 (constraint was ==1.11.16)\n  found candidate django-haystack==2.5.0 (constraint was ==2.5.0)\n\nFinding secondary dependencies:\n  django==1.11.16 not in cache, need to check index\n  django==1.11.16           requires pytz\n  django-haystack==2.5.0 not in cache, need to check index\n  django-haystack==2.5.0    requires Django>=1.8\n\nNew dependencies found in this round:\n  adding [u'django', '>=1.8', '[]']\n  adding [u'pytz', '', '[]']\nRemoved dependencies in this round:\nUnsafe dependencies in this round:\n------------------------------------------------------------\nResult of round 1: not stable\n\n                          ROUND 2\nCurrent constraints:\n  django==1.11.16,>=1.8\n  django-haystack==2.5.0\n  pytz\n\nFinding the best candidates:\n  found candidate django==1.11.16 (constraint was ==1.11.16,>=1.8)\n  found candidate django-haystack==2.5.0 (constraint was ==2.5.0)\n  found candidate pytz==2018.7 (constraint was )\n\nFinding secondary dependencies:\n  django-haystack==2.5.0    requires Django>=1.8\n  django==1.11.16           requires pytz\n  pytz==2018.7 not in cache, need to check index\n  pytz==2018.7              requires -\n------------------------------------------------------------\nResult of round 2: stable, done\n\n#\n# This file is autogenerated by pip-compile\n# To update, run:\n#\n#    pip-compile --output-file requirements.txt requirements.in\n#\ndjango-haystack==2.5.0\ndjango==1.11.16\npytz==2018.7              # via django\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$ pip-sync\nCollecting django==1.11.16\n  Using cached https://files.pythonhosted.org/packages/44/e7/872bbf76aa16b7a061698d75325dac023285db33db4bda8ba8fe5d3bb356/Django-1.11.16-py2.py3-none-any.whl\nCollecting django-haystack==2.5.0\n  Using cached https://files.pythonhosted.org/packages/62/d4/5005a14ef8e94d007f71f2d0ece55411e803b74253212f4c40d02968356f/django_haystack-2.5.0-py2-none-any.whl\nCollecting pytz==2018.7\n  Using cached https://files.pythonhosted.org/packages/f8/0e/2365ddc010afb3d79147f1dd544e5ee24bf4ece58ab99b16fbb465ce6dc0/pytz-2018.7-py2.py3-none-any.whl\ndjango-haystack 2.5.0 has requirement Django<1.10, but you'll have django 1.11.16 which is incompatible.\nInstalling collected packages: pytz, django, django-haystack\nSuccessfully installed django-1.11.16 django-haystack-2.5.0 pytz-2018.7\n(tmp.jwx9JSitF9) vagrant@ubuntu-xenial:~$\n```\n\nnote that pip-sync / pip even knew this was incorrect: `django-haystack 2.5.0 has requirement Django<1.10, but you'll have django 1.11.16 which is incompatible.` \n\n\n. Hi @vphilippon, \nThanks for the reply!  Granted its buried halfway through the post, but I had also noticed the multiline Django requirement.\n\nIts interesting to me that the 'failing' one only has the django constraint adding [u'django', '<1.10', '[]'] and the 'working' one only has the constraint adding [u'django', '>=1.8', '[]']\n\nWhat I didn't know before is if this was a semantically correct definition or not.  Your answer suggests its isn't.  Is there a PEP or other documentation on the formatting of install_requires?  I guess its academic, just curious.  \nDo you think it's reasonable to warn user when dependencies are specified incorrectly?   Like this multiline / same package criteria?  I know these safeguards can never be exhaustive, but I wonder if this is a common misconfiguration issue that could be detected.  \nThanks again.. FWIW, with regard to:\n\nOn a side note, I'm not entirely sure why pip-tools is picking a different one in certain conditions. Not sure if that's in pip-tools or pip realm...\n\nI removed pip-tools from the equation and did a crude test that suggests that pip 'sees' both the lower and upper bounds of the django-haystack requirement even though they are on different lines.  \nWith hardcodes requirements.txt django==1.11.16:\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ cat requirements.txt\ndjango-haystack==2.5.0\ndjango==1.11.16\npytz==2018.7\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ pip freeze\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ pip install -r requirements.txt\nCollecting django-haystack==2.5.0 (from -r requirements.txt (line 1))\n  Using cached https://files.pythonhosted.org/packages/62/d4/5005a14ef8e94d007f71f2d0ece55411e803b74253212f4c40d02968356f/django_haystack-2.5.0-py2-none-any.whl\nCollecting django==1.11.16 (from -r requirements.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/44/e7/872bbf76aa16b7a061698d75325dac023285db33db4bda8ba8fe5d3bb356/Django-1.11.16-py2.py3-none-any.whl\nCollecting pytz==2018.7 (from -r requirements.txt (line 3))\n  Using cached https://files.pythonhosted.org/packages/f8/0e/2365ddc010afb3d79147f1dd544e5ee24bf4ece58ab99b16fbb465ce6dc0/pytz-2018.7-py2.py3-none-any.whl\ndjango-haystack 2.5.0 has requirement Django<1.10, but you'll have django 1.11.16 which is incompatible.\nInstalling collected packages: pytz, django, django-haystack\nSuccessfully installed django-1.11.16 django-haystack-2.5.0 pytz-2018.7\nRelevant line: django-haystack 2.5.0 has requirement Django<1.10, but you'll have django 1.11.16 which is incompatible.\nWith hardcoded requirements.txt django==1.7:\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ cat requirements.txt\ndjango-haystack==2.5.0\ndjango==1.7\npytz==2018.7\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ pip freeze\n(tmp.74S289LswG) vagrant@ubuntu-xenial:~$ pip install -r requirements.txt\nCollecting django-haystack==2.5.0 (from -r requirements.txt (line 1))\n  Using cached https://files.pythonhosted.org/packages/62/d4/5005a14ef8e94d007f71f2d0ece55411e803b74253212f4c40d02968356f/django_haystack-2.5.0-py2-none-any.whl\nCollecting django==1.7 (from -r requirements.txt (line 2))\n  Using cached https://files.pythonhosted.org/packages/d5/5e/a1328223c382024b2b184dacc713086463ce1a60fe1471d500b3b66f840a/Django-1.7-py2.py3-none-any.whl\nCollecting pytz==2018.7 (from -r requirements.txt (line 3))\n  Using cached https://files.pythonhosted.org/packages/f8/0e/2365ddc010afb3d79147f1dd544e5ee24bf4ece58ab99b16fbb465ce6dc0/pytz-2018.7-py2.py3-none-any.whl\ndjango-haystack 2.5.0 has requirement Django>=1.8, but you'll have django 1.7 which is incompatible.\nInstalling collected packages: django, django-haystack, pytz\nSuccessfully installed django-1.7 django-haystack-2.5.0 pytz-2018.7\nRelevant line: django-haystack 2.5.0 has requirement Django>=1.8, but you'll have django 1.7 which is incompatible.. ",
    "GreatBahram": "Thank you and wish me luck! . ",
    "dinio31": "Thanks for your answer. Probably I was not clear enough: I was speaking of the help message which is misleading when there is a custom pip configuration.\n```\n\npip-compile --help\nUsage: pip-compile [OPTIONS] [SRC_FILES]...\n\nCompiles requirements.txt from requirements.in specs.\nOptions:\n...\n  -i, --index-url TEXT            Change index URL (defaults to PyPI)\n```\n(defaults to PyPI)  is hardcoded:\nhttps://github.com/jazzband/pip-tools/blob/master/piptools/scripts/compile.py#L39\nIn the case where a pip.ini or pip.conf file is provided to pip (at the root of a virtualenv for instance), this message is wrong.\nThis message could be: (defaults to PyPI except if you specified an 'index-url' field in pip configuration), or something like that. But it is a bit long.\nA better option would be to dynamically provide the true default coming from pip configuration. I have dug into the code but did not find a simple way to do it. Using pip.index.PackageFinder (as done in PyPIRepository class) should certainly do  the job, but I cannot figure out how to call it prior to all the Click stuff.\n. ",
    "RDIL": "Sorry, I\u2019m no longer a part of the organization and am not authorized to review code at this time.  . Fixed. In that case I'll hold off on merging this. . ",
    "wonderbeyond": "Were I using any cache somewhere?\nI have ever cleaned cache in ~/.cache/pip, ~/.cache/pip-tools.\nHow could I hit the missing kombu==4.2.2? maybe it was ever released but removed from pypi? @thedrow (from @celery project) can you tell? Thanks.. @thedrow However, I've got into trap and cannot find way out. pip-compile always resolve into kombu==4.2.2.\nCan you consider re-release a new kombu==4.2.2?. @atugushev Thanks! after remove requirements.txt and re-execute pip-compile, I got the right kombu==4.2.2.post1.\nI've no expect that pip-compile would read also requirements.txt as input.. ",
    "thedrow": "It was released but removed. Please use 4.2.2-post1. ",
    "codecov[bot]": "Codecov Report\n\n:exclamation: No coverage uploaded for pull request base (master@2598a6d). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #711   +/-\n=========================================\n  Coverage          ?   85.13%         \n=========================================\n  Files             ?       31         \n  Lines             ?     2059         \n  Branches          ?      308         \n=========================================\n  Hits              ?     1753         \n  Misses            ?      235         \n  Partials          ?       71\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2598a6d...bc4960f. Read the comment docs.\n. # Codecov Report\nMerging #715 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #715      +/-\n==========================================\n+ Coverage   85.13%   85.17%   +0.03%   \n==========================================\n  Files          31       31            \n  Lines        2059     2064       +5   \n  Branches      308      309       +1   \n==========================================\n+ Hits         1753     1758       +5   \n  Misses        235      235            \n  Partials       71       71\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli.py | 97.86% <100%> (\u00f8) | :arrow_up: |\n| piptools/repositories/pypi.py | 89.37% <100%> (+0.34%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e03ce4f...5029fcc. Read the comment docs.\n. # Codecov Report\nMerging #718 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #718      +/-\n==========================================\n+ Coverage   85.17%   85.18%   +<.01%   \n==========================================\n  Files          31       31            \n  Lines        2064     2065       +1   \n  Branches      309      309            \n==========================================\n+ Hits         1758     1759       +1   \n  Misses        235      235            \n  Partials       71       71\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_sync.py | 98.21% <\u00f8> (\u00f8) | :arrow_up: |\n| piptools/sync.py | 78.94% <100%> (+0.22%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 62a5b4d...7c46006. Read the comment docs.\n. # Codecov Report\nMerging #720 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #720      +/-\n==========================================\n+ Coverage   85.26%   85.32%   +0.06%   \n==========================================\n  Files          31       31            \n  Lines        2076     2085       +9   \n  Branches      309      310       +1   \n==========================================\n+ Hits         1770     1779       +9   \n  Misses        235      235            \n  Partials       71       71\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli.py | 97.92% <100%> (+0.06%) | :arrow_up: |\n| piptools/logging.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 87.76% <100%> (+0.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c14bcca...097aab3. Read the comment docs.\n. # Codecov Report\nMerging #723 into master will increase coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #723      +/-\n==========================================\n+ Coverage   85.18%   85.26%   +0.07%   \n==========================================\n  Files          31       31            \n  Lines        2065     2076      +11   \n  Branches      309      309            \n==========================================\n+ Hits         1759     1770      +11   \n  Misses        235      235            \n  Partials       71       71\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_sync.py | 98.34% <100%> (+0.13%) | :arrow_up: |\n| piptools/sync.py | 79.38% <100%> (+0.43%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 49efbec...82b6734. Read the comment docs.\n. # Codecov Report\nMerging #727 into master will increase coverage by 5.44%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #727      +/-\n==========================================\n+ Coverage   91.29%   96.73%   +5.44%   \n==========================================\n  Files          35       34       -1   \n  Lines        2262     1930     -332   \n  Branches      306      252      -54   \n==========================================\n- Hits         2065     1867     -198   \n+ Misses        158       40     -118   \n+ Partials       39       23      -16\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/utils.py | 93.68% <\u00f8> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/writer.py | 93.68% <100%> (-0.38%) | :arrow_down: |\n| tests/test_writer.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/conftest.py | 95.18% <100%> (+0.3%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d81936...e246b2a. Read the comment docs.\n. # Codecov Report\nMerging #729 into master will decrease coverage by 0.09%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #729     +/-\n=========================================\n- Coverage   85.32%   85.22%   -0.1%   \n=========================================\n  Files          31       31           \n  Lines        2085     2085           \n  Branches      310      310           \n=========================================\n- Hits         1779     1777      -2   \n- Misses        235      236      +1   \n- Partials       71       72      +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/repositories/pypi.py | 88.12% <0%> (-1.25%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9a789ae...214ccf7. Read the comment docs.\n. # Codecov Report\nMerging #731 into master will increase coverage by 0.23%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #731      +/-\n==========================================\n+ Coverage   86.33%   86.56%   +0.23%   \n==========================================\n  Files          34       34            \n  Lines        2129     2137       +8   \n  Branches      303      302       -1   \n==========================================\n+ Hits         1838     1850      +12   \n+ Misses        226      224       -2   \n+ Partials       65       63       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 90% <100%> (+3.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1545aa7...37ae661. Read the comment docs.\n. # Codecov Report\nMerging #732 into master will increase coverage by 0.29%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #732      +/-\n==========================================\n+ Coverage   85.32%   85.62%   +0.29%   \n==========================================\n  Files          31       31            \n  Lines        2085     2128      +43   \n  Branches      310      310            \n==========================================\n+ Hits         1779     1822      +43   \n  Misses        235      235            \n  Partials       71       71\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_writer.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_top_level_editable.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_cache.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_repositories.py | 100% <0%> (\u00f8) | :arrow_up: |\n| piptools/repositories/base.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_resolver.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_minimal_upgrade.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_cli.py | 97.94% <0%> (+0.01%) | :arrow_up: |\n| tests/test_sync.py | 98.38% <0%> (+0.03%) | :arrow_up: |\n| piptools/resolver.py | 94.66% <0%> (+0.07%) | :arrow_up: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 83453d5...0c9a46c. Read the comment docs.\n. # Codecov Report\nMerging #733 into master will increase coverage by 0.24%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #733      +/-\n==========================================\n+ Coverage   85.57%   85.81%   +0.24%   \n==========================================\n  Files          31       31            \n  Lines        2128     2122       -6   \n  Branches      310      305       -5   \n==========================================\n  Hits         1821     1821            \n+ Misses        236      233       -3   \n+ Partials       71       68       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli.py | 97.92% <100%> (-0.02%) | :arrow_down: |\n| tests/test_writer.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/writer.py | 94.05% <100%> (+4.87%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9976139...7d12a5d. Read the comment docs.\n. # Codecov Report\nMerging #735 into master will increase coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #735      +/-\n==========================================\n+ Coverage   86.25%   86.33%   +0.07%   \n==========================================\n  Files          33       34       +1   \n  Lines        2117     2129      +12   \n  Branches      303      303            \n==========================================\n+ Hits         1826     1838      +12   \n  Misses        226      226            \n  Partials       65       65\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_repositories.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 86.92% <100%> (-0.94%) | :arrow_down: |\n| piptools/pip.py | 100% <100%> (\u00f8) | |\n| tests/test_repository_local.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_top_level_editable.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_repository_pypi.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ebc0e10...ce413df. Read the comment docs.\n. # Codecov Report\nMerging #737 into master will increase coverage by 0.15%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #737      +/-\n==========================================\n+ Coverage   85.81%   85.96%   +0.15%   \n==========================================\n  Files          31       31            \n  Lines        2122     2124       +2   \n  Branches      305      304       -1   \n==========================================\n+ Hits         1821     1826       +5   \n+ Misses        233      232       -1   \n+ Partials       68       66       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_sync.py | 98.44% <100%> (+0.06%) | :arrow_up: |\n| piptools/sync.py | 81.91% <100%> (+2.53%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ede4c62...18d05c0. Read the comment docs.\n. # Codecov Report\nMerging #739 into master will decrease coverage by 0.08%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #739      +/-\n==========================================\n- Coverage   85.96%   85.88%   -0.09%   \n==========================================\n  Files          31       31            \n  Lines        2124     2126       +2   \n  Branches      304      305       +1   \n==========================================\n  Hits         1826     1826            \n- Misses        232      234       +2   \n  Partials       66       66\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/sync.py | 80.2% <0%> (-1.71%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6d871f2...c2c0034. Read the comment docs.\n. # Codecov Report\nMerging #740 into master will increase coverage by 0.17%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #740      +/-\n==========================================\n+ Coverage   85.96%   86.14%   +0.17%   \n==========================================\n  Files          31       31            \n  Lines        2124     2115       -9   \n  Branches      304      304            \n==========================================\n- Hits         1826     1822       -4   \n+ Misses        232      227       -5   \n  Partials       66       66\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/utils.py | 93.68% <\u00f8> (+4.26%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6d871f2...4685448. Read the comment docs.\n. # Codecov Report\nMerging #741 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #741      +/-\n==========================================\n+ Coverage   85.96%   86.03%   +0.06%   \n==========================================\n  Files          31       31            \n  Lines        2124     2120       -4   \n  Branches      304      303       -1   \n==========================================\n- Hits         1826     1824       -2   \n+ Misses        232      231       -1   \n+ Partials       66       65       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli.py | 98.73% <100%> (+0.8%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6d871f2...96798aa. Read the comment docs.\n. # Codecov Report\nMerging #742 into master will increase coverage by 0.03%.\nThe diff coverage is 90.9%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #742      +/-\n==========================================\n+ Coverage   86.03%   86.07%   +0.03%   \n==========================================\n  Files          31       33       +2   \n  Lines        2120     2126       +6   \n  Branches      303      303            \n==========================================\n+ Hits         1824     1830       +6   \n  Misses        231      231            \n  Partials       65       65\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli_sync.py | 100% <100%> (\u00f8) | |\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | |\n| tests/utils.py | 66.66% <66.66%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ad448a1...5f4d333. Read the comment docs.\n. # Codecov Report\nMerging #743 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #743      +/-\n==========================================\n+ Coverage   91.23%   91.29%   +0.06%   \n==========================================\n  Files          34       34            \n  Lines        2258     2274      +16   \n  Branches      305      306       +1   \n==========================================\n+ Hits         2060     2076      +16   \n  Misses        159      159            \n  Partials       39       39\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 89.97% <100%> (+0.07%) | :arrow_up: |\n| #windows | 90.4% <100%> (+0.06%) | :arrow_up: |\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/repositories/pypi.py | 89.59% <100%> (+0.5%) | :arrow_up: |\n| piptools/resolver.py | 94.73% <100%> (+0.07%) | :arrow_up: |\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b92635d...e23e617. Read the comment docs.\n. # Codecov Report\nMerging #745 into master will increase coverage by 1.19%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #745      +/-\n==========================================\n+ Coverage   86.07%   87.27%   +1.19%   \n==========================================\n  Files          33       33            \n  Lines        2126     2082      -44   \n  Branches      303      303            \n==========================================\n- Hits         1830     1817      -13   \n+ Misses        231      201      -30   \n+ Partials       65       64       -1\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 85.83% <\u00f8> (?) | |\n| #windows | 86.4% <\u00f8> (?) | |\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/__main__.py | 83.33% <0%> (-2.39%) | :arrow_down: |\n| piptools/scripts/sync.py | 52.83% <0%> (-0.88%) | :arrow_down: |\n| piptools/repositories/local.py | 90.69% <0%> (-0.8%) | :arrow_down: |\n| tests/conftest.py | 93.75% <0%> (-0.7%) | :arrow_down: |\n| piptools/utils.py | 89.32% <0%> (-0.11%) | :arrow_down: |\n| piptools/scripts/compile.py | 87.76% <0%> (-0.09%) | :arrow_down: |\n| piptools/cache.py | 94.44% <0%> (-0.08%) | :arrow_down: |\n| piptools/resolver.py | 94.59% <0%> (-0.08%) | :arrow_down: |\n| tests/test_writer.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_cli_compile.py | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c45862b...fc69f3a. Read the comment docs.\n. # Codecov Report\nMerging #747 into master will increase coverage by 3.19%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #747      +/-\n==========================================\n+ Coverage   88.03%   91.23%   +3.19%   \n==========================================\n  Files          34       34            \n  Lines        2148     2258     +110   \n  Branches      302      305       +3   \n==========================================\n+ Hits         1891     2060     +169   \n+ Misses        195      159      -36   \n+ Partials       62       39      -23\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 89.9% <100%> (+3.26%) | :arrow_up: |\n| #windows | 90.33% <100%> (+3.27%) | :arrow_up: |\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_cli_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/sync.py | 100% <100%> (+18.08%) | :arrow_up: |\n| tests/test_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/conftest.py | 94.87% <100%> (+0.42%) | :arrow_up: |\n| piptools/exceptions.py | 83.72% <0%> (+9.3%) | :arrow_up: |\n| piptools/scripts/compile.py | 100% <0%> (+10%) | :arrow_up: |\n| piptools/scripts/sync.py | 100% <0%> (+46.29%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 99a3e41...b5328c3. Read the comment docs.\n. # Codecov Report\nMerging #748 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #748   +/-\n=======================================\n  Coverage   87.97%   87.97%         \n=======================================\n  Files          34       34         \n  Lines        2137     2137         \n  Branches      302      302         \n=======================================\n  Hits         1880     1880         \n  Misses        195      195         \n  Partials       62       62\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 86.56% <\u00f8> (\u00f8) | :arrow_up: |\n| #windows | 86.99% <\u00f8> (+1.48%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c1a5d3a...b6f83b9. Read the comment docs.\n. # Codecov Report\nMerging #750 into master will increase coverage by 0.05%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #750      +/-\n==========================================\n+ Coverage   91.23%   91.29%   +0.05%   \n==========================================\n  Files          34       35       +1   \n  Lines        2258     2262       +4   \n  Branches      305      306       +1   \n==========================================\n+ Hits         2060     2065       +5   \n+ Misses        159      158       -1   \n  Partials       39       39\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 89.96% <100%> (+0.06%) | :arrow_up: |\n| #windows | 90.39% <100%> (+0.06%) | :arrow_up: |\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/__main__.py | 100% <100%> (+14.28%) | :arrow_up: |\n| tests/test_data/small_fake_package/setup.py | 100% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b92635d...4d8d8c4. Read the comment docs.\n. # Codecov Report\nMerging #751 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #751   +/-\n=======================================\n  Coverage   91.23%   91.23%         \n=======================================\n  Files          34       34         \n  Lines        2258     2258         \n  Branches      305      305         \n=======================================\n  Hits         2060     2060         \n  Misses        159      159         \n  Partials       39       39\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | ? | |\n| #windows | ? | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b92635d...b99eb55. Read the comment docs.\n. # Codecov Report\nMerging #752 into master will decrease coverage by 1.53%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #752      +/-\n=========================================\n- Coverage   91.23%   89.7%   -1.54%   \n=========================================\n  Files          34      34            \n  Lines        2258    2204      -54   \n  Branches      305     305            \n=========================================\n- Hits         2060    1977      -83   \n- Misses        159     187      +28   \n- Partials       39      40       +1\n```\n| Flag | Coverage \u0394 | |\n|---|---|---|\n| #linux | 89.7% <\u00f8> (-0.21%) | :arrow_down: |\n| #windows | ? | |\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/io.py | 51.2% <0%> (-8.68%) | :arrow_down: |\n| piptools/__main__.py | 83.33% <0%> (-2.39%) | :arrow_down: |\n| tests/test_sync.py | 98.63% <0%> (-1.37%) | :arrow_down: |\n| piptools/repositories/local.py | 90.69% <0%> (-0.8%) | :arrow_down: |\n| tests/conftest.py | 94.2% <0%> (-0.67%) | :arrow_down: |\n| piptools/cache.py | 94.44% <0%> (-0.08%) | :arrow_down: |\n| piptools/resolver.py | 94.59% <0%> (-0.08%) | :arrow_down: |\n| piptools/scripts/sync.py | 100% <0%> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 100% <0%> (\u00f8) | :arrow_up: |\n| tests/test_writer.py | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b92635d...a6a8b83. Read the comment docs.\n. # Codecov Report\nMerging #754 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #754   +/-\n=======================================\n  Coverage   91.29%   91.29%         \n=======================================\n  Files          35       35         \n  Lines        2262     2262         \n  Branches      306      306         \n=======================================\n  Hits         2065     2065         \n  Misses        158      158         \n  Partials       39       39\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d81936...3add05d. Read the comment docs.\n. # Codecov Report\nMerging #756 into master will increase coverage by 0.33%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #756      +/-\n==========================================\n+ Coverage   91.29%   91.63%   +0.33%   \n==========================================\n  Files          35       35            \n  Lines        2262     2282      +20   \n  Branches      306      306            \n==========================================\n+ Hits         2065     2091      +26   \n+ Misses        158      156       -2   \n+ Partials       39       35       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_writer.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/writer.py | 100% <0%> (+5.94%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d81936...17ffcc4. Read the comment docs.\n. # Codecov Report\nMerging #757 into master will increase coverage by 0.31%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #757      +/-\n==========================================\n+ Coverage   91.63%   91.94%   +0.31%   \n==========================================\n  Files          35       35            \n  Lines        2282     2296      +14   \n  Branches      306      306            \n==========================================\n+ Hits         2091     2111      +20   \n+ Misses        156      153       -3   \n+ Partials       35       32       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/utils.py | 100% <\u00f8> (+6.31%) | :arrow_up: |\n| tests/test_utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e4c1d6e...a72ad7b. Read the comment docs.\n. # Codecov Report\nMerging #758 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #758      +/-\n==========================================\n+ Coverage   91.94%   91.97%   +0.03%   \n==========================================\n  Files          35       35            \n  Lines        2296     2305       +9   \n  Branches      306      307       +1   \n==========================================\n+ Hits         2111     2120       +9   \n  Misses        153      153            \n  Partials       32       32\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/repositories/pypi.py | 89.15% <100%> (+0.06%) | :arrow_up: |\n| tests/test_cli_compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/scripts/compile.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update adf6afc...4ab052f. Read the comment docs.\n. # Codecov Report\nMerging #760 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #760   +/-\n=======================================\n  Coverage   91.94%   91.94%         \n=======================================\n  Files          35       35         \n  Lines        2296     2296         \n  Branches      306      306         \n=======================================\n  Hits         2111     2111         \n  Misses        153      153         \n  Partials       32       32\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb83ccf...e171967. Read the comment docs.\n. # Codecov Report\nMerging #763 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #763   +/-\n=======================================\n  Coverage   91.94%   91.94%         \n=======================================\n  Files          35       35         \n  Lines        2296     2296         \n  Branches      306      306         \n=======================================\n  Hits         2111     2111         \n  Misses        153      153         \n  Partials       32       32\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb83ccf...09e62cf. Read the comment docs.\n. # Codecov Report\nMerging #763 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #763      +/-\n==========================================\n+ Coverage   91.94%   91.96%   +0.02%   \n==========================================\n  Files          35       35            \n  Lines        2296     2302       +6   \n  Branches      306      306            \n==========================================\n+ Hits         2111     2117       +6   \n  Misses        153      153            \n  Partials       32       32\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb83ccf...09e62cf. Read the comment docs.\n. # Codecov Report\nMerging #765 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #765      +/-\n==========================================\n+ Coverage   91.99%   92.03%   +0.03%   \n==========================================\n  Files          35       35            \n  Lines        2311     2322      +11   \n  Branches      307      308       +1   \n==========================================\n+ Hits         2126     2137      +11   \n  Misses        153      153            \n  Partials       32       32\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_cli_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8de11ae...118c6ec. Read the comment docs.\n. # Codecov Report\nMerging #765 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #765      +/-\n==========================================\n+ Coverage   91.99%   92.03%   +0.03%   \n==========================================\n  Files          35       35            \n  Lines        2311     2322      +11   \n  Branches      307      308       +1   \n==========================================\n+ Hits         2126     2137      +11   \n  Misses        153      153            \n  Partials       32       32\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| tests/test_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| tests/test_cli_sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/sync.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8de11ae...118c6ec. Read the comment docs.\n. # Codecov Report\nMerging #766 into master will increase coverage by 0.47%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #766      +/-\n==========================================\n+ Coverage   91.99%   92.47%   +0.47%   \n==========================================\n  Files          35       35            \n  Lines        2311     2325      +14   \n  Branches      307      307            \n==========================================\n+ Hits         2126     2150      +24   \n+ Misses        153      147       -6   \n+ Partials       32       28       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| piptools/resolver.py | 100% <100%> (+5.33%) | :arrow_up: |\n| tests/test_resolver.py | 100% <100%> (\u00f8) | :arrow_up: |\n| piptools/exceptions.py | 88.37% <0%> (+4.65%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8de11ae...b4ef774. Read the comment docs.\n. \n",
    "weddige": "I had a look at the issue myself and realized, that the issue is not on the side of pip-tools, but pip itself.\npip-tools does use the cert option implicitly, but pip ignores it for editable requirements:\n```bash\n\nThis does work:\npip install --cert /path/to/bundle.crt git+https://domain/user/repo.git@master#egg=repo\nThis does not work:\npip install --cert /path/to/bundle.crt -e git+https://domain/user/repo.git@master#egg=repo\n```\n\nPip only uses the cert option for its pypi session. However, editable requirements do not use the pypi session: https://github.com/pypa/pip/blob/18.1/src/pip/_internal/operations/prepare.py#L305. See https://github.com/pypa/pip/issues/6183 for the pip issue.. ",
    "impredicative": "I'm having the same error when running pip-compile under Python 3.6.7, pip-tools==3.2.0, and pip==19.0. The problem doesn't happen with pip==18.1.. I don't see a new pypi release out yet for pip-tools, so I don't know why this was closed so soon.. Well, so let's reopen it until the issue is actually resolved. Or should users file a new issue? Ridiculous automated actions are no justification for disregarding commonsense.. ",
    "NejcZupec": "We have the same issue with Python 2.7.15 and 3.7.1, pip-compile-multi==1.2.1 and pip==19.0.\nThe issue was fixed with:\npip install -U pip==18. ",
    "SpotlightKid": "I'm not sure if this related, but the latest release (3.3.0) apparently hasn't been uploaded to PyPI yet as well.. Cool. AUR package updated.. ",
    "eliasbrange": "Trying to investigate a bit further. Noticed that pip-compile succeeds with pytzdata which also uses poetry as a build system: https://github.com/sdispater/pytzdata/blob/master/pyproject.toml. Which is described in pendulums pyproject.toml as per PEP517/PEP518.\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\". So that should probably be an optional flag to let consumers choose between installing required packages themselves, or doing isolated builds (which will take forever if there are lots of packages using different build systems).. ",
    "jkp": "Just ran into this with a package of mine that uses pyproject.toml to declare some build system requirements.  Forcing build isolation as mentioned above fixed the issue.\nUnderstand that a breaking change is problematic - is there a plan to at least add an optional flag to allow users to specify this behaviour if they need it?. Will this get merged?. OK! So basically put the new requirement first and it should solve the issue...nice.  I will try that!. ",
    "bjmc-globus": "Ah, my mistake.. ",
    "tribbloid": "Thanks a lot! yeah you are right, it is indeed displayed after enabling '-v'.\nHowever after switching to the latest github build it triggers another error:\n```\n$ pip-compile -v\nUsing indexes:\n  https://pypi.org/simple\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/bin/pip-compile\", line 11, in \n    sys.exit(cli())\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/click/core.py\", line 764, in call\n    return self.main(args, kwargs)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/click/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/click/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, ctx.params)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/click/core.py\", line 555, in invoke\n    return callback(args, **kwargs)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/piptools/scripts/compile.py\", line 173, in cli\n    dist = run_setup(src_file)\n  File \"/home/shared/anaconda3/lib/python3.6/distutils/core.py\", line 227, in run_setup\n    script_name)\nRuntimeError: 'distutils.core.setup()' was never called -- perhaps 'setup.py' is not a Distutils setup script?\n```. In the meantime, I still hope forced dependency overriding is allowed (maven's primary rule of breadth-first dominating others and auxiliary rule of transitive dependency exclusion could come in handy). Otherwise if all dependency projects have a transitive version lockdown, then they almost become mutually exclusive\n. sorry, that was a low-level mistake, I accidentally ran a git revert which removed my requirements.in file. Problem solved. ",
    "y3g0r": "@atugushev exactly, the issue is on that line. In my fork I have a quick fix, but it's too dirty to use it for PR.\nAfter some investigation I realized that the issue is in fact on pip side, because Link.is_artifact is supposed to return true only if the link is something like a link to a tarbal. \nSo we can raise/ping ticket on pip side and create a quick fix on pip-tools side or just wait until it's fixed in pip. ",
    "NickG123": "Hi @atugushev, that does solve the issue.\nApologies for the duplicate issue, I will follow along #721.. ",
    "milind-shakya-sp": "@atugushev opened a pr here which seems fix this issue here https://github.com/jazzband/pip-tools/pull/763. @atugushev Please review when you get a chance.. @atugushev added test.. @atugushev updated.. ",
    "jzoldak": "nit: typo s/elemnt/element\n. nit: line break not required, seems not to be the convention in this repo.\n. Looks like it'd be pretty easy to add a testcase for this new code path in tests/test_fake_index.py.\n. This logic (and the same in sync.py) would probably be better off in one place, like utils.py.\n. Could this condition be triggered by a requirement that comes through from a sub-dependency? If so it might be confusing for the user, given that it wasn't something they configured in their own .in file.\n. nit: typo s/rais/raise\n. It looks like you might be able to easily add some testcases to test_resolver.py.\n. Since the variable isn't used in the loop, by convention it should start with \"_\"\n\"_name\" would be better\n. Is there a way to write this in a more straightforward manner? Popping off the links list feels to me like there might be a bug in that the links set is not ordered, so there might be a potential for this 1st call to the 'first' method could be looking for the link associated with the 2nd ireq. And then the next line won't find the other ireq because its link value has already been popped off the list of links.\nE.g.\n```\n\n\n\nfrom first import first\nlinks = set(['bar', 'foo'])\nfirst(['bar', 'foo'], key=lambda x: x == links.pop())\nlinks\nset([])\n```\n. \n\n\n",
    "jonathanburns": "I think you mean WAS set, right?. ",
    "browniebroke": "I think this makes a nicer contributor experience, as the Github status wouldn't be \"failed\".\nLet me know if you'd rather not have that, I'll take it out.. "
}